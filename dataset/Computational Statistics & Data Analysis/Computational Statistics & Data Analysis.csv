name,institution,publish_date,doi,cite,abstract,introduction,Title,Url,Time,Year,Type,Unnamed: 0
"Bar Haim,Wells Martin T.","Department of Statistics, University of Connecticut, Room 315, Philip E. Austin Building, Storrs, 06269-4120, CT, USA,Department of Statistics and Data Science, Cornell University, 1190 Comstock Hall, Ithaca, 14853, NY, USA","Received 2 June 2022, Revised 6 June 2023, Accepted 7 June 2023, Available online 14 June 2023, Version of Record 14 June 2023.",https://doi.org/10.1016/j.csda.2023.107800,Cited by (0), features when ,", cannot exceed the sample size, ====. Using conventional notation, where the outcome (response) variable is ==== predictors, ====, plus some random (Gaussian) noise, ====, that is,====. If ==== does not exist, and we say that ==== is unidentifiable. Even if ====, inference about ==== may be impractical when ==== is sufficiently large because standard errors are often large and the width of the confidence interval grows with ====. For example, Hotelling's ==== yields confidence intervals with width which is proportional to ====.====To deal with the fact that many modern applications involve a large number of putative predictors and often a modest sample size, statisticians had to develop variable selection methods capable of identifying the true predictors, while limiting the number of irrelevant predictors from being included in the regression model. Arguably, most famous among such methods is the LASSO (====. For example, ==== denote the active set of variables by ==== and its cardinality by ====. To prove their main result (Theorem 2.2) they further require that the ==== samples are i.i.d. Gaussian, ==== is row-wise sparse: ====. Under these assumptions ==== and obtain an ====. We elaborate on other related methods and results in Section ====.====While applying the linear model with Gaussian errors to high-dimensional problems has been a natural step which yielded extraordinary advances, both in theory and in applications, it also required making strong assumptions, including sparsity of the mean vector and rows of the covariance (or precision) matrix. In the original, small-==== setting, the assumption that the predictors are uncorrelated seemed reasonable, but correlation between columns of ==== is inevitable when ==== is large, and for the most part, the approaches to deal with such correlations have relied on a somewhat ad-hoc two-stage approach, where in the first step a dimension reduction is performed (e.g., by clustering) in order to restore at least in part the validity of the requirement that ==== is invertible.====Another motivation for extending the linear model framework to the large-==== is associated with ==== units increase in ====’. However, in many cases involving a large number of predictors there is no reason to think that the relationship between ==== and ==== is linear. For example, a quantitative trait may depend on the expression of many genes in an intricate way, so that we cannot use statements like ‘holding all other variables constant’, and we cannot draw conclusions like ‘an increase in expression of gene ==== is associated with ==== units increase in the trait’, because a change in the expression of that gene may not occur without a simultaneous change in many other genes. For the same reason the ====, as is the case if the predictors represent repeated measurements (for example, daily log-returns of stocks). It is possible to reduce the dimensionality by taking representative predictors, but doing so results in loss of information about the most prominent feature of the data, namely, its ==== structure.==== and ====, nor the uniqueness of ==== variables. Our approach, which is described in the next section, does not require sparsity assumptions, and attains the estimated network structure in a single step. We may, however, choose to treat one variable as a ‘response’, and use our method to perform variable selection by identifying all the nodes (variables) connected via an edge to the response node in the graph.====Like LARS (====), our method uses partial correlations, but in a very different way. LARS is a stepwise process in which each step involves updating the coefficients of the regression and the residuals, and recalculating the correlations between the remaining predictors and the residuals. Our method calculates all the pairwise correlations just once. LARS is a variable selection method, used when the goal is to find which predictors are associated with a response variable, while our method finds all the connections between all variables simultaneously. Furthermore, LARS relies on ====Our method is based on ideas and results from convex geometry, some of which may seem counter intuitive at first glance. The relevant theorems are stated in Section ====, but for a comprehensive (and very enjoyable) introduction to convex geometry see ==== and ====, Chapter 2. The latter reference contains applications to modern data science challenges. The key to our method is ‘flipping’ the roles of variables and observations and treat the data as ==== points in ====, so that each predictor is characterized by ==== samples. The classical approach views data as ==== points in ====, and in the high dimensional setting where ====, all the ====. This degeneracy causes difficulties for classical statistical methods. However, if we view the data as ==== vectors in ====.====Using a key distribution result (see ==== in Sec. ==== is sufficiently large. The null distribution of the squared sine of angles between random pairs is ==== and ==== are estimated from the data. This allows us to get more power, and check model adequacy. It also allows us to adapt the model to situations where the samples are not i.i.d., and we do so by replacing the ==== parameter in the null component by ====, where ==== is the ‘effective sample size’. In many cases, the i.i.d. assumption is unrealistic, and using ==== as the null distribution will lead to many false discoveries.====Furthermore, the convex geometry literature establishes a remarkable asymptotic relationship between ==== and ====. Specifically, ==== in Sec. ==== states that the number of random lines that go through the origin in ==== and are approximately perpendicular, grows exponentially with ====. In the context of graphical models this means that as the sample size grows, the number of possible null edges grows exponentially with ==== (equivalently, ====). Combined with ====, this means that our method can detect null edges in large graphs with ====, while controlling the false detection rate.====; ====; ====; ====; ====; ====; ====). The primary difficulty is that these models give rise to curved exponential families. The zero entries in the covariance matrix Σ translate into complicated restrictions on the entries of the natural parameter ==== matrix, so a covariance graphical model cannot be viewed as a concentration graphical model.====In Section ==== we describe some mathematical background for convex geometry, and our mixture model. In Section ==== we demonstrate the betaMix model in simulations, and in Section ==== we show how the method can be applied to a wide range of big-data applications. In Section ==== we review related work, and we conclude with a brief discussion in Section ====.====Despite the widespread and applicability in statistical modeling, linear subspaces suffer from the drawback that they cannot be analyzed using Euclidean geometry. Indeed, subspaces of ==== lie on a special type of Riemannian manifolds, the Grassmann manifold, which has a nonlinear structure. The Grassmann manifold ==== is used to study the geometry of the space of all ==== dimensional subspaces of ====. ==== is isomorphic to the quotient set ====, where ==== denotes the group of ==== orthogonal matrices (====; ====).====The manifold ==== has an invariant measure (====; ====) which can be used to calculate the volumes of sets which are specified in terms of the principal angles ==== between ==== and ==== dimensional subspaces of ====. The principal angles between subspaces are the generalization of the concept of the angle between lines. Let ==== and ==== be two subspaces in ==== and ==== (====) having a set of principal angles ====, with ==== where ==== for ==== and ====. The corresponding ==== pairs of orthogonal unit vectors are ====. By setting ==== gives the canonical correlations ==== and corresponding pairs of canonical variables ==== (====). The chordal distance between ==== and ==== is ==== (====) and the maximum chordal distance is ==== (====; ====). The invariant measure of the principal angles ==== can be constructed by viewing ==== as ====, where ==== denotes the Stiefel manifold of all orthonormal ====-frames in ==== (====). By deriving the exterior differential forms on those manifolds, ==== gave an expression for the invariant (uniform) measure of the principal angles ==== for (====)==== over ====. The normalization constant is given by==== where ==== is the area of the unit sphere ====.====In the special case where ==== (which is the focus of this paper) the Grassmann manifold ==== is a generalization of the projective space ==== corresponding to the lines passing through the origin of the Euclidean space (====, pg.30). The chordal distance between two lines is the sine of their angle. On ==== the invariant measure ==== has a simple expression for the density of the canonical angle ==== (====; ====)==== where ==== is the beta function. This nice result implies that the ==== has a ==== distribution or equivalently ==== has a ==== distribution. These are precisely the measure the underlie the spherical cap calculations discussed in the previous section (====, ====; ====, ====; ====; ====) that was used in the development of correlation screening rules. Note that ==== can be expressed in terms of the invariant measure ==== for the case ==== in ====.====To generalize our method we would like to consider the sine of the principal angles between two random ==== dimensional subspaces (rather than just lines). ==== use the Gauss hypergeometric function ==== with a matrix argument to give the density of the largest principal angle between two random subspaces. We give the necessary definitions of the Gauss hypergeometric function in the Supplementary Material. Using Theorem 1 in ==== and the transformation ==== gives the multivariate analog of ====:",On graphical models and convex geometry,https://www.sciencedirect.com/science/article/pii/S0167947323001111,Available online 14 June 2023,2023,Research Article,0.0
"Chen Yewen,Chang Xiaohui,Luo Fangzhi,Huang Hui","School of Mathematics, Sun Yat-sen University, Guangzhou, Guangdong 510275, China,College of Business, Oregon State University, Corvallis, OR 97331, USA","Received 5 August 2022, Revised 20 May 2023, Accepted 5 June 2023, Available online 13 June 2023.",https://doi.org/10.1016/j.csda.2023.107799,Cited by (0),"Numerical air quality models are pivotal for the prediction and assessment of air pollution, but numerical model outputs may be systematically biased. An additive dynamic model is proposed to correct large-scale raw model outputs using data from other sources, including readings collected at ground monitoring networks and weather outputs from other numerical models. An additive partially linear model specification is employed for the nonlinear relationships between air pollutants and covariates. In addition, a multi-resolution basis function approximate is proposed to capture the different small-scale variations of biases, and a discretized stochastic integro-differential equation is constructed to characterize the dynamic evolution of the random coefficients at each spatial resolution. An expectation-maximization algorithm is developed for parameter estimation and a multi-resolution ensemble-based scheme is embedded to accelerate the computation. For statistical inference, a conditional simulation technique is applied to quantify the uncertainty of parameter estimates and bias correction results. The proposed approach is used to correct the biased raw outputs of PM==== from the Community Multiscale Air Quality (CMAQ) system for China's Beijing-Tianjin-Hebei region. Our method improves the root mean squared error and continuous rank probability score by 43.70% and 34.76%, respectively. Compared to other statistical methods under different metrics, our model has advantages in both correction accuracy and computational efficiency.","Numerical air quality models, such as the Nested Air Quality Prediction Modeling System (NAQPMS) (====) or the Community Multiscale Air Quality (CMAQ) model (====), have been extensively used in environmental science, especially air pollution forecast. Based on numerical models, the pollution concentration forecasts at fine spatial and temporal scales are generated by solving deterministic differential equations. However, numerical model outputs are usually biased. The reasons are manifold. Due to the expensive computation of the numerical models, the unknown parameters of the differential equations cannot be adjusted timely when new data become available (====). The differential equations in numerical models depend heavily on initial inputs such as meteorology variables, emission, transportation dynamics, and ground characteristics (====; ====; ====). The errors in initial inputs also introduce forecast bias. Therefore, calibration is critical for both the model parameters (====; ====; ====; ====) and model outputs. While the more general framework for forecasting is to tune the input parameters of computer models than to calibrate numerical model outputs, the latter has gained attention in statistics; see ====; ====; ====, ====; ==== and references therein. In this work, our motivation is to correct the bias of raw CMAQ outputs instead of the model parameters inputs, and hope to apply the corrected pollution map to other studies (e.g., to access the health impacts of air pollutants, see ====). To this end, we focus on building spatiotemporal statistical bias correction models for the raw CMAQ data along with other data sources. The proposed approach is a data-adaptive way to describe how the bias in raw data is developed, and provides a flexible and efficient framework to generate accurate pollution maps, especially for researchers outside the environmental science community who have no access to numeral modeling systems.====To improve the bias correction accuracy, a strategy commonly used is to combine the numerical model outputs with data from other sources. The most important data source in our study is reading from monitoring stations. Although monitoring stations are usually sparsely located and occasionally suffer from malfunctions, they provide direct measurements of actual pollutant concentrations and serve as a valuable reference for regional corrections.====In the literature, there are several methods to fuse model outputs with data from multiple sources. Under a point-to-block framework, ==== proposed a fully model-based scheme, also known as the “Bayesian melding,” where the grid-level model output is expressed as an integral of the underlying point-level process that is related to actual observations. The Bayesian melding approach provides a flexible Bayes procedure to account for the bias in numerical model outputs. However, for a large spatial domain, grid cells are too many to make the Bayesian melding computationally infeasible. To reduce the computational cost, ==== employed a latent process on the grid level rather than the point level and developed a spatio-temporal version of the Bayesian melding method. Different from the Bayesian melding approach, ====, ====, ==== proposed an alternative model-based strategy, known as “downscaling,” where the outputs from the numerical model are downscaled from grid level to point level. To be specific, the numerical model outputs are downscaled as a site-specific explanatory variable, which is related to the site observational data via a spatially varying coefficients (SVC) regression model (====). The varying coefficients are assumed to be Gaussian processes via the coregionalization method (====), and then the correction in each grid cell can be computed by a stochastic integral over the corresponding grid cell; see ==== for more details.====Despite the widespread usage of the approaches mentioned earlier such as the Bayesian statistical downscaling approach (====) for improving CMAQ-based outputs (====; ====; ====), it remains challenging to further improve the bias correction accuracy and computational efficiency (====). Firstly, most models used in bias correction are performed in a linear regression framework (====), which may not be able to accurately characterize the relationships between the response and other variables. For example, ==== shows that the PM==== (i.e., fine particulate matters with aerodynamic diameters less than ====) concentrations are highly nonlinear with respect to weather variables in China's Beijing. Secondly, classical space-time downscaling approaches such as the smoothed downscaler model (====) often adopt conditional autoregressive structures (e.g., AR(1)) to capture temporal correlation, while the evolution of spatio-temporal interactions is ignored. Lastly, the Bayesian methods used in earlier studies face computational difficulties for large datasets and hinder their broader applications.====In this work, we propose an additive dynamic model to correct raw numerical model outputs. An additive partially linear model is employed to capture the relationships between observations and numerical model outputs. We model the small-scale variations of the additive biases of model outputs using a multi-resolution basis function approximation. An effective discretized stochastic integro-differential equation (IDE) model defined on multi-resolution regular grid cells is used to characterize the dynamic evolution of the random coefficients. For parameter estimation, we develop an efficient algorithm by embedding a multi-resolution ensemble-based method in the expectation-maximization (EM) algorithm, which enables fast computations of the latent spatio-temporal process filtering, smoothing, and parameter estimation. Moreover, the uncertainties of parameter estimate and bias correction are quantified using a conditional simulation technique that is computationally efficient (====). Our approach differs from earlier methods in that, it not only captures the nonlinear relationship between response and other variables, but also pictures the dynamic evolution of multi-resolution space-time interaction. In addition, the computational complexity of our method is not heavily affected by the data size, especially the number of grid cells of numerical model outputs. The proposed bias correction approach is demonstrated using the PM==== concentration data in China's Beijing-Tianjin-Hebei (BTH) region along the CMAQ model outputs and the readings at the monitoring stations.====The rest of the paper proceeds as follows. We introduce the PM==== datasets in Section ====, describe the proposed addictive dynamic correction model (ADCM) in Section ====, and explain the implementation of the method in Section ====. Section ==== investigates the ADCM's ability to capture nonlinear relationships and spatio-temporal patterns using a simulation study. In Section ====, using the data from the BTH region, we first compare the prediction performance of the ADCM with several widely used methods and then apply the ADCM to correct the raw CMAQ PM==== outputs. We conclude with a discussion in Section ====. Technical details of the competing models can be found in the Supplementary Material. Data and code for this work are available online under ====.",Additive dynamic models for correcting numerical model outputs,https://www.sciencedirect.com/science/article/pii/S016794732300110X,Available online 13 June 2023,2023,Research Article,1.0
"Boente Graciela,Parada Daniela","CONICET and Universidad de Buenos Aires, Argentina,Universidad de Buenos Aires, Argentina","Received 3 October 2022, Revised 29 May 2023, Accepted 5 June 2023, Available online 12 June 2023.",https://doi.org/10.1016/j.csda.2023.107798,Cited by (0),"Functional quadratic regression models postulate a polynomial relationship rather than a linear one between a scalar response and a functional covariate. As in functional linear regression, vertical and especially high–leverage outliers may affect the classical estimators. For that reason, providing reliable estimators in such situations is an important issue. Taking into account that the functional polynomial model is equivalent to a regression model that is a polynomial of the same order in the functional principal component scores of the predictor processes, our proposal combines robust estimators of the principal directions with robust regression estimators based on a bounded loss function and a preliminary residual scale estimator. Fisher–consistency of the proposed method is derived under mild assumptions. Consistency, asymptotic robustness as well as an expression for the influence function of the related functionals is derived when the covariates have a finite–dimensional expansion. The results of a numerical study show the benefits of the robust proposal over the one based on sample principal directions and least squares for the considered contaminating scenarios. The usefulness of the proposed approach is also illustrated through the analysis of a real data set which reveals that when the potential outliers are removed the classical method behaves very similarly to the robust one computed with all the data.","In the last decades, functional explanatory variables have been included in regression models either nonparametrically or through parametric models. Within the field of functional data analysis, some excellent overviews are provided in ==== who presents a careful treatment of nonparametric models and also in the books by ====, ====, ==== and ==== who place emphasis on the functional linear model. Various aspects of this last model including implementations and asymptotic theory, have been studied among others in ====, ====, ====, ====, ====, ==== and ====. The functional linear model imposes a structural linear constraint on the regression relationship which may or may not be satisfied. Some procedures to test the goodness of fit in such models have been discussed among others in ====, ==== and ====.====The linear constraint circumvents the ==== present when considering fully nonparametric models since in the infinite–dimensional function space, the elements of a finite sample of random functions are very far away from each other. However, as pointed out in ==== and ====, this linear model imposes a constraint on the regression relationship that may be too restrictive for some applications. To preserve a reasonable structural constraint, but at the same time improving the model flexibility within the class of parametric models, ==== defined a functional polynomial model analogous to the extension from simple linear regression to polynomial regression. As in functional linear regression, regularization is a key step to define the estimators. For that reason, ==== and ==== project the predictor on the eigenfunctions basis of the process, which is then truncated at a reasonable number of included components, leading to a parsimonious representation. With this representation, ==== have shown that the functional polynomial regression model can be represented as a polynomial regression model in the functional principal component scores of the predictor process.====In this paper, we consider independent and identically distributed observations with the same distribution as ====, where the response ==== is related to the functional explanatory variable ==== according to the quadratic model ====, where ==== denotes the usual ==== inner product, ==== is a residual scale parameter, and ==== is the error term, independent of ====. In this model, the regression parameter ==== is assumed to be in ==== and ==== is a linear operator, that without loss of generality may be assumed to be self–adjoint, that is, if ==== then ====. Furthermore, we will also assume that ==== is Hilbert–Schmidt, that is, ====. The quadratic term ==== appearing in the model reflects that beyond the effect that the values ====, ====, have on the response, the products ====, for ====, are also included as additional predictors.====As it has been extensively described, small proportions of outliers and other atypical observations can affect seriously the estimators for regression models and the situation in functional linear or quadratic models is not an exception. Robust proposals for functional linear regression models using ====-splines or ====-splines were considered in ====, ==== and ====, while an approach combining robust functional principal components and robust linear regression was studied in ====.====As mentioned in ====, different types of outliers may arise when considering functional data. These authors pointed out that, in the functional setting, atypical data might consist of curves that behave differently from the others displaying a persistent behaviour either in shift, amplitude and/or shape making more difficult their detection. Several detection criteria have been given in the literature based on different notions of depths, dimension reduction and/or visualization tools. Among others, we can mention the procedures described in ====, ====, ====, ====, ====, ====, ====.====However, it should be noticed that when providing robust procedures for linear regression models with covariates in ====, outliers in the covariates are not automatically eliminated in a first step using some diagnostic method. The main reason is that the atypical data in the explanatory variables may not always be bad high–leverage observations, since some of them may help in the fitting process. ====-estimators with bounded loss functions provide an alternative choice for robust regression, in which good leverage points are not discarded. The same approach should be followed when dealing with functional covariates and quadratic models, so even when different outlier detection rules exist, it is better to adapt the best practices of robust estimation to this setting.====In this paper, we adapt the robust procedures for multiple linear regression estimators to the functional quadratic regression model. More precisely, we first compute robust estimators of the principal directions with the aim of providing finite–dimensional candidates for the estimators of both the functional regression parameter and the quadratic operator. We then apply ====-regression estimators (====) that are based on a bounded loss function and a preliminary residual scale estimator to the residuals obtained from these finite–dimensional spaces. The initial scale estimator ensures that the estimators of ====, ==== and ==== are scale equivariant, while the bounded loss function and the robust principal directions guarantee that the resulting procedure will be robust against high–leverage outliers. It is worth mentioning that the presence of outliers in the functional covariates may affect the estimation procedure when the sample principal components are used to estimate the regression function and quadratic operator, even when ====-estimators are used. The main reason is that a distorted estimator of the principal direction will affect the scores of all the observations in that direction, that is why robust estimators of the principal direction are needed. Among others, one may consider the spherical principal components introduced in ==== and studied in ====, ==== and ==== or the projection–pursuit approach considered in ==== and ====.====We illustrate our approach with the Tecator data set (see ====). This food quality–control data contains 215 samples of finely chopped meat with different percentages of fat, protein and moisture content. For each sample, a spectrometric curve of absorbances was measured using a Tecator Infratec Food and Feed Analyzer. To predict the fat content of a meat sample from its absorbance spectrum, ==== fitted a functional quadratic model, while ==== tested the significance of the quadratic term. However, ==== and ====, among others, showed the presence of atypical data in the spectrometric curves. Thus, a reliable analysis of the Tecator data set requires procedures protecting from outliers in the absorbance spectrum.====The rest of the paper is organized as follows. The model and our proposal are described in Section ====. One important issue to be considered when defining a robust estimator is if it is indeed estimating the target quantity. This property is known as Fisher–consistency and is usually a first step before obtaining consistency results. Fisher–consistency can be easily described when the estimator can be written as a functional applied to the empirical distribution, see for instance ====. Let ==== be such functional, that is, ==== where ==== denotes the space of probability measures over the space where the observations belong and Θ stands for the parameter space, in our case ==== with ==== the space of linear, self–adjoint and Hilbert–Schmidt operators. Then, if the target is to estimate a parameter ==== and the observations have distribution ====, the estimator is called Fisher–consistent when ====. In Section ====, conditions ensuring Fisher–consistency of the proposed procedure are studied both for finite and infinite–dimensional processes, while in Section ==== robustness properties and consistency are studied for finite–dimensional ones. The performance and advantages of the proposed methods for finite–samples are numerically illustrated in Section ====. Section ==== contains the Tecator data set analysis, while final comments are given in Section ====. All proofs are relegated to the supplementary file.====The following is the Supplementary material related to this article.",Robust estimation for functional quadratic regression models,https://www.sciencedirect.com/science/article/pii/S0167947323001093,Available online 12 June 2023,2023,Research Article,2.0
"Priddle Jacob W.,Drovandi Christopher","Queensland University of Technology, 2 George St, Brisbane, 4000, Queensland, Australia","Received 4 January 2022, Revised 30 May 2023, Accepted 5 June 2023, Available online 12 June 2023.",https://doi.org/10.1016/j.csda.2023.107797,Cited by (0),"Bayesian synthetic likelihood (BSL) is an established method for performing approximate Bayesian inference when the likelihood function is intractable. In synthetic likelihood methods, the likelihood function is approximated parametrically via model simulations, and then standard likelihood-based techniques are used to perform inference. The Gaussian synthetic likelihood estimator has become ubiquitous in BSL literature, primarily for its simplicity and ease of implementation. However, it is often too restrictive and may lead to poor posterior approximations. Recently, a more flexible semi-parametric Bayesian synthetic likelihood (semiBSL) estimator has been introduced, which is significantly more robust to irregularly distributed summary statistics. This work proposes a number of extensions to semiBSL. First, even more flexible estimators of the marginal distributions are considered, using transformation kernel density estimation. Second, whitening semiBSL (wsemiBSL) is proposed – a method to significantly improve the computational efficiency of semiBSL. wsemiBSL uses an approximate whitening transformation to decorrelate summary statistics at each algorithm iteration. The methods developed herein significantly improve the versatility and efficiency of BSL algorithms.","Simulator models are a type of stochastic model that is often used to approximate a real-life process. Unfortunately, the likelihood function for simulator models is generally computationally intractable, and so obtaining Bayesian inferences is challenging. Approximate Bayesian computation (ABC) (====) and Bayesian synthetic likelihood (BSL) (====; ====; ====) are two methods for approximate Bayesian inference in this setting. Both methods eschew evaluation of the likelihood by repeatedly generating pseudo-observations from the simulator, given an input parameter value. ABC and BSL methods have been applied in many different fields; recently, in biology, to model the spread of the Banana Bunchy Top Virus (====); in epidemiology, to model the transmission of HIV (====) and tuberculosis (====), and, in ecology, to model the dispersal of little owls (====). ABC is a more mature and established technique than BSL, and so it is more prevalent in applied fields. However, ABC can suffer from the curse of dimensionality with respect to the dimension of the summary statistic, requires a large number of model simulations, and the results can be highly dependent on a set of tuning parameters. BSL methods can scale more efficiently to the number of summary statistics and hence require substantially fewer model simulations (====), and require less tuning (====). Thus in various likelihood-free problems, BSL may be preferred over ABC, and thus there is interest in further improving its performance and extending its applicability.====Synthetic likelihood methods approximate the likelihood function with a tractable distribution; in contrast, ABC methods are effectively non-parametric (====). The original synthetic likelihood method of ==== approximates the summary statistic likelihood with a Gaussian distribution and then uses a Markov chain Monte Carlo (MCMC) sampler for maximum likelihood estimation. Later, ==== consider the Gaussian synthetic likelihood in the Bayesian setting, and refer to their method as Bayesian synthetic likelihood. In practice, the Gaussian assumption of the summary statistic vector may be too restrictive, leading to a poor estimate of the likelihood, and then a poor estimate of the posterior. Herein, we refer to the Gaussian BSL method as standard BSL, denoted sBSL.====A few authors have considered more flexible density estimators to improve the robustness of sBSL to irregular summary statistic distributions (e.g. ====; ====; ====). In particular, the semi-parametric Bayesian synthetic likelihood (semiBSL) method of ====, estimates the intractable summary statistic likelihood semi-parametrically – non-parametrically estimating the marginal distributions using kernel density estimation (KDE), and parametrically estimating the dependence structure using a Gaussian copula. ==== show empirically that semiBSL performs favourably to sBSL when summary statistics are distributed irregularly. semiBSL maintains many of the attractive properties of sBSL, including its scalability to a high dimensional summary statistic and ease of tuning.====Despite the appeal of semiBSL, the number of model simulations required to accurately estimate the correlation matrix scales poorly with the dimension of the summary statistic. The equivalent problem for sBSL, the scaling of the estimation of covariance matrix with the number of model simulations, has been explored by ====, ====, ====, ====, ==== and ====. However, there are currently no methods designed specifically for the semi-parametric estimator, which, in practice, may preclude its application to problems where model simulation is computationally expensive. The first contribution of this article adapts and extends the methodology presented in ====, which combines a whitening transformation with shrinkage covariance matrix estimation, to the semiBSL context.====SemiBSL provides additional robustness over sBSL when the summary statistic marginals deviate from normality. However, as we demonstrate in subsequent sections, for some distributions the KDE will fail. For instance, when a marginal summary statistic distribution has extremely heavy tails, the KDE will allocate essentially no density to the center of the distribution, and all weight to the tails (see the second last column of ====). In addition, it is well-known that the global bandwidth KDE rarely provides adequate smoothing over all features of the underlying distribution (====; ====). Our second contribution addresses this problem with a procedure that draws upon and extends the vast body of literature on density estimation. Specifically, we consider transformation kernel density estimation (TKDE, ====) to estimate the marginal distributions of the summary statistic. The idea is to transform the distribution so that the standard global bandwidth KDE is accurate, and then transform back to the original domain to estimate the density. We adapt the hyperbolic power transformation of ====, and propose a procedure to effectively apply TKDEs in a semiBSL algorithm.====The remainder of this article is structured as follows. In sections ==== and ====, we provide an overview of sBSL and semiBSL, respectively. In section ====, we present our method to significantly improve the computational efficiency of semiBSL. In section ====, we propose a new estimator of the marginal summary statistic distributions for semiBSL using TKDE. We assess the accuracy of the TKDEs on a number of test densities with known distribution. In section ====, we apply our new methods to four different examples. Last, we conclude in section ====.",Transformations in semi-parametric Bayesian synthetic likelihood,https://www.sciencedirect.com/science/article/pii/S0167947323001081,Available online 12 June 2023,2023,Research Article,3.0
"Tan Xin,Zhan Haoran,Qin Xu","School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China,NUS (Chongqing) Research Institute, Chongqing, China","Received 14 October 2022, Revised 25 May 2023, Accepted 26 May 2023, Available online 8 June 2023, Version of Record 16 June 2023.",https://doi.org/10.1016/j.csda.2023.107793,Cited by (0),The projection pursuit regression (PPR) has played an important role in ,"Projection pursuit techniques were first proposed by ====. The idea is to use lower-dimensional projections of high-dimensional data to reveal the most details about the structure of the data. Once an interesting set of projections has been found, existing structures (clusters, surfaces, etc.) can be extracted and analyzed separately. ==== extended the idea to the projection pursuit regression (PPR), and popularized the techniques in the 1980s; see for example ====. PPR is an algorithm that is able to approximate general functions (====; ====; ====For a general regression with response ==== and predictor ====, we are interested in estimating ====, or==== where ====. PPR approximates function ==== by the following expansion:==== where ==== and ==== are called ridge functions and projections, respectively. Note that equation ==== holds under very mild conditions (====; ====; ====). In other words, ==== is not a model but a general mathematical expansion. As a consequence, PPR can be used for general function estimation. Specifically, if the number of terms in the summation on the right-hand side of ==== is finite, ==== is a PPR statistical data model; see for example ====, ====, ==== and ====.====Note that most estimation methods of PPR, including ==== and ====. Because ReLU is “nearly linear”, it preserves many of the properties that make linear models easy to optimize with gradient-based methods; see for example ==== and ====. In this paper, we will use ReLU in the estimation of PPR.==== and diverging ==== that grows with the sample size ====. We also investigate the statistical efficiency of PPR as an estimator of a general regression that is not necessarily a PPR model.====The rest of this paper is organized as follows. Section ==== investigates the asymptotic consistency of the estimated projections and the general regression function ====. Selection of tuning parameters in the estimation is discussed in section ====. Simulation studies and real data analysis are presented in Section ==== and Section ====. Technical proofs are put in the appendix.",Estimation of projection pursuit regression via alternating linearization,https://www.sciencedirect.com/science/article/pii/S0167947323001044,Available online 8 June 2023,2023,Research Article,4.0
"Wu Shihao,Li Zhe,Zhu Xuening","School of Data Science, Fudan University, China,Department of Statistics, University of Michigan, United States of America","Received 30 August 2022, Revised 11 May 2023, Accepted 27 May 2023, Available online 5 June 2023, Version of Record 14 June 2023.",https://doi.org/10.1016/j.csda.2023.107794,Cited by (0),.) to implement the distributed algorithm on a Spark system and establish theoretical properties with respect to the estimation accuracy and mis-clustering rates under the stochastic block model. Experiments on a variety of synthetic and empirical datasets are carried out to further illustrate the advantages of the methodology.,"; ====; ====), biology (====, ====; ====).====Among the existing literature on large scale network data, the stochastic block model (SBM) is widely used due to its simple form and its usefulness as a statistical model (====). In the SBM, the network nodes form connections depending on their memberships in ==== disjoint communities. Within the same community, nodes are more likely to form edges with each other. On the other hand, the nodes from different communities are less likely to form connections. Understanding the community structure is vital in a variety of fields. For instance, in social network analysis, users from the same community are likely to share similar social interests. For this reason, community memberships can be applied to particular marketing strategies.====Statistically, the communities in the SBM are latent therefore need to be detected. Recovering community memberships from the observed network relationships is one of the most fundamental problems in the SBM. Researchers have proposed various estimation methods to accomplish this task. For instance, ====, ==== and ==== and ====), methods of moments (====) and spectral clustering (====; ====; ====).====Among the approaches, spectral clustering (====; ====; ====; ====; ====; ====; ====, ==== and ====.====Despite the statistical advantages of using spectral clustering for the community detection problem under stochastic block models, the procedure is computationally demanding, especially when the network is of large scale. First, handling such enormous datasets requires great computational power and storage capacity. This makes it difficult to complete ====; ====; ====, ====; ====; ====; ====; ====). In the regards of distributed community detection, ==== groups and conducts clustering on each subgraph in parallel. Then, the clustering results are merged using a trace-norm based optimization approach. ==== devised two algorithms to solve the problem, which conduct clustering based on sampling subgraphs, and then patch the results into a single clustering result. Other methods for scalable community detection tasks include random walk based algorithms (====) and randomized sketching methods (====).====In this work, we propose a new distributed community detection (DCD) algorithm using a distributed system. A distributed system consists of a master server and multiple worker servers. In each round of computation, the master server is responsible for broadcasting tasks to workers. The workers then conduct computational tasks using local datasets and communicate the results to the master. Specifically in our algorithm, we distribute the network nodes together with their network relationships on the master and worker servers. On the master server we distribute ==== network nodes, which are referred to as ==== The network relationships among the pilot nodes are stored on the master server. On the ====th worker, we distribute ==== network nodes together with ==== pilot nodes. The network relationships between the ==== network nodes and the pilot nodes, along with the relationships among the pilot nodes, are recorded. Compared with storing all of the network relationships on a single server, we store only a partial network, which leads to much lower storage requirements.====This article is organized as follows. In Section ====, we introduce the SBM and our distributed community detection algorithm. In Section ====, we develop the theoretical properties of the estimation accuracies of the community detection task. In Sections ==== and ====, we study the performance of our algorithm via simulation and real data analysis. Section ==== concludes the article with a discussion. All proofs and technique lemmas could be found in the Appendix.==== We use ====, ====. For any vector ====, ==== in the diagonal entries. ==== and ==== denote the maximal and minimal singular values of ====, respectively. For two sequences of real numbers ==== and ====, we write ==== if ====, ==== if ==== is bounded for all ====, ==== if ==== for all ==== and some positive ====, ==== if ==== if ==== is bounded in probability.====The following is the Supplementary material related to this article.",A distributed community detection algorithm for large scale networks under stochastic block models,https://www.sciencedirect.com/science/article/pii/S0167947323001056,Available online 5 June 2023,2023,Research Article,5.0
"Ouyang Jiangrong,Bondell Howard","School of Mathematics and Statistics, University of Melbourne, Australia","Received 14 February 2023, Revised 17 May 2023, Accepted 21 May 2023, Available online 29 May 2023, Version of Record 12 June 2023.",https://doi.org/10.1016/j.csda.2023.107785,Cited by (0),.,"). As in GLMMs, the regression parameters of the conditional models also have a conditional interpretation; moreover, they are only meaningful when the number of repeated observations is the same for all subjects.====Generalized estimating equations (GEEs), introduced by ==== and ====; ====; ====) to enable a fully Bayesian regularization and inference.====Empirical likelihood (EL; ====, ====; ====, ====) and extended to GLMs (====; ====). Regularization via penalization can also be applied to EL-based approaches in a similar fashion as in regular likelihood approaches (====; ====).====However, empirical likelihood is not directly applicable to the GEE setting since the estimating equations contain unknown nuisance correlation parameters, which are often estimated along with the regression parameters iteratively in a cyclical manner. ==== proposed an EL-based approach via the use of quadratic inference functions (====), which circumvents the estimation of nuisance correlation parameters. This, however, at least doubles the number of estimating equations needed to construct the empirical likelihood function. When the sample size ==== is not large or the number of covariates ==== is relatively large, the incorporation of the extra estimating equations in empirical likelihood can have a negative impact on accuracy (====; see also ====). To address this issue, we profile out the nuisance correlation parameters as a function of the regression parameters, thus retaining the form of generalized estimation equations.====While empirical likelihood behaves like an ordinary likelihood and has been used regularly in the Frequentist inference setting, it is not a true likelihood computed as the density from which the data is assumed to be generated. ====, and since then it has received more attention as a potential way to bridge the gap between Bayesian methods and those that are likelihood-free. ====), small area estimation (====). Algorithms for Bayesian computation were also developed (====; ====).==== of longitudinal data without having to specify a potentially intractable likelihood function. We demonstrate its use in the context of two popular priors for Bayesian inference and regularization, the Laplace prior and the horseshoe prior, but we note that the framework can accommodate any choice of prior.====The remainder of this paper is organized as follows. Section ==== gives a brief background of GEEs, empirical likelihood, and global-local shrinkage priors. Section ==== presents the proposed Bayesian method. Section ==== reports two simulation studies of the proposed method on longitudinal data. Section ====In this section, we present a Markov chain-based method for generating binary vectors with correlated components. Two correlation structures are considered, AR1(====) and EX(====), where ====. Note that not all values of ==== are feasible. There is a natural constraint imposed by the marginal probability. Consider two binary random variables, ==== and ====. The non-negative span of feasible correlation between ==== and ==== is given by==== where ==== and ====.",Bayesian analysis of longitudinal data via empirical likelihood,https://www.sciencedirect.com/science/article/pii/S0167947323000968,29 May 2023,2023,Research Article,6.0
"Ke Chenlu,Yang Wei,Yuan Qingcong,Li Lu","Virginia Commonwealth University, United States of America,Miami University, United States of America,Shanghai Jiao Tong University, China","Received 13 August 2022, Revised 27 March 2023, Accepted 18 May 2023, Available online 26 May 2023, Version of Record 8 June 2023.",https://doi.org/10.1016/j.csda.2023.107784,Cited by (0),Variable screening as a fast and effective dimension reduction tool plays an important role in analyzing ultrahigh dimensional data. While a very large number of actual datasets contain both continuous and ,") and Kolmogorov filter (====, ====), varying coefficient linear model (====; ====; ====), and model-free regression (====; ====; ====). A common but not negligible issue of the aforementioned unconditional or conditional screening methods is that important predictors making little or no marginal contribution (or conditional contribution given the control variables) to the response cannot be detected, while spurious variables that are highly correlated with some important predictors may be falsely selected (====). For unconditional screening, ====; ====; ====, etc.) which at each step select variables that explain the “residuals” from the previous iteration. Nevertheless, defining “residuals” without model specification is not obvious and no theoretical support has been provided for these algorithms.====Moreover, despite the rich literature in variable screening, existing methods are limited primarily to continuous predictors. ==== but assumes no underlying model structure. Each continuous variable is assessed after adjusting for the effect of the other variables and the categorical controls. As a payoff, the proposed screening mechanism achieves “sufficiency” and solves the long-existing issue in variable screening that we mentioned earlier. We establish its sure screening and rank consistency properties in a “large ==== small ====. Secondly, control sets that are partly or completely continuous can also be considered. Lastly, if the control variables are simply constants, partial screening degenerates to generic unconditional screening.====In Section ====, we extend the theory of partial dimension reduction to partial variable selection, and map its connections to marginal and within-subpopulation variable selection. Section ==== is dedicated to the development of the framework of partial sufficient variable screening, along with a novel screening procedure using kernel-based partial ==== and final remarks are given in Section ====. All technical proofs are deferred to the appendix.",Partial sufficient variable screening with categorical controls,https://www.sciencedirect.com/science/article/pii/S0167947323000956,26 May 2023,2023,Research Article,7.0
"Liang Weijuan,Zhang Qingzhao,Ma Shuangge","School of Statistics, Renmin University of China, Beijing, China,Department of Statistics and Data Science, School of Economics, The Wang Yanan Institute for Studies in Economics, and Fujian Key Lab of Statistics, Xiamen University, Xiamen, China,Department of Biostatistics, Yale School of Public Health, New Haven, CT, USA","Received 4 January 2022, Revised 1 December 2022, Accepted 10 May 2023, Available online 25 May 2023, Version of Record 29 May 2023.",https://doi.org/10.1016/j.csda.2023.107782,Cited by (0)," has been extensively conducted. In this study, we consider a partially functional model, under which some ","; ====; ====; ====). In particular, both mean and robust estimations have been developed (====; ====; ====).====As a natural extension of the aforementioned model, in a partially functional model, there are two types of covariates. The first type of covariates is functional, as described above. In addition, there are also scalar covariates with linear effects. Such a model shares some similar spirit with the partially linear regression (====; ====), and statistical and computational analysis of such interactions has been shown to be highly nontrivial. To the best of our knowledge, there has been no interaction analysis with partially functional models that consist of two distinct types of covariate effects.====; ====; ====).====If there are no interactions in the model, conceptually, some of the existing penalized sparse methods for functionals can be adapted to the partially functional models, although we note that there has been very limited research in this aspect (====; ====; ====). When interactions are present, however, these methods may lead to a violation of the “main effect, interaction” variable selection hierarchy. This hierarchy has been strongly stressed in the recent parametric interaction analysis studies. Under this hierarchy, if an interaction effect is identified, then one or both of the corresponding main effects have to be identified, corresponding to the weak and strong hierarchy, respectively. It has been argued that in interaction analysis, this hierarchy is statistically sensible and necessary. For the specific model we are interested in, this hierarchy means that, for any specific subregion, if a functional effect is nonzero in an interaction term, then the corresponding main functional effect must be nonzero in this subregion. This brings additional constraints and complexity to estimation. To the best of our knowledge, there is no existing estimation technique that can respect this hierarchy in estimation for our proposed model.==== and those alike. Overall, with the significant statistical developments and strong application potential, this study is warranted beyond the existing literature.====The rest of the article is organized as follows. In Section ====, we first describe the data setting, proposed model, and estimation approach. An effective computational algorithm is developed, and statistical properties are then rigorously established. Practical performance of the proposed approach is examined using simulation (Section ====) and data analysis (Section ====). The article concludes with brief discussions in Section ====. Additional theoretical developments and numerical results are presented in the Appendix and Supplemental Materials.",Locally sparse quantile estimation for a partially functional interaction model,https://www.sciencedirect.com/science/article/pii/S0167947323000932,25 May 2023,2023,Research Article,8.0
"Kim Jonathan,Sandri Brian J.,Rao Raghavendra B.,Lock Eric F.","Division of Biostatistics, University of Minnesota, Minneapolis, 55455, USA,Division of Neonatology, Department of Pediatrics, University of Minnesota, Minneapolis, MN, USA,Masonic Institute for the Developing Brain, University of Minnesota, Minneapolis, MN, USA","Received 16 September 2022, Revised 5 May 2023, Accepted 14 May 2023, Available online 19 May 2023, Version of Record 24 May 2023.",https://doi.org/10.1016/j.csda.2023.107783,Cited by (0),"A ==== to predict a continuous or binary outcome from data that are collected from multiple sources with a multi-way (i.e., multidimensional tensor) structure is described. As a motivating example, molecular data from multiple 'omics sources, each measured over multiple developmental ","Technological advancements in biomedical research are producing datasets that are very large and have complex structures. Some data are represented as a multi-way array, also called a tensor, which extends the two-way data matrix to higher dimensions. Some data are multi-source, which involves features from different sources of data matched by samples (this is also known as multi-view data). A growing number of datasets are simultaneously multi-source and multi-way (MSMW). As a motivating example of MSMW data, we consider predictors of early-life iron deficiency (ID) in infant monkeys using data described in ==== of infants develop ID and anemia between 4 and 6 months of age due to a combination of lower iron stores at birth and rapid postnatal growth rate (====; ====). Prior studies in this model have shown that the ID infants have metabolomic and proteomic abnormalities in the serum and cerebrospinal fluid in the preanemic and anemic periods with residual changes persisting even after the resolution of anemia with iron treatment (====; ====; ====; ====, ====; ====, ====, ====To understand the significance of incorporating MSMW structure into analysis, consider a ==== in which each source's multi-dimensional data array is transformed into a vector and features from all sources are concatenated into a single vector. While this approach would produce data that could be analyzed using one of the many methods available for vector-valued data, it would also have a number of shortcomings. Ignoring the multi-way structure would not allow for consideration of dependence across dimensions. Ignoring the multi-source structure would mean that any signal present in features from smaller sources could be overrun by noise from larger sources with comparatively less signal.====A common aspect of MSMW data is the presence of far more features than samples, often referred to as high-dimension low-sample size (HDLSS) data. While MSMW data need not necessarily be HDLSS, it is sufficiently common that methods for handling MSMW data will ideally allow for HDLSS structure. A ====) and multi-way (Section ====); our methodological contributions are summarized in Section ====.====Additionally, we ran random forests as a comparator for the high-dimensional probit scenario and found average misclassification rates of 0.47–0.48, which is a bit higher than the misclassification rates observed with our models and demonstrates the inherent difficulty of this simulation scenario.",Bayesian predictive modeling of multi-source multi-way data,https://www.sciencedirect.com/science/article/pii/S0167947323000944,19 May 2023,2023,Research Article,9.0
"Agboola Oluwagbenga David,Yu Han","Department of Applied Statistics and Research Methods, University of Northern Colorado, CO 80639, USA,Data & Modeling Sciences, Procter & Gamble, OH 45217, USA","Received 2 July 2022, Revised 25 December 2022, Accepted 3 May 2023, Available online 18 May 2023, Version of Record 6 June 2023.",https://doi.org/10.1016/j.csda.2023.107780,Cited by (0),"High-dimensional data are increasingly popular in various physical, biological and social disciplines. A common existing approach of repeatedly splitting data was suggested to address the overfitting problem in high-dimensional statistics, however it is computationally expensive in high dimensions. A computationally efficient data splitting method is proposed and referred to as Neighborhood-Based Cross Fitting (NBCF) double ","High-dimensional data are nowadays rule in medicine, health, epidemiology, econometrics, business, finance, technology, sociology and political science, to name just a few. Much of research questions in these areas are an attempt to infer causal and counterfactual relationships with statistical evidence collected under conditions where fully randomized controlled experiments are not possible. The types of research questions in these areas continues to increase in their complexity with the advance of technology.==== of ==== have values in an entropically simple set (Donsker class) of complexity or empirical process conditions. The assumption rules out most of the new generation of nonparametric methods, branded as “machine learning” (ML) methods, in the high-dimensional settings. In other words, the assumption limits the complexity of the function space and makes it impossible to estimate ====. Consequently, there is a need to go beyond the Donsker classes that the high-dimensional nuisance can be estimated with the new generation of nonparametric statistical ML methods. Parametric-based methods in high-dimensional data can lead to unreliable results (namely, biased estimates) (====). Hence, there is a need for modern semiparametric methods where the estimators have values in an infinite-dimensional function space.====The model ==== is equivalent to==== which can be written in the form of the conditional moment restrictions framework in Equation ==== with ====, ====, ====, ====, and ====. Hence, using the expressions for ==== and ==== above and setting ==== for ====, yields==== Based on this choice of ==== and ====, the result becomes==== and so ==== and ==== yield==== The score ==== above is efficient and Neyman orthogonal.",Neighborhood-based cross fitting approach to treatment effects with high-dimensional data,https://www.sciencedirect.com/science/article/pii/S0167947323000919,18 May 2023,2023,Research Article,10.0
"Zhao Kaige,Zou Tingting,Zheng Shurong,Chen Jing","School of Mathematics & Statistics and KLAS, Northeast Normal University, Changchun 130024, China,School of Mathematics, Jilin University, Changchun 130012, China,Academy for Advanced Interdisciplinary Studies, Northeast Normal University, Changchun 130024, China,Shanghai Zhangjiang Institute of Mathematics, Shanghai 201203, China","Received 27 February 2022, Revised 30 April 2023, Accepted 30 April 2023, Available online 10 May 2023, Version of Record 22 May 2023.",https://doi.org/10.1016/j.csda.2023.107779,Cited by (0)," of these statistics are constructed under the null hypothesis and a specific alternative hypothesis. Moreover, extensive simulation studies and an analysis of real data are conducted to evaluate the performance of our proposed tests. The simulation results show that the proposed tests outperform existing methods."," be an independent and identically distributed (i.i.d.) random sample from a ====-dimensional population ==== with mean vector ==== and covariance matrix ==== where ==== is the sample mean.==== where the unknown parameters ==== and ====, ==== is the ==== denotes the ====-dimensional vector with all elements equalling to 1. The population covariance matrix ==== is a positive matrix with the compound symmetry structure under the null hypothesis. For convenience of expression, we set==== Note that the covariance matrix ==== is unbounded in spectral norm since the maximum eigenvalue of ==== is ==== as ====.====Many statisticians have studied ==== in low-dimensional settings where the dimension ==== is fixed. For instance, ==== was the first to discuss the hypothesis ==== or 3. ==== and ==== and ====. ==== derived the exact non-null moments and distribution of the likelihood ratio criterion. ==== proposed a locally most powerful test. ==== distribution.====In the high-dimensional framework, where the dimension ==== tends to infinity, there has been significant research on testing the hypothesis ====. Most of these studies assumed that the sample comes from normal distribution. For example, ==== and ==== derived the null distribution of the corresponding LRT as ====. ==== and ==== developed a novel LRT that is applicable to a broader range of problems for testing the covariance matrix structure, including the hypothesis ====. ====. ==== derived the asymptotic distributions of the Rao score test statistic and the LRT. ==== developed a test suitable for the case that the dimension ==== is much larger than the sample size ====. To remove the normality assumption, ====, which is powerful for dense ====. In addition, ====The contributions of this paper are as follows:====The rest of this paper is arranged as follows. Section ==== describes the test statistics and establishes the joint limiting null distribution. We propose three test procedures for testing the null hypothesis ==== in Section ====. Section ==== investigates the power functions of the proposed tests under an alternative hypothesis. Section ====. Our conclusion is given in Section ====. Theoretical proofs are presented in the Appendix and Supplementary File.",Hypothesis testing on compound symmetric structure of high-dimensional covariance matrix,https://www.sciencedirect.com/science/article/pii/S0167947323000907,10 May 2023,2023,Research Article,11.0
"Sun Xiaofei,Wang Hongwei,Cai Chao,Yao Mei,Wang Kangning","School of Statistics, Shandong Technology and Business University, Yantai, China,School of Mathematics, Hefei University of Technology, Hefei, China","Received 29 June 2022, Revised 18 April 2023, Accepted 4 May 2023, Available online 10 May 2023, Version of Record 12 May 2023.",https://doi.org/10.1016/j.csda.2023.107781,Cited by (0),This paper concerns ,"Quantile regression (QR) proposed in ==== has become an alternative to mean regression. It can describe the entire conditional quantiles, thus is more comprehensive than mean regression. Also, it is robust to heavy-tailed distributions and outliers. Thus, it has been widely used in various fields (e.g., ====, ====; ====, ====; ====).====The main purpose mean regression and quantile regression is data analysis. Modern data analysis often needs to deal with big data, which is large in scale and updated quickly (====, ====, ====Since the computer memory is usually limited, and the data size is unbounded in the stream data settings, once the current data are processed, they cannot be retained (====, ==== and ====).====, references ====, ==== and ==== further developed some alternative versions of SGD, respectively. Also, there are some online second-order learning algorithms (e.g., ====, ====), and these strategies are also widely considered, one can see ==== and ====.====), the cumulative estimating equation estimator (====). Also, ==== further extended it to a general framework.====However, all of the above methods and algorithms mainly focus on mean regression, or are built upon likelihood framework. They are not robust, and are sensitive to outliers or heavy-tail error distributions. At first glance, quantile regression is a natural alternative. However, the check loss function is non-smoothing and the resulting estimators have no closed form expression in quantile regression. Thus the aforementioned methods can not be used directly. Although ==== and ====, which may not be satisfied in the real applications when streaming data sets arrive perpetually with ====.====Therefore, it is an interesting issue to study a more convenient quantile regression strategy for the streaming datasets, which can be free of the constraint and then are adaptive to the situation where streaming data sets arrive fast and perpetually. To the best of our knowledge, there is little related work. In this article, we first propose a smooth quantile loss, which is convex and infinitely differentiable. Moreover, the smooth quantile regression loss can approximate the quantile regression loss accurately with theoretical guarantees, i.e., it converges to the quantile loss uniformly. Then, based on the new smooth quantile loss, we propose an online renewable quantile regression estimation framework, which updates the estimator with current data and summary statistics of historical data, and does not use any old batch level data. Theoretically, we prove that the online updating estimator is statistically consistent and equivalent with the global oracle estimator without any condition on ====. These theoretical results guarantee that the new method achieves the same statistical efficiency as merging all the data together, and is particularly useful even when the number of batches of data increases fast or perpetually.====The rest of this paper is organized as follows. Section ==== introduces our new method and related theoretical results. Sections ==== and ==== validate our new method by using numerical experiments on both synthetic and real data, respectively. Concluding remarks are given in Section ====. All the technical proofs are provided in the Appendix.",Online renewable smooth quantile regression,https://www.sciencedirect.com/science/article/pii/S0167947323000920,10 May 2023,2023,Research Article,12.0
"Shi Xiangyu,Xu Min,Du Jiang","Faculty of Science, Beijing University of Technology, Beijing 100124, China,Beijing Institute of Scientific and Engineering Computing, Beijing, 100124, China","Received 31 October 2022, Revised 8 March 2023, Accepted 23 April 2023, Available online 2 May 2023, Version of Record 10 May 2023.",https://doi.org/10.1016/j.csda.2023.107768,Cited by (0)," is developed by combining the proposed quadratic with extreme value statistics, and the ==== of the resulting statistical test is established. The simulation results demonstrate that the proposed max-sum test performs well in empirical power and robustness, regardless of whether the data is sparse dependence or not. Finally, to illustrate the use of the proposed test method, two empirical examples of Leaf and ==== datasets are provided.",", significant research has been put into investigating the independence testing problem and its variants. The general framework usually considers data with fixed ====-dimensional vectors given a sample size of ====. With the development of science and technology, a growing number of statistical applications, such as biology, finance, and medicine, frequently collect data with the dimension ==== that may be considerably larger than the sample size ====. Data with ==== is called high-dimensional data. For example, ==== collected the closing prices of 452 stocks in the Standard and Poor 500 index, and to test the independence for high-dimensional data. Therefore, in this case the dimension is ====, and sample size is 59, i.e., ====.==== is fixed and ==== diverges to infinity. However, these classical methods may be invalid for high-dimensional data (====). For high-dimensional testing problems, many researchers developed various test methods under the setting that the ==== and ==== are comparable, i.e., ====, to name just a few, see, for example, ====, ====, ====, ==== and ====. Recently, related studies have gradually focused on the high-dimensional settings where ==== is much larger than ====, denoted by ====. In the latter case, the tests mentioned above may perform poorly or even be not well-defined, especially for heavy-tailed random vectors or outlying observations.====This paper focuses on the independence testing problem of high-dimensional data with dimension ====. Let ==== be a ==== be a matrix that formed by ==== independent observations ==== where ====. Let ==== be the columns form ====, where ====. Suppose that the dimension ==== is considerably larger than ====. We are interested in testing a high-dimensional hypothesis====If we suppose that the random vector ==== is equivalent to testing the diagonal of the covariance Σ. The current literature mainly consider two alternative hypotheses, dense and sparse covariance matrices, which are defined as==== where ==== ==== stand for small elements and ==== ==== stand for large elements. In Section ====Under dense alternatives with ====, many scholars have considered quadratic-type test statistics for testing high-dimensional independence. For instance, ==== and ====, these statistics converged to the Gaussian distributions without specific distributions or moment constraints under the high-dimensional setting.====Under sparse alternatives with ==== extended the results of ==== to high dimension. For non-Gaussian data, ====, the test may be invalid if we choose the statistics inappropriately. Moreover, the two existing test methods based on the Pearson correlation coefficient are sensitive to outliers and invalid for heavy-tailed error distributions.====It is well known that Spearman's footrule, a nonparametric measure with the properties of robustness, simplicity, and natural interpretation, provides a useful tool to discover the relationship between two continuous random variables. Thus, this paper proposes the Spearman's footrule rank-based quadratic type and extremum value type test statistics for dense and sparse alternatives, respectively. The resulting asymptotic null distributions are established, which are asymptotically distribution-free. To make the test to adapt to the underlying dense and sparse alternatives automatically, we proposed the max-sum test statistic by combining the quadratic type with the extreme value type tests. Under mild conditions, the asymptotic null distribution of the proposed max-sum statistic is established. Consequently, the practitioner can use the asymptotic null distribution to calculate the asymptotic critical values or ====-values.====The reminder of this paper is organized as follows. Section ==== introduces three test statistics. Section ==== provides asymptotic null distributions of the quadratic type test and the extreme value type test statistics and their asymptotic independence. Additionally, we present the asymptotic properties of the proposed max-sum test statistic. In Section ==== examines two specific examples. Section ==== summarizes this paper. Finally, Section ==== provides some extended results and technical proofs.",Max-sum test based on Spearman's footrule for high-dimensional independence tests,https://www.sciencedirect.com/science/article/pii/S0167947323000798,2 May 2023,2023,Research Article,13.0
Moscovich Amit,"Department of Statistics and Operations Research, Tel Aviv University, Tel Aviv, 69978, Israel","Received 20 December 2020, Revised 22 April 2023, Accepted 23 April 2023, Available online 28 April 2023, Version of Record 9 May 2023.",https://doi.org/10.1016/j.csda.2023.107769,Cited by (0)," of two different sizes are then used to efficiently propagate the probabilities of the non-crossing paths. This approach has applications beyond statistics, for example in financial risk modeling.","Let ==== and let ==== where ==== are arbitrary upper bounds. This probability is equal to==== where ==== are the order statistics of a uniform sample in ==== and ====. This equivalence follows by expressing ==== where ==== holds if and only if ====.====Let ==== be a sample of independent uniform variables with order statistics ==== and empirical cumulative distribution function ====. In this appendix, we present the reduction between the non-crossing probability of the empirical cumulative distribution,==== and the simultaneous non-crossing probability of the order statistics,==== This reduction is well-known and has also been extended to discontinuous distributions (====; ====; ====). Nonetheless, we thought it would benefit the reader to include a concise proof of this basic result, which is at the foundation of the methods described in this paper. First, we show that, rather than considering the entire boundary function ====, it suffices to consider its first integer passage times,==== where ==== is the largest integer for which the set ==== is non-empty. The following lemma holds the key observation that allows one to replace the infinite set of inequality constraints ==== with a finite set of inequalities. ==== ",Fast calculation of p-values for one-sided Kolmogorov-Smirnov type statistics,https://www.sciencedirect.com/science/article/pii/S0167947323000804,28 April 2023,2023,Research Article,14.0
"Sohn Jinwon,Jeong Seonghyun,Cho Young Min,Park Taeyoung","Department of Statistics, Purdue University, IN 47907, USA,Department of Applied Statistics, Yonsei University, Seoul 03722, Korea,Department of Statistics and Data Science, Yonsei University, Seoul 03722, Korea,Department of Computer and Information Science, University of Pennsylvania, PA 19104, USA","Received 22 June 2021, Revised 8 April 2023, Accepted 15 April 2023, Available online 25 April 2023, Version of Record 11 May 2023.",https://doi.org/10.1016/j.csda.2023.107766,Cited by (0)," as a function of time, while also investigating similar patterns of time-dependent interactions. We present a novel generalized varying-coefficient model that accounts for within-subject variability and simultaneously clusters varying-coefficient functions, without restricting the number of clusters nor overfitting the data. In the analysis of a heterogeneous series of binary data, the model extracts population-level fixed effects, cluster-level varying effects, and subject-level random effects. Various simulation studies show the validity and utility of the proposed method to correctly specify cluster-specific varying-coefficients when the number of clusters is unknown. The proposed method is applied to a heterogeneous series of binary data in the German Socioeconomic Panel (GSOEP) study, where we identify three major clusters demonstrating the different varying effects of socioeconomic predictors as a function of age on the working status.","Mixed-effects models are commonly used in binary longitudinal studies in the social, behavioral, and health sciences. These models' popularity stems from their ability to capture longitudinal effects generated by repeated-measurement processes. To be more specific, random effects are introduced into linear models with only fixed effects to reflect the correlation between observations on the same subject. This extension also avoids some of the technical issues that can arise during the analysis of variance. For example, ====; ====).====A varying-coefficient model has been shown to be extremely effective for modeling of time-varying effects in longitudinal studies (====; ====; ====; ====; ====; ====; ====; ====; ====; ====), series priors (====), Bayesian wavelets (====), and free-knot splines and adaptive knot selection (====; ====; ====; ====; ====).====; ====; ====; ====; ====; ====; ====). For example, ==== suggested modeling individual basis coefficients by random effects with the mean indexed by the cluster. ====), the two-parameter Poisson-Dirichlet process (====), and the generalized stick-breaking process (====), to name a few. As they are basically infinite-dimensional priors, they have become essential clustering tools for modeling an infinite number of clusters in various areas (====; ====; ====; ====; ====; ====; ====). In the context of nonparametric regression, ==== incorporates the dependent DP prior (====) into the Gaussian process prior for spatial analysis. ==== further considered the local heterogeneity in a subgroup of curves by proposing a hybrid Dirichlet prior that overcomes the global heterogeneity. ==== employed the generalized DP prior to cluster curves smoothed by the free-knot spline method. ==== assigned the DP prior on each wavelet coefficient independently not jointly on the set of coefficients with the sparsity structure used in ====. ==== applied the DP prior to the clustering of functional principle scores, which improves the curve and correlation reconstruction.==== devised a model-free clustering method for binary longitudinal data using a pairwise penalty to nearby clusters, but their model does not account for any functional effects. We find that none of the aforementioned studies deal with both model-based clustering on a mixed-effects model (especially with the DP prior) and nonparametric function estimation with guaranteed smoothness.====Our contribution is threefold. First, we propose a flexible framework for simultaneously modeling population-level fixed effects, cluster-level varying effects, and subject-level random effects in the analysis of binary longitudinal data. The proposed model is a probit varying-coefficient mixed model that flexibly and adaptively identifies different subpopulations having their own varying-coefficient functions that can be either constant, linear, or nonlinear. Second, we devise new prior distributions for the posterior analysis and effective functional clustering of the proposed model. In particular, it is well known that the measurement scale of data must be considered in choosing the base measure for the DP prior (====). We carefully design a prior distribution with a reasonable scale so that it can start a new cluster well within a sampler to account for an infinite number of clusters, while achieving suitable smoothing for function estimation with spatial adaptation. Third, we construct a partially collapsed Gibbs (PCG) sampler to cover the varying-dimensional parameter issue of a standard Gibbs sampler and facilitate posterior computation via the method of partial collapse (====; ====). To maintain a transition kernel, a PCG sampler, unlike a standard Gibbs sampler, requires its steps to be performed in a specific order. We thus develop a PCG sampler that can be used in the fitting of the proposed model.====The remainder of this paper is organized as follows. In Section ====, we describe the probit varying-coefficient mixed model and discuss how the DP prior constructs model-based clustering. Section ==== specifies prior distributions and constructs efficient sampling steps based on the method of partial collapse. In Section ====, simulation studies are presented to validate the proposed method. Section ==== applies the proposed model to the GSOEP data, and Section ==== discusses the results. Appendix ==== contains a detailed description of the proposed method, while Appendix ==== describes how to install the R package for the proposed method. The R package is currently available on the first author's github==== to demonstrate that all of the results in Sections ==== and ==== can be reproduced.====In this section, we describe the details of ====. Let ==== denote a set of clusters containing at least one subject and let ==== denote a set of clusters containing no subject, in the ====th sampling iteration. For ====th iteration:",Functional clustering methods for binary longitudinal data with temporal heterogeneity,https://www.sciencedirect.com/science/article/pii/S0167947323000774,25 April 2023,2023,Research Article,15.0
"Zhang Fan,Chen Ray-Bing,Hung Ying,Deng Xinwei","School of Mathematical and Statistical Science, Arizona State University, AZ, USA,Department of Statistics, National Cheng Kung University, Tainan, Taiwan,Institute of Data Science, National Cheng Kung University, Tainan, Taiwan,Department of Statistics and Biostatistics, Rutgers University, NJ, USA,Department of Statistics, Virginia Tech, VA, USA","Received 26 April 2022, Revised 14 March 2023, Accepted 31 March 2023, Available online 25 April 2023, Version of Record 28 April 2023.",https://doi.org/10.1016/j.csda.2023.107757,Cited by (0)," (GP) models are commonly used in the analysis of computer experiments. Variable selection in GP models is of significant scientific interest but existing solutions remain unsatisfactory. For each variable in a GP model, there are two potential effects with different implications: one is on the mean function, and the other is on the ====. However, most of the existing research on variable selection for GP models has focused only on one of the effects. To tackle this problem, we propose an indicator-based Bayesian variable selection procedure to take into account the effects from both the mean and covariance functions. A variable is defined to be inactive if both effects are not significant, and an indicator is used to represent the variable being active or not. For active variables, the proposed method adopts different prior assumptions to capture the two effects. The performance of the proposed method is evaluated by both simulations and real applications in computer experiments.","; ====, ====, ====, ====, ====, ====, etc.====An important issue in GP modeling is to identify variables with significant impacts on the simulation responses. For complex systems, there are usually a large number of variables involved in computer experiments. These variables can have very different impacts on the responses. Correct identification of significant variables not only provides scientific insights into the underlying systems but also improves the prediction accuracy of the emulator (====In general, a GP model contains two parts: a mean function ==== and a Gaussian process ====. The input variables ====). Furthermore, including unimportant variables in the mean function can also deteriorate the prediction performance.====Most of the existing works on variable selections for GP models often focus only on one of the effects. For example, ==== introduces variable screening methods to identify important correlation parameters sequentially. The idea of sequential variable selection in the correlation function was also used for the high-dimensional Gaussian process (====). ====, ==== and ==== proposed subsample aggregating (subagging) approach to deal with the variable selection in the mean function of GP models.====The remaining of this paper is organized as follows. Section ==== briefly reviews the Gaussian process model. Section ==== details the proposed method. Simulation studies are conducted to examine the proposed method in Sections ==== and ====. Section ==== contains an illustration using real data. In Section ====, the proposed method is extended to a two-indicator approach for the mean function and the correlation parameter separately. We conclude this work with some discussion in Section ====.====The following is the Supplementary material related to this article.",Indicator-based Bayesian variable selection for Gaussian process models in computer experiments,https://www.sciencedirect.com/science/article/pii/S0167947323000683,25 April 2023,2023,Research Article,16.0
"Zhao Zhiwei,Chen Chixiang,Adhikari Bhim Mani,Hong L. Elliot,Kochunov Peter,Chen Shuo","Department of Mathematics, University of Maryland, 4176 Campus Drive, College Park, 20742, MD, USA,Division of Biostatistics and Bioinformatics, Department of Epidemiology and Public Health, University of Maryland School of Medicine, 655 W. Baltimore Street, Baltimore, 21201, MD, USA,Maryland Psychiatric Research Center, Department of Psychiatry, University of Maryland School of Medicine, 655 W. Baltimore Street, Baltimore, 21201, MD, USA","Received 19 May 2022, Revised 13 December 2022, Accepted 13 April 2023, Available online 24 April 2023, Version of Record 27 April 2023.",https://doi.org/10.1016/j.csda.2023.107765,Cited by (0), (====) is proposed to simultaneously extract the latent systematic mediation patterns and estimate the mediation effects based on a dense bi-cluster graph approach. A computationally efficient algorithm is developed for dense bi-cluster structure estimation and inference to identify the mediation patterns with multiple testing correction. The performance of the proposed method is evaluated by an extensive simulation analysis with comparison to the existing methods. The results show that ,"; ====), and CBF on ReHo (====; ==== is to use a ==== (====). We use multivariate mediation methods in the integrated analysis of SBP through multimodal imaging mediation.====; ====; ====). Certain models have been adopted for image-based mediators. For example, ==== used a sparse principal component analysis in a multiple-mediator analysis. Advanced models have also been developed to handle mediation analysis with multivariate exposures and multivariate mediators (====; ====). However, there is still a methodological gap in mediation analysis where the mediator and outcome are both multivariate.==== for the detailed introduction. In a dataset with 100 mediators and 100 outcomes, there exist ==== potential pathways, among which 200 (2%) are positive pathways, and ==== are negative. If positive pathways are evenly distributed, a bi-cluster consisting of 10 mediators and 20 outcomes is expected to cover around four positive pathways (====). The 10-mediator-20-outcome bi-cluster is dense if it covers a large number (say 120) of positive pathways suggesting unevenly distributed positive pathways. The ====; ====). We performed the pathway analysis as our data example is collected from an observational study.====This paper is organized as follows. Section ==== describes ==== and the steps for dense bi-cluster extraction, mediation effect estimation, and inference. Section ==== applies our framework to the Amish Connectome data and explains the identified CBF regions and mediation effects. Section ==== implements simulations to demonstrate the empirical performance of our method. We conclude the paper with a discussion in Section ====.====The following is the Supplementary material related to this article.",Mediation analysis for high-dimensional mediators and outcomes with an application to multimodal imaging data,https://www.sciencedirect.com/science/article/pii/S0167947323000762,24 April 2023,2023,Research Article,17.0
"Li Xiaoting,Joe Harry","Department of Statistics, University of British Columbia, Vancouver, Canada","Received 10 August 2022, Revised 3 April 2023, Accepted 8 April 2023, Available online 17 April 2023, Version of Record 21 April 2023.",https://doi.org/10.1016/j.csda.2023.107761,Cited by (0),For ,". Our methods can estimate multivariate tail probabilities and tail dependence functions in arbitrary directions, allowing for more flexible tail inference.====In the multivariate extreme value literature, asymptotic dependence refers to the domain of attraction to an extreme value distribution with strictly positive dependence and asymptotic independence refers to the domain of attraction to a joint independent distribution. For asymptotically dependent distributions, the strength of tail dependence is captured by the multivariate tail dependence function, as in ==== studied the nonparametric estimator of tail dependence coefficient of bivariate copulas. ==== used the same approach but generalized it to estimate the tail dependence function of multivariate copulas. Both require the asymptotic dependence assumption. Our estimation methods are developed based on a weaker tail expansion assumption on the copulas, and this assumption has been verified for a broad class of parametric copula families. The tail expansion takes into account both the tail dependence function and tail order of the copula. It allows us to distinguish the case of asymptotic dependence and independence, estimate the tail dependence function for asymptotically dependent distributions, and quantify the rate of convergence for asymptotic independent distributions.==== tail asymmetry (iii) heterogeneous tail dependence. For example, financial data often exhibit strong permutation asymmetry in the joint tails when the institutions play asymmetric roles in the market. In this case, the classical methods of measuring tail dependence along the diagonal path do not necessarily capture the maximal degree of dependence. As our methods can measure tail dependence along different directions, they can be used to approximate the direction of maximal tail dependence and check for the degree of tail asymmetry. These features make them not only good inference tools, but also useful diagnostic tools in suggesting suitable parametric copula models. Compared to the commonly used likelihood-based criterion that tend to be dominated by the data in the middle, our methods are more informative about the mis-specification in the tails.==== defined a quantile region with the Kendall's distribution function and the estimator is constructed from empirical copula. Our methods lead to a definition of multivariate quantile, which admits a useful probabilistic interpretation in terms of the tail orthant probability.====Precisely, let ====-variate random vector ====. Each ==== has a continuous univariate margin ====, ====. Let ==== be its associated copula function such that ====. For small ====, the quantile set is==== for the lower tail, and==== for the upper tail.====For ==== close to zero, ==== and ==== or ====), we are able to generate the tail quantile curves or surfaces. They provide good visualizations of the tail dependence properties of the underlying distribution so that we can visually decide whether tail asymmetry exists. As the dimension ==== increases, the methods still apply but they require a larger sample size and become difficult to visualize.====The remainder of the paper is organized as follows. In Section ====, we describe three estimation methods and provide details on parameter estimation. In Section ==== presents an extensive simulation study to assess the performance of the methods. Section ==== presents a data example to illustrate the use of the proposed approaches. Section ==== has some discussion for further research.",Estimation of multivariate tail quantities,https://www.sciencedirect.com/science/article/pii/S0167947323000725,17 April 2023,2023,Research Article,18.0
Bee Marco,"Department of Economics and Management, University of Trento, via Inama, 5, Trento, 38122, Italy","Received 5 November 2022, Revised 5 April 2023, Accepted 10 April 2023, Available online 17 April 2023, Version of Record 20 April 2023.",https://doi.org/10.1016/j.csda.2023.107764,Cited by (0),", and the Cramér - von Mises distance is used to measure the discrepancy between observed and simulated samples. After deriving the theoretical properties of the estimators, a hybrid procedure is developed, where standard maximum likelihood is first employed to determine the bounds of the uniform priors required as input for AMLE. Simulation experiments and two real-data applications suggest that this approach yields a major improvement with respect to standard maximum likelihood estimation."," for the tail is challenging, because samples of extreme observations are typically small.====).====An even more flexible alternative is the dynamic model developed by ====. This approach employs a non-negative distribution with a size distribution supported on ==== for the body and the Generalized Pareto distribution (GPD) for the tail. Unlike classical finite mixtures, the density ==== is based on a dynamic weight, which is a monotonically increasing function of ====. In this way, the GPD is given more and more weight as we move farther into the right tail, and the tail behavior is governed by the GPD. If the weight function is continuous, the density ====Despite its interesting properties, to the best of our knowledge this model has not been used in practice. The likely reason is that maximum likelihood estimation (MLE) is rather complex. On one hand, the mixing weight and the component densities share all parameters, so that, as pointed out by ====Given the difficulties related to MLE, simulation-based methods are an appealing alternative that avoids the evaluation of the normalizing constant, since there is no optimization of the likelihood function. Hence, the possible error caused by an inaccurate approximation of the normalizing constant is eliminated, at the price of the introduction of simulation error.====In this paper we explore the second strategy, and use the Approximate Maximum Likelihood Estimation (AMLE) method proposed by ==== and applied to similar setups by ====, ==== and ====, ====, ====. In the early stages of ABC, the crucial issue was the choice of the summary statistics used to assess the proximity of true and simulated data. This issue is shared by AMLE and is not easy to address in general, except for uncommon setups where sufficient statistics are available.====To overcome this problem, in recent years several papers have developed ABC versions that compare empirical distributions of observed and simulated data, thus bypassing the need of choosing summary statistics. This way of proceeding was first proposed by ====, who employed a non-parametric approach; a more comprehensive treatment was given by ====. See ==== and the references therein for a thorough review. In the following, we will term these ways of proceeding ====, to emphasize the use of all the data, with no dimensionality reduction transformation. Among the various proposals in the literature, we base our implementation on the Cramér-von Mises (CvM) distance, which is a simple and effective measure.====AMLE and MLE are compared via simulation: our outcomes suggest that AMLE has smaller root-mean-squared-error (RMSE) in all setups considered in the experiments. Two empirical analyses confirm the suitability of the approach for modeling and estimating skewed and fat-tailed distributions. Furthermore, the estimated weight allows one to identify the approximate number of observations generated by the heavy-tailed component.====On the theoretical side, full-data AMLE yields a likelihood approximation that converges ==== to the posterior distribution. Under slightly stronger conditions, the mode of the approximation converges pointwise to the mode of the likelihood.====The rest of the paper is organized as follows. In Section ==== we describe the dynamic mixture distribution. Section ==== contains a detailed account of the AMLE versions with and without summary statistics. In sections ==== and ==== we report the outcomes of the simulation experiments and of the empirical analysis, respectively. Finally, Section ==== concludes the paper and outlines open problems and possible further developments.",Unsupervised mixture estimation via approximate maximum likelihood based on the Cramér - von Mises distance,https://www.sciencedirect.com/science/article/pii/S0167947323000750,17 April 2023,2023,Research Article,19.0
"Miecznikowski Jeffrey C.,Wang Jiefei","SUNY University at Buffalo, Department of Biostatistics, 723 Kimball Tower, Buffalo, NY, 14214, USA,University of Texas Medical Branch, Department of Biostatistics & Data Science, Galveston, TX, USA","Received 21 May 2021, Revised 20 March 2023, Accepted 1 April 2023, Available online 14 April 2023, Version of Record 19 April 2023.",https://doi.org/10.1016/j.csda.2023.107758,Cited by (2), control of the false discovery proportion (FDP) can provide an interpretable method for addressing the variability in the false discovery proportion estimates. Exceedance control of FDP can be viewed as constructing a confidence interval for FDP and as such inverting a hypothesis test is a viable method for achieving exceedance control. A novel powerful approach for exceedance control is presented based on using a directional Berk-Jones goodness-of-fit statistic. The approach employs a high-precision implementation procedure to accurately compute confidence envelopes for FDP. The procedure is compared against other methods and generalized to include other goodness-of-fit statistics that follow an ==== condition.,". FDR is defined as the expected value of the false discovery proportion (FDP) which is the number of false rejections divided by the number of rejections. FDR control is often more desirable in a large-scale multiple hypothesis test setting than the family-wise error rate (FWER) control since the power with FDR is much larger than with FWER control.====While the false discovery rate controls the expectation of the false discovery proportion, in some cases, it can be useful to control the ====Exceedance control of the false discovery proportion has received a good deal of attention (see ====; ====; ====; ====; ====; ====; ====; ====; ====; ====). There are mainly two classes of methods to control the exceedance probability, namely the interpolation approach from ====; ====; ====.====The interpolation method starts with the simultaneous probability control of the false discovery proportion for some sets of rejections. The sets will then be interpolated to obtain the simultaneous probability bound for an arbitrary rejection set. For example, in the recently developed interpolation-based method in ==== (denoted as KR), the KR method sorts the hypotheses based on some prior information (or p-values). The sorted hypotheses will be examined in sequential order and the probability bound for the false discovery proportion will be built at each step. This results in a sequence of probability bounds, with the ====th bound corresponding to a rejection from the first hypothesis to the ====th hypotheses in the sorted hypotheses list. An interpolation method is applied to the sequence of probability bounds to obtain the probability bound for an arbitrary set of rejections. We include their method as proposed in Theorem 1 of ==== in our simulations.====An alternative method of exceedance control is coined the inversion method and as such is based on inverting a hypothesis test, see, for example, Chapter 8 in ====. Particularly, this approach of inverting a test to obtain a confidence interval thus controlling, say, the exceedance probability is presented and discussed in ==== and utilizes a Berk-Jones based test statistic that provides superior performance under an assumption of independent tests.====In this paper, we will focus on the inversion method, which is a post hoc method that provides the upper ==== of the FDP for an arbitrary set of rejections after observing the data. This property makes the method very flexible and allows researchers to adjust the hypotheses that will be rejected according to different research endpoints.====As ==== mentioned, inverting traditional uniformity tests, such as the Kolmogorov-Smirnov test do not fare well in exceedance control settings as those tests are designed to look for uniformity deviations equally through all ====-values while reasonable procedures should focus on the left tail. In order to restrict focus on the left tail, ==== propose a method to control exceedance probability utilizing, say, the smallest 10 p-values. This approach is generalized as combining ==== tests. We note this approach is reasonable however it requires the user to choose ====, that is, the number of p-values to use in creating the confidence interval.====In this article, we present an exceedance control approach based on inverting a test derived from a directional Berk-Jones statistic. This method has improved power versus combining ==== tests, is faster, and does not require the user to specify which ====-values to combine. Thus, we feel this contribution will be useful to the statistical community.====Our manuscript is presented as follows: First we present a background section summarizing exceedance control via inversion method and the (directional) Berk-Jones statistic which can be tuned to detect specific violations of uniformity. In Section ==== we present a fast algorithm for controlling the FDR exceedance probability based on the directional Berk-Jones statistic. In Sections ==== and ==== we present Simulations and an Example, respectively. Lastly we note the availability of our ==== package in addition to Discussion and Conclusion. An Appendix contains theorems and proofs used to support our method.====The following is the Supplementary material related to this article.",Exceedance control of the false discovery proportion via high precision inversion method of Berk-Jones statistics,https://www.sciencedirect.com/science/article/pii/S0167947323000695,14 April 2023,2023,Research Article,20.0
"Lenzi Amanda,Bessac Julie,Rudi Johann,Stein Michael L.","School of Mathematics, University of Edinburgh, United Kingdom,Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA,Computational Science Center, National Renewable Energy Laboratory, Golden, CO, USA,Department of Mathematics, Virginia Tech, Blacksburg, VA, USA,Department of Statistics, Rutgers University, Piscataway, NJ, USA","Received 17 March 2022, Revised 6 April 2023, Accepted 8 April 2023, Available online 14 April 2023, Version of Record 20 April 2023.",https://doi.org/10.1016/j.csda.2023.107762,Cited by (1), for deep learning in statistical parameter estimation and can be extended to other estimation problems.,"; ====; ====), no obvious strategy exists for selecting the composite likelihood terms optimally, and the loss in efficiency makes inference for high-dimensional data still challenging (====; ====; ====).====As an alternative to composite likelihood inference in models for spatial extremes, ==== suggested augmenting the componentwise block maxima data with their occurrence times and showed that partitioning the locations based on whether maxima coincided simplifies the likelihood function substantially. To avoid the bias that emerges from fixing these partitions, ====. ==== employed a simulation-based approach using approximate Bayesian computation (ABC) for estimating parameters of max-stable models. In its most basic form, ABC first samples a set of parameters from the prior distribution and then simulates data under the assumed statistical model. Instead of independently sampling parameters, ==== to learn parametric representations of the true posterior faster than Monte Carlo ABC methods. ==== proposed to use informative statistics to train a deep NN upon which classical or Bayesian indirect inference may be based. ==== predicted summary statistics from simulated data, which were then used as an estimate of the posterior mean of an ABC procedure. Recently, ====) and time-series data (====). ====) and Schalther's (====). However, classic deep NNs do not incorporate uncertainty information and only return a point estimate. Constructions such as Bayesian deep learning, deep Gaussian processes, and ensemble techniques for NNs are used to estimate sets of distributional parameters (====The remainder of this paper is organized as follows. Section ==== provides an overview of the proposed approach and briefly introduces CNNs. In Section ==== we recall the definition of max-stable processes and inference based on pairwise likelihoods and conduct a simulation study to assess the performance of the estimators. In Section ==== we illustrate our method on a dataset of temperature minima, and we conclude with a discussion in Section ====.====We assess further the quality of the selected NN learning schedules to estimate parameters of Brown-Resnick processes from the simulated experiment in Section ====. Specifically, we monitor how the accuracy and the loss change as the number of epochs progress during training according to different training data sizes and learning rates. We choose to track epochs rather than iterations since the former measure the number of times that every data split has been seen during training in expectation, and therefore, it does not depend on arbitrary batch sizes, as opposed to the number of iterations (====). This validation experiment aims to give intuitions about the choice of different hyperparameter settings and how to change them for more efficient learning.",Neural networks for parameter estimation in intractable models,https://www.sciencedirect.com/science/article/pii/S0167947323000737,14 April 2023,2023,Research Article,21.0
"Li Mengyu,Wang Xiaoguang","School of Mathematical Sciences, Dalian University of Technology, Dalian, Liaoning 116024, China","Received 8 April 2022, Revised 1 January 2023, Accepted 3 April 2023, Available online 12 April 2023, Version of Record 14 April 2023.",https://doi.org/10.1016/j.csda.2023.107759,Cited by (0),"In biomedical and clinical research, predicting the survival probabilities for patients is a core task. Accurate survival probability predictions can help physicians make better treatments or prevention plans for patients. A novel semiparametric ",") which the logarithmic relative risk appears in the form of partly linear additive is an extension of the Cox proportional hazards model. For the case of multi-dimensional covariates, the partly linear additive Cox model keeps the characteristics of simplicity and easy explanation of Cox proportional hazards model, while allowing flexible structures of covariates.====), sure independent screening technique (====) and dimension reduction (====, ==== and ==== proposed the model averaging method to choose the weights by Mallows criterion and obtained the weighted summation of various models. A jackknife model averaging method to deal with heteroscedastic data was developed by ==== which the model weights were selected by minimizing a cross-validation criterion. ==== proposed the optimal model averaging method for linear mixed-effects models. Based on the Mallows model averaging estimator and the jackknife model averaging estimator, ====. Other related literature includes ====, ====, ====, ====, ====, ====, ====, ====, ====, ==== and ==== and references therein.==== and was further generalized by ====. Motivated by the nonparametric model averaging method, ====. ====. And ====, ====, ====, ====, ==== and ====. In the context of survival analysis, the model averaging methods in the high-dimensional survival models were studied in ==== and ==== proposed a semiparametric model averaging procedure based on maximum partial likelihood method for nonparametric proportional hazards regression.====In this paper, the model averaging technique is used to predict the survival probabilities of individuals based on classical Cox proportional hazards model. Our proposed method considers that each submodel has some nonparametric components and exactly constructs the form of a partly linear additive Cox model. For these candidate models, the nonparametric parts are approximated by B-splines tool (====The article is organized as follows. In Section ====, we propose a novel semiparametric model averaging method to estimate and predict the conditional survival probabilities of individuals. We obtain the corresponding weights of submodels and show the asymptotic properties of the selected weights in Section ====. Simulation studies are reported in Section ==== to evaluate the finite sample performance of our proposed method. Section ==== provides an application of the proposed method to the heart failure data. The article ends with a discussion in Section ====. The detailed proof of the theorem is presented in Appendix. Finally, other results of the simulation are given in the Supplementary Materials and an open-source software package is available for implementing the proposed method.",Semiparametric model averaging method for survival probability predictions of patients,https://www.sciencedirect.com/science/article/pii/S0167947323000701,12 April 2023,2023,Research Article,22.0
"Park Yeonjoo,Kim Hyunsung,Lim Yaeji","Department of Management Science and Statistics, University of Texas at San Antonio, 78249, TX, USA,Department of Statistics, Chung-Ang University, Seoul, 06974, Korea","Received 15 January 2022, Revised 17 March 2023, Accepted 19 March 2023, Available online 7 April 2023, Version of Record 14 April 2023.",https://doi.org/10.1016/j.csda.2023.107745,Cited by (0), of the subspace of data and reconstruction of full trajectories. The proposed method is then applied to hourly monitored air pollutant data containing anomaly trajectories with random missing segments.,", ====, ====, and ====The large complex data acquisition, however, concurrently increases the chance of containing atypically behaved trajectories or having imperfections, such as missing values. Similar to the ====, a severe drawback of FPCA is its sensitivity to atypical curves due to its reliance on sample covariance estimation. The functional observation can be atypical in many ways with its infinite-dimensional nature, for example, by deviating from the mean or displaying the presence of unusual variation components at specific part(s) of trajectories. Moreover, such trajectories often include missing functional segments, which poses challenges in many practical applications.====There has been attention paid to robust FPCA in large part on complete functional data and existing methods include; eigenanalysis on robust covariance function estimates (====; ====); robust projection-pursuit approach (====; ====; ====), inevitably including nonparametric smoothing steps. Technically, it is possible to apply them to partially observed functional data. However, a nonparametric approach with a set of smoothing parameters has no clear advantages when relatively rich information is available over observed segments. It rather consequently increases the avoidable asymptotic smoothing bias.==== and ====, we derive the definition of the elliptical stochastic process, which allows for non-Gaussian functional behaviors but preserves many of the attractive properties of the Gaussian process. The collected functional data is viewed as the sample path of a stochastic process, and it enables modeling the partially sampled trajectories using the missing indicator process. Furthermore, our restated definition connects definitions of the heavy-tailed stochastic process independently introduced and developed in the engineering and computer science literature (====; ====). Under this framework, we propose implementing the robust FPCA through the eigenanalysis on the scatter function of the elliptical process. The computationally efficient robust scatter function estimator is introduced based on pairwise covariance computation using location and scale M-estimators. The desired asymptotic consistency properties are shown under general conditions.====The rest of this article is organized as follows. We present the definition of the elliptical stochastic process and formulate the partially observed heavy-tailed functional data using the missing indicator process in Section ====. In Section ====, we propose the robust FPCA method based on robust estimation of scatter function and derive asymptotic uniform consistency properties of estimates. Also, a practical algorithm for implementing the proposed method is presented. We compare our method with existing methods in simulation studies and conduct the sensitivity analysis in Section ====. The application of the proposed method to ==== concentration data is illustrated in Section ====. Finally, we present our concluding remarks in Section ====.==== ==== ====Let ====, over ====, and we define the M-functional,==== where ==== and ====. Suppose that the marginal distribution of ==== has the density ==== for all ====, then ==== and ==== marginally minimize ==== for each ====. Under regularity conditions on ====, the expectation in ==== exists for every joint probability measure of ==== and there exist an unique minimizer. Let ==== and ==== denote a parameter space of ====. We then assume the following conditions by extending Conditions D1 and D2 in ====:==== ==== ====We have conducted experiments to evaluate the performance under various missing proportions in Model 1 of Section ====. we specifically set missing proportions as 2%, 5%, 8%, 10%, 13%, 15%, 18%, and 20%, which correspond to ==== in our setting, respectively. Here, missing proportion denotes the average of missing rates over 51 grid points. We then further compare the performance between our method and Sparse FPCA (====) under various missing proportions from the Gaussian process to examine when the two methods start to make a significant difference in the performance. We note that Sparse FPCA is proposed for implementation FPCA under sparsely sampled trajectories.",Functional principal component analysis for partially observed elliptical process,https://www.sciencedirect.com/science/article/pii/S0167947323000567,7 April 2023,2023,Research Article,23.0
"Zhao Yujie,Huo Xiaoming","Biostatistics and Research Decision Sciences Department, Merck & Co., Inc, United States of America,The Stewart School of Industrial and System Engineering, Georgia Institute of Technology, United States of America","Received 12 April 2022, Revised 24 March 2023, Accepted 26 March 2023, Available online 3 April 2023, Version of Record 7 April 2023.",https://doi.org/10.1016/j.csda.2023.107747,Cited by (0)," is caused by the undesirable behavior of the absolute function at the origin. To expedite the convergence, an algorithm called ==== (HOSKY) is proposed. It helps expedite the warm-up stage of the existing algorithms. With the acceleration by HOSKY in the warm-up stage, one can get a provable convergence rate lower than ====. The main idea of the proposed HOSKY algorithm is to use a sequence of surrogate functions to approximate the ==== penalty that is used in Lasso. This sequence of surrogate functions, on the one hand, gets closer and closer to the ====, where ==== is the precision used in the warm-up stage (====). Additionally, the numerical simulations also show that HOSKY empirically performs better in the warm-up stage and accelerates the overall convergence rate.",") and later by ==== (HOSKY). The initial points generated by HOSKY, on the one hand, are more computationally efficient; on the other hand, accelerate the overall convergence rate.====In the rest of this section, we first introduce the problem formulation in Section ====. Then, we list criteria to compare different algorithms in Section ====. Next, we summarize the existing literature in Section ==== and compare their computational efficiency under the criteria in Section ====. Finally, we discuss the motivation and contributions of HOSKY in Section ====.====The following is the Supplementary material related to this article.",Accelerate the warm-up stage in the Lasso computation via a homotopic approach,https://www.sciencedirect.com/science/article/pii/S0167947323000580,3 April 2023,2023,Research Article,24.0
"Vexler Albert,Gao Xinyu,Zhou Jiaojiao","The State University of New York at Buffalo, Department of Biostatistics, Buffalo, NY, USA","Received 12 September 2022, Revised 21 March 2023, Accepted 22 March 2023, Available online 28 March 2023, Version of Record 5 April 2023.",https://doi.org/10.1016/j.csda.2023.107746,Cited by (0),"The aim is twofold: (1) to indicate that the one-sample ==== cannot be used directly when a center of symmetry is unknown; and (2) to propose and examine correct schemes for applying the Wilcoxon signed rank test with an estimated center of symmetry. It turns out that the Wilcoxon signed rank test and the sign test for symmetry do not provide valued outputs, when unknown centers of symmetry are estimated using underlying data. In such scenarios, these tests are not null-distribution-free and break down completely even based on samples with large numbers of observations. Theoretical propositions are shown to propose a simple correction of the corresponding R built-in function, employing p-value-based procedures. To perform the proposed algorithms, we develop new customized procedures for estimating the integrated squares of densities, probability weighted moments and special values of density functions. It is shown that the proposed testing strategies have Type I error rates under good control as well as exhibit high and stable power characteristics.====The proposed algorithms can be applied for modifying Wilcoxon tests type procedures in different statistical software.","In order to evaluate the goodness-of-fit of the null hypothesis, say ====, that assumes ==== independent and identically distributed data points, say ====, often users of R (====) employ the simple built-in function wilcox.test(), e.g., ====; ====. The hypothesis ==== claims ==== and ==== are identically distributed, meaning that ==== have a symmetric distribution about the fixed center ====. The alternative hypothesis ==== says that ==== is an asymmetric distribution. If ==== is known, we can use transformed observations ====, assessing ==== via ====-based wilcox.test() with ====. In various practical applications, investigators are often interested in testing ==== when a value of the center of symmetry ==== is unknown (e.g., ====; ====; ====). We can also remark that the natural location parameter for ==== is ====. For more details regarding this claim as well as methods for estimating ====, we refer the reader to ====.====Note that, according to the wilcox.test()'s manual, this function contains an argument described in the form “mu: a number specifying an optional parameter used to form the null hypothesis”. Then, it seems that we can try to use an estimator of ====, performing wilcox.test(), if ==== is unknown. In this framework, for an illustrative example, we conducted the following Monte Carlo experiment, employing the simple R code:====We drew ==== samples of ==== from the symmetric ==== distribution in order to implement the Wilcoxon procedure based on ==== at 5% level of significance. The location parameter was estimated using the sample mean, ==== ====. In this study, wilcox.test() did not reject the null hypothesis in all generated events (i.e., the Monte Carlo Type I error rate was 0), whereas the corresponding Type I Error rate was expected to be 0.05. In a similar manner, we considered, for example, ====, where ==== and ====, i.e. ==== are generated from an alternative asymmetric distribution. The corresponding Monte Carlo power had a value of 0.0044, indicating that the applied Wilcoxon signed-rank test has been biased, in this case.====In the example above, the sample mean was used to estimate ====. According to ==== and ====, one can also evaluate the symmetry center ==== estimation manner or/and an increase of ==== cannot improve the Wilcoxon procedure. This issue is in effect, since, under ==== is large) distribution of the Wilcoxon test statistic is subject to using a known value of ==== or its underlying data-based estimator involved in the test statistic's structure. Thus, in general, critical values of the Wilcoxon test cannot be tabulated, when ==== is unknown.====This paper focuses on developing a simple correction to the R function wilcox.test() and does not target to introduce an essentially new decision-making policy for assessing ====, when ==== is unknown. In this context, it can be noted that in the considered nonparametric statement of the testing problem, there are no most powerful tests. We show that the proposed testing strategies have Type I error rates under well control as well as exhibit high and stable power characteristics.==== in order to assert that, under ====, the wilcox.test()'s statistic with known ==== and the wilcox.test()'s statistic with estimated ==== are not identically distributed, since their variances are significantly different. We also refer the interested reader to proof schemes of the shown results, since they are new, simple and can be employed for considering different ranks-based tests with estimated parameters. In Section ==== we apply Section ===='s outputs to obtain correct decisions via wilcox.test() with estimated ====, in a simple manner.====. Development of estimating tools for such quantities is of a theoretical and practical interest (e.g., ====; ====; ====). In Section ==== we propose new customized procedures for estimating the integrated squares of densities, probability weighted moments and special values of density functions. For example, under ====, we employ a technique based on characteristic functions and the Parseval–Plancherel identity in order to present a new procedure, significantly simplifying the estimation of the integrated square of ===='s density.====In Section ====. This paper concludes with a short discussion in Section ====. All the proofs are relegated to the Appendix.====In the present paper we introduce algorithms for computing the p-values of the one-sample Wilcoxon tests when the center of symmetry is unknown. We treat strategies that can be suitable for analyzing paired observations. The proposed methods can be easily applied for modifying rank-based decision-making procedures in different statistical software.==== ",How to implement signed-rank wilcox.test() type procedures when a center of symmetry is unknown,https://www.sciencedirect.com/science/article/pii/S0167947323000579,28 March 2023,2023,Research Article,25.0
"Wang Xuqin,Li Muyi","Department of Statistics and Data Science, School of Economics, Xiamen University, Xiamen, Fujian, China,Wang Yanan Institute for Studies in Economics (WISE), Xiamen University, Xiamen, Fujian, China,Key Laboratory of Econometrics (Xiamen University), Ministry of Education, China","Received 20 August 2022, Revised 3 February 2023, Accepted 18 March 2023, Available online 27 March 2023, Version of Record 4 April 2023.",https://doi.org/10.1016/j.csda.2023.107744,Cited by (0),"We study the ==== inference on the goodness-of-fit test for generalized autoregressive conditional heteroskedastic (GARCH) models. Note that the commonly-used portmanteau tests for model adequacy checking necessarily impose moment conditions on innovations, we hence construct the test on the sample ==== of a bounded transformation of absolute residuals, which are obtained by the least absolute deviation estimation from a fitted GARCH model. Specifically, we employ the ==== of absolute residuals as the transformation. Thus the corresponding portmanteau tests are applicable for very heavy-tailed innovations with only finite fractional moments. We bootstrap both the estimation equation and sample autocorrelations of transformed residuals to approximate the test statistics. The asymptotic validity of the bootstrap procedure is established. Monte Carlo experiments compare the finite-sample performance of the proposed bootstrap-based test with other existing tests. An empirical analysis of modeling exchange rates illustrates its usefulness.","Since the seminal work of ==== and ====, the generalized autoregressive conditional heteroscedastic (GARCH) model has been widely used in finance and economics to capture the time-varying volatility of asset returns. The model is defined as==== where ==== and ==== are unknown parameters, ==== is an i.i.d. sequence of innovations. To estimate the parameters in model ====, see ==== and ====. Meanwhile, the heavy-tail phenomenon is one of the well-accepted empirical facts of asset returns (====; ====; ====), implying the finite fourth-order moment of innovations is too restrictive to be satisfied in practice. Therefore, robust estimations for GARCH models with heavy-tailed innovations have been widely developed. For example, the Laplacian quasi-maximum likelihood estimation (LQMLE) in ====, the least absolute deviation (LAD) estimation in ==== and the three-step quasi-maximum likelihood procedure with non-Gaussian likelihood functions (NGQMLE) in ====, all of which require only ==== to guarantee the asymptotic normality of the estimations. Recently, ==== and ==== considered the global LAD and the Pearson-type quasi-maximum likelihood estimations for GARCH models and derived their asymptotic normality only with ==== for some ====. Up to now, robust estimation for GARCH models has attracted considerable attention.====In contrast to the development of robust estimations, progress on the goodness-of-fit test for GARCH models with heavy tails is quite limited. Since ==== and ====). ==== proposed two portmanteau tests for GARCH models when ==== and ====. However, none of them is valid when ====. To overcome this problem, ==== proposed a sign-based portmanteau test for GARCH models, where only a finite fractional moment on innovations was required. Recently, ==== and establish the corresponding bootstrap inference of it.====, ====, ==== and ==== for the various estimation and testing issues related to GARCH models.====In addition to the aforementioned residual-based bootstrap which bootstrap residuals directly, ==== proposed a new bootstrap procedure to obtain pseudo residuals by perturbing the minimand of the objective function with i.i.d. random weights, which was extended by ====; ====; ====; ====). Moreover, it has been shown to have a good performance in bootstrapping portmanteau tests, see ====, ==== and ====. Motivated by these aforementioned references, we consider the RW bootstrap on the robust portmanteau test for heavy-tailed GARCH models. The test is constructed on sample autocorrelations of a bounded transformation of absolute residuals, which are obtained by the robust LAD estimation from a fitted GARCH model. We establish the corresponding bootstrap inference of the portmanteau test.====The rest of the paper is organized as follows. Section ====. Section ==== presents the RW bootstrap procedure to approximate the portmanteau test and shows its asymptotic validity. Monte Carlo experiments in Section ==== report the empirical sizes and powers of the proposed bootstrap testing procedure and comparisons with other existing tests. Section ==== illustrates the application of our test on modeling the exchange rates of US Dollars to Chinese Yuan. We conclude the paper in Section ====. All technical proofs are given in the Appendix.====This appendix provides the proof of ====. ==== denotes the expectation conditional on ====, ==== denotes a sequence of random variables converging to zero (bounded) in probability conditional on ====. We firstly state three lemmas which are needed in the proof of ====. The first one is Lemma A.4 and Theorem 1 in ====, which shows the consistency and the asymptotic distribution of the bootstrapped LAD estimation ====. The second one is from ====. The third one is crucial to prove ==== in this paper.",Bootstrapping the transformed goodness-of-fit test on heavy-tailed GARCH models,https://www.sciencedirect.com/science/article/pii/S0167947323000555,27 March 2023,2023,Research Article,26.0
"Dong Qingkai,Liu Binxia,Zhao Hui","School of Statistics and Mathematics, Zhongnan University of Economics and Law, Wuhan 430070, China","Received 21 July 2022, Revised 7 March 2023, Accepted 13 March 2023, Available online 17 March 2023, Version of Record 22 March 2023.",https://doi.org/10.1016/j.csda.2023.107743,Cited by (0),This paper proposes a new model averaging method for the accelerated failure time models with right ,"). Various strategies have been proposed to estimate the parameters in the AFT model, including Miller's estimator (====), Buckley-James estimator (====; ====), KSV estimator (====, ====; ==== show that it performs much better than the other estimators, particularly when the number of covariates is large or the censoring is heavy.====; ====; ====; ====; ====. About model selection in the AFT model, there are some methods based on the penalized weighted least squares estimator, such as ====; ====; ====. However, when a single model is not overwhelmingly supported by the data, these model selection methods may ignore contributions of other candidate models and suffer from the model selection uncertainty and bias problem (====To address these issues and improve prediction accuracy, various model averaging approaches have been proposed by exploiting all information from every candidate model. Inspired by AIC and BIC, ==== proposed smoothed AIC (SAIC) and smoothed BIC (SBIC) methods. ==== proposed a local misspecification framework to establish properties of model averaging estimators. ==== proposed a model averaging estimator with weights selected by minimizing a Mallows criterion. This Mallows model averaging (MMA) estimator asymptotically achieves the smallest possible squared error in the class of model averaging estimators. ==== modified the conditions of ==== by allowing non-nested candidate models and continuous weights. These improvements make the conditions of MMA more natural at the cost of limiting the number of candidate models. Another important model averaging criterion is the Jackknife model averaging (JMA) proposed by ====As for the AFT model, ==== proposed a high dimensional JMA procedure, where the penalized Buckley-James method (====) was used to obtain the coefficient estimators. However, the convergence of Buckley-James estimate cannot be guaranteed, and the possible overlap of variables in different candidate models is not considered in ====. Recently, ==== proposed another model averaging method based on KSV estimate and MMA criterion. As specified by ====, in many cases, the effect of KSV estimator is not as good as WLS estimator. Moreover, constructing a linear model for the synthetic response may face the problem of excessive error variance.====. In particular, as the variances of error terms are unknown in many applications, we also consider the estimation of variance in the Mallows criterion and prove that even when the variances of the error terms are estimated and the feasible weighted least squares estimators are averaged, our method still has asymptotic optimality, which is the most important theoretical property of model averaging method and one of main theoretical contributions of this paper. Besides, our method allows continuous weights, and the variables in each candidate model can be overlapped, which greatly improves the flexibility and applicability of the method. Extensive simulation shows that our WLSMA method outperforms many existing model selection and model averaging methods. In the empirical study of the PBC dataset, WLSMA method has also obtained good prediction accuracy.====The rest of the paper is organized as follows. We begin in Section ==== with the description of some notations, the AFT model and the WLS estimate. In Section ====, we propose our WLSMA method and present the asymptotic optimality of this new method. Sections ==== and ==== report the simulation results and the application in the PBC dataset. Finally, we provide some concluding remarks in Section ==== and outline the proofs of the theorems in the Appendix.====Before giving the proof of ====, we need two lemmas:====These are conclusions from ==== and ====. Their assumptions are similar to ours so we can get these conclusions directly. The details are omitted here.",Weighted least squares model averaging for accelerated failure time models,https://www.sciencedirect.com/science/article/pii/S0167947323000543,17 March 2023,2023,Research Article,27.0
"Kawakubo Yuki,Kobayashi Genya","Graduate School of Social Sciences, Chiba University, 1-33, Yayoi-cho, Inage-ku, 2638522, Chiba, Japan,School of Commerce, Meiji University, 1-1, Kanda Surugadai, Chiyoda-ku, 1018301, Tokyo, Japan","Received 20 September 2019, Revised 8 December 2022, Accepted 9 March 2023, Available online 16 March 2023, Version of Record 22 March 2023.",https://doi.org/10.1016/j.csda.2023.107741,Cited by (0),"This paper proposes a new model-based approach to small area estimation of general finite-population parameters based on grouped data or frequency data, often available from sample surveys. Grouped data contains information on frequencies of some pre-specified groups in each area, for example, the numbers of households in the income classes. Thus, grouped data provide more detailed insight into small areas than area-level aggregated data. A direct application of the widely used small area methods, such as the Fay–Herriot model for area-level data and nested error regression model for unit-level data, is not appropriate since they are not designed for grouped data. Our novel method adopts the multinomial likelihood function for the grouped data. In order to connect the group ","Sample surveys are generally designed to estimate finite population parameters, such as total, mean, variance, and ====. Decision-makers of both public and private agencies have become interested in such parameters for smaller subpopulations (small areas) as well, created by cross-classifying geographical and demographical variables, such as age, sex and race. However, direct survey estimators of small area parameters, sample mean, sample variance, sample quantiles, and others, are often unstable and unreliable because the sample size for each area is too small, mainly due to the budget constraint.====, ====, ==== and others.====. A single population parameter, such as an areal mean, can be estimated at a time by using Fay–Herriot model. The nested error regression model for unit-level data and its extensions (====; ====; ====; ====) can be used to estimate general finite population parameters provided that a unit-level data is available. In general, the Fay–Herriot model is more widely used in practice as the accessibility of unit-level data is still limited in many cases.==== and references therein. Especially in the analysis of income data, the individual households often are grouped into some predefined income classes (====The grouped data literature, mainly from the viewpoint of the income data analysis, predominantly focused on developing a more flexible underlying ==== or semiparametric form for a single nation, region, or period. However, when we face the grouped data over multiple local areas as in the HLS data, the existing grouped data methods do not suffice. This is because the reported frequency distributions are based on the survey sampling, and they are not reliable for areas with small sample sizes and thus call for a correction through an small area estimation method.====None of the existing small area estimation methods can be used directly to reduce uncertainty in grouped data. This is because grouped data do not contain unit-level information required by the nested error regression model, and that an appropriate direct estimator to be used in the Fay–Herriot model is difficult to define for many small area parameters. Therefore a new small area estimation method designed explicitly for grouped data is required.==== and ====The unknown model parameters (hyperparameters) are estimated by maximizing the marginal likelihood, which integrates out the random effects. Since the marginal likelihood cannot be evaluated analytically, we develop a Monte Carlo EM (MCEM) algorithm (====; ====). For these reasons, the present paper adopts the empirical Bayesian method. Of course, we acknowledge the advantages of the full Bayesian approach, the development of the full Bayesian method is a future work.====, ==== and ==== studied small area estimation methods for compositional data, fitting a multivariate Fay–Herriot model after transformation. Hence, their mixed model directly builds upon transformed compositional data. This is the key difference from our approach which introduces latent variables and thus models grouped data in a natural way.====The rest of the paper is organized as follows. Section ==== first describes the proposed model. Then the methods for hyperparameter estimation and calculation of EB estimates are described. Section ==== demonstrates the proposed method on the HLS income data of Japan. The patchy maps of the areal mean income and Gini coefficient are completed using our method. In Section ====, the performance of the proposed model is examined through the model-based and design-based simulation studies. Finally, Section ==== concludes the paper with some discussion.",Small area estimation of general finite-population parameters based on grouped data,https://www.sciencedirect.com/science/article/pii/S016794732300052X,16 March 2023,2023,Research Article,28.0
"Capanu Marinela,Giurcanu Mihai,Begg Colin B.,Gönen Mithat","Memorial Sloan Kettering Cancer Center, NY, USA,Department of Public Health Sciences, University of Chicago, IL, USA","Received 24 August 2022, Revised 2 March 2023, Accepted 6 March 2023, Available online 11 March 2023, Version of Record 23 March 2023.",https://doi.org/10.1016/j.csda.2023.107740,Cited by (0),"A novel variable selection method for low-dimensional ==== of the OPT-STABS estimator are derived, and its root-n consistency and ",", ====, ====, among others. Faced with so many options and insufficient guidance, many practitioners continue to employ time-tested standards such as stepwise techniques, their known pitfalls notwithstanding.====To further complicate the problem, recent technological progress has led to increasingly frequent analyses of high-dimensional data (====), the group LASSO (====), the adaptive LASSO (====), and regularized LASSO (====). Different penalties have also been introduced, notably the smoothly clipped absolute deviance penalty (SCAD) (====). Building on these methods, stability selection (====In summary, many variable selection procedures are available both for the low-dimensional (classical setting, ====) and high-dimensional settings and an exhaustive review of this vast literature is beyond the scope of this article. It is intriguing that for an age-old problem such as variable selection, there still is no standard go-to method that researchers can trust to use in their day-to-day analyses.====In ==== for more details). This cutoff is then used as a screening threshold to determine which variables to include in the final model. Through an extensive simulation study, we showed that for low-dimensional linear regression models, this method has superior performance compared to commonly used techniques.====In addition to this novel method, we also adapt the STOPES approach from the linear regression to the GLM setting. For this purpose, we modify the optimization objective function by replacing the mean squared prediction error used for linear regression with the AIC which is applicable to any GLM. Taking advantage of the AIC's property of being asymptotically equivalent with leave-one-out cross-validation (====), we no longer use the validation step and, rather than splitting the data into training and validation, we now propose to use subsampling of half of the data, just as above.====In Section ====, we describe some commonly used variable selection techniques and in Sections ==== and ====, we introduce the proposed methods. In Section ====, we report an extensive simulation study to evaluate the methods for logistic regression and we apply the methods to a binary outcome dataset on transfusion. In Section ====.====We outline here the main lemmas used in the proof of the main theoretical result (====). Let ==== denote bounded in probability. ==== below shows the asymptotic distribution of ==== for ==== and provides an asymptotic expansion of ==== for ====.",Subsampling based variable selection for generalized linear models,https://www.sciencedirect.com/science/article/pii/S0167947323000518,11 March 2023,2023,Research Article,29.0
Grömping Ulrike,"Berliner Hochschule für Technik, Luxemburger Str. 10, D-13353 Berlin, Germany","Received 21 March 2022, Revised 28 February 2023, Accepted 1 March 2023, Available online 6 March 2023, Version of Record 17 March 2023.",https://doi.org/10.1016/j.csda.2023.107739,Cited by (0)," where the original constructions pay no attention to column orthogonality. All presented constructions are implemented in the R package SOAs. As an aside, it is argued that “stratum” is a better choice than “strong” for the “S” in the acronym SOAs."," (====, ====, ====, ====, ====, and most recently ====. This author is attracted by the concept, but considers the adjective “strong” in its label as misleading: by a stretch of concept, SOAs can be seen as orthogonal arrays, but as very ==== ones (low OA strength) only. For the sake of clarity, this paper explicitly distinguishes between OA strength and SOA strength, because these are related, but distinct, concepts. In order to avoid confusing use of the adjective “strong”, this paper uses the acronym SOAs, but connects it with the alternate long form “Stratum Orthogonal Arrays”. The rationale behind that expression: when columns are collapsed to strata, SOAs become strong(er) orthogonal arrays.====Arrays with many levels per column, such as SOAs, are primarily used for computer experiments with quantitative variables. The most well-known examples are LHDs, which were first proposed by ====: for these, each column has as many levels as there are experimental runs. The most important property of LHDs is their “space-filling” behavior, which can be measured in a variety of ways (see Section ====). Many constructions are based on orthogonal arrays, e.g. ===='s (====) proposal to expand the levels of an OA, ===='s (===='s (====. It should be noted that there are SOA constructions for LHDs (e.g. 6 columns with 125 levels in 125 runs) or for non-LHD arrays with many levels for each column (e.g., the small SOA of ==== or an SOA in 81 runs with 12 27-level columns), but also for arrays with many columns at as few as 4 levels each (e.g. 40 columns at 4 levels each in 88 runs).====This paper provides a unifying overview of diverse SOA constructions that have been proposed in recent years. It has been written with a clear focus on practically feasible constructions, which have been implemented in the R package SOAs (====). Section ==== introduces SOAs, provides the equations used for the constructions in this paper, presents and illustrates the earliest constructions, details the practically relevant classes of SOAs and states necessary and sufficient requirements for obtaining the different classes, as far as they can be stated in general terms. Sections ==== and ==== provide further specific constructions in the unifying notation of this paper. Section ==== gives an overview of the constructions and their properties in terms of run sizes, numbers of columns and quality criteria. The discussion gives an overall assessment and an outlook at future developments, and three appendices provide details that would disrupt the flow but are nevertheless of interest; in particular, Appendix ==== (==== ff.) lists many example designs that are constructed throughout the paper, together with an overview of their properties and the ingredients of their construction. Furthermore, supplemental online material is available for this paper.",A unifying implementation of stratum (aka strong) orthogonal arrays,https://www.sciencedirect.com/science/article/pii/S0167947323000506,6 March 2023,2023,Research Article,30.0
"Holter Julia C.,Stallrich Jonathan W.","Department of Statistics, North Carolina State University, Raleigh, NC 27695, USA","Received 3 June 2022, Revised 14 November 2022, Accepted 19 February 2023, Available online 28 February 2023, Version of Record 16 March 2023.",https://doi.org/10.1016/j.csda.2023.107729,Cited by (0),"-modification. When applied to the Lasso estimator, the ====-modification is closely related to the relaxed Lasso estimator. The approach is demonstrated on a functional variable selection problem to identify optimal placement of surface electromyogram sensors to control a robotic hand prosthesis."," Variable selection is often performed to select a predictive model that depends on as few predictor variables as possible. For example, ==== discussed an important functional variable selection problem to develop a prosethesis controller (PC) for a robotic hand. Electromyogram (EMG) signals from surface sensors placed on the residual forearm muscles of an amputee were input into a PC and translated into movement of the robotic hand. For able-bodied subjects, it is known that certain movements are caused by contractions of only a few muscles, implying a predictive PC requires a few strategically-placed EMG sensors.====This paper concerns problems that are well approximated by a sparse linear model:==== where ====, ====, ====, and ==== is the number of important variables. Without loss of generality, assume ==== and predictor variables, ====, are centered and the diagonals of ==== equal ====. Let ==== denote the support of ====. A predictive model's estimate for ====, denoted ====, will ideally also have support ==== and will be close to ==== in some other sense, such as ====.====For high-dimensional data like that in ====, simultaneous support recovery and parameter estimation can be performed via penalized estimation. A penalized estimator is represented generally by ==== where ==== is a loss function comparing ==== to its predicted values ====, and ==== is a penalty function that depends on tuning parameter(s) ====. Henceforth we let ====. Penalty functions can take myriad forms but we are interested in those that increase as ==== moves away from ====. The Lasso (====) penalty, ====, is one such penalty that can force estimates to equal 0, thereby performing simultaneous variable selection and estimation. For such sparsity-inducing estimators, we are interested in comparing the estimated support, ====, to ====.====The chosen ==== balances the importance of minimizing ==== relative to ====, so it is recommended to explore the tuning parameter space to identify an “optimal” value. Potential criteria for an optimal value include identifying a ==== that minimizes ====, minimizes ====, or has ====. The latter criterion is referred to as support recovery and is the primary focus of this paper. Even if a ==== exists where ====, there is no guarantee that we will be able to correctly identify it. Popular approaches, such as minimizing information criteria (====; ====-fold cross validation often choose a ==== that overselects the number of important variables (====; ====), i.e., ====. Post-selection inference techniques (====; ====) and multi-stage modifications (====; ====) can correct for this overselection, albeit with added computations and assumptions.====All models shown in ==== include the 5 important variables. Both the CV 1SE and Min PB models have small APE and large ====, but many false positives. The model in ====(c) has no false positives, but has relatively large APE. The model under ====(d) also has no false positives and its in-sample ==== approximately equals that of the Min PB estimate. This motivates a tuning parameter selection strategy based on an ==== metric rather than APE to compromise between prediction error and variable selection.====After justifying the ==== metric, we highlight and investigate an equivalence between the metric and a multiplicative adjustment on ====, referred to here as the ====-modification. We argue that for ==== with certain statistical properties, the adjustment can reduce the bias of ==== thereby improving its predictive potential. We go on to study the ====-modification for the Lasso, highlighting its similarities to the Nonnegative Garrote (====) and relaxed Lasso (====). Unlike these two methods, the ====This paper is organized as follows. In Section ==== we review classes of penalized estimators, popular tuning parameter selection strategies, and post-selection inference methods. Section ==== justifies the value in the ==== metric and provides statistical properties for a general class of penalized estimators. Finite-sample properties are then derived for the ====-modification for the Lasso in Section ====. Section ==== presents a simulation study of the new approaches and Section ==== applies our new methods to the EMG data of ====. Section ==== provides a discussion on the implications of our new framework for evaluating model fit and propose avenues of future research.",Tuning parameter selection for penalized estimation via ,https://www.sciencedirect.com/science/article/pii/S0167947323000403,28 February 2023,2023,Research Article,31.0
"Escobar-Bach Mikael,Van Keilegom Ingrid","LAREMA, 2 Bd de Lavoisier, Angers, 49000, France,ORSTAT, KU Leuven, Naamsestraat 69, Leuven, 3000, Belgium","Received 20 November 2021, Revised 10 February 2023, Accepted 11 February 2023, Available online 21 February 2023, Version of Record 24 February 2023.",https://doi.org/10.1016/j.csda.2023.107728,Cited by (0),"When analyzing time-to-event data, it often happens that some subjects do not experience the event of interest. Survival models that take this feature into account (called ‘cure models’) have been developed in the presence of ",", ====, ==== and ==== and ====When the survival time is subject to random right censoring, as is common in survival analysis, all cured subjects will be censored, whereas the non-cured ones can be either censored or uncensored. Hence, it is clear that in order to identify the cure fraction we need to impose certain assumptions on the model. A common way to identify a cure model is to impose the assumption of ====, which means that the right endpoint of the support of the censoring time is larger than the right endpoint of the support of the survival time of the non-cured subjects (conditional on the covariates in case there are covariates in the model). See e.g. ==== for more details. In non- and semiparametric cure models, this assumption is standard. An informal way to verify this assumption in practice is to check whether the ====, ====, ====, ====, ==== and ====In the absence of covariates, ==== proposed a method to improve the estimation of the cure rate by using extreme value theory. With respect to the latter paper, this paper makes several steps forward. First, covariates are included in the model, which is an important improvement towards the application of the method in practice, but which complicates the theory considerably. And second, in this paper we do not only estimate the conditional cure rate, but we also propose an estimator of the conditional extreme value index and of the conditional survival function. The latter estimator will be based on the local Kaplan-Meier estimator proposed by ====, to which a term is added that corrects the bias caused by the insufficient follow-up. Also note that the estimator that we propose for the conditional cure rate, does not reduce to the estimator proposed in ==== in the absence of covariates. In fact, we propose an improved estimator that depends on two tuning parameters that we choose in a data-driven way, whereas the estimator suggested in ==== is based on a different development, it uses one tuning parameter and does not allow for covariates.====The remainder of the paper is organized as follows. In the next section we introduce our proposed estimators of the conditional extreme value index, the cure rate and the survival function. In Section ==== for various models within a Fréchet domain of attraction, while in Section ====The following is the Supplementary material related to this article.",Nonparametric estimation of conditional cure models for heavy-tailed distributions and under insufficient follow-up,https://www.sciencedirect.com/science/article/pii/S0167947323000397,21 February 2023,2023,Research Article,32.0
"Liang Jinwen,Härdle Wolfgang Karl,Tian Maozai","College of Statistics and Data Science, Faculty of Science, Beijing University of Technology, Beijing, 100872, China,Center for Applied Statistics and Economics, Humboldt-Universität zu Berlin, 10178 Berlin, Germany,Center for Applied Statistics, School of Statistics, Renmin University of China, Beijing, 100872, China,School of Statistics and Data Science, Xinjiang University of Finance, Urumqi China","Received 5 December 2021, Revised 18 September 2022, Accepted 29 January 2023, Available online 14 February 2023, Version of Record 22 February 2023.",https://doi.org/10.1016/j.csda.2023.107713,Cited by (0), concentration level is conducted to verify the efficiency of the estimation procedure.,"); spatial autoregressive panel data models with fixed effects and spatial autoregressive errors (====); random effects and fixed effects models with spatial errors (====); fixed-effects SDPD models with short panels (====).====; ====) for more reviews. ==== used similar methods to model the spatial-temporal characteristics of emergency department expenditures data in Dalamo, North Carolina, USA.====All of the models mentioned above are developed for complete data. As for incomplete data, ==== and references therein. Extending these quantile-related methods in the presence of missing values to spatial temporal data is not straightforward. Therefore, the research for spatial temporal missing data in quantile regression is meaningful.====; ====) and maximum margin matrix factorization (====) together and put forward an efficient algorithm for large matrix factorization and completion.====studied the regression problem with a multidimensional array (tensor) response and a vector predictor. These approaches don't allow missing entries and only work for complete data. ====is one of the few literature dealing with tensor regression between partially observed dynamic tensor and vector valued covariates. This regression model is different from our later proposed model.====, ====The rest of the article is organized as follows. Section ==== introduces the model and methodology. In Section ==== the theoretical properties of the proposed estimators are presented. In Section ==== simulation studies are conducted to verify the efficiency of this approach. In Section ==== we analyze the air quality data set using our method. And finally in Section ==== we draw some conclusions and directions for future research.====Proof in ====. Let us first give two Lemmas needed to prove ====. By ====, the nuclear norm can be bounded by the Frobenius norm. Equation ==== is the triangle property as introduced in ====. ",Imputed quantile tensor regression for near-sited spatial-temporal data,https://www.sciencedirect.com/science/article/pii/S0167947323000245,14 February 2023,2023,Research Article,33.0
"Zhu Hanbing,Zhang Yuanyuan,Li Yehua,Lian Heng","School of Big Data and Statistics, Anhui University, Hefei 230601, China,School of Mathematical Sciences, Soochow University, Suzhou 215006, China,Department of Statistics, University of California, Riverside, CA 92521, USA,Department of Mathematics, City University of Hong Kong, Hong Kong, China","Received 1 March 2022, Revised 27 January 2023, Accepted 8 February 2023, Available online 13 February 2023, Version of Record 17 February 2023.",https://doi.org/10.1016/j.csda.2023.107727,Cited by (0),In this paper we propose a new semiparametric function-on-function ==== regression model with time-dynamic single-index interactions. Our model is very flexible in taking into account of the nonlinear time-dynamic interaction effects of the multivariate longitudinal/functional ,". The model is given by==== where ==== is the response, ====-dimensional vector ==== and a ====-dimensional vector ==== with ==== studied a more general single-index varying coefficient model in which ==== can be different for different function ====. To assess how multiple environmental factors act jointly to modify individual genetic risk on complex disease, ==== considered a partial linear varying multi-index coefficient model which includes SVCM as a special case. ==== and ====The aforementioned works studied SVCM in the context of independent observations. As far as we know, there is limited literature on SVCM for longitudinal/functional data. However, with modern technology related to data collection and storage, functional data have become increasingly available in many scientific fields, such as meteorology, chemistry, economics and epidemiology, and thus have gained considerable attention in the literature in recent years; see ====, ==== and ====. Recently, to capture the dynamic interaction effects between population aging and socio-economic variables on COVID-19 mortality rate, ==== where ==== is a natural extension of SVCM by allowing SVCM to hold for any given time ====, ====, and ==== for details on the relationship between model ==== and existing models.====; ====; ====; ====). Hence, in this paper, we focus on the following quantile regression model:==== where ====, ==== with ====, and ====, for ====. We call ==== semiparametric function-on-function quantile regression model with dynamic single-index interactions. Model ==== includes many existing quantile regression models for longitudinal data as special cases; see ====, ====, ====, and ====.==== as well as the bivariate coefficient functions ==== in model ====The main contributions of our work or the key differences between our paper and ==== are three-fold. First, we extend model ==== to quantile regression framework. Quantile regression is a valuable alternative to mean regression for analyzing longitudinal data. Compared to mean regression, quantile regression for model ==== is more technically challenging and has not yet been considered in the literature. Second, we consider the sparse and unbalanced longitudinal data case. ====The rest of this paper is organized as follows. In Section ====. Section ==== and ====, respectively. Some concluding remarks are given in Section ====. All proofs are deferred to Appendix ====.====We propose the following algorithm to obtain the initial estimator ==== of ====.====  Obtain an estimator of ====, say ====, through==== Standardize ==== with ====.==== Given ====, ==== can be estimated as==== Thus, we obtain the estimator ==== of ====, for ====.====   We update the estimate of ==== by minimizing the objective function==== with respect to ==== through nonlinear optimization. This can be achieved by “optim” function in the R software. Let ==== be the minimizer of ==== and ====.====  Repeat Steps 1 and 2 until convergence to obtain the final estimator ====.====Next, we present some notations and lemmas that will be used in our proofs. The double sum ==== below always means ====. For any matrix ====, ==== and ==== denote its Frobenius and operator norms, respectively, and ====. We denote ==== as ====.==== ==== ==== ",Semiparametric function-on-function quantile regression model with dynamic single-index interactions,https://www.sciencedirect.com/science/article/pii/S0167947323000385,13 February 2023,2023,Research Article,34.0
"Xu Danli,Wang Yong","Department of Statistics, The University of Auckland, Private Bag 92019, Auckland, New Zealand","Received 4 May 2022, Revised 2 February 2023, Accepted 4 February 2023, Available online 11 February 2023, Version of Record 22 February 2023.",https://doi.org/10.1016/j.csda.2023.107715,Cited by (0),"Nonparametric density estimation is studied for spherical data that may arise in many scientific and practical fields. In particular, nonparametric mixture models based on likelihood maximization are used. A nonparametric mixture has component distributions mixed together with a mixing distribution that is completely unspecified and needs to be determined from data. For mixture components, a two-parameter distribution family can be used, with one parameter as the mixing variable and the other to control the smoothness of the density estimator. For example, the popular von Mises-Fisher distributions can be readily used for this purpose. ==== with various spherical data sets show that the resultant mixture-based density estimators are strong competitors with the best of the other density estimators.","In many real-world applications, observations can just be directions. A direction in ==== is known as a circular observation, and one in a higher-dimensional Euclidean space a (hyper-)spherical observation. Since the lengths of the vectors that represent directions are irrelevant, they can be normalized and thus are equivalent to points on the unit ====-sphere ====. Spherical data may arise in many fields, such as earth science (====), astronomy (====), bioinformatics (====), and text mining (====). In this paper, we study spherical data, for ====, and specifically their nonparametric density estimation.==== and has been developed extensively for data in ==== and ==== in recent decades.====The remainder of the paper is organized as follows. Section ==== provides some basic background knowledge on spherical data and distributions, along with some density estimation methods, which will be needed later in the paper. In Section ==== and ====, respectively. Conclusions and final remarks are given in Section ====.====The following is the Supplementary material related to this article.",Density estimation for spherical data using nonparametric mixtures,https://www.sciencedirect.com/science/article/pii/S0167947323000269,11 February 2023,2023,Research Article,35.0
"Motegi Ryosuke,Seki Yoichi","Graduate School of Science and Technology, Gunma University, 1-5-1 Tenjin, Kiryu, 376-8515, Gunma, Japan,Faculty of Informatics, Gunma University, 1-5-1 Tenjin, Kiryu, 376-8515, Gunma, Japan","Received 3 March 2022, Revised 31 January 2023, Accepted 3 February 2023, Available online 10 February 2023, Version of Record 15 February 2023.",https://doi.org/10.1016/j.csda.2023.107714,Cited by (0),". The extended SOM defines a graph with clusters as vertices and neighborhood relations as links and updates the graph structure by cutting weakly connected and unnecessary vertex deletions. The weakness of a link is measured using the Kullback–Leibler divergence, and the redundancy of a vertex is measured using the ====. Those extensions make it efficient to determine the appropriate number of clusters. Compared with existing methods, the proposed method is computationally efficient and can accurately select the number of clusters.","; ====; ====). In the implementation of clustering, the selection of the number of clusters ==== can be difficult. In most situations in which an application must perform clustering, the true number of clusters ==== is generally unknown. The selection of ==== as ==== or ==== could cause misleading results. In many methods that have been proposed to solve the problem of selecting ====, it is considered to be a problem of model selection (====In the penalized likelihood method, an optimal ==== is selected among candidate models that are obtained by performing clustering with different values of ====) is derived by minimization of the Kullback–Leibler (KL) divergence (====) and minimum message length (MML) (====; ====), which are derived in terms of coding theory, are also popular. Another form, integrated classification likelihood (ICL) (==== using reversible-jump Markov chain Monte Carlo (RJMCMC) (====) requires the initial value dependence to be considered. Although the method based on Bayesian inference can provide abundant information regarding ====, the MCMC-based method is computationally expensive.====In terms of methods with low initial value dependency and computational efficiency for selecting ==== by splitting each cluster. The decision as to whether to split a cluster is made using a predetermined split decision criterion. X-means (====), which is a representative method of this approach, uses a K-means algorithm and BIC to apply the split decision criterion. Some studies on improving X-means have been reported (====; ====; ====).====G-means (====PG-means (====) does not adopt the splitting method, but it is also an extension of G-means and related to X-means. PG-means assumes the dataset has been generated from a GMM and uses the EM algorithm to estimate the model. PG-means projects the dataset and model (means and covariances) into multiple one-dimensional spaces and tests model fitness in each space. The projections are generated randomly. If any test rejects, the number of clusters is increased by one.====Dip-means (====) is another approach that uses a hypothesis test as a split decision method. The unimodality of the cluster is tested using Hartigan's dip test (====), and the cluster is split into two until the distribution within a cluster becomes unimodal. It tests the unimodality of a distance distribution of each sample within a cluster and splits the cluster with the large proportion of rejected samples.====In contrast to a cluster-splitting method, such as X-means, methods that remove a cluster that is no longer a good representation of the data distribution also exist. The method of ====, called MML-EM, determines the value of ==== by fitting a GMM to the data distribution with MML as the objective function. In their method using the EM algorithm, learning starts from a sufficiently large number of clusters, which are gradually annihilated during the learning process. Clusters that are not supported by most samples during the learning process (i.e., clusters whose mixing probability is close to 0) are removed. The objective function is minimized by the EM algorithm, and cluster annihilation and optimization are performed until no further improvement in the MML occurs. The method has displayed a lower initial value dependence than the standard EM algorithm in artificial and real data experiments.====The above greedy methods use either K-means or the EM algorithm as the learning method. However, K-means is known to be strongly dependent on the initial positions of ==== and to easily converge to a local solution. One of the drawbacks of the EM algorithm is its slow convergence. Although MML-EM can reduce the computational time compared to the standard EM, it remains computationally inefficient compared with methods such as X-means.====Examples of methods with little dependence on the initial positions, yet offering fast convergence, are Kohonen's self-organizing maps (SOMs) (====) and neural gas (NG) (====), which are classical vector quantization methods. These methods have advantages and disadvantages. NG is less likely to converge to a local solution than SOM but requires more computational time for learning than SOM.====Our objective is to construct a fast algorithm with a low initial dependence for selecting a suitable number of clusters. As with MML-EM, the algorithm starts with a sufficiently large ==== and searches for a suitable number of clusters while decreasing ====. Although SOM and NG are popular for clustering tasks, the ==== computations of NG learning compared with the ==== of SOM are unacceptable for our purpose. Therefore, we select SOM as the learning method and propose a greedy method for automatically selecting ====The proposed method is the decreasing approach similar to MML-EM. The method repeats learning by SOM and removing an unnecessary cluster based on the MDL criterion. Each cluster is modeled as a ==== and constructed by samples based on maximum likelihood classification. Thus, we call the method the shrinking maximum likelihood self-organizing map (SMLSOM).====The following two extensions to Kohonen's SOM are made for our approach. First, in Kohonen's SOM, clusters are constructed as sample averages, but our method constructs each cluster as a probabilistic model. Therefore, we extended the SOM learning to a probabilistic setting. Second, in Kohonen's SOM, each cluster is tied to a node of a two-dimensional lattice map, and the map structure is fixed. However, our method removes an unnecessary cluster in the learning; the map structure should vary. Therefore, we introduced a dynamically update method of the map structure to decrease the number of clusters. This update method is made possible by using a graph structure as the map in combination with two procedures: weakly-connected link cutting and unnecessary node deletion.====The remainder of this paper is organized as follows. In Section ====, we present the work related to the proposed method. In Section ==== reports the experimental results obtained using artificial and real data. Section ==== discusses the strengths and weaknesses of the proposed method. Section ==== concludes the paper.====Let ====; the initial reference vector of node ====, ====, is calculated as==== where ====, and ==== are the first and second largest eigenvalues and corresponding eigenvectors of ====, respectively. Further, ==== constitute a sequence of numbers from −2 to 2 with a common difference; they are given by",SMLSOM: The shrinking maximum likelihood self-organizing map,https://www.sciencedirect.com/science/article/pii/S0167947323000257,10 February 2023,2023,Research Article,36.0
"Zou Yuye,Wu Chengxin","School of Economics and Management, Shanghai Maritime University, Shanghai, 201306, China,Department of Statistics, School of Management, Fudan University, Shanghai, 200433, China,School of Mathematics, Hefei University of Technology, Hefei, 230009, China,School of Mathematics and Statistics, Huangshan University, Huangshan, 245041, China","Received 5 June 2022, Revised 23 January 2023, Accepted 29 January 2023, Available online 6 February 2023, Version of Record 14 February 2023.",https://doi.org/10.1016/j.csda.2023.107711,Cited by (0)," (SICM), which is a very general and flexible tool for exploring the relationship between response variable and a set of predictors. The ",", missing data often appear and may be accompanied by the cause-of-failure information that may not be observed completely due to various causes. For instance, in clinical trials, the death of an individual is attributable to the cause of interest may require information that is not collected or lost, or it may be difficult to determine the cause for some patients. Specially, in this paper, we analyze survival data from a double-blind adjuvant clinical trial involving 168 elderly women with stage II breast cancer conducted by the Eastern Cooperative Oncology Group (ECOG). By the end of the trail, some patients died from breast cancer. Some patients died from other known cause, and the remaining died from unknown reasons. All patients were divided into two groups, one receiving the tamoxifen treatment, and one receiving placebo. In order to compare the tamoxifen and placebo among patients who died by the end of the trail, as well analyze other factors affect the survival time, such as the estrogen receptor status, the number of axillary lymph nodes and the size of primary tumors, we seek more suitable models and methods in this paper.====.==== where ==== is the survival time, ==== is an unknown index parameter. The functions ==== and ==== are unknown univariate and vector of real-valued, respectively. The model error ==== are independent and identically distribution with zero mean. For sake all identification, we set the index-coefficient ==== and ====, SICM reduces to varying coefficient model (VCM) (====). When ====, SICM boiled down into a single-index model (SIM) (====). ==== obtained the estimation of ==== and achieved its best convergence rate. ==== considered a confidence interval of ====, ==== (====, ====), ====) can be seen as a more robust approach to explore the underlying relationship between the multidimensional covariates and response. However, the estimation efficiency may fluctuate with the particular value of the quantile. To solve this puzzle, ====). Therefore, CQR method is more efficient than single QR and least square methods by integrating the information in a number of quantile regressions when the error distribution is contaminated by outliers or heavy-tail errors. The breast cancer data set has been studied by least square method (====), empirical likelihood approach (====), QR method (====) and linear CQR method (====In addition, in real data analysis, irrelevant variates may often be included in the covariates, especially for the high-dimension covariates. Hence, it is necessary to take variable selection into account. Various powerful penalization techniques have been developed for variable selection, such as LASSO (Least absolute shrinkage and selection operator, ====), SCAD (Smoothly clipped absolute deviation, ====), adaptive LASSO (==== developed linear regression model. ==== discussed partially linear varying coefficient model. ==== considered partially linear single-index model. ==== studied SICM under complete data. However, there exists a mass of incomplete data in practical fields.====In the breast cancer clinical trial, the survival time ==== cannot be completely observed due to some censoring time ==== with distribution function (df) ====. One can only observe ==== with df ==== and censoring indicator ==== showing whether death was caused by breast cancer. Define ==== be missing indicator of whether cause of death was known, which is 0 if ==== is missing and is 1, otherwise. In this paper, we assume that ==== is independent ==== and ==== is MAR, which implies that the missing cause of death depends on the observed variables including covariates, but not on the information whether death was caused by breast cancer. In other words, ==== and ==== are conditionally independent given ====, i.e., ====. The MAR assumption is common in statistical analysis and is reasonable in many practical situations; see ====The naive way to deal with missing data is to directly ignore the cause of death. It is the so-called complete case (CC) method. However, this method may result in bias and loss of efficiency when the cause-of-failure information is not missing complete at random (MCAR), which is a stronger assumption than MAR condition. Various methods have been proposed to deal with missing data to reduce bias and inefficiency. Inspired by ====, they take the weight ====, where ====. It is unfortunate that they did not consider the missing cause-of-failure information. Following the idea of ====, we replace ==== by a weight average of ==== and its estimated conditional mean ====, that is, ====. Hence, the estimators of parametric and nonparametric parts in SICM are proposed by minimizing the weighted objective functions. When ====, we replace the known censoring indicators with their estimated condition expectation, which yields regression calibration estimation. When ====, where ==== is an estimator for conditional mean of ====, we substitute the former weight ==== inversely proportional to its estimated conditional mean for the weight ==== and ==== applied simple and augmented inverse probability weighted idea for linear quantile regression. Wang and Dinse (2010) and ==== suggested regression calibration, imputation and inverse probability weighted for linear regression model. ====, ==== and ==== employed regression calibration, imputation and so on for semi-parametric QR models. ==== developed regression calibration and imputation methods for linear CQR model, among others.====In this paper, we concern with statistical inference and application of CQR estimation and variable selection for SICM with cause-of-failure information MAR. The main contribution can be summarized as follows. We apply regression calibration, imputation and inverse probability weighted approaches to deal with the missing data, the proposed methods can avoid bias and inefficiency caused by the problem of missing data. We propose a CQR estimation method to construct estimators of unknown parametric and non-parametric parts and prove their ====. The method effectively overcomes the drawback of a relative small efficiency that may result from a single QR procedure compared with the least square estimation. At the same time, variable selection by applying adaptive LASSO penalty is adopted to construct the penalized estimators, which enjoy the oracle property. Simulation analysis is conducted to assess the finite sample performance of these proposed estimators and variable selection procedure. The proposed methods have also been applied to the analysis of the survival data with missing cause of death from a clinical trial on breast cancer.====The rest of this paper is organized as follows. In Section ====, we propose CQR procedure and local linear smoothing technique to construct estimators of the parametric and nonparametric parts in SICM. Meanwhile, we introduce an adaptive LASSO penalty procedure to conduct variable selection. In Section ====, the asymptotic normalities of these estimators and the oracle property of the variable selection are established. A simulation study with normal error and heavy-tail error is conducted to demonstrate the finite sample performance of the proposed estimators in Section ====. In Section ====, we apply the proposed method to analyze the breast cancer clinical trial real date. Conclusion is given in Section ====. Some lemmas and proofs of the main results are shown in the Appendix ====.====To prove ====, ====, it suffices to prove the following lemmas. For simplicity, we give some notations. ====,==== ==== ==== ==== ",Composite quantile regression analysis of survival data with missing cause-of-failure information and its application to breast cancer clinical trial,https://www.sciencedirect.com/science/article/pii/S0167947323000221,6 February 2023,2023,Research Article,37.0
De Iaco S.,"Department of Economic Sciences, University of Salento, Via per Monteroni, Complesso Ecotekne, Lecce, 73100, Italy,National Biodiversity Future Center, Palermo, Italy,National Center of High Performance Computing, Big Data and Quantum Computing, Bologna, Italy","Received 10 December 2021, Revised 28 January 2023, Accepted 28 January 2023, Available online 6 February 2023, Version of Record 9 March 2023.",https://doi.org/10.1016/j.csda.2023.107709,Cited by (0),", is provided. Indeed, starting from the Lajaunie and Béjaoui models extended to a space-time domain, generalized families of complex models are obtained through the integration with respect to a positive measure. A procedure for fitting the two parts of the spatio-temporal complex models and for defining the density function considered for the integration is also illustrated. The computational details of this procedure are discussed through a case study on a spatio-temporal dataset of sea currents and the performance of these classes of models is assessed.","In Geostatistics, the theory of complex-valued random fields is often recalled for analyzing vectorial data in two dimension, as the observations of wind speed, sea current and electric field, naturally specified through two components, i.e. modulus and direction. In the literature, there are well-known contributions regarding the complex approach for studying vector data, starting from ====, ==== in a one-dimensional domain (time domain) and ==== in a two-dimensional domain (space domain), together with other subsequent works by ====; ====; ====, ====; ====, ====; ====, which have contributed to the definition of models and interpolation methods for a complex variable. Recently, ====; ====; ====; ====; ====; ====; ====).====, ====; ==== proposed some advances in this direction. Apart from these works, there do not exist contributions in the geostatistical literature with regard to the spatio-temporal complex-valued random fields ====, where ==== and ==== are two real-valued space-time random fields, defined on ====, ==== and ==== are the spatial and temporal coordinates and ==== is the imaginary number. In this context, proposing further developments in modeling the joint spatial and temporal behavior of vector data with a plausible representation on a complex domain represents a considerable challenge for its implications in solving interpolation or simulation problems.====The novelty of this paper concerns the construction of new general classes of spatio-temporal complex covariance models obtained by integration, which have great potential in the applications thanks to their flexibility. Indeed, these families are built by integrating the Lajaunie and Béjaoui models, extended to a space-time domain, with respect to a positive measure. A procedure for fitting the two parts of the spatio-temporal complex models and for defining the density function considered for the integration is also illustrated. It is worth pointing out that the capability of this approach is related to the possibility of describing various behaviors of the real and imaginary parts of the empirical complex-valued covariance functions, starting from a separable real covariance model in space-time and the corresponding compatible imaginary part. This is a very useful characteristic from a computational point of view since the model to be integrated is separable and then can be easily fitted on the basis of the empirical marginals in space and time. At the same time the proposed approach is such that the use of integration generates non-separable real and ====), with some examples, has been presented in Sect. ==== and the connected practical aspects concerning the fitting procedure have been provided in Sect. ====).====In this Appendix, other examples of spatio-temporal complex-valued covariance models, obtained by applying the ====, ==== have been provided.",Spatio-temporal generalized complex covariance models based on convolution,https://www.sciencedirect.com/science/article/pii/S0167947323000208,6 February 2023,2023,Research Article,38.0
"Zeng Yicheng,Zhu Lixing","Shenzhen Research Institute of Big Data, The Chinese University of Hong Kong, Shenzhen, Guangdong, China,Research Center for Statistics and Data Science, Advanced Institute of Natural Sciences, Beijing Normal University at Zhuhai, Zhuhai, Guangdong, China,Department of Mathematics, Hong Kong Baptist University, Hong Kong","Received 24 February 2022, Revised 23 November 2022, Accepted 24 January 2023, Available online 3 February 2023, Version of Record 15 February 2023.",https://doi.org/10.1016/j.csda.2023.107704,Cited by (0),"For large dimensional spiked models, the order (number of spikes) determination is an important issue for dimension reduction. The authors propose a generic criterion to estimate the order when the dimension is proportional to the sample size and the order is divergent as the dimension goes to infinity. To handle the divergence of the order, the criterion is defined by location-shift truncated eigenvalues, unlike the existing criteria. They suggest two versions of the criterion: the first defines an objective function that is a sequence of ridge ratios of the defined eigenvalues in order to have a clear separation between the ratio at the true order and other ratios; and the second uses an objective function of double ridge ratios to enhance such a separation. To alleviate the effect of the bias in the scale estimation when the order is large, an iterative procedure is utilized for the estimation. ==== are conducted on spiked population models and spiked Fisher matrices to examine the finite sample performances of the proposed methods."," is an instructive reference on this topic.====; spiked Fisher matrices (see ====; ====), which are motivated from testing the equality of two covariance matrices; and sliced inverse regression-based matrices (see ====; ====).====In this paper, we will focus on the order determination for large (or high-dimensional) random matrices where the dimension ==== is proportional to the sample size ==== and the order ==== is also divergent as ==== and ==== for a comprehensive theoretical background for this direction.====When ==== is fixed or divergent at a slow rate and ====) and several methods developed for SDR but readily apply to other order determination problems, such as the sequential testing method (====), the BIC-type criterion (====), the ridge ratio estimation (====), the threshloding double ridge ratio estimation (====) and the ladle estimation (====, there are several methods in the literature. One class of frequently used methods is scree plot-based, see examples in ====, ====, and ====. More relevant references include ==== and ==== for factor models, ==== for spiked Fisher models and ==== for canonical correlation analysis. More recently, ==== introduced a novel “valley-cliff” criterion with two different versions.====However, most existing methods for estimating ==== were designed for fixed ==== cases and thus their performances deteriorate when the order ==== increases with ==== because the asymptotics of eigenvalues become different. So the theoretical study of spiked-type models with a divergent number of spikes is a foundation for the order determination problem. ==== investigated a general spiked model for covariance matrix with ==== made another theoretical contribution to the spiked Fisher matrices with a divergent ====.====We will introduce a generic criterion for the order determination to accommodate the divergent order. Before doing so, we may need to figure out why existing methods for the fixed ==== cases do not apply to the divergent ==== cases. To construct an objective function for the formulation of the criterion, it is crucial to make the function value at the true order well separated from others. When ==== is divergent, existing methods have difficulty doing this well since the separation becomes difficult, as shown in the toy example in the next section. The criterion proposed by ==== is based on an objective function that can well achieve this goal when ==== is required to ensure the estimation consistency. This restriction makes no sense in divergent ==== cases. As a solution, we will define an objective function that is based on a sequence of location-shift and truncated eigenvalues to form ridge ratios and avoid this restriction. The definition can be found in Subsection ====. For short, write location-shift and truncated as LsT.====It is worth noticing that the most significant feature of the proposed objective function at the population level is, as a function of all ratios, its discontinuity at the true order such that the corresponding ratio can be very much separated from the others. This inherits the good separation mechanics of the “valley-cliff” criterion in ====, which is unique compared with other existing criteria in the literature to provide a better way to identify the order. The details can be found in Section ====. On the other hand, we also like to point out that to achieve this separation, more than one tuning parameters get involved in the criterion. This seems, as a tradeoff, to make the estimation procedure complicated compared with other existing criteria. The natural questions/concerns are: whether the criterion is easy to implement; whether it is sensitive to the tuning parameters; whether the tuning parameters are difficult to select such that the order determination difficulty for existing criteria has been transferred to the tuning parameter selection difficulties for the proposed criterion; and whether it performs better than some existing criteria that involve less tuning parameter(s). In this paper, we will answer these questions and explain why such a tradeoff is worthwhile.====Another issue is about the bias in the largest non-spiked eigenvalue when the dimension is not large enough, particularly when ==== is large, which affects the performance of the criteria for the order determination. ====The remainder of this paper is organized as follows. In Section ====, we first illustrate two issues of order determination in divergent ==== cases such that we can see what would be a possible solution. Then we introduce a general model feature to describe the spiked structure to which our criterion can be applied. Subsequently, we propose two versions of the criterion we will construct: the thresholding LsT-based ridge ratio criterion (LTRR) and an enhancement version named the thresholding LsT-based double ridge ratio criterion (ELTRR). Also, their consistencies and the selection of ridges in their constructions are discussed in this section. In addition, we highlight their ability to detect weak signals in Section ====. Section ==== contains two models as applications of our method. In Section ====The following is the Supplementary material related to this article.",Order determination for spiked-type models with a divergent number of spikes,https://www.sciencedirect.com/science/article/pii/S0167947323000154,3 February 2023,2023,Research Article,39.0
"Roy Arkaprava,Sarkar Abhra","Department of Biostatistics, University of Florida, 2004 Mowry Road, Gainesville, FL 32611, USA,Department of Statistics and Data Sciences, The University of Texas at Austin, 2317 Speedway D9800, Austin, TX 78712-1823, USA","Received 9 April 2022, Revised 24 January 2023, Accepted 25 January 2023, Available online 1 February 2023, Version of Record 7 February 2023.",https://doi.org/10.1016/j.csda.2023.107706,Cited by (0),"In multivariate density ====, the distribution of a random vector needs to be estimated from replicates contaminated with measurement errors. A novel approach to multivariate deconvolution is proposed by stochastically rotating and stretching or contracting the replicates toward the corresponding true latent values. The method further accommodates conditionally heteroscedastic measurement errors commonly observed in many real data applications. The estimation and inference schemes are developed within a ==== framework implemented via an efficient ",". The variable ====, however, may not be observed precisely, instead surrogate replicates ==== contaminated with measurement errors ==== may only be available. The replicates ==== and the density of the measurement errors ====, and the problem of estimating the density of ==== from available contaminated measurements ====; ====; ====), where the problem of estimating long-term average intakes of different dietary components from their error-contaminated 24-hour recalls is of fundamental importance.====This article proposes a robust approach to multivariate density deconvolution in the presence of conditionally heteroscedastic errors ==== toward the underlying latent ==== in a statistically principled manner.====Throughout this article, for random vectors ==== and ====, we denote the marginal density of ====, and the conditional density of ==== given ====, by the generic notation ==== and ====, respectively. Likewise, for univariate random variables ==== and ====, the corresponding densities are denoted by ==== and ====, respectively.====The literature on density deconvolution is really vast (====; ====). The early literature focused primarily on univariate problems with a single contaminated measurement for each subject and the measurement errors independently and identically distributed according to some known probability law ====, often normal. Deconvoluting kernel-based approaches have been studied (====; ====; ====, ====, and others). See also ====; ====. The distribution of measurement errors is, however, rarely known in practice. Robust deconvolution methods with unknown aspects of the error density estimated using replicated proxies ==== for the unknown values of ==== have thus been considered (====; ====; ==== has recently been developed in ====.====The assumption of independence of ==== from ====; ====). In their seminal work, ==== assumed the errors ==== to be normally distributed but allowed the variability of ==== to depend on ====, employing positive mixtures of B-splines to flexibly characterize both ==== and the conditional variability ====. ==== further relaxed the assumption of normality of ====, employing flexible mixtures of normals (====; ====) to model both ==== and ====.====In stark contrast to the univariate setting, the multivariate problem has garnered little attention in the literature. ====; ====; ====; ====, ====; ==== considered scenarios with errors ==== from a known probability law, independent from ====. ==== obtained a Bayesian maximum-==== estimate of ==== modeled by flexible mixtures of multivariate normal kernels, assuming ==== to be multivariate normal, independent from ====, ====, etc. ==== modeled ==== and ==== using flexible mixtures of multivariate normals whereas ==== adopted a complementary approach, modeling the marginals ==== and ==== first and then building the joint distributions ==== and ====The focus of this article is also on multivariate deconvolution with conditionally heteroscedastic measurement errors from an unknown distribution in the presence of replicated proxies for each subject. To that end, we propose a novel approach to multivariate density deconvolution that assumes the replicates ==== to be generated by first stochastically rotating the underlying true ==== and then stochastically stretching or contracting their lengths. This is achieved by multiplying each ==== and then with a scalar length adjustment factor ====. Going a significant step further, we also accommodate conditional heteroscedasticity by allowing the distributions of both the rotation matrices and the length-adjusting factors to flexibly depend on the latent true ====, and hence ====, are then obtained as novel functions of the ===='s. For the main density of interest ====, we adopt a copula-based approach with the marginals modeled by flexible mixtures of truncated normals with shared atoms as in ====. We take a Bayesian route to estimation and inference, implemented via an efficient MCMC algorithm, appropriately accommodating uncertainty in all aspects of our analysis. We illustrate our method's empirical efficacy through simulation experiments. Its practical utility is demonstrated in nutritional epidemiology applications in estimating the long-term joint average intakes of different dietary components from their measurement error-contaminated 24-hour dietary recalls.====Traditionally, the literature on deconvolution almost exclusively assumes the measurement errors to be additive. In Section ====, ====; ====; ====; ====). In the deconvolution literature, stochastic rotations have been proposed for directional data in ====. Our work, however, is focused on Euclidean deconvolution problems in the presence of replicates contaminated with conditionally heteroscedastic measurement errors. The deconvolution problem we consider and the solution we propose are thus very different from ====; ====.====Overall, this article makes several important contributions to the literature on multivariate density deconvolution - (a) we introduce a new framework for multivariate deconvolution via stochastic rotation of the error-contaminated replicates toward their underlying true values, (b) additionally, we also address the significantly challenging problem of accommodating conditionally heteroscedastic errors in this newly introduced framework, and (c) we introduce HMC-based advanced MCMC methods to the deconvolution problem, significantly improving computational efficiency.====The rest of this article is organized as follows. Section ==== presents some important preliminary results used in the construction of our likelihood function. Section ==== details our proposed stochastic rotation-based approach to multivariate deconvolution, including likelihood construction, prior specification, and outline of posterior computation. Section ==== presents the results of some simulation experiments, illustrating the method's empirical performances. Section ==== presents the results produced by the proposed method applied to the problem of estimating the true long-term average intakes of different dietary components from their measurement error-contaminated 24-hour recalls. Section ==== contains concluding remarks. Substantive additional details are presented in the supplementary materials.====The following is the Supplementary material related to this article.",Bayesian semiparametric multivariate density deconvolution via stochastic rotation of replicates,https://www.sciencedirect.com/science/article/pii/S0167947323000178,1 February 2023,2023,Research Article,40.0
Khan Ruhul Ali,"Department of Mathematics, University of Arizona, Tucson, AZ 85721, USA","Received 18 April 2022, Revised 22 January 2023, Accepted 28 January 2023, Available online 1 February 2023, Version of Record 9 February 2023.",https://doi.org/10.1016/j.csda.2023.107708,Cited by (0),"In the last few decades, several works have been undertaken in the context of proportional reversed hazard rates (PRHR) but any specific statistical methodology for the PRHR hypothesis is absent in the literature. This paper proposes a two-sample "," with associated distribution function ====, the reversed hazard rate (RHR) is defined as==== exists then ==== can be written as ==== for ====. The concept of RHR was first introduced by ==== and was discussed briefly by ====. Later, ==== called it ‘dual failure function’ but the term ‘reversed hazard rate’ first appeared explicitly in ====). The RHR has also been called retro-hazard in ====. ==== derived some interesting results which compared order statistics in the RHR orders. ==== pointed out that the hazard rate and RHR may look quite similar but they are very different by some interesting illustrative examples. For application in medical studies and reliability analysis of RHR, one may refer to ====, ==== and ====.====Let ==== and ==== be two nonnegative random variables having distribution functions ==== and ==== respectively. Then the PRHR model is defined by the following relationship between distribution functions==== where ==== is called the baseline distribution function and ====. If the probability density function ==== exists, ==== and ==== are the reversed hazard rate functions corresponding to ==== and ====, respectively. ==== denoted ==== as a resilience parameter family with underlying distribution ==== and ==== as the resilience parameter. Note that the relationship given by ==== corresponds to the PRHR assumption since ====. ====. ==== and ==== investigated the PRHR model and its applications. ==== pointed out that PRHR is applicable where the PHM is inappropriate. ==== developed an estimation methodology for the confidence intervals of the family of PRHR distributions based on lower record values. A proportional cause-specific reversed hazards model was introduced by ==== and ====. ==== proposed a new statistical method for one-way classification which is based on the RHR function of the response variable and ==== studied the relationship between RHR and mean inactive time function. Recently ====), Topp-Leone distribution (====), generalized Gompertz distribution (====) and exponential-type distribution (====). Ever since the inception of the RHR, due to its diverse applications, many researchers have contributed to it and a vast number of publications on this topic have appeared. Here, we have tried to mention some significant works regarding RHR with a brief literature survey.====Let ==== and ==== be the cumulative reversed hazard functions corresponding to ==== and ==== respectively. Thus the relation given in ==== is equivalent to ==== and to ====. Suppose ==== for all ====. The ratio of reversed hazard rates, ==== for ==== (both the failures occur just before the time point ====, given that both units have not survived longer than time ====). Thus PRHR means ==== and ==== age equally fast and the ratio of reversed hazard rates is an increasing function for ==== means ==== ages faster than ====.====Motivated by the seminal work of ====, our primary goal in this paper is to construct a test procedure for testing==== where ==== is an unknown constant. Similarly one can also consider the alternative as ==== is a decreasing function of ====. To the best of the present author's knowledge, there does not exist any statistical methodology for testing the proportional reversed hazard assumption. This seems to be the first work in this direction. Thus, there is further scope of developing a good statistical test procedure where the alternative hypothesis involves nonmonotonic behaviour of ====.====The rest of the paper is arranged as follows. Section ==== deals with the formulation of the test procedure. There are three subsections in Section ====. In Subsection ====. In Subsection ====, we extend the result of Subsection ==== based on the adjusted jackknife empirical likelihood method. In Section ====. The first data in the context of a brain injury-related biomarker accepts the null hypothesis of proportionality in the reversed hazard rate. The second data related to Duchenne muscular dystrophy rejects the null hypothesis in favour of the alternative. Both the findings are also in accordance with a visual method through the log-log plot for the proportional reversed hazard assumption. Finally, Section ==== contains some concluding remarks and a discussion about possible avenues of future work.",Two-sample nonparametric test for proportional reversed hazards,https://www.sciencedirect.com/science/article/pii/S0167947323000191,1 February 2023,2023,Research Article,41.0
"Borgonovo Emanuele,Ghidini Valentina,Hahn Roman,Plischke Elmar","Department of Decision Sciences, Bocconi University, Milan, 20136, Italy,Faculty of Energy Research and Economics, Clausthal University of Technology, Clausthal-Zellerfeld, 38678, Germany","Received 28 November 2021, Revised 13 January 2023, Accepted 13 January 2023, Available online 1 February 2023, Version of Record 14 February 2023.",https://doi.org/10.1016/j.csda.2023.107701,Cited by (0),A new class of probabilistic ==== that quantifies the degree of association between ,"; ====; ====).====Determining feature importance is essential for model simplification, dimensionality reduction, and for understanding whether predictions are at risk of unfair discrimination (====). As underlined in ====, there is a variety of techniques for calculating feature importance. Methods such as the Local Interpretable Model-agnostic Explanations (LIME) (====), and the SHapley Additive exPlanations (SHAP) (====; ====), knock-offs (====; ====) and Sure Independence Screening (====) provide an indication of importance at the dataset level.====Measures of statistical association are an alternative way for assessing feature importance: they provide dataset scores and are model agnostic (====). While they have been widely studied beginning with works such as ====, ==== and ====, the size and complexity of modern datasets have generated new interest in their construction, as the recent works of ====, ==== and ==== highlight. We note that several measures of statistical association rely on the assumption that the target is a real number (or vector). However, in certain ML applications, targets and/or features may be images or objects, for which a mathematical order relation may not be appropriate. To illustrate, consider an image recognition task in which the target consists of three different types of pictures representing, say, cats, tigers and rabbits. Then the alphabetical ordering “cat”, “rabbit”, “tiger” is a possible ordering, but it is as valid as the ordering “cat”, “tiger”, “rabbit” that is based on the number of the characters in each name, and none of them qualifies as a natural ranking. The issue is underlined in recent works (====; ====) and permutation (====). The rationale of this comparison is to test the agreement between results provided by measures of statistical association with one representative of variable importance measures that are post-hoc and model-dependent (Split and Count) and one representative of measures that are post-hoc but model agnostic (permutation feature importance). Measures of statistical dependence produce additional and complementary insights with respect to alternatives currently in use, with the advantage of being model agnostic and computationally convenient.====The remainder of the work is organized as follows: Section ==== sets up the relevant framework and reviews the literature. Section ==== introduces the new dependence measures for classification. Section ==== presents the Xi-method. Section ==== illustrates results for alternative datasets. Section ==== offers conclusions.",Explaining classifiers with measures of statistical association,https://www.sciencedirect.com/science/article/pii/S0167947323000129,1 February 2023,2023,Research Article,42.0
"Cui Junfeng,Wang Guanghui,Zou Changliang,Wang Zhaojun","School of Statistics and Data Science, LPMC, KLMDASR and LEBPS, Nankai University, China,School of Statistics, Academy of Statistics and Interdisciplinary Sciences, and KLATASDS-MOE, East China Normal University, China","Received 6 April 2022, Revised 5 January 2023, Accepted 25 January 2023, Available online 1 February 2023, Version of Record 8 February 2023.",https://doi.org/10.1016/j.csda.2023.107705,Cited by (0),"Large parallel data sets—consisting of paired measurements of responses and covariates—collected over time from numerous sources are ubiquitous. It is of great interest to identify the data sources where the underlying regression relationship of each data set has shifted. To be specific, the ==== is changed to another one at some ====. Under mild conditions, the asymptotic validity for both the false discovery proportion and FDR control is established. Extensive numerical results further confirm the effectiveness and robustness of the proposed method.","In the big data era, large parallel data streams collected over time from numerous sources are ubiquitous in the areas of economics, genetics, engineering, and so forth. For instance, a security system may monitor many different data streams from different information sources (====). As another example, a large number of securities are available in the factor pricing problem (====). In such large-scale or parallel situations, the underlying distribution of each data stream may not stay the same during the time-course of data collection. Ignoring the structural changes would affect a sequence of statistical analyses such as data integration, clustering and subgroup identification, and result in biased or misleading inference and decisions. Hence it is of great necessity to identify the data streams that have undergone changes.==== data streams ==== are collected from the following linear models==== where ===='s are ====-dimensional covariates, ===='s are scalar responses, ==== are independent and identically distributed (IID) random errors with mean ====. To detect shifts in the regression coefficients ==== for ==== and measure the ==== in the detection, we introduce the following multiple testing problem==== for ====, where ===='s are referred to as (potential) change-points that may be different across all data streams. Let ==== be the set of indices corresponding to non-null patterns, namely, the ===='s for which ==== holds. Denote ====. Any recovery procedure that yields an estimate of ====, say ==== is preferred while guaranteeing the expected number (or rate) of the false positives at a prescribed level. In this paper, we will investigate how to identify ==== while maintaining the FDR being controlled.====A natural choice is to use the Benjamini-Hochberg (BH) method (====), which depends on the p-values of all individual tests for ==== for ====, ==== developed a test based on the cumulative sums (CUSUM) of recursive residuals, ==== proposed some tests based on the maxima of the weighted CUSUM processes of residuals, and ====; ==== considered ====-type tests. For more references we refer to ==== and ====, ====, ====, ====, ====, ==== and ====.====A related problem is high-dimensional change-point detection, which has received much attention recently, see, for example, ====, ====, ====, ====, ====, ====, ====, ====, ==== and ====. These works focus on testing whether there exists a change-point in high-dimensional mean vectors, and/or estimating multiple change-points (presumed the existence). They do not aim to find the components of the mean vector that occur changes; the only exception is ==== which provides consistent estimation of the components corresponding to mean changes. However, consistent estimation may need stringent conditions for the change magnitudes, and thus may be conservative for practical applications. Besides, ==== only focus on the mean vectors. There are also a few works on the estimation of change-points in high-dimensional linear regression, see, for example, ====; the story is obviously different compared to the current study. Besides, there are many works on change-point detection in high-dimensional factor models with some common change-points in factor loadings, see, for example, ====, ====, ==== and ====. This problem shares some similarity to our model. However, the goals are quite different as we are interested in identifying which series have undergone changes.====In this paper, inspired by the symmetrized data aggregation (SDA) filter introduced by ====, we propose a new multiple testing procedure for ====, called “Residual-Aggregated Testing” (RAT). First, for each data stream, we split it into two parts by using an order-preserving splitting strategy (====th data stream are constructed to quantify the evidence against ==== for ====. Next, each pair of the statistics will be aggregated—exploiting the correlation information among all data streams—to form a sequence of test statistics, which is further used to approximate the number of false discoveries. At last, we choose a data-driven threshold which controls the false discovery proportion (FDP) and FDR at some prescribed FDR level ====The rest of this paper is organized as follows. In Section ====, we describe in detail the RAT method and give some intuitions on how it works. In Section ====. Some discussions are provided in Section ====. All technical details are deferred to the Appendix.==== For a scalar ====, ==== denotes the maximal integer less than or equal to ====. For two scalars ====, ====, let ==== denote the larger of the two. For a vector ====, let ====, ====. Similarly, for a matrix ====, let ====, ====. Let ====, ==== for a matrix ====, let ==== formed by columns with indices in ====. For a matrix ====, let ==== be the ==== norm, ====. Let ==== and ====, respectively. For matrices ====, ====, let ==== denote the Hadamard product of ==== and ====. The cardinality of a set ==== is denoted by ====.====Appendix ==== includes the detailed proofs of ====, ====, ==== and some extra necessary lemmas.====The first lemma is the standard Bernstein's inequality. ====The second one is a classical bound for the sample covariance matrix. See ====. ====The following one establishes uniform bounds for ====. ====The next lemma gives some convergence results on the design matrix. ====The next one establishes the uniform consistency of variance estimation. ====Let ==== be as defined in the proof of ====. The lemma below establishes uniform bounds for ==== and ====. ====Let ====, ==== be as defined in the proof of ====. The following lemma establishes uniform bounds for ==== and ====. ====Let ==== be as defined in the proof of ====. The next lemma bounds the difference between ==== and ====.====We are now ready to give the proofs of ====, ==== and ====.",Change-point testing for parallel data sets with FDR control,https://www.sciencedirect.com/science/article/pii/S0167947323000166,1 February 2023,2023,Research Article,43.0
"Song Jun,Kim Kyongwon,Yoo Jae Keun","Department of Statistics, Korea University, Seoul, Republic of Korea,Department of Statistics, Ewha Womans University, Seoul, Republic of Korea","Received 3 August 2022, Revised 30 November 2022, Accepted 25 January 2023, Available online 31 January 2023, Version of Record 8 February 2023.",https://doi.org/10.1016/j.csda.2023.107707,Cited by (0)," ====. The kernel principal fitted component is a nonlinear extension of the principal fitted component model, and it is found in the theory of mapping low dimensional input space to the higher "," by projecting it onto a subspace while preserving the dependence of ==== on ==== of ==== such that ====, where ====. If the intersection of all of the dimension reduction subspaces exists, it is the smallest dimension reduction subspace by its nature. This subspace is called the central subspace and is denoted by ====. There has been a number of methods to extract a dimension reduction subspace such as Sliced Inverse Regression (====), Sliced Averaged Variance Estimation (====), Principal Hessian Direction (====), Contour Regression (====), and Directional Regression (====).====), kernel canonical correlation analysis (KCCA) (====), kernel principal component analysis (====). Recently, ==== and ==== generalize this framework to nonlinear SDR, where the goal is to recover ==== where ==== is an RKHS of ==== such that==== Note that ==== is not identifiable. Thus ==== generalize the problem by defining the possible target, a sub ====-field generated by ==== and this is called an SDR sub ====-field. Finally, the intersection of all SDR ====-field is again an SDR sub ====-fields and this is called the central ====-field and it is denoted as ====. Since the ====-field is difficult to estimate directly, we can detour it by estimating the class of functions that are ====-measurable, which is called the central class and denoted as ====. This central class can be considered as a generalized version of the central subspace and it is the goal of nonlinear SDR. Nonlinear SDR greatly expands the scope, flexibility, and effectiveness of SDR as ====, and introduced new estimators for this class. For more details of nonlinear SDR, see ====. On a different front, the theory of SDR has been developed under the model-based approach. In particular, the principal fitted component (PFC) in ====, ====. It can outperform previously mentioned model-free SDR methods.==== under mild condition. As a result, we open the door for the nonlinear extension of other model-based SDR techniques, and one can create numerous new methods that outperform currently existing nonlinear model-free SDR methods. We demonstrate the performance of our new method using simulation results and ovarian cancer data.====The rest of the paper is organized as follows. Section ==== introduces linear version of the PFC model. The basic formulation of an RKHS is described in Section ====. Section ==== extends the PFC model to their nonlinear versions using the kernel trick with an RKHS. Section ==== introduces the coordinate representation method to express infinite dimensional population level operators to the finite dimensional sample versions and provides a method to select tuning parameters. Numerical experiments including simulation studies and the real-world data application are presented in Sections ====, and ====, respectively. We finalize this paper with some discussions in Section ====.",On a nonlinear extension of the principal fitted component model,https://www.sciencedirect.com/science/article/pii/S016794732300018X,31 January 2023,2023,Research Article,44.0
"Wang Kai Y.K.,Chen Cathy W.S.,So Mike K.P.","Department of Statistics, Feng Chia University, Taiwan,Department of Information Systems, Business Statistics and Operations Management, The Hong Kong University of Science and Technology, Hong Kong","Received 1 April 2022, Revised 8 January 2023, Accepted 15 January 2023, Available online 27 January 2023, Version of Record 3 February 2023.",https://doi.org/10.1016/j.csda.2023.107702,Cited by (0),"The Fama-French three-factor model advances the ==== performance via simulation studies in which the designated models are misspecified and considers some daily stock returns from NASDAQ to help further select the best model via the posterior odds ratio. It is clear that the various market conditions and the GARCH effect should be incorporated into the model. Findings show that the estimation of the size factor turns insignificant for lower quantiles - i.e., when the market is in a panic, investors ignore the size effect of a company's assets.","There is considerable evidence in the literature suggesting that the relationship between market excess return and asset excess return varies under different conditions (====, ====). There is also evidence that the market factor (excess return on a market portfolio) does not explain the asset return sufficiently. ==== advance three risk factors, market, size, and value, to explain average asset returns. ==== later propose a five-factor asset pricing model capturing market, size, value, profitability, and investment patterns in average stock returns. Many studies in the literature use monthly data that allow one to consider other factors; for example, ==== and ====. For real data analysis, we prefer to employ daily data rather than monthly data, as the higher the number is for independently measured observations, the higher is the flexibility, which implies more precise estimators. Therefore, we focus on the three factors since daily observations are available.====Size factor, also called the book-to-market factor, is the difference between the return on a portfolio of small-capitalization stocks and the return on a portfolio of large-capitalization stocks. ==== test the zero-intercept in the null hypothesis; if the intercepts are close to zero, then regressions (that use the three factors to absorb common time-series variation in returns) aptly explain the cross-section of average stock returns. However, regardless of explosive, dormant, and rising markets, the same parameters of the Fama-French three-factor model are applied for various market conditions.====The quantile regression models by ==== and ==== have gained significant prominence in many fields, especially in economics and finance. In that context, the interest is not only on average returns, but also on volatility; i.e., low extreme or high extreme. Their papers offer a semi-parametric framework, incorporating the asymmetric Laplace distribution (ALD) for the likelihood construction. In this set-up the constant variance would be unrealistic in finance.====Some highly recognized stylized features relating to financial time series are leptokurtic, volatility clustering, and asymmetry in conditional mean and variance. The generalized autoregressive conditional heteroscedastic (GARCH) family of models gives popular choices to capture these features of financial time series (====; ====). ==== include GARCH-type dynamics into the standard quantile regression model to examine quantile causal correlations between dynamic variables. Subsequently, several studies develop various quantile models with GARCH specification and ALD, such as ====, ====, and ====. The aim of the present study is to propose a quantile three-factor model with GARCH-type dynamics and asymmetric error distribution and to improve both the conventional Fama-French three-factor model with the assumptions of normality and homoskedasticity and the model by ====.====Empirical evidence in the literature shows that the distribution of financial data usually exhibits a fat tail with volatility skew. Several contemporary studies utilize asymmetric distribution to describe asset investment returns or stock options. The asymmetric Student t (AST) distribution is perhaps the most common one to handle these features in econometric models, while many studies in the literature employ ALD for quantile regression (see ====; ====; ==== to estimate the parameters of the proposed quantile model.====, includes two parameters where ==== is a skewness parameter related to a specified quantile, and ====; ====; ====; ====; ====We compare the proposed model with ALD and AST assumptions using the posterior odds ratio (POR) and deviance information criterion (DIC) of ====. The result of model selection shows that the proposed quantile model with GARCH and AST errors outperforms a quantile model with GARCH and ALD in terms of POR and DIC. By including the additional market conditions, GARCH effect, skewness, and leptokurtosis, the proposed model offers a better tool for evaluating manager performance.====The rest of this paper runs as follows. Section ==== introduces the quantile three-factor model with heteroskedasticity and AST errors. Section ==== presents MCMC methods to parameter estimation and performs model selection for the specified models. Section ==== provides a simulation study. Specifically, we examine for any misspecification of the distribution. Section ==== demonstrates an empirical example for stock returns from NASDAQ. Section ==== offers some concluding remarks.","Quantile three-factor model with heteroskedasticity, skewness, and leptokurtosis",https://www.sciencedirect.com/science/article/pii/S0167947323000130,27 January 2023,2023,Research Article,45.0
"Soave David,Lawless Jerald F.","Department of Mathematics, Wilfrid Laurier University, Waterloo, Ontario, N2L 3C5, Canada,Computational Biology Program Ontario Institute for Cancer Research, Toronto, Ontario, M5G 0A3, Canada,Department of Statistics and Actuarial Science, University of Waterloo, Waterloo, Ontario, N2L 3G1, Canada","Received 20 July 2021, Revised 31 August 2022, Accepted 18 January 2023, Available online 24 January 2023, Version of Record 6 February 2023.",https://doi.org/10.1016/j.csda.2023.107703,Cited by (0),". A detailed simulation study to examine the performance of the proposed methodology is described. The variable selection and estimation procedure is then used to obtain a model for predicting acute myeloid leukaemia using somatic stem cell mutation profiles derived from blood samples, based on a two-phase sample from the European Prospective Investigation into Cancer and Nutrition (EPIC) study.","The scientific research community is increasingly studying disease through the synthesis of data from large, prospective cohorts and sub-studies focused on specific areas. For example, the European Prospective Investigation into Cancer and Nutrition (EPIC) (====Two-phase study designs are ideal for focused sub-studies based on large prospective cohorts. They allow certain variables, say outcomes and covariates ====, that have been measured for all individuals in the large cohort (phase 1), to be augmented with additional variables (====) that are measured for a subset of these individuals (phase 2). The motivation for these studies is often that either the outcome of interest is an event that is rare in the full cohort, or that the additional covariates ==== may be expensive or difficult to measure. By allowing the selection of individuals for inclusion in the phase 2 sample to depend on the available information ====, the power and efficiency of analyses can be increased (====). We focus here on studies where the outcome of interest ==== is a time-to-event variable (which we term a failure time for convenience) and the objective is investigation of the relationship between ==== and covariates ====; ====) are widely used with two-phase studies involving failure times; these are often referred to as case-cohort studies.====Researchers in genomics often wish to examine large numbers of covariates for association with outcomes of interest. In the context of cancer, hundreds to millions of genetic markers may be considered, along with environmental exposures. The high cost of extracting genetic information from very large numbers of individuals in the cohort motivates the use of two-phase study designs, sometimes resulting in more covariates than observational units for analysis. Variable selection methods such as regularized regression have been proposed in such situations, including for the Cox model (====To date, little work has been done on penalized regression in the setting of two-phase study data. ==== add a penalty to a weighted pseudo-partial likelihood to estimate regression coefficients in the Cox model. To find the penalized estimator, the approach uses a modified Newton-Raphson algorithm starting from the full model with all covariates included, which makes it unsuitable when the number of covariates is larger than the sample size (==== can increase the efficiency of estimation for covariates ==== in some situations, for example, when a ==== variable is a surrogate for an ==== variable. ==== and ==== discuss such settings. In the EPIC study discussed in Section ====, stratification on age and sex was used for the phase 2 sample of controls (non-cancer cases). This allowed the phase 2 controls to have the same age, sex distribution as the cases. Weights used in estimation must in this case reflect the dependence on covariates; discrepancies between sampling weights and analysis weights can lead to biased effect estimates and underestimated standard errors ====.====). The methods are implemented in an R software package and are freely available for download on GitHub (====).====The remainder of the paper is organized as follows. In section ==== we describe penalized regression for the Cox pseudo partial likelihood and detail the PCCD algorithm for various penalty functions. We discuss weight-adjusted variance estimates for post selection inference and describe tuning parameter selection methods. In section ====, we describe a detailed simulation study to examine the performance of the proposed methodology and compare it with other approaches. In section ====, we use the variable selection and estimation procedure to obtain a model for identifying genomic factors associated with the development of acute myeloid leukaemia (AML) using somatic stem cell mutation profiles derived from blood samples, based on a case-cohort sample from the EPIC study. Section ==== makes some concluding remarks.====The following is the Supplementary material related to this article.",Regularized regression for two phase failure time studies,https://www.sciencedirect.com/science/article/pii/S0167947323000142,24 January 2023,2023,Research Article,46.0
"Choi Semin,Kim Yesool,Park Gunwoong","School of Cross-disciplinary Studies, University of Seoul, 02504, Seoul, Republic of Korea,Department of Statistics, University of Seoul, 02504, Seoul, Republic of Korea,Department of Statistics, Seoul National University, 08826, Seoul, Republic of Korea","Received 28 October 2021, Revised 13 December 2022, Accepted 3 January 2023, Available online 16 January 2023, Version of Record 19 January 2023.",https://doi.org/10.1016/j.csda.2023.107691,Cited by (0),"-regularized regression and (ii) the presence of edge estimation using ====-regularized regression. Hence, the proposed algorithm can recover a large degree graph with a small indegree constraint. Also proven is that the sample size ==== is sufficient for the proposed algorithm to recover a sub-Gaussian linear SEM provided that ====, where ==== is the number of nodes and ====. Therefore, the proposed algorithm is statistically consistent and computationally feasible for learning a densely connected sub-Gaussian linear SEM with large maximum degree. Numerical experiments verified that the proposed algorithm is consistent, and performs better than the state-of-the-art high-dimensional linear SEM learning HGSM, LISTEN, and TD algorithms in both sparse and dense graph settings. Also demonstrated through real data is that the proposed algorithm is well-suited to estimating the Seoul public bike usage patterns in 2019.","; ====; ====; ====; ====).====Nevertheless, graphical model learning from the observational distribution remains an open challenge because the unknown (causal or topological) ordering of a graph causes non-identifiability. Moreover, it is computationally expensive because the number of possible models grows double-exponentially in the number of nodes. Therefore, several studies have focused on either inferring (i) a directed graph under the known ordering condition (e.g., ====; ====; ====; ====) or (ii) a partially directed graph without the known ordering condition (e.g., ====; ====; ====; ====; ====).====; ====; ====; ====; ====; ====), quadratic variance function DAG models (====, ====; ====), and linear structural equation models (SEMs) with restrictions on the edge weights and error variances (====; ====; ====; ====; ====).====). Various algorithms have been proposed based on this idea of the uncertainty level. For example, ==== develops a likelihood-based algorithm under the same error variance in a low-dimensional setting. ==== provides a graphical lasso-based approach for high-dimensional sub-Gaussian linear SEMs, and it establishes the consistency of its algorithm with sample complexity ====, where ==== is the maximum degree of the moralized graph and ==== is the number of nodes in the graph. ====, ==== develop constrained ==== when the error variances are homogeneous and heterogeneous, respectively. ==== develops a best subsets-based algorithm, under the same error variance assumption, with sample complexity ====, where ==== is the predetermined upper bound of the indegree. Lastly, ====A linear SEM is one of the popular methods to find the data generating procedure. In principle, it is plausible that the data generating model is sparse where a few variables are involved when generating a variable. Hence, it is natural to put a constraint on the maximum ====, that is a sparse inverse covariance matrix. Additionally, the small maximum degree condition can be very strong even if a graph has a small indegree (see details in Sections ==== and ====). Hence, it is important to develop a learning algorithm that does not depend on the maximum degree, but the indegree.====The main goal of this paper is to investigate if high-dimensional consistency results could be established when learning an identifiable sub-Gaussian linear SEM without any constraint on the maximum degree. Hence, this paper develops a new learning approach that estimates the ordering using ====-regularized regression, and then determines the presence of edges using ====-regularized regression. As discussed, there are many consistent edge learning approaches given the ordering. Hence, this paper concentrates mainly on the consistency of the ordering estimation, when ==== scales as ====, even with a large maximum degree. It proves that the proposed algorithm can recover the true graph with sample complexity ==== when indegree ====The remainder of this paper is structured as follows. Section ==== explains sub-Gaussian linear SEMs. Section ==== introduces a new algorithm for learning densely connected sub-Gaussian linear SEMs. Furthermore, Section ==== and ==== explains the numerical experiments and evaluates the proposed algorithm against the state-of-the-art high-dimensional DAG learning HGSM (====), LISTEN (====), and TD algorithms (====). Section ==== compares the performance of the proposed algorithm in estimating the Seoul public bike usage pattern in 2019 with the comparison algorithms. Lastly, Section ==== offers a discussion, and suggests future studies.====Let ==== be a sum of all paths from ==== to ====; i.e.,==== where ==== are all elements of ordering between ==== and ====. Here, the length of a directed path is defined by the number of sequential edges.====Then, the conditional distribution of ==== given ==== is expressed as follows:==== where ====. Moreover, given the observations ====, the conditional model is expressed in the matrix form==== where ====.====Then, the conditional variance estimator ==== is as follows:====Applying simple algebra, it can be decomposed into the following:====It is noted that ==== and ====. Moreover, since==== where ====, it holds that====Hence, it completes the proof:",Densely connected sub-Gaussian linear structural equation model learning via ,https://www.sciencedirect.com/science/article/pii/S0167947323000026,16 January 2023,2023,Research Article,47.0
"Pini Alessia,Sørensen Helle,Tolver Anders,Vantini Simone","Department of Statistical Sciences, Università Cattolica del Sacro Cuore, Largo Gemelli, 1, 20123 Milan, Italy,Department of Mathematical Sciences, University of Copenhagen, Copenhagen, Denmark,Department of Mathematics, Politecnico di Milano, Milan, Italy","Received 7 April 2022, Revised 30 December 2022, Accepted 30 December 2022, Available online 11 January 2023, Version of Record 24 January 2023.",https://doi.org/10.1016/j.csda.2022.107688,Cited by (1),-value function on the same domain as the data. Such adjusted ====-value functions can be thresholded at level ,"; ====; ====, ====). Research in FDA has been rapidly growing the last decades due to its many different applications, and there now exist functional versions of more or less any statistical discipline: functional (generalized) linear models, classification with functional data, functional principal component analysis, etc.====In the framework of inference, a particular question arises with functional data from controlled experiments: “In which regions, if any, of the time domain are data affected by experimental conditions?” Local inferential procedures, i.e., procedures that compute local ====; ====). This is an objective method and furthermore gives the opportunity to study local features, i.e., features affecting only certain parts of the gait cycle. Our data consist of three-dimensional acceleration signals from 12 horses with lameness artificially induced on each limb at a time. Practitioners would like to know how the intervention affects different parts of the gait cycle and different directions of acceleration. For example, they expect that acceleration is mainly affected during stance on the affected limb, and that certain symmetry patterns are present.==== in the data and tackle multiple testing problems arising from measurements being taken over time and in several dimensions.====Valid procedures for constructing hypothesis tests are not straightforward for functional data. The solution typically adopted in the literature has been to perform global tests (====; ====; ====; ====; ====; ====; ====; ====. The authors still propose a global test, but local information can be assessed by graphically exploring a global envelope, drawn jointly for all samples of functions. However, also this paper assumes independent data and cannot be directly applied in our setting.====In contrast, we are concerned with local inference for dependent curves. In general, local inference can be approached in two different ways: simultaneous confidence bands, and tests procedures. In this paper we focus on the latter approach, while a procedure to construct confidence bands for dependent functional data is described in ====.====There has been some recent advances regarding local testing techniques for functional data. ====, ==== discretized the functional data using an a priori selected partition of the domain. A drawback with this type of procedures is that the test conclusions might depend on the functional basis or on the partition selected to initialize the methods. To overcome this problem a technique called interval-wise testing (IWT) was developed by ==== for testing differences between two populations, and later extended to function-on-scalar linear models (====) and to including derivatives (====). IWT defines an adjusted ====-value function, assigning an adjusted ====-value to each point of the domain of functional data. The adjusted ====-value function can be thresholded at level ==== for selecting the parts of the domain responsible for the rejection of the null hypothesis. The adjustment corrects for multiple testing over the time domain and yields control over the so-called interval-wise error rate (====). The above-mentioned papers assume independence among the observed curves, and computation of ====) to adjust for multi-dimensionality.====The paper is structured as follows: Section ==== describes the inferential procedure for local testing and its theoretical properties, and Section ==== reports the results of the method applied to the horse acceleration data. Section ==== include two simulation studies, one of them mimicking some aspects of the horse data, and Section ==== draws some conclusions. We provide an online supplementary material that includes the horse acceleration dataset, the ==== code implementing all proposed methods and the code reproducing simulations.====The following is the Supplementary material related to this article.",Local inference for functional linear mixed models,https://www.sciencedirect.com/science/article/pii/S0167947322002687,11 January 2023,2023,Research Article,48.0
"Van Niekerk Janet,Krainski Elias,Rustand Denis,Rue Håvard","Statistics Program, Computer, Electrical and Mathematical Sciences and Engineering Division, King Abdullah University of Science and Technology (KAUST), Thuwal 23955-6900, Saudi Arabia","Received 13 April 2022, Revised 24 November 2022, Accepted 4 January 2023, Available online 10 January 2023, Version of Record 27 January 2023.",https://doi.org/10.1016/j.csda.2023.107692,Cited by (4),". The increased computational efficiency and accuracy when compared with sampling-based methods for Bayesian inference like MCMC methods, are some contributors to its success. Ongoing research in the INLA methodology and implementation thereof in the ==== package ====, ensures continued relevance for practitioners and improved performance and applicability of INLA. The era of big data and some recent research developments, presents an opportunity to reformulate some aspects of the classic INLA formulation, to achieve even faster inference, improved numerical stability and scalability. The improvement is especially noticeable for data-rich models.====Various examples of data-rich models, like Cox's "," package ==== (R-INLA).====More than 250 studies on COVID-19 used INLA to perform Bayesian inference, see among others ====; ====; ====; ====; ====; ====; ====. Public health studies on HIV and associated risk factors were performed using INLA by ====; ====; ====; ====; ==== while models for water and soil pollution were considered by ====. In ecology, ====; ====; ====; ====; ====; ====. These are merely a selected few of the recent studies that entails Bayesian inference completed with INLA, and from this the impact of INLA on the applied sciences is evident.====The classic formulation of the INLA methodology as presented by ====, with ==== observed pairs ====, then the classic INLA formulation will represent this as a graph of size ====, (==== and ==== together with the ==== linear predictors), whereas the modern formulation will use a graph of size 2 to represent ==== and ====, as the linear predictors are no longer an explicit part of the latent field.====In many cases, the augmentation of the latent field by the linear predictors does not increase the computational cost of the inference by much, but for low-dimensional models with high-dimensional data, the nested Laplace approximation (classic INLA) gets more computationally expensive, mainly due to this augmentation. Without the augmentation, the posteriors of the linear predictors should be calculated post-inference of the latent field. In the case of the posterior ====Recent developments by ==== enables us to define a new framework for approximate Bayesian inference using INLA without the nested component, by using a Variational Bayes correction to the Gaussian approximation. Based on this proposition, we can perform inference for the model where the linear predictors are not included in the latent field, also defined as an extended latent Gaussian model by ====. The low-rank Variational Bayes correction to the posterior means of a Gaussian latent field proposed by ====, after some preliminaries about the classic formulation and other details of INLA in Section ====. This new approach is then illustrated with simulated and real examples in Sections ==== and ====. We present a discussion of the work and possible impact in Section ====.====The code for the examples and application is available at ====.",A new avenue for Bayesian inference with INLA,https://www.sciencedirect.com/science/article/pii/S0167947323000038,10 January 2023,2023,Research Article,49.0
"Febrero-Bande Manuel,González-Manteiga Wenceslao,Prallon Brenda,Saporito Yuri F.","Departamento de Estadística, Análisis Matemático y Optimización, Universidade de Santiago de Compostela, Spain,Department of Economics, Cornell University, United States,Escola de Matemática Aplicada, Fundação Getulio Vargas, Brazil","Received 28 June 2021, Revised 26 December 2022, Accepted 27 December 2022, Available online 10 January 2023, Version of Record 16 January 2023.",https://doi.org/10.1016/j.csda.2022.107687,Cited by (1),"A ==== for predicting the main activity of ==== rather than curve behavior. The proposed approach, on the other hand, does not require any network information for prediction. Furthermore, functional features have the advantage of being straightforward to build, unlike expert-built features. Results show improvement when combining functional features with scalar features, and similar accuracy for the models using those features separately, which points to the functional model being a good alternative when domain-specific knowledge is not available."," – which follows from anonymity certainly being an attractive for law-breakers; for instance, the Silk Road darknet market, closed by the FBI in 2013 and used mainly for commercializing illegal drugs, moved approximately fifteen million dollars annually in transactions, ====. Thus, identifying the main activity of a bitcoin address is a relevant issue, since it both aids law enforcement and sheds some light on the bitcoin market organization.",Functional classification of bitcoin addresses,https://www.sciencedirect.com/science/article/pii/S0167947322002675,10 January 2023,2023,Research Article,50.0
"Sui Yuelei,Holan Scott H.,Yang Wen-Hsi","Amazon.com, Inc., 1440 Broadway, New York, NY 10018, USA,Department of Statistics, University of Missouri, MO, USA,Office of the Associate Director for Research and Methodology, U.S. Census Bureau, Washington, DC, USA,School of Agriculture and Food Sciences, The University of Queensland, St Lucia, Queensland 4072, Australia,School of Mathematics and Physics, The University of Queensland, St Lucia, Queensland 4072, Australia","Received 24 June 2022, Revised 28 December 2022, Accepted 1 January 2023, Available online 5 January 2023, Version of Record 24 January 2023.",https://doi.org/10.1016/j.csda.2023.107690,Cited by (0),"Nonstationary ====. To illustrate the effectiveness of the proposed approach, a comprehensive comparison with other competing methods is conducted through simulation studies and finds that, in most cases, the proposed approach performs superior in terms of the average squared error between the estimated and true time-varying ====. Finally, the methodology is demonstrated through applications to quarterly Gross Domestic Product (GDP) data and Northern California wind data.","; ====; ====; ====). Another example in which these models are useful is multi-channel electroencephalography (EEG) data, where the data are analyzed through their time-frequency representation to reveal how the neuronal activity in one area of the human brain may influence another (====; ====; ====). In the era of big data, the number of series and series length within a multivariate time series have been increasing due to technological advances. Consequently, there has also been a fundamental increase in data complexity. Thus, developing efficient methods that scale to such an enormous amount of time series data is imperative.====; ====; ====).====Parametric models for nonstationary multivariate time series play an increasingly important role when modern techniques make high-dimensional data available for analysis. They have several advantages: 1) easy to make forecasts, 2) straightforward to build assessment of uncertainty, and 3) concise descriptions of the underlying model scheme. The TV-VAR model is arguably the most widely used model among the parametric methods. In the time domain, such models have been developed and applied to correlated economic and financial data (====; ====; ====; ====). In contrast, in the frequency domain, the spectral density and the coherence can often reveal features of each time series that are not readily apparent in the time domain. For example, frequency domain approaches to multi-channel EEG data can aid in the understanding of connective activities within the brain by estimating their spectrum and coherence (====, ====; ====; ====).====; ====; ====; ==== allow both the coefficients and the innovation variance to change over time and have been applied to multivariate economic index data (====; ====; ==== extended the Bayesian lattice filter (BLF) of ==== to TV-VAR models with a constant innovation covariance by implementing the multivariate Durbin–Levison algorithm (====To overcome the drawbacks of the high computation cost and the constant innovation covariance assumption, we propose an approach that uses a “one channel at-a-time” scheme (====. This approach allows us to estimate the AR coefficients one series at a time and, therefore, minimizes the need to calculate big matrices. In other words, by eliminating the need for computationally expensive matrix calculations, our strategy significantly improves the computational efficiency in situations where the time series is high-dimensional and/or the model order is large. ==== applied this circular lattice to the time-varying multivariate AR model using a smoothness prior (====) on the AR coefficients. This method uses a two-stage estimation approach for the coefficients and innovation covariance and only allows coefficients to be time-varying. The assumption of constant innovation covariance limits application for many types of data. Our new approach uses dynamic linear models (DLMs) to facilitate the estimation in each stage of the lattice structure, allowing both the coefficients and the innovation variances to vary with time (see also, ====). By modeling the time-varying innovation covariance, the resulting models are more broadly applicable. Additionally, we adopt the methods by ==== to address the problem of ordering uncertainty within the multivariate time series.====The remainder of the paper is organized as follows. First, we introduce Bayesian circular lattice filters in Section ==== and evaluate the method via extensive simulation studies in Section ====. In Section ====, we apply the method to two applications, modeling quarterly GDP for five countries and modeling wind speed from three stations in Northern California. Finally, Section ==== contains conclusion and discussion. Comprehensive details and algorithms for parameter estimation, model selection, and forecasting are provided in the Appendix.====We summarize the algorithm of our approach to fitting a TV-VAR model as follows:",Bayesian circular lattice filters for computationally efficient estimation of multivariate time-varying autoregressive models,https://www.sciencedirect.com/science/article/pii/S0167947323000014,5 January 2023,2023,Research Article,51.0
"Greco Luca,Pacillo Simona,Maresca Piera","University Giustino Fortunato, Benevento, Italy,University of Sannio, Benevento, Italy,Universidad Politecnica, Madrid, Spain","Received 20 March 2022, Revised 19 December 2022, Accepted 23 December 2022, Available online 3 January 2023, Version of Record 10 January 2023.",https://doi.org/10.1016/j.csda.2022.107686,Cited by (0),"Accurate circle fitting can be seriously compromised by the occurrence of even few anomalous points. Then, it is proposed to resort to a robust fitting strategy based on the idea of impartial trimming. Malicious data are supposed to be deleted, whereas estimation only relies on a set of genuine observations. The procedure is impartial in that trimmed points are not decided in advance but they are detected simultaneously to parameters estimation, according to an ====: in each step a fixed proportion of the data is trimmed after sorting their ==== and real data examples.","The accurate detection of circles is of primary interest in several fields, from metrology, for what concerns the characterization of circular profiles in micro- and nano manufacturing by digital optical devices (====; ====; ====; ====; ====; ==== for an extended list of applications and references).====Let ==== denote a set of points on the circumference with center ==== and radius ====, with ====. The literature offers many circle fitting algorithms (see ====, ====, ====, hyper accurate (====) methods. On the opposite, the geometric fit minimizes the sum of squared geometric distances of each ==== from the circle with center ==== and radius ====, that is==== where ====.====; ====; ====; ====)==== According to the model in ====, the measurements ==== are made with errors ==== around the ==== and ==== denotes the ==== is an unknown scale parameter. The parameters ==== are structural parameters, whereas the ====It is well known that maximum likelihood fails to return consistent estimates in the presence of incidental nuisance parameters (====). Several solutions have been suggested: consistent estimates for ==== have been already proposed in ==== stemming from bias adjustments, whereas a set of unbiased estimating equations for ==== has been discussed in ====; ==== developed likelihood estimation assuming a uniform distribution for the angles over ====; in ====, the author developed a consistent solution based on a bias adjustment to the MLE, leading to an adjusted MLE (AMLE); in ====; ====, the AMLE from ==== and the maximum marginal likelihood estimates of the circles's structural parameters are expected to exhibit negligible differences and perform all quite satisfactory when the ratio ==== is small enough.==== for a gentle introduction to robustness).====We aim to develop a strategy for robust circle fitting that could be used in combination with several classical fitting algorithms. In particular, the interest will be on the geometric fit but the methodology can be extended to algebraic fits, as well. The robust method is designed to provide reliable estimates in the presence of outliers but also to lead to results not significantly different from those stemming from classical methods when data are not corrupted. Furthermore, an effective tool for ==== is also developed, based on formal rules and the fitted model.====The proposed methodology is based on the idea of impartial trimming (====): the circle is fitted based on a subset of the original data points, whereas the remaining part is deleted from the estimation process. The procedure is ==== since points in the trimmed set are not known in advance, as stemming from some filtering process, but determined according to formal rules in an iterative fashion, simultaneously to parameters estimation. A heuristic robust circle fitting methodology has been proposed in ====. Their proposal is based on a modification of the Taubin method but, as claimed by the authors, any other (algebraic) fit could have been employed. However, this approach shows some weaknesses that will be illustrated in the following sections. Another robust strategy has been developed in ==== that is based on the minimization of the Absolute Geometric Error ====One anonymous referee raised one question concerning the existence of potential connections with the statistical analysis of circular data and whether the structural parameters in ==== and those of a standard model for circular/directional data (====; ====) are related. Let ==== denote the sample angles, for a fixed center. It turns out that the sample radii ====). The analysis of circular data can also deal with potential outliers, defined as anomalous angles or directions, rather than outlying coordinates' measurements (====; ====; ====). However, we notice that a point with outlying coordinates not necessarily leads to an anomalous angle and an outlying direction can correspond to a genuine measurement along the circumference. In summary, in this paper the interest lies on fitting a circle to a set of points without investigating the distribution of the data around the circle. The focus is on coordinates' measurements rather than angles.====The paper is structured as follows. An algorithm for robust fitting based on trimming is outlined in Section ====. Further estimation strategies based on impartial trimming are discussed in Section ====. An outliers detection rule is given in Section ====. Real data applications are given in Section ====. Final remarks end the paper in Section ====.====The distribution of the sample radii ==== is a chi distribution with two degrees of freedom and non centrality parameter ==== as defined in Subsection ====. The probability density function can be obtained as in ==== or in ====. We find that==== where ==== is the modified Bessel function of order zero. The following expansion holds (====)==== Furthermore, let us consider the expansion==== Then, after substitution==== The normal approximation is valid for ==== (or ==== equivalently) and ====, which means that the distribution of ==== is sufficiently concentrated around ====, that is ====.",An impartial trimming algorithm for robust circle fitting,https://www.sciencedirect.com/science/article/pii/S0167947322002663,3 January 2023,2023,Research Article,52.0
"Xiong Wei,Chen Yaxian,Ma Shuangge","School of Statistics, University of International Business and Economics, Beijing 100872, PR China,Department of Statistics and Actuarial Science, The University of Hong Kong, Hong Kong,Department of Biostatistics, Yale School of Public Health, USA","Received 30 December 2021, Revised 18 December 2022, Accepted 20 December 2022, Available online 28 December 2022, Version of Record 3 January 2023.",https://doi.org/10.1016/j.csda.2022.107684,Cited by (0), level of signals. An efficient two-stage algorithm is developed to make the proposed approach scalable to ultrahigh-dimensional data. Simulations and the analysis of TCGA LUAD data further establish the practical superiority of the proposed approach.,"For many practical high-dimensional analysis problems, interactions have been increasingly confirmed as playing critical roles beyond main effects (====; ====; ====; ====; ====). The two analysis paradigms serve different purposes, with joint analysis possibly better reflecting, for example, the biology of complex diseases (====). In this article, we focus on joint analysis. In the literature, many joint interaction analysis methods have been developed, and we refer to ====; ====), rank correlation (====), distance correlation (====), and other techniques (====; ====; ====; ====). It is noted that some of the existing techniques have relatively narrow applications. For example, the mean-variance-based sure independence screening ==== is limited to classification problems. The distance-correlation-based sure independence screening ==== requires predictors to have continuous distributions. The existing methods sometimes take quite different forms for different types of response variables, lacking uniformity – this is especially true for model-based methods.====Compared to the analysis of main effects, marginal screening for interaction analysis has been less developed. It may seem that the methods for main-effects screening can be directly applied. However, the validity of such methods often relies on certain no/weak correlation assumptions, which easily break down in interaction analysis. One solution has been brought by the unique variable selection hierarchy of interaction analysis. In particular, it has been argued both statistically and biologically that, if an interaction term is important, then one (under the weak hierarchy) or both (under the strong hierarchy) of the corresponding main effects should also be important (====). With this hierarchy, progressive screening methods have been developed, which first conduct marginal screening with main effects, and then screen for important interactions corresponding to the selected main effects (====; ====; ====). It is noted that the existing progressive methods are mostly model-based. In addition, they also demand certain correlation conditions to ensure that important main effects can be identified in the first place. To identify gene-gene and gene-environment interactions, entropy-based methods have been proposed. Examples include an index based on information gain to quantify the interaction effect between two categorical predictors and a response (====) and utilization of mutual information and mutual information gain to quantify gene-environment (====) and gene-gene ==== interactions. There are also approaches that take GINI purity gain into account to detect gene-gene interactions (====; ====). However, these entropy-based methods generally overestimate dependence (====), exponential loss (====; ====), rank-based (====; ====), and many other works. Fourth, the proposed approach can flexibly accommodate discrete, categorical, and continuous predictors, overcoming the stringent demand for continuous distributions by some methods. Lastly, statistical properties are rigorously established, providing the proposed approach a strong statistical ground and also shedding insights into coefficient of variation and entropy theory under high-dimensional settings. Overall, this study can provide a statistically well-grounded and numerically well-performed approach for alleviating computational burden and improving performance in high-dimensional interaction analysis.====The following is the Supplementary material related to this article.",Unified model-free interaction screening via CV-entropy filter,https://www.sciencedirect.com/science/article/pii/S016794732200264X,28 December 2022,2022,Research Article,53.0
"da Silva Murilo,Sriram T.N.,Ke Yuan","Department of Statistics, University of Georgia, Athens, GA 30602, USA","Received 2 May 2022, Revised 6 December 2022, Accepted 7 December 2022, Available online 14 December 2022, Version of Record 19 December 2022.",https://doi.org/10.1016/j.csda.2022.107682,Cited by (0),"Consider a time series, where the conditional mean is assumed to be an unknown function of linear combinations of past ==== observations and the conditional variance is assumed to be an unknown function of linear combinations of past ==== squared residuals. The linear combinations are assumed to contain all the necessary information about the time series that is available through the conditional mean and conditional variance, respectively. Nadaraya-Watson kernel smoother is used to estimate the unknown mean and variance function and an iterative approach is proposed to estimate the parameter matrices associated with the linear combinations. The estimators are shown to be consistent. To overcome computational challenges and provide numerical stability, a novel angular representation of parameter matrices is introduced. The numerical performance of the proposed method on forecasting the conditional mean is assessed by simulations studies. A real data of Brazilian Real (BRL)/U.S. Dollar Exchange Rate is analyzed. For the BRL/USD series, the estimated linear combinations yield a better time series model than an AR-ARCH model in terms of out-of-sample forecasts.","In time series models, the forecast of the current value, ====, based upon the past information is simply the conditional mean of ====, which depends upon the past values, ====, of a series. Traditional econometric models assume that the conditional variance of ==== stays constant at any given time point and does not depend on the past values. However, in applied economics, there are examples where the conditional variance of ==== is larger for some time points (or a range of time points) than for others, leading to so-called heteroscedasticity. In financial applications, where the dependent variable is the return on an asset or portfolio and the variance of the return represents the risk level of those returns, some time periods may be riskier than others in that the magnitude of variance of the return at some times is greater than at others. There are also instances where the risky times are not scattered randomly across data; instead, there is a degree of autocorrelation in the riskiness of financial returns.====To handle the issue of heteroscedasticity in time series, ==== proposed a new class of models called Autoregressive Conditionally Heteroscedastic (ARCH) models, where the conditional variance depends upon the past values of the series. He also illustrated the usefulness of ARCH models in economics and finance. ==== generalized the purely autoregressive ARCH model to an autoregressive-moving average model called the Generalized Autoregressive Conditional Heteroscedastic (GARCH) model. The ARCH and GARCH models are widely used for modeling heteroscedastic time series, where the goal is to provide a volatility measure that can be used in financial decisions concerning risk analysis, portfolio selection and derivative pricing; see ====.====Literature (====) proposed a general nonlinear system of the ARCH type and considered nonparametric estimation of the conditional mean function and the conditional variance function characterizing the system. Following the single-indexing idea (e.g. ====; ====) extended the ARCH model of ==== to a flexible form called single-index volatility models and focused on estimating only the unknown variance function and the associated single-index coefficient. In related literature, ==== considered efficient estimation of conditional variance functions in stochastic regression and, more generally, ==== provided modern parametric and nonparametric methods for analyzing nonlinear time series data, and ==== studied dynamic structure for high dimensional covariance matrices. Recently, in a regression set up, ==== proposed a class of locally efficient semiparametric estimators to simultaneously estimate the central mean subspace and central variance subspace with the help of a parameterization strategy. Although their work and our work share some similarities, there are differences in the two approaches. First, we study out-of-sample prediction of the conditional mean function in the presence of conditional heteroscedasticity. Second, we adopt a fully nonparametric iterative estimation of the conditional mean and the variance functions without any model assumption.====Motivated by the work of ==== and ====, during the past decade, some authors have extended the theory of sufficient dimension reduction to time series. Without specifying a model, ==== developed the notion of ==== (TSCS), which represents a reduction in the dimension of ==== for a pre-specified ==== such that the conditional distribution of ==== is same as the conditional distribution of ==== for some known ====. This is equivalent to saying that ==== is conditionally independent of ==== given ====, where ====. They estimated the ==== matrix ==== nonparametrically by maximizing an estimating function based on Kullback-Leibler divergence and showed that the estimator of ==== is strongly consistent when ==== and ==== are known. ==== proposed the notion of ==== for time series ==== which represents a reduction in the dimension of ====, where all the information in the conditional mean ==== is contained in ====. They estimated ==== by minimizing the residual sum of squares based on a Nadaraya–Watson smoother of the conditional mean function. Once again, they showed that the estimator of ==== is strongly consistent when ==== and ==== are known.====Assuming that the conditional mean of ==== is zero, ==== developed a notion of ==== (TSCS) for the squared series, ====, which represents a reduction in the dimension of ==== for a known ====, where all the information in ==== is contained in ====. To estimate ====, they used the same approach as in ====. For the square series, ====, ==== considered reduction in the dimension of ==== for a pre-specified ==== such that the conditional distribution of ==== is same as the conditional distribution of ====. To estimate ====, they proposed a robust estimation methodology based on Density Power Divergences (DPD) that is indexed by a tuning parameter ==== which yields a continuum of estimators, ====, where ==== controls the trade-off between robustness and efficiency of the DPD estimators.====The aforementioned literature on sufficient dimension reduction for time series focuses either on the reduction in the dimension of ==== or on the reduction in the dimension of ==== when the conditional mean of ==== is assumed to be zero. Incidentally, the general set up in ==== allowed them to consider a simulation model (see Model 1 in Section 4 of ====) where both the conditional mean and variance are (nonlinear) functions depending on ====, and their TSCS approach was successful in estimating the unknown dimensions in the mean and the variance. Therefore, without assuming a model, it is of interest to develop an approach to reduce the dimension in the unknown conditional mean function as well as the conditional variance function of a time series, when ==== is conditionally heteroscedastic as in an AR-ARCH model.====Recently, ==== proposed an approach called ==== (similar to the TSCS approach of ====) which ==== estimates the dimensions in the mean and the variance functions. More specifically, their first-stage assumes that ==== is conditionally independent of ==== given ====, and estimates ==== for a fixed ==== and ==== with ====. In the second-stage, they construct a residual series, ==== based on the unknown ====, set ==== for some ====, assume that ==== is conditionally independent of ==== given ====, and estimate the ==== matrix ==== for a fixed ==== and ==== with ====. They also prove the consistency of estimators of ==== and ====. A major drawback of ===='s work is that their residual series, ====, is not observable because it depends on the unknown parameter matrix ====, which makes the estimation of ==== questionable! In fact, ==== recognize this drawback in their article and say that, “====.” While they do mention that in applications one should use an estimated residual that is observable, their theoretical result for the conditional variance function holds only for an unobservable residual series. Thus, the work of ==== does not satisfactorily address the problem of estimation of the dimensions in the conditional mean and the variance functions based on observable quantities.====In this article, we assume that the conditional mean is ==== for an unknown, possibly nonlinear, function ==== and ==== defined above. We also assume that the conditional variance is ==== for an unknown, possibly nonlinear, function ==== and for ==== and ==== defined above. Without assuming a parametric model for ====, we use a Nadaraya-Watson smoother of ==== and find an initial estimator, ====, of ==== by minimizing the residual sum of squares. This step focuses only on the estimation of the mean function. We then construct the estimated residuals, ====, use a Nadaraya-Watson smoother of ==== and find an estimator, ====, of ==== by minimizing a sum of squared errors involving ====. This step focuses only on the estimation of the variance function. Finally, we use ====, the Nadaraya-Watson smoother of ==== and ====, respectively, and obtain a revised estimator ==== of ==== by minimizing a weighted residual sum of squares. All these details are given in the subsequent sections of the article.====In Section ====, we introduce the notations and assumptions that are used throughout the article. In Section ====, we define the Nadaraya-Watson (N-W) smoother for the mean and the variance functions, respectively, and state two lemmas concerning the N-W smoother that are needed to prove our main theorems. In Section ====, we define an iterative estimation procedure to estimate the parameter matrices, ==== and ==== and state the three main theorems of the article. The theorems are proved in Appendix ====. While Section ==== introduces a new angular representation for parameter matrices to overcome computational challenges, Section ==== discusses selection of hyperparameters ====. To assess the performance of the estimators numerically, simulation studies are carried out in Section ==== and a real data analysis of the Brazilian Real (BRL)/U.S. Dollar Exchange Rate series is carried out in Section ====. Concluding remarks are given in Section ====.====Assume that ====, ====, ==== and ==== defined earlier are fixed and known numbers. Recall that with ====, we assumed that the conditional mean function ==== and the conditional variance function ====, where ==== for ==== with ====. Denote ====, where ==== is defined as in ==== with ====. Next, we state the assumptions for technical lemmas and the main theorems stated in this section.",Dimension reduction in time series under the presence of conditional heteroscedasticity,https://www.sciencedirect.com/science/article/pii/S0167947322002626,14 December 2022,2022,Research Article,54.0
"D'Angelo Nicoletta,Adelfio Giada,Mateu Jorge","Department of Economics, Business, and Statistics, University of Palermo, Palermo, Italy,Department of Mathematics, University Jaume I, Castellon, Spain","Received 5 April 2022, Revised 6 October 2022, Accepted 4 December 2022, Available online 9 December 2022, Version of Record 13 December 2022.",https://doi.org/10.1016/j.csda.2022.107679,Cited by (0),". Simulation studies to assess the performance of the proposed fitting procedure are presented, and an application to seismic spatio-temporal point pattern data is shown.","; ====; ====) and the log-Gaussian Cox processes (LGCPs) (====).====LGCPs are arguably the most prominent clustering models, for their flexibility and relatively tractability for describing spatial and spatio-temporal correlated phenomena specifying the moments of an underlying ==== (GRF). Therefore, the main interest is in the estimation of the first and second-order characteristics of the process that depend on the moments of the GRF.====In both purely spatial and spatio-temporal studies, the choice of the estimation procedure depends on several aspects that relate to the application context, the goals of the analysis, and the required computational time. ====The aforementioned models can be referred to as ‘global’ models, as they are globally defined and the process properties and estimated parameters are assumed to be constant all through the study area. However, a model with constant parameters, may not adequately represent detailed local variations in the data, since the pattern may present spatial and temporal variations due to the influence of covariates, the scale or spacing between points, and also perhaps due to the abundance of points. Indeed, a different way of analysing a point pattern can be based on local techniques identifying specific and undiscovered local structure, for instance sub-regions characterised by different interactions among points, intensity and influence of covariates.====On one hand, local second-order statistics have been used to obtain further insight into the local structure of the analysed point pattern. While the use of global spatio-temporal second-order summary statistics is a well-established practice to describe global interaction structures between points in a point pattern (====; ====; ====), the use of local tools was firstly advocated by ====, introducing the Local Indicators of Spatio-Temporal Association (LISTA) functions as an extension of the purely spatial Local Indicators, whose definition was given by ====. Successively, ==== introduced local versions of both the homogeneous and inhomogeneous spatio-temporal ====-functions, and used them as diagnostic tools, while also retaining for local information, showing that the local inhomogeneous ====-functions can be helpful to assess the goodness-of-fit of different spatio-temporal models, with the advantage of not relying on any particular model assumption on the data.====On the other hand, the literature about local models for point processes is quite rare. For spatial point processes, ==== showed that ===='s purely local models provide good inferential results by applying them to earthquake data. However, that work did not account for the temporal dimension of the seismic events, whose realisations depend on their past history, as proved by the existence of aftershocks. This leaves a gap in the literature about the availability of local spatio-temporal point process models. The challenge of extending ===='s work would be to enrich the literature on this topic. Additionally, it is well documented in recent literature that many spatial point patterns are just snapshots of real spatio-temporal ones, and thus they should be treated as such. Our paper enlarges the statistical tools available for spatio-temporal patterns.====Motivated by these reasons, we propose a local version of spatio-temporal LGCPs employing LISTA functions plugged into the minimum contrast procedure to obtain space as well as time-varying parameters. For the parameters of the deterministic part, we follow ==== firstly evaluated minimum contrast parameter estimation for both the spatial and spatio-temporal LGCPs. However, their estimation setting does not fit to all the contexts and type of observed data (such as when recorded in a continuous time resolution or when presenting high cluster structure such that counts by unit of time have several zeros), but most importantly it cannot be used if a non-separable correlation structure of the GRF is assumed. This limitation was overcome by the proposal of ====. The possibility of fitting models based on non-separable covariances is crucial when separability cannot be assumed.====We point out that our proposal represents the first attempt to fit a localised version of a log-Gaussian Cox process. Therefore, the main contribution of this paper stands in the novel availability of methods for fitting spatio-temporal point process model involving the estimation of local ==== language, and are available from the first author.====The structure of the paper is as follows. Section ==== is devoted to the introduction of basic definitions of spatio-temporal point processes. Section ==== briefly reviews the class of spatio-temporal LGCPs. In Section ====, the proposed local minimum contrast estimation based on the LISTA functions is presented. Section ==== reports the diagnostic procedure used to assess the goodness-of-fit of a global LGCP, and a proposed modification to deal with local LGCPs. Then, the performance of the proposed local estimation method is assessed in Section ==== through a simulation study. An application to real seismic data comes in Section ====. Finally, the paper ends with some conclusions in Section ====.",Locally weighted minimum contrast estimation for spatio-temporal log-Gaussian Cox processes,https://www.sciencedirect.com/science/article/pii/S0167947322002596,9 December 2022,2022,Research Article,55.0
"Yamaguchi Hikaru,Murakami Hidetoshi","Department of Applied Mathematics, Graduate School of Science, Tokyo University of Science, 1-3 Kagurazaka, Shinjuku-ku, Tokyo 162-8601, Japan,Department of Applied Mathematics, Tokyo University of Science, 1-3 Kagurazaka, Shinjuku-ku, Tokyo 162-8601, Japan","Received 10 December 2021, Revised 2 December 2022, Accepted 5 December 2022, Available online 7 December 2022, Version of Record 9 December 2022.",https://doi.org/10.1016/j.csda.2022.107680,Cited by (0),"The two-sample problem is one of the most important topics in various fields, such as biomedical experiments and product quality maintenance. The Lepage-type test, which is the ==== based on these moments is compared with the chi-square approximation based on the limiting null distribution. Simulation studies and data examples demonstrate the usefulness of gamma approximation in the case of small sample sizes.","The two-sample problem is among the most important topics in various scientific fields, such as biomedical experiments and product quality maintenance. A distribution-free test is recommended if the available sample size is small because a specific distribution cannot be assumed. Particularly, linear rank statistics are commonly used in location-shift and scale-shift models. See, for example, ==== and ==== for the fundamental properties of linear rank statistics. ==== proposed the Lepage-type test using the Mood statistic instead of the Ansari–Bradley statistic. If ties are not included in the data, ==== showed that the Lepage-type statistic proposed by ==== is essentially equivalent to the Cucconi test (==== applied the adaptive test based on the concept of ==== to the Lepage-type tests. ==== proposed the Lepage-type test based on the Mahalanobis distance between the generalized Wilcoxon and Mood statistics defined by ==== has proposed a class of Lepage-type tests based on the Mahalanobis distance between percentile-modified test statistics for location and scale parameters proposed by ====.====As a test statistic that combines more than two linear rank statistics, ==== proposed the Euclidian distance-based and the Mahalanobis distance-based statistics of the Wilcoxon, Ansari–Bradley, and anti-Savage statistics for a combination of the location-scale and the Lehmann alternatives. As an application, for example, the Euclidian distance-based and the Mahalanobis distance-based statistics of some standardized linear rank statistics are frequently used to compare new drugs with placebo in biomedical experiments (====; ====; ====). Additionally, Lepage-type tests are used to assess and maintain product quality in process monitoring schemes in the production lines (====; ====; ====; ====, ====; ====; ====; ====).====-value of the two-sample test repeatedly. Moreover, the higher criticism statistic suggested by ==== requires ====-value. In the presence of ties, the (conditional) null variance of the linear rank statistics tends to be smaller than that in the absence of ties. Therefore, it is necessary to correct the approximate distribution of the test statistic that combines the linear rank statistic.====In the presence of ties, ==== modified the Lepage statistic by replacing the standardized Wilcoxon and Ansari–Bradley statistics with modified versions of ties. ====, ==== proposed the Lepage-type tests for several-sample location-scale problems. ==== showed that the tie-adjusted version of the ====-sample Lepage-type test converges to the chi-square distribution with ==== degrees of freedom under the appropriate assumptions. However, ==== does not discuss the approximate distribution in the case of finite sample sizes. ==== modified the Lepage-type statistic defined by ==== as a test statistic based on the Mahalanobis distance. However, there has been no detailed discussion (for example, the finite-sample theory, such as higher-order moments) of these Lepage-type tests when ties occur.====This study investigates the properties of the tie-adjusted version of the Euclidian and Mahalanobis distance-based statistics of some standardized linear rank statistics. In Section ====, we define these tie-adjusted test statistics and investigate the asymptotic null distribution. Furthermore, we derive the moment-generating function of the vector of tie-adjusted linear rank statistics and enable the calculation of the moments of the proposed statistic. In Section ====, we apply the proposed tests to datasets including many ties. Lastly, Section ==== concludes and summarizes the paper.",The multi-aspect tests in the presence of ties,https://www.sciencedirect.com/science/article/pii/S0167947322002602,7 December 2022,2022,Research Article,56.0
"Eendebak Pieter T.,Schoen Eric D.,Vazquez Alan R.,Goos Peter","University of Antwerp, Faculty of Business and Economics, Belgium,KU Leuven, Faculty of Bioscience Engineering, Belgium,University of Arkansas, Department of Industrial Engineering, USA","Received 4 February 2022, Revised 26 August 2022, Accepted 30 November 2022, Available online 5 December 2022, Version of Record 13 December 2022.",https://doi.org/10.1016/j.csda.2022.107678,Cited by (0), The best ranked 64-run designs substantially improve on benchmark designs from the literature.," and ==== for recent reviews.====In this paper, we consider two-level designs for the situation where many of the main effects could be active, while there could also be two-factor interactions. ==== call factor screening for this situation ====. Obvious candidate designs to consider here are strength-4 orthogonal designs. In these designs, the estimators of all the main effects and all the two-factor interactions are independent. Strength-4 designs exist for run sizes that are a multiple of 16 (====). The smallest strength-4 designs thus have 16, 32, 48 or 64 runs and include up to 5, 6, 5 and 8 factors, respectively. For intensive screening of larger numbers of factors, it is natural to turn to strength-3 designs. Strength-3 designs have run sizes that are multiples of 8. A strength-3 design with ==== runs can include up to ==== factors. In such a design, the estimators of the main effects are independent from each other and independent from those of the two-factor interactions. Estimators of two-factor interactions are not pairwise independent, however, so that there is a challenge in identifying active interactions.==== and ==== showed that fold-over designs permit estimation of many models including all the main effects and a good number of interactions. However, ==== showed that fold-over designs in ==== runs provide at most ==== degrees of freedom for estimating two-factor interactions. In the rest of this paper, we refer to the number of degrees of freedom for estimating two-factor interactions as the interaction rank of a design.====Strength-3 designs that cannot be constructed by folding over may provide a much higher interaction rank. So, they may allow a larger number of two-factor interactions to be estimated simultaneously. For reasons explained later, such designs are called even-odd designs. ==== showed that even-odd designs exist only if the number of factors ==== is smaller than ====. By exploring a complete catalog of strength-3 designs with up to 48 runs (====), ==== established empirically that the maximum numbers of factors for which even-odd designs with 32, 40 and 48 runs exist are 10, 10 and 14, respectively. A substantial number of the designs they recommended are even-odd designs. For example, for 48 runs and 5–14 factors, 16 out of the 17 recommended designs are even-odd designs.====To date, no complete catalogs exist of strength-3 designs with run sizes ====, because it is challenging or even infeasible to enumerate these designs completely. However, partial catalogs did appear in the literature. ==== enumerated all regular strength-3 designs for ====. In such designs, the two-factor interactions in any pair are either completely aliased or orthogonal to each other. ====, ==== and ==== extended the work of ==== enumerated two-level designs with up to 256 runs based on quaternary linear codes; many of these designs have a strength of 3. Another partial collection of designs is given by ====, ====, who constructed even-odd designs with 64, 80, 96, 112 and 128 runs by concatenating smaller strength-3 designs.==== vectors. The second step of the algorithm is to remove designs which do not comply with a special structure. The innovation in this step is a computationally fast procedure to test for this structure.====The rest of this paper is organized as follows. In Section ====, we review background material, including evaluation criteria for strength-3 designs. In Section ====, we introduce our algorithm to generate even-odd designs with at least one nonzero correlation between a two-factor and a three-factor interaction contrast vector. In Section ====, we validate the algorithm by generating even-odd designs with 32, 40 and 48 runs. Based on these results, we estimate that the algorithm completes the search for even-odd designs at least 35 times faster than the more general algorithm of ====. In Section ====, we discuss the enumeration of the 56-run even-odd designs and highlight the best designs in terms of the interaction rank. The most significant output of our enumeration is a list of 31,189,268,267 even-odd designs with 64 runs. In Section ====, we discuss the enumeration results of these designs in detail. Finally, Section ==== provides a brief discussion of the strengths and weaknesses of our approach.====The following is the Supplementary material related to this article.",Systematic enumeration of two-level even-odd designs of strength 3,https://www.sciencedirect.com/science/article/pii/S0167947322002584,5 December 2022,2022,Research Article,57.0
"Wang Jiangzhou,Cui Tingting,Zhu Wensheng,Wang Pengfei","College of Mathematics and Statistics, Shenzhen University, Shenzhen 518060, China,Key Laboratory for Applied Statistics of MOE and School of Mathematics and Statistics, Northeast Normal University, China,School of Statistics, Dongbei University of Finance and Economics, Dalian 116025, China","Received 23 July 2021, Revised 12 November 2022, Accepted 19 November 2022, Available online 2 December 2022, Version of Record 5 December 2022.",https://doi.org/10.1016/j.csda.2022.107664,Cited by (1), (FDR) and largely ignored ,"), among others.==== proposed a concept, termed as false discovery rate (FDR), for multiple testing. FDR reflects a trade-off between false discoveries and true discoveries, and has become one of the most commonly used control criterion for multiple testing. Most of the conventional multiple testing procedures with FDR control largely ignored the dependence among tests and implicitly assumed that observations arising from multiple tests are independent. However, recent studies have shown that these correlations are not negligible. ==== and ==== showed that ignoring the dependence among the tests can decrease statistical accuracy and testing efficiency in multiple testing. ==== pointed out that high correlation leads to high variability of testing results and the irreproducibility of scientific findings. Hence it is imperative to take into account the dependence among tests when conducting large-scale multiple testing.====For the past decade, there has been an increasing interest in large-scale multiple testing under dependence. The hidden Markov model (HMM), as an effective tool for modeling local correlations, has been widely used in various fields, including speech recognition, bioinformatics, finance, among others (====). ==== creatively applied the HMM to large-scale dependent multiple testing and proposed a multiple testing procedure, termed as LIS (local index of significance) procedure hereafter. They have shown that the LIS procedure controls FDR at the pre-specified level and achieves higher statistical power especially when the non-null hypotheses are clustered. To date, a considerable amount of extensions of the LIS procedure have been proposed. For example, recently, ====, ==== and ====.====Although these aforementioned multiple testing procedures are capable of characterizing the local dependence among tests, they largely fail to incorporate the covariate effect. In practice, the problems of large-scale multiple testing are often accompanied with various covariate effects. For example, an increasing number of researches have reported that the etiology of complex diseases, such as asthma, bipolar disorder and alcoholism, depends not only on the genetic effect, but also on the covariate (====; ====; ====). A proper covariate-adjustment in GWAS is critical for identifying genetic variants of interest. At present, an increasing number of multiple testing procedures with covariate-adjustment have been proposed. ==== extended the local false discovery rate procedure, referred to as Lfdr procedure (====) hereafter, and suggested to leverage locus-specific covariates, such as functional annotations, to improve gene discoveries in GWAS. Furthermore, ==== employed B-spline densities to approximate the non-null distributions and proposed a semi-parametric Lfdr procedure for covariate-modulated multiple testing. Recently, ==== suggested that the ====-value rejection threshold should be adaptively selected by using generic side information as covariates. It should be pointed out that these covariate-modulated multiple testing procedures implicitly assumed that ====-values or ====-values are independent, which rarely holds in reality. In a recent work, ==== only considered the presence or absence of locus-specific covariate effects and did not quantify the effect of covariates on the test statistic.====To overcome the above-mentioned limitations, we develop an interpretable and flexible multiple testing procedure referred to as covariate-modulated local index of significance (cmLIS). Our proposal can not only capture the local correlations among tests but also allow for the covariate effect via an extended HMM. We assume that the ====, ====. When the parameters of the covariate-modulated HMM are known, theoretical results show that the cmLIS procedure controls marginal false discovery rate (mFDR) at the pre-specified level ==== and has the smallest marginal false non-discovery rate (mFNR) among all ====-level multiple testing procedures. When the parameters of the covariate-modulated HMM are unknown, we use the Markov chain Monte Carlo (MCMC) algorithm to estimate model parameters. Since the normal mixture model can approximate a large amount of distributions (====; ====), here it is utilized for modeling the non-null distributions of ====-values. In practice, the number of components in the normal mixture model, denoted by ====The rest of this paper is organized as follows. Section ==== starts with a brief description of the HMM for large-scale multiple testing, then introduces the covariate-modulated HMM and the cmLIS procedure for covariate-modulated large-scale multiple testing under dependence. Section ==== presents a series of simulation results in various model settings. In Section ====, we apply the cmLIS procedure to an RNA sequencing data and a schizophrenia (SCZ) data. Some discussions and suggestions for future work are concluded in Section ====.====The following is the Supplementary material related to this article.",Covariate-modulated large-scale multiple testing under dependence,https://www.sciencedirect.com/science/article/pii/S0167947322002444,2 December 2022,2022,Research Article,58.0
"Zelaya Mendizábal Valentina,Boullé Marc,Rossi Fabrice","SAMM EA 4543, Université Paris 1 Panthéon-Sorbonne, France,CEREMADE, CNRS, UMR 7534, Université Paris-Dauphine, PSL University, 75016 Paris, France,Orange Labs, France","Received 28 March 2022, Revised 22 November 2022, Accepted 25 November 2022, Available online 2 December 2022, Version of Record 5 December 2022.",https://doi.org/10.1016/j.csda.2022.107668,Cited by (0),"G-Enum histograms are a new fast and fully automated method for irregular histogram construction. By framing histogram construction as a density estimation problem and its automation as a model selection task, these histograms leverage the ","; ====.====In theory, histograms require very few parameters and can adapt to any kind of distribution given enough bins. In practice however, the choice of the binning can have an unforeseeable effect on the accuracy of data estimation, on the ==== of the visualisation and on the size of the representation.====Numerous strategies exist to automatically select the number of bins in a regular histogram with equal width bins (for instance the pioneering Sturges' formula (====) and the Freedman-Diaconis' rule (====)). A major problem with regular histograms is that regions with higher or lower densities are treated the same, which gives histograms their limited reputation. For complex distributions, irregular histograms with bins of different widths are more adapted (====), but fully automated construction methods are scarce or limited, as pointed out in ====; ====). Most fully automated methods in the literature view histogram construction as a model selection task and implement it via some form of penalized quality criterion that balances the quality of the histogram as a representation of the data with its complexity. Among these, the ==== (MDL (====; ====)) provides a sound general framework to implement model selection. The key idea of MDL is that any regularity in the data can be used to “compress the data”, i.e. to describe it in a shorter manner. More formally, the best choice for a model and its parameters is the one that minimizes the coding length of the model parameters and of the data given the model.====. Another work formalized a MDL criterion via the ====). Notice however that the NML criterion for histograms relies on a single user-defined parameter, the ==== at which the data is to be approximated, ====We introduce a new MDL based histogram construction method that aims to reduce the computational burden of previous solutions and to enable automatic tuning of the accuracy parameter ====. Our formulation is based on a Bayesian maximum ====) rather than a polynomial one. As previous MDL based criteria, our criterion uses an accuracy parameter. We study the effect of this parameter on the histograms obtained by optimising our criterion. Then we introduce a new ==== criterion, derived from the base one, that automates the choice of ====.====The rest of this paper is organized as follows. Section ==== provide a short overview of other fully automated histogram construction methods. Section ==== specifies in details the histogram construction problem. Section ==== describes the NML approach from ==== and discusses its limitations. Section ==== introduces two enumerative criteria for histogram construction, while their theoretical properties and consistency are discussed in section ====. In section ====, experiments demonstrate the performance of all three criteria and other state-of-the-art methods on synthetic and real large-scale datasets. Concluding remarks are given in section ====.====The final form of K&M's NML criterion in their paper is as follows.====To ease the comparison of both criteria, we simplified the likelihood term as described hereafter.==== Then, we have==== And",Fast and fully-automated histograms for large-scale data sets,https://www.sciencedirect.com/science/article/pii/S0167947322002481,2 December 2022,2022,Research Article,59.0
"Zhuang Haoxin,Diao Liqun,Yi Grace Y.","Department of Statistics and Actuarial Science, University of Waterloo, 200 University Avenue West, Waterloo, ON, N2L 3G1, Canada,Department of Statistical and Actuarial Sciences, Department of Computer Science, University of Western Ontario, 1151 Richmond Street, London, ON, N6A 5B7, Canada","Received 19 January 2022, Revised 18 November 2022, Accepted 21 November 2022, Available online 25 November 2022, Version of Record 8 December 2022.",https://doi.org/10.1016/j.csda.2022.107665,Cited by (0),None,"). When the inverse of a cumulative distribution function (CDF) is available, sampling from the distribution is commonly conducted through the “inversion method” (====; ====). Commonly-used MCMC algorithms include Metropolis-Hasting (MH) algorithm (====).====). To reach convergence, MCMC algorithms may require a large number of iterations and carefully tuned stepsizes to achieve a suitable acceptance rate (====). Furthermore, MCMC algorithms can be inefficient in sampling from multi-modal distributions (====).====), finite mixture distributions (e.g., ====; ====), piecewise approximating functions (====). Adaptive algorithms (====; ====) and the Delayed Rejection Adaptive Metropolis (DRAM) (====) were developed to achieve efficient sampling procedures. Methods concerning the step-size tuning in MCMC were discussed by ==== and ====. Methods of efficiently exploring the domain space were considered by ==== and ==== for multi-modal distributions.==== and further investigated by ====, ==== and ==== to demonstrate the merits of the proposed method.====The rest of the paper is organized as follows. In Section ====, we describe the proposed PTMC method and several sampling algorithms. In Section ====, we perform simulation studies to evaluate the finite sample performance of the proposed PTMC method and compare it with the MCMC algorithm. In Section ====, we analyze two heterogeneous datasets based on Gaussian mixture models, and examine the capacity of the PTMC algorithms for sampling from complex multi-modal distributions. We conclude the article with discussions presented in Section ====.====The following is the Supplementary material related to this article.",Polya tree Monte Carlo method,https://www.sciencedirect.com/science/article/pii/S0167947322002456,25 November 2022,2022,Research Article,60.0
Soale Abdul-Nasah,"Department Applied and Computational Mathematics and Statistics, University of Notre Dame, Notre Dame, IN, 46556, USA,Children's Environmental Health Initiative, University of Notre Dame, Notre Dame, IN, 46556, USA","Received 21 March 2022, Revised 16 November 2022, Accepted 21 November 2022, Available online 25 November 2022, Version of Record 29 November 2022.",https://doi.org/10.1016/j.csda.2022.107666,Cited by (0),", and ==== are included as well."," that allows us to transform high-dimensional predictors to a low-dimensional representation without loss of relevant regression information.====], slice inverse regression [SIR; ====], slice average variance estimation [SAVE; ====], directional regression [DR; ====], expectile-assisted inverse regression estimation [EA-IRE; ====], and principal asymmetric least squares [PALS; ====To expand the scope of application of SIR, ==== proposed the partial central space for predictors with a mix of quantitative and qualitative predictor variables. Their method was later extended to SAVE by ====To avoid the linearity assumption, a new method called the minimum average variance estimation [MAVE; ====], sliced regression [SR; ====], adaptive MAVE [====] have been proposed as alternatives. Both dMAVE and SR can capture the central space exhaustively, but SR is more robust against extreme values. qOPG exploits the gradient of the local conditional quantiles and is able to handle heteroscedastic error variance. However, it is computationally intensive.====]. For the rest of the paper, we will call this class of estimators the MAVE-type methods.====Now, let ==== denote a ====-dimensional predictor and ==== be a continuous response. For a generic vector, ====, the characteristic function of ==== equals the section of the characteristic function of ==== along ====. Thus, the distribution of any ==== is uniquely characterized by its 1-dimensional projections [====]. We exploit this property of the characteristic function to solve the problem of predictors with complex structures in dimension reduction. The basic idea is to project ==== onto a random unit vector to obtain a univariate projected predictor ====, which will be used as the surrogate for ====. Next, we regress ==== on ==== at different expectile levels. Then, we estimate the dimension reduction subspace in the direction of ==== from the outer product of the expectile slopes and ====. We repeat the process for several unit vectors and average their corresponding subspaces to estimate the central space. We call our method projection expectile regression (PER).====Our proposal has some appealing features. Firstly, the univariate projections help to reveal interesting features such as outliers and temporal clusters in the data [====]. Thus, PER fares well with non-smooth monotone links, which means it can handle stratification and clustering in the response surface. Secondly, because PER utilizes expectiles, it is robust to extreme values. In addition, by regressing ====The rest of the paper is organized as follows. In Section ====. In Section ====, we apply projection expectile regression to a real application on medical cost in the United States. We conclude the paper in Section ====. All proofs are relegated to the appendix.====We provide the upcoming Lemma to help us with the proof of ====. ",Projection expectile regression for sufficient dimension reduction,https://www.sciencedirect.com/science/article/pii/S0167947322002468,25 November 2022,2022,Research Article,61.0
"Gangloff Hugo,Morales Katherine,Petetin Yohan","Samovar, Telecom Sudparis, Institut Polytechnique de Paris, 91011 Évry, France","Received 1 March 2022, Revised 18 November 2022, Accepted 19 November 2022, Available online 24 November 2022, Version of Record 28 November 2022.",https://doi.org/10.1016/j.csda.2022.107663,Cited by (1),None,"Let ====, ====, ====, ==== and ====, ====, respectively. As far as notations are concerned, we do not distinguish r.v. and their realizations. By hidden, we mean that ==== is an unobserved r.v. that we wish to estimate from ====. It represents an interpretable class associated to ==== contrary to ==== which is an intermediate auxiliary latent r.v. These three processes are ==== under the distribution ==== is denoted as ====. Finally, for ====, we note the sequence ====; note that when the sequence is considered from the beginning, we have ====.",Deep parameterizations of pairwise and triplet Markov models for unsupervised classification of sequential data,https://www.sciencedirect.com/science/article/pii/S0167947322002432,24 November 2022,2022,Research Article,62.0
"Chen Kunzhi,Shen Weining,Zhu Weixuan","Wang Yanan Institute for Studies in Economics (WISE), Xiamen University, Xiamen, China,Department of Statistics, University of California, Irvine, CA, USA,Department of Statistics and Data Science, School of Economics, Xiamen University, Xiamen, China","Received 25 March 2022, Revised 10 November 2022, Accepted 16 November 2022, Available online 21 November 2022, Version of Record 22 November 2022.",https://doi.org/10.1016/j.csda.2022.107662,Cited by (1), with that of the Beta-GOS. The excellent numerical performance of the method is demonstrated via simulation and two real data examples.,"), psychology (====), cognitive science (====) and neuroscience (==== where ==== is the number of distinct values ==== of the sequence, ==== are positive weights which satisfy ====, ==== is a non-atomic probability measure, and ==== is the point mass function. If the weights ===='s depend only on the cluster sizes of the partition, the resulting sequence is an exchangeable SS sequence (====). The most well known example is the Dirichlet Process (DP) with a mass parameter ==== and a base measure ====, where ==== and ====. The Pitman-Yor process (====) is another commonly used example of exchangeable SS.====Nonexchangeable species sampling sequences have also received a great deal of attention recently. They can be conveniently constructed by letting the weights ==== in the sequence model ====. For example, ==== and ==== specify the weights as functions of the distance between observations such that nearby observations are more likely to be clustered together. ==== and ==== propose generalized product models that are predictor/covariate dependent.====The main goal of this paper is to introduce a new class of ====; ====, ====, where the main idea is to let the base measure ==== in the DP to depend on the covariates ====, denoted by ====, while keeping the stick breaking weights the same. ==== for a detailed review of the DDP literature.====; ====). However, to the best of our knowledge, there is still a lack of development for covariate-dependent nonexchangeable priors; and it is our goal in this paper to fill this gap. In particular, we propose dependent Beta-GOS (DBG), which is a covariate-dependent extension of the Beta-GOS model (====The rest of this paper is organized as follows. In Section ====, we conduct simulation studies to evaluate the performance of DBG. We further demonstrate the utility of DBG via two applications to Valencian influenza data and sleep state data in Section ====. A few future working directions are discussed in Section ====.",Covariate dependent Beta-GOS process,https://www.sciencedirect.com/science/article/pii/S0167947322002420,21 November 2022,2022,Research Article,63.0
"Naderi Mehrdad,Mirfarah Elham,Wang Wan-Lun,Lin Tsung-I","Institute of Statistics, National Chung Hsing University, Taichung 402, Taiwan,Department of Statistics and Institute of Data Science, National Cheng Kung University, Tainan 701, Taiwan,Department of Public Health, China Medical University, Taichung 404, Taiwan","Received 9 January 2022, Revised 9 November 2022, Accepted 13 November 2022, Available online 21 November 2022, Version of Record 30 November 2022.",https://doi.org/10.1016/j.csda.2022.107661,Cited by (1),"Mixture regression models (MRMs) are widely used to capture the heterogeneity of relationships between the response variable and one or more predictors coming from several non-homogeneous groups. Since the conventional MRMs are quite sensitive to departures from normality caused by extra skewness and possible heavy tails, various extensions built on more flexible distributions have been put forward in the last decade. The class of normal mean-variance mixture (NMVM) distributions that arise from scaling both the mean and variance of a normal ==== with a common mixing distribution encompasses many prominent (symmetric or asymmetrical) distributions as special cases. A unified approach to robustifying MRMs is proposed by considering the class of NMVM distributions for component errors. An ==== maximization either (ECME) algorithm, which incorporates membership indicators and the latent scaling variables as the missing data, is developed for carrying out maximum likelihood (ML) estimation of model parameters. Four simulation studies are conducted to examine the finite-sample property of ==== and the robustness of the proposed model against outliers for contaminated and noisy data. The usefulness and superiority of our methodology are demonstrated through applications to two real datasets.","Mixture regression models (MRMs), which initially emerged as ‘switching regression’ in the economics literature (====; ====), have been widely used to explore the source of heterogeneity when groups of individuals respond to a predictor differentially. Alternatively, ==== for a comprehensive overview.====Let ==== be a ====, for ====. Moreover, we introduce vectors of unobservable membership indicators ==== with entry ==== which equals one if ==== comes from the ====th group (====, for ====, are subject to the constraint of ====, where ==== is the number of components in the MRM. Thus, the random vector ====, denoted by ====. The classical MRM, named as N-MRM henceforth, can be formulated as follows:==== where ==== is a ==== with intercept ====, and ==== are random errors assumed to be mutually independent. For model ====, denote by ==== the collection of all unknown parameters to be estimated, where ==== and ==== where ==== and variance ====. However, the task of ====) for finding ==== within a complete-data framework (====). An alternative way of estimating parameters in ==== is to exploit an EM-like algorithm for the semi-parametric mixture model introduced by ====. The use of semi-parametric approach could be much more flexible than other existing algorithms, and might be applicable even for the non-identifiable model. Generally, model ==== is identifiable under the conditions provided by ==== and ====. To overcome the non-identifiability due to label switching (====), we adopt a constraint on the mixing proportions ==== (====), for relabeling model parameters.==== distribution (====; ====), the contaminated normal distribution (====), the skew-normal distribution (====), and the scale mixtures of skew-normal (SMSN) distributions (====; ====), among others.====This paper aims at enriching the palette of non-normal MRM by considering the class of normal mean-variance mixture (NMVM) distributions (====) for component errors, referred to as NMVM-MRM. Analogous to the SMSN distributions, the NMVM distributions can accommodate both strong degree of skewness and heavy tails in the data. This class of distributions includes a wide variety of notable members, such as the generalized hyperbolic (GH) distribution (====), the normal mean-variance mixture of Birnbaum-Saunders (NMVBS) distribution (====), and the normal mean-variance mixture of Lindley (NMVL) distribution (====) for ML estimation of the NMVM-MRM and further offer a simple way of computing the standard errors via the outer product of the scores (====).====The paper is structured as follows. Section ==== briefly reviews the NMVM distribution and outlines some of its related properties. Section ==== formulates the proposed model and presents an EM-based procedure for undertaking ML estimation as well as the stopping rule. Section ==== investigates practical issues related to the estimation of standard errors and the specification of valid initial values. Applications to two real data examples are provided in Section ==== for illustration. Four simulation studies are conducted in Section ==== to examine the effectiveness of the proposed methodology. Finally, some concluding remarks are given in Section ====.====A random variable ==== is said to follow a NMVM distribution, ====, if it has the stochastic representation:==== where ====, ====, ====, ==== is an arbitrary univariate positive random variable with pdf ==== indexed by a possibly parameter vector ====, and ‘⊥’ indicates independence. The convenient hierarchical representation of the NMVM distribution is given by ====. Accordingly, the probability density function (pdf) of ==== can be derived through====An attractive sub-class of the NMVM family can be constructed by assuming the GIG distribution on ====, denoted by ====, with pdf==== where==== is the modified Bessel function of the third kind with ====, the index ====, and two parameters ==== and ==== satisfying the conditions: (i) ==== if ====; (ii) ==== if ====; and (iii) ====, ==== if ====. Denoting by ====, we have for ====:====Following representation ==== coupled with the assumption of ====, ==== is said to have the GH distribution with the pdf==== where ====.",Robust mixture regression modeling based on the normal mean-variance mixture distributions,https://www.sciencedirect.com/science/article/pii/S0167947322002419,21 November 2022,2022,Research Article,64.0
"Cao Yongxiu,Yu Jichang","School of Statistics and Mathematics, Zhongnan University of Economics and Law, Wuhan, Hubei 430073, China","Received 3 April 2022, Revised 5 October 2022, Accepted 12 November 2022, Available online 17 November 2022, Version of Record 22 November 2022.",https://doi.org/10.1016/j.csda.2022.107660,Cited by (1),"Unmeasured confounding is an important problem in observational studies, which brings a great challenge to eliminate or reduce bias. A large main data set with unmeasured ","; ====).====; ====; ====; ====). Although these large data sets may not contain full confounder information to obtain the consistent estimation of causal effect, they can be useful to increase the estimation's efficiency.====; ====; ====). Propensity score calibration (PSC) proposed by ====; ====).==== took the gold standard propensity score as a missing variable in the large main data set under the Bayesian framework, which did not need the assumption of surrogacy. ==== applied this method to causal inference with the response being completely observed. Recently, ====Compared to unmeasured confounders with the outcome being completely observed, the statistical methods for the outcome being time-to-event subject to censoring have been less attended. Recently, ==== proposed a martingale residual-based method to control for confounders measured only in the validation sample. ==== generalized the two-stage calibration method to survival and dichotomous outcomes.====In this article, we extend the method of leveraging correlation between a good estimator and an error-prone estimator to improve the efficiency of the good estimator to estimate the treatment effect with the response subject to right censoring. Specifically, we consider the situation where there exist a large main data set with unmeasured confounders and a small validation data set with detailed information on these confounders and the outcome is time-to-event subject to right censoring. The initial estimator is obtained by the doubly robust method based on the small validation data set (====; ====The remainder of this article is organized as follows. In Section ====. We present some concluding remarks in Section ====. The proofs for theoretical results are presented in the Appendix.==== ====By the formula of ====, we can obtain==== The second term of the right side of ==== is equal to==== where ==== is between ==== and ====, and ==== is between ==== and ====. Hence, we can obtain==== It is easy to show the second term of the right side of ==== converges to zero in probability. Hence, the term ==== can be rewritten as==== Using the proof method of Theorem 1 of ====, we can obtain==== where the definition of ==== is obvious.",Adjusting for unmeasured confounding in survival causal effect using validation data,https://www.sciencedirect.com/science/article/pii/S0167947322002407,17 November 2022,2022,Research Article,65.0
"Vo Thanh Huan,Chauvet Guillaume,Happe André,Oger Emmanuel,Paquelet Stéphane,Garès Valérie","Univ Rennes, INSA, CNRS, IRMAR - UMR 6625, F-35000 Rennes, France,Univ Rennes, ENSAI, CNRS, IRMAR - UMR 6625, F-35000 Rennes, France,IRT b<>com, Rennes, France,EA 7449 REPERES, France","Received 18 January 2022, Revised 4 November 2022, Accepted 4 November 2022, Available online 14 November 2022, Version of Record 16 November 2022.",https://doi.org/10.1016/j.csda.2022.107656,Cited by (0),"Probabilistic record linkage is a process of combining data from different sources, when such data refer to common entities and identifying information is not available. A probabilistic record linkage framework that takes into account multiple non-identifying information that this is limited to simple binary comparison between matching variables has been previously proposed. An extension of this method is proposed for mixed-type comparison vectors. A mixture model for handling comparison values of low prevalence categorical matching variables, and a mixture of hurdle ==== for handling comparison values of continuous matching variables have been developed. The parameters are estimated by means of the ==== estimation for a record pair to be a match and the prediction of matched record pairs are evaluated. The simulation results indicate that the proposed methods outperform existing ones in most considered cases. The proposed methods are applied on a real dataset, to perform linkage between a registry of patients suffering from venous thromboembolism in the Brest district area (GETBO) and the French national health information system (SNDS).","). However, some large health databases may not contain such identifying information. In other cases, this information is available but may contain errors, or may not be used for record linkage due to ethical reasons. ====; ====).====The French SNDS (Système National des Données de Santé) is the national health data system including the national health insurance information (SNIIRAM: Système National d'Information InterRégimes de l'Assurance Maladie) of around 99% of the French population (====The Fellegi-Sunter probabilistic record linkage model laid the foundation for most record linkage models until now (====). Although this model is useful for many applications in sample surveys and epidemiology, it has a limitation when some matching variables are binary and with a low prevalence (e.g., medical diagnoses). In that case, the simple binary comparison method proposed by ==== can not distinguish the agreement of low prevalence values, which is much more informative than the agreement of high prevalence values. Such cases are considered in ====Another limitation is that most probabilistic record linkage models only make use of simple binary or categorical comparison values (see ====) even if the matching variables are continuous. Some authors introduced continuous similarity measures for comparing string data, but then comparison values are transferred to categorical values representing different levels of agreement (e.g., ====; ====; ====), which may result in a loss of information.====In this article, we propose a new linkage model adapted from the framework of Fellegi and Sunter, which handles such situations. We aim at better taking into account the nature of matching variables (e.g., low-prevalence binary, or continuous), so as to improve the performances of record linkage. The article is organized as follows. In Section ====, we review the Fellegi-Sunter probabilistic record linkage model and some relevant problems. We then propose two comparison strategies for low prevalence binary or continuous matching variables in Section ====. An extended mixture model taking into account both categorical and continuous comparison values is also introduced in Section ====. In Section ====, we evaluate the proposed methods through simulation studies. In Section ====, a real data application is proposed, where we perform record linkage between SNDS and the GETBO (Groupe d'Etude de la Thrombose de Bretagne Occidentale) registry. Finally, possible further research is discussed in Section ====.====The following is the Supplementary material related to this article.",Extending the Fellegi-Sunter record linkage model for mixed-type data with application to the French national health data system,https://www.sciencedirect.com/science/article/pii/S0167947322002365,14 November 2022,2022,Research Article,66.0
"Ma Xuan,Zhao Jianhua,Wang Yue,Shang Changchun,Jiang Fen","School of Statistics and Mathematics, Yunnan University of Finance and Economics, Kunming, 650221, China,College of Science, Guilin University of Technology, Guilin, 541004, China","Received 13 December 2021, Revised 27 October 2022, Accepted 5 November 2022, Available online 11 November 2022, Version of Record 15 November 2022.",https://doi.org/10.1016/j.csda.2022.107657,Cited by (0)," distribution as a robust modeling tool for vector data has a very long history, its application to matrix data is very limited. The main reason is that the dimension of the vectorized matrix data is often very high, and the higher the dimension, the lower the breakdown point that measures the robustness. To solve the robustness problem suffered by FPCA and make it applicable to matrix data, in this paper a robust extension of FPCA (RFPCA) is proposed, which is built upon a ====-type distribution called matrix-variate ==== distribution. Like the multivariate ==== distribution, the matrix-variate ==== distribution can adaptively down-weight outliers and yield robust estimates. A fast EM-type algorithm for parameter estimation is developed. Experiments on synthetic and real-world datasets reveal that (i) RFPCA is compared favorably with several closely related methods. Importantly, RFPCA has a significantly higher breakdown point than its vector-based cousin multivariate ==== PCA (====PCA. Such detection is rarely available with existing matrix-based methods, especially for gross matrix-valued outliers.","Principal component analysis (PCA) (====). Moreover, the vectorized data dimension ==== is usually very high (====) while the number of parameters in PCA increases linearly with ====. When ==== is large and the sample size is relatively small, PCA is easily trapped into the ====.====), generalized low rank approximations of matrices (GLRAM) (====) and factored PCA (FPCA) (==== and column dimension ==== and hence are much less than that in PCA. As a result, these methods can greatly alleviate the high-dimensional problem and significantly reduce computation costs (====). However, the normal assumption is sensitive to the departure of normality. It may result in biased or even misleading results when data involves heavy tails or outliers (====; ====).====For low-dimensional vector data, it has been proved that the multivariate ==== distribution is effective for robust estimation (==== distribution (====PCA, in short). The extensions with the multivariate ==== distribution to related models such as probabilistic PCA and factor analysis can be found in ====; ====. However, the multivariate ==== distribution only applies to vector data. When it is applied to matrix data, the dimension ==== of the vectorized matrix data is usually very high, while the upper bound of its breakdown point, which measures the robustness, is ==== (====). Consequently, the larger the value of ====, the lower the breakdown point. This property dramatically limits its applicability to matrix data. To improve the fitting for matrix data, ==== suggest using a ====-type distribution, which is called the matrix-variate ==== or ==== distribution in ====, to replace the matrix-normal distribution, and their experimental results show that the model with a ==== distribution yields better performance in classification applications. However, they fail to consider the robustness of their model. It is not clear how robust their model is in comparison with the models with the matrix-normal and, particularly, multivariate ==== distributions. This is one of our main concerns in this paper. For clarity, the FPCA using a ==== distribution is denoted by TPCA in this paper.====To solve the robustness problems suffered by FPCA and ====PCA and make the resulting method applicable to matrix data. In this paper we propose a new robust FPCA method for matrix data, denoted as RFPCA. RFPCA uses another ====-type distribution, which is called the matrix-variate ==== or ==== distribution in ====The rest of this paper is organized as follows. In Sec. ====, we briefly review some related works. In Sec. ====, we conduct a number of experiments on simulated and real-world data. In Sec. ====, we close the paper with some conclusions and discussions.",Robust factored principal component analysis for matrix-valued outlier accommodation and detection,https://www.sciencedirect.com/science/article/pii/S0167947322002377,11 November 2022,2022,Research Article,67.0
"Yang Xi,Hoadley Katherine A.,Hannig Jan,Marron J.S.","The University of North Carolina at Chapel Hill, United States of America","Received 7 October 2021, Revised 21 July 2022, Accepted 19 October 2022, Available online 10 November 2022, Version of Record 25 November 2022.",https://doi.org/10.1016/j.csda.2022.107649,Cited by (0),"In the age of big data, data integration is a critical step especially in the understanding of how diverse ","As modern data sets have grown large they have also grown more complex. A canonical example is cancer ====). As noted in Section 3.1.4 of ====, PCA provides many data insights through ====, representing directions of maximal variation in the data space, with ====, where ==== and ==== are matrices comprised of ====). Each rank 1 matrix ==== is a mode of variation.====PCA scores are very interpretable as they give a clear impression of how the high dimensional data vectors relate to each other (often called dimension reduction). Scatter plot matrix views of scores often give clear views of data set structure, e.g. finding scientifically important clusters or groups of samples with shared features (see Section 4.1 of ====) designed for high-dimensional problems.==== analysis (also referred to ==== in bioinformatics). These analyses consist of multiple data matrices (one for each data type) called blocks, which consist of data vectors coming from a ==== set of experimental units (e.g. tissue samples).====An important example of multi-omics data is The Cancer Genome Atlas (TCGA) (====). The present paper focuses on just two of the data types available in TCGA. The first is Gene Expression (GE), widely known to be fundamental to cancer biology. The second is Copy Number Variation (CNV), which quantifies repeated replication and also deletion of chromosomal parts (that play an important role in many cancer types). In this paper, we work with Copy Number Region (CNR), medians of CNV over chromosomal regions called cytobands. There is strong biological interest in the association between GE and CNV, which motivates our AJIVE - Jackstraw analyses.====A useful method for analyzing multi-block data is the Angle-based Joint and Individual Variation Explained (AJIVE), proposed by ====. In a similar spirit to PCA, AJIVE reveals population structure through a decomposition into modes of variation. However, AJIVE takes multi-block analysis to the next level by providing two types of such modes of variation. The first type is ==== between data blocks, in the sense of having ==== scores vectors for each block. Here, each data block represents a different type of data measurement made on the same set of experimental samples and therefore it is possible to have common scores, but not possible to have common loadings. The second type are modes of variation that are ==== in the sense of having scores vectors that are orthogonal to the joint scores. Both types of scores are usefully visualized to find interesting population structure. The loadings indicate which features in each data block work together (joint) or separately (individual).====The main contribution of the present paper is the adaptation of jackstraw for inference on loadings in AJIVE analysis. In particular, it assigns statistical significance to both joint and individual loadings, which indicates the significance of the ranked feature loadings. Basic ideas are illustrated using a Toy Example in Section ====. Section ==== gives the algorithmic development of our novel jackstraw for AJIVE. Application of our AJIVE based jackstraw, in the context of cancer genomics research appears in Section ====.",Jackstraw inference for AJIVE data integration,https://www.sciencedirect.com/science/article/pii/S0167947322002298,10 November 2022,2022,Research Article,68.0
"Weishampel Anthony,Staicu Ana-Maria,Rand William","PepsiCo R&D, United States of America,Department of Statistics, North Carolina State University, United States of America,Poole College of Management, North Carolina State University, United States of America","Received 7 December 2021, Revised 11 October 2022, Accepted 16 October 2022, Available online 4 November 2022, Version of Record 15 November 2022.",https://doi.org/10.1016/j.csda.2022.107647,Cited by (0),Technological advancement has made possible the collection of data from ,"). These accounts have affected political elections (====; ====), propagated medical misconceptions (====), and impacted stock markets (====; ====; ====). In this paper, we propose a new method to classify social media users and identify malicious accounts based on their online posting behaviors.====Early work on bot detection analyzed many features of the users including: the content of their tweets (====; ====), proportion of known fake followers (====), or a combination of all these features (====; ====). Most previous literature ignored the accounts' posting pattern, or the temporal trend of their posting behavior (====; ====; ====). Recent bot detection methods utilize the tweeting behavioral differences through time series analyses (====), and summary statistics of the posting behavior (====). Though effective, these methods often require months worth of data and are computationally intensive. Moreover, they have been primarily studied in the context of identifying bots and it is unclear whether they would work successfully for identification of state-linked accounts. Recently, ====In this paper, we explore the potential of the posting behavior of Twitter accounts in providing insight into their identity (genuine/malicious/foreign state). For each account, the posting behavior is summarized by binning the total time period under study into short ====; the activity is then encoded as 0 if no post happens during a time interval or 1 if the user posts to social media during the time interval.====; ====; ====; ====. We propose to classify the accounts based on the latent trajectories.====The paper makes three main contributions. First it introduces a methodology to classify binary-valued functional data using flexible and nonparametric approaches. These ideas are built on the classification methods of the latent trajectories proposed by ====; ====; ====. The approach is computationally fast, is easily parallelizable, and leads to interpretable results. Second, it shows that for optimal Bayes classification, the coefficients of the common basis of the latent trajectories require a different estimation approach than the one considered for the existing classification of noisy and densely observed continuous curves. Third, the method is applied to a social media problem where, in addition to identification of the various social media agents with high accuracy, it provides a description of the behavior of the posting accounts.====The paper is organized as follows. Section ==== presents the modeling framework and the rationale for classification of binary time series also referred to as binary-valued curves. Section ==== details the estimation of the model components and the classification procedure. The performance of the proposed methodology is evaluated numerically in Section ==== via simulation studies and in Section ==== via the Twitter data application. Section ==== concludes with final remarks.====The following is the Supplementary material related to this article.",Classification of social media users with generalized functional data analysis,https://www.sciencedirect.com/science/article/pii/S0167947322002274,4 November 2022,2022,Research Article,69.0
"Kim Daeju,Kawano Shuichi,Ninomiya Yoshiyuki","AI Strategy Office, Technology Unit, Softbank Corp., Japan,Faculty of Mathematics, Kyushu University, Japan,Department of Statistical Inference and Mathematics, The Institute of Statistical Mathematics, Japan","Received 1 May 2021, Revised 7 February 2022, Accepted 14 October 2022, Available online 3 November 2022, Version of Record 9 November 2022.",https://doi.org/10.1016/j.csda.2022.107644,Cited by (0),A basis expansion with ,"), ====-splines (====) or the lasso method (====) is basically used.====While basis expansion with such a regularization method works well in many situations, it is often inappropriate when the underlying regression function has inhomogeneous smoothness. Let us call the region where the function is considerably smooth the strongly smooth one and the region where the function is not smooth the weakly smooth one. Basis expansion that is described above often leads to over-fitting in the strongly smooth region and under-fitting in the weakly smooth region. The local likelihood method (====, ====), adaptive wavelet filtering (====), adaptive sparse grids (====, ====) and free-knot splines (====, ====, ====The purpose of this study is to propose a simple new approach for the above-mentioned problem, without calculation by each explanatory variable or search for optimal basis functions. To achieve our aim, we propose a new efficient nonlinear regression modeling with basis expansions. The proposed method is based on the following idea. To begin with, we prepare tuning parameters by each coefficient of the basis function, while conventional regularization methods usually have one or a few tuning parameters for the whole set of model parameters. The prepared many tuning parameters will lead to over-fitting, and so we penalize them so that tuning parameters for basis functions whose positions are close have close values. Then we obtain estimates of the coefficients and appropriate values of the tuning parameters simultaneously by maximizing the sum of the likelihood function, the penalty term for the coefficients, and the penalty term for the tuning parameters.====) because the estimator considered in this method is not the maximum likelihood one. However, it is not possible to obtain an analytical expression of the GIC, because the estimation function for the proposed method is not expressed explicitly, unlike the estimation function of the ridge method. We, therefore, propose an estimating function with explicit expression and derive an approximated GIC, which is assured by some asymptotics. Our proposed nonlinear regression modeling is investigated through some numerical examples and real data analysis. Numerical results suggest that our proposed method performs well in various situations.====As methods using tuning parameters different by basis functions, we can refer to ==== and its extensions, e.g., ==== and ====The rest of this article is organized as follows. In Section ====, we describe a framework of Gaussian regression modeling based on basis expansion with the ridge method. Section ==== studies a new nonlinear regression modeling by proposing a regularization method with adaptive-type penalties and derives the approximated GIC. Section ====.",Smoothly varying regularization,https://www.sciencedirect.com/science/article/pii/S0167947322002249,3 November 2022,2022,Research Article,70.0
"Yang Haoyu,Qin Yichen,Wang Fan,Li Yang,Hu Feifang","Center for Applied Statistics and School of Statistics, Renmin University of China, Beijing, 100872, China,Department of Operations, Business Analytics, and Information Systems, University of Cincinnati, OH 45221, USA,Department of Statistics, George Washington University, Washington, DC 20052, USA","Received 3 December 2021, Revised 2 October 2022, Accepted 9 October 2022, Available online 31 October 2022, Version of Record 16 November 2022.",https://doi.org/10.1016/j.csda.2022.107642,Cited by (0),"Multi-arm trials are common in medical and health research for comparing the efficacy of competing drugs and interventions, among other applications. While ensuring ","). In multi-arm trials, multi-valued treatments may include more than two discrete conditions (such as comparing the efficacy of competing drugs or interventions) or multiple levels of one treatment (such as various doses of a particular drug). ==== note that multi-arm trials usually provide efficiency gains over separate trials when several experimental treatments are available for testing. ==== develop the multiple sclerosis-secondary progressive multi-arm randomized trial in which three treatments for multiple sclerosis patients are compared with a common control group. ==== conduct a multi-arm randomized trial to explore the impact of training with an electromechanically assisted gait training system when integrated with conventional rehabilitation focused on gait and mobility.====The covariate imbalance among treatment groups has a negative impact on the subsequent analysis. ==== note that the bias in the treatment effect estimation is proportional to the degree of covariate imbalance at the baseline. According to ====, any multi-arm trial can, if improperly designed, provide misleading information on the basis of the baseline covariates distributions in the treatment groups, potentially leading researchers to draw false conclusions. Indeed, if a substantial covariate imbalance exists, any inferences of the treatment effect will be inaccurate and any subsequent claims would rely on unverifiable assumptions (====; ====). Although some ex-post adjustments can cope with such an imbalance, they are far less efficient than achieving the ex-ante balance from the start since additional coefficients must be estimated and further model assumptions are required (====). ====), 237 of them used covariate-adaptive randomization. Therefore, randomization designs that minimize the covariate imbalance are essential in multi-arm trials.====; ====). ==== note that the covariate balance in multi-arm trials tends to be more important since including more treatment groups in the experiment even raises the chance of observing imbalanced covariates. To balance the covariates in multi-arm trials, ==== introduce the allocation ratio preserving biased coin minimization (BCM) design. BCM discretizes the continuous covariates so as to measure the covariate imbalance which is often less efficient and changes the nature of the continuous covariates. ==== propose a balanced match weighted design which aims to select the assignment that yields the best covariate balance. They worry about the high computational cost of obtaining an approximate optimal solution under such a design, especially as the sample size and the number of covariates increases. ====. The major computational concern for RR is how long it takes to find the acceptable randomization, which is closely related to the selection of the threshold. Despite recent improvements in computational power, finding the most efficient randomization method with covariate balance for multi-arm trials remains a challenging research task.====In this paper, we present a new method namely the adaptive randomization via Mahalanobis distance for multi-arm design (ARMM), for balancing continuous covariates among multi-treatments and thus improving the subsequent estimation efficiency. The proposed method takes the covariates of units into account sequentially and allocates units adaptively since in many trials units are not all available for simultaneous assignment of treatments but rather arrive sequentially. In the proposed method, we adjust the ==== of assignment according to the covariates of current units and the existing covariate imbalance of the previously allocated units. The proposed method is investigated both theoretically and numerically.====We contribute to the literature in the following aspects. We design an easy-to-implement covariate-adaptive randomization method specifically for the continuous covariates in multi-arm trials, whereas most of the existing methods are either designed for discrete covariates or lack theoretical justification. For our method, we establish its theoretical properties in randomization and treatment effect estimation. In particular, we demonstrate its ==== for estimation precision, i.e., the estimated treatment effect under the proposed method attains the minimum variance asymptotically. In addition, compared with other competing methods, the computational cost of the proposed method is also favorable.====The remainder of this paper is organized as follows. We introduce the proposed method in Section ==== and investigate its theoretical properties in Section ====. We present a real data analysis to demonstrate its superiority in Section ====. Finally, we conclude with a discussion in Section ====.====The following is the Supplementary material related to this article.",Balancing covariates in multi-arm trials via adaptive randomization,https://www.sciencedirect.com/science/article/pii/S0167947322002225,31 October 2022,2022,Research Article,71.0
"Fernández-de-Marcos Alberto,García-Portugués Eduardo","Department of Statistics, Carlos III University of Madrid, Av. de la Universidad, 30, 28911 Leganés, Spain","Received 10 December 2021, Revised 25 October 2022, Accepted 26 October 2022, Available online 29 October 2022, Version of Record 4 November 2022.",https://doi.org/10.1016/j.csda.2022.107653,Cited by (0)," critical values for varying sample sizes ====. However, detail on the accuracy of these and subsequent transformations in yielding exact ====-values, or even deep understanding on the derivation of several transformations, is still scarce nowadays. The latter stabilization approach is explained and automated to (====) expand its scope of applicability and (====) yield upper-tail exact ","The classical one-sample goodness-of-fit problem is concerned with testing the null hypothesis in which the cumulative distribution function (cdf) ==== of an independent and identically distributed (iid) random sample ==== equals a certain prescribed cdf ==== is arguably that based on ====, the empirical cumulative distribution function (ecdf) of ====. Ecdf-based test statistics confront ==== against ====, their best-known representatives being the Kolmogorov–Smirnov (====), Cramér–von Mises (====), and Anderson–Darling (==== against ====. When ==== is continuous, testing ==== reduces to testing whether the iid sample ====, ====, ====, is distributed as ====. Hence, tests of uniformity, despite their a priori limited applicability, provide powerful approaches to most of the goodness-of-fit problems concerned with fully-specified null hypotheses. In particular, the above ecdf-based statistics have the attractive property of being distribution-free, i.e., their exact null distributions do not depend on ====.====Both ecdf-based tests and uniformity tests have been exported to deal with data naturally arising in supports different from ====, ====, which commonly occurs in the form of circular (====) or spherical (====) data. The analysis of directional data faces specific challenges due to the non-Euclideanity of the support; see ==== for a book-length treatment of tailored statistical methods and ==== for a review of recent advances. In particular, tests of uniformity on ==== must be invariant under arbitrary rotations of the data coordinates, as these do not alter the uniform/non-uniform nature of the data. While a sizable number of tests of uniformity on ==== exist (see a review in ====), perhaps the two most known omnibus tests are those of ==== and ==== on ====: their statistics, ==== and ====, can be regarded as the rotation-invariant versions of the Kolmogorov–Smirnov and Cramér–von Mises tests of uniformity, respectively. Moving beyond ==== has proven a challenging task for ecdf-based tests up to relatively recent years, with ==== proposing a class of projected-ecdf statistics that extends ===='s test to ==== (see Section ====). As in the classical setting, tests of uniformity on ==== allow for testing the goodness-of-fit of more general distributions: in ====; the case ====, ====, is remarkably more complex and has been recently put forward in ====.====In this paper we build on Stephens' transformations to expand and automate them. First, we present a data-driven procedure to achieve a better stabilization, with respect to the sample size ====, of the exact null distribution of a generic test statistic ==== of interest, for a wider range of significance levels ==== (i.e., upper ====-quantiles of ====). Specifically, new modifications for the (one-sample) Kolmogorov–Smirnov, Cramér–von Mises, Kuiper, and Watson test statistics are derived and shown to extend the scope of applicability of previous approaches. To the best of our knowledge, we also provide the first instance of such a stabilization for the Anderson–Darling test statistic. Second, we provide a method to approximate upper-tail exact ====-values for the tests constructed from stabilized statistics. Through an extensive simulation study, we show a significant improvement in the precision of the stabilization of the exact critical values of ==== for several sample sizes, as well as a competitive computational cost when compared with statistic-specific methods for evaluating exact null distributions. We also show large improvements, both in precision and computational efficiency, over the use of Monte Carlo simulation, arguably the most popular test calibration approach nowadays. Third, we develop an extension of our stabilization procedure to deal with several recent test statistics for assessing uniformity on ====, ====, and which hence have dimension-dependent distributions. In particular, we stabilize the exact null distribution of a novel Anderson–Darling test statistic for circular data. Finally, the introduced stabilization methodology allows us to perform tests in batches of small-to-moderate samples in an accurate and fast manner that does not require Monte Carlo simulation. This is illustrated in an astronomical dataset comprised of the longitudes at which sunspots appear, which exhibits a suspected temporal mix of uniform and non-uniform patterns.====The rest of the paper is organized as follows. Section ==== introduces Stephens' approach (Section ====) and our proposed extension (Section ====), together with simulation studies and a comparison between several modifications (Section ====). Section ==== briefly introduces the projected-ecdf statistics for testing uniformity on the hypersphere (Section ====), develops the parameter-dependent transformations to achieve their stabilization (Section ====), and analyzes the empirical performance of these transformations (Section ====). Section ==== gives an application of the modified statistics in astronomy. A final discussion of the obtained results concludes the paper in Section ====. Further analyzes and empirical results are included in the Supplementary Material (SM). All the code and data are available at ====.====The following is the Supplementary material related to this article.",Data-driven stabilizations of goodness-of-fit tests,https://www.sciencedirect.com/science/article/pii/S016794732200233X,29 October 2022,2022,Research Article,72.0
"Li Yanxin,Walker Stephen G.","Department of Statistics and Data Sciences, University of Texas at Austin, United States of America,Department of Mathematics, University of Texas at Austin, United States of America","Received 1 February 2022, Revised 22 October 2022, Accepted 25 October 2022, Available online 28 October 2022, Version of Record 8 November 2022.",https://doi.org/10.1016/j.csda.2022.107652,Cited by (3),"Motivated by a ==== for discrete spaces, a variation of the slice sampler for continuous spaces is introduced. It utilizes latent variables and is related to Neal's slice sampler. The key difference is that the additional latent variables allow the sequential stepping out or doubling procedures, which makes the basic slice sampler difficult to use in high dimensional problems, to be avoided. On the other hand, the latent slice sampling algorithm is applicable on high dimensional problems where the variables can all be treated in a single block.","The motivation for this paper is to be found in a discrete sampler presented in ==== and we briefly describe it here. One of the key ideas behind the Metropolis–Hastings algorithm, ====; ====, is the transition density ====, defined for all ====, for some space of values Ω, satisfying==== where ==== is the target density. The Metropolis–Hastings algorithm has transition density ====, where ==== is a proposal density, to be chosen,==== and ====. It is easily seen that this ==== satisfies ====.====An alternative when the sample space is discrete, say ====, with ==== satisfying equation ====, is given by==== where ====, and ==== is to be chosen. So ==== for ====. The choice of ==== is easy to set; as large as possible so long as computations required to sample ==== remain time feasible. So note that with this transition density there is no possibility for the sampler to get stuck and neither is there an accept/reject component. Note also that ==== only needs to be known up to a normalizing constant, a strong requirement in any sampler as often the target density is only specified up to an unknown normalizing constant. Finally, note that ==== is easy to sample. A multivariate version of ====.====The aim in the present paper is to find a continuous counterpart to ====,==== where here we have ==== and ====. Here we have re–written ==== as ==== in recognition that while originally ==== was an integer, it now can be any non–negative value. Just as ==== so clearly ==== is the required marginal density. Then ==== is given by ====, where ==== is uniform on the interval ====. Further, ====. The only outstanding question is how to sample ====, which is the main focus of the paper. For this we use the slice sampler, but the existence of the framework already established means we can avoid some of the difficulties associated with the slice sampler. This is important as there are some issues which make the slice sampler prohibitively slow in high dimensional problems.====In section ==== the aim is to show how to sample from ==== given by ==== but with necessary extensions involving making ==== random. This also requires some further latent variable; specifically a “slice” variable, similar in spirit to ====, ==== and ==== show that slice samplers are nearly always geometrically ergodic while ==== provide sufficient conditions for a slice sampler to be uniformly ergodic. On the other hand, ====Recent uses of Neal's approach include the elliptical slice sampler, see ====, the generalized elliptical slice sampler, see ====, and factor slice sampling, see ====. Once the slice variable has been incorporated within ====, it is then possible to compare the new sampler with Neal's slice sampler. Indeed, as it stands with ==== fixed, it is precisely a version of Neal's algorithm. Both us and Neal extend from this fixed ====, but in different directions. Neal adopts the reversible framework while we adopt a random ==== approach and use the framework established by the joint density ====. This allows us to maintain a Gibbs sampling framework while avoiding a slow (i.e. doubling/stepping out) detailed balance constraint. We make a direct comparison with Neal's slice sampler in section ====. Numerous illustrations are presented in section ==== and section ",A latent slice sampling algorithm,https://www.sciencedirect.com/science/article/pii/S0167947322002328,28 October 2022,2022,Research Article,73.0
"Li Xun-Jian,Sun Yuan,Tian Guo-Liang,Liang Jiajuan,Shi Jianhua","Department of Statistics and Data Science, Southern University of Science and Technology, Shenzhen 518055, Guangdong Province, PR China,Department of Statistics and Data Science, Guangdong Provincial Key Laboratory of Interdisciplinary Research and Application for Data Science, BNU–HKBU United International College, Zhuhai 519087, Guangdong Province, PR China,School of Mathematics and Statistics, Minnan Normal University, Zhangzhou 363000, Fujian Province, PR China","Received 13 December 2021, Revised 26 September 2022, Accepted 19 October 2022, Available online 26 October 2022, Version of Record 28 October 2022.",https://doi.org/10.1016/j.csda.2022.107650,Cited by (0),"Zero-truncated count data (e.g., days of staying in hospital; survival weeks of female patients with breast cancer) often arise in various fields such as medical studies. To model such data, the ==== (MLEs) of ====. This paper aims to introduce a new mean regression model for the ZTP distribution with a clear interpretation about the regression coefficients. Because of a challenge that the original Poisson mean parameter ==== cannot be expressed explicitly by the ZTP mean parameter ","Newton's iteration ==== works only when ==== for each ====-th round iteration because there exists a unique positive root for the equation ====, where ==== is a fixed number and ==== is a given ====-dimensional vector in ====. Note that==== because==== for any ====. So, for a given ==== satisfying ====, if ==== holds for each ====-round iteration, then the convergence of the Newton iteration ==== can be guaranteed.====In fact, we have==== Hence, we only need to prove that==== which is equivalent to the following inequality==== if we set ==== and ====.====For any fixed ====, define ====, then we obtain==== indicating that ==== is a strictly increasing function of ==== for any ====. On the other hand, since the right limit of ==== at the original point is==== we have ====. So we have proved the fact that ==== as long as ====. Combined with the fact that there exists a unique positive root for equation ====, the Newton iteration ==== will always converge to its unique positive root provided that the initial value ==== is chosen larger than ====.",Mean regression model for the zero-truncated Poisson distribution and its generalization,https://www.sciencedirect.com/science/article/pii/S0167947322002304,26 October 2022,2022,Research Article,74.0
"Hao Meiling,Lin Yuanyuan,Shen Guohao,Su Wen","School of Statistics, University of International Business and Economics, Beijing, China,Department of Statistics, The Chinese University of Hong Kong, Hong Kong,Department of Applied Mathematics, The Hong Kong Polytechnic University, Hong Kong,Department of Statistics and Actuarial Science, University of Hong Kong, Hong Kong","Received 21 May 2022, Revised 13 October 2022, Accepted 14 October 2022, Available online 26 October 2022, Version of Record 8 November 2022.",https://doi.org/10.1016/j.csda.2022.107645,Cited by (0),"This paper studies the global estimation in semiparametric ==== regression models. For estimating unknown functional parameters, an integrated quantile regression loss function with penalization is proposed. The first step is to obtain a vector-valued functional Bahadur representation of the resulting estimators, and then derive the ====. Extensive simulation studies demonstrate the effectiveness of the proposed method, and applications to the real estate dataset and world happiness report data are provided.","Quantile regression has been gaining much popularity as an alternative to linear regression for its far-reaching applications in numerous disciplines such as econometrics, genetics, and social sciences (====; ====; ====). Let ==== denote the response variable and ==== represent a ==== given ==== is ====. For a given quantile level ====, ==== defined the quantile regression model as follows:==== where ==== is the ==== under the linearity assumption. Refer to ==== and ==== for a comprehensive review on quantile regression.====Following the pioneering work by ====, numerous extensions and enhancements have been developed to address specificities in various types of datasets and tackle different issues. Under the framework of linear models, ==== (====, ====) incorporated the adaptive L-estimators and subsequently proposed algorithms to significantly shorten computational time; ==== explored the kernel weighted local linear fitting. Moreover, ==== and ==== proposed estimation approaches to tackle issues arising from potential quantiles crossing, while ==== and ==== formulated various tests for quantile regression models. ====, ====, ====, ====, ====, ====, and ====. Composite quantile regression has been explored by ====, ====, ====, ====; ====; ====; ====; ====; ====) as well as high dimensional settings (e.g., ====; ====; ====; ====; ====; ====).==== established weak convergence of quantile regression process (QRP) for nonparametric and semiparametric estimators. ====, ====, ====, ====, ====. ==== (====, ====) and ====When model ==== is assumed for all ====, such a globally concerned quantile regression with quantile regression function ==== defined on ==== suffices to characterize the conditional distribution of ==== given ==== and ====The rest of the paper is organized as follows. Section ==== provides an estimation approach for unknown functional quantile regression coefficient. Section ==== discusses the inference procedure about functional coefficients with the resampling approach. Section ==== reports some results from simulation studies conducted for evaluating the proposed method. An application to a world happiness report dataset is provided in Section ====, and some concluding remarks are given in Section ====. All technical proofs are given in the online Supplementary Material.====The following is the Supplementary material related to this article.",Nonparametric inference on smoothed quantile regression process,https://www.sciencedirect.com/science/article/pii/S0167947322002250,26 October 2022,2022,Research Article,75.0
"Chen Sixia,Haziza David","Department of Biostatistics and Epidemiology, University of Oklahoma Health Sciences Center, 801 NE 13th ST, Oklahoma City, 73104, OK, USA,Department of Mathematics and Statistics, University of Ottawa, 150 Louis-Pasteur Private, Ottawa, K1N 6N5, Ontario, Canada","Received 17 September 2021, Revised 15 October 2022, Accepted 16 October 2022, Available online 21 October 2022, Version of Record 26 October 2022.",https://doi.org/10.1016/j.csda.2022.107646,Cited by (0),Missing data occur frequently in practice. Inverse ,". In multiple imputation, a missing value is replaced by ==== replacement values, leading to ====. In the present article, we consider fractional imputation (FI) (====; ====; ====) as an alternative to multiple imputation. The rationale behind FI consists of using more than one donor for a recipient, whereby each donor is assigned a fractional weight.====With any imputation procedure, one specifies an outcome regression model, describing the relationship between the variable ==== requiring imputation and a set of fully observed ====. The resulting estimator is consistent provided that the specified model holds; otherwise, it may suffer from an appreciable bias. In semi-parametric imputation, the imputer specifies the first moment of the outcome regression model, ====, only. The resulting estimator is consistent provided that the first moment of the outcome regression model is correctly specified. In this article, we focus on semi-parametric fractional imputation (====).====; see also ====, (====, ====), ====, and ====. The rationale behind these procedures is to postulate a set of candidate outcome regression models and/or a set of candidate nonresponse models. Each model is fitted, leading to multiple predictions of the ==== is large. In this case, it is common practice to perform some variable selection prior to imputation and impute the missing data according to the selected model. However, different variable selection methods and/or different tuning parameters may produce multiple candidate models. With multiply robust procedures, there is no need to select one model among the candidate models, as all of them can be incorporated in the imputation procedure.====. The resulting estimators are vulnerable to model misspecification. In this paper, we propose multiply robust procedures for estimating any parameter that can be defined as the solution of an estimating equation. This includes means, regression coefficients, and quantiles. Unlike the procedures of ====, our procedures are semi-parametric in the sense that they only require the correct specification of the first moment of the outcome regression model. We mainly focus on continuous outcome variables. The extension to categorical outcome variables is briefly discussed in ====. Note that the concept of multiply robustness considered in this paper differs from the one in ==== and ==== for estimators of functionals defined by the longitudinal g-computation formula of ====.====The paper is organized as follows. In section ====, we introduce the basic setup. The proposed methods are presented in Section ====. In Section ====. In Section ====, we make some final remarks. The technical proofs are relegated to the Appendix.====Denote==== Define==== and==== with ====. Also, define ====, ====, ====, ====, ====,==== where ====,==== where ====. Let==== and==== with ====, or==== and==== with ====, or==== and==== with ====. One can write ====.====The following regularity conditions are assumed for establishing ==== in Section ==== and ==== and ==== in Appendix ==== and Appendix ====.",A unified framework of multiply robust estimation approaches for handling incomplete data,https://www.sciencedirect.com/science/article/pii/S0167947322002262,21 October 2022,2022,Research Article,76.0
"Yang Yihe,Dai Hongsheng,Pan Jianxin","Mathematical College, Sichuan University, Chengdu 610065, China,Department of Mathematical Sciences, University of Essex, UK,Research Center for Mathematics, Beijing Normal University, Zhuhai 519087, China,Guangdong Provincial Key Laboratory of Interdisciplinary Research and Application for Data Science, BNU-HKBU United International College, Zhuhai 519087, China","Received 8 December 2020, Revised 28 September 2022, Accepted 1 October 2022, Available online 14 October 2022, Version of Record 18 October 2022.",https://doi.org/10.1016/j.csda.2022.107630,Cited by (0),A method that estimates the ,"Estimation of the precision matrix ====). A good estimation for ==== is essential in many scientific areas such as gene expression studies (====), financial portfolio investment studies (====). Nowadays obtaining reliable estimation for ==== becomes extremely challenging in these areas because of the ultrahigh dimensionality of datasets studied. Especially, in some specific fields like genetic analysis, the number of individuals (sample-size ====) is much smaller than the number of variables (dimension ====), making the estimation of precision matrix extremely challenging.====-penalized optimization, originally pioneered by ====, is a standard tool to impose sparsity on the precision matrix estimator. ==== and ==== and ==== further analysed the convergence rate and sign consistency of the penalized MLE, respectively. ==== suggested estimating ==== proposed a neighborhood selection approach to estimate ==== through column by column. ==== turned the lasso regularizer (====) in the neighborhood selection to Dantzig selector (====). ==== provided an alternative column-wise estimation of ====, which is known as constrained ==== further improved the CLIME for better computational performance.====Another major breakthrough was made by ====, which combined the penalized MLE and the neighborhood selection approach and introduced a coordinate descent algorithm known as graphical lasso (glasso) in order to find the MLE of ==== efficiently. ==== demonstrated via statistical theories and numerical examples that the bias resulted by the graphical lasso algorithm can be attenuated when using concave penalty (====) and adaptive lasso (====) rather than lasso. Moreover, there is a vast of literature that focused on improving the computational performance of glasso. For example, ==== and ====, which leads to significant computational improvement because it only needs to apply graphical lasso to each block of the block-diagonal matrix.====This paper concentrates on a more extreme scope of precision matrix estimation, for example, a genetic study with data having several thousands and even millions of gene expressions but with a sample-size of just several dozens. Almost all the aforementioned methods are struggling to solve problems in this more extreme scope. For instance, the ====-penalized estimations such as the glasso and Witten's method need to specify a tuning parameter in lasso penalty, but the empirical rule such as cross-validation for choosing the tuning parameter is unstable when the sample-size is small. Another issue is that there are multiple saddle points in the ====-penalized optimization when sample-size is small, which makes the associated algorithm numerically unstable. Moreover, the computing time of the existing algorithms increases dramatically as the dimension of ==== diverges.====To address these challenges, we propose the block-diagonal (BD) regularization, which identifies a small sub-group of significantly correlated variables from thousands of variables and vanishes all the insignificant correlations of variables outside this sub-group. We then approximate the precision matrix by using a BD matrix composed of two blocks, where the low-dimensional block corresponds to this small sub-group of the significantly correlated variables, while the rest high-dimensional block is diagonal. As a result, the estimation of the high-dimensional precision matrix reduces to the estimation of the low-dimensional block plus the high-dimensional but diagonal block.====An identification procedure called covariance column-wise screening (CCS) is proposed to find the small sub-group of highly correlated variables. Consider a multivariate variable ====. The CCS procedure uses the ==== to indicate the significance of the ====th component ====, where ==== is an initial covariance matrix estimator. We retain the dependencies of components that have significantly large screening statistics ====), distance correlation coefficient (====), and ball correlation coefficient (====), can be employed to measure the importance of certain variables. The statistical interpretation of ==== is clear: the expectation of ==== is the sum of non-diagonal entries in the ====th column of ====. If a component ==== is independent of the others, ==== has an expectation of zero. Hence, ==== is an appropriate quantity to measure the importance of the ====th component ==== in our BD regularization.====Compared with the existing approaches, the BD regularization has the following advantages. First, the BD regularization is not sensitive to the choice of involved tuning parameters. Whereas, almost all the precision matrix estimations based on ====-penalization encounter numerical instability, due to the fact that different tuning parameters in the ====-penalty may lead to different results. Besides, the BD regularization is able to produce a consistent precision matrix estimator, while the covariance-insured screening method (====) only answers which entries in the precision matrix are non-zero. Furthermore, our method can identify the sub-group of highly correlated variables by using an initial covariance matrix estimator ====. Although ==== proposed an analogous method to our BD regularization, their method has a major constraint that the sub-group of highly correlated variables must be known in advance.====The rest of this paper is organized as follows. In Section ====, we introduce the BD regularization of the precision matrix in detail and investigate the CCS identification procedure. In Section ====, we provide the simulation studies. In Section ====, we apply the proposed approach to a real data analysis. Discussion is given in section ====, and all technical proofs are relegated to the Appendix.====The following is the Supplementary material related to this article.",Block-diagonal precision matrix regularization for ultra-high dimensional data,https://www.sciencedirect.com/science/article/pii/S0167947322002109,14 October 2022,2022,Research Article,77.0
"Zhao Xin,Zhang Jingru,Lin Wei","School of Mathematical Sciences and Center for Statistical Science, Peking University, Beijing 100871, China,Department of Biostatistics, Epidemiology and Informatics, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA 19104, USA","Received 21 August 2021, Revised 19 August 2022, Accepted 5 October 2022, Available online 13 October 2022, Version of Record 17 October 2022.",https://doi.org/10.1016/j.csda.2022.107634,Cited by (0)," generally results in a nonconvex loss function, which does not fit into the existing convex clustering framework. Moreover, prior knowledge of a network over the samples, often available from citation or similarity relationships, is not taken into account. We introduce Dirichlet-multinomial network fusion (DMNet) for clustering multivariate count data, which models the samples via Dirichlet-multinomial distributions with individual parameters and employs a weighted group ","-means clustering (====). See, for example, ==== for an overview.====; ====; ====; ====Inspired by the success of the fused Lasso (====; ====; ====). By using a (group) ==== problems related to linkage-based hierarchical clustering and ====) and strong theoretical guarantees (====; ====; ====; ====). A similar model-based clustering approach has also been proposed by combining the ====In addition to the observations, prior knowledge of a network over the samples is often available in practice, which may provide useful information for the clustering problem. For instance, in bibliometrics, citations between two papers would suggest similar topics and hence memberships to the same cluster; in gut microbiome studies, similarities on dietary patterns would indicate that two microbiomes tend to be clustered in the same enterotype (====). In principle, such prior network information can be incorporated into the convex clustering framework by appropriately choosing the affinity parameters or weights in the adaptive ==== fusion penalty (====). ====To bridge the gap in the literature, we introduce Dirichlet-multinomial network fusion (DMNet) for clustering multivariate count data. The method models the samples via DM distributions with individual parameters and employs a weighted group ==== and one novel, to the DM distribution. This results in two convex formulations, DMNet+ and DMNet++, which are amenable to efficient optimization and theoretical analysis. We derive an ADMM algorithm for implementing the proposed methods. Moreover, we establish theoretical guarantees for DMNet++ in terms of nonasymptotic error bounds, which shed light on the role of the network structure in the clustering problem.====The rest of the article proceeds as follows. Section ==== introduces our model and methodology. Section ==== describes the algorithms for the proposed methods. Nonasymptotic theory for DMNet++ is presented in Section ====. Simulation studies and two real data applications are given in Sections ==== and ====, respectively. Section ==== concludes the article with some discussion. Proofs of theoretical results are provided in the Appendix.",Clustering multivariate count data via Dirichlet-multinomial network fusion,https://www.sciencedirect.com/science/article/pii/S0167947322002146,13 October 2022,2022,Research Article,78.0
"Welz Thilo,Viechtbauer Wolfgang,Pauly Markus","TU Dortmund University, Vogelpothsweg 87, 44221, Dortmund, Germany,Maastricht University, Vijverdalseweg 1, 6226 NB, Maastricht, the Netherlands,UA Ruhr, Research Center Trustworthy Data Science and Security, 44227, Dortmund, Germany","Received 22 April 2022, Revised 1 October 2022, Accepted 2 October 2022, Available online 13 October 2022, Version of Record 17 October 2022.",https://doi.org/10.1016/j.csda.2022.107631,Cited by (1),"Meta-analyses frequently include trials that report multiple outcomes based on a common set of study participants. These outcomes will generally be correlated. Cluster-robust variance-covariance estimators are a fruitful approach for synthesizing dependent outcomes. However, when the number of studies is small, state-of-the-art ==== can yield inflated Type 1 errors. Therefore, two new cluster-robust estimators are presented, in order to improve small sample performance. For both new estimators the idea is to transform the estimated variances of the residuals using only the diagonal entries of the ====. The proposals are asymptotically equivalent to previously suggested cluster-robust estimators such as the bias reduced linearization approach. The methods are applied to a dataset of 81 trials examining overall and disease-free survival in neuroblastoma patients with amplified versus normal MYC-N genes. Furthermore, their performance is compared and contrasted in an extensive simulation study. The focus is on ==== meta-regression, although the approaches can be applied more generally.","In psychometric and medical research, studies frequently report multiple dependent outcomes. These outcomes can be synthesized across studies, while incorporating study level moderators, via multivariate meta-regression (====; ====). This is a more sophisticated approach than averaging the effects within studies to create aggregate outcomes, which are then synthesized. A fruitful approach to achieve reliable inference in the case of a multivariate meta-regression is to use a cluster-robust (CR) variance-covariance estimator (====).==== and later extended in ==== and ====, were first proposed in the meta-analytic literature by ====) and mixed-effect meta-regression (====; ====; ====; ====; ====). Therefore it is recommended to instead use one of various improvements that have been suggested. We discuss some of these, such as the bias reduced linearization approach and ==== as introduced in ====, as well as two new proposals in Section ====. In Section ====, we describe multiple CR estimators, including two new suggestions ==== and ====. In Section ==== describes the design and results of our simulation study. We close with a discussion of the results and an outlook for future research (Section ====).====The following is the Supplementary material related to this article.",Cluster-robust estimators for multivariate mixed-effects meta-regression,https://www.sciencedirect.com/science/article/pii/S0167947322002110,13 October 2022,2022,Research Article,79.0
"Kim Seungkyu,Park Seongoh,Lim Johan,Lee Sang Han","Department of Statistics, Seoul National University, Seoul, Republic of Korea,School of Mathematics, Statistics and Data Science, Sungshin Women's University, Seoul, Republic of Korea,The Nathan Kline Institute, NY, USA,Department of Child and Adolescent Psychiatry, New York University School of Medicine, NY, USA","Received 3 December 2021, Revised 3 October 2022, Accepted 4 October 2022, Available online 13 October 2022, Version of Record 18 October 2022.",https://doi.org/10.1016/j.csda.2022.107633,Cited by (0), data.,", ====, and ====, or a book ==== for general introduction.====). In ==== where systolic and diastolic blood pressures are repeatedly measured in a prospective study, multi-dimensional variables and time (or trial) are two related factors. In genetics, a single instance of matrix-variate data is commonly observed with non-trivial correlation structures among genes and among samples (====; ====), and there is also a growing interest in robust estimation on it.==== study a bigraphical model using a (matrix-variate) nonparanormal distribution, which is more robust at data-generation by nature than the Gaussian bigraphical model. ==== and ==== explore Tyler's M estimator under separability (with/without a rank constraint) and their algorithmic properties (e.g. global convergence of geodesically convex estimators). ====The question of our interest is whether a scatter (shape) matrix ==== and ====. This hypothesis can be formally described by==== This has constantly attracted attention of statisticians, but mostly depending on restrictive assumptions on the underlying distribution. ==== and ====). ====The remainder of this paper is organized as follows. Section ==== to verify our methods and apply them to real data in Section ====. Finally, we conclude with discussion this paper in Section ====.====The following is the Supplementary material related to this article.",Robust tests for scatter separability beyond Gaussianity,https://www.sciencedirect.com/science/article/pii/S0167947322002134,13 October 2022,2022,Research Article,80.0
"Zhao Yi,Luo Xi","Department of Biostatistics and Health Data Science, Indiana University School of Medicine, United States of America,Department of Biostatistics and Data Science, School of Public Health, The University of Texas Health Science Center at Houston, United States of America","Received 19 January 2022, Revised 29 August 2022, Accepted 15 September 2022, Available online 12 October 2022, Version of Record 14 October 2022.",https://doi.org/10.1016/j.csda.2022.107623,Cited by (1), for implementation is available on CRAN.,") and its extensions (====The assumptions involved to perform mediation analysis have been widely studied in the literature, usually, for one-level experimental studies. See ==== and ==== for a review and many references therein. Three critical assumptions in mediation analysis are the no unmeasured exposure-outcome confounding, no unmeasured exposure-mediator confounding, and no unmeasured mediator-outcome confounding assumptions. Existing literature aims to seek unbiased estimate of the causal effects by relaxing one or multiple of these assumptions. In a recent study, ==== introduced a nonparametric approach for estimating a new type of indirect effect, the population intervention indirect effect, that allows the presence of an unmeasured common cause of the exposure and outcome, but still assumes no unmeasured confounding between the mediator and outcome. This paper aims to investigate the settings under which the causal effects remain identifiable without the no unmeasured mediator-outcome confounding assumption.====The motivating example of the existence of unmeasured mediator-outcome confounding and multilevel data comes from an fMRI experiment. Scientists are interested in how the stimulus is processed in the human brain across the population. In particular, based on the prior findings (====; ====) or apply the instrumental variable (IV) approach under additional structural assumptions (====; ====). ==== applied the IV approach for an fMRI dataset when the outcome variable is outside the brain, where the structural assumptions are more likely to hold.====Recently, mediation analysis has been extended to hierarchically organized experiments, partly due to the increasing popularity of such experiments. These approaches are generally developed for two-level data in practical settings. The first-level data are usually modeled following the Baron-Kenny method, and various second-level models are introduced for the parameters (see ====; ====, for example). It has not been rigorously studied if the resulting parameters have causal interpretation and suffers from similar limitations of assuming no unmeasured mediator-outcome confounding.====). Asymptotic analyses show that the estimators are consistent with the parametric rates. Third, efficient computational algorithms are developed to compute for a large number of parameters.====This paper is organized as follows. We introduce the multilevel SEMs for mediation analysis with correlated errors in Section ====. In Section ==== (and Section E of the supplementary materials) and compare the analysis results of different methods in the fMRI data application in Section ====. Section ==== summarizes the paper with discussions.====The following is the Supplementary material related to this article.",Multilevel mediation analysis with structured unmeasured mediator-outcome confounding,https://www.sciencedirect.com/science/article/pii/S0167947322002031,12 October 2022,2022,Research Article,81.0
"An Hyowon,Zhang Kai,Oja Hannu,Marron J.S.","The University of North Carolina at Chapel Hill, Chapel Hill, NC, USA,University of Turku, 20500 Turku, Finland","Received 16 October 2021, Revised 1 July 2022, Accepted 3 October 2022, Available online 6 October 2022, Version of Record 26 October 2022.",https://doi.org/10.1016/j.csda.2022.107632,Cited by (1),"An important challenge in big data is identification of important variables. For this purpose, methods of discovering variables with non-standard univariate marginal distributions are proposed. The conventional moments based summary statistics can be well-adopted, but their sensitivity to outliers can lead to selection based on a few outliers rather than distributional shape such as bimodality. To address this type of non-robustness, the L-moments are considered. Using these in practice, however, has a limitation since they do not take zero values at the ==== to which the shape of a marginal distribution is most naturally compared. As a remedy, Gaussian Centered L-moments are proposed, which share advantages of the L-moments, but have zeros at the Gaussian distributions. The strength of Gaussian Centered L-moments over other conventional moments is shown in theoretical and practical aspects such as their performances in screening important genes in cancer genetics data.","Data quality is an issue that is currently not receiving as much attention as it deserves in the age of big data. Traditional analysis of small data sets involves a study of marginal distributions, which easily finds data quality challenges such as skewness and suggests remedies such as the Box-Cox transformation (====Conventional summaries such as the ==== ==== and ==== ==== defined as==== respectively, can be very useful for this screening process. However, as studied in ====, ==== and ====In this paper, the limitation of conventional summary statistics is demonstrated using a modern high dimensional data set from cancer research. These data are part of the TCGA project (====), and the precise version of the data here was used in ====, the sheer data size means that there have only been cursory studies of the marginal, or individual gene, distributions. In this study, we conduct a much deeper search for genes with unexpected marginal structure such as skewness and kurtosis. This can yield relationships between genes and biologically meaningful features such as breast cancer subtypes.==== shows that even though the genes with the smallest sample skewness values were screened, the genes such as CSTF2T, C1orf172 and BET1L have a couple of outliers on their left sides rather than distributional skewness to the left. The sample skewness seems inadequate for effectively screening interesting genes in terms of distributional skewness. This challenge is well addressed using the ==== proposed in ==== as shown in the bottom two rows of ====. All the genes screened by the sample L-skewness except GSTT1 have clusters of Basal-type samples (blue triangles). These show that the sample L-skewness screens genes with distributional skewness especially coming from cancer subtypes, which is desirable since the former is clinically much more important than the latter.====, the population ====These ideas motivate development of improved Gaussian centered versions of the L-moments. The unpublished results ==== discovered a variant of the L-moments called the ====Another simple approach to shift the center of the L-moments is to subtract the population ==== (====) value at the Gaussian distributions from itself. However, this can result in loss of theoretical soundness of the definition of the L-moments since they are no longer differences of expected order statistics as in Equation (2.1) of ====. We propose an alternative definition of the L-moments by which the numerical subtraction can be understood as a theoretically principled shift of the zeros of the L-moments to the Gaussian distributions.====These ==== are developed in Section ==== as new univariate summaries. Their theoretical properties such as consistency and robustness are studied in Sections ==== and ====, respectively. Their abilities to screen variables with interesting marginal distributions are quantitatively analyzed in Section ====. A computational study that shows a strength of a proposed estimator is given in Section ====. Section ==== summarizes our findings and presents potential future considerations.====The following is the Supplementary material related to this article.",Variable screening based on Gaussian Centered L-moments,https://www.sciencedirect.com/science/article/pii/S0167947322002122,6 October 2022,2022,Research Article,82.0
"Pereira Luz Adriana,Gutiérrez Luis,Taylor-Rodríguez Daniel,Mena Ramsés H.","Escuela de Estadística, Universidad del Valle, Cali, Colombia,Departamento de Estadística, Pontificia Universidad Católica de Chile, ANID–Millennium Science Initiative Program–Millennium Nucleus Center for the Discovery of Structures in Complex Data, Avenida Vicuña Mackenna 4860, Santiago, Chile,Department of Mathematics and Statistics, Portland State University, Portland, USA,Departamento de Probabilidad y Estadística, IIMAS-UNAM, Mexico","Received 26 January 2022, Revised 26 September 2022, Accepted 28 September 2022, Available online 5 October 2022, Version of Record 12 October 2022.",https://doi.org/10.1016/j.csda.2022.107629,Cited by (0),A ,"In ====Let ==== be the ====th time-dependent response for the ====th, where the time indexes ====, ====, ==== are values in the set ====, not necessarily equally spaced. Furthermore, let ====th experimental unit. The most commonly used approach for analyzing longitudinal data is to assume ====, where ==== and ==== denote the fixed and random effects associated to ==== and ====, which are partitions of ====. In such case, ====). Under this scenario, the inference is based on the marginal model (====), which is obtained by integrating out the random effects. As a consequence, research questions are typically about the influence of the fixed effects over the response.====. Although there are methods to estimate the number of degrees of freedom (====; ====); such methods can lead to notable differences in the corresponding p-values as they exhibit a dependence on the significance level ====). Such difference in the p-values is perhaps one of the reasons why libraries as ==== (====).====The literature on longitudinal data analysis is vast, see, for example, ====−values, the classical mixed model has been extended. ==== proposed to use a semi-parametric smooth density for the random effects, which can be skewed. In the same spirit, ==== relaxed the Gaussian assumption considering skew-normal distributions for the random effects and the errors. An extension to the skew Student-t distribution for the random effects was proposed by ====, and ==== who proposed to model the random effects and the errors terms with skew-elliptical distributions.==== relaxes the distribution of the random effects using mixtures of Dirichlet processes. Other constructions using Dirichlet processes under different parameterizations have also been studied in ====; ====; ====; ====; ====; ==== and ====. Most of these works are mainly devoted to proposing flexible models for longitudinal data and do not consider hypothesis tests as a final objective. However, in biomedical research concerning longitudinal studies, hypothesis testing is of primary interest. Unfortunately, the literature on flexible models that can perform model selection is still insufficient. Some exceptions are the works of ====In this work, we consider the time evolution of longitudinal data using a Markovian Dependent Dirichlet processes (DDP) (====, ====; ====). In particular, our proposal considers the correlation between repeated measurements and provides a hypothesis testing procedure to find possible effects of the predictors on the response variable. Specifically, the time dynamics enter through the underlying stick-breaking weights of the DDP species sampling representation, using the approach proposed by ====; ====; ====; ====). Indeed, we use the prior specification introduced by ==== to test both main effects and interactions, importantly, preserving the polynomial hierarchy among predictors.====In Section ==== we give a description of the proposed model, including a description of the prior over the space of hypotheses and the construction of the DDP. Throughout Section ====, in particular to contrast the power of our model to that of other, more traditionally, alternatives. Finally, in Section ====, we used the proposed method to analyze the efficacy of the mask-use and vaccination over the monthly average number of confirmed COVID-19 cases, in some counties of the United States. The closing Section ==== gives some concluding remarks. Proofs, the posterior inference algorithm, Web Figures, Web Tables and an ==== example are given in the Supplementary Material.====The following is the Supplementary material related to this article.",Bayesian nonparametric hypothesis testing for longitudinal data analysis,https://www.sciencedirect.com/science/article/pii/S0167947322002092,5 October 2022,2022,Research Article,83.0
"Schmid Lena,Gerharz Alexander,Groll Andreas,Pauly Markus","Department of Statistics, TU Dortmund University, 44227 Dortmund, Germany,UA Ruhr, Research Center Trustworthy Data Science and Security, 44227 Dortmund, Germany","Received 14 January 2022, Revised 23 September 2022, Accepted 27 September 2022, Available online 4 October 2022, Version of Record 12 October 2022.",https://doi.org/10.1016/j.csda.2022.107628,Cited by (3),"Tree-based ensembles such as the ==== are modern classics among statistical learning methods. In particular, they are used for predicting univariate responses. In case of multiple outputs the question arises whether it is better to separately fit univariate models or directly follow a multivariate approach. For the latter, several possibilities exist that are, e.g. based on modified splitting or stopping rules for multi-output regression. These methods are compared in extensive simulations and a real data example to help in answering the primary question when to use multivariate ensemble techniques instead of univariate ones."," output variables, say ====. Then, we are often interested in finding a functional relationship between the output ==== and some feature variables ====), which also includes multiple features ====; ====; ====; ====; ====; ====; ====), there do not exist exhausting studies that exploit the potential of multivariate regression methods for prediction.==== The answer to this question is not clear as of now. In fact, univariate tree-based ensembles as the Random Forest (====) or Extra Trees (====) have been shown to be good predictive tools in various applications (====; ====; ====; ====; ====) and are applied far more often than multivariate regression approaches. One reason for this could be that there are still only a few multivariate extensions, such as (====; ====; ====; ====; ====), where the impurity measure used in the tree construction was multivariatly extended. ==== developed an impurity function based on a generalized entropy criterion to handle multiple binary outputs, this work was extended by ==== to ordinal outputs by transforming the outputs to binary-valued indicator functions. ====One of the first approaches for multi-output regression trees was proposed by ====, which extended CART to multi-output regression by using the sum of squared errors over the multivariate outputs as the impurity function of a node (==== also developed more general forms of distance-based impurity functions). This method is implemented in the R-package ==== (====) and was used by ==== to develop multivariate Random Forests. They have also adapted the method for an application to identify yeast transcriptional regulation networks ====. ==== and ==== (====) to construct multivariate Random Forests. Furthermore, ==== to develop regression trees for multivalued numerical outputs. ====. ====There are also several approaches that extended the GUIDE algorithm (====) to multivariate and longitudinal outputs (====; ====). ==== developed a method that can be used with multivariate outputs of any type. Here, GEE techniques are used to find the splits. Multivariate trees for a mixture of categorical and continuous outputs were presented by ====, where the splits are derived from a likelihood-based approach for a general location model. For Extra Trees, there is also an extension for multi-task learning (====), where the split criterion has been adapted to handle multiple tasks. The method is implemented in the R-package ==== (====). ==== proposed an extension of Extra Trees for multioutput regression based on the predictive clustering trees framework.====To answer the central research question, we compare the ==== of separate univariate analyses with a simultaneous multivariate analysis by means of exhaustive simulations and in an illustrative data analysis.====This work is structured as follows: Section ==== presents the univariate and multivariate ensemble methods. More precisely, univariate and multivariate Random Forest and Extra Trees algorithms are presented. In addition, the multi-task Extra Trees algorithm ==== is described in more detail. The simulation design and framework are then presented in Section ====, while Section ==== summarizes the main simulation results. In Section ====, illustrative real-world data examples are shown before the manuscript concludes with a discussion of our findings and an outlook for future research (Section ====).",Tree-based ensembles for multi-output regression: Comparing multivariate approaches with separate univariate ones,https://www.sciencedirect.com/science/article/pii/S0167947322002080,4 October 2022,2022,Research Article,84.0
"Jeong Kuhwan,Chae Minwoo,Kim Yongdai","Samsung Advanced Institute of Technology, South Korea,Department of Industrial and Management Engineering, Pohang University of Science and Technology, South Korea,Department of Statistics, Seoul National University, South Korea","Received 9 April 2021, Revised 19 September 2022, Accepted 25 September 2022, Available online 4 October 2022, Version of Record 11 October 2022.",https://doi.org/10.1016/j.csda.2022.107626,Cited by (0)," of the ====. We propose a novel mini-batch online learning algorithm based on assumed density filtering, which takes full advantage of available computing resources to improve performance and achieves better performances relative to existing online algorithms based on variational inference.",") is one of the most popular BNP models, and it is widely used for clustering and density estimation (====; ====; ====; ====; ====; ====; ====). Various inferential algorithms for the DP mixture model have been developed; however, the need to scale up these algorithms is growing, especially in recent times, as the size of data increases drastically.====; ====; ====; ====) and variational inference (VI) (====; ====). Notably, VI algorithms are deterministic, generally faster than MCMC algorithms, and can be scaled up based on stochastic VI algorithms (====; ====). They are also easily modifiable for online learning. However, the approximated posterior distribution could be highly biased compared to MCMC algorithms.====As an alternative to VI algorithms, in this study, we propose an online learning algorithm for the DP mixture model based on assumed density filtering (ADF) (====; ====; ====; ====). ADF seeks the best approximation to the posterior distribution in a predetermined class of distributions, called ==== for details.====; ====), and our theoretical comparison confirms this empirical phenomenon.====The second advantage of ADF is that the corresponding KL divergence can be numerically approximated if samples from the posterior distribution are available. To realize this advantage, we would use a prior distribution, where an efficient MCMC algorithm is available, to generate a sample from the posterior distribution. In this study, this prior distribution is said to be ====. We propose an online learning algorithm, called ==== (WCA), which approximates the current posterior distribution by a proxy distribution using ADF. Because the approximated posterior distribution is weakly conjugate, an MCMC algorithm can be used without difficulty in the processing of the newly arrived data.==== at a time, and they are not easily modifiable for mini-batch learning.====We consider a generalized DP (gDP) as a proxy distribution for the DP mixture model. A gDP is designed to add information about fixed components into the DP, and is already implicitly or explicitly considered as an approximation of the posterior distribution in existing VI-based online learning algorithms (====; ====; ====). A difference in the WCA from these algorithms is the method used to choose the optimal approximate distribution among gDPs.====Our work is closely related to ====, who applies the concept of WCA to hierarchical Dirichlet process (HDP) topic models. We not only consider the DP mixture model, which is more popularly used in various areas than the HDP topic model, but also develop a more refined online learning algorithm that resolves the label-switching problem existing in ====.====The remainder of this paper is organized as follows: We review existing sequential learning algorithms based on VI in Section ====. In Section ====, the proposed WCA algorithm is comprehensively explained. First, we develop a sequential version of WCA and compare it with the VI algorithms. Thereafter, we propose a WCA algorithm for mini-batch learning. The numerical results on simulated and real datasets are presented in Section ====, while the concluding remarks are given in Section ====.",Online learning for the Dirichlet process mixture model via weakly conjugate approximation,https://www.sciencedirect.com/science/article/pii/S0167947322002067,4 October 2022,2022,Research Article,85.0
"Wang Xiaodi,Huang Hengzhen","School of Statistics and Mathematics, Central University of Finance and Economics, Beijing, China,School of Mathematics and Statistics, Guangxi Normal University, Guilin, China","Received 5 August 2021, Revised 23 July 2022, Accepted 25 September 2022, Available online 3 October 2022, Version of Record 12 October 2022.",https://doi.org/10.1016/j.csda.2022.107625,Cited by (0), Latin ==== on benchmark test problems demonstrate that the proposed design is effective in shrinking the estimation bias and volatility with economically feasible sample sizes.,"; ==== (GSA, ====), which aims to identify the most contributing input factors and their interactions to the output behavior of a complex mathematical model. By fixing unessential input variables at any values in their domains, the input space for analysis will shrink.====To perform GSA, the experimenter does not need to know detailed information of the model but only output values evaluated from the model. Only limited evaluations can be used for GSA because each run of the model can be expensive. During the past decade, the GSA has been successfully applied in the fields of environmental assessment (====), aircraft infrared signature (====) and drug combination studies (====), among others. Despite significant advances in many scientific investigations, conventional GSA methods like the well-known Sobol' sensitivity analysis (SSA, ====, ====, ====) may lose efficacy for moderate-to-low dimension situation. In such a situation, due to nonnegligible high-order interactions in the complex model (====), all input variables may be judged as influential, and the input dimension can not be reduced.====). In order-of-addition experiments, the experimental results have symmetry about the orders that ingredients are added, which may be used to reduce the number of tests (====; ====). By bringing symmetry in GSA, ==== and ==== advocated the ==== (SGSA) to study the symmetry of model. The SGSA method first decomposes the model ==== into summands of independent ====-ary functions, namely==== where each component function ==== describes a symmetrical characteristic for ====. Then ==== (SGSIs), defined as the ratios of variances computed from each ==== to the total variance of ====, are used to identify the influential component functions. Synthesizing the symmetry information of influential ====s yields the symmetry structure of model. It has been shown that the model complexity can be well simplified by SGSA when using conventional GSA may lose efficacy (====).====An important issue for implementing the SGSA method is the computation of SGSIs, which typically involves high-dimensional integrals. This problem is challenging for two reasons. First, the objective function is typically “black-box,” in that little is known on component functions prior to experiments. Because of this, the popular ways for addressing integral problems, such as quasi-Monte Carlo sampling and the Latin hypercube design (====; ====) are not applicable. Second, each evaluation of the function can be expensive, requiring time-consuming simulations. ==== proposed a ==== to estimate SGSIs, which imposes a group closed condition on designed vectors so that the analytic expressions of component functions are not required. ==== further defined an identifiable structure for the symmetrical design to reduce the chance of misjudgment of the influential SGSI with small sample sizes, and then developed an algorithm to construct such structure. The symmetrical design framework, however, restricts the inputs to a fixed number of levels. As will be shown in this paper, the estimated SGSIs by symmetrical design may be volatile, and have large biases even for large sample cases.====In this work, we propose a new type of design for SGSA, called ==== (GSLHD). Such a design is a symmetrical design that inherits the structure of a Latin hypercube. The notion “group” refers to a ==== in Algebra. Using this design, each factor takes ==== equally spaced values in its domain, where ==== is the run size. Due to this property, a GSLHD can shrink the estimation bias at a small sample size. A random procedure to construct GSLHDs is given based on a track decomposition of the design. By noting that a randomized GSLHD may have poor space-filling properties, we further propose a numerical algorithm to improve the space-filling properties of the GSLHDs. After implementing such an algorithm, the proposed GSLHDs may have three appealing features: (i) feasibility for estimating SGSIs directly by model outputs, (ii) optimal one-dimensional projection property, and (iii) a good space-filling property over the entire domain. These design features, especially (ii) and (iii), lead to accurate and robust estimates of SGSIs. The derived sampling properties and simulation results demonstrate the effectiveness of the proposed design when applying to the estimation of SGSIs. To perform a sensitivity analysis, the use of this design does not need a meta-modeling procedure, thus avoids a prior step of parameter estimation.====The remainder of this paper is organized as follows. Section ==== gives a review on symmetrical global sensitivity analysis and introduces the group symmetric Latin hypercube design. Section ==== provides a method to constructing GSLHDs. Section ==== presents a numerical algorithm for optimizing GSLHDs. Sampling properties of the designs are derived in Section ====. Section ==== uses two simulative examples to illustrate the effectiveness of the proposed design. Some concluding remarks are given in Section ====. All the proofs are deferred to the Appendix.",Group symmetric Latin hypercube designs for symmetrical global sensitivity analysis,https://www.sciencedirect.com/science/article/pii/S0167947322002055,3 October 2022,2022,Research Article,86.0
"Bhatnagar Sahir R.,Lu Tianyuan,Lovato Amanda,Olds David L.,Kobor Michael S.,Meaney Michael J.,O'Donnell Kieran,Yang Archer Y.,Greenwood Celia M.T.","Department of Epidemiology, Biostatistics and Occupational Health, McGill University, Montréal, Canada,Department of Diagnostic Radiology, McGill University, Montréal, Canada,Quantitative Life Sciences, McGill University, Canada,Lady Davis Institute, Jewish General Hospital, Montréal, QC, Canada,Statistics Canada, Ottawa, ON, Canada,Department of Pediatrics, University of Colorado School of Medicine, Denver, United States of America,Department of Medical Genetics, University of British Columbia, BC, Canada,Singapore Institute for Clinical Sciences, Singapore,Departments of Psychiatry and Neurology & Neurosurgery, McGill University, Canada,Department of Obstetrics, Gynecology and Reproductive Sciences, Yale School of Medicine, United States of America,Department of Mathematics and Statistics, McGill University, Canada,Departments of Oncology and Human Genetics, McGill University, Canada","Received 13 October 2020, Revised 16 September 2022, Accepted 17 September 2022, Available online 24 September 2022, Version of Record 12 October 2022.",https://doi.org/10.1016/j.csda.2022.107624,Cited by (1)," for detecting non-linear interactions with a key environmental or exposure variable in high-dimensional settings which respects the strong or weak heredity constraints is proposed. It is proven that asymptotically, ==== possesses the oracle property, i.e., it performs as well as if the true model were known in advance. A computationally efficient fitting algorithm with automatic tuning parameter selection, which scales to high-dimensional datasets is proposed. Simulation results show that ==== is applied to detect non-linear interactions between genes and a prenatal psychosocial intervention program on cognitive performance in children at 4 years of age. Results show that individuals who are genetically predisposed to lower educational attainment are those who stand to benefit the most from the intervention. The proposed algorithms are implemented in an R package available on CRAN (====).","Computational approaches to variable selection have become increasingly important with the advent of high-throughput technologies in genomics and brain imaging studies, where the data has become massive, yet where it is believed that the number of truly important variables is small relative to the total number of variables. Although many approaches have been developed for main effects, there is an enduring interest in powerful methods for estimating interactions, since interactions may reflect important modulation of a genomic system by an external factor and vice versa (====).====Interactions may occur in numerous types and of varying complexities. In this paper, we consider one specific type of interaction model, where one exposure variable ==== is involved in possibly non-linear interactions with a high-dimensional set of measures ==== leading to effects on a response variable, ====. We propose a multivariable penalization procedure for detecting non-linear interactions between ==== and ====. Our method is motivated by the Nurse Family Partnership (NFP); a program of prenatal and infancy home visiting by nurses for low-income mothers and their children (====), and more recent studies have shown that cognitive performance is also strongly influenced by genetic factors (====). Given the important role of both environment and genetics, we are interested in finding interactions between these two components on cognitive function in children.====The following is the Supplementary material related to this article.",A sparse additive model for high-dimensional interactions with an exposure variable,https://www.sciencedirect.com/science/article/pii/S0167947322002043,24 September 2022,2022,Research Article,87.0
"Srinivasan Arun,Xue Lingzhou,Zhan Xiang","Department of Statistics, Pennsylvania State University, University Park, PA, USA,Department of Biostatistics, School of Public Health and Beijing International Center for Mathematical Research, Peking University, Beijing, China","Received 29 January 2022, Revised 8 September 2022, Accepted 10 September 2022, Available online 22 September 2022, Version of Record 7 February 2023.",https://doi.org/10.1016/j.csda.2022.107621,Cited by (1),In many ,"). In many microbiome studies, a popular mode of analysis is the association analysis, which usually serve as the first step to disentangle complicated relationships between the microbiome and outcomes of interest. To identify microbial features (e.g., taxa abundances) that are associated with outcomes, most current microbiome-wide association studies are primarily based on the paradigm of individual taxon-based association analysis, which tests for statistical association between each individual taxon and disease-related outcomes followed by multiple testing correction, and then taxa which survive multiple correction adjustment are further investigated in downstream laboratory studies (====; ====). However, the power of this individual approach is often limited by the heavy multiple testing burden inheriting from the high-dimensionality of microbiome data. Moreover, existing multiple correction methods may fail due to the complicated correlation structure among individual taxon-based association analysis (====; ====; ====).====In many microbiome studies, it is common that multiple secondary traits were measured and collected besides the primary disease outcome. For example, it is well known that both gut microbiota and host genetics play crucial roles in the onset of inflammatory bowel disease (IBD), yet how host genetics and microbiome interact in IBD patients is unknown. Understanding these interactions is essential for basic biological insights in the pathogenesis of IBD, and hence, it is necessary to study the association between microbiome and host gene activities, which is characterized by multiple host genes sharing similar functions (====). Also, some clinical outcomes are inherently characterized by multiple secondary traits. For example, multiple symptoms (e.g., vaginal dryness, itch, burn, discharge and pain) are often treated as indicators of bacterial vaginosis (====; ====) and also in microbiome association analysis with multivariate outcomes (====). However, to the best of our knowledge, most of the aforementioned approaches can only examine the overall statistical association between a group of features (either genetic variants or microbial taxa) and multiple outcomes, and are not applicable to identify specific individual features (within the group) that are associated with these outcomes, which are essential for downstream functional studies targeting at elucidating underlying biological mechanisms of certain scientific questions on disease conditions. To address this research gap and provide useful computational tools for microbiome researchers who aim at detecting outcomes-associated microbial features, we will investigate new statistical methods in the context of microbiome compositional data analysis in this paper.====; ====; ====; ====, ====; ====). Therefore, it is crucial to incorporate the sample size characteristic of microbiome data into our analysis in order to obtain robust and accurate microbial feature selection results.====The final challenge that is more in line with our analysis goal is the reproducibility of selection. On the one hand, we want to be able to detect as many relevant microbial features as possible in order to maximize scientific discoveries. On the other hand, we also want to be cautious to ensure that findings of our method are replicable and do not include too many spurious microbiome-outcome associations (e.g., false positives). In other words, we want to have some confidence in our selection results in order to avoid potential costly and fruitless downstream validation and functional studies. To this end, we adopt the recent knockoff filter strategy (====; ====) which controls the false discovery rate (FDR) of the selected variables. An appealing feature of the knockoff filter framework is its finite-sample performance guarantee, which is especially desired here to address the potential undesired unstable small-sample performance of traditional statistical variable selection methods in microbiome data analysis (====; ====).====) such that our final selection results enjoy the benefit of FDR control. Extensive simulation studies have been conducted to evaluate the superior performance of MRKF and then we demonstrated the potential usefulness of MRKF with an application to an actual microbiome data set to gain new biological insights.====The following is the Supplementary material related to this article.",Identification of microbial features in multivariate regression under false discovery rate control,https://www.sciencedirect.com/science/article/pii/S0167947322002018,22 September 2022,2022,Research Article,88.0
Liu Ran,"School of Mathematics and Statistics, Beijing Jiaotong University, Beijing, China,Advanced Institute of Natural Sciences, Beijing Normal University at Zhuhai, Zhuhai, Guangdong, China,Department of Mathematics, Hong Kong Baptist University, Hong Kong, China","Received 10 January 2022, Revised 5 September 2022, Accepted 5 September 2022, Available online 16 September 2022, Version of Record 13 January 2023.",https://doi.org/10.1016/j.csda.2022.107616,Cited by (0),"Checking the models about the ongoing Coronavirus Disease 2019 (COVID-19) pandemic is an important issue. Some famous ==== (ODE) models, such as the SIR and SEIR models have been used to describe and predict the epidemic trend. Still, in many cases, only part of the equations can be observed. A test is suggested to check possibly partially observed ODE models with a fixed design sampling scheme. The ==== and Algeria, respectively, maybe not be appropriate by applying the proposed test.","Since December 2019, a new infectious disease called the Coronavirus Disease 2019 (COVID-19) has spread worldwide, resulting in an ongoing pandemic. To help understand the characteristic of this epidemic and to act suitably to reduce its risk, numerous studies have been done by using several approaches to estimate the sizes of the COVID-19 infected populations in different countries and regions and further predict their trends (e.g., ====; ====; ====; ====Consider a general ====-dimensional ODE model as==== with an initial condition ====. Here ==== where ==== are chosen from ==== and ====. The measurement error ==== satisfying ==== has a nonsingular variance-covariance matrix ====, and is independent with ==== for ====. If ====, the ODE system is fully observed, otherwise is a partially observed ODE system, where only a small subset of components can be measured (====).====Using ODE models to describe dynamic processes can estimate parameters with particular relevance and conduct further statistical analysis. However, an important issue is whether the assumed ODE model rightly describes the measurement data. For instance, we may want to know whether an SEIR model is good enough to model real COVID-19 infective population data for a period of time. If not, we may have to be careful to use the results from the statistical analysis. Thus, a model checking for the assumed ODE model should be accompanied. Given a certain parametric family ====, the hypotheses we consider are formally stated as==== where ====, points out that local smoothing methods are sensitive to alternative models which are oscillating/highly frequent than global smoothing methods in general. Examples contain ====, ====, ==== and ====. Some others are based on residual-marked empirical processes and the average over an index set. Then they are called the global smoothing tests. Examples include ====, ====, ====, ==== and ====. These tests can better detect smooth alternative models. However, testing for ODE models specification still receives less attention. For testing the hypotheses in ====, ==== often oscillates. See ==== as a good reference. This phenomenon suggests that local smoothing tests for model checking should be more appropriate than global smoothing tests. ====, takes advantage of the solution trajectory of the ODE system. We note that the good theoretical results of ==== rely on a random design sampling scheme. The sample ===='s are independent and identically distributed (i.i.d.), thus the limiting properties can be derived by using the existing theory of U-statistics. But for ODE systems such as the epidemic model, the variable ==== under fixed design rather than i.i.d. random design. The test ==== that is based on this non-i.i.d. sequence of ===='s needs to be investigated to see its usability.====In this paper, we give a modification of ==== with a fixed design and study the respective properties under the null, local and global alternatives. Without notational confusion, we still write it as ====. We then check the SEIR model for two real COVID-19 data sets. Since the sample ====The paper is organized as follows. In Section ====, we will present some general asymptotic properties of U-statistics. Section ==== will contain the construction of the test ==== and its asymptotic properties. Section ==== will include some simulation results. In Section ====. The technical proofs will be relegated to Appendix.",Specification testing for ordinary differential equation models with fixed design and applications to COVID-19 epidemic models,https://www.sciencedirect.com/science/article/pii/S0167947322001967,16 September 2022,2022,Research Article,89.0
"Li Lu,Ke Chenlu,Yin Xiangrong,Yu Zhou","Shanghai Jiao Tong University, China,Virginia Commonwealth University, United States of America,University of Kentucky, United States of America,East China Normal University, China,Key Laboratory of Advanced Theory and Application in Statistics and Data Science - MOE, China","Received 29 October 2021, Revised 5 June 2022, Accepted 5 September 2022, Available online 15 September 2022, Version of Record 13 January 2023.",https://doi.org/10.1016/j.csda.2022.107618,Cited by (0), to detect such an independence. Then the proposed generalized martingale difference correlation is utilized as a marginal utility to do high-dimensional variable screening. Both simulation results and real data illustrations show the promising performance of the developed indexes.,") measures the dependence between two random vectors. It is analogous to the usual covariance, but generalizes and extends this so that DC is zero if and only if those two random vectors are independent. Martingale difference divergence (MDD, ====) captures the conditional mean independence between a random variable and a random vector. In addition, ==== as well.====More recently, ==== and ==== are better for capturing conditional mean independence, while ====, ==== and ====, ====, ==== and ==== are continuous negative definite functions, which will be introduced in following section. Moreover, GMDC likes DC and the Hilbert-Schmidt independence criterion (HSIC, ====) belongs to the class of maximum mean discrepancy (MMD, ====).====The rest of this article is organized as follows. Section ==== proposes GMDD, GMDC, and their sample versions, as well as the respective properties. Section ==== describes the GMDC-based screening procedures, illustrating their sure screening consistency of screening procedures. Section ==== shows the simulation results. Section ==== presents two real data examples. Section ==== concludes the paper with a short discussion. All technical proofs are gathered in the supplementary material.====The following notation will be consistently used throughout the rest of the paper. Let ==== is denoted by ====, where ====. Denote the conjugate of the complex-valued function ==== is ==== and ====. We define the norm for a complex-valued function ==== as ====, where ==== is a Lévy measure. Following convention, we use ==== and ====The following is the Supplementary material related to this article.",Generalized martingale difference divergence: Detecting conditional mean independence with applications in variable screening,https://www.sciencedirect.com/science/article/pii/S0167947322001980,15 September 2022,2022,Research Article,90.0
"Chakraborty Nilanjan,Sakhanenko Lyudmila","Department of Statistics and Probability, Michigan State University, 619 Red Cedar Rd, East Lansing, 48824, USA","Received 20 December 2021, Revised 6 September 2022, Accepted 8 September 2022, Available online 14 September 2022, Version of Record 20 September 2022.",https://doi.org/10.1016/j.csda.2022.107619,Cited by (0)," tests with unequal cell sizes and unequal unknown cell covariances, as well as contrast tests in elegant and unified way. New tests are compared theoretically and on simulations studies with existing popular contemporary tests. They enjoy consistency, computational efficiency, very mild moment/tail conditions. They avoid the estimation of correlation or ====, and allow the dimension to grow with sample size exponentially. Additionally, they allow the number of groups and the ==== to grow with the sample size exponentially, thus broadening their applicability.","There are ==== independent groups of random vectors-columns ====, drawn from ==== populations with means ====These vectors do not have to come from the same distribution. Also we can let the dimension ==== grow with ==== or the number of groups ==== grow with ==== or both. This is quite rare in the literature but it is a very useful setup in practice. ==== argued that functional data can be viewed this way. We adopt this viewpoint in the real data example later in this paper. This setup was recently used in ====.====Let ==== denote the ====-th entry of the ====-th observation from the ====-th group, where ====. Given a class of non-singular matrices ==== with components ====, we propose the test statistics of the form==== Intuitively, in ==== short ====-column-vectors ==== are stacked into long ====-vectors that are then multiplied by matrices ====, averaged over ====, normalized and finally maximized over the components of the resulting long vector and over the class of matrices (over ====).====Next, introduce the multiplier bootstrapped form of the test statistics==== where a vector ==== of i.i.d. ===='s and ====, are the groupwise averages. Intuitively, the symmetrized sums would have distribution that approximates the distribution of the original test statistics under the null hypothesis that all the group means are equal (MANOVA) or are related linearly (GLHT).==== rejects if==== where ==== only.====We show that these bootstrap tests have excellent level and power performance for MANOVA and GLHT in high dimensional setup including contrasts tests. Our approach produces a whole class of tests that are flexible and can be tuned to many different scenarios. Indeed, the class of matrices ====, serves as a tuning mechanism, where this class can be chosen to maximize components of ====The high dimensional MANOVA problem for testing ==== for ==== has been a focus of many recent works due to its growing importance in genomics, econometrics, and neuroimaging among many other fields of science. For example, ==== considered the ratio of the traces of between-sample covariance and within-sample covariance. Meanwhile, ==== proposed a test based on the difference of those two traces. ==== used the Moore-Penrose inverse of the within-sample covariance matrix to construct a test. ==== proposed a test based on the maximum-norm of the squared differences between ==== proposed a thresholded ====From a different point of view, this work also extends the recent work by ==== for ==== to the case ====. This extension is elegant and less technical than the direct reproving of the results in ====, where one would have to tackle intricate block-type dependency structures and work with ====-statistics similar to what was done in ====. The class of our tests enjoys all the good properties of the tests in ==== and ====, which are specifically designed for this scenario.====The rest of the paper is organized as follows. Section ==== contains main theoretical results devoted to the statistical applications: one- and two-way MANOVA and GLHT in high-dimensional setup. Section ==== establishes the connection with other recent tests that are related to our approach. In section ==== we perform detailed high-dimensional simulation study where we compare our method with two existing competitors by ==== and ====. We consider various distributions and sparsity scenarios. We illustrate the practicality of our test on a real data example about fish shape comparisons in section ====, which is followed by a discussion with a conclusion in section ====. Technical conditions, details, and proofs are gathered in the Appendix.====The ====-th group after centering consists of ==== independent observations in ====. Now stack those vectors ==== into ====, stack vectors ==== into ====, and so on, stack ==== into ====. We obtain centered long vectors ==== with ====. Note that the test statistics ==== are functionals of normalized sums ====. Note that using long stacked vectors the test statistics can be rewritten as==== where ==== consists of vectors ====, stacked into one long vector. Under ==== test statistics become==== due to condition (A2). Also note that the bootstrapped statistics can be rewritten as ====.====Introduce the following moment and tail conditions collectively denoted by (C). Let ==== be sequences of positive constants, possibly growing to infinity. Let ====.====(S) ==== for some fixed constant ====.====(E1) ==== a.s.====(E====) For Orlicz norms based on function ====, we have ====.====(E====) For Orlicz norms based on function ====, we have ====.====Recall ==== for a random vector ====.====(E3) ====.====(E3====) ====.====(M) ====.====(M====) ====.====For completeness recall the conditions on the class of matrices:====(A1) ====;====(A2) ====;====(A3) ==== for some fixed constant ====.====We utilize multiplier bootstrap, thus, introduce==== where a vector ==== of i.i.d. ==== random variables is independent of ====. Consider the Kolmogorov distance defined as==== where ==== stands for the probability with respect to the Gaussian vector ==== only. Recall that ==== for ====.====We remark that Kolmogorov distance KD goes to 0 as ==== grows as long as ====, and ==== grow not too fast. In particular, under condition (E1) we need ====, which would be the same under (M====) and (E====) if ==== and otherwise under (M====) and (E====) we need ====. Meanwhile, under (M====) and (E3====) with ==== we need ====. However, for ==== three conditions should be satisfied without a clear winner: ====, ====, and the most intricate condition of the three ====. Under (M) and (E====) we have a special case when ==== grows faster or the same as ==== and ====, otherwise we need both ==== and ====. Finally, under (M) and (E3) we need both ==== and ====.====Note that the proof of ==== follows from the proof of ==== immediately.====We also remark that one can choose a sequence of ==== such that ==== and consider events in ==== and ==== in the spirit of Proposition 4.1 and remark 4.2 in ====. Then apply Borel-Cantelli lemma to obtain the results with probability tending to 1.",Novel multiplier bootstrap tests for high-dimensional data with applications to MANOVA,https://www.sciencedirect.com/science/article/pii/S0167947322001992,14 September 2022,2022,Research Article,91.0
"Yang Yuehan,Xia Siwei,Yang Hu","School of Statistics and Mathematics, Central University of Finance and Economics, China,School of Science, Civil Aviation Flight University of China, China,College of Mathematics and Statistics, Chongqing University, China","Received 12 September 2021, Revised 12 May 2022, Accepted 8 September 2022, Available online 14 September 2022, Version of Record 16 September 2022.",https://doi.org/10.1016/j.csda.2022.107620,Cited by (0), for the high-dimensional multivariate regression models. We consider two graphical structures among predictors and responses. The proposed method explores the regression relationship allowing the predictors and responses derived from different ==== with general ,"Many large-scale statistical applications involve building interpretable models, linking a large set of predictors to multiple responses, such as protein-DNA associations (====), genomic selection (====) and stock market associations (====; ==== and ==== and ====.====Incorporation of variables structure like grouping structure of predictors has been found to improve the model accuracy (====; ====; ====). In the multivariate regression framework, ==== proposed a multivariate sparse group lasso with any arbitrary group structure for the regression coefficient matrix. ====; ====; ====; ====; ====). The structures among multi-response interaction models are also discussed by ====, ====, ==== and ====.====Although recent work dedicated to the multivariate models, there is still a gap in our understanding that most of the previous research either focused on the graph estimation of noises (====; ====; ====, ====) or the model is assumed to have some specific format, i.e., block-structured regularization (====; ====; ====), uncorrelated covariance for the noise matrix (====). Yet, in many real applications of multivariate models, the structures within responses and predictors are complex.====On the other hand, combining a graph structure information among variables by Laplacian quadratic penalty is shown to improve model estimation and variable selection. In the univariate regression model, ==== proposed Weighted fusion combining the ==== penalty and the Laplacian quadratic penalty term constructed using sample correlation. ==== proposed Sparse Laplacian shrinkage (SLS), which considers sparse graph structure by employing a hard threshold function. ==== proposed a graph-constrained regularization procedure, named Sparse Laplacian shrinkage with the graphical lasso estimator (SLS-GLE), which combines an estimated sparse graph structure information through the Gaussian graphical model among predictors.====We focus on the multivariate high-dimensional data settings in which responses and predictors both have graph structures. This type of data is prevalent in real life, but difficult to tackle due to its complex correlations and ====. In this paper, we propose a method named Multivariate sparse Laplacian shrinkage (MSLS) for these models, aiming to identify the correct model under complex correlations. Incorporating the conditional covariances among predictors and responders through Laplace quadratic penalties, we allow the graphical structures existing in both the predictors and responses respectively, which have not been studied in previous research. In the MSLS, the graph structures are well interpreted by the estimated precision matrix based on the Gaussian graphical model, and we apply the Laplace quadratic penalty to capture the graphical structure information among variables.====), still resulting in computational burden. Second, the graph estimation of the noises does not capture any patterns, sharing with the regression matrix since it only encodes the structure in responses that cannot be explained by predictors (====). We shall discuss the merits of this method and give a more detailed comparison in the next section.====The rest of the paper is organized as follows. We define in Section ==== shows related theoretical properties under reasonable conditions. The simulations and applications in Section ==== and Section ==== analyse the performance of the MSLS and compare it with several existing methods. We conclude in Section ====. Technique details are provided in the Supplementary Material.====The following is the Supplementary material related to this article.",Multivariate sparse Laplacian shrinkage for joint estimation of two graphical structures,https://www.sciencedirect.com/science/article/pii/S0167947322002006,14 September 2022,2022,Research Article,92.0
"Andrade Ana C.C.,Pereira Gustavo H.A.,Artes Rinaldo","Department of Statistics, Federal University of São Carlos, São Carlos, Brazil,Department of Statistics, Federal University of São Carlos, Rod. Washington Luís, km 235 - SP-310, São Carlos, CEP 13565-905, Brazil,Insper Institute of Education and Research, São Paulo, Brazil","Received 13 July 2021, Revised 22 August 2022, Accepted 2 September 2022, Available online 13 September 2022, Version of Record 22 September 2022.",https://doi.org/10.1016/j.csda.2022.107612,Cited by (0)," in small samples. To study the behavior of this residual, two regression models are introduced, and two applications are used to show that the proposed residual can detect ====.","; ====; ====). Some examples of these variables include wind directions (====), movement of animals in response to stimuli (====), and angle of regions of the human eyes (====).====The von Mises family of distributions is often used to model the dependent variable of regression models with a circular response (====) due to the similarities with the normal distribution and its main inferential properties. ==== and ====.====Residuals play an important role in checking model adequacy and in the identification of outliers and influential observations. They are a measure of the discrepancy between the observed value and the value fitted by the model (====, ====, and ==== showed that the distribution of the quantile residual is well approximated by the standard normal distribution, even in small samples, for beta regression, Johnson ====Due to the topological differences between the real line and the circle, many of the residuals in the literature for linear data do not apply in the circular context, especially those obtained using a standardization of the difference between observed and fitted values. However, there are few studies related to diagnostic analysis in circular regressions (====). Most of the existing work deals only with diagnostics of influence (====; ====) or detection of outliers (====; ====; ==== proposed residuals that, in addition to satisfying this need (checking the model's adequacy), are approximately standard normally distributed. These residuals consist of a standardization of the deviance residual but are limited to the von Mises regression models.====This paper proposes an extension of the quantile residual for circular regression models, since the topological differences between the real line and the circle do not allow the use of ====, whose response variable is von Mises distributed. In addition, we studied the properties of our residual using two extensions of Fisher and Lee's model that is proposed here, contemplating the sine-skewed von Mises (====) and the wrapped Cauchy (====) distributions. We also introduced these models, since there is a certain negligence in theoretical studies of regression models with circular response (====; ====), especially in the scope of distributions that differ from von Mises. These models encompass an interesting range of behaviors for the circular dependent variables, but the technique can be used in models that have response variable with other circular distributions, such as those proposed by ==== and by ====.====The remainder of this paper is organized as follows. Section ==== presents the regression models considered in this work. Section ====. Concluding remarks are provided in Section ====.====In this Appendix, we prove ==== using the following four steps:====(I) ====;====(II) ====;====(III) ====;====(IV) ====.====For ====, ====, where ==== is such that ====.====As ====, then ==== and so ====.====As ==== is consistently estimated, ==== and ==== (step (I)), using a modification of Slutsky's theorem (====, page 203), ====.====As ==== is consistently estimated, ==== and ==== (step (II)), using the same modification of Slutsky's theorem used in (II), ====.====As ==== (step III), by Sverdrup's theorem (====, page 180),==== and so",The circular quantile residual,https://www.sciencedirect.com/science/article/pii/S016794732200192X,13 September 2022,2022,Research Article,93.0
"Ke Baofang,Zhao Weihua,Wang Lei","School of Statistics and Data Science & LPMC, Nankai University, Tianjin, 300071, PR China,School of Science, Nantong University, Nantong, 226019, PR China","Received 26 January 2022, Revised 6 August 2022, Accepted 27 August 2022, Available online 13 September 2022, Version of Record 20 September 2022.",https://doi.org/10.1016/j.csda.2022.107609,Cited by (0),"As extensions of vector and matrix data with ultrahigh dimensionality and complex structures, tensor data are fast emerging in a large variety of scientific applications. In this paper, a two-stage estimation procedure for linear tensor ==== of the proposed estimators are established. Simulation studies and a real example on Beijing Air Quality data set are provided to show the performance of the proposed estimators.",". Values of air quality are measured from 9 impact factors by nationally controlled air-quality monitoring sites for 24 hours, i.e., the sampling unit is a ==== matrix or a two-way tensor, including measurements of SO====, NO====, CO, O====For longitudinal data, it has been recognized that the within-cluster correlation structure plays an important role and then a major aspect is how to take into account the correlation structure to improve estimation efficiency. However, since the underlying correlation structure is difficult to describe and specify, a naive and simple way is to use an independent working model, which may lose some efficiency when strong correlations exist; see ====, ==== and references therein. To address this issue, generalized estimating equations (GEEs) proposed by ====; ====; ====; ====; ====). For example, ==== proposed tensor ridge regression and support tensor regression based on CANDECOMP/PARAFAC (CP) decomposition. ==== and ==== proposed an estimation procedure for the linear tensor regression model based on the CP and Tucker decomposition (====), respectively. These methods can also be applied to longitudinal tensor data, aiming to reduce the number of parameters by imposing low-dimensional structures on tensors (====; ====; ====; ====) can provide a more complete picture of effects of the covariates on the response variable. Moreover, QR also can exhibit a higher level of robustness compared with the traditional mean regression (====; ====, ====). A comprehensive review on QR can be found in ==== and ====The rest of this paper is organized as follows. In Section ====. Simulation studies and a real data analysis are given in Sections ==== and ====. We discuss unbalanced cases in Section ====. All the proofs of theoretical results are given in the Appendix.==== ==== ==== ==== ",Smoothed tensor quantile regression estimation for longitudinal data,https://www.sciencedirect.com/science/article/pii/S016794732200189X,13 September 2022,2022,Research Article,94.0
"Li Xiao,Li Yuqiang,Wu Xianyi","School of Statistics & KLATASDS-MOE, East China Normal University, Shanghai, China","Received 25 December 2021, Revised 30 July 2022, Accepted 27 August 2022, Available online 13 September 2022, Version of Record 13 January 2023.",https://doi.org/10.1016/j.csda.2022.107610,Cited by (0),"-exploration (abbreviated as empirical ====-Gittinx index rule) is proposed to solve such MAB problems. This procedure is constructed by combining the idea of ====-exploration (for exploration) and empirical Gittins indices (for exploitation) computed by applying the Largest-Remaining-Index algorithm to the estimated underlying distribution. The convergence of empirical Gittins indices to the true Gittins indices and expected discounted total rewards of the empirical ====-Gittinx index rule to those of the oracle Gittins index rule is provided. A numerical simulation study is demonstrated to show the behavior of the proposed policies, and its performance over the ====-mean reward is discussed.","), finance (====, ====, ====), communication and networks (====; ====), clinical trials (====; ====), autonomous vehicles (====; ====; ====; ====), among a huge number of others. The first MAB model was originally proposed by ==== and then further analyzed and popularized in the realm of mathematicians and statisticians by ====, ====, ====Just like every problem in the random world, by and large, the investigation of MAB models can be categorized into two lines: probability theory and statistics, according to whether the underlying distributions of the processes are known or not. Within the probability theory framework, four fundamental but possibly overlapping models have been extensively investigated: closed MAB, open MAB, continuous-time MAB and restless MAB. Every MAB model consists of a set of independent reward processes that are generally referred to as arms. The first two MAB forms are mostly established on a discrete-time basis, but for open MAB, new projects will add to the system. While arms in the first three MAB models which are not in operation are “frozen” with their states remaining and zero rewards contributed, in the restless model, the arms continue to change their states even when they are not being operated (although by different transition rules, in general, ====). No matter what of the first three models is considered, it has been well known that the so-called Gittins index rules are optimal, by an elegant theory pioneered by ====. Restless MAB processes are proposed to be operated by a more general index rule, known as Whittle's index rule, due to ==== (====, ====), ====, ====, and ==== (====, ====, ====, ====, among quite a few others. A comprehensive treatment for closed MAB processes in discrete time can be found in Section 6.1 of ====. For the open MAB problem, after the first problem was analyzed in ====, a rich line of work appeared in, e.g., ==== (====, ====), ====, ==== and ====, ====, ====, etc.====Early versions of continuous-time MAB were first analyzed by ==== and ====. The followers include, e.g., ====, ==== (====, ====), ==== (====, ==== unified and greatly extended the above theory for discrete and continuous-time settings.====The other parallel line of research is regarding the statistical treatment of MAB in the situation where the underlying distribution of a system is not completely specified. It will be referred to as MAB learning. In this line, the earliest pieces of work are ==== and ====. ==== and further investigated and developed by quite a few followers such as ====, ====, ====, ====, ====, and so on. Another type of important algorithms, known as explore-then-commit (ETC) algorithms, can be attributed to ==== and was revisited by ====, a beautifully written paper. ETC algorithms were also studied under such names as ‘explore-then-exploit’, ‘forced exploration’ and ‘phased exploration and greedy exploitation’, by, for example, Abbasi-Yadkori (====, ====), ====, ====. Still another type of algorithms towards the exploration-exploitation dilemma is ====-greedy algorithms. Their history is unclear, but they are popular and widely known algorithms in reinforcement learning and thus have been discussed by many research papers (====). For example, ==== analyzed the regret of ====-greedy with slowly decreasing exploration probabilities. There are other kinds of randomized exploration as well, including Thompson sampling (====. Among other monographs, ==== provided an excellent collection of MAB models, algorithms and theoretical foundations, as well as its beautiful writing.==== are taken so that the uncertainty of distributions is quantified by prior distributions (parametric or nonparametric).====For example, consider an institute of a few servers to serve a sequence of customers so as to make certain rewards, measured by money, utility, or 0-1 to indicate success or not (as is usually the case in recommendation systems). Then a server, when assigned to a customer, will possibly raise its service quality due to a passed period of service that can promote its experience, and, at the same time, consume his physical energy to block him from providing good service. This may cause a change in the state of the server in operation, and thus affect the possible reward to be collected in the future. Let's for the time being suppose the state of a server can be summarized as a set of numbers, say ==== for the detailed problem formulation. This setting is basically different from the MAB learning that has so far been extensively studied in the literature in the following aspects. First, it is no longer optimal to pull a single arm all the time horizon. Second, the maximum of the total discounted expected reward is not so easy to compute. Finally, while only the means of the iid rewards of arms are what need to be estimated in previously analyzed MAB learning problems, in the current setting to be explored, one needs to estimate, at least, the transition probabilities among states, the distributions of the rewards at states (at least their expectations), and, more difficultly, the GIs associated with the states. Solutions to such kind of MAB learning are still quite limited. Remarkable contributions include ==== and ====. ==== used also a Q-learning approach to learn Whittle index rules that can apply to both classic multi-armed bandits and restless bandits.====For the exploration-exploitation conflict, this paper employs the concept of ====-greedy together with a direct procedure to estimate the unknown GIs by means of the estimated transition probabilities and reward distributions (thus generating what will be called empirical Gittin indices below), so as to give another exploratory solution to this MAB model. The analysis is organized as follows. Section ==== defines the model and then gives a minimal review of the GI theory that can accommodate the model. In Section ====, the proposed solution is presented in detail, including the estimation procedure of the transition probabilities and means of rewards, the GIs through ===='s Largest-Remaining-Index method, and the ====-greedy algorithms. Some theoretical results are also provided there. A small simulation study is discussed in Section ====, where the performance of the estimation of GIs and the value functions under the ====-greedy rules are reported and discussed. Some conclusions are discussed in Section ====. Some materials are deferred to the Appendix to make the text easier to follow.",Empirical Gittins index strategies with ,https://www.sciencedirect.com/science/article/pii/S0167947322001906,13 September 2022,2022,Research Article,95.0
"Zhang Yangchun,Zhou Yirui,Liu Xiaowei","Department of Mathematics, College of Sciences, Shanghai University, Shanghai, 200444, China","Received 1 November 2021, Revised 21 April 2022, Accepted 5 September 2022, Available online 13 September 2022, Version of Record 19 September 2022.",https://doi.org/10.1016/j.csda.2022.107617,Cited by (0), and dimension ==== and ====. Real data implementation are illustrated on a radio frequency dataset.,None,Applications on linear spectral statistics of high-dimensional sample covariance matrix with divergent spectrum,https://www.sciencedirect.com/science/article/pii/S0167947322001979,13 September 2022,2022,Research Article,96.0
"Yu Jun,Meng Xiran,Wang Yaping","School of Mathematics and Statistics, Beijing Institute of Technology, Beijing 100811, China,LMAM, School of Mathematical Sciences and Center for Statistical Science, Peking University, Beijing 100871, China,KLATASDS-MOE, School of Statistics, East China Normal University, Shanghai 200062, China","Received 1 October 2021, Revised 30 June 2022, Accepted 5 September 2022, Available online 13 September 2022, Version of Record 19 September 2022.",https://doi.org/10.1016/j.csda.2022.107615,Cited by (0),"With the increasing popularity of personalized medicine, it is more and more crucial to capture not only the dose-effect but also the effects of the prognostic factors due to individual differences in a dose-response experiment. This paper considers the design issue for predicting semi-parametric dose-response curves in the presence of linear effects of ","Adequately describing the dose-response curve is an important issue in pharmacology. There is a lot of evidence indicating that the dose-response is affected not only by the dosage, but also by the participants' prognostic factors (====; ====; ====. Here ==== is the response at dose ====, ==== is the maximum dosage, ==== is the ====-dimensional vector of unknown parameters accounting for linear covariate effects, ==== is a function describing dose-response relationship and ===='s are independent and normally distributed error terms.====Finding optimal clinical trial designs is gaining interest among data scientists and pharmacologists since it can assist investigators to achieve higher quality results while working with limited resources. Important works include but are not limited to ====, ====; ====; ====; ====; ====; ====; ====; ====; ==== and ====. When researchers are only interested in a few pre-specified dose levels, the corresponding optimal design problem has been studied by ====, ==== and ====. However, these results cannot be applied to the case of infinite candidate dose levels, especially when the dose varies in a continuous region. Some efforts have been made to investigate the optimal design problems for the continuous design region case under specific dose-response models. For example, ==== obtained explicit forms of the optimal designs for the emax, exponential, and linear-in-log models. ====; ====; ====, and ====, also suffer from a similar design problem.====To study the dose-response relationship in a continuous design region without making too many model assumptions, it is natural to assume that the dose-response function ==== is nonparametric. The polynomial spline has been commonly used for estimating the nonparametric function ==== because of its similarity to polynomials and conceptual simplicity. For example, ====In general, current research on design optimality for polynomial spline models can be divided into two categories. One focuses on minimizing the prediction variance and the function is assumed to be well approximated by the polynomial splines (====; ====). The corresponding optimal design is known as the ====-optimal design (====). The other approach focuses on the model “misspecification” problem. Correspondingly, a class of designs called all-bias designs, which focus on minimizing the difference between the expected predicted response from the assumed model and the expected response from the “true” model, has found considerable interest among statisticians. See ====; ====; ====; and ==== for examples.====; ====; ====; ====; ====; ====; ====, among others. In such a framework, the prediction error is divided into two parts: the error associated with possible misspecification of the model and the random error derived from other sources.====Despite the success of the two paradigms in finding robust designs against the possible misspecification of the regression model, the existing work still cannot fully satisfy the practical needs for our problem due to the following reasons. Firstly, it is risky for a patient to use a certain dose of the drug when the estimated dose-response curve has prediction bias. Secondly, while the square of bias and the variance contribute equally to the IMSE, the consequences of controlling bias and controlling variance are not the same. According to ====, when the variance and square of bias are considered to be of similar magnitude, the design that minimizes the IMSE is close to the all-bias design. ==== showed that the errors associated with misspecification are often larger than those derived from other sources, which implies that using ====-optimal design and all-bias design will typically lead to quite different results in finding the response surface.====The rest of this paper is organized as follows. Section ==== formulates the problem. Section ==== proposes the bias constraint design criterion motivated by the Neyman-Pearson framework. Section ==== presents some characteristics of the proposed design and an approximation criterion for searching the corresponding optimal design. Section ==== illustrates our methodology through several examples. Section ==== concludes this paper. All proofs and additional simulation details are postponed to the Supplementary Material.====The following is the Supplementary material related to this article.",Optimal designs for semi-parametric dose-response models under random contamination,https://www.sciencedirect.com/science/article/pii/S0167947322001955,13 September 2022,2022,Research Article,97.0
"Ghosal Rahul,Ghosh Sujit,Urbanek Jacek,Schrack Jennifer A.,Zipunnikov Vadim","Department of Epidemiology and Biostatistics, University of South Carolina, United States of America,Department of Statistics, North Carolina State University, United States of America,Department of Medicine, Johns Hopkins University, School of Medicine, United States of America,Department of Epidemiology, Johns Hopkins Bloomberg, School of Public Health, United States of America,Department of Biostatistics, Johns Hopkins Bloomberg, School of Public Health, United States of America","Received 30 June 2021, Revised 1 September 2022, Accepted 2 September 2022, Available online 9 September 2022, Version of Record 16 September 2022.",https://doi.org/10.1016/j.csda.2022.107614,Cited by (3),"Shape restrictions on functional ==== such as non-negativity, monotonicity, convexity or ","Functional regression (====), physical activity research (====), marine ecology (====), radiomics (====), environmental modeling (====; ====; ====; ====, ====).====). In longitudinal clinical studies exploring the effect of a drug on disease severity (e.g., ====) a negative functional coefficient corresponding to the treatment group would prove the effectiveness of the drug while a negative and decreasing functional coefficient would suggest the effectiveness of the drug to increase in the follow up weeks. In a Quantile Function-on-Scalar Regression (QFOSR) framework introduced in ==== a non-decreasing functional coefficient provides a sufficient condition (====) for ensuring monotonicity of the predicted quantile functions. In modeling of growth curves (====), the mean function ==== is required to be non-decreasing as ‘growth’ is necessarily non-decreasing. In clinical studies, often odds-ratios of a disease can be known to be positively (negatively) associated with a functional biomarker - a knowledge that can be modeled using a constrained scalar-on-function regression (SOFR) model. Incorporation of such shape constraints on functional regression coefficients can often lead to reduced uncertainty of the coefficient estimates in the restricted parameter space (====; ====) and can regulate the model fit, particularly, for smaller sample sizes. Several methods have been developed for shape constrained estimation in nonparametric regression using kernel-based approaches (====; ====; ====), smoothing splines (====), regression splines (====, ====; ====; ====) among many others. ====In this article, we extend a Bernstein polynomial (BP) estimation approach from shape-constrained nonparametric regression (====) to a wide class of functional regression models under various shape constraints. We follow a method of sieve (====; ====), making the estimators more efficient. The shape constraints on the coefficient functions automatically regularize the coefficient functions, as often required in FDA, and smoothness of the coefficient functions is achieved using a truncated basis approach (====; ====Bernstein polynomials have various attractive shape-preserving properties (====; ====; ====). Optimal stability of BPs (====The rest of this article is organized as follows. We present our modeling framework, illustrate the proposed estimation method for shape constrained functional regression, establish the theoretical properties of the estimator, and propose a bootstrap test in Section ====. In Section ====, we perform numerical simulations to evaluate the performance the proposed methods and provide comparisons with existing unconstrained functional regressions. In Section ====) and ii) a quantile function-on-scalar regression model of accelerometry-estimated physical activity data from the Baltimore Longitudinal Study of Aging (BLSA). We conclude in Section ==== with a brief discussion on our proposed method and some possible extensions of this work.====The following is the Supplementary material related to this article.",Shape-constrained estimation in functional regression with Bernstein polynomials,https://www.sciencedirect.com/science/article/pii/S0167947322001943,9 September 2022,2022,Research Article,98.0
Martínez Alejandra Mercedes,"CONICET and Universidad de Buenos Aires, Argentina,CONICET and Universidad Nacional de Luján, Argentina","Received 7 August 2021, Revised 28 July 2022, Accepted 2 September 2022, Available online 7 September 2022, Version of Record 16 September 2022.",https://doi.org/10.1016/j.csda.2022.107611,Cited by (1),-splines with robust linear ====-proposal based on ," independent and identically distributed (i.i.d.) with the same distribution as ==== where ====, ==== and ====. The response and covariates are related through==== where the error ==== is independent from ====, ==== is assumed to be smooth. Furthermore, in the classical setting, it is usually assumed that ==== and ====, so ==== stands for the unknown scale parameter.====The particular situation where ==== is considered in ==== and ==== who describe different procedures based on kernels or splines to estimate the unknown quantities. As in linear regression models, these estimators are very sensitive to atypical observations since they are based on the least squares principle. When ==== and to deal with more reliable procedures when atypical data arise, ====-estimators, while ==== and ==== studied ====-estimators based on splines.====It is worth mentioning that, when ====, model ==== requires multivariate smoothing, so that the ‘‘curse of dimensionality’’ is not overcome specially when ====) provide an attempt to solve this problem, since the covariates related to the nonparametric component enter the model through an additive structure, leading to the relationship==== where the univariate unknown functions ==== (====), the coefficients ==== and ==== and the scale parameter ==== are the quantities to estimate. Usually, the functions ==== are assumed to be continuous with support on a compact interval ==== which is also the support of the distribution of ==== are required. Some of the most common conditions consist in assuming that ====, for ====, or ====, for ====.====The partially linear additive regression model ====, model ==== for a discussion on this topic.====. To estimate the components of the model, ==== introduced two least squares kernel approaches, which have a high computational cost, since they perform sequential estimations in the direction of interest, see also ====. A related kernel based method that, at the initial stage when estimating ====, makes use both of the additive structure of the nonparametric component and of the fact that ====. A different family of kernel based estimators, that reduces the computational requirement, was studied in ====, while ==== considered the situation where the linear covariate is measured with error. Following a different approach, ==== combined local linear smoothers and backfitting to provide a root−====. In this direction, a robust algorithm based on backfitting and the generalized Speckman estimator was considered in ====The described methods do not estimate the parameters and nonparametric components simultaneously. For that reason and following the spline approach given in ==== for additive models, ==== and ====, respectively.====All these estimators are based on a least squares approach and assume that the error has finite variance, so, as in partial linear models, a small proportion of atypical data may seriously affect the estimations. A more resistant approach based on quantile regression and spline approximation was suggested in ==== and ==== who also considered variable selection in sparse models. An extension to censored partially linear additive models was studied in ====, while an approach based on penalized splines was discussed in ====-splines to approximate the additive components with ====-regression estimators (====). As it is well known, ==== and ==== as in the classical setting, we only require that the error ====. The scale parameter of ==== is assumed to be 1 to identify ==== show, our proposal leads to estimators resistant to vertical outliers and to atypical data with high–leverage in the covariates corresponding to the linear component.==== and ==== for nonparametric regression models and by ==== for additive models. As mentioned in ====. Throughout this paper, as in ==== who considered a least squares approach or ==== and ==== whose proposal is based on quantile regression, we will assume that discrete variables if present are included in the model as some of the components of the vector ==== whose relation with the responses is modeled parametrically.====The rest of the paper is organized as follows. In Section ==== in Section ====-estimator are summarized in Section ====, while Section ==== contains the analysis of the Boston housing data set. Some final comments are presented in Section ====. All proofs are relegated to the Appendix or to the supplementary file available on-line, where some additional numerical results are also included.====From now on, for any measure ==== and class of functions ====, ==== and ==== will denote the covering and bracketing numbers of the class ==== with respect to the distance in ====, as defined, for instance, in ====. Furthermore, ==== stands for ====, where ==== stands for the empirical probability measure of ====, ====, and ==== is the probability measure corresponding to ====.====Let us state some notation that will be helpful in the sequel. Given a loss function ====, we define the function ==== as==== Note that ==== is the sample version of the function ==== defined in ====.====Recall that ====, ====, denotes the linear space spanned by the centered ====-splines bases of order ==== and size ==== as defined in ====. From now on, for ====, ====, and identifying the functions with their coefficients, we denote indistinctly ==== as defined in ==== and ==== as defined in ==== with ====.====To derive uniform results, ==== below provides a bound to the covering number of the class of functions==== where ==== was defined in Section ====. ==== is a direct consequence of Lemma S.2.1 in ==== noting that the number of parameters involved is ==== and that the class ==== has envelope 1, for that reason, its proof is omitted.====To derive the consistency of the ====-estimators and the ====-scale the following Lemma will be helpful. It shows that ==== converges to ==== with probability one, uniformly over ====, ====, ==== and ==== and its proof uses arguments similar to those appearing in the proof of Lemma S.1.2 in ====. We include it in the supplementary file for the sake of completeness.",A robust spline approach in partially linear additive models,https://www.sciencedirect.com/science/article/pii/S0167947322001918,7 September 2022,2022,Research Article,99.0
"Merrell David,Chandereng Thevaa,Park Yeonhee","Department of Computer Sciences, University of Wisconsin-Madison, USA,Department of Biostatistics, Columbia University, USA,Department of Biostatistics and Medical Informatics, University of Wisconsin-Madison, USA","Received 20 January 2022, Revised 5 August 2022, Accepted 18 August 2022, Available online 1 September 2022, Version of Record 8 September 2022.",https://doi.org/10.1016/j.csda.2022.107599,Cited by (0),"In clinical trials, response-adaptive ==== (RAR) has the appealing ability to assign more subjects to better-performing treatments based on interim results. Traditional RAR strategies alter the randomization ratio on a patient-by-patient basis. An alternate approach is blocked RAR, which groups patients together in blocks and recomputes the randomization ratio in a block-wise fashion; past works show that this provides robustness against time-trend bias. However, blocked RAR poses additional questions: how many blocks should there be, and how many patients should each block contain?==== is an algorithm that designs two-armed blocked RAR clinical trials. It differs from other trial design approaches in that it optimizes the ==== in addition to their treatment allocations. More precisely, the algorithm yields an ==== that chooses the ==== of the next block, based on results seen up to that point in the trial. ====The algorithm maximizes a utility function balancing ==== statistical power, ==== patient outcomes, and ","Randomization is a common technique used in clinical trials to eliminate potential bias and ====Traditional RAR designs recompute the randomization ratio on a patient-by-patient basis (====), usually after a burn-in period of fixed randomization. However, traditional RAR designs have been widely criticized (====). Traditional RAR designs are susceptible to bias caused by temporal trends in clinical trials. Temporal trends are especially likely to occur in long duration trials. Patients' characteristics might be completely different throughout the trial or even at the beginning and end of the trial (====). Standard RAR analyses assume that the sequence of patients who arrive for entry into the trial represents samples drawn at random from two homogeneous populations, with no drift in the probabilities of success (====; ====). This assumption is usually violated. For example, in the BATTLE lung cancer elimination trials (====), a larger proportion of smokers enrolled in the latter part of the trial than at the beginning.====Different strategies have been proposed to address the serious flaws of traditional RAR designs. ====). Other approaches described by ==== and ==== modify traditional RAR by treating patients in groups (====; ====). The trial updates its randomization ratio for each consecutive group, and uses a group-stratified statistical test in the final analysis; this stratified analysis is robust to time trends. We refer to these as blocked RAR designs. ==== further examined the operating characteristics of blocked RAR designs (====). They concluded that blocked RAR provides a good trade-off between ethically assigning more subjects to the better-performing treatment group and maintaining high statistical power. Yet despite its strengths, blocked RAR introduces new questions. How many blocks should there be? Should the blocks have equal size, or should they vary in size?====, ====, ====, ====). These works have important limitations. The Gittins index approaches of ==== assume either ==== a fully sequential trial with similar weaknesses to traditional RAR or ==== yield allocation rules that ==== are deterministic, ==== are fully sequential, or ==== assume a blocked trial with a fixed number of blocks. At the time, ===='s approaches were also limited by computer speed and memory, which have improved famously over the years.====This paper presents ====, an algorithm that designs blocked RAR trials. ==== (====, ====, ====). However, our method differs in that it optimizes the ==== as well as their treatment allocations. That is, the algorithm yields a policy that adaptively chooses the ==== of the next block, based on results seen up to that point in the trial. ==== inherits the ==== of blocked designs, but adapts more flexibly to interim results. The current version of ==== is tailored for two-armed trials with binary outcomes. Future versions may permit a more general class of trials.====Our paper has the following structure. In Section ==== we describe our problem formulation and algorithmic solution. In Section ==== we compare ===='s designs against other designs that have been widely used in clinical trials. We use our proposed method to redesign a phase II trial in Section ====. We discuss ===='s limitations and potential improvements in Section ====. The appendices cover some additional mathematical and algorithmic details, and show additional results.====The following is the Supplementary material related to this article.",A Markov decision process for response-adaptive randomization in clinical trials,https://www.sciencedirect.com/science/article/pii/S0167947322001797,1 September 2022,2022,Research Article,100.0
"Huang Danyang,Hu Wei,Jing Bingyi,Zhang Bo","Center for Applied Statistics, Renmin University of China, Beijing, China,School of Statistics, Renmin University of China, Beijing, China,Department of Statistics & Data Science, Southern University of Science and Technology, Shenzhen, China","Received 28 January 2022, Revised 18 August 2022, Accepted 20 August 2022, Available online 29 August 2022, Version of Record 5 September 2022.",https://doi.org/10.1016/j.csda.2022.107601,Cited by (2),"With the development of the internet, network data with replications can be collected at different ==== and technical conditions are investigated. To demonstrate the performance of the proposed GSAR model and its corresponding estimation methods, ==== was performed on simulated and real data.","In recent years, there has been a considerable increase in research interest in network data analysis (====; ====; ====; ====; ====). One typical generalized SAR model is the spatial autoregressive panel (SARP) model (====; ====), which assumes time-invariant parameters and can obtain more efficient estimates than the traditional SAR model (====). Other spatial-temporal models include the spatial temporal autoregressive model (====; ====) model, and time-space dynamic spatial panel data model (====; ====; ====).====Compared to these models, the SARP model is more effective for analyzing real network data with strong spatial effects and weak temporal effects (====). For instance, this can be observed in a network based on geographic information, such as an air pollution network (====; ====), transportation network (====; ====), house price network (====; ====), and brain network (====; ====). A weak temporal effect means that different measurement times have little effect on responses (====). For example, the house price of a residential area is often more dependent on its location than on time (====; ====).====While the SARP model assumes a homogenous network autocorrelation parameter for all nodes in the network, this is not always true for real data. For example, some nodes in a network tend to be more correlated with their connected neighbors (====; ====; ====), some considered heterogeneity across known groups (====; ====; ====). While in contrast to SARP and NAR models that treat the network as pre-determined, ==== and ==== regarded the network structure as unknown parameters, to characterize the heterogeneity in relationships between individuals. Based on the NAR model, ==== and ====; ====), such as gross regional product (====) and house price data (====; ====). Moreover, the GNAR model requires long time periods, while assuming strong time effects and weak spatial effects.====The above studies focused on heterogeneity of network autocorrelations. This also represents susceptibility in networks, which can be important to understand the spread of behaviors (====). Susceptibility represents the extent to which each node is affected by other nodes (====). Conversely, influence represents the intensity of each node's influence on the other nodes (====). Moreover, to characterize the influence of heterogeneity on network data with no replications, ====, ====, and ==== proposed models with different influence parameters for each node. They also developed corresponding methods to estimate the influence parameters.====To model autocorrelation heterogeneity for network data with independent replications across different time points and strong spatial effects, we propose a grouped spatial autoregressive (GSAR) model, which was adapted from the SARP model. It assumes that each node belongs to a latent specific group, which is characterized by a set of parameters in the SARP model. The model could be estimated using the expectation maximization (EM) algorithm. However, the EM algorithm can get stuck in local maxima and is sensitive to the starting point (====; ====The remainder of this paper is organized as follows. Section ====. Simulation studies are explained in Section ====. Further, we describe a real example in Section ==== by considering house prices in Beijing, obtained from an established house information service platform. Finally, we conclude in Section ====. The technical proofs are shown in the Appendix.====We present basic matrix forms in Appendix ====. The proof of ==== is provided in Appendix ====. Finally, ==== and its related proof are provided in Appendix ====.",Grouped spatial autoregressive model,https://www.sciencedirect.com/science/article/pii/S0167947322001815,29 August 2022,2022,Research Article,101.0
"Qin Yichen,Wang Linna,Li Yang,Li Rong","Department of Operations, Business Analytics, and Information Systems, University of Cincinnati, Cincinnati, OH, USA,Department of Mathematical Sciences, University of Cincinnati, Cincinnati, OH, USA,School of Statistics and Center for Applied Statistics, Renmin University of China, Beijing, China","Received 13 March 2021, Revised 2 October 2021, Accepted 6 October 2021, Available online 27 August 2022, Version of Record 14 September 2022.",https://doi.org/10.1016/j.csda.2022.107598,Cited by (1),"Although model selection is ubiquitous in scientific discovery, the stability and uncertainty of the selected model is often hard to evaluate. How to characterize the random behavior of the ","Model selection plays an important role in modern scientific discoveries. There has been exciting work on developing penalized model selection methods for linear regressions, such as Lasso (====), SCAD (====), MCP (====), and others. With many methods at our disposal, it is crucial to understand their stability and uncertainty before committing to one or a few methods. No matter which selection method is applied, selection uncertainty is a ubiquitous issue and has complex impact on the subsequent inference. Therefore, in this article, we propose a few graphical and numerical tools to understand and quantify the model selection uncertainty.==== possible regression models.====Such a distribution, if available, would give analysts a comprehensive understanding of the random behavior of the selected model. Among all the aspects of the random behavior, the stability is one of the most important ones because it measures the selection uncertainty and trustworthiness of the selected model. The issue of model selection uncertainty is two folds. First, given different samples drawn from a common population, the same selection method may identify different models. Second, different selection methods, when applying to the same data set, will result in different models. Although many methods claim to have achieved the optimal performance under specific settings, their selection results are often quite different.====Model selection uncertainty has always been an active area of research (====). ==== propose model confidence set (MCS) to yield information about the uncertainty surrounding the model selection, which has been frequently used to measure the estimation uncertainty (====; ====). ==== propose the variable selection deviation measure to evaluate the reliability and trustworthiness of the selected model based on a model averaging approach (====; ==== adopt bootstrap resampling to assess variable selection stability. ==== propose to estimate F- and G-measures to compare different variable selection methods. ====, ==== propose a model-free criterion for selecting the tuning parameter based on a new estimation stability metric. For a comprehensive review on model selection, please see ====.====On the other hand, the concept of the distribution of the selected model, which is a broader topic containing selection uncertainty, is relatively less studied and has only started to gain more attention. ==== for finite sample and large sample limit. ==== completely characterize the distribution of the Lasso estimator in finite samples for linear regressions and study the model selection properties of the Lasso. These existing works mostly focus on the distribution of the parameter estimate as opposed to the selected model. Although some theoretical results have been established for the distribution of selected model by Lasso, much less work is done for visualizing such a distribution, which is difficult but also useful in practice. To fill this gap in the literature, we propose to visualize the distribution of the selected model in this article and use the visualization to measure selection uncertainty.====In this article, we introduce new visualization tools for the distribution of the selected model. By grouping models of a similar structure together, we are able to visualize the distribution more efficiently and clearly, and reveal important patterns in the distribution that are not available through other types of analysis. The proposed visualization is useful in graphical comparison of different selection methods, giving analysts a good sense of level of randomness each method comes with. With the help of the proposed visualization, we further introduce the concept of model selection deviation (MSD) which can be considered as the standard deviation of the distribution of the selected model. Such a measure allows numerical comparison of various model selection methods in terms of their stability. Under appropriate transformation, we further develop a fast bootstrap estimation procedure for model selection deviation and demonstrate its desirable performance in simulation and real data analysis. Throughout the article, we have focused on linear regressions. However, we would like to point out that the methodology developed in this article can be extended to more complex settings, such as generalized linear models and graphical models, with minimum modifications.====Note that, under a consistent model selection procedure, the probability that the selected model equals to the true model converges to one. Therefore, the model selection uncertainty vanishes asymptotically given a fixed number of covariates. However, under the finite sample size, the model selection uncertainty is nonnegligible. Most of the analysis presented in this article focus on the ====.====This article contributes to the literature in the following aspects. To the best of our knowledge, this article presents the first attempt in the literature to visualize the distribution of selected model. Using the proposed visualization techniques, the random behavior of the different model selection procedures can be compared and studied. Such visualizations allow us to define various attributes of the distribution, such as the mode and the skewness, characterizing various aspects of the distribution. One of the most important numeric attributes is our model selection deviation, which is an extension of the traditional standard deviation of a ==== to the distribution of the selected model. Therefore, the tools provided in this article allow analysts to both quantitatively and graphically compare various selection procedures in terms of their stability and other aspects.====The rest of the paper is organized as follows. In Section ====, we introduce the framework and notations. In Section ====, we propose several new graphical tools to visualize the distribution of the selected model and discuss their connections and distinctions. Based on these visualizations, in Section ====, we introduce a new numeric measure, model selection deviation, to quantify the model selection uncertainty, and further discuss a bootstrap estimation procedure. Lastly, we demonstrate the desirable performance of the proposed visualization and uncertainty measure in simulation in Section ==== and in real data analysis in Section ====. We conclude in Section ==== and provide additional results in the supplementary materials.====The following is the Supplementary material related to this article.",Visualization and assessment of model selection uncertainty,https://www.sciencedirect.com/science/article/pii/S0167947322001785,27 August 2022,2022,Research Article,103.0
"Hu Zhixiong,Prado Raquel","Department of Statistics, University of California, Santa Cruz, 1156 High St, Santa Cruz, 95064, CA, USA","Received 17 November 2021, Revised 9 August 2022, Accepted 12 August 2022, Available online 19 August 2022, Version of Record 22 August 2022.",https://doi.org/10.1016/j.csda.2022.107596,Cited by (3)," matrix grows quadratically, making estimation and inference rather challenging. The proposed novel ==== is proposed for the highly parallelizable posterior inference that provides computational flexibility for modeling high-dimensional time series. The accurate empirical performance of the proposed method is illustrated via extensive simulation studies and the analysis of two datasets: a wind speed data from 6 locations in California, and a 61-channel electroencephalogram data recorded on two contrasting subjects under specific experimental conditions.","Spectral analysis has been widely used to characterize the properties of one or more time series in the frequency domain. Accurate inference of spectral density matrices is critical for understanding the structure underlying the components of a given multivariate temporal process, and for revealing potential relationships across its components. However, inference of spectral density matrices suffers from the ====Several frequency domain methods have been developed for Bayesian spectral analysis of stationary multivariate time series. ==== modeled the spectral density matrix with matrix-valued mixture weights induced by a Hermitian positive definite Gamma process, effectively extending the univariate Bernstein-Dirichlet process prior approach of ====; ====; ====).====-dimensional time series, where a ====-by-==== spectral matrix needs to be inferred. Following an approach similar to that of ====, we model each component in the Cholesky decomposition of the inverse spectral density matrix via a smoothing spline with ==== bases. In this case, the total number of unknown parameters in the model is proportional to ====, which grows quadratically with the number of time series ====, and linearly on the number of basis functions ====. Using low-rank factor representations such as those proposed by ==== and ==== are large, resulting in a tremendous computational overhead.====). For instance, ====; ====). In comparison to MCMC, VI tends to be much faster and scalable for large-scale inference problems.====The remainder of the article is organized as follows. Section ==== specifies the model and priors used for spectral analysis of stationary multivariate time series. Section ==== describes the proposed stochastic gradient variational Bayes scheme for fast and scalable posterior inference. Section ==== reports results of extensive simulation studies that illustrate the accuracy and scalability of the proposed approach. Section ====).====The following is the Supplementary material related to this article.",Fast Bayesian inference on spectral analysis of multivariate stationary time series,https://www.sciencedirect.com/science/article/pii/S0167947322001761,19 August 2022,2022,Research Article,104.0
"Robin Stéphane,Scrucca Luca","MIA-Paris, Université Paris-Saclay – AgroParisTech – INRAE, Paris, 75005, France,Department of Economics, Università degli Studi di Perugia, Perugia, 06123, Italy","Received 5 January 2022, Revised 22 June 2022, Accepted 25 July 2022, Available online 8 August 2022, Version of Record 17 August 2022.",https://doi.org/10.1016/j.csda.2022.107582,Cited by (2),The entropy is a measure of uncertainty that plays a central role in ,", is an extension of the concept of entropy introduced by ====. Consider a multivariate continuous random variable ====. The entropy of ==== is defined as==== where ==== is the support of the random variable. The entropy is a measure of the average uncertainty or information content in a random variable, and forms one of the core ideas in ====. Entropy estimation is therefore a widespread problem that occurs in a broad range of domains, including image analysis (see Section ====) to name only two. For a comprehensive introduction to the field see ====.====Estimation of the entropy in ==== by drawing iid samples ==== of size ==== from ==== and then compute:==== as ====, which depends on the unknown (possibly a vector) parameter ==== for the multivariate Gaussian, and ==== or ==== for related distributions. In particular, if ====, where ==== is the mean vector and ====. Thus, when a closed-form expression is available the problem becomes that of estimating the unknown parameter ====. If ====, then the estimate of the entropy is guaranteed to be both asymptotically unbiased and efficient (see ====, Theorem 7.3, p. 183).====If we cannot assume a particular known distribution, we need to estimate ==== (====), and so the entropy as by-product, are provided by the R package ==== (====), and by the R package ==== (====). A brief description of each of these methods is given at the beginning of Section ====.====A semiparametric estimate of ====, “[f]inite mixture distributions can be used to derive arbitrarily accurate approximations to practically any given probability distribution, provided that the number of components is not limited”. Following this line of thought, we propose a generic approach to estimate the entropy of an arbitrary (multivariate) distribution ==== using a finite mixture as an approximation for ====. We show that a natural estimate can be derived as a side product of the celebrated EM algorithm (====). Because of their popularity, we focus here on Gaussian mixture models (GMMs), but the approach can be extended to any mixture. Our approach takes advantage of the huge amount of both computational tools and theoretical knowledge about GMMs. One interesting feature is that, thanks to the model selection theory of GMMs, our method does not resort to any tuning parameter.====In this contribution we propose a method for estimating the entropy of a (possibly multivariate) distribution, using a finite mixture as an approximation of the distribution of interest ====. The paper is organised as follows. Section ==== derives the entropy for a general mixture, while Section ==== contains the proposed estimation method. Section ==== presents some simulation studies to evaluate the performance of the proposed entropy estimation procedure and compares it with some of the estimation methods mentioned above. Section ====, and structure identification of tree-shaped graphical models ====. The final section provides some concluding remarks.====Several approximations to the entropy of a random variable ==== with Gaussian mixture distribution ==== have been proposed in the literature. In the following, we briefly describe some approximations that have been used in the simulation studies in Section ====. ",Mixture-based estimation of entropy,https://www.sciencedirect.com/science/article/pii/S0167947322001621,8 August 2022,2022,Research Article,105.0
"Hector Emily C.,Luo Lan,Song Peter X.-K.","Department of Statistics, North Carolina State University, Raleigh, NC, 27695, USA,Department of Statistics and Actuarial Science, The University of Iowa, Iowa City, IA, 52242, USA,Department of Biostatistics, University of Michigan, Ann Arbor, MI, 48109, USA","Received 8 November 2021, Revised 14 July 2022, Accepted 29 July 2022, Available online 8 August 2022, Version of Record 17 August 2022.",https://doi.org/10.1016/j.csda.2022.107587,Cited by (0),"Two dominant ====, that uses the strengths of both strategies for computationally fast and statistically efficient supervised learning. PASA's architecture nests online streaming processing into each distributed and parallelized data process in a MapReduce framework. PASA leverages the advantages and mitigates the disadvantages of both the MapReduce and online streaming approaches to deliver a more flexible paradigm satisfying practical computing needs. The authors study the analytic properties and ==== of PASA, and detail its implementation for two key statistical learning tasks. PASA's performance is illustrated through simulations and a large-scale data example building a prediction model for online purchases from advertising data."," independent observations ====, ==== denoting the outcome and ==== may be so big that a direct analysis of the whole data using conventional methodology is computationally intensive or prohibitive. Existing approaches such as subsampling-based methods (====, ====) are effective in downsizing the data volume and reducing the computational burden of analysis, but are statistically less efficient than the traditional methods using the full dataset. In this paper, we model the relationship between the mean ==== of outcome ==== and explanatory variables ====, ====, where ==== is a known link function and ====The MapReduce strategy, due to its scalability, is one of the preferred strategies to handle the intensive computing demands of estimating ==== when the sample size ==== is very large. Essentially, this approach divides data into smaller data blocks and analyses individual blocks separately, typically in parallel. The results of these separate analyses are then combined using, for example, summary statistics (====), estimating functions (====) or ====-value functions (====); see also ====; ====; ====; ==== and references therein. The main appeal of this approach is a trade-off between statistical efficiency and computational speed achieved by selecting the number and size of data blocks. An approach with more and smaller blocks sacrifices statistical efficiency for lower computational cost, whereas an approach with fewer and larger blocks retains desirable statistical efficiency at the price of increased computational cost. For the latter approach with large data blocks, using a computationally inexpensive and statistically efficient estimation procedure in each large data block, such as renewable learning (====), would mitigate the trade-off between computational speed and statistical efficiency.====The renewable learning proposed by ====; ====; ====The rest of this paper is organized as follows. Section ==== describes the formal modeling setup and statistical framework. Section ==== illustrates the performance of the proposed algorithm with simulations. Section ==== illustrates the implementation of PASA with the analysis of online advertising data. Section ==== concludes. Technical details are deferred to the Appendices.====The following is the Supplementary material related to this article.",Parallel-and-stream accelerator for computationally fast supervised learning,https://www.sciencedirect.com/science/article/pii/S0167947322001670,8 August 2022,2022,Research Article,106.0
"Tang Qingguo,Tu Wei,Kong Linglong","School of Economics and Management, Nanjing University of Science and Technology, Nanjing 210094, People's Republic of China,Department of Public Health Sciences and Canadian Cancer Trials Group, Queen's University, Kingston, Canada,Department of Mathematical and Statistical Sciences, University of Alberta, Edmonton, Canada","Received 3 November 2020, Revised 23 July 2022, Accepted 27 July 2022, Available online 4 August 2022, Version of Record 11 August 2022.",https://doi.org/10.1016/j.csda.2022.107584,Cited by (2),", which is desirable for high dimensional setting. ==== such as convergence rate and ==== have been established. Finite sample performances are studied through simulations and data analysis in functional neuroimaging and real estate analytic.",", ====, ====, ====, ====, ====, ====, ====, among many others.====) are popularly used due to the simplicity and flexibility. ==== generalized additive models to the functional regression setting and proposed functional additive model (FAM) considering only functional predictors. However, in practice, it is beneficial to simultaneously consider both functional and scalar predictors while modeling. To address this issue, ==== proposed a functional semiparametric additive model for the effects of a functional covariate and several scalar covariates and a scalar response. ==== proposed a partially linear functional additive model with multivariate functional covariates. ====, ==== proposed a class of partially linear model allowing more than one functional predictor, incorporating both continuous and discrete effects of functional variables. ====Let ==== be a ====-dimensional vector of random variables. Let ==== be a ====-dimensional vector of random variables and ==== be a zero-mean and second-order (i.e., ==== for all ====, where ==== where ==== is an unknown constant, ==== is an unknown square integrable slope function on ====, ==== is a ====, ==== are unknown functions, ==== is a random error with mean zero and variance ====, and ==== is independent of the covariates ==== for all ====.====, ====, ==== and ==== to be observed at a set of discrete points with noise.====Traditional methods for estimating unknown functions in additive models include Spline approach (====; ====) and backfitting procedures (====; ==== in model ====. Functional principal components analysis (FPCA) is used to estimate of rest unknown parameters in model ====The rest of the paper is organized as follows: Section ==== introduces the estimation procedures. The asymptotic properties of the proposed estimators are established in Section ==== and Section ==== presents two data analysis in neuroimaging and real estate market analysis. Discussion and concluding remarks can be found in Section ====.====The following is the Supplementary material related to this article.",Estimation for partial functional partially linear additive model,https://www.sciencedirect.com/science/article/pii/S0167947322001645,4 August 2022,2022,Research Article,108.0
"Bianconcini Silvia,Cagnone Silvia","Department of Statistical Sciences, University of Bologna, Italy","Received 6 December 2021, Revised 20 July 2022, Accepted 27 July 2022, Available online 3 August 2022, Version of Record 9 August 2022.",https://doi.org/10.1016/j.csda.2022.107585,Cited by (0),"When dynamic ==== are specified for discrete and/or mixed observations, problems related to the integration of the likelihood function arise since analytical solutions do not exist. A recently developed dimension-wise quadrature is applied to deal with these likelihoods with high-dimensional integrals. A comparison is performed with the pairwise likelihood method, one of the most often used remedies. Both a real data application and a simulation study show the superior performance of the dimension-wise quadrature with respect to the pairwise likelihood in estimating the parameters of the latent autoregressive process.","; ====; ====; ====).====Given a time series of count data, ==== where ==== represents a vector of time-dependent latent variables, with ====, and ==== with ==== the corresponding vector of coefficients.====Inference for these models is cumbersome because the likelihood function depends on an intractable ====). However, the simplicity of the standard Laplace approximation has a cost related to the fact that the order of the approximation error is fixed. Moreover, it becomes less adequate as the discreteness in the data increases (====). Several authors developed higher-order versions of the method to improve the approximation error (====; ====). However, their implementation requires computing many partial derivatives of the integrand.====Simulation-based approaches have also been proposed and, among them, importance sampling techniques and some of their extensions (====; ====; ====) have been primarily discussed. Monte Carlo expectation-maximisation algorithm that treats the latent variables as missing values (====), and, as an alternative to the latter, a Monte Carlo implementation of the Newton-Raphson (====) have also been developed. These simulation-based procedures can be computationally intense and provide stochastic rather than numerical convergence, which is difficult to assess.====Alternative methods that produce estimators with desired statistical properties and that, in addition, simplify the estimation process are greatly needed. To tackle the problem, a recently developed dimension-wise technique (====; ====) is proposed and compared with one of the most popular methods, that is the composite likelihood (====) recently extended to the parameter-driven models for count data (",The dimension-wise quadrature estimation of dynamic latent variable models for count data,https://www.sciencedirect.com/science/article/pii/S0167947322001657,3 August 2022,2022,Research Article,109.0
"Zhong Yan,Sang Huiyan,Cook Scott J.,Kellstedt Paul M.","KLATASDS-MOE, School of Statistics, East China Normal University, China,Department of Statistics, Texas A&M University, United States of America,Department of Political Science, Texas A&M University, United States of America","Received 10 November 2021, Revised 18 July 2022, Accepted 22 July 2022, Available online 29 July 2022, Version of Record 9 August 2022.",https://doi.org/10.1016/j.csda.2022.107581,Cited by (1),Large spatial datasets with many spatial ,"Large spatial data, which include numerous observations and variables on a large spatial domain, have become increasingly common across a number of real-world applications. Using these spatial data, researchers are often curious to discover the relationship between a response variable and a set of ====, and ==== covariates, ====, are collected at ==== locations with ====. For each location, the SVC model assumes that ==== and ==== follow==== where ==== is the location-specific coefficient vector, ==== is the global intercept term fixed for all locations, and ==== is a random error term. To capture the spatial dependence unexplained by covariates, the first entry of ==== can be set to 1, and then the first entry of ==== refers to the spatially varying intercept for the ==== location. Among existing SVC models, two different kinds of assumptions are usually made on the spatial dependence of ====: The first kind of assumption is that ==== changes smoothly across the spatial domain. For example, the geographically weighted regression model (GWR, ====) extends the local linear regression to the geographical content to estimate the varying coefficients with smooth spatial kernel weighting functions. ====. Recently, ==== propose a generalized SVC with ==== approximated by smoothing splines. The second kind of assumption is that the effects of covariates vary across the study domain rather abruptly across areas within the study domain, thereby forming latent spatially clustered patterns. Under this assumption, ==== propose a spatially clustered coefficient regression (SCC) model to estimate ====, which builds a graph fused lasso regularized optimization to encourage homogeneity between ==== at two adjacent locations. ==== extend the SCC model to a regression setting over networks, and ==== provide a clustered SVC model for spatio-temporal data using scan statistics. ==== construct the Bayesian spatially clustered coefficient (BSCC) model with a random spanning tree cut partition prior. Each of the studies mentioned above is designed for small ====, that is, for studies with few covariates. When ==== is large, model overfitting will occur in most of these approaches, and many also face the high computational cost issue, especially the Bayesian methods.====Secondly, in research on high-dimensional statistics, several methods have been proposed for spatial variable selection. ==== and ==== consider binary spatial regression models and proposed penalized quasi-likelihood methods with spatial dependence for variable selections. ==== and ====Currently, the study of variable selection in SVC models is at an early stage with limited work. ==== work on lattice data and propose a Bayesian variable selection procedure for SVC models. ==== and ==== provide GWR-based variable selection method with lasso and elastic-net penalties respectively. Both of them return variable selection results at each location that may not be easy to interpret in real applications. Recently, ====; ====In addition, the proposed RSCC model has several other advantages. It allows the investigation of different clustered patterns with great flexibility in their shapes for different regression coefficients. Moreover, the number of selected variables and clustered patterns are both treated as unknown and determined from data-driven approaches. Finally, since the method is built upon graphs, it can be used beyond the spatial context to solve the variable selection problem in other clustered coefficient models where observations can be related by a graph or a network. For examples of such contexts, see our discussion in Section ====.====To demonstrate the utility of the RSCC model to real data, we undertake a study of county-level COVID-19 vaccination rates in the United States. Several recent studies have shown that COVID-19 vaccination uptake varies by county and is not consistently correlated to key characteristics (====; ====). As these studies demonstrate, the novelty of the recent pandemic means researchers have yet to arrive at a canonical set of covariates that explain cross-county vaccination rates. Therefore, rather than only focus on a small pre-defined set of variables, we instead collect 38 social, demographic, and economic variables to serve as the candidate set of potential covariates. Our analysis of 3076 counties identifies a number of influential factors (e.g., education, households, and cumulative COVID-19 cases) that govern vaccination acceptance. Moreover, our findings also indicate several interesting within-state and across-state spatially clustered patterns of covariate effects.====The paper is organized as follows. In Section ====, we propose our RSCC model. Section ==== describes the efficient algorithm for the implementation of our model. In Section ====, we present the theoretical results of this model. Sections ==== and ==== include the simulations to illustrate the model performance and the application to the COVID-19 vaccination acceptance. We offer discussions in Section ====. The proofs, additional simulation results, and information about the real data are provided in the supplementary material.====The following is the Supplementary material related to this article.",Sparse spatially clustered coefficient model via adaptive regularization,https://www.sciencedirect.com/science/article/pii/S016794732200161X,29 July 2022,2022,Research Article,110.0
"McElroy Tucker S.,Jach Agnieszka","U.S. Census Bureau, Research and Methodology Directorate, Washington, DC, USA,Department of Finance and Economics, Hanken School of Economics, Helsinki, Finland","Received 10 September 2021, Revised 21 June 2022, Accepted 22 July 2022, Available online 29 July 2022, Version of Record 8 August 2022.",https://doi.org/10.1016/j.csda.2022.107580,Cited by (1),"A ==== for identifying the differencing operator of a non-stationary time series is presented and tested. Any proposed differencing operator is first applied to the time series, and the ==== is tested for zeroes corresponding to the ","), and can be described through differencing polynomials (an approach summarized in ====) or through deterministic functions of unknown structure – see ==== (====, ====)), wavelets (==== or ==== for discussion), and is the focus of this article.====The differencing polynomial ==== has all its roots on the unit circle of the complex plane (henceforth, “unit roots”), and is defined such that the time series ==== is stationary after differencing, viz. ==== is stationary, where==== and ====). The approach to time series data analysis first advocated in ==== involves determining marginal transformations of the data, followed by identification of ====, and the summary in ====. Although extensions were made in ==== and ====, difficulties with the practical performance have been remarked on by many authors (====, ====, ====, ====).====The goal of this paper is to provide a procedure to identify ==== determined by exploratory techniques – such that we are confident that ====. This approach of testing for over-specification has a rich history: ====, ====, and ====, ====, and ====. In order to remove the nuisance of ARMA specification, nonparametric approaches to estimating the spectral density were considered in ====; our procedure is similar in spirit, but avoids the difficulties in implementation of ====) together with subsampling methodology (====), thereby obtaining a testing procedure that avoids post-model selection biases.====In over-specification testing, one supposes that an initial specification of ==== – this latter technique is explained in detail in Section ==== below. Then one seeks to test whether this ==== is specified correctly. The main novelty of our contribution is to adopt a nonparametric approach to testing for zeroes in the spectral density using a more modern asymptotic theory (cf. the vanishing bandwidth fraction asymptotics of ====), and to construct a feasible methodology through backwards deletion and subsampling. The pitfalls of specifying an ARMA model are avoided; although the user must choose taper and bandwidth for the spectral estimator, the impact of these choices upon critical values is automatically addressed through the subsampling methodology.====The following is the Supplementary material related to this article.",Identification of the differencing operator of a non-stationary time series via testing for zeroes in the spectral density,https://www.sciencedirect.com/science/article/pii/S0167947322001608,29 July 2022,2022,Research Article,111.0
"Lee Sangyeol,Kim Dongwon,Kim Byungsoo","Department of Statistics, Seoul National University, Seoul 08826, Republic of Korea,Department of Statistics, Yeungnam University, Gyeongsan 38541, Republic of Korea","Received 28 September 2021, Revised 17 July 2022, Accepted 18 July 2022, Available online 22 July 2022, Version of Record 26 July 2022.",https://doi.org/10.1016/j.csda.2022.107579,Cited by (1)," are verified. As an application, the change point test based on the QMLE and MDPDE is illustrated. The ==== study and real data analysis using the number of weekly syphilis cases in the United States are conducted to confirm the validity of the proposed method.",", ====, ====, and ====. Many authors have developed theories and applications for these models, including ====, ====, ====, ====, ====, ====, ====, and ====.====In the aforementioned studies, the time series of the counts was univariate. However, because time series of counts are often multivariate, multivariate versions of INAR and INGARCH models have also been developed in the literature. ==== and the papers cited therein. By contrast, ==== proposed bivariate Poisson-INGARCH models constructed using a trivariate reduction method. See ==== and ====. ==== recently proposed the Poisson multivariate INGARCH (MINGARCH) and log-MINGARCH models and established asymptotic theorems for the QMLE of model parameters. Those authors invested more effort to specify the joint distribution of multivariate time series and the marginal distributions of its components, which inevitably escalates the complexity of modeling in comparison with the univariate INGARCH case. To ease the burden of modeling, we propose a simpler scheme that applies the univariate INGARCH model of ====This simplification significantly increases the tractability of the models because it is not concerned with the specification of the joint distributions. Moreover, as the marginal distributions can differ from each other and the conditional mean equation is not necessarily linear, unlike in previous studies, model flexibility can be enhanced. This modeling scheme is inspired by the viewpoint that the conditional mean equation constitutes the main body of modeling while the specification of underlying joint distributions is not a primary concern, particularly when parameter estimation and subsequent prediction are the main tasks in inference. See ====, ====, and ====, to cope with model bias and outliers. We employ MDPDE because it efficiently makes robust inferences under various circumstances, and is capable of adjusting the trade-off between efficiency and robustness by controlling the tuning parameter. See ====, ====, ====, and ====, who deal with the MDPDE for INGARCH models and demonstrate the validity of the MDPDE.====As an application, we also consider the problem of detecting a change point based on the cumulative sum (CUSUM) test because of its importance in practice. The change point problem has a long history of returning to ====. Since then, the CUSUM test has been acclaimed as a popular method for detecting parameter changes in the underlying models. For a review, we refer the reader to ====, ====, and ====. Several authors have studied the change point test for INGARCH models as well, including ====, ====, ====, ====, ====, ====, ====, ====, ====, and ====. For relevant references regarding the MDPDE-based CUSUM test for univariate INGARCH models, refer to ==== and ====The remainder of this study is organized as follows: Sections ==== and ==== considers a change point test based on these estimators. Section ==== provides a real data analysis using the number of weekly syphilis cases in the United States. Finally, Section ==== concludes the paper. The proof of the main theorem is provided in the Appendix ====.====In this Appendix, we prove the theorems in the previous sections. Although the proofs for the consistency and asymptotic normality and the Browian bridge result on the CUSUM tests have some overlaps with those in ==== and ====, they are subtly different in detail because of the different sets of conditions.====Below we address the first and second derivatives of ====:==== with==== In particular, we can express",Modeling and inference for multivariate time series of counts based on the INGARCH scheme,https://www.sciencedirect.com/science/article/pii/S0167947322001591,22 July 2022,2022,Research Article,112.0
"Uddin Md Nazir,Gaskins Jeremy T.","Medpace Inc, 5375 Medpace Way, Cincinnati, OH 45227, USA,Department of Bioinformatics and Biostatistics, University of Louisville, 485 E. Grey Street, Louisville, KY 40202, USA","Received 12 July 2021, Revised 19 April 2022, Accepted 10 July 2022, Available online 20 July 2022, Version of Record 28 July 2022.",https://doi.org/10.1016/j.csda.2022.107568,Cited by (2),Multiple ,"Variable selection in ==== is an unordered categorical response with ==== categories, ==== ====, and there are ==== predictors for each observation ====. Here ==== to provide the intercept term for ==== subject. The MNL regression consists of ==== for ====. In this model, ==== is the vector of log-odds ratios of class ==== versus class ====. Without loss of generality, we let class ==== serve as the baseline class, and this model is known as the baseline category logit model. By combining all ==== will be in the ====th class (====) comes from==== Note that to make the equation ==== identifiable, we define ==== since ====.====In contrast to the MNL, the multinomial probit model uses ====, the cumulative distribution function of a standard normal, as the link function so that ==== for each ====. While this model can fit using Metropolis-Hastings sampling (as in ====), it is often difficult to appropriately tune the sampler and to ensure convergence. More commonly, the MNL model can be reframed by considering ==== in terms of normally distributed latent variables ====. The response ==== is determined by through the ====-dimensional ==== through==== The latent variable ==== is modeled as==== where ==== is ==== and ====. Although it is relatively easy to conceptualize and fit a (Bayesian) hierarchical model framework connecting the predictors to latent variables to the response, interpretation is not straightforward due to the fact that the coefficients act on the latent ==== rather than on the response ====. ==== and ==== have directly compared the MNP and MNL models and argue that there is no general rule in favor of one model over the other. Our perspective is to prefer MNL over MNP due to its more straight-forward interpretation and because the Polya-Gamma (PG) data augmentation strategy developed by ==== facilitates conjugate sampling. However, many aspects of our proposed methodology could be implemented in an MNP approach.====In the case of a large number of predictors ====Discrete choice priors, the most common of which is the spike-and-slab (S&S), explicitly characterize each predictor as active or inactive through a discrete binary model that determines whether ====, the coefficient of the ==== predictor, will be non-zero or zero. For the ====s that are active (non-zero), a conditional prior specifies a continuous distribution for these coefficients. This model originates from ====, and numerous authors have explored various adjustments to this general approach (e.g., ====; ====; ====; ====; ====In contrast to the discrete selection models, the global-local (GL) shrinkage framework (==== is expressed as==== with ==== and ====. Here, ==== is the local variance component which is predictor-specific. This allows deviation in the degree of shrinkage across the predictors, and ==== is the global variance component which determines the overall level of shrinkage towards zero. Both distributions ====, for local shrinkage parameter, and ====, for the global parameter, should have large mass close to zero to allow aggressive shrinkage, but the local distribution should also be heavy-tailed to leave large signals unshrunk. Some of the most common variable shrinkage methods within the GL framework include the following: Horseshoe (HS; ====), Normal Gamma (NG; ====), Dirichlet-Laplace (====), and Horseshoe+ (====). The interested reader is encouraged to consider the survey paper on different Bayesian shrinkage methods by ====While the literature is rich for variable selection and shrinkage for linear regression models, there has been limited work in the context of categorical regression. ==== proposed a variable shrinkage method in MNL regression model with a Normal Gamma prior structure. Recently, ==== explored Bayesian variable shrinkage with a horseshoe prior on the coefficients. Additionally, ==== considered a selection-based approach using a mixture of ====-priors combined with discrete predictor choice. Similarly, ====In the case of correlated continuous multi-outcome responses, ==== categories, but the MNL yields ==== models that must each be fit. The main goal of this research work is leveraging the features in the GL framework to improve variable selection by developing new shrinkage priors that encouraging similar predictors to be selected across models for different response levels. Similarly to the strategy in ====, this may be achieved by sharing the local parameters across category models.====The remainder of this manuscript is divided into the following five sections. In section ====, we introduce the proposed shared shrinkage framework. Parameter estimation through Markov chain Monte Carlo (MCMC) is described in section ==== along with the data augmentation strategy required to gain conjugate sampling. In section ====, we perform simulation studies to evaluate the performance of our model. In particular, we consider the coverage rates of the coefficient credible intervals as an additional criteria for model performance evaluation. In section ==== provides some concluding remarks and discussion. This manuscript also has a corresponding supplementary Appendix file with additional details, tables, and figures.====The following is the Supplementary material related to this article.",Shared Bayesian variable shrinkage in multinomial logistic regression,https://www.sciencedirect.com/science/article/pii/S0167947322001487,20 July 2022,2022,Research Article,113.0
"Claramunt González Juan,van Delden Arnout,de Waal Ton","Leiden University, Wassenaarseweg 52, 2333 AK Leiden, the Netherlands,Statistics Netherlands, PO Box 24500, 2490 HA The Hague, the Netherlands,Tilburg University, PO Box 90153, 5000 LE Tilburg, the Netherlands","Received 11 March 2021, Revised 8 July 2022, Accepted 12 July 2022, Available online 19 July 2022, Version of Record 11 August 2022.",https://doi.org/10.1016/j.csda.2022.107569,Cited by (0),"A Multivariate Mixed method for Statistical Matching (MMSM) is proposed. The MMSM is a predictive mean matching method to impute values when integrating two datasets from the same population without overlapping units measuring several common and non-common variables. It considers the multivariate structure of the data by using multivariate ==== regression. The MMSM can also include auxiliary information from an additional dataset to improve the computation of intermediate values, and constraints to improve the selection of the donors. The results from a simulation study show that including information from an auxiliary dataset leads to far better results, especially in terms of bias and percentage of correct imputations. The inclusion of constraints also increases the quality of the imputations, and hence of the statistical matching.","National Statistical Institutes (NSIs) all over the world aim to reduce the response burden. One way to do so is by re-using already available data sources. For example, at ====In case such overlapping datasets do not exist, one may try to combine two representative samples from the same population with no overlapping units containing the variables of interest. Datasets without overlapping units cannot be joined by using record linkage methods. The statistical technique that handles this situation is statistical matching. Statistical matching is fundamentally different from record linkage: whereas in record linkage data from the same unit are linked, in statistical matching data from different units are matched.====The basic statistical matching structure is the following: two datasets with no overlapping units, ==== and ====, are obtained from two independent sample surveys from the same population. ==== contains information about the sets of variables ==== = ==== and ==== = ==== for all ==== units in ====, ==== = 1,…, ==== where ==== is the number of variables in ====, and ==== = 1,…, ==== where ==== is the number of variables in ====. Similarly, ==== contains information about the sets of variables ==== and ==== = ==== for all ==== units in ====, ==== = 1,…, ====, and ==== = 1,…, ====, where ==== is the number of variables in ====. The ==== and ==== variables are only available in either ==== or ==== while the ==== variables are available in both datasets. Throughout this article we will assume that datasets ==== and ==== have been obtained by means of simple random sampling (SRS) from the population.==== and ====, (e.g. the correlations between ==== and ====, ====, ==== and ==== for all units of ==== and ==== and compute the parameters of interest based on this synthetic dataset (the micro approach) (see, e.g. ====). According to ==== micro and macro approaches lead to essentially equivalent results for the parameters of interest.====The current article focuses on the micro approach. In the micro approach, the datasets are fused to construct a single synthetic dataset (see, ====). In practice, it is usually sufficient to complete only one of the datasets. Without loss of generality, we will consider ==== as the recipient dataset (i.e. the dataset which will be completed), and ==== as the donor dataset. This is, in fact, a form of imputation where values that are missing (by design) for ==== in dataset ==== are imputed using donor values from dataset ====. In this article we will therefore often say that values from dataset ==== are imputed for missing values in dataset ====, rather say than that units of dataset ==== are matched to units from dataset ====.====). These procedures substitute the missing values in the units in the recipient dataset by values from similar units in the donor dataset. Non-parametric models lead to values that are actually observed in the population. Finally, to exploit the benefits of the previous two types of methods a third type appeared, the mixed methods (====). In the present study we focus on a group of mixed methods, Predictive Mean Matching (PMM) methods, which were introduced for statistical matching by ==== (see also ====) and have shown promising results when dealing with missing data, see, e.g., ==== and ====For statistical matching, an important issue related to parametric and mixed approaches is the identification problem (==== and ====, i.e. ====, are not identifiable. The most common solution is to assume that the conditional independence assumption (CIA) holds, i.e. that ==== and ==== are conditionally independent given ====. If the CIA is assumed to hold, the identification problem vanishes. However, the CIA rarely holds and, in fact, it is not possible to assess if it holds from the information in datasets ==== and ==== only. Consequently, in many situations an alternative to the CIA is needed. The alternatives require either other assumptions such as the pairwise independence assumption (====) or the use of auxiliary information. Auxiliary information may be available in different forms such as an additional sample, ====, containing proxy values for ====, ==== and ==== (or at least for ==== and ====) for all units, or parameter estimates obtained from other datasets. Most researchers are unlikely to have access to such as an additional sample ==== with complete proxy values for all target variables. NSIs, however, are in the luxury situation that they often do have access to such an additional sample. In fact, without such an additional sample most NSIs would likely not even consider statistically matching two disjoint datasets.====In the terminology of ====, the availability of a third sample ==== with complete proxy values for the target variables means that the statistical matching problem (almost) falls into their non-monotone missing pattern I (see their Table 1) instead of their more complicated non-monotone missing pattern II, with the important difference that in our case we only have proxy values for the units in ==== rather than the true values.====In our study a complete third dataset ==== containing proxy values for ====, ==== and ==== will be considered for solving the identification problem whenever the CIA is not assumed to hold. In particular, we will assume that the conditional correlations ==== in ==== are the same as in the target population. In ==== this is referred to as the partial correlation assumption (PCA), and we will do so too in the current article. The PCA is a more general assumption than the CIA in the sense that it allows the conditional correlation between a variable in ==== and a variable in ==== given ==== to differ from zero. Throughout this article we will assume that dataset ==== has been obtained by means of SRS from the corresponding population.====), constraints based on inequalities (====) and/or constraints on the parameter values were used, e.g. in ====. In the current article, we focus on continuous data. Furthermore, in our study, we also consider so-called soft constraints (see Section ====), a type of constraints which have not been considered before in statistical matching. The main goals of our research are to assess whether using an additional dataset ==== and using constraints on continuous variables improve the quality of the estimated joint distribution of ==== and ====.====We propose a new Multivariate Mixed method for Statistical Matching (MMSM) which belongs to the family of PMM methods. We evaluate MMSM under different cases with and without constraints to assess the suitability of the method and the effect of the addition of different types of constraints on its quality.====It is important to discuss the intended use of our statistical matching approach. As pointed out by a reviewer, a major issue that may arise when statistical matching is applied is “entity ambiguity” (see, e.g., ==== and ====). Entity ambiguity arises “whenever it is not possible to state with certainty that the integrated source corresponds to the target population of interest” (====). Entity ambiguity may lead to what in the folklore at Statistics Netherlands is called the “dog food problem” (see ====). In the context of statistical matching this problem arises when a dataset containing the target variable “amount of money spent per month on dog food” is statistically matched with another dataset – with different units – containing the target variable “do you have a dog as pet?”, where the matching is done using some common background variables, say, “Gender”, “Age” and “Education level”. Since the units in such a statistically matched dataset are synthetic ones rather than units that actually occur in the target population, one may end up with many synthetic units who do not have a dog as pet but do spend money on dog food, and, conversely, many synthetic units who do have a dog as pet but do not spend any money on dog food. This illustrates how entity ambiguity can easily lead to invalid conclusions.====In order to avoid problems due to entity ambiguity, we do not intend to release any statistically matched datasets based on our proposed statistical matching approach. Instead, we intend to use our statistical matching approach only to estimate the correlations between target variables ==== in dataset ==== and target variables ==== in dataset ====. These estimated correlations between ==== and ==== can subsequently be used to answer questions by researchers who want to know more about the relation between ==== and ====. Some researchers may simply be interested in the correlations between variables in ==== and variables in ====, say ====, on some other target variables in ====, say ==== and ====, and some auxiliary variables. The estimated correlations between ==== and ====, in combination with the two available datasets ==== (with ==== and ====) and ==== (with ==== and ====), allow us to answer these kinds of research questions.====Under the PCA, the estimated correlations between variables in ==== and variables in ==== can be obtained from a data ==== with proxy values for ====, ==== and ==== (or in any case for ==== and ====), and under the CIA all conditional correlations between variables in ==== and variables in ==== given ==== are equal to zero. So, under both the PCA and the CIA, the estimated correlations are not affected by entity ambiguity. We stress that our statistical matching approach should only be applied when either the PCA or the CIA holds (at least approximately).====Note that if either the PCA or the CIA were to hold perfectly, statistical matching would not be necessary for our intended use, since the correlations between variables in ==== and variables in ==== could then either be computed directly from dataset ==== (in the case of the PCA) or by setting all correlations between variables in ==== and variables in ==== to zero (in the case of the CIA). However, in practice neither of these assumptions is likely to hold exactly, and, at best, either the PCA or the CIA holds only approximately. By statistically matching the data, and by taking constraints on the matched data into account, we aim to improve the estimated correlations between variables in ==== and variables in ====. Besides, our statistical matching approach also allows us to assess sampling and imputation/matching variance. In our simulation study we examine the effect of departures from the PCA or the CIA.====The remaining sections are organized in the following way. Section ==== describes the data and the variables in the study. Section ==== describes the methods that are used in this study, the types of constraints and their implementation in MMSM. Section ==== explains the procedures to measure the quality of the methods. Section ==== describes the set up of the simulations used to test the MSMM and assess the effect of the constraints. Section ==== shows the results for the different cases. Section ==== discusses the results and their implications in statistical matching.====The following is the Supplementary material related to this article.",Assessment of the effect of constraints in a new multivariate mixed method for statistical matching,https://www.sciencedirect.com/science/article/pii/S0167947322001499,19 July 2022,2022,Research Article,114.0
"Chiou Jeng-Min,Ferraty Frederic,Goldsmith Jeff,Paul Debashis,Shi Jian Qing","Academia Sinica, Taiwan,Toulouse University, France,Columbia University, United States of America,UC Davis, United States of America,Southern University of Science and Technology, China","Available online 9 February 2023, Version of Record 2 May 2023.",https://doi.org/10.1016/j.csda.2023.107726,Cited by (0),None,None,Editorial for the 2nd special issue on high-dimensional and functional data analysis,https://www.sciencedirect.com/science/article/pii/S0167947323000373,9 February 2023,2023,Research Article,118.0
"Ma Shuangge,Mittlboeck Martina,Rubio F. Javier,Liu Catherine C.","Yale School of Public Health, USA,Medical University of Vienna, Austria,University College London, UK,The Hong Kong Polytechnic University, China","Available online 7 December 2022, Version of Record 7 February 2023.",https://doi.org/10.1016/j.csda.2022.107681,Cited by (0),None,None,2,https://www.sciencedirect.com/science/article/pii/S0167947322002614,7 December 2022,2022,Research Article,121.0
"Fokianos Konstantinos,Kirch Claudia,Ombao Hernando","University of Cyprus, Cyprus,Otto-von-Guericke University, Germany,King Abdullah University of Science and Technology (KAUST), Saudi Arabia","Available online 1 December 2022, Version of Record 7 February 2023.",https://doi.org/10.1016/j.csda.2022.107675,Cited by (0),None,None,Editorial for the special issue on Time Series Analysis,https://www.sciencedirect.com/science/article/pii/S0167947322002559,1 December 2022,2022,Research Article,122.0
"Codazzi Laura,Colombi Alessandro,Gianella Matteo,Argiento Raffaele,Paci Lucia,Pini Alessia","Hamburg University of Technology, Institute for Algorithms and Complexity, Hamburg, Germany,Department of Economics, Management and Statistics, University of Milano-Bicocca, Milan, Italy,Department of Mathematics, Politecnico di Milano, Milan, Italy,Department of Economics, University of Bergamo, Bergamo, Italy,Department of Statistical Sciences, Università Cattolica del Sacro Cuore, Milan, Italy","Received 29 June 2021, Revised 22 November 2021, Accepted 29 December 2021, Available online 6 January 2022, Version of Record 9 June 2022.",https://doi.org/10.1016/j.csda.2021.107416,Cited by (6),"Motivated by the analysis of spectrometric data, a Gaussian ","The analysis of the interaction of infrared radiation with matter by absorption, emission, or reflection is accomplished by ====. This technique is used to study and identify chemical substances in solid, liquid, or gaseous forms. For instance, mid-infrared spectroscopy coupled with chemometrics or statistical techniques have been used in the literature to study the substance composition and detect the presence of adulterants in food (====; ====; ====; ====).====).====; ====; ====. As a consequence, we can detect a hidden association between different bands of the spectrum through an association between the corresponding smoothing coefficients.====Inference on B-spline basis coefficients has been used in the functional data analysis literature for assessing the mean structure of functional data along the domain (see e.g., ====; ====; ====) are employed to induce smoothness on the ===='s, translating into a banded precision matrix that is fixed and depends only on a smoothing parameter. However, in our application, assuming that the dependence structure is known and restricted to the structure induced by a random walk prior appears inadequate. Indeed, coefficients associated with closer bands of the spectrum are not necessarily expected to be similar since they may relate to different components. Rather, long range interactions may also exist between far apart portions of the spectrum. Hence, a more flexible modeling framework, encompassing the random walk case, is advocated.====A widely used approach for modeling dependence in samples of functional data is functional principal component analysis (FPCA). We mention here ====, studying FPCA in the Bayesian framework. However, information about the dependency structure obtained by this method is sometimes based on qualitative considerations on the shape of the principal components.====Graphical models have been widely applied to infer several types of networks, particularly under the Bayesian framework, see among others ====; ====; ====; ====; ====; ====, ==== and ====In addition, recent works focused on graphical modeling for multivariate functional data analysis (====; ====; ====, ====). These approaches, known as functional graphical models, aim at depicting the conditional dependence structure among multiple random functions observed over a set of individuals, i.e., they assume a graphical model where the vertices of the underlying graph are the multiple functions and the edges are estimated from the conditional covariance function extended to the functional domain. Rather, in our setting, we work with univariate functional data modeled through a B-spline basis expansion and we assume a graphical model where the nodes of the graph are the smoothing coefficients and the edges reflect the conditional dependence structure among the portions of the functional domain associated to such coefficients. Graphical model and splines have been jointly studied in the literature although in a different context, see e.g., ==== and ====.====Summarizing, our contribution is to introduce a Bayesian hierarchical model for simultaneously smoothing functional data and learning the independence structure along the domain of the curves. Absorbance spectra are modeled as continuous functional data through a cubic B-spline basis expansion such that the dependence between two bands of the spectrum is reflected in the relationship among the corresponding smoothing coefficients. A Gaussian graphical model is then assumed as a prior for basis ====The remainder of this paper is organized as follows. Section ==== introduces the Bayesian graphical model for functional data analysis, discusses the smoothing procedure as well as the covariance selection model. In Section ==== we describe the sampling strategy implemented to sample from the joint posterior distribution of all model parameters, including the graph, and discuss how to summarize the output. Section ==== presents a simulation study, while Section ==== illustrates the results of a real data analysis carried out to study the infrared absorbance spectra of strawberry purees. Finally we conclude with a brief discussion in Section ====.====The following is the Supplementary material related to this article.",Gaussian graphical modeling for spectrometric data analysis,https://www.sciencedirect.com/science/article/pii/S0167947321002504,6 January 2022,2022,Research Article,126.0
Thompson Ryan,"Department of Econometrics and Business Statistics, Monash University, VIC 3800, Australia","Received 14 November 2020, Revised 29 November 2021, Accepted 27 December 2021, Available online 4 January 2022, Version of Record 13 January 2022.",https://doi.org/10.1016/j.csda.2021.107415,Cited by (5),"The best ====. This procedure, referred to as “robust subset selection” (or “robust subsets”), is defined by a ","We study the canonical linear regression model ==== with response ====, predictors ====, and noise ====. It is assumed that the response is centered and that the predictors are standardized. In the low-dimensional regime, where the number of predictors ==== is smaller than the number of observations ====, it is straightforward to estimate ==== can be (much) greater than ====, in which case the least squares estimator is no longer statistically meaningful. One way to navigate such situations is to assume that the underlying model is sparse, i.e., to assume only a small fraction of the available predictors are important for explaining the response. Even when ==== where ==== is an integer such that ====, and the ====-norm ====. Unlike other well-known sparsity-inducing estimators such as the Lasso (====), the best subsets estimator (via its sparsity constraint) directly controls the number of predictors in the model.==== suggests that to solve for the best subsets estimator, one must conduct a combinatorial search for the subset of (at most) ====; ====; ====; ====), actually solving the (nonconvex) combinatorial problem is no small feat. In fact, finding the best subset(s) is an NP-hard problem (====), and popular implementations such as the ==== package ==== do not scale well beyond ====. However, in recent work, ==== showed that the best subsets problem ==== can be formulated and solved (to global optimality) as a ==== has experienced a nearly 60-fold hardware-independent speedup (==== showed that their mixed-integer optimization approach for best subsets can be applied to problems with dimensions as large as ====. This development represents the first time that the best subsets estimator has been tractable for contemporary high-dimensional data after at least 50 years of literature and has paved the way for exciting new research (====; ====; ====; ====; ====; ====; ====; ====; ====; ====).====Despite the impressive developments in computational tools for best subset selection, certain fundamental limitations in the estimator itself remain. Particularly relevant to real-world applications is the robustness of best subsets to contamination in the data or lack thereof. Similar to the nonsparse least squares estimator, best subsets is highly susceptible to contamination in both the response and the predictors. Specifically, in the ====, where a portion of the rows of ==== and ==== are outliers, a single contaminated data point can have an arbitrarily severe effect on best subsets. The absence of robustness to casewise contamination is an important practical limitation of best subsets and raises doubts about the appropriateness of the estimator for many applications of sparse regression involving outliers, e.g., earnings forecasting (====), analytical chemistry (====), and biomarker discovery (====). Although robust adaptions of other sparse estimators such as the Lasso have been studied fairly intensively (====; ====; ====; ====; ====; ====; ====; ====; ====; ====; ====), a lack of similar research is available on the topic of best subsets due to computational considerations. The objective of this paper is to address this gap.====The following is the Supplementary material related to this article.",Robust subset selection,https://www.sciencedirect.com/science/article/pii/S0167947321002498,4 January 2022,2022,Research Article,127.0
"Yang Kai,Yu Xinyang,Zhang Qingqing,Dong Xiaogang","School of Mathematics and Statistics, Changchun University of Technology, Changchun 130012, China","Received 21 May 2021, Revised 1 December 2021, Accepted 2 December 2021, Available online 28 December 2021, Version of Record 4 January 2022.",https://doi.org/10.1016/j.csda.2021.107410,Cited by (11), Monte Carlo (MCMC) methods have been shown to be a useful tool in many branches in ,"An integer-valued time series is count data formed by the states of a certain phenomenon at different moments. It is widely used in various fields in the real world, including industrial (====), commercial (====), economics (====), insurance actuarial (====), quality control (====). Such data appears in, for example, the number of hospital visits per day, the number of insurance claims per week, and the number of confirmed patients of a certain infectious disease per month. Since the pioneering work by ====, who proposed the popular first-order integer-valued autoregressive (INAR(1)) process, the theoretical researches of INAR models have received growing attention. ====, ==== developed the INAR(1) model to ====th order cases, and studied the parameter estimation problems for the INAR(====) processes. ==== studied the model selection, estimation and forecasting problems for the INAR(==== studied the efficient estimation of the autoregressive parameters and innovation distributions for the semiparametric INAR(====) model. ==== studied the likelihood estimation problem for the INAR(==== gave detailed reviews of the recent achievements on inferences for integer-valued time series of counts.====Recently, the applications of such INAR type models have also received considerable attention. ==== applied a ====-valued INAR type model to the price change data set. ==== used a change point INAR model to fit two biometrical data sets. ==== conducted a comparative analysis for the numbers of skin lesions and anorexia data sets in New Zealand via a Poisson-Lindley INAR(1) model. ==== used a zero truncated INAR(1) process to detect the mean increases for the complaint volume data in public calls in Changchun. ==== gave a methodological review for the applications of INAR type models.====). Thereby, it is not suitable to fit such data set with an INAR type model. Researchers also found that (see, e.g., ====, ==== considered the empirical likelihood estimation and non-linearity test problems for the SETINAR(2,1) process. ==== studied the quasi-likelihood inference for the SETINAR(2,1) process, and applied it to a drug crime data set in Pittsburgh. ==== introduced a periodic SETINAR model for the daily births counts in the Quebec. For the recent achievements on modeling such nonlinear phenomena, see ====, ====, ==== and ====.====, ====, ====, ====). As pointed out by ====, MCMC technologies can easily incorporate data augmentation which facilitates inference for such models. In the field of integer-valued time series, ==== considered a Bayesian inference for low integer-valued time series models with intractable likelihoods. ==== studied the Bayesian nonparametric forecasting for the INAR models.====To conduct Bayesian inference for the integer-valued threshold time series models, there are at least two difficult problems that must be solved first. The first problem is that the threshold model is characterized by a piecewise structure, which yields a highly complicated likelihood function. The second problem is that the threshold parameter is unknown, so the estimation of the threshold parameter must be solved first. Motivate by ====, ====, ====, ====, ==== and ====.====The outline of the paper is as follows. In Section ====, we study the Bayesian inference for the SETINAR(2,1) model, a MCMC algorithm with constraints for Bayesian calculation is developed. In Section ====, some numerical results to show the well performances of the MCMC algorithm are presented. A real data example is given in Section ====. Some concluding remarks are given in Section ====.",On MCMC sampling in self-exciting integer-valued threshold time series models,https://www.sciencedirect.com/science/article/pii/S0167947321002449,28 December 2021,2021,Research Article,128.0
"Nibbering Didier,Hastie Trevor J.","Department of Econometrics & Business Statistics, Monash University, Australia,Department of Statistics, Stanford University, United States of America","Received 12 May 2021, Revised 6 October 2021, Accepted 17 December 2021, Available online 21 December 2021, Version of Record 29 December 2021.",https://doi.org/10.1016/j.csda.2021.107414,Cited by (5), in the multiclass-penalized regression model is discussed. Applications to simulated and real data show in- and out-of-sample improvements in performance relative to a standard multinomial regression model.,"Many statistical problems involve the estimation of the ====, performs variable selection by estimating parameter values exactly zero and shrinks the remaining parameter values towards zero. ==== discusses parameter estimation of the lasso in a multinomial logistic regression. ==== extend the lasso in the multinomial logistic regression model to the group-lasso of ====This paper proposes an alternative to lasso by shrinking the parameter space in multiclass models over the class space instead of shrinking over the variable space. The multiclass-penalized regression model penalizes the differences between class-specific parameter pairs to reduce the number of parameters. When the difference between a pair of class-specific parameter vectors is estimated to be zero, the corresponding classes collapse into a ====. We exclude the intercepts from the penalty, and show how unpenalized intercepts ensure that class probabilities within a superclass reflect the observed class frequencies. This provides a rule for classification: The explanatory variables distinguish between superclasses, and within a superclass an observation is classified into the class with the largest number of observations.==== and applied to convex clustering by ====, to maximize the quadratic approximation with respect to the model parameters and subject to a penalty on the difference between class-specific parameters.====We were unaware of the multiclass penalty proposed by ==== at the time we developed our method. We improve upon their penalty specification in three ways. First, they cluster all class-specific parameters toward each other, including intercepts. This is a different approach as it implies that all classes within a superclass have identical class probabilities, even if they have a different number of observations, and therefore does not allow for classification. Second, we introduce weights that vary the strength of penalization on class-specific parameter pairs by the similarity of the two classes. The weighting results in substantial computational gains and enhances the quality of the clustering. Third, we use an approximation to the Hessian that makes maximum likelihood with a large number of classes feasible. We demonstrate this in an empirical application with 49 classes, where the applications in ==== have at most seven classes.====A ====The outline of the remainder of this paper is as follows. Section ==== discusses the general specification of the multinomial logistic regression model, introduces the multiclass penalty, and discusses the clustering mechanism. Section ==== explains the parameter estimation method. Section ==== performs a simulation study to analyze the clustering mechanism and the performance of multiclass-penalized regression. Section ==== applies the model to two empirical applications and Section ==== concludes.====The following two figures provide the descriptive statistics of the data used in Section ====.====This figure shows the frequency counts for the categorical dependent variable. The categories represent destinations of foreign holidays of more than seven days of Dutch households.====This figure shows the frequency counts for the categorical explanatory variables used in the application to holiday destinations. In addition to these dummy variables, holiday destination is explained by the log of the household income.",Multiclass-penalized logistic regression,https://www.sciencedirect.com/science/article/pii/S0167947321002486,21 December 2021,2021,Research Article,129.0
"Granados-Garcia Guilllermo,Fiecas Mark,Babak Shahbaba,Fortin Norbert J.,Ombao Hernando","King Abdullah University of Science and Technology (KAUST), Saudi Arabia,University of Minnesota, United States of America,University of California Irvine, United States of America","Received 2 April 2021, Revised 2 December 2021, Accepted 2 December 2021, Available online 16 December 2021, Version of Record 9 June 2022.",https://doi.org/10.1016/j.csda.2021.107409,Cited by (4)," (LFP) activity from the hippocampus of laboratory rats across different conditions in a non-spatial sequence memory experiment, to identify the most prominent frequency bands and examine the link between specific patterns of brain ==== and trial-specific cognitive demands.","Considerable research indicates that the hippocampus — a brain region highly conserved across mammals — plays a key role in mammals' ability to remember the order in which daily life events occur (====). However, there is little insight into how these processes are accomplished at the neuronal level.====; ====; ====; ====).====However, the current segmentation of the frequency range into the delta-to-gamma frequency bands is ==== and is primarily driven by pragmatic considerations (====). In current research, many neuroscientists now consider some of these bands to be ==== and thus they do not possess the required level of precision in order to identify differences between different type of stimuli and between patient or treatment groups (see ====; ====). There is an increased demand for more precise analyses with finer subdivisions within the bands, for example as “low”-alpha and “high”-alpha (see ====; ==== of these oscillations.====The remainder of this paper is organized as follows. In Section ====. A realistic simulation study is conducted, where the observed simulated LFP is a mixture of AR(2) latent processes with peaks that were actually observed in the LFP signals recorded during the experiments in the Fortin laboratory. In Section ====A Metropolis-Hastings within Gibbs is next described to sample from the posterior distribution of the parameters and is available on github through ====. The algorithm first updates the number of components with a birth-death process, which at each iteration proposes with equal probability to increase the number of components by one or decrease it by one. In the case of a birth step, the M-H ratio is==== where ==== represent a new generation for the partition in the interval ==== assuming the random selection of the index ==== to split that subinterval and ==== is the indicator function. While in the case of choosing a death step the M-H probability is computed as==== where ==== represents the uniform draw when two component are joined by deleting the value ==== from the partition. The proposal distribution to update of the location parameters is uniform in the interval ==== with appropriate conditions to take the modulus over the subinterval defined by the partition. For the scale parameter, is used a similar uniform draw over the interval ====.====The alpha parameter is updated based on the slice sampling to generate a random walk over the subgraph of the marginal posterior distribution of ==== given by:==== where ==== is the prior over ==== in the algorithm is set ==== as log-normal, ==== the observed process size, and ==== is the beta function. The next algorithm presents the steps described.",Brain waves analysis via a non-parametric Bayesian mixture of autoregressive kernels,https://www.sciencedirect.com/science/article/pii/S0167947321002437,16 December 2021,2021,Research Article,130.0
"Battauz Michela,Vidoni Paolo","Department of Economics and Statistics, University of Udine, via Tomadini 30/A, 33100 Udine, Italy","Received 19 March 2021, Revised 9 December 2021, Accepted 9 December 2021, Available online 15 December 2021, Version of Record 21 December 2021.",https://doi.org/10.1016/j.csda.2021.107412,Cited by (1),". To lighten the computational burden of the inferential procedure, a suitable pseudolikelihood, called pairwise likelihood, is exploited. In addition, a group lasso penalty is considered in order to automatically select the number of latent variables included in the model. The good performance of the proposal is illustrated through a simulation study and a real-data example.","Latent trait models (see, for example, ====; ====; ====).====; ====). Although the two approaches seem quite different, in some cases, such as for the probit/normal model, it is possible to specify the same model under the URV approach (see, for example, ====, Section 4.4).====Various inferential procedures have been proposed for the estimation of the simple and the multidimensional IRT models (====; ====; ====; ====) and for the estimation of the models defined within the factor analysis approach (see, for example, ==== and ==== in the IRT framework.====In the present paper we adopt the URV approach and we concentrate on factor analysis models for binary data. Following ====, we reduce the computational burden of the inferential procedure using a simple solution based on a pseudolikelihood, called pairwise likelihood, which belongs to the general class of composite likelihoods (====; ====; ====) are not straightforward to extend to the models of interest in this paper. Therefore, a novel boosting approach based on directions of negative curvature is proposed (====).====The paper is organized as follows. A review of the statistical boosting methods and an introduction to latent trait models for binary data and their likelihood-based inferential procedures are given in Section ====. In Section ==== a new boosting algorithm is proposed and the computational aspects of the procedure are discussed. The method is then applied to simulated and real data in Section ====. Finally, some concluding remarks are given in Section ====.==== propose an efficient linesearch algorithm for solving unconstrained optimization problems specified as==== where ==== is a real valued, twice continuously differentiable function defined on ====, ====, with gradient ==== and Hessian matrix ====. This algorithm belongs to the class of linesearch procedures that use the additional information given by the Hessian matrix and are proved to converge to a second-order critical point, namely a point ==== such that ==== and ==== is positive semidefinite. Its peculiarity, useful for the specification of boosting algorithms, is that only the most promising direction, between that one based on the Newton method and that one related to a negative curvature, is employed at each iteration step. The approach of ==== is extended, for example, by ====, where a suitable combination of these two directions is also considered. As mentioned in Section ====, in the ====-th iteration a pair of directions ==== is computed. The first vector defines a Newton-type direction that guarantees convergence under convexity assumptions, namely under a positive curvature given by the Hessian matrix. The second one specifies, if any, a negative curvature direction (that is, ==== such that ====), which allows moving away from regions of local non-convexity. More precisely, in order to ensure the convergence of the algorithm, the following conditions are required:====Furthermore, the rate of decrease of the objective function ==== is evaluated along the associated quadratic approximation obtained from a Taylor series expansion around ====, and this approximation determines the choice between ==== and ====. For each selected direction, the adaptive algorithm proposed by ==== defines a specific linesearch step which improves the efficiency of the optimization procedure.====The pairwise likelihood-based boosting algorithm introduced in Section ==== follows a component-wise approach and, for this reason, it can be viewed as special case of the algorithm proposed by ====, with a Newton-type direction ==== taking values different from zero for only two component parameters, namely the ====-th elements defined in ====, and a negative curvature direction ==== taking values different from zero for only two component parameters, namely the ====-th elements defined in ====. It is easy to verify that these directions satisfy the conditions C1. and C2., and hence the convergence to a second-order stationary point is assured. More specifically, concerning the first condition, since in our case ====, it is always possible to find a positive constant ==== small enough to satisfy the first inequality in ====, provided that ==== is positive definite, namely that the function is convex in this direction. Furthermore, if we consider ==== it is always possible to find a positive constant ==== large enough to satisfy the second inequality in ====. Finally, with regard to the second condition, the first inequality in ==== is always satisfied by taking the appropriate sign for the eigenvector as done in ====. Since, in our case, ====, the second inequality in ==== is also verified because this direction is chosen only in such a case. Furthermore, it is always possible to find ==== small enough to fulfill the third inequality in ====.",A likelihood-based boosting algorithm for factor analysis models with binary data,https://www.sciencedirect.com/science/article/pii/S0167947321002462,15 December 2021,2021,Research Article,131.0
"Lai Wei-Ting,Chen Ray-Bing,Chen Ying,Koch Thorsten","Department of Statistics, National Cheng Kung University, Tainan, Taiwan,Institute of Data Science, National Cheng Kung University, Tainan, Taiwan,Department of Mathematics, National University of Singapore, Singapore,Risk Management Institute, National University of Singapore, Singapore,Institute of Data Science, National University of Singapore, Singapore,Chair of Software and Algorithms for Discrete Optimization, Technische Universität Berlin, Berlin, Germany,Department of Applied Algorithmic Intelligence Methods, Zuse Institute Berlin, Berlin, Germany","Received 18 February 2021, Revised 26 November 2021, Accepted 28 November 2021, Available online 14 December 2021, Version of Record 21 December 2021.",https://doi.org/10.1016/j.csda.2021.107406,Cited by (3)," (MCMC)-based sampling approaches, the VB approach achieves enhanced computational efficiency without sacrificing estimation accuracy. In a real data analysis scenario of day-ahead natural ==== prediction in the German gas ==== with 51 nodes between October 2013 and September 2015, the VB approach delivers promising forecasting accuracy along with clearly detected structures in terms of dynamic dependence.","Networks have emerged and become widely available in various fields such as in energy transmission, logistics and transportation, and financial systems. Networks are dynamic in terms of their ==== and ==== proposed a sparse graphic network. ==== and ====. According to ==== proposed a network-based independence screening approach for NAR. In the proposed screening approach, they considered the network structure and attempted to screen out the usefulness predictors (covariates) from NAR.====The application of NAR/VAR for large-scale network analysis however remains challenging. Given a network with ==== nodes, corresponding to ==== time series, and supposing that the dynamics depend on the last ==== lags, there will be ==== becomes large or the temporal dependence ==== investigated the theoretical properties of ==== established bounds on the nonasymptotic estimation error of the Lasso-type estimator for structured VAR parameters. ==== proposed several structures for VAR and Lasso, group Lasso and sparse group penalty functions to achieve sparsity in the elements and groups of a network; see also ====, ====, and ====. In addition, ==== developed a two-step approach for NAR/VAR models to investigate the dynamic interconnection in a large-scale network based on sparse assumptions. ==== proposed a lag-one VAR model for low-rank and structured sparse parameter matrices with an underlying network structure. ==== discussed the regularized joint estimation of multiple related VAR models by leveraging a group LASSO penalty and a regular LASSO penalty.====. ==== introduced Bayesian-constrained variable selection. ==== implemented a Bayesian variable selection approach in the VAR framework and named it as the vector autoregression-based Gibbs sampler algorithm (VAGSA). For the high-dimensional VAR model, ==== and ====In our study, we adopt a sparse structure for NAR/VAR model fitting to alleviate the overparameterization inherent in the large dimension network (vector) autoregression model. We consider the three possible structure assumptions with a general framework in elements, groups and lags. Instead of an attempt to verify the feasibility of such a sparse structure, we focus on developing an efficient estimation approach that can be used to handle large-dimensional NAR/VAR at a reasonable computational cost under the sparse assumption. We propose a variational Bayesian (VB) approach for estimating a large-scale dynamic network model, named VB-NAR, which allows for the automatic identification of dynamic structure of networks and directly obtains an approximation of the posterior density. A series of simulations are conducted to demonstrate the numerical performance of VB-NAR. For comparison, we implement two Bayesian-type methods, VAGSA and BIVAS, and the Lasso method of NAR/VAR, BigVAR (====Our work is motivated by ==== and ==== but with significant difference in several aspects. ==== proposed the MCMC algorithm (VAGSA) to address the same structure selection problem. The MCMC algorithm is computationally expensive when the dimension is high and/or when multiple lags are considered. Moreover, there also needs to tune the prior parameters (hyperparameters) before the implementation, which is again time consuming. By contrast, VB-NAR shows fast convergence by utilizing the EM algorithm, where the prior parameters are automatically determined in the maximization step. ==== proposed the VB algorithm (BIVAS) for estimating large-dimensional multiple-response type models. BIVAS considers the row sparse group assumption, whereas VB-NAR segments the diagonal element from each row, when disconnecting the groups, even though both adopt similar EM-type approaches. There is practically relevant economic meaning of the diagonal element, containing the temporal dependence of a node on its own past value. This makes the model structure of VB-NAR different from that of BIVAS, leading to differences in interpretation and performance. In addition, the VB-NAR model takes into account the correlated structures among the nodes, while BIVAS assumes independence in the estimation. Though more flexible, the temporal dependence requires update at every time step in VB-NAR, leading to higher computational cost than BIVAS. Numerical results show that VB-NAR method provides higher accuracy than BIVAS for adopting different model structure assumptions. In summary, the contributions of our study are as follows:====The rest of the paper is organized as follows. Section ==== presents the dynamic network model in the VAR framework. Several types of structural assumptions are also demonstrated. Section ==== presents the proposed variational Bayesian algorithms for large-scale dynamic network inference. Section ==== investigates the finite-sample performance of the proposed VB approach. Section ==== provides a brief summary of our work. We have shared the R codes for VB-NAR in the GitHub VB-NAR.",Variational Bayesian inference for network autoregression models,https://www.sciencedirect.com/science/article/pii/S0167947321002401,14 December 2021,2021,Research Article,132.0
"Gámiz María Luz,Mammen Enno,Martínez-Miranda María Dolores,Nielsen Jens Perch","Department of Statistics and Operations Research, University of Granada, Spain,Institute of Applied Mathematics, Heidelberg University, Germany,Bayes Business School, City, University of London, UK","Received 30 March 2021, Revised 25 November 2021, Accepted 27 November 2021, Available online 13 December 2021, Version of Record 21 December 2021.",https://doi.org/10.1016/j.csda.2021.107405,Cited by (0), are utilized and the missing data information is both estimated and used for further estimation in each iterative step. Theory is developed and a good finite sample performance is illustrated by simulations. The main motivation is an application to French data on the temporal development of the number of hospitalized Covid-19 patients.,", ==== and ====), survival models have not - to our knowledge - been developed taking account of missing information on the origin of durations as investigated in this paper. The missing data survival model described and analysed in this paper is similar but different from the backcalculation method of ==== and ====, see also ====, ====, ==== and ====, however such methodology seems to be too complicated to serve as a benchmark methodology in the beginning of a pandemic. Our methodology is simpler than the EM-algorithm where complicated conditional means have to be considered, see for example ==== and ==== for recent contributions in this direction. It is also different from missing data work as introduced in ====, because in our model it is the starting point of a duration that is missing. The reason our algorithm is working anyway is that the starting point of the duration can be recovered via the changes in new cases over time, the pattern of changes of new cases translates into a pattern of durations. The preliminary theory provided in this paper provide evidence for this claim.====Our method is illustrated on recent data from the Covid-19 pandemic that is available for most countries or even regions within countries. The empirical illustration of this paper focuses on duration effects on mortality and recovery of hospitalized Covid-19 patients via aggregated data. Since the beginning of the Covid-19 pandemic there has been daily information on the number of hospitalized patients with the virus. Often data are aggregated and contain only information about daily numbers of patients staying in hospitals or leaving the hospital at this day. Thus key statistical information has been lost including information on current duration in hospital of each Covid-19 patient. In this paper we show that one can analyse this new sampling scheme - with important duration information missing - almost as well as if one indeed had full information from the beginning. This is only one important ==== while understanding the development of a pandemic. Other building blocks like the spread of the virus provide similar missing data issues when cross-country data is applied. We believe that the insights of this paper provide an important first step towards making mathematical statistical sense of the kind of data that is actually available during a pandemic.====Forecasting the development of a pandemic is complicated and the mathematical analysis of the missing data application in this paper is non-trivial providing a new theoretical problem of mathematical statistics. However, the data input needed to apply the forecasting methodology suggested by this paper is easy to understand and monitor. Also the forecast itself, the output, is easy to understand and apply to monitor the development of the pandemic. The ambition of this paper is to provide the first indication of a new methodology that can be communicated to epidemiologists or other practitioners in the field. When the input and the output is easy to understand, one could imagine or hope for that our method could enter basic textbooks in the field, see for example ====. Such basic communication is possible when both input and output are easy to understand even when the theoretical mathematical statistical steps are highly sophisticated.====. In our aggregated data, one cannot isolate and impute the missing data via the modelling provided in ==== for a recent example. Also, aggregated data analysis as in ==== or ==== does also not match to our type of data, because we do not have observed information of the exact timing between aggregated data points. It is exactly this timing that is our missing data. In other words: we have identified a new data missing problem in survival analysis that is applicable to the kind of data all of us have witnessed during the year 2020. Our practical and theoretical solutions provided below show that one can overcome this missing data problem with almost as good results as had we known the missing information on duration.",Missing link survival analysis with applications to available pandemic data,https://www.sciencedirect.com/science/article/pii/S0167947321002395,13 December 2021,2021,Research Article,133.0
"Rios Nicholas,Winker Peter,Lin Dennis K.J.","Department of Statistics, The Pennsylvania State University, State College, PA, 16802, United States of America,Department of Economics, University of Giessen, Licher Strasse 64, 35394, Giessen, Germany,Department of Statistics, Purdue University, West Lafayette, IN, 47907, United States of America","Received 24 May 2021, Revised 5 November 2021, Accepted 3 December 2021, Available online 9 December 2021, Version of Record 14 December 2021.",https://doi.org/10.1016/j.csda.2021.107411,Cited by (3),"In a mixture experiment, ==== components are mixed to produce a response. The total amount of the mixture is a constant. This classical experiment has been studied for a long time, but little attention has been given to the addition order of the components. In an Order-of-Addition (OofA) Mixture experiment, the response depends on both the mixture proportions of components and their order of addition. The overall goal of the OofA Mixture experiment is to identify the addition order and mixture proportions that produce an optimal response. Methodology for constructing full OofA Mixture designs is discussed, but the size of these full designs increases rapidly as ==== increases. A Threshold Accepting (TA) algorithm is used to find a subset of ","In a mixture experiment, there are ==== components that are mixed together in a fixed total amount to produce a response ====. It is typically assumed that the response only depends on the proportion of each ingredient that is included in the mixture. The objective of a mixture experiment is to find values of the mixture proportions that produce an optimal response. Typically, this means finding the values of the mixture proportions that maximize a response, minimize a response, or have the response match a pre-existing target value.====In an Order-of-Addition experiment, the response depends on the order in which ==== components are added to a system. The Order-of-Addition problem surfaces in many practical applications. In the pharmaceutical industry, ====, the efficiency of synthesis of carbonate products depended on the order of addition of alcohols. Another example is the field of combinatorial drug therapy, where ==== demonstrate that both the ratio and order of three drugs have an impact on the treatment of oral cancer. The OofA experiment has been well-explored by ==== and ====.====In an Order-of-Addition Mixture experiment (OofA Mixture), researchers are concerned with both the addition order of the components and their mixture proportions. Assuming that the ==== components are sequentially mixed, there are ====! possible orderings. Suppose a mixture design in ==== components has ==== mixture experimental runs, where ==== depends on ==== components, so some of the ==== runs may be redundant. See ====, where an algorithm is proposed for creating such a design in the case where there are no constraints on the mixture proportions.====The focus of this paper is on designing experiments for the OofA Mixture experiment in the case where funds are limited, and researchers can only afford a small number of runs. This will be accomplished by using the Threshold Accepting (TA) algorithm to select a small number of runs from a larger pool of possible runs according to the popular D-optimality criterion. There are many possible criteria for selecting an optimal design (e.g. A-,I-), and the TA algorithm proposed in this paper may be adapted to different criteria without changing the overall algorithm. The Threshold Accepting heuristic originally proposed by ====). Further contributions focused on the construction of low discrepancy and optimal designs on a simplex (e.g. ====, ====), optimal designs on flexible regions (====) and robust designs (====).====The following is the Supplementary material related to this article.",TA algorithms for D-optimal OofA Mixture designs,https://www.sciencedirect.com/science/article/pii/S0167947321002450,9 December 2021,2021,Research Article,134.0
"Kirkby J.L.,Nguyen Dang H.,Nguyen Duy,Nguyen Nhu N.","School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, GA 30318, United States,Department of Mathematics, University of Alabama, Tuscaloosa, AL 35487-0350, United States,Department of Mathematics, Marist College, Poughkeepsie, NY 12601, United States,Department of Mathematics, University of Connecticut, Storrs, CT 06269, United States","Received 10 June 2021, Revised 25 September 2021, Accepted 2 December 2021, Available online 8 December 2021, Version of Record 13 December 2021.",https://doi.org/10.1016/j.csda.2021.107408,Cited by (3),A novel method is presented for estimating the parameters of a ==== (time-discretization) and Exact MLE (when applicable) demonstrate favorable performance of the CTMC estimator. Simulated examples are provided in addition to real data experiments with FX rates and constant maturity interest rates.,") or the Cox–Ingersoll–Ross (CIR) diffusion, which was introduced by ====; ====), (generalized) method of moments (====; ====), (non)parametric density matching (====, ====; ====). ====, multivariate time-homogenous (====) and time-inhomogeneous diffusions (====), stochastic volatility (====) and affine multi-factor models (====).====Note that to apply the method of ====, ==== for univariate diffusions, one must be able to transform the given diffusion to a unit diffusion process where the volatility is the identity. The method is inapplicable if the diffusion is not reducible in this fashion, although the reducible condition was later relaxed in the work of ==== and recently of ====. This is a particularly active research area, and for further extensions along this direction, please see the recent works of ====; ====; ====; ====; ====, ====, ====; ==== and the references therein.====; ==== for a comprehensive account of existing methods), and 2) spatial discretization, where one can discretize the state space into a finite discrete grid of spatial points, while preserving the continuous time dimension of the diffusion process. We take the later approach and approximate the evolution of the diffusion process through a continuous-time Markov chain (CTMC). Research in this direction was initiated in ====; ====), and realized variance derivatives (====) and Bermudan options (====; ====. Simulation of two-dimensional diffusions was proposed in ====.====The rest of this paper is organized as follows: Section ==== introduces the problem of estimating parameters of one-dimensional diffusions. In Section ====, we consider approximating the one-dimensional diffusion by a finite state CTMC by explicitly constructing the transitional matrix which governs the dynamics of the CTMC. Section ====, we provide numerous numerical examples to demonstrate the effectiveness of the proposed method, including a real data example using 10-Year Constant Maturity interest rates, and another using foreign exchange data. We also compare the obtained results with various numerical approaches in the literature. Comparisons with existing approaches, including Exact MLE when applicable, demonstrate that the method is quite reliable. Section ==== concludes the paper.",Maximum likelihood estimation of diffusions by continuous time Markov chain,https://www.sciencedirect.com/science/article/pii/S0167947321002425,8 December 2021,2021,Research Article,135.0
"Zhao Yan-Yong,Lin Jin-Guan,Zhao Jian-Qiang,Miao Zhang-Xiao","School of Statistics and Data Science & Key Laboratory of Financial Engineering, Nanjing Audit University, Nanjing 211815, China,School of Mathematics and Statistics, Xuzhou University of Technology, Xuzhou 221018, China,School of Mathematics and Statistics, Nanjing University of Information Science and Technology, Nanjing 210044, China","Received 23 February 2021, Revised 28 October 2021, Accepted 30 October 2021, Available online 8 December 2021, Version of Record 21 December 2021.",https://doi.org/10.1016/j.csda.2021.107389,Cited by (1)," of the resulting estimators under some mild conditions. The practical problems of implementation are also addressed. Finally, three numerical experiments are conducted to verify the finite sample performance of the proposed methods, and an application to the CD4 cell data is provided for illustration."," highlighted this issue and they gave an excellent overview of various approaches to model this type of data sets.==== (====, ====, ====, and Fan and Wu (==== developed a data-driven approach to model mean and covariance simultaneously based on the modified Cholesky decomposition. Other literatures could be referred to ====, ====, ====, ====, ====, ==== and references therein.==== where ==== is the ====th measurement of the ====th subject, ==== is a ==== a ====-dimensional covariate vector at time ====, ==== is a vector of unknown functions of ====. ==== is a random error with ====, and can be modeled as follow==== where ==== denotes the difference between the ====th and (====)th observation times of the ====th subject, ==== is the lag order that needs to be specified prior or can be determined using the existing model selection method, ==== and ==== is an independent and identically distributed (i.i.d.) random error with mean 0 and variance ====.====The model ====), difference-based estimate (Fan and Wu, ====), penalized least squares method (Hu and Xia, ====), two-stage orthogonality-based estimate (====), etc. The error structure ==== ==== or ====, it reduces to a standard autoregressive model. The item ==== in ==== accommodates irregular and subject-specific observation times and describes possibly nonstationary correlation patterns.====In contrast to ====, Hu and Xia (====) and ====, ==== who modeled the covariance function to estimate the unknown parameters and functions, the main purpose of this paper is to model the error process via using the model ==== and study model ==== in the mean function and error structure ==== and ====, consistent estimates of covariance matrices, an easily implemented bandwidth selector, and determination of the lag order.====The remainder of this paper is organized as follows. In Section ====. Extensive simulations and an application to the CD4 cell data are presented in Section ====. We conclude with a discussion in Section ====. Detailed proofs are given in Appendix ====.====Before proving our main theorems, we introduce some notations. Let ==== be the true value of ====. Denote ==== and ====. The following three lemmas are needed.",Estimation of semi-varying coefficient models for longitudinal data with irregular error structure,https://www.sciencedirect.com/science/article/pii/S0167947321002231,8 December 2021,2021,Research Article,136.0
"Szarek Dawid,Maraj-Zygmąt Katarzyna,Sikora Grzegorz,Krapf Diego,Wyłomańska Agnieszka","Faculty of Pure and Applied Mathematics, Hugo Steinhaus Center, Wroclaw University of Science and Technology, Wyspianskiego 27, 50-370 Wroclaw, Poland,Department of Electrical and Computer Engineering, Colorado State University, Fort Collins, CO 80523, USA","Received 11 May 2021, Revised 11 November 2021, Accepted 12 November 2021, Available online 1 December 2021, Version of Record 8 December 2021.",https://doi.org/10.1016/j.csda.2021.107401,Cited by (3),. Theoretical results and simulation studies are supported by the analysis of experimental data describing the sub-diffusive motion of ==== in agarose hydrogels.,"; ====; ====; ====; ====; ====; ====; ====; ====. In general, Gaussian systems have found various interesting applications, see e.g. ====; ====; ====.==== is classified as anomalous diffusive if its second moment scales asymptotically as a power function ====, where ==== is called the anomalous diffusion exponent. Depending on the ==== parameter one can distinguish between sub-diffusive (====) and super-diffusive (====) regimes. In the case ====), as recently observed in various physical phenomena, see e.g. ====; ====; ====.====The classical example of Gaussian anomalous diffusion is fractional Brownian motion (FBM). This process was introduced in ====, ====, characterized by the Hurst index ==== where ====, FBM reduces to Brownian motion. In addition to FBM, the class of anomalous diffusion processes is very rich, (====; ====). It includes, for instance fractional Lévy stable motion, (====), continuous-time random walk, (====; ====) and subordinated processes (called also time-changed processes), (====; ====). See also these publications ====; ====; ====; ====; ====; ====; ====; ====; ====; ====; ====; ====.====The so-called statistics of time averages play a crucial role in the analysis of the anomalous diffusion processes. Three statistics of time averages have been proposed for Gaussian anomalous diffusion processes, namely, the sample autocovariance function (ACVF), (====), time-averaged mean squared displacement (TAMSD), (====) and detrended moving average (DMA), (====). For interesting properties and applications of these statistics we refer the readers to references (====; ====; ====; ====; ====; ====, where the general properties and possible applications were presented. In this paper, we study the probabilistic properties of this statistic using its quadratic form representation for Gaussian processes (====). This representation gives the possibility to recognize the theoretical foundations and thus we propose a strict statistical test for anomalous diffusion behavior identification. The testing procedure consists of two steps. At the first stage, the type of anomalous diffusion for real trajectories can be discriminated. At the second stage, one can test the theoretical Gaussian model for data description. While the proposed testing procedure is based on strong mathematical foundations, it can be considered as a guide for experimentalists who deal with anomalous diffusive data.====; ====), however, there are also approaches, similar to the EAM-based method, that can be applicable for the general case, see e.g. (====).====The rest of the paper is organized as follows: In Section ====, we describe the testing procedure that consists of the two steps, described above. In Section ====, we compare the effectiveness of the new approach with the methodology based on the sample ACVF. The study is performed for FBM. In Section ====, we describe the holistic testing procedure based on EAM that can be considered as the practical guide for experimentalists. In this procedure we take into account the EAM values for all possible arguments, thus one can infer the whole information about the process being tested. In this section, we also demonstrate the effectiveness of the holistic procedure for simulated trajectories of FBM. In Section ",Statistical test for anomalous diffusion based on empirical anomaly measure for Gaussian processes,https://www.sciencedirect.com/science/article/pii/S0167947321002358,1 December 2021,2021,Research Article,137.0
"Ameijeiras-Alonso Jose,Gijbels Irène,Verhasselt Anneleen","Department of Statistics, Mathematical Analysis and Optimization, Universidade de Santiago de Compostela, Santiago de Compostela (A Coruña), Spain,Department of Mathematics and Leuven Statistics Research Center (LStat), KU Leuven, Leuven, Belgium,Center for Statistics, Data Science Institute, Hasselt University, Hasselt, Belgium","Received 16 February 2021, Revised 17 November 2021, Accepted 18 November 2021, Available online 24 November 2021, Version of Record 25 November 2021.",https://doi.org/10.1016/j.csda.2021.107403,Cited by (2),"A new way of constructing flexible and unimodal circular models, focusing on the modal direction, is proposed. Starting from a base symmetric density and a weight function, a two–piece four parameters density is introduced. The proposed density provides an extension of the base density to allow for sharply peaked and flat–topped ",".====The complicated features that data tend to exhibit on the circle (====, Section 1.4), such as skewness or varying peakedness (i.e., the curvature of the density function) around the modal direction, lead to the exploration of new more flexible models in this context. ====, proposal), the transformation of variables distributions (====; ====), the mixture distributions (====) and the two–piece distributions (see, e.g., ====; ====; ====; ====, for the linear case). Up to our knowledge, none of the proposed models for circular data directly followed this last approach. The only remarkable exception constitute the circular densities, constructed from linear ones, using the wrapping approach or projections (see, e.g. ====). The aim of this paper is to provide a genuine new flexible two–piece circular density. Therefore, we propose a method to introduce asymmetry and varying peakedness around the modal direction. We refer to this issue as the concept of peakedness–free model. We hereby start from any symmetric circular density ====.====; ==== and ====.====Several examples where the modal direction is of special relevance and a flexible circular distribution is needed can be found in the literature. Some applications include: modeling the daily time of gun crimes (====), analyzing the yearly time of wildfire occurrences (====), modeling the wind orientation (====), analyzing the hourly temperature cycle changes (====), or studying the flight orientation of migrating raptors (====). This last example is revisited here to complement the findings of ====.====The paper is organized as follows. Section ==== introduces some useful terminology and includes a summary of circular models that are of special relevance for this paper. Section ====, we illustrate the application of the new proposal in an example in the ecology field. Section ==== summarizes relevant points of discussion. The Appendix summarizes some basic circular terminology and it contains proofs of the main theoretical results provided in Sections ==== and ====The first objective of this section is to define the main terminology, related with a circular random variable Θ. With that objective, let us denote the ====th order complex exponential with ====, ==== and ==== the imaginary unit. Then, the cosine and sine trigonometric moments are, respectively, defined as ==== and ====, where ==== denotes the real part and ==== the imaginary part of ====. The circular mean direction ==== is equal to the argument of ====, i.e., ====. The mean resultant length ==== is the modulus of ====, ====. The cosine and sine trigonometric moments about the mean direction are, respectively, ==== and ====. Using the previous notation, in general, in the circular literature (see, e.g. ====, Section 3.4), the skewness coefficient is defined as ====, while the kurtosis coefficient is ====.",On a family of two–piece circular distributions,https://www.sciencedirect.com/science/article/pii/S0167947321002371,24 November 2021,2021,Research Article,138.0
"Villa Cristiano,Walker Stephen G.","School of Mathematics, Statistics and Physics, Newcastle University, UK,DEMM, Università degli Studi di Milano, Italy,Department of Mathematics, University of Texas at Austin, United States of America","Received 31 March 2021, Revised 17 November 2021, Accepted 18 November 2021, Available online 22 November 2021, Version of Record 24 November 2021.",https://doi.org/10.1016/j.csda.2021.107404,Cited by (4),"A new look at the use of improper priors in ==== for model comparison is presented. As is well known, in such a case, the Bayes factor is only defined up to an arbitrary constant. Most current methods overcome the problem by using part of the sample to train the Bayes factor (Fractional Bayes Factor) or to transform the improper prior in to a proper distribution (Intrinsic Bayes Factors) and use the remainder of the sample for the model comparison. It is provided an alternative approach which relies on matching ==== between density functions so as to establish a value for the constant appearing in the Bayes factor. These are the Kullback–Leibler divergence and the ==== divergence; the latter being crucial as it does not depend on an unknown normalizing constant. Demonstrations of the performance of the proposed method are provided through numerous illustrations and comparisons, showing that the main advantage over existing ones is that it does not require any input from the experimenter; it is fully automated.",", one wishes to compare model ==== to model ====; i.e.==== where ==== and ==== are the parameters determining, respectively, the densities ==== and ====, while ==== and ==== are the corresponding prior distributions. The Bayes factor of model ==== against model ==== is given by==== where ==== is the marginal likelihood of the data under model ====. The Bayes factor can also be used to compute the so–called ====, that is==== where ==== and ==== are the model prior probabilities, while ==== and ====.====Besides computational challenges arising when, for example, the parameter spaces have large dimension, a key limitation of the Bayes factor in ==== comes from the use of improper prior distributions. In fact, when prior information is insufficient to elicit priors ==== and ====, a common solution is to make use of objective priors, such as the Jeffreys prior (====) or the reference prior (====). See ==== for a thorough review of objective Bayesian methods, including model comparison.====Quite often the objective prior is improper. This means that, if the improper prior is ====, then we have that ====. Therefore, the prior should be written as==== for some arbitrary constant ====. While improper priors do not present a concern for the Bayesian inferential procedure, as long as the resulting posterior is proper, their use for the derivation of a Bayes factor is not appropriate. To see this, consider the following two improper prior distributions for models ==== and ====, respectively: ==== and ====. The Bayes factor of model ==== in favour of model ==== will be==== It is obvious that the Bayes factor in ==== will depend on the ratio of the two arbitrary constants ==== and ====. As such, one could simply choose ==== to favour either model ==== or model ==== as one desires.====Different solutions have been proposed to deal with the above issue. The most important can be categorised in methods that are based on an imaginary minimal experiment and methods based on training samples.====In the former category, we have the work of ====, where the idea is to imagine a data set, say ====, so to obtain the Bayes factor ==== which, in turn, allows the constants ==== and ==== in ==== to be determined. The method is considered within nested linear and log–linear models; yet it appears to have some fundamental flaws, see ====.====The second group of methods to deal with improper priors includes Partial, Intrinsic, Posterior and Fractional Bayes factors. The general idea at the core is to partition the observations ==== in two subsets: a training sample, say ====, and the remainder used for model comparison, say ====. If ==== is an improper prior chosen to represent “ignorance” about the parameters of ====, then one can derive the posteriors using the training sample, that is==== and use ==== for the computation of the Bayes factor,==== The criticism levelled at partial Bayes factors is that the result would depend on how the observations are partitioned; i.e., on the choice of ====. To circumvent the above issue, Intrinsic Bayes factors (====) and Fractional Bayes factors (====) have been proposed.====The Intrinsic Bayes Factor (====) is a partial Bayes factor, which is based on a minimum training sample ====. This sample is minimal, in the sense that ====, for ====, and there are no smaller samples with the same property. This part of the sample is then used to transform the improper prior ==== into the proper ====, while the remaining of the data will be devoted to construct the IBF, using ==== as priors. The partial Bayes factor to compare models ==== and ====, is then given by==== where==== To avoid to have a dependence of the Bayes factor from the training sample ====, an average (either arithmetic or geometric) on the set of all training sample has been introduced. That is, the ==== IBF (AIBF) and the ==== IBF (GIBF) are, respectively,==== where ==== is the number of minimal training samples. Other combinations of the Bayes factor based on the training samples are possible; for example, the ==== IBF, where the median of the ==== is considered. A thorough review of the Intrinsic Bayes factor, such as properties and limitations, is beyond the scope of this paper.====For Fractional Bayes factors (FBF), the idea of training is not based on a training sample, but rather the improper prior is “trained” by using a fraction of the marginal likelihood. This is achieved by raising the likelihood based on the full sample to a power ====. Thus, the FBF is given by==== where==== The FBF can also be expressed as==== We then see that both the IBF and FBF are formed by two factors: the first one, ====, is in common, while the other terms are a way to remove the dependence of the arbitrary constants. These two terms are different and, while the one for the IBF is completely specified, the one for the FBF depends on the value of ==== and has to be assessed. Discussions and comparison of the above methods can be found in ==== and in ====. However, a key property of the IBF, when it is considered for nested models, is that it can generate proper prior distributions, the so–called intrinsic priors. These are obtained by imposing that the IBF is (at least asymptotically) equivalent to an actual Bayes factor (see, for example, ==== and ====). In other words, the use of intrinsic priors allows the use of an actual Bayes factor in an objective scenario which can be found by averaging over all training samples.====In this paper we discuss an alternative method to deal with improper priors in the derivation of Bayes factors. In particular, we show how the constants ==== and ==== in equation ==== can be calibrated by considering the Kullback–Leibler divergence (====) and the Fisher distance, see e.g. (====). Recall the Kullback–Leibler divergence is a measure of the difference between two distributions, say ==== and ====. The key advantage of the proposed objective Bayes factor is that it is fully automated. In fact, no input is required by the experimenter (or the practitioner) to compute it. This, among other things, is a limitation of both FBF and IBF. For the IBF, as discussed above, it is necessary to calibrate the parameter ==== to obtain an optimal behaviour for the Bayes factor. For the IBF, although the dependence on the training sample can be removed by averaging, there is still some choice to be made to understand which IBF is more suitable for the problem at hand, i.e. geometric IBF, arithmetic IBF, ====-trimmed IBF, etc. In Section ==== we illustrate in detail, through an example, the advantages of the proposed Bayes factor.====The paper is organised as follows. Section ==== introduces the proposed Bayes factor. In Section ==== we present a series of examples that illustrate the implementation of the proposed Bayes factor and compare it with the FBF. Finally, Section ==== is dedicated to some final remarks and discussion points.====The following is the Supplementary material related to this article.",An objective Bayes factor with improper priors,https://www.sciencedirect.com/science/article/pii/S0167947321002383,22 November 2021,2021,Research Article,139.0
"Ning Jing,Pak Daewoo,Zhu Hong,Qin Jing","Department of Biostatistics, The University of Texas MD Anderson Cancer Center, Houston, TX 77030, USA,Division of Data Science, Yonsei University, Wonju 26493, Republic of Korea,Department of Population and Data Sciences, University of Texas Southwestern Medical Center, Dallas, TX 75390, USA,National Institution of Allergy and Infectious Diseases, Bethesda, MD 20892, USA","Received 24 March 2021, Revised 21 August 2021, Accepted 15 November 2021, Available online 19 November 2021, Version of Record 24 November 2021.",https://doi.org/10.1016/j.csda.2021.107402,Cited by (1), assumption of truncation and failure times conditioning on ==== of the test statistic are established and an easy implementation for obtaining its distribution is developed. The performance of the proposed test has been evaluated through simulation studies and two real studies.,"Truncated data arise when a time-to-event variable of interest, termed as a failure time, is observable only if it falls in a subject-specific truncation region (====). Analyzing the truncated data must account for the sampling constraint and account for the biased nature of the data induced by the sampling constraint. The analysis of left-truncated data has received a fair amount of attention with diverse applications in the literature; however, testing for independence between the truncation and failure times is relatively underdeveloped and often overlooked in practice. Prior work has mainly focused on estimation of failure time distribution (====; ====; ====; ====; ====; ====; ====), which often requires a certain level of independence assumption between the truncation and failure times. Hence, it is essential to test the independence assumption, even though is not of primary research interest, as this ensures the correct inference on the failure time. With violations to the assumption, only those methods that allow for dependent truncation are applicable (====; ====; ====).====; ====). Further, women having higher risks of spontaneous abortions were more likely to seek medical counseling from TIS. It implied that the truncation time (e.g., time from the first day of the last menstrual period to the first time contacting TIS) and the failure time (e.g., time from the first day of the last menstrual period to the spontaneous abortion) were potentially dependent upon each other, and hence the data could be dependently left truncated. However, there was no clear evidence whether the truncation and failure times were still dependent given the drug-exposure history. The dependence level would determine appropriate statistical methods to analyze the data and answer the scientific question of interest.====In the absence of covariate information, several tests are available for detecting the dependence between the truncation and failure times (====; ====; ====; ====; ====).====), and causal inference (====). This assumption can usually help dramatically reduce the number of parameters required by the joint distribution and the associated computational burden. A number of methods have addressed the conditional independence test (====; ====), in which smoothing techniques, such as kernel smoothing, are commonly used. These methods do not consider additional challenges due to left truncation and right censoring, however, and they cannot be used for testing the conditional independence between truncation and failure times.====, and we describe applications to two datasets in Section ====. A discussion is provided in Section ====.====We assume the following regularity conditions:",Conditional independence test of failure and truncation times: Essential tool for method selection,https://www.sciencedirect.com/science/article/pii/S016794732100236X,19 November 2021,2021,Research Article,140.0
"Wei Kecheng,Qin Guoyou,Zhang Jiajia,Sui Xuemei","Department of Biostatistics, School of Public Health, Fudan University, Shanghai, China,Department of Epidemiology and Biostatistics, University of South Carolina, Columbia, SC, USA,Department of Exercise Science, University of South Carolina, Columbia, SC, USA","Received 8 August 2020, Revised 8 November 2021, Accepted 12 November 2021, Available online 17 November 2021, Version of Record 18 November 2021.",https://doi.org/10.1016/j.csda.2021.107399,Cited by (1),"Estimation of the average treatment effect (ATE) and the average treatment effect on the treated (ATT) are two important topics of causal inference. However, when using the ==== for causal inference, two main problems including unbalanced ",") is a main statistical approach for causal inference. Consider a binary group indicator ==== (i.e. it could be treatment status: ==== if treated, ==== if control), each subject has a pair of potential outcomes ==== and ====. For continuous outcomes, the mean difference ==== is often of interest. For binary outcomes, ==== is the risk difference, log of risk ratio ==== and log of odds ratio ==== are nonlinear causal estimands (====). These estimands measure the causal effect, at the population level, of moving an entire population from treated to control. A related measure of causal effect compares ==== and ====, which measures the causal effect of treatment on those subjects who received the treatment (====).====); (2) only 77.5% participants had METs measured at the end of the three-year follow-up, thus naively ignoring missing subjects (22.5% participants in the ACLS) might induce selection bias (====).====Typical approaches for adjusting confounding bias include G-computation (====) and propensity score methods (====; ====), stratification (====; ====), covariate adjustment (====; ====) and weighting (====; ====). Combining the propensity score model and the outcome regression model, an estimator is doubly robust if it remains consistent when either one of the two models is correctly specified, but not necessarily both (====), and regularized calibrated estimator (====).====). Various approaches have been developed to handle the MAR data, including likelihood-based estimation (====) and weighting estimators (====). Recently, the doubly robust estimators for missingness have been improved by sample-bounded methods (====; ====), efficiency-improved approaches (====; ====), and bias-reduced estimators (====, ====).====Although doubly robust estimators attracted the attentions in either casual inference or missing data, to the best of our knowledge, few studies have considered both features simultaneously. In this article, we mainly focus on estimation of the average treatment effect (ATE), ====, and the average treatment effect on the treated (ATT), ====, and developing new doubly robust methods allowing missing outcomes. The proposed estimators remain consistent when the models for treatment allocation and missingness are correctly specified, or the model for outcome regression is correctly specified, but not necessarily both.====The rest of the article is organized as follows. Section ==== introduces the notations, key assumptions and doubly robust estimators of ATE and ATT with missing outcomes. Section ==== conducts simulation studies to evaluate the finite-sample performance of the proposed estimators. Section ==== covers real data application. Some relevant discussions are given in Section ====.====The following is the Supplementary material related to this article.",Doubly robust estimation in causal inference with missing outcomes: With an application to the Aerobics Center Longitudinal Study,https://www.sciencedirect.com/science/article/pii/S0167947321002334,17 November 2021,2021,Research Article,141.0
"Agarwal Gaurav,Tu Wei,Sun Ying,Kong Linglong","Statistics Program, King Abdullah University of Science and Technology, Thuwal 23955-6900, Saudi Arabia,Department of Public Health Sciences and Canadian Cancer Trials Group, Queen's University, Kingston, ON, Canada,Department of Mathematical and Statistical Sciences, University of Alberta, Edmonton, AB T6G 2G1, Canada","Received 15 August 2020, Revised 5 September 2021, Accepted 12 November 2021, Available online 16 November 2021, Version of Record 24 November 2021.",https://doi.org/10.1016/j.csda.2021.107400,Cited by (4)," and geopotential height over time in the Northeastern United States; the estimated contours highlight the nonconvex features of the joint distribution, and the functional quantile curves capture the dynamic change across time."," over time. Computing quantiles is one of the essential tasks for data analysis; however, the extension of quantiles to multivariate functional data is quite challenging. In this paper, we propose a method to estimate the quantile contours of multivariate functional data.==== poses a considerable risk for human health and have known to cause respiratory and cardiovascular illness (====; ====; ====; ====). Various meteorological variables, such as temperature, wind, geopotential height, affect the ==== concentrations significantly (====; ====). However, most of these analyses were done in a univariate regression framework, and the effects were estimated at the mean level without reflecting any dynamic change in time. Consequently, a deeper understanding of the relationship between ==== and meteorological variables can help us in the formulation of pollution control strategies and health-care policies. In this article, we study the joint distribution of ==== and geopotential height at a given time may have nonconvex features and is of particular interest as high-pressure systems can trap pollutants and cause high ==== concentrations. Previous literature has shown that several air pollution crises have occurred when maximum values of geopotential height were recorded above the city (====; ====). The proposed method is useful in applications where the marginal distribution is of interest which is observed over time. For the ==== and geopotential height data application, the scientific goal is to study the time-evolving nonconvex features of the bivariate distribution.====; ====; ====; ====, ====; ====; ====). For example, ==== use projection-based quantiles to estimate multivariate quantile contours, which coincide with Tukey depth contours (====); see also ====; ====; ====. For univariate functional data, ==== use spatial depth to estimate quantile curves, see also ====, ====. ====). ==== defined projection-based quantiles for high dimensional multivariate data and univariate functional data. For multivariate functional data, ==== modeled bivariate functional data using directional quantiles, and the estimated quantile contours for the marginal distribution coincides with the Tukey depth contours, see also (====). However, most existing depth-based contours are convex by nature, which is not desirable for distributions with nonconvex density contours, which we refer to as nonconvex distributions. Most of these depth-based methods follow linear monotonicity axiom, which imposes star-convexity of its contours (====; ====). They also suffer from computational issues in higher dimensions, and even computing quantiles for three-dimensional data is very time-consuming and challenging (====).==== introduced a concept of multivariate quantiles based on deterministic maps that transform a reference distribution to the distribution of interest. Their approach gives up the linear monotonicity axiom and can pick up nonconvex features of the distribution. ====, where ==== is the multivariate quantile index. The estimated multivariate quantile function follows the property of monotonicity and uniqueness, and consistency has been established. The estimated quantile contour can account for non-Gaussian and even nonconvex distributions, which the depth-based methods fail to do. The estimated multivariate quantile function is a continuous function of time for a fixed quantile index, which is useful to capture the change in the distribution over time. Computationally, the proposed method is also efficient for higher dimensions, while existing methods only deal with bivariate functional data.====In the simulation studies, we apply the proposed methods to bivariate and trivariate functional data. We show that in the case of Gaussian marginals distribution, the predicted quantile contours coincide with the density contours, and for nonconvex marginal distribution, the contours can correctly pick up the shape of the distribution, while the depth-based methods failed to do so. The prediction was identified to be reasonably accurate in predicting the center of the distribution. For the application, the bivariate distribution of ==== and geopotential height is estimated using flexible quantile contours for the northeastern United States across six months. The quantile contours explain the nonconvexity in the marginal distribution, and the functional quantile curves capture the dynamic change in the distribution over time.====The rest of the paper is organized as follows. In Section ====, we define the multivariate functional quantile model, discuss the estimation of multivariate functional quantiles and construction of quantile contours. We discuss the theoretical properties of the multivariate quantile function in Section ====. In Section ====, we conduct simulation studies on bivariate and trivariate functional data. In Section ====, we apply our methods to air pollution data, and, finally, Section ==== concludes the paper with a discussion.",Flexible quantile contour estimation for multivariate functional data: Beyond convexity,https://www.sciencedirect.com/science/article/pii/S0167947321002346,16 November 2021,2021,Research Article,142.0
"Wang Yue,Zhou Yan,Li Rui,Lian Heng","Department of Mathematics, City University of Hong Kong, Hong Kong, China,Department of Mathematics, Shenzhen University, Shenzhen, China,School of Statistics and Information, Shanghai University of International Business and Economics, Shanghai, China,City University of Hong Kong Shenzhen Research Institute, Shenzhen, China","Received 6 November 2020, Revised 31 July 2021, Accepted 3 November 2021, Available online 9 November 2021, Version of Record 18 November 2021.",https://doi.org/10.1016/j.csda.2021.107388,Cited by (1),"We consider partially linear ====. We establish the overall learning rate in this setting, as well as the rate of the linear part separately. Our proof relies heavily on the empirical processes and the ","Quantile regression introduced by ==== studied the spatial quantile function-on-scalar regression model.====While most of the existing literature focuses on linear quantile regression or fully nonparametric quantile regression, we often encounter cases where partial information on the form of ====When only the linear part exists, ====; ====). For quantile regression, ====; ====; ====. Technically, the key innovation in the proof is to introduce the basic inequality ==== that compares the objective function value at two points. Our results can also be regarded as extensions of ==== which studied the partially linear least squares regression in high dimension (the rates we have are also the same as this least squares case).====The rest of the article is organized as follows. In Section ====, we present the model, and state and prove the main result on the overall convergence rate. In Section ====, using the orthogonalization trick combined with Rademacher complexity, under slightly stronger assumptions, we establish the optimal rate of the high-dimensional linear part. Section ==== presents some simulation results as well as an empirical application. We conclude with some discussions in Section ====.",Sparse high-dimensional semi-nonparametric quantile regression in a reproducing kernel Hilbert space,https://www.sciencedirect.com/science/article/pii/S016794732100222X,9 November 2021,2021,Research Article,143.0
"Ahfock Daniel,Pyne Saumyadipta,McLachlan Geoffrey J.","School of Mathematics and Physics, University of Queensland, Brisbane, Australia,Department of Statistics and Applied Probability, University of California Santa Barbara, Santa Barbara, CA, USA,Health Analytics Network, Pittsburgh, PA, USA","Received 6 January 2021, Revised 31 October 2021, Accepted 3 November 2021, Available online 9 November 2021, Version of Record 17 November 2021.",https://doi.org/10.1016/j.csda.2021.107387,Cited by (0)," ==== under ==== loss. For non-Gaussian models, imputation using the minimax optimal strategy can lead to different results compared to generic methods. Computationally feasible procedures for parameter estimation can be implemented using data augmentation schemes and the EM algorithm. Comparisons of the minimax optimal imputation scheme to standard algorithms on real data from flow cytometry show that minimax strategies can better preserve the joint distribution of the variables.","A common objective in the statistical file-matching problem is to impute the missing observations in each dataset to allow complete-data techniques to be used in downstream analyses (====; ==== and ==== observations complicates missing-data imputation as there is not enough information in the observed data to uniquely identify the full joint distribution ==== (====; ====.====; ====).====Many non-Gaussian models can be represented in terms of latent variables, and we show that this can cause problems for imputation methods that rely on conditional independence assumptions. Statistical file-matching can be carried out without the conditional independence assumption. However, this typically requires the design of specialized algorithms (====; ====; ====). This has been the case in cytometry analysis, where a number of practical algorithms for integrating cytometry datasets have been developed (====; ====; ====; ====; ====; ====; ====).====The paper is organised as follows. Background information and related work are discussed in Section ====. In Section ==== we derive the maximum entropy models for skew distributions and mixture models. In Sections ==== and ==== we show that for these distributions, the minimax optimal model can be estimated in a computationally efficient manner using the EM algorithm (====). In Section ==== we compare different imputation methods on real flow cytometry data and show that minimax model based imputation can improve over off-the-shelf algorithms in situations where the conditional independence assumption is unreasonable. Conclusions are reported in Section ====.====The following is the Supplementary material related to this article.",Statistical file-matching of non-Gaussian data: A game theoretic approach,https://www.sciencedirect.com/science/article/pii/S0167947321002218,9 November 2021,2021,Research Article,144.0
"Su Miaomiao,Wang Qihua","Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing 100190, China","Received 6 December 2020, Revised 10 October 2021, Accepted 11 October 2021, Available online 8 November 2021, Version of Record 15 November 2021.",https://doi.org/10.1016/j.csda.2021.107371,Cited by (0)," when the response is missing at random. Some existing methods define root-==== for the conditional distribution function, without the requirement to specify the selection probability function. Moreover, the proposed estimator can be asymptotically more efficient than the existing estimators. The proposed method is evaluated by a simulation study and is illustrated by a real data example.",", ====, ====, ====, ====, ====, ====, ====, ====, and ==== of the covariable vector diverges with the sample size ==== and even possibly is larger than ====. With the help of some important techniques in high dimensions, such as the Lasso (====), the adaptive Lasso (==== can be used to make inference on the response mean, it crucially relies on correct specifications of both the outcome regression model and the selection probability function. To alleviate the conditions on the model specification of unknown functions, ==== proposed an approximate residual balancing debiasing method and obtained a ====, ==== and ====; ====; ====; ====) with low dimensional or high dimensional covariate vector. However, one can not resort to conditional quantile regression to obtain an estimator for the marginal response quantile directly. To our knowledge, the only estimator for the marginal response quantile that is shown to be ====-asymptotically normal is proposed in ====. The ====-consistency of the estimator in ====. This method consists of the following three steps. First, we assume a single index model for the conditional distribution of response given covariates and establish a conditional distribution-based estimating equation. Second, we make an adjustment to the equation by adding the difference between the weighted empirical distribution and the weighted conditional distribution to the estimating function. Third, we solve the adjusted estimation equation to obtain the proposed estimator. The weight in the second step is obtained by solving a convex program which makes the variance of the proposed estimator attain minimum and constrains its bias such that it is ====-consistent.====The rest of this paper is organized as follows. In Section ====. Section ==== provides an equivalent easy-to-implement method for calculating the weights. Section ==== presents some simulation studies to examine the finite sample performance of the proposed method. The real data application is reported in Section ====. Outlines of the proofs of the main theorems are presented in the appendix, and the technical details are relegated to the supplementary material.====Appendix contains proofs of ==== and ====. Note that constant ==== may vary from lines and all of them are positive. ",A convex programming solution based debiased estimator for quantile with missing response and high-dimensional covariables,https://www.sciencedirect.com/science/article/pii/S016794732100205X,8 November 2021,2021,Research Article,145.0
"Luo Renwen,Pan Jianxin","College of Mathematics, Sichuan University, Chengdu 610065, China,Department of Mathematics, The University of Manchester, Manchester M13 9PL, UK","Received 5 January 2021, Revised 21 October 2021, Accepted 24 October 2021, Available online 8 November 2021, Version of Record 15 November 2021.",https://doi.org/10.1016/j.csda.2021.107386,Cited by (1)," hardly understandable. As an alternative, a data-driven method that models directly the mean, variances and correlation coefficients for clustered data is proposed. Comparing to the existing methods, the proposed approach not only has no need of natural order in responses but also works on original correlation coefficients and variances. The proposed models are flexible and interpretable, and the parameter estimators in joint generalized estimating equations (GEE) are shown to be consistent and asymptotically normally distributed. Consistent ==== in spirit of quasi-likelihood under independence model criterion (QIC) are considered. The use of the proposed approach is demonstrated by intensive simulation studies and real data analysis.",". GEE has an attractive advantage that the resulting mean parameter estimators are consistent even if the working correlation structure is misspecified (====). However, efficiency loss of the estimated mean parameters may arise when variance is misspecified (====) or within-cluster correlation structure is not correctly identified (====; ====; ====; ==== with geometric intuition for longitudinal data. The aforementioned approaches also provide good within-subject correlation interpretation in terms of time series (====, where they considered the selection consistency of a similar algorithm under normality assumption. In contrast, our approach relaxes the normality assumption and only requires the existence of the first four moments of responses.====This paper is organized as follows. In section ====, we focus on model, estimation procedure and computational algorithm. Theoretical properties of the proposed parameter estimators are studied in section ====. Model selection strategy with an efficient search algorithm is provided in section ====. Section ==== presents intensive numerical simulation studies, which confirms the advantage of the proposed approach. Real data analysis is conducted in section ====. A concluding summary of main findings and future interests is presented in section ====. Technical proofs of theoretical properties are provided in Appendix and the supplementary materials.====When ==== follows normal distribution ====, ==== follows normal distribution ====. Therefore by Wick's Theorem (====), we have==== Thus ====. We also have==== Therefore,",Conditional generalized estimating equations of mean-variance-correlation for clustered data,https://www.sciencedirect.com/science/article/pii/S0167947321002206,8 November 2021,2021,Research Article,146.0
"Weiß Christian H.,Ruiz Marín Manuel,Keller Karsten,Matilla-García Mariano","Helmut Schmidt University, Department of Mathematics and Statistics, Hamburg, Germany,Departamento de Métodos Cuantitativos e Informáticos, Universidad Politécnica de Cartagena, Spain,Institute of Mathematics, University of Lübeck, Germany,Departamento de Economía Aplicada y Estadística, UNED, Spain","Received 21 January 2021, Revised 20 October 2021, Accepted 20 October 2021, Available online 8 November 2021, Version of Record 15 November 2021.",https://doi.org/10.1016/j.csda.2021.107381,Cited by (8),.,"Independence is one of the most valuable notions in ==== and in statistical and econometric modeling due to the fact that most tests boil down to checking some sort of independence assumption in the time series or in the residuals of a given model. Our approach to test the null of independence is of a non-parametric nature. An important challenge of non-parametric tests is to be able to deal with small data sets. This is especially true in the case of ordinal-pattern-based tests.====The use of ordinal patterns to identify a dynamic structure in time series is not new. The basic idea is to transform a continuous time series into a sequence of symbols, each describing the local ordering of a number of ====), ==== approaches infinity. This fundamental result, which was generalized to a larger class of functions in ====, supported the rigorous use of the Shannon entropy of ordinal patterns to probe the complexity of dynamical systems. Although the use of ordinal patterns is responsible for obvious information losses (such as the numeric values of the time series), it offers a range of practical advantages that often offset this limitation. For example, ordinal representations are more robust to the presence of noise and can naturally capture complexities and dynamical changes in the time series. Following the work by ====, ordinal patterns and related complexity measures have gained great popularity in the analysis of dynamical systems and are now part of the established toolbox for time series analysis. For instance, ordinal patterns have been used to test for serial dependence (====; ====; ====), to determine critical lags in time series (====, ====; ====), in a wide range of health and life science applications (====; ====; ====; ====; ====), for example to test for structural breaks.====It is straightforward to check that if the time series under consideration is independent and identically distributed (i.i.d.), then the ordinal patterns follow a uniform distribution. Now, the crucial idea of this article is as follows. When checking the whole marginal distribution of ordinal patterns for deviations from uniformity, local deviations might be overlooked if the major part of the distribution is sufficiently close to uniformity. We conjecture, however, that violations of the i.i.d.-null hypothesis may have a particularly strong effect on the occurrence of strictly monotone decreasing or increasing patterns. So, these two types of extreme patterns might be more informative when checking for serial dependence than the remaining indeterminate ordinal patterns.====In this paper, we propose a new approach to the analysis of real-valued time series based on ordinal patterns by providing a new, non-parametric set of aggregated-pattern test ==== for serial dependence. Moreover, we prove that under mild conditions, ordinal-patterns-based statistics are nuisance free, that is, they are not affected by the intermediate step of parameter estimation when applied to the residuals of a model. In addition, a probabilistic analysis of ordinal patterns is carried out.====The remainder of this article is organized as follows. In Section ====, we introduce the basic notations and definitions together with a probabilistic analysis of ordinal patterns. In Section ==== illustrates the finite sample behavior of the new test statistics compared to a natural benchmark; namely, the symbolic correlation integral by ====, by carrying out a comprehensive simulation study that also evaluates the nuisance-parameter-free property of the tests. In Section ==== (====). An empirical financial example on how to use the tests is given in Section ====. Finally, we conclude in Section ==== and outline issues for future research.",Non-parametric analysis of serial dependence in time series using ordinal patterns,https://www.sciencedirect.com/science/article/pii/S0167947321002152,8 November 2021,2021,Research Article,147.0
"Deng Jianqiu,Yang Xiaojie,Wang Qihua","Academy of Mathematics and Systems Science, Chinese Academy of Sciences, China,School of Statistics and Mathematics, Yunnan University of Finance & Economics, China,Zhejiang Gongshang University, China","Received 31 October 2020, Revised 1 September 2021, Accepted 15 October 2021, Available online 4 November 2021, Version of Record 15 November 2021.",https://doi.org/10.1016/j.csda.2021.107374,Cited by (0),"Sufficient dimension reduction (SDR) for nonignorable nonresponse poses a challenge and the literature about this issue is very rare. In the nonignorable case, the ==== developed for ignorable missing data generally yield serious estimation bias and thus are invalid. A regression-calibration-based cumulative mean estimation (RC-CUME) procedure is proposed to recover the central subspace (CS) with the aid of a surrogate subspace. ==== of the RC-CUME are investigated. A modified BIC-type criterion is used to determine the structural dimension of the CS. Some extensions to other SDR methods are presented. Simulation studies are conducted to access the finite-sample performance of the proposed RC-CUME approach, and a ==== is analyzed for illustration."," on a ====. When ====. In this situation, sufficient dimension reduction (SDR; ====) provides an efficient dimension reduction approach without any additional parametric assumption, by projecting ==== onto a dimension-reduction subspace ==== which satisfies ====, where ==== stands for statistical independence and ==== denotes the projection operator onto ==== (CS) and denoted by ====; its dimension ==== is called the structural dimension.====Since the precursive work of sliced inverse regression (SIR; ====, typically including sliced average variance estimation (SAVE; ====), contour regression (====), directional regression (DR; ====), likelihood acquired directions (====), cumulative slicing estimation (CUME; ==== proposed a fusion-refinement (FR) procedure to recover ====, by utilizing the additional information carried by the missingness. To the best of our knowledge, there is no literature on SDR with nonignorable nonresponses. This is mainly because nonignorable nonresponses allow the missingness to depend on the missing responses themselves such that developing SDR methods for this case becomes more challenging.====Suppose that ==== is fully observed, and Y is subjected to nonignorable missingness. Let ==== be the missingness indicator for ====, with ==== if ==== is observed and ==== is called as the propensity score (PS). Let ==== be the conditional density or probability mass of ==== given ====, and ==== be the conditional density or probability mass of ==== given ====, with ====. In the nonignorable case, since ==== and ====, the complete-case (CC) analysis which simply deletes those subjects with missing values may result in serious estimation bias, especially when the missing rate is high. This motivates us to develop a new method for estimating ==== with nonignorable nonresponse.==== on ====, i.e. ====, whereas our method is aimed to estimate the central subspace for the regression of ==== on ====, i.e. ====. To be specific, we estimate ==== based on the CUME method which is proposed by ==== for full data. The CUME aims to estimate the kernel matrix ====, where ==== with ==== being an indicator function and ==== as an independent copy of ====, and ==== is a nonnegative weight function. In the presence of missing responses, one may use inverse probability weighted method (IPW; ====; ====) or regression calibration approach (RC; ====; ====; ====) to estimate ====. The IPW works by weighting each completely observed subject with the inverse propensity score, ====. Unfortunately, for the case of nonignorable nonresponses, estimating ==== is intractable because it depends on ====. Instead, the RC approach imputes the conditional mean ==== for ==== when ==== in order to obtain an estimate of the kernel matrix ====. However, the estimation of ==== is not trivial owing to double difficulties, that is, ==== suffers from nonignorable missingness and meanwhile ==== has a possibly high dimension. To alleviate “the curse of dimensionality” caused by the high dimension of ====, we hope to obtain an estimate of the central subspace conditional on ====, i.e. ====, such that ==== in ==== but without changing this conditional expectation. However, it is not straightforward to do that since ==== is missing when ====. To this end, we introduced a subspace called as ==== which contains ==== and can be estimated using the completely observed data. In this way, we can estimate ==== and further estimate ==== by borrowing information from the conditional distribution on ====. After obtaining the estimator of ====, ====. It is shown that the proposed estimator of ==== is ====-consistent and the estimator of the structural dimension is also consistent.====The rest of this article is organized as follows. In Section ====, we make basic setup and model assumptions. In Section ====, we present the methodology of estimating ==== is also estimated. In Section ====, we conduct simulation studies to assess the performance of the proposed RC-CUME approach. In Section ====, ==== are given in Appendix ====. Some additional simulation results and two useful Lemmas with detailed proofs are provided in the supplementary materials.",Surrogate space based dimension reduction for nonignorable nonresponse,https://www.sciencedirect.com/science/article/pii/S0167947321002085,4 November 2021,2021,Research Article,148.0
"Wiemann Paul F.V.,Klein Nadja,Kneib Thomas","Chair of Statistics, Göttingen University, Humboldtallee 3, 37073 Göttingen, Germany,Humboldt-Universität zu Berlin, Emmy Noether Research Group in Statistics and Data Science, Unter den Linden 6, 10099 Berlin, Germany","Received 15 October 2020, Revised 30 September 2021, Accepted 21 October 2021, Available online 29 October 2021, Version of Record 6 December 2021.",https://doi.org/10.1016/j.csda.2021.107382,Cited by (3)," and models all distributional parameters as functions of the ====. Covariate effects are not limited to being purely linear and other effect types, such as smooth functional effects, are available. As a consequence, the approach presented provides increased flexibility with respect to the ","Most standard statistical analyses rely on the assumption that data are in some sense representative for the population of interest. If, however, the observed data are the result of an informative selection process, i.e., a selection process that is not independent from the data generating process of interest, naive analyses bear the risk of erroneous results and biased estimates. Classical examples include self selection in medical studies, when participants are allowed to decide whether they want to participate in the study, or self selection into the labour market in economics. In both cases, it is likely that the selection process is not independent from the outcome of main interest. For example, when analyzing wages, it is more likely to decide against participation in the labour market if the offered wage is low. Pioneered by the seminal work of ====, sample selection models attempt to correct for the bias induced by the non-independent selection mechanism by integrating it in the statistical model.==== where ==== is a latent variable driving the selection decision, ==== is the regression predictor for the selection equation and ==== is an i.i.d. Gaussian error. From the latent selection model, the observed selection indicator is derived as ====. The outcome on the second stage of the hierarchy, i.e., the outcome of main interest, is observed if and only if the selection indicator from the first stage equals one, such that==== where ==== results from a second regression specification with predictor ==== and i.i.d. error term ====, if the observation is indeed selected, and is not available (NA) otherwise. In case of non-random selection, i.e., if ====, estimating the regression effects for ==== based on the selected observations alone will lead to biased results. The sample selection bias can now be corrected either by following a two step procedure, where the inverse Mills ratio (====; ====, for details).====In the application that motivated our research, we ==== from a psychological experiment in a judge-advisor paradigm. More precisely, participants are asked to complete a series of numerical estimation tasks (e.g., estimating the calorie content of different kinds of food). After providing their initial estimate and stating their confidence in their initial guess, the participants are given an advice, i.e., they are presented with external information on the estimation task from a hypothetical advisor. They are then asked to provide another estimate for the quantity of interest, the main interest being by how much the participants adjust the initial guess in their second assessment. We cast this question to the framework of sample selection, where the participants first decide whether they want to take advice and to revise their initial guess (selection equation). Consequently, a shift of the initial guess towards the advice is only observable if this decision was positive in the first place.====; ====). We refer to this framework as ====; ====) or conditional transformation models (====).==== or ====. Combined with the latent probit specification, this not only allows us to introduce flexible forms of dependence with non-normal outcomes, but also to make the strength of the dependence depends on covariates. Other authors who investigated copula regressions are for instance ====, ====, and ====.====).==== and ====. Their model has similar modelling capabilities, e.g., it allows for non-normal outcomes via a copula approach and for structured additive predictors linked to all distributional parameters. However, the model is implemented within the framework of penalized likelihood and, therefore, follows a different philosophy to prevent overfitting (penalization vs. informative prior distributions). Furthermore, our approach enables access to the Bayesian toolbox of priors for regression models, e.g., hierarchical structures, shrinkage priors, or spike-and-slab priors for effect selection. Our method is implemented in the developer version of the free and open-source software BayesX (====). BayesX users will find it straightforward to alter their models to account for sample selection if the required covariate data is available.====).==== and ==== summarize the efforts of relaxing the normality assumption using non-parametric and semi-parametric approaches. More recently, ==== considers a non-parametric trimming procedure and ==== avoid the exclusion restriction in a non-parametric approach. A semi-parametric series approach can be found in ====. ==== and ==== to deal with heavy-tailed outcome distributions. A flexible approach avoiding the need to specify a parametric outcome distribution upfront is suggested in ====To deal with a more flexible specification of non-linear covariate effects, ==== consider a partial-linear single index model. After ==== introduced sample selection modelling with the tobit model to the Bayesian framework, efforts have been made to extend the inference to flexible non-linear covariate effects (====). ==== focus on normally distributed outcomes in a sample selection setting while enabling the use of flexible covariate effects based on semi-parametric additive predictors. ==== and ==== implement similar modelling capabilities of effect types within the frequentist penalized likelihood estimation framework and, in addition, consider binary outcomes.==== proposes to separate the dependence structure of selection equation and outcome of interest, and gives examples of how to do so by using the Gaussian and student-==== copulae. ==== and ==== show how copulae can be used in sample selection models when knowledge about the marginal distributions is available to the applied researcher. ==== combined the specification of flexible covariate effects with the use of copulae, thus disentangling marginals and dependence structure while staying fully parametric in the distribution. With this paper, we introduce similar modelling capabilities to the Bayesian framework by extending sample selection models to the model class of Bayesian distributional regression (BDR).====The remainder of this paper is organized as follows: Section ====. Section ==== presents three simulation studies to validate the methodology and Section ==== covers the application of our model to the judge-advisor data. The paper concludes in Section ==== with comments on directions of further research.====The following is the Supplementary material related to this article.",Correcting for sample selection bias in Bayesian distributional regression models,https://www.sciencedirect.com/science/article/pii/S0167947321002164,29 October 2021,2021,Research Article,149.0
Weng Jiaying,"175 Forest Street, Bentley University, Waltham, MA 02452, United States of America","Received 23 April 2021, Revised 20 October 2021, Accepted 21 October 2021, Available online 29 October 2021, Version of Record 13 January 2022.",https://doi.org/10.1016/j.csda.2021.107380,Cited by (0),None,"With the rapid development in high-throughput data generation and storage, high-dimensional data are available for researchers to develop sophisticated methods in building parsimonious models. However, high-dimensional data raise many challenges and difficulties that do not exist in the lower dimension scenarios. Sufficient dimension reduction (SDR; ====, ====) is a paradigm for reducing the dimension of predictors through seeking a dimension reduction matrix ====, the response ==== is statistically independent of predictors ====, denoted as==== This is equivalent to saying that ==== contains all information in ==== about ====. Meanwhile, the dimensionality of ====, ====, reduces to ==== by replacing ==== with ====. Matrix ==== is not identifiable, but the space spanned by the columns of ==== denoted by ==== is identifiable, called dimension reduction subspace. The intersection of all dimension reduction subspaces if the intersection itself exists and is a dimension reduction subspace then is called the central subspace ====. Under mild conditions, (====; ====), the central subspace exists and is unique.====Sliced inverse regression (SIR; ====) and sliced average variance estimate (====), contour regression (====), directional regression (====), projective resampling (PR; ====) and sliced regression (====), among many others. Furthermore, ====). To eliminate the effect of a slicing scheme for which most inverse regression approaches require, ====; ====; ====). To circumvent this difficulty, ==== and ==== proposed Fourier transform approaches in the multivariate response. Several recent works shed light on the development of Fourier transform approaches. ==== presented the Fourier transform (FT) in various scenarios, along with the partial SDR (====; ====) and sequential SDR (====). Recently, ====For recent development on high-dimensional data analysis, regularized regression has been one of the most popular research fields (i.e., ====, the sample size), such as lasso (====), SCAD (====), and adaptive lasso (====, ====, ====, ====, ====, ====, ====, ====, ====, ====, and many references therein. ====) has been widely implemented to solve optimization problems (====; ====; ====; ====; ====). The idea of ADMM is to convert the optimization problem into a sequence of simpler problems that involve only the smooth loss or nonsmooth penalty (====). ==== employed the ADMM algorithm to sparse SIR over various loss functions. However, the ADMM algorithm is still received less attention in SDR, especially in minimum discrepancy approaches.====The rest of the article is organized as follows. Section ==== reviews the quadratic discrepancy function via a sequence of response functions and establishes estimation consistency and oracle properties. Section ==== proposes an iterated ADMM algorithm and discusses how to choose tuning parameters. Section ==== and ==== conduct the simulation studies and real data analysis indicating numerical efficacy of the proposed methods. Section ==== concludes with a discussion of our work and other potential research directions. In the end, the appendix presents theoretical proofs.====This proof follows Theorem 6 in ====. Let ==== be a minimizer of ==== with ====. Let ==== and ==== be a minimization of ====. Then ====,==== which implies==== Rearrange ====Without loss of generality, assume ====. Because ==== is sub-Gaussian distribution (C1) and ==== is a bounded function (C2), ==== follows sub-Gaussian, meaning that there exist constants ==== such that for every ====,==== Then for ==== and large enough ==== using the Bernstein inequality (====),==== Take ==== and ====, then ==== for large enough ====, we have==== As a result, with probability greater than ====, for large enough ====, and by union bound,====The first term of LHS ==== is==== where ====. Under the condition (C1), there exist constants ==== for every ====,==== where ==== and ==== are ==== and ====, respectively. Let ==== with ====, then ====. Using the union bound, with probability greater than ====,==== Because ====, we have ==== based on Condition (C6). Hence,==== which implies that the second term of LHS in ==== is====Then substitute the bounds ==== and ==== into ====,==== where ==== and ====. For every ====, ====, and every ====, ====. Let ====, then==== That is to say,==== By choosing ==== and ====, with conditions (C4) and (C5), we have==== then==== which gives==== Let ====, we get==== Also, define ==== as the index subset in ==== that corresponds to the ==== largest ===='s for ====. Define ====,==== which implies that==== We can also have ==== (====), combining the above two results==== Hence, by Wedin's Theorem (====),====To show ====, define ====. If choose ====, similar arguments can be applied to get estimate ==== for ====. Note that, for every ====, ====, and every ====, ====. For every ==== with large enough ====,==== For every ====,==== By the choice of ==== and ====, for ==== For every ====, there is a constant ==== such that==== Therefore, ==== is satisfied.====To prove the second statement holds, we have ==== for any ====. Therefore, ==== as ====. Given any ====, if ====, by KKT conditions,==== Similarly, there a constant ==== such that==== We have ==== and ====, which leads to contradiction. Therefore, ==== as ====. We complete the proof of ====.  □",Fourier transform sparse inverse regression estimators for sufficient variable selection,https://www.sciencedirect.com/science/article/pii/S0167947321002140,29 October 2021,2021,Research Article,150.0
"Zhang Hong,Wu Zheyang","Biostatistics and Research Decision Sciences, Merck Research Laboratories, Rahway, NJ 07065, USA,Department of Mathematical Sciences, Worcester Polytechnic Institute, Worcester, MA 01609, USA","Received 30 March 2021, Revised 15 September 2021, Accepted 21 October 2021, Available online 29 October 2021, Version of Record 29 October 2021.",https://doi.org/10.1016/j.csda.2021.107379,Cited by (4), on the CRAN.,"; ====). They are broadly used in enormous applications for signal detection, meta-analysis, and data integration. For example, in genetic association studies, the ====-values of single-nucleotide polymorphisms (SNPs) have been combined by various GOF type tests for detecting novel disease genes (====; ====; ====). In recent years, association studies based on summary statistics have become an increasingly important strategy for dissecting the genetics of complex traits (====). The GOF type tests are a flexible and powerful strategy for this purpose. The popularity and the potential of these tests are due to both their convenience to apply and their excellent performance. For example, a collection of GOF tests are proven asymptotically optimal for detecting weak and rare signals, e.g., the Higher Criticism (HC) (====), the Berk-Jones (BJ) test (====), and a spectrum of ====-divergence tests (====).====-value calculation or an interpretation perspective. For example, the GATES statistic (====) extended Simes statistic (====) so that the ====-value can be obtained easily under correlations. The GHC and the GBJ extended the original HC and BJ statistics, respectively, motivated by the statistic formulas' interpretations under correlations (====; ====). These extended test versions provided reasonable practical solutions to address correlated data analysis. At the same time, compared with the original versions, the extended statistic formulas are much more complicated and the computations are much slower. The statistical power is either very similar or has pros and cons depending on correlation patterns.====This paper presents an alternative solution for correlated data analysis based on two considerations. First, it considers the problem of ====-value calculation separately from the necessity of extending original statistics. In principle, the original statistics remain legitimate to analyze correlated data. It is a computational problem of controlling type I errors for properly utilizing them under correlations. The computational problem is highly desired to resolve because many traditional GOF statistics have excellent performances for analyzing correlated data (====). Second, instead of addressing statistics individually, the paper considers all statistics of a similar type as a whole family and provides a general framework to deal with correlations. Different GOF statistics have relative advantages under different signal and correlation patterns; therefore, an omnibus procedure that automatically adapts to statistics of complementary advantages would robustly retain high statistical power.====This paper has three main contributions. First, it establishes a framework for applying a general family of goodness-of-fit tests (the GGOF (====)) to analyze correlated data. The GGOF family is well interpretable under correlations, following the essential idea of determining whether the input ====-values have a good “fit” to their “null behavior.” It is well defined based on a general statistic formula, or equivalently, an expression of rejection boundary. It broadly includes many existing tests, such as the traditional GOF tests, the minimal ====-value test (minP), Simes test, the GATES, the GHC, the GBJ, the one-sided ====-divergence tests, etc. The testing framework includes the ====-value calculation for GGOF tests. The exact calculation is deduced under positive equal-correlation. Based on that, the effective correlation coefficient (ECC) method is proposed for addressing arbitrary correlations. Besides its generality, the ECC is much faster than existing statistic-specific calculations (e.g., the effective number of independent test (ENIT) method for the GATES, and the ====-package for the HC/BJ/GHC/GBJ), and is more accurate in many situations.====-value calculation can be applied for efficient computation. Moreover, by adapting to different GGOF statistics that possess relative advantages under different signal and correlation patterns, the GGOF-O is robustly powerful in broad scenarios. For example, the GGOF-O that adapt to the minP, the HC, and the BJ is shown a faster and often more powerful test when comparing with the extended tests (such as GATES / GHC / GBJ) and some other omnibus tests (such as SKAT-O (====)).====This paper's third contribution lies in a systematic study on the influence of correlation patterns to both ====-value calculation and the statistical power of typical GGOF tests. The accuracy and the limitations of related ====The remainder of the paper is organized as follows. In Section ==== we define the GGOF family and reveal their performances by the rejection boundaries. A universal ====-value calculation approach is developed and assessed in Section ====. Section ==== presents the statistical power study. An applicational study of detecting novel genes associated with bone mineral density is given in Section ====. Section ==== concludes the work and discusses the limitations and future plans.",The general goodness-of-fit tests for correlated data,https://www.sciencedirect.com/science/article/pii/S0167947321002139,29 October 2021,2021,Research Article,151.0
"Golovkine Steven,Klutchnikoff Nicolas,Patilea Valentin","Groupe Renault & Ensai, CREST - UMR 9194, Rennes, France,Univ Rennes, CNRS, IRMAR - UMR 6625, F-35000 Rennes, France,Ensai, CREST - UMR 9194, Rennes, France","Received 13 April 2021, Revised 18 October 2021, Accepted 18 October 2021, Available online 28 October 2021, Version of Record 15 November 2021.",https://doi.org/10.1016/j.csda.2021.107376,Cited by (5),"A model-based clustering algorithm is proposed for a general class of functional data for which the components could be curves or images. The random functional data realizations could be measured with errors at discrete, and possibly random, points in the definition domain. The idea is to build a set of binary trees by recursive splitting of the observations. The number of groups are determined in a data-driven way. The new algorithm provides easily interpretable results and fast predictions for online data sets. Results on simulated datasets reveal good performance in various complex settings. The methodology is applied to the analysis of vehicle trajectories on a German roundabout.",", ====; ====; ====; ====; ====Clustering procedures for functional data have been widely studied in the last two decades, see, for instance, ====, ====; ====; ====; ==== and references therein. See also ==== show that the cluster centers found with ==== for the analysis of a bike sharing system from cities around the world.====Algorithms built to handle multivariate functional data have gained much attention in the last few years. Some of these methods are based on ====-means algorithm with a specific distance function adapted to multivariate functional data. See ==== ====; ====. ==== consider Self-Organizing Maps built on the coefficients of the curves into orthonormalized Gaussian basis expansion. ==== have recently extended the previous model by modeling all principal components whose estimated variances are non-null. The underlying model for these methods usually consider only amplitude variations. Unlike others, ==== present a specific model for functional data to consider phase variations. Finally, ==== propose a combination of dimension reduction and non-parametric approaches by deriving the envelope and the spectrum from the curves and have applied it to nuclear safety experiments.====) to the functional setting, and we therefore call it ====. At each node of the tree, a model selection test is performed, after expanding the multivariate functional data into a well chosen basis. Similarly to ==== and ====; ====, is its ability to estimate the number of groups within the data while this number have to be pre-specified in the other methods. Moreover, the tree structure allows us to consider only a small number of principal components at each node of the tree and not to estimate a global number of components for the clustering. Considering tree methods designed for functional data, ==== algorithm also allows for classes defined by certain types of phase variations.====The remainder of the paper is organized as follows. In Section ====, we define a model for a mixture of curves for multivariate functional data with the coordinates possibly having different definition domains. Given a dataset, that is a set of, possibly noisy, intermittent measures of an independent sample of realizations of the stochastic process, in Section ====, we explain how to compute the different quantities that are required in the clustering procedure. In Section ====, we develop the construction of our clustering algorithm, named ====. In Section ====, we study the behavior of ==== and compare its performance with competing methods both on simulated and real datasets. Our algorithm performs well for estimating the number of groups in the data as well as for grouping similar objects together. Once the tree has been grown, it can be used to predict the labels given new observations. The prediction accuracy is compared with those derived from supervised methods, and exhibits good performance. A real data application on the analysis of vehicle trajectories illustrates the effectiveness of our approach. Section ==== presents an extension of the method to image data based on the eigendecomposition of the image observations using the FCP-TPA algorithm (====). The proofs are left to the Supplementary Material.====The following is the Supplementary material related to this article.",Clustering multivariate functional data using unsupervised binary trees,https://www.sciencedirect.com/science/article/pii/S0167947321002103,28 October 2021,2021,Research Article,152.0
"Zhang Jin-Ting,Zhu Tianming","Department of Statistics and Data Science, National University of Singapore, Singapore 117546, Singapore","Received 9 February 2021, Revised 14 July 2021, Accepted 23 October 2021, Available online 27 October 2021, Version of Record 15 November 2021.",https://doi.org/10.1016/j.csda.2021.107385,Cited by (0),"In recent decades, with rapid development of data collecting technologies, high-dimensional data become increasingly prevalent, and much work has been done for hypotheses on mean vectors for high-dimensional data. However, only a few methods have been proposed and studied for the general ==== testing (GLHT) problem for high-dimensional data which includes many well-studied problems as special cases. A centralized ====-norm based test statistic is proposed and studied for the high-dimensional GLHT problem. It is shown that under some mild conditions, the proposed test statistic and a chi-square-mixture have the same normal or non-normal limiting distributions. It is then justified that the null distribution of the test statistic can be approximated by using that of the chi-square-type mixture. The distribution of the chi-square-type mixture can be well approximated by a three-cumulant matched chi-square ==== with its approximation parameters consistently estimated from the data. Since the chi-square-type mixture is obtained from the test statistic under the null hypothesis and when the data are normally distributed, the resulting test is termed as a normal reference test. The asymptotic power of the proposed test under a local alternative is established. The impact of the data non-normality on the proposed test is also studied. Three simulation studies and a real data example demonstrate that in terms of size control, the proposed test performs well regardless of whether the data are nearly uncorrelated, moderately correlated, or highly correlated and it outperforms two existing competitors substantially.","-test (====; ==== may be in hundreds or thousands and it is often much larger than the sample size ====. Under such a “small ====, large ====” setting, Hotelling's ==== who showed that when the data dimension ==== tends to infinity proportionally with the sample size ====, the classical Hotelling's ====-test for two-sample problems for high-dimensional data is powerless and is less powerful than the Dempster's non-exact test (====, ====) and the ====-norm based test they proposed. This is mainly due to the fact that the sample covariance matrix used in the Hotelling's ====-test is nearly degenerated when the data dimension is large and is comparable with the sample size.====Since then, many new methods have been proposed for the high-dimensional two-sample problem. ===='s test and Dempster's non-exact test assume that the underlying covariance matrices of the two high-dimensional samples are the same. This condition may be not realistic for many high-dimensional data sets. To overcome this difficulty, ==== proposed a U-statistic based test where the covariance matrices of the two high-dimensional samples may be different. There are also many other methods developed to deal with the singular sample covariance matrix other than just removing it from Hotelling's ====-statistic. ==== and ====, ====, ====, ====, and ==== among others. Robust tests for two-sample problems with long tailed or outlier-polluted high-dimensional data are also available, including ====, ====, and ==== among others. Some of the above methods for two-sample problems have also been extended to the ====-sample problem, including ====, ====, ====, ==== and ==== among others.====. The data are from a consulting project on a keratoconus disease study with Ms. Nancy Tripoli and Dr. Kenneth L. Cohen of the Department of Ophthalmology, University of North Carolina at Chapel Hill. According to varying degrees of the keratoconus disease, when the corneas are misshaped, 150 corneal surfaces are classified into four groups, and each corneal surface has 6912 measurements. Of interest is to check whether the keratoconus disease has a strong impact on the shapes of the corneal surfaces, i.e., whether the four corneal surface groups have the same mean corneal surface. Since it is difficult to check whether the four groups have the same covariance matrix, it is not reasonable to assume that the underlying four covariance matrices are equal. This leads to a four-sample problem for high-dimensional data under heteroscedasticity or a four-sample heteroscedastic one-way MANOVA (multivariate analysis of variance) problem for high-dimensional data.====Formally a multi-sample heteroscedastic one-way MANOVA problem for high-dimensional data can be described as follows. Suppose we have the following ==== independent samples:==== where the data dimension ==== is very large and may be much larger than the total sample size ====. One may want to test whether the ==== mean vectors are equal:==== without assuming the equality of ====. The above problem is usually referred to as the ==== where ==== is a matrix collecting all the ==== mean vectors and ====. In fact, the heteroscedastic GLHT problem ==== reduces to the heteroscedastic one-way MANOVA test ==== when we set ====, where ==== and ==== and the ====-dimensional vector of ones, respectively.====The heteroscedastic GLHT problem ==== is very general. It includes not only the heteroscedastic one-way MANOVA test ==== but also various post hoc and contrast tests as special cases since any post hoc and contrast tests can be written in the form of ====. For example, when the heteroscedastic one-way MANOVA test is rejected, it is of interest to further test if ==== or if a contrast is zero, e.g., ====. In fact, these two testing problems can be written in the form of ==== with ==== and ====, respectively, where and throughout ==== denotes a unit vector of length ==== with ====-th entry being 1 and others 0.====Note that the coefficient matrix ==== for the null hypothesis in ==== is not unique. For example, ==== is also a contrast matrix for the null hypothesis in ====. Therefore, it is important to construct a test which is invariant under the following transformation of the coefficient matrix ====:==== where ==== is any ==== non-singular matrix. It is clear that the GLHT problem ==== is the same as the one obtained via replacing ==== with ====. Set ==== and ====. We can then further write the GLHT problem ==== equivalently into the following form:==== where ==== and ==== and their proposed tests are invariant under the transformation ====, e.g., ==== and ==== among others.====To overcome this difficulty, ==== proposed a normal reference ====-norm based test with the following test statistic==== which equals the first term in the expression ==== showed that ==== and a chi-square-type mixture have the same normal or non-normal limiting distributions. It is then natural to approximate the null distribution of ==== using the distribution of the chi-square-type mixture. Since the coefficients of the chi-square-type mixture are positive and unknown, ==== proposed to approximate the distribution of the chi-square-type mixture using the well-known Welch–Satterthwaite (W–S) ====-approximation (====; ====) with the approximation parameters consistently estimated from the data. Some simulations conducted in ==== demonstrated that in terms of size control, the ZZG-test outperforms the ZGZ-test generally, regardless of whether the high-dimensional data are nearly uncorrelated, moderately correlated, or highly correlated. However, the ZZG-test is still somewhat liberal since many of its empirical sizes are around 6% (which are quite larger than the nominal size 5%) in Tables 1 and 5 in ==== and ====, ====, ==== presented in Section ====.====In this paper, we propose a new test based on the centralized ====-norm based test statistic ==== ==== which outperforms the ZGZ-test (====) and the ZZG-test (====) in terms of size control. The main contributions of this work are as follows. First of all, under some regularity conditions and the null hypothesis, we show that the test statistic ==== and a new chi-square-type mixture have the same normal or non-normal limiting distributions. Therefore, it is not always applicable to approximate the null distribution of ==== using the normal distribution as in the ZGZ-test, and it is justifiable to approximate the null distribution of ==== using the distribution of the new chi-square-type mixture. This new chi-square-type mixture is obtained from ==== itself when the null hypothesis holds and when the data ==== are normally distributed, then we term this approach as a new normal reference approach. Second, in the ZZG-test, the distribution of the chi-square-type mixture associated with ==== is approximated with the well-known W–S ====-approximation of ==== with the approximation parameters consistently estimated from the data. The resulting ratio-consistent estimators of the approximation parameters in the 3-c matched ==== demonstrate that in terms of size control, the new normal reference test performs very well regardless of whether the data are nearly uncorrelated, moderately correlated or highly correlated and it outperforms the ZGZ-test (====) and the ZZG-test (====) substantially.====The rest of the paper is organized as follows. The main results are presented in Section ====. Simulations are presented in Section ==== and a real data example is studied in Section ====. Some concluding remarks are given in Section ====. Technical proofs of the main results are outlined in Appendix ====.====For easy reference, following some suggestions from two reviewers, we restate Lemma 1's of ==== and ==== respectively as ====, ==== below. Their proofs are also adopted here for simplicity because both ==== and ==== are unpublished.==== ",A new normal reference test for linear hypothesis testing in high-dimensional heteroscedastic one-way MANOVA,https://www.sciencedirect.com/science/article/pii/S016794732100219X,27 October 2021,2021,Research Article,153.0
"Coube-Sisqueille Sébastien,Liquet Benoît","Laboratoire de Mathématiques et de leurs Applications, Université de Pau et des Pays de l'Adour, UMR CNRS 5142, E2S-UPPA, Pau, France,Department of Mathematics and Statistics, Macquarie University, Sydney, Australia","Received 24 October 2020, Revised 6 October 2021, Accepted 7 October 2021, Available online 26 October 2021, Version of Record 13 January 2022.",https://doi.org/10.1016/j.csda.2021.107368,Cited by (1)," caused by high model dimension. Frugal alternatives such as response or collapsed algorithms are one answer. An alternative approach is to keep full data augmentation, but to try and make it more efficient. Two strategies are presented.====The first is to pay particular attention to the seemingly trivial fixed effects of the model. Empirical exploration shows that re-centering the latent field on the intercept critically improves chain behavior. Theoretical elements support those observations. Besides the intercept, other fixed effects may have trouble mixing. This problem is addressed by interweaving, a simple method that requires no tuning, while remaining affordable thanks to the ==== of NNGPs.====The second accelerates sampling of the random field using Chromatic samplers. This method boils long ==== down to group-parallelized or group-vectorized sampling. The attractive possibility for parallelizing NNGP density can therefore be carried over to field sampling.====A ====, an extensive vignette is provided. The presented implementation is run on two synthetic toy examples, along with the state of the art package ","Many social or natural phenomena happen at the scale of a territory and must be observed at various sites and possibly times. The rise of modern GPS and Geographic Information Systems made large and high-quality point-referenced data sets increasingly available. Assume that, in a collection of sites ==== of the space or space-time domain ====, we have measurements ==== with some kind of space or space-time coherence. This coherence can be accounted for by introducing a spatially-indexed process ==== of standard deviation ====, giving the following classical model formulation====, we denote the vector ==== as ==== for continuous data, see ====. The GP prior distribution of ==== is ====. The mean parameter of ==== is usually fixed to ==== to avoid identification problems with the linear regression intercept ====, such as Matérn's covariance and its exponential and squared-exponential special cases. It can then be written as ====, and its entries are ====. We denote ==== the GP density, and we abbreviate it as ====. The covariance parameters can have modeler-specified hyperpriors developed in ====; ====.====The weakness of GPs is that computing the GP prior density of ==== involves the determinant and inverse of ====, incurring a computational cost that is cubic in the size of ====; ====; ====; ==== and software presented in ====; ==== locations of ==== which we will denote ====. The ordering may have an impact on the quality of the approximation, and is discussed in ====; ====. The joint latent density of ==== is then written under the recursive conditional form==== Since ==== is a Multi-Variate Normal (MVN) distribution function, the conditional density ==== is a Normal as well. A NNGP is obtained by replacing the vector ==== that conditions ==== by a much smaller parent subset denoted ==== for each conditional density. The NNGP approximation to the GP prior joint density of ==== is defined as==== as ===='s nearest neighbors among ====, explaining the denomination “Nearest Neighbors Gaussian Process” given in ====. However, ====; ==== argue that mixing close and far-away observations can improve the approximation. This approximation is cheap and easily parallelizable. The latent density ==== can be split into small jobs and dispatched to a cluster of calculators (====). Its cost is linear in the number of observations, under the condition that the size of each parent set is bounded. More advanced strategies exist, such as grouping, proposed by Guinness in ====.====Although NNGPs work around the bottleneck of GP likelihood computation, they do not solve the problem of slow MCMC convergence. In ====, ==== and ====, ==== is fixed to 0. The latent field ==== is updated sequentially or by blocks. This sampler suffers from slow mixing, in particular when ==== increases. Other strategies have been proposed by ==== that avoid sampling the field in order to reduce the dimension of the model. Yet another method (====; ====) is to use convenient conjugate distributions for models where the range of ==== and ==== are fixed, and select the fixed parameters by cross-validation. Our approach is nevertheless to improve implementations of NNGP models where the latent field is explicitly sampled. Our first reason is that there may be situations where some of the methods presented in ==== perform poorly while full data augmentation works well. For example, the ==== of ==== and the dimension of ====. The ==== of ==== retrieves the covariance parameters ==== but not the latent field ====. The sparse Cholesky factor in a NNGP makes it possible to use the Ancillary-Sufficient Interweaving Strategy (AS-IS) presented in ====. The third reason is that full latent field sampling is all terrain, and can address many data models or be plugged into complex, non-stationary models like ====, while collapsed MCMC or conjugate models are much pickier.====Here is an outline of the article. Section ==== focuses on the seemingly trivial fixed effects of the hierarchical model. In ====, we extend centering to other fixed effects, and use interweaving from ==== to propose a robust, tuning-less application. Section ==== targets the simulation of the random field. In ====, we propose to use the chromatic samplers developed by ==== in order to carry the attractive parallelizability of NNGP density over to field sampling. In ====. We present our implementation (available at ====) in ====. We test our implementation along with the state of the art package ==== presented in ==== on synthetic toy examples in ====. In ====.====We obtain the full conditionals of ==== and ==== using the conditional expectation and variance formulas using precision matrices of ====, the joint precision of both ==== or ==== being ====. The expectation of ==== is 0, the expectation of ==== is ====, and the expectation of ==== is always ====.====The full conditional distributions of ==== and ==== are:==== Note ==== the vector of length ==== and filled with ones. From the second full conditional, we have a formula for the mean of ====, which is obtained with ====. It has 3 terms: one is fixed, the second is a geometric carry-over of ====, and the third is stochastic:==== Injecting the full conditional of ==== into ===='s, we identify an expression with 3 terms like before:====The full conditional of ==== and ==== are:==== The mean of ==== behaves like the mean of ==== except for the term that depends on ====:==== Injecting the full conditional of ==== into the full conditional of ====, we have====Let's compare first the expressions of ==== and ====. Denote the diagonalization ====, ==== being a square matrix of eigenvectors and ==== being a diagonal matrix of eigenvalues. The eigenvalues are positive since ====. Using the fact that adding ==== adds 1 to every eigenvalue without affecting the eigenvectors,==== Let ==== be the coordinates of ==== in the orthonormal basis defined by ====.==== Using that ==== is positive-definite on the left and that ==== on the right, we have====As for the centered model, we re-write:==== Once this is done, we can express the fraction of ==== which is conserved in ==== in the centered model as==== Like before, thanks to the fact that the eigenvalues of ==== are positive,",Improving performances of MCMC for Nearest Neighbor Gaussian Process models with full data augmentation,https://www.sciencedirect.com/science/article/pii/S0167947321002024,26 October 2021,2021,Research Article,154.0
"Smida Zaineb,Cucala Lionel,Gannoun Ali,Durif Ghislain","Institut Montpelliérain Alexander Grothendieck, Université de Montpellier, France","Received 20 July 2020, Revised 11 October 2021, Accepted 18 October 2021, Available online 26 October 2021, Version of Record 27 October 2021.",https://doi.org/10.1016/j.csda.2021.107378,Cited by (5),"A nonparametric scan method for functional data indexed in space is introduced. The associated scan statistic is derived from the Wilcoxon-Mann-Whitney test statistic defined for infinite dimensional data. It is completely nonparametric as it does not assume any distribution concerning the functional marks. Whatever the clustering scenario, this scan test seems to be efficient to detect and locate the cluster. This method is applied to a data set for extracting features in Spanish province population growth. A significant spatial cluster of low demographic evolution rates is found, exhibiting a specific phenomenon in the North-West of Spain."," for a thorough review. One of the most popular cluster detection technique is the scan statistic which was firstly introduced by ====. It was defined as the maximum number of events observed within a window with constant size, known as the scanning window, as it moves continuously over the studied region. Knowing the distributions of these scan statistics (====) helps to decide whether exceptional or not observing a cluster of events. The field of spatial scan statistics was highly enhanced by the article written by ====), normal (====), multivariate Gaussian (====), etc.====) and, even if a few studies have been conducted on modelling (====) or clustering (====) such data, to our knowledge, there is no spatial cluster detection method designed for this kind of data yet.====In the present work, we develop a scan statistic for functional data indexed in space: thanks to this statistic, we are able to detect spatial clusters in which the observations of a functional random variable are different than elsewhere, and also to compute the significance of these differences. Since no likelihood is associated with functional random variables (====), maximising a likelihood ratio test is not possible here. Thus, we follow the idea by ==== that any test for equality of two distributions can give birth to a scan statistic.====The rest of this paper is organized as follows. In section ====, we build a nonparametric spatial scan statistic for functional data based on the Wilcoxon-Mann-Whitney statistic proposed by ====, first, the spatial scan statistic is compared to other methods on simulated datasets. Then, we apply it to a real dataset illustrating the demographic evolution over time in Spanish provinces and we exhibit a specific behaviour in the North-West of Spain in the last twenty-two years. We conclude with a discussion and a brief scope for future work.",A Wilcoxon-Mann-Whitney spatial scan statistic for functional data,https://www.sciencedirect.com/science/article/pii/S0167947321002127,26 October 2021,2021,Research Article,155.0
"Zhu Ke,Liu Hanzhong","Center for Statistical Science, Department of Industrial Engineering, Tsinghua University, Beijing, 100084, China","Received 29 December 2020, Revised 28 September 2021, Accepted 20 October 2021, Available online 25 October 2021, Version of Record 13 January 2022.",https://doi.org/10.1016/j.csda.2021.107383,Cited by (1), between multiple variables over time. Estimation and inference of the ,"; ====; ====), which is equivalent to testing whether each element of the transition matrices is equal to zero or not, to detect the most predictive industry-specific economic sentiment indicators for macroeconomic variables. ==== proposed testing procedures in multi-block VAR models to test whether a block “Granger-causes” another block of variables and applied them to analyze the temporal dynamics of the S&P 100 component stocks and key macroeconomic variables.====) to estimate the parameters, and established its theoretical properties (====; ====; ====; ====; ====; ====; ====; ====; ====; ====; ====; ====).==== and ==== performance (====). Inferences are important for high-dimensional data analysis because they can provide practitioners, such as policymakers and business owners, with more valuable and solid information for assessing the significance of the correlations between variables or factors. For this purpose, one direction of the research proposed the de-biased Lasso method, which focuses on obtaining an asymptotically normal estimator by correcting the bias of the Lasso estimator (====; ====; ====; ====; ====). We will refer the method in ==== and ==== as LDPE (low dimensional projection estimator) and the method in ====) to approximate the asymptotic distribution of the Lasso, such as the bootstrap threshold Lasso (====), bootstrap adaptive Lasso (====), bootstrap Lasso + OLS (====), and bootstrap de-biased Lasso (====). Other inference methods include post-selection inference (====; ====), sample splitting inference (====), and so on. We refer to ====Following the de-biased approach, several researchers proposed methods to constructing confidence intervals and performing hypothesis testing in high-dimensional sparse VAR models (====; ====; ====; ====; ====). To improve the finite-sample performance of the de-biased Lasso method, ==== and ====Second, we develop feasible and parallelizable algorithms to implement our methods. Specifically, we perform statistical inference separately for each of the ==== equations in the VAR models. As the ==== is large.====Third, we conduct extensive simulation studies to compare our methods with the bootstrap Lasso, bootstrap Lasso+OLS, and another de-biased Lasso method proposed by ====The rest of the paper is organized as follows. In Section ====, we introduce the high-dimensional sparse VAR models and the de-biased Lasso, residual bootstrap de-biased Lasso and multiplier wild bootstrap de-biased Lasso methods. In Section ====, we study the theoretical properties of the proposed methods. In Section ====, we conduct simulation studies to evaluate the finite-sample performance of the proposed methods. In Section ====, we use our methods to analyze the S&P 500 constituent stocks data set. We summarize the results and discuss possible extensions in the last section. All the proofs and additional simulation results are given in the supplementary material.====The following is the Supplementary material related to this article.",Confidence intervals for parameters in high-dimensional sparse vector autoregression,https://www.sciencedirect.com/science/article/pii/S0167947321002176,25 October 2021,2021,Research Article,156.0
"Wu Suofei,Hannig Jan,Lee Thomas C.M.","Department of Statistics, University of California at Davis, One Shields Ave, Davis, 95616 CA, United States of America,Department of Statistics & Operations Research, University of North Carolina at Chapel Hill, 318 Hanes Hall, CB #3260, Chapel Hill, 27599 NC, United States of America","Received 2 June 2021, Revised 14 October 2021, Accepted 17 October 2021, Available online 21 October 2021, Version of Record 27 October 2021.",https://doi.org/10.1016/j.csda.2021.107377,Cited by (1),A new method is developed for quantifying the uncertainties of the estimates and predictions produced by honest ,"). It is commonly used to make predictions for future observations. Denote the observed sample as ====, ====, where ==== are scalar responses and ==== are vector predictors. The general regression model is==== where the iid noise ===='s follows ====. An ensemble learning method approximates the model ==== by a weighted sum of weak learners ===='s with weights ===='s:====). Recently, ====) in regression, which takes all ===='s in ==== equally as ====.====; ====, where the authors proposed a method that produces standard error estimates ==== for random forests predictions. The method is based on jackknife and infinite jackknife (====) and can be used for constructing Gaussian confidence intervals.====Another related work is ==== provided some theoretical results on using BART for inference.====In this paper, we apply generalized fiducial inference (====The rest of this paper is organized as follows. First, a brief introduction of generalized fiducial inference is provided in Section ====. Then the main methodology is presented in Section ====, and the theoretical properties of the method are studied in Section ====. Section ==== illustrates the practical performances of the proposed method. Lastly, concluding remarks are offered in Section ==== while technical details are delayed to the appendix.====This appendix provides the proof for ====.====WLOG, assume ==== and fix ====. We first prove that====Rewrite==== where==== and==== ====.====Now calculate==== Let ====, consider the second term in equation ==== and denote==== then==== and ==== since ====. Furthermore,==== Therefore,====Consider the third term in equation ====:====Notice that ====. Thus,==== Therefore, ====, and ==== as ====. Thus, we have ==== as ====.====In addition,==== which means==== Thus,====Moreover, ====. Therefore, ====.==== ==== and ====.====Recall ==== is fixed. First notice that ====, where ==== is a chi-square random variable depending on ==== with degrees of freedom ====.==== It implies that====Therefore,==== uniformly over ====, ====. Thus, we show that====Meanwhile, the calculation of ==== is similar to Case 1, ====, so we have ====.====Combining Case 1 and Case 2, we have:====Furthermore,====Equivalently,",Uncertainty quantification for honest regression trees,https://www.sciencedirect.com/science/article/pii/S0167947321002115,21 October 2021,2021,Research Article,157.0
"Franke Jürgen,Hefter Mario,Herzwurm André,Ritter Klaus,Schwaar Stefanie","Fachbereich Mathematik, TU Kaiserslautern, Postfach 3049, 67653 Kaiserslautern, Germany,R+V Versicherung AG, Raiffeisenplatz 2, 65189 Wiesbaden, Germany,Fraunhofer ITWM, Fraunhofer-Platz 1, 67663 Kaiserslautern, Germany","Received 6 February 2021, Revised 16 October 2021, Accepted 16 October 2021, Available online 21 October 2021, Version of Record 25 October 2021.",https://doi.org/10.1016/j.csda.2021.107375,Cited by (1)," for change-point detection. The new algorithm employs an adaptive (sequential) time ==== for the trajectories of the Brownian bridge. A simulation study shows that the new algorithm by far outperforms the standard approach, which employs a uniform time discretization.",".====In some cases like the first example above, cf. ====In this paper, we show that this problem can be overcome by using fast adaptive approximation methods for the strong (or pathwise) approximation of nonlinear functionals of Gaussian processes. Adaptive algorithms employ sequential strategies to construct the underlying discretization, which may therefore be adjusted to key features of the individual trajectories.====For illustrating our approach, we focus on weighted CUSUM tests for change-points, which leads to the distribution of the supremum of a weighted reflecting Brownian bridge, i.e., the supremum norm of a weighted Brownian bridge. We stress that the basic idea can be transferred to other situations where, e.g., quantiles of nonlinear functionals of Gaussian processes have to be calculated by Monte Carlo simulation.====Change-point tests are of interest in many areas of applications, e.g., in production monitoring, see ====, on-line-monitoring of intensive-care patients, see ====, or global warming studies, see ====, to name just a few. The first publications about testing for a change in data go back to the 1950s, see, e.g., ==== for ====, so that the weighted cumulated sums are variance stable. The distribution of those statistics is determined asymptotically in a variant of the Darling-Erdös Theorem, see ====, which immediately yields asymptotic quantiles.====To cope with performance problems regarding size and power, the standard weights of CUSUM statistics have been modified which results asymptotically in the distribution of the supremum of a weighted reflecting Brownian bridge. Those statistics have no size problems and better power against changes of the mean closer to the boundaries of the observation interval. Simulation studies using different weight functions and analyzing the power of the corresponding CUSUM-type tests for different positions of the change (early, middle and late) show the importance of the weight function, see ====, ====, and ====. An overview to such general CUSUM-type tests is given in ====.====In the present paper we consider weight functions of the form==== for ==== with parameters ==== and ====, and the corresponding convergence result for ==== is formulated in ==== and ====, there is no known method for analytically calculating quantiles or other characteristics of the limit distributions for these or more general weight functions. Hence, we have to use Monte Carlo simulation where a crucial part consists in generating paths of a Brownian bridge. This is in particular computationally very expensive if we are interested in calculating, e.g., extreme quantiles beyond the common levels 0.05 or 0.01 with a high precision up to ====. Such extreme levels of confidence are common in many applications in industry or medicine where a high degree of reliability is required. Also, in view of the Bonferroni inequality, low ====-values of tests are of interest in multiple testing situations including many hypotheses, see, e.g., ====.====In this paper, we propose an adaptive algorithm which reduces the computation time for calculating asymptotic quantiles of CUSUM test statistics with weight function ==== for ====, where the reduction turns out to be dramatic in the more challenging situations. Consider, for instance, the task to compute the 0.95-quantile with accuracy ==== for the parameters ==== and ====. On a common up-to-date processor the standard algorithm with an equidistant grid requires more than two hours of computation time, while the adaptive algorithm achieves the same goal within 12 seconds. The reason for this is, roughly speaking, that the adaptive algorithm allows to sample almost exactly from the correct limit distribution at a reasonable computational cost.====This paper is organized as follows. In Section ==== we give a brief sketch of the change-point application that is used as a demonstrator for our approach. In Sections ==== and ==== we consider the strong approximation of the supremum of the unweighted Brownian motion and of weighted reflecting Brownian bridges, respectively. For the former problem, ==== have constructed an adaptive algorithm that strikingly outperforms all non-adaptive algorithms with respect to an ====-error criterion, see ====, ==== for the case ====. See also ====, ====, ==== and ==== for related convergence results. No such result is known for weighted reflecting Brownian bridges, but still we construct a modification of the adaptive algorithm for these processes in Section ====. Numerical experiments reveal again the vast superiority of the adaptive algorithm over, at least, the standard algorithm that is based on an equidistant grid, see Section ====. In Section ==== we study quantile computation for the supremum of a weighted reflecting Brownian bridge. We present a new algorithm, with the adaptive algorithm from Section ==== as the key ingredient, that yields a quantile up to a user-specified error tolerance, see Section ====. Numerical experiments, which show the superiority of the new algorithm over the standard approach, are presented in Section ====.",Adaptive quantile computation for Brownian bridge in change-point analysis,https://www.sciencedirect.com/science/article/pii/S0167947321002097,21 October 2021,2021,Research Article,158.0
Ollier Edouard,"INSERM, U1059, Dysfonction Vasculaire et Hémostase, Saint Etienne, France","Received 1 March 2021, Revised 11 October 2021, Accepted 15 October 2021, Available online 21 October 2021, Version of Record 22 October 2021.",https://doi.org/10.1016/j.csda.2021.107373,Cited by (1),None,"Nonlinear Mixed effects modeling (====), oncology (====; ====) and forestry (====; ====). The development of a NLMEM consists then in the selection of statistically influent correlations and/or covariates for each latent variable. Even if the great majority of problems are not high dimensional ones, the number of all the possible models might be very important. As an example, for a NLMEM with ==== latent variables and ==== potential covariates to study, the number of all possible the model is equal to ==== proposed the use of the ==== norm to select covariates in pharmacogenomics studies but without calibrating the penalty parameters. ==== used the generalized fused lasso penalty to select between group differences of fixed effects and variance of random effect. Although the proposed algorithms showed good numerical properties, no theoretical work studied the statistical convergence of such extensions. Moreover, these ==== algorithm solves numerically a non-smooth optimization problem at each iteration. This inner/outer iteration scheme has a non-negligible computational cost. Recently, ==== and ====; ====) of the covariance matrix. This decomposition allows the separate estimation of parameters related to variances and correlation of random effects.====Like with other penalized likelihood approach, the optimal value of penalty parameters must be estimated using a criterion like cross-validation error (====) and can be easily parallelized. Particle swarm optimization has already been used in various applications such as chemometrics (====) or biomedical signal analysis (====).==== introduces NLMEM, the penalized maximum likelihood problem used to jointly select correlation and covariates parameters is introduced in section ====. In section ====, the stochastic proximal gradient algorithm specifically developed to solve such penalized likelihood problems is described. Calibration of penalty parameters is discussed in section ====. In section ====, the proposed algorithm is evaluated on simulated data. Finally, a real data application focusing on the population pharmacokinetics of an antifibrinolytic drug, and an antibiotic is provided in section ====.",Fast selection of nonlinear mixed effect models using penalized likelihood,https://www.sciencedirect.com/science/article/pii/S0167947321002073,21 October 2021,2021,Research Article,159.0
"Liu Jie,Ge Huilin","International Institute of Finance, School of Management, University of Science and Technology of China, China","Received 13 November 2020, Revised 15 October 2021, Accepted 16 October 2021, Available online 20 October 2021, Version of Record 15 November 2021.",https://doi.org/10.1016/j.csda.2021.107372,Cited by (2),"NN (==== Nearest Neighbor) network and apply walktrap to detect communities. Thus, more detailed and comprehensive community structures can be detected than when using the walktrap method. These results have practical significance for researching collaboration models and guiding future collaboration.","), of mathematicians (====), and a coauthorship network to help assess scientific impact (====). Unfortunately, coauthorship networks of statisticians are rarely studied.====Recently, ====; ====; ====; ====; ====; ====). In our models, we account for the exogenous covariate effects using single-node main effects. Homophily effects are used to measure the similarity between node pairs, assuming that node pairs with similar features are more likely to form an edge (====). To avoid degenerate or nonconvergent parameter estimates, ==== presented the modularity function ====; ====; ====; ====; ====, ====; ====; ====; ====; ====). ==== compared the performance of the many popular methods included in the igraph package for community detection, and the results showed that walktrap got the highest accuracy.====NN-walktrap to combine the network structures and the homophily features of the node pairs and detect more detailed and comprehensive communities, which improves upon existing walktrap algorithms. Our results can help to better discover the potential mutual collaboration habits and collaboration modes of the authors, as well as help to strengthen cross-university and even transnational collaboration in the future.",Collaboration mechanisms and community detection of statisticians based on ERGMs and ,https://www.sciencedirect.com/science/article/pii/S0167947321002061,20 October 2021,2021,Research Article,160.0
"Brunet-Saumard Camille,Genetay Edouard,Saumard Adrien","twice.ai, Rennes, France,Université de Rennes, Ensai, CREST-UMR 9194, Rennes F-35000, France,LumenAI, Tours, France","Received 10 December 2020, Revised 3 September 2021, Accepted 8 October 2021, Available online 15 October 2021, Version of Record 19 October 2021.",https://doi.org/10.1016/j.csda.2021.107370,Cited by (7),"The median-of-means is an estimator of the mean of a ==== that has emerged as an efficient and flexible tool to design robust learning algorithms with optimal theoretical guarantees. However, its use for the ","Massive and complex datasets are often corrupted by outliers. Classical ==== procedures such as K-means or more general EM algorithms for instance, are however, sensitive to the presence of outliers, which can induce time consuming data pre-processing.====; ====; ====, ====; ====, ====, ====; ====). Other approaches to robustness for K-means also exist in the literature, such as for instance, K-median or trimmed K-means (====; ====).====. We prove in Section ==== that if enough blocks are generated from the bootstrap sampling, then for a fixed block size, bMOM has a higher breakdown point than MOM. In other words, bMOM is more robust to contamination than the classical MOM. Note that one strength of bMOM, that will be very useful in the context of clustering, is that sampling is done ====).====We propose a robust-to-outliers version of K-means, that we call K-bMOM, and that performs Lloyd-type iterations through the use of bMOM estimates for the K-means risk, as further explained in Section ====. In that section, a robust variant to the traditional K-means++ initialization strategy by applying the bMOM strategy is also presented.====Theoretical results are summarized in Section ====. We prove in particular in Section ====, that the K-bMOM algorithm is robust in a sense of a probabilistic version of a breakdown point if the initial data is in a well-clusterizable situation. This is very much in line with the results on the trimmed K-means for example (====). Further theoretical results, concerning an idealized version of our algorithm, can be found in a Supplementary Material (====).====In Section ====, the scope of application of K-bMOM is illustrated and practical considerations and guidelines are provided for choosing the number and size of the blocks. In Section ====Our framework is close to the recent work (====) that investigates the use of median-of-means statistics to produce a robust K-means type clustering. However, the latter work is theoretical only and the authors study probabilistic performance bounds for the minimizer of the median-of-means of the K-means distortion loss under a finite second moment assumption. In particular the authors do not discuss the use of median-of-means through Lloyd-type iterations nor a practical way to compute the estimator. Neither do they discuss the possibility of generating blocks with replacements in the dataset.====The following is the Supplementary material related to this article.",K-bMOM: A robust Lloyd-type clustering algorithm based on bootstrap median-of-means,https://www.sciencedirect.com/science/article/pii/S0167947321002048,15 October 2021,2021,Research Article,161.0
"Liu Jicai,Si Yuefeng,Niu Yong,Zhang Riquan","School of Statistics and Mathematics, Shanghai Lixin University of Accounting and Finance, Shanghai, China,Department of Statistics, University of Hong Kong, Hong Kong,Department of Mathematics and Physics, Hefei University, Hefei, China,KLATASDS-MOE, East China Normal University, Shanghai, China","Received 9 November 2020, Revised 11 May 2021, Accepted 9 October 2021, Available online 14 October 2021, Version of Record 19 October 2021.",https://doi.org/10.1016/j.csda.2021.107369,Cited by (0)," and outliers, and does not require moment conditions on the predictors. We obtain theoretical properties of the PQC and its empirical counterpart. We then use the measure to select the grouped predictors that contribute to the conditional quantile of the response for high-dimensional data with group structures. We also establish sure independent screening properties for the group screening method. We illustrate the finite sample performance of the proposed method through simulations and an application to a dataset.","; ====; ====.====), group bridge (====), group exponential Lasso (====) among others. All these methods have be shown to be very useful for moderate number of predictors. However, when the number of groups is much larger than sample size, these methods may suffer from statistical inaccuracy, algorithm instability and heavy computational burden. To overcome the limitations, some recent studies have extended ===='s two-step strategy, which mainly deal with the ungrouped variables, to the setting of grouped predictors. See, ====, ==== and ====. Specifically, they first screen out grouped predictors that do not contribute to the response in the first step, and then use the above regularization procedures in the second step. It has been shown that the two-step methods have low computational cost and good theoretical properties.==== and ====, given by==== where ==== is the ====th quantile of ==== and ==== is the conditional quantile of ==== given ==== at the ====th quantile level ====. Notice that ==== suggests that the conditional quantile of ==== given ==== is independent of ====. In this article, we shall introduce a new metric using projection-averaging technique, called projection quantile correlation (PQC), to measure the above relationship. The projection-averaging method is an efficient and commonly used trick in the literature of multivariate or high-dimensional inference. For example, ====, ==== and ==== developed Cramér-von Mises statistic via the projection method to investigate the goodness-of-fit problem; ====, ==== and ==== generalized the Cramér-von Mises statistic to obtain robust tests for the multivariate two-sample problem.====The conditional quantile independence in ==== has been explored in recent literature. For example, in the setting that ==== is univariate, ==== proposed a marginal utility to quantify the relationship for feature screening. Note that ===='s marginal utility is substantially rank-based and thus computationally expensive to implement if ==== is high-dimensional. ==== and the scale variable ====. ==== holds under the moment condition ====. It is worth noting that ==== is not always satisfied, especially when ==== is high-dimensional or its underlying distributions are heavy-tailed or contain outliers.====In this paper, we shall develop the properties of the PQC and sample PQC. It can be shown that the proposed measure is applicable as an index to quantify the conditional quantile independence. Unlike ==== and ====, the PQC relaxes the restriction on the dimension of ==== and thus can handle predictors with an arbitrary dimension. Another attractive property of PQC is that it does not require the moment condition used in ====The rest of the paper is organized as follows. In Section ====, the PQC measure and its estimator are introduced, and their theoretical properties are also presented. In Section ==== and ====, respectively. We complete the paper with a brief discussion in Section ====. All the technical proofs are deferred to the Appendix.",Projection quantile correlation and its use in high-dimensional grouped variable screening,https://www.sciencedirect.com/science/article/pii/S0167947321002036,14 October 2021,2021,Research Article,162.0
"Pavithra Celeste R.,Deepak T.G.","Department of Mathematics, Indian Institute of Space Science and Technology, Thiruvananthapuram, Kerala, India","Received 13 January 2021, Revised 16 September 2021, Accepted 26 September 2021, Available online 12 October 2021, Version of Record 14 October 2021.",https://doi.org/10.1016/j.csda.2021.107362,Cited by (1),", which are random variables of the continuous type is the main objective here. The following two cases are considered: (====) the set ==== has at most a finite number of elements for every real ==== and ==== for every real number ","Phase type (PH) distributions, introduced by ====, form a dense family of distributions (in the metric of weak convergence of distributions) on ====Phase type distributions, however, can be used for modeling non-negative random variables and they have infinite support and an exponentially decaying tail. But in many instances, they may be too restrictive. Even in the case of non-negative random variables, though theoretically, the denseness property of the PH class allows us to fit all distributions on ====, where ==== is a PH variable. In ====, it is proved that the tail probabilities of ==== obey power law in contrast to the exponentially decaying tail for the class of PH distributions. They called this transformed distributions as ==== class of distributions. Also, the PH distributions can model only non-negative data and they have infinite support. It is rather obvious that unless the right tail of the distribution drops down fast, approximation of a finite support distribution (distribution having support on intervals of finite lengths) with a PH distribution could result in a large dimensionality for the fitted distribution. This shows the need for some modifications in PH distributions so that they can be used efficiently in the case of fitting finite support data. Such problems can be addressed by using finite support phase type (FSPH) random variables, introduced by ==== (a modified version of FSPH distributions has been studied recently, by ====). Similarly, the ==== (BPH) distribution (studied by ====.====The expectation-maximization (EM) algorithm introduced by ==== was the first to develop a general approach to ML estimation of continuous PH distributions. ==== suggested an alternative method for calculating matrix exponentials and related integrals appearing in the E-step in the EM algorithm to speed up the execution of the algorithm considerably for small and medium-sized data sets. Apart from this, the main contribution of ==== is that of proposing methods for calculating the FIM for discrete and continuous PH distributions using formulae that are related to both the EM algorithm and the Newton-Raphson approach.====Here we discuss the parameter estimation and FIM computation for some functions of the PH class of distributions using the EM approach. The main objective here is to develop a general approach for the parameter estimation of some functions of PH random variables – LogPH and FSPH are particular cases – based on the one developed by ==== for the PH class, and their FIM computation based on an approach similar to the one used in ==== for the PH class. We hope that this general approach will help researchers to carry out estimation and inference for many classes of PH (other than the usual PH class), not only the ones that have already been introduced (like those as mentioned above) but also those which are yet to come.====The terms appearing in the computation of the FIM under case (i) also appear in case (ii). So, their computation is easy once we know the computation technique of the partial derivatives appearing in case (ii). Hence, we omit the calculation part of case (i).====In order to compute the derivatives appearing in case (ii), we proceed as follows:====Let==== and==== Hence, ====, ==== and ==== become==== and==== Now for ====,====In order to compute the above derivatives, we need to find out ====, and ====, where ====. Now, for computing ====, we consider ==== as a sub-intensity matrix, and compute the derivative using uniformization, as in ====.====By uniformization, it is proved in ==== that, for a sub-intensity matrix ==== where==== and ====.====Also, for ==== and ====. By assuming that the maximum of the diagonal of −==== appears in row ====, we get====and==== Now, ==== is given by==== and ====Also, by substituting ==== in ==== and differentiating, we get==== Thus, for evaluating the above derivatives we need the terms ==== and ====. By uniformization, we have==== where ====. Hence,==== Now, from ====, we have==== Also, ==== yields:==== Substituting these values in ====–====, we get the FIM for the distributions that come under case ====. Once the FIM is computed, its inverse gives the covariance between the MLE of various pairs of parameters. ==== ",Parameter estimation and computation of the Fisher information matrix for functions of phase type random variables,https://www.sciencedirect.com/science/article/pii/S0167947321001961,12 October 2021,2021,Research Article,163.0
"Yang Qi,He Haijin,Lu Bin,Song Xinyuan","School of Management, Shandong University, Jinan, China,College of Mathematics and Statistics, Shenzhen University, Shenzhen, China,Institute of Economics and Finance, Nanjing Audit University, Nanjing, China,Department of Statistics, Chinese University of Hong Kong, Hong Kong, China","Received 17 April 2021, Revised 16 September 2021, Accepted 25 September 2021, Available online 11 October 2021, Version of Record 15 October 2021.",https://doi.org/10.1016/j.csda.2021.107365,Cited by (4),". The satisfactory performance of the suggested method is demonstrated by simulation studies. Application to the corporate default data illustrates the utility of the proposed methodology and its superiority over conventional methods. The empirical results reveal that defaulted companies usually have low profitability, high debt level, and poor operating capacity. The findings also help differentiate between groups that are susceptible and nonsusceptible to default and provide new insights into the warning signs and effective strategies for preventing defaults.","Corporate assets and debts are vital items in corporate finance and play a significant role in the economic development of a country. They reveal the financial situation of a specific company and also provide significant references for the government to establish economic policies, maintain financial stability, and promote economic development. Therefore, detecting risk factors for corporate default are of great importance to financial stability.====; ====; ====; ====). With the use of the factor analysis technique, a latent factor can be characterized by multiple manifest variables through a confirmatory factor analysis (CFA) model, such that a complete measurement of the latent factor can be obtained without losing much information. With reference to variable description in Section ====; ====; ====; ====) have illustrated the necessity of treating the entire population as a mixture of cured (nonsusceptible) and uncured (susceptible) subjects because of the existence of a substantial proportion of subjects who never experience the event of interest, even if the follow-up is sufficiently lengthy. In finance, for example, ==== applied a mixture PH model to investigate the personal loan portfolio of a UK bank and revealed the competitive performance of their model over traditional approaches in terms of discrimination and calibration ability. ====The main contribution of this study is two-fold. From a methodological standpoint, the proposed model, which incorporates CFA, AH, and cure models into a unique framework to jointly analyze latent variables, the hazard of default, and the probability of nonsusceptibility to default, is innovative. This joint modeling approach facilitates the extraction of useful latent factors from noisy indicators, eliminates the multicollinearity problem, and simplifies the model by reducing the dimensionality of independent variables. We utilize the expectation-maximization (EM) algorithm (====The remainder of this article is organized as follows. Section ==== reports the simulation study results, and Section ==== explores the application of the proposed methodology to the corporate default study. Section ==== concludes the paper.====The MH algorithm can be implemented as follows: at the ====th iteration with a current value ====, a new candidate ==== is generated from a proposal distribution ====, where ==== can serve as an informative covariance matrix based on Model ====.====The new candidate is accepted according to the following probability==== The value of ==== is chosen such that the average acceptance rate is approximately 25% (====). The conditional expectations in ==== can be obtained by using the Monte Carlo approximation with the posterior samples of ====. Let ==== be the ==== posterior samples of ==== generated through the MH algorithm, then for any function ====, the conditional expectation ==== can be calculated using its Monte Carlo estimate ====.",Mixture additive hazards cure model with latent variables: Application to corporate default data,https://www.sciencedirect.com/science/article/pii/S0167947321001997,11 October 2021,2021,Research Article,164.0
"Samaddar Arunava,Jackson Brooke S.,Helms Christopher J.,Lazar Nicole A.,McDowell Jennifer E.,Park Cheolwoo","JP Morgan Chase, Mumbai, Maharashtra 400064, India,Department of Psychology, University of Georgia, Athens, GA 30602, USA,Yoh, LLC, Philadelphia, PA 19130, USA,Department of Statistics, Penn State University, University Park, PA 16802, USA,Department of Mathematical Sciences, KAIST, Daejeon, 34141, South Korea","Received 29 September 2020, Revised 23 September 2021, Accepted 23 September 2021, Available online 8 October 2021, Version of Record 12 October 2021.",https://doi.org/10.1016/j.csda.2021.107361,Cited by (0),"In the analysis of functional ==== imaging (fMRI) data, a common type of analysis is to compare differences across scanning sessions. A challenge to direct comparisons of this type is the low signal-to-noise ratio in fMRI data. By using the property that brain signals from a task-related experiment may exhibit a similar pattern in regions of interest across participants, a semiparametric approach under shape invariance to quantify and test the differences in sessions and groups is developed. The common function is estimated with local ==== and the shape invariance model parameters are estimated using evolutionary optimization methods. The efficacy of the semi-parametric approach is demonstrated on a study of ","). One specific measure of cognitive control uses saccadic eye movement tasks.====Saccades are rapid redirections of gaze and exist in a hierarchy of behavior from simple to complex. Prosaccades are reflex-like responses towards a newly appearing visual stimulus. Antisaccades are more cognitively complex. In the presence of a newly appearing visual cue, participants are instructed to inhibit a glance towards that cue and generate a saccade to the mirror image location (opposite side, same distance from center) - without the benefit of a physical stimulus. An initial glance towards the cue instead of away is an antisaccade error, and is considered a failure of cognitive control. People with schizophrenia show higher antisaccade error rates than healthy participants, despite having intact prosaccade responses (====; ====; ====).====; ====; ====; ====). People with schizophrenia show disruptions in antisaccade circuitry, primarily reduced BOLD signal in higher-order cortical areas (PFC and anterior cingulate) (====; ====).====While cognitive control tasks initially recruit diffuse higher-order neural circuits, exposure to task practice induces changes in performance, as well as changes in neural circuitry (====). Neural circuitry may become more efficient, as evidenced by decreased signal in higher-order areas, such as prefrontal and/or parietal cortex (====; ====). These effects have been reported in both healthy samples (====; ====; ====) and psychiatric samples (====; ====). Attenuation of neural circuitry following practice of cognitively complex tasks, however, is not supported by every study. A review by ==== highlights discrepant results, which appear to be dependent on multiple factors including the task type, practice duration, and specific neural regions examined.====The motivation for the proposed approach results from the study of practice effects analyzed in ====. At the initial stage (pre-practice), participants with schizophrenia and age-and-gender-matched control participants performed an antisaccade task while MR images were acquired. Following this initial scanning session, the participants conducted eight days of daily practice in the lab. All participants then underwent a second scanning session performing the same antisaccade task (post-practice). A more detailed description of the procedure is provided in Section ==== and in the Appendix. ==== found that both groups displayed improved performance and neural attenuation following antisaccade practice. In the current paper we consider BOLD signal changes in 20 ROIs comprising neural circuitry supporting antisaccade performance (see the Appendix for the complete list).====Two main objectives in this study are to statistically test i) which groups and ROIs show attenuation at the post-practice session and ii) which ROIs show a difference among the two groups. ==== combines feature extraction and mixed models to test ii), but does not test attenuation. Additionally, ==== manually extracts features to account for lags in the brain signals, which takes time and effort for fine-tuning.====). We extend the work of ==== on a single group to the comparison of multiple sessions and groups through the parameters in the model.====The proposed approach is attractive for a variety of reasons. First, by employing the shape invariance model that takes advantage of common features shared across participants (e.g. the box-car shape in ====), we effectively obtain a nonparametric estimate of the common function from multiple participants. Curve estimation for each subject is particularly challenged by the low signal-to-noise ratio (SNR) in fMRI data. Second, the horizontal shift parameter allows us to account for the registration issue frequently found in fMRI data due to the different response times among participants. Third, the vertical scale parameter in the model enables us to conduct the main tests for attenuation and comparison of groups. Hence, we conduct a statistical test on the parameter rather than on the function itself, and thus apply conventional test statistics.====The rest of paper is organized as follows. Section ==== describes the dataset in detail. Section ==== introduces the proposed semiparametric approach under shape invariance model. In Section ====, we demonstrate the performance of the proposed approach using simulated data, and then apply it to the fMRI data described in Section ====. Section ==== concludes with summary and discussion for future work. The proposed method can be applied to various scientific problems beyond fMRI studies where a common feature is shared in multiple groups, such as human growth (====; ====), human circadian rhythms (====), and bioassay (====).",A group comparison in fMRI data using a semiparametric model under shape invariance,https://www.sciencedirect.com/science/article/pii/S016794732100195X,8 October 2021,2021,Research Article,165.0
"Boland Joanna,Telesca Donatello,Sugar Catherine,Jeste Shafali,Goldbeck Cameron,Senturk Damla","Department of Biostatistics, University of California Los Angeles, Los Angeles, CA 90025, USA,Department of Statistics, University of California Los Angeles, Los Angeles, CA 90025, USA,Department of Psychiatry and Biobehavioral Sciences, University of California Los Angeles, Los Angeles, CA 90025, USA","Received 12 February 2021, Revised 27 September 2021, Accepted 30 September 2021, Available online 8 October 2021, Version of Record 15 October 2021.",https://doi.org/10.1016/j.csda.2021.107367,Cited by (2),"EEG experiments yield high-dimensional event-related potential (ERP) data in response to repeatedly presented stimuli throughout the experiment. Changes in the high-dimensional ERP signal throughout the duration of an experiment (longitudinally) is the main quantity of interest in learning paradigms, where they represent the learning dynamics. Typical analysis, which can be performed in the time or the frequency domain, average the ERP waveform across all trials, leading to the loss of the potentially valuable longitudinal information in the data. Longitudinal time-frequency transformation of ERP (LTFT-ERP) is proposed to retain information from both the time and frequency domains, offering distinct but complementary information on the underlying cognitive processes evoked, while still retaining the longitudinal dynamics in the ERP waveforms. LTFT-ERP begins by time-frequency transformations of the ERP data, collected across subjects, electrodes, conditions and trials throughout the duration of the experiment, followed by a data driven multidimensional principal components analysis (PCA) approach for dimension reduction. Following projection of the data onto leading directions of variation in the time and frequency domains, longitudinal learning dynamics are modeled within a mixed effects modeling framework. Applications to a learning paradigm in autism depict distinct learning patterns throughout the experiment among children diagnosed with Autism Spectrum Disorder and their typically developing peers. LTFT-ERP time-frequency joint transformations are shown to bring an additional level of specificity to interpretations of the longitudinal learning patterns related to underlying cognitive processes, which is lacking in single domain analysis (in the time or the frequency domain only). Simulation studies show the efficacy of the proposed methodology.","; ====). Typical analysis of ERP data averages the ERP signal over all the trials of the experiment, enhancing the signal-to-noise ratio (SNR) (====; ====; ====). While this common technique is effective in increasing the SNR of the ERP data, it collapses information gained during the course of the experiment. This longitudinal information is important, especially in learning experiments, where it characterizes the learning trends across the study participants, including speed of learning. Previous works have been proposed to study the longitudinal changes over the course of a learning experiment. ==== proposed the moving average preprocessed ERP (MAP-ERP) which averages ERPs over trials in a sliding-window to retain the inherent longitudinal information. ==== and ====The two previously proposed approaches of ==== and ====; ==== for studying longitudinal trends in EEG experiments analyze ERP in different domains; in the time domain and the frequency domain, respectively. While the time domain analysis of ERP data concentrates on interpretations of the commonly studied ERP phasic components, such as the P3 as shown in ====(c), the frequency domain analysis concentrates on interpretations of power from different frequency bands: delta (1-4 Hz), theta (4-8 Hz), alpha (8-16 Hz), beta (16-32 Hz) and gamma (over 32 Hz). In our motivating implicit learning paradigm, N1 and P3 are the phasic components typically observed in the ERP waveforms. The N1 dip, with a short latency (time-delay), is thought to be related to early category recognition, while the P3 peak, with a long latency, is traditionally related to cognitive processes such as signal matching, decision making and memory updating (====; ====). In the frequency domain, frequencies in the delta and theta bands have been reported to contribute to a P3 phasic component where frequencies in the delta band are associated with evaluative cognitive processing, and in the theta band are associated with the orienting response to novel stimuli (====; ====). Since both time and frequency domain analysis of ERP data carry different but complimentary information about the observed signal, we consider a time-frequency decomposition of the ERP data, targeting even richer information than is available in single domain analysis. We propose the longitudinal time-frequency transformation of ERP (LTFT-ERP) method, where a wavelet transformation is applied to the ERPs. LTFT-ERP, not only targets richer information in the signal through time-frequency transformations, it also allows modeling of longitudinal changes in the signal over trials throughout the learning experiment, adding an additional dimension for analysis (referred to as the longitudinal dimension).====, ==== proposed time-frequency transformations (TFTs) of ERP data via wavelets and further included dimension reduction of the high dimensional time-frequency power surfaces through principal components analysis (PCA). To capture the longitudinal changes throughout the learning experiment, the resulting data from the proposed LTFT-ERP is even higher dimensional in our applications since the TFTs are repeated over multiple trials of the experiment. To adopt a data-driven approach to dimension reduction, similar to ====The paper is organized as follows. Section ==== outlines the proposed LTFT-ERP approach, including dimension reduction via MDPCA following the multidimensional TFT decompositions and modeling of the longitudinal trends via a mixed effects model. Simulation studies to study the efficacy of the LTFT-ERP in modeling of longitudinal trends in the MDPCA scores are outlined in Section ====, followed by applications to the implicit learning paradigm in Section ====. We conclude with a brief discussion given in Section ====.====The following is the Supplementary material related to this article.",A study of longitudinal trends in time-frequency transformations of EEG data during a learning experiment,https://www.sciencedirect.com/science/article/pii/S0167947321002012,8 October 2021,2021,Research Article,166.0
"Nattino Giovanni,Song Chi,Lu Bo","Istituto di Ricerche Farmacologiche Mario Negri IRCCS, Via G.B. Camozzi 3, Ranica (BG), 24020, Italy,Division of Biostatistics, College of Public Health, The Ohio State University, 1841 Neil Avenue, Columbus (OH), 43210, USA","Received 31 January 2021, Revised 23 September 2021, Accepted 27 September 2021, Available online 6 October 2021, Version of Record 7 October 2021.",https://doi.org/10.1016/j.csda.2021.107364,Cited by (1)," matching and full matching. With multiple treatment arms, however, the optimal matching problem cannot be solved in polynomial-time. This is a major challenge for implementing matched designs with multiple arms, which are important for evaluating causal effects with different dose levels or constructing evidence factors with multiple control groups. A polymatching framework for generating matched sets among multiple groups is proposed. An iterative multi-way algorithm for implementation is developed, which takes advantage of the existing optimal two-group matching algorithm repeatedly. An upper bound for the total distance attained by our algorithm is provided to show that the distance result is close to the optimal solution. Simulation studies are conducted to compare the proposed algorithm with the nearest neighbor algorithm under different scenarios. The algorithm is also used to construct a difference-in-difference matched design among four groups, to examine the impact of Medicaid expansion on the health status of Ohioans.","; ====; ====). When there are multiple treatment arms, however, matching designs have received limited attention. ====).====In presence of ==== groups, we denote with polymatching the design where the matched ====-tuples have the structure of ====-====-====-====, where ==== is the number of subjects from the ====-th group in each ====-tuple and ====. Specifically, if ====, the design is referred to as unity polymatching. Since our paper primarily discusses the unity polymatching design, for simplicity, polymatching is used to refer to unity polymatching hereafter. ====, we prove that the total distance from our algorithm is bounded by the optimal distance multiplied by a constant factor. For matching with ==== groups, the factor is no greater than ====. Since matched design is used with a relatively small number of groups, such bound ensures that our result is close to the optimal one. To make the presentation simple, we illustrate the idea by using ==== throughout the paper. In section ====, we run an extensive simulation study to compare our algorithm with NN algorithm under various distribution scenarios. Section ==== applies polymatching design to examine the impact of Medicaid expansion in Ohio residents using two years of the Ohio Medicaid Assessment Surveys (OMAS). Section ==== discusses the limitations and potential extensions of the polymatching algorithm.====The following is the Supplementary material related to this article.",Polymatching algorithm in observational studies with multiple treatment groups,https://www.sciencedirect.com/science/article/pii/S0167947321001985,6 October 2021,2021,Research Article,167.0
"Wang Jiangyan,Gu Lijie,Yang Lijian","School of Statistics and Data Science, Nanjing Audit University, Nanjing 211815, China,Soochow College, School of Mathematical Sciences, Soochow University, Suzhou 215006, China,Center for Statistical Science and Department of Industrial Engineering, Tsinghua University, Beijing 100084, China","Received 13 February 2021, Revised 26 September 2021, Accepted 27 September 2021, Available online 4 October 2021, Version of Record 7 October 2021.",https://doi.org/10.1016/j.csda.2021.107363,Cited by (1),"Kolmogorov-Smirnov (K-S) simultaneous confidence band (SCB) is constructed for the error distribution of dense functional data based on kernel distribution estimator (KDE). The KDE is computed from residuals of B spline trajectories over a smaller number of measurements, whereas the B spline trajectories are computed from the remaining larger set of measurements. Under mild and simple assumptions, it is shown that the KDE is a uniformly oracle-efficient estimator of the error distribution, and the SCB has the same asymptotic properties as the classic K-S SCB based on the infeasible empirical cumulative distribution function (EDF) of unobserved errors. Simulation examples corroborate with the theoretical findings. The proposed method is illustrated by examples of an EEG (Electroencephalogram) data and a stock data.","Functional data analysis (FDA) has emerged as a focused area of statistics research over the last decade, due to the needs from many disciplines to obtain information from complex data in the form of discretized curves and/or surfaces. These curves/surfaces are generally viewed as randomly sampled functions of time, spatial location, wavelength etc, see ==== and ==== for examples of functional data.====A typical functional data set consists of ====, the ====-th observation of the ====-th subject, where ==== stands for response and ==== predictor, with ==== observations taken for the ====-th subject. As mentioned above, the variable ==== is often time, but it can also be other numerical measures, such as wavelength. In this paper, regular dense design is studied, in other words, ==== with ====. For the ====-th subject, ====, its sample path ==== is the noisy realization of a stochastic process ==== of continuous time in the sense that====in which the ====-th subject mean functions ====, also known as the ====-th subject trajectories, are iid realizations of a stochastic process ==== which is ====, i.e., ====, with unknown mean function ==== and covariance function ====. The measurement errors ==== are also iid, and satisfy ====, ====, with cumulative distribution function (cdf) ==== and probability density function ====.====The presence of errors ==== makes all recorded sample paths ==== discontinuous at each recording location ====. Take, for instance, the EEG (Electroencephalogram) signal in ====, recorded at each millisecond over 33 seconds (i.e., ====) for the 40-th of ==== volunteers participating in experiments conducted by Tsinghua University Department of Mechanical Engineering. The unprocessed “raw” data form a dotted line ====, which is visibly discontinuous despite the large number 33000 of observations. The smoothed “data” ==== represented by the solid line is an estimate ==== of the unobservable signal process ====. We view the noisy discontinuity in raw functional data as a rich source of useful information, which should not be swept under the rug by naming instead the smoothly estimated sample path as “functional data”.====The error distribution plays an important role in statistical inference, forecasting, and applications. For example, if one knew the error (or innovation) distribution of a times series, paths of the underlying model could be generated via Monte Carlo simulations (====). A great number of interesting publications emerge in this popular field, see ==== for bootstrap simultaneous error bars for nonparametric regression, ==== for the conditional error distribution estimation, ==== for the change-point tests of the error distribution, ==== for estimation of the error distribution in mixed effects models, ==== for the residual empirical process of a long-run dependent series.====According to Mercer Lemma (Lemma 1.3 of ====), there exist ====, ====, and an orthonormal basis ==== of ==== such that ====, which implies ====, so sequences ====, ==== are the eigenvalues and eigenfunctions of ==== respectively. The standard process ==== then allows the Karhunen-Loève expansion ==== according to Theorem 1.5 of ====, in which ==== are uncorrelated with mean 0 and variance 1, ====, and the rescaled eigenfunctions ====.====Going back to the functional data, the ====-th process ==== is written as ====, in which the random coefficients ====, are iid copies of ====. Model ==== is rewritten as====Although the sequences ====, ==== and the functional principal components ==== exist mathematically, they are unknown or unobservable respectively to the statistician.====Recent years have seen growing interest in functional data analysis, with problems related to the estimation of functions ==== and parameters ==== addressed. For instance, the estimation of ==== has been studied in ====, ====, ====, while the functional principal components analysis (FPCA), which concerns the estimation of ====, has been extensively studied in ====, ====, ====, ====, and ====. More recently, asymptotic simultaneous confidence band (SCB) has been constructed for ==== in ====, ====, ====, ==== and ==== based on strong approximation arguments, while ==== provided simultaneous confidence envelope for ====. A bootstrap procedure was introduced in ==== to construct SCB over a dense grid for each trajectory ====, but without theoretical proof. For theoretical results and applications of asymptotic SCB in various contexts, see ==== for stratified sampling, ==== for penalized spline estimators, ==== for time trend modeling, ==== for conditional variance function, ==== for correlation curve, ==== for the mean of functional time series, ==== for the mean of sparse functional data, ==== for the single-index link function, and ==== for mean and variance functions based on deterministic design, among others.====Methods for SCB in FDA, however, are somewhat underdeveloped. It is known in ====, ==== that smoothed bootstrap SCBs improve asymptotic SCBs for nonparametric regression. This resampling scheme proves to be extremely difficult to extend to FDA, partly due to probability distribution of measurement errors ==== being unknown, so generation of data based on equations ==== or ==== infeasible. There exists a vast literature on error distribution estimation, see, for instance, ====, ====, ====, ==== and ====. Although the inference of error distribution of time series model has been studied, similar development in the context of FDA has yet to be seen, and Kolmogorov-Smirnov (K-S) SCB is available only for error distribution of linear models, see ====. For distribution function ==== of measurement errors ==== in functional data ====, none of the existing standard methods adapt directly, especially in terms of making available any K-S SCB for the distribution function ====. Thus, this work tackles a problem of fundamental importance: quantifying uncertainty of error distribution estimation in the context of FDA with the aid of SCB.====To circumvent the above difficulty, one may divide the index set ==== into two subsets====in which ==== is a positive integer smaller than ====, ====, ====. ==== is a set of equally spaced points with distance ==== between neighboring grid points. If the subset of ==== measurement errors ==== were actually known by “oracle”, one could compute the empirical cdf of ====, ==== and also a would-be kernel distribution estimator (KDE) ==== could be computed as====in which ==== is a kernel function, rescaled as ====, with bandwidth ====, see ==== for details. Similar to ====,====while according to ====, the infeasible KDE ==== is asymptotically equivalent to ==== in the sense that ====, hence Slutsky's Theorem implies that====The above two equations yield one nonsmooth infeasible K-S SCB for ==== based on ====:====and one smooth infeasible SCB based on ====:====As ====, both ==== and ==== are also infeasible and useful only as a benchmark for performance, suggesting instead a plug-in oracle KDE====with residuals ==== to mimic ====, ====, in which the B spline estimator of ====, ====, is defined in ====. A naturally raised question is if one could construct a smooth SCB based on ====:====by showing uniform closeness between the infeasible KDE ==== and the oracle KDE ====: ====, then ==== as well? This SCB would then be called “oracle” because the KDE ==== computed from residuals ====, ==== ==== without exact knowledge of trajectories ====, is asymptotically equivalent to the KDE ==== and ==== computed from errors ====, ==== ==== using complete information of ==== available only by “oracles”.====To our best knowledge, our analysis provides the first asymptotic SCB for the error distribution in functional data, with nearly exact coverage and exact finite sample simultaneous credibility. The estimator and SCB are robust for various errors distribution, which may or may not be Gaussian, and do not require estimating quantiles of the maximum of stochastic process with complicated covariance structure. We emphasize that the analog of ==== does not hold for====In other words, one could not establish ====, which is due to the same reason that makes the difference between stochastic processes ==== and ==== nonnegligible.====The rest of the paper is organized as follows. In Section ==== we state uniform closeness of ==== and ==== as the main theoretical result which establishes the SCB ====. Section ==== provides asymptotic results via decomposition of the B spline estimators of trajectories, which are essential for the proof of main ====. Section ==== describes the actual data-driven steps to implement the SCB (====). Section ==== contains results from Monte Carlo study, and Section ==== detailed analysis of the aforementioned EEG data and a stock data. Section ==== discusses significance of the work and possible extension. Proofs of all technical results are deferred in the Appendix.",Oracle-efficient estimation for functional data error distribution with simultaneous confidence band,https://www.sciencedirect.com/science/article/pii/S0167947321001973,4 October 2021,2021,Research Article,168.0
"Ghaderinezhad Fatemeh,Ley Christophe,Serrien Ben","Department of Applied Mathematics, Computer Science and Statistics, Ghent University, Ghent, Belgium,Experimental Anatomy Research Group, Vrije Universiteit Brussel, Brussels, Belgium","Received 5 January 2021, Revised 14 September 2021, Accepted 14 September 2021, Available online 4 October 2021, Version of Record 9 June 2022.",https://doi.org/10.1016/j.csda.2021.107352,Cited by (3),", and its choice will impact the subsequent inference. It is therefore important to have a convenient way to quantify this impact, as such a measure of prior impact will help to choose between two or more priors in a given situation. To this end a new approach, the Wasserstein Impact Measure (WIM), is introduced. In three simulated scenarios, the WIM is compared to two competitor prior impact measures from the literature, and its versatility is illustrated via two real datasets.",", ====There does not exist a formal definition of prior impact in the literature. According to ====, defined as the approximate number of observations equivalent to the information conveyed by the prior, see for instance ====; ====; ====; ====; ==== and the references therein. ==== recently extended their approach to any two priors for one-dimensional parameters, provided that the posteriors are nested and have finite first moments; see also ====.====For practical purposes, the power of the Wasserstein distance idea has not been exploited so far. The obtained bounds and rare explicit results are obtained for tractable posteriors; indeed, most examples considered in ==== and ====, namely the effective sample size (in what follows we will use its most recent version called MOPESS) and the Neutrality mentioned above. We illustrate the practical aspects of the WIM on two data examples in Section ====. Finally, we conclude the paper with a discussion in Section ====.",The Wasserstein Impact Measure (WIM): A practical tool for quantifying prior impact in Bayesian statistics,https://www.sciencedirect.com/science/article/pii/S0167947321001869,4 October 2021,2021,Research Article,169.0
"Conti Pier Luigi,Mecatti Fulvia,Nicolussi Federica","Dipartimento di Scienze Statistiche, Sapienza Università di Roma, P.le A. Moro, 5, 00185 Roma, Italy,Dipartimento di Sociologia e Ricerca Sociale, Università di Milano-Bicocca, Via Bicocca degli Arcimboldi, 8, 20126 Milano, Italy,Dipartimento di Economia, Management e Metodi Quantitativi, Università degli Studi di Milano, Via Festa del Perdono 7 - 20122 Milano, Italy","Received 16 December 2020, Revised 20 July 2021, Accepted 26 September 2021, Available online 2 October 2021, Version of Record 13 October 2021.",https://doi.org/10.1016/j.csda.2021.107366,Cited by (3), indicates that the proposed resampling technique outperforms its two main competitors for confidence interval construction of various population parameters including ====.,", does not work in sampling from finite populations, because of the dependence among sample units due to the sampling design. Several techniques have been proposed to overcome this problem; a nice, recent review is the paper of ====; cfr. also ====, ====, ====.====ps sampling designs, for short, a special role is played by methodologies based on pseudo-populations; cfr. ====, ==== for general aspects, and ====, where the problem of resampling for finite populations is addressed as a problem of sampling with replacement directly from the sample data, the ==== henceforth, with different drawing probabilities.====Interesting steps forward are in ====, ====. In particular, in ==== is avoided. Essentially along the same path, in ====, henceforth. As a matter of fact, an intuitive requirement is that a resampling design, as both the sample size and the population size become large, should become closer and closer to the original sampling design. From a more formal point of view, as shown in ====, the key property for the asymptotic correctness of a resampling design is that its first order inclusion probabilities should asymptotically coincide with the first order inclusion probabilities of the original sampling design. In the present paper, a new resampling technique, essentially based on sampling with replacement from the original sample, is proposed. The basic idea is to use appropriate drawing probabilities in order to reproduce, at least approximately, pre-fixed first order inclusion probabilities. Its relationships with resampling based on pseudo-populations will be discussed. The relative merits of the proposed resampling technique will be evaluated through a simulation study.====The paper is organized as follows. In Section ====, basic preliminary aspects are exposed. Section ==== deals with a general approach to resampling based on drawing of “types” from the observed sample through a ====-based technique. In Section ==== relationships with pseudo-populations are clarified; they are particularly useful to provide a sound theoretical justification of the proposed resampling technique. In Section ==== theoretical justifications are provided. The merits of the proposed resampling scheme are evaluated in Section ==== through a simulation study. Finally, Section ==== is devoted to conclusions.",Efficient unequal probability resampling from finite populations,https://www.sciencedirect.com/science/article/pii/S0167947321002000,2 October 2021,2021,Research Article,170.0
"Kruse René-Marcel,Silbersdorff Alexander,Säfken Benjamin","Chair of Statistics, University of Göttingen, Humboldtallee 3, 37073 Göttingen, Germany,Campus-Institut Data Science, Goldschmidtstraße 1, 37077 Göttingen, Germany","Received 22 December 2020, Revised 16 September 2021, Accepted 16 September 2021, Available online 29 September 2021, Version of Record 4 October 2021.",https://doi.org/10.1016/j.csda.2021.107351,Cited by (1),". Yet, the method of model averaging has been sparsely explored in this context. A weight finding criterion for model averaging of linear mixed models is introduced, as well as its implementation for the ==== Since the optimization of the underlying criterion is non-trivial, a fast and robust implementation of the augmented ==== optimization technique is employed. Furthermore, the influence of the weight finding criterion on the resulting model averaging estimator is illustrated through simulation studies and two applications based on real data.","; ====), for spline smoothing (====; ====; ====).====, to open-source versions like the de-facto standard in ==== (====) or the ==== (====) Package for ====) (====). Furthermore, evaluating the suitability of the included random effects of models with nested or clustered structures suffer from limitations like boundary issues with likelihood-ratio tests (====; ====). An overview of measures of explained variation and model selection in linear mixed-effects models can be found in ====.==== show, however, that it is possible to derive an AIC from the conditional form of the linear mixed effect model, which has proven to be particularly suitable accounting for possible shrinkage within the random effects (====). ==== prove that an analytical solution can be derived and thus, reduce the computational intensity of the corrected version of the conditional AIC.====), a weighted average of the considered models is calculated and then used for analysis. An important key factor when applying model averaging is the selection of the underlying weights. Different proposals have been brought forward, the most prominent being the approach of information criteria based weights by ====. Yet, a majority of proposals aim at classical linear models and do encounter difficulties when applied to the model framework of linear mixed models. A proposal by ==== (====).====In this paper we present a weight finding criterion for the calculation of asymptotically optimal weights based on the work of ==== and ====. In addition we present an implementation of the proposed weight finding criterion for the programming language ====, which we have released as part of the R-Package ==== (====; ====) and present our implementation of the algorithm.====This paper is structured as follows: Section ==== introduces the theory and formulations of linear mixed models, as well as the estimation and the application of linear mixed models for spline smoothing. Section ==== analyzes the properties of the implemented methods by applying them in three different simulation settings. Section ==== studies the proposed model averaging method applied to real-world examples, whereas the last Section ====, gives a summary of the findings of the previous sections and also gives an outlook of further work concerning model selection and model averaging of linear mixed models.",Model averaging for linear mixed models via augmented Lagrangian,https://www.sciencedirect.com/science/article/pii/S0167947321001857,29 September 2021,2021,Research Article,171.0
"Xiao Zhen,Zhang Qi","Qingdao University, Qingdao 266071, China,Central China Normal University, China","Received 22 September 2020, Revised 30 March 2021, Accepted 11 September 2021, Available online 23 September 2021, Version of Record 4 October 2021.",https://doi.org/10.1016/j.csda.2021.107348,Cited by (0),", whose estimation of the central subspace and variable selection are performed simultaneously. This method can be directly applied to block-missing data without imputation. The algorithm called Adjusted Linearized Alternating Direction of Method of Multipliers (Adjusted L-ADMM) is addressed correspondingly. The ==== are investigated. Numerical results show that our method can effectively and robustly identify important ==== in the high-dimensional case.","In modern scientific research, such as biomedicine, genomics and transportation, high dimensional and block-missing data are often collected from multiple modalities, so how to process this type of data has become a hot statistical research issue recently. Consider a regression model of the univariate output variable ==== given ====. When ====). To this end, we introduce the notion of sufficient dimension reduction.====There exists a small number ==== and a matrix ====, whose columns are made up of ==== unknown vectors ====. Then sufficient dimension reduction requires that==== where ==== represents independence. In other words, ==== is sufficient to infer the distribution of response ====. In fact the matrix ==== is substitutable, so the subspace spanned by its columns called the dimension reduction subspace ==== is of interest indeed. ==== are correspondingly referred to as the sufficient dimension reduction directions. Obviously, the subspaces that satisfies the above conditions are not unique, but the intersection of them is the unique minimum dimension reduction subspace, which is defined as central subspace ==== (==== and ====.====In order to obtain a basis of ====, ==== proposed sliced inverse regression, which is one of the classical sufficient dimension reduction methods. Compared with other methods, the principle and calculation of sliced inverse regression are simpler and more popular. However, the dimension reduction directions obtained from the sliced inverse regression, which performs no variable selection, include all ====Numerous methods have been proposed to identify important covariates for sliced inverse regression. Popular methods include central subspace estimation via selection variable (====), and sparse estimation in each dimension reduction direction (====; ====), and the SCAD (====; ====; ====). Since these models are not convex, there is no guarantee that the solution is global optimal. ==== estimated a sparse reduction through a novel representation of sliced inverse regression, while their algorithm still corresponded to non-convex problem. ==== recently imposed a lasso penalty on the objective function to fit a sparse sliced inverse regression, which is a potential approach. One problem with lasso is that it can lead to the instability in estimate in case of high-dimensional settings or existence of strong correlation between variables. The proposed convex sparse sliced inverse regression with elastic net in this paper is inspired by ====.====In addition, one particular challenge in high-dimensional settings is the block-missing problem, which is inevitable in multi-modality data. ==== only used the data with complete observations to predict the response, but this strategy would waste a lot of useful information. There are also many attempts to impute the missing data first with existing methods (====), which fit poorly for the block-missing data (====; ====; ====). This paper adopts an analogous strategy. The flexible estimate of covariance matrix is applied to sparse sliced inverse regression, thus sufficient dimension reduction is achieved for block-missing multi-modality data.====The rest of this paper is organized as follows. In Section ====, we introduce the convex sparse sliced inverse regression with elastic net, and give the detailed process of solving the parameters and the estimate of ====. We give the theoretical results of estimation for both complete data and block-missing multi-modality data in Section ====. The excellent performance of our method in high-dimensional settings is demonstrated through abundant simulations in Section ====. We conclude this article with a discussion in Section ==== and provide technical proofs in the Appendix.====Without loss of generality, assume that ====. The fact that the central inverse regression curve is contained in the subspace spanned by ==== has been proved (1991). Now we give the proof of the process from this theorem to the generalized eigenvalue problem. We need to show that ==== also resides in the linear subspace spanned by ====; that is, ==== (====). Consider any vector ==== that's in the orthogonal complement of the space spanned by ==== (====); that implies ====, and then we can derive that ====, which equals ====. The proof of ==== (====) is complete.====For some constants ====, we have====Then, by the orthogonal normalization condition==== we have,==== where==== Hence, there's only one term left on the right-hand side of ====. By replacing ==== with ====, the generalized eigenvalue problem can be obtained.==== Finally, similar to principal component analysis, the orthogonal normalization condition of ==== with respect to ==== is added on the basis of ====. This completes the proof.",Dimension reduction for block-missing data based on sparse sliced inverse regression,https://www.sciencedirect.com/science/article/pii/S0167947321001821,23 September 2021,2021,Research Article,172.0
"Chen Jiaxun,Micheas Athanasios C.,Holan Scott H.","Eli Lilly and Company, Lilly Corporate Center, Indianapolis, IN 46285, United States of America,Department of Statistics, University of Missouri, 146 Middlebush Hall, Columbia, MO 65211-6100, United States of America","Received 6 October 2020, Revised 19 May 2021, Accepted 13 September 2021, Available online 20 September 2021, Version of Record 28 September 2021.",https://doi.org/10.1016/j.csda.2021.107349,Cited by (1),"To model spatial point patterns with discrete time stamps a flexible spatio-temporal area-interaction point process is proposed. In particular, this model is suitable for describing the dependency between point patterns over time, when the new point pattern arises from the previous point pattern. A hierarchical model is also implemented in order to incorporate the underlying evolution process of the model parameters. For parameter estimation, a double Metropolis-Hastings within ","Studying point processes over time has become increasingly important in many disciplines. For example, in criminology or in seismology we wish to predict future occurrences of criminal activity or earthquakes, respectively. The time an event occurs can be crucial to the understanding of several characteristics of the phenomenon, including the evolution of the point process, connections with other events in the same domain, or the time of possible observations of future events. As such, to fully understand the stochastic mechanism of the process of locations as it evolves over time, it is natural to include the time of occurrence as another coordinate in the event.====, space-time Epidemic-Type Aftershock Sequence (ETAS) model introduced by ====, and the multi-scale spatio-temporal area-interaction model developed by ====.====One of the advantages of the ====), the hardcore process (====), the Geyer's saturation process (====) and the area-interaction process (====). The Strauss process and its special case, the hardcore process, can be used to model inhibition between points. The saturation process and area-interaction process can be applied to both repulsive and attractive point patterns. Properties of the Poisson and Markov point process models have been extensively studied over the past fifty years. Theoretical and practical treatments can be found in the texts by ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ==== and ====.====In particular, Gibbs processes offer a large class of models which allow any type of interaction (attraction or repulsion) between the point process events, across space and time and, as such, have received more attention over the past few years (see ====, for a recent review). Moreover, space-time Gibbs processes can describe phenomena with interactions at different spatial or spatio-temporal scales. For example, in the case of seismic data, different sources of earthquakes (faults, active tectonic plates and volcanoes) produce events with different displacements (e.g., ====) and can be seen as the superposition of background earthquakes and clustered earthquakes (e.g., ====), in ecology (====; ====), in epidemiology (====), and in seismology (====; ====; ====), mainly based on Gibbs and Cox point processes, but not exclusively (e.g., ====).====In contrast, there are very few spatio-temporal models in the literature; ==== and ==== modeled the multi-scale spatio-temporal structure of forest fires occurrences using the log-Gaussian Cox processes (LGCP) and the multi-scale Geyer saturation process, respectively. Additionally, ==== developed a multi-scale area-interaction model for varicella cases and ====In this paper, we propose a discrete-time spatio-temporal area-interaction process, where we implement a Markovian structure for the ====We propose a hierarchical model structure to describe the underlying evolution process. A time indexed parameter structure and autoregressive structure are implemented in the next level of the hierarchy. The time indexed structure allows us to evaluate the point process at each time period, while the autoregressive structure focuses more on the evolution of the underlying process.====For parameter estimation, we implement the double Metropolis-Hastings (DMH) (====; ====) with a standard MCMC sampler. According to (====), the DMH is the most practical approach for high-dimensional data. Based on the estimates of the parameters at each time period and the underlying evolution model, we can effectively predict the parameter values. Consequently, future point patterns can be predicted based on the evolution parameter estimates and the point pattern in the previous time period.====This paper proceeds as follows. Section ==== begins by defining the proposed spatio-temporal area-interaction point process. Section ==== provides a sensitivity analysis of the model parameters. Section ==== presents the results of a simulation study that illustrates the effectiveness of the estimation method and forecast procedure. Section ==== illustrates an application to the United States natural caused wildfire data from 2002 to 2019. Finally, in Section ====, we provide concluding remarks.====The full conditional distributions for the evolution STAI process are shown as follows",Hierarchical Bayesian modeling of spatio-temporal area-interaction processes,https://www.sciencedirect.com/science/article/pii/S0167947321001833,20 September 2021,2021,Research Article,173.0
"Rügamer David,Baumann Philipp F.M.,Greven Sonja","Chair of Statistical Learning and Data Science, Department of Statistics, LMU Munich, Germany,KOF Swiss Economic Institute, ETH Zurich, Switzerland,School of Business and Economics, Humboldt-Universität zu Berlin, Germany","Received 20 November 2020, Revised 13 September 2021, Accepted 13 September 2021, Available online 17 September 2021, Version of Record 28 September 2021.",https://doi.org/10.1016/j.csda.2021.107350,Cited by (3)," on which the model conditions). Properties of the method are validated in simulation studies and in an application to a data set in monetary economics. The approach is particularly useful in cases of non-standard selection procedures, as present in the motivating application.","; ====, ====) to select among different random effect structures or based on tests (see, e.g. ====To overcome overconfident inference results after model selection, a variety of post-selection inference approaches exist. These adjust classic inference for the additional stochastic aspect in selecting the model and hence the tested hypothesis. Sparked by various proposals such as ====, many authors have addressed the problem of valid inference after model selection in different ways. ==== (or short ====). Another research direction in valid post-selection inference focuses on ====, a post-model selection procedure that motivates a range of methods that target specific selection algorithms, especially the Lasso (see, e.g., ====; ====; ====), methods based on the generalized degrees of freedom (====; ====) or custom selection procedures, potentially combining different selection mechanisms. An example of the latter is given in our application.====In the following, we summarize the theory of selective inference for linear models in Section ==== and extend existing approaches for selective inference to mixed models in Section ==== and present simulation results for the proposed methods in Section ====. We apply our results to the world inflation data, compare results with existing approaches in Section ==== and summarize our concept in Section ====.====The following is the Supplementary material related to this article.",Selective inference for additive and linear mixed models,https://www.sciencedirect.com/science/article/pii/S0167947321001845,17 September 2021,2021,Research Article,174.0
"Zhou Haiming,Huang Xianzheng","Department of Statistics and Actuarial Science, Northern Illinois University, DeKalb, IL 60115, USA,Department of Statistics, University of South Carolina, Columbia, SC 29208, USA","Received 19 January 2021, Revised 29 August 2021, Accepted 30 August 2021, Available online 10 September 2021, Version of Record 20 September 2021.",https://doi.org/10.1016/j.csda.2021.107345,Cited by (4),", is developed for easy implementation of the proposed regression methodology.","Researchers in a wide range of fields encounter bounded data in their studies. For example, environmental scientists monitor the proportion of hygienic waste in residential solid waste. Asset allocations in a portfolio and the share of household income spent on food are bounded data of interest in economics. Psychologists analyze confidence ratings and bounded scores from cognitive tests administered to study subjects. Examples of bounded data in the ==== include prevalence rates and death rates of the coronavirus disease 2019 (COVID-19), and body fat percentages of athletes. Different from unbounded data, central tendency measures, skewness, and other features of the underlying distribution for bounded data are inextricable from the support of the distribution. Consequently, more caution is necessary when drawing inference for these features based on bounded data, especially when the support is unknown.====Existing approaches for analyzing bounded data typically assume a prefixed support such as ====, sometimes after scaling the raw data. The beta mean regression model proposed by ====, ====, ====, and ====. The model has also been extended to allow the precision parameter to vary with covariates (====; ====). An R package ==== (====; ====) is available on CRAN for fitting the beta mean regression model with varying precision and performing model diagnostics. This R package also allows for fitting a finite mixture of beta regression models (==== include the Bayesian beta mean regression model (====), a beta rectangular regression model based on a mixture of a beta distribution and a uniform distribution (====), a mixed effects beta model (====), and a flexible beta model based on a special mixture of two beta distributions (====). Unlike all the above regression models which focus on inferring the conditional mean of a bounded response, ==== considered a beta mode regression model where the mode of the response is related to covariates through a link function.====), the job-search problem (====; ====), and the procurement-auction problem (====; ====; ====; ====). Existing methods for estimating the four-parameter beta distribution include the moment-based estimation (====; ====), the maximum likelihood estimation when both shape parameters are greater than two (====), and the penalized likelihood approach (====), among others. The penalized likelihood approach by ==== is applicable without restricting the shape parameters to be above one or two, but standard error estimation for estimators of the four parameters are not provided.====These existing works on four-parameter beta distributions are not in a regression context. In fact, we can find little research on the four-parameter beta distribution in a regression setting. In this article, we present a class of Bayesian regression models that permit an inference for the support boundaries by considering the four-parameter beta distribution supported on ====) is provided for gross assessment of the model fit. Furthermore, all methods developed in the paper can be easily implemented in a freely-available R package, ====, calling complied C++. The ready availability of software allows researchers to empirically compare various competing beta regression models on their own data with a continuous bounded response.====The remaining of the article is organized as follows. Section ==== describes the four-parameter beta regression models, including prior development and posterior inference. We consider in Section ==== presents simulations to illustrate the quality of inference results when comparing to relevant existing methods. Section ==== where we summarize the contributions of our study and discuss future research directions.====The following is the Supplementary material related to this article.",Bayesian beta regression for bounded responses with unknown supports,https://www.sciencedirect.com/science/article/pii/S0167947321001791,10 September 2021,2021,Research Article,175.0
"Heuchenne Cédric,Jacquemain Alexandre","HEC Liège, University of Liège, Rue Louvrex 14, Liège, B-4000, Belgium,Université catholique de Louvain, ISBA, Voie du Roman Pays 20, Louvain-la-Neuve, B-1348, Belgium","Received 21 December 2020, Revised 15 July 2021, Accepted 1 September 2021, Available online 9 September 2021, Version of Record 16 September 2021.",https://doi.org/10.1016/j.csda.2021.107347,Cited by (1),"The Lorenz regression procedure quantifies the inequality of a response explained by a set of covariates. Formally, it gives a weight to each covariate to maximize the concentration index between the response and a weighted average of the covariates. The obtained index is called the explained Gini coefficient. Unlike methods based on decompositions of inequality measures, the procedure does not assume a linear relationship between the response and the covariates. Inference can be performed by noticing a similarity with the monotone rank estimator, introduced in the context of the single-index model. A continuity correction is presented in the presence of discrete covariates. The Lorenz-==== is a goodness-of-fit measure evaluating the proportion of explained inequality and is used to build a test of joint significance of several covariates. Monte-Carlo simulations and a real-data example are presented.","In the eighth of his ====, ==== expresses the idea that economics has methodologically shifted from a concern with representative agents and averages to a concern with heterogeneity and inequality. This academic focus is paralleled with rising voices among social observers concerning increasing inequalities all over the world. In this context, it is of prime importance to understand what factors can explain the inequality pattern observed in, say, an income or wealth distribution. Practically speaking, we have in mind a microeconomist facing a cross-sectional dataset, with information on incomes, along with many other variables. She would like to determine to what extent income inequality is attributable to, say, gender, age or education.====Available tools to address such a challenge are mainly decomposition methods, where the observed income inequality is divided into the contributions of each of the explanatory variables. However, these procedures require to assume a linear model between income and the explanatory variables. To bypass this restrictive assumption, we set the problem differently. We propose the Lorenz regression, in which each explanatory variable is given a weight in order to maximize a measure of explained inequality. On a more statistical aspect, we show that the obtained optimization programme consists in a special case of the monotone rank estimator developed by ==== in the context of single-index models. This link will enable us to use existing results concerning this estimator to perform inference on the weight vector mentioned above.====In this paper, we view inequality as a statistical measure of relative dispersion of a random variable, where relative means that it is independent of the scale of the variable. It is interesting to see that a similar relative dispersion exists in risk analysis in finance, see for example ====. Consequently, while we build interpretations in a context of inequality measurement, these could easily be translated to the risk of a financial asset. We close this introduction by a brief review of the existing literature.",Inference for monotone single-index conditional means: A Lorenz regression approach,https://www.sciencedirect.com/science/article/pii/S016794732100181X,9 September 2021,2021,Research Article,176.0
"Xing Xin,Xie Rui,Zhong Wenxuan","Department of Statistics, Virginia Tech, Blacksburg, VA 24060, USA,Statistics and Data Science, University of Central Florida, Orlando, FL 32826, USA,Department of Statistics, University of Georgia, Athens, GA 30602, USA","Received 21 December 2020, Revised 21 August 2021, Accepted 22 August 2021, Available online 9 September 2021, Version of Record 15 September 2021.",https://doi.org/10.1016/j.csda.2021.107336,Cited by (1)," level and how to adapt the estimation method to large-scale studies are also addressed. A fast EM algorithm is proposed for estimation, and its superior performance is demonstrated in simulation and multiple real applications such as image denoising, brain connectivity study, and spatial transcriptomic imaging.","Sparse coding aims at decomposing an ==== interpretable vectors, a collection of which is also referred to as a dictionary. Each vector in a dictionary is referred to as an atom (====; ====). Compared to the wavelet or kernel estimation methods which use predefined basis functions (====; ====; ====; ====).====, ====, and optimize the empirical loss function==== where ==== is the dictionary, ==== are the corresponding coefficients for the ====th signal, and ",Model-based sparse coding beyond Gaussian independent model,https://www.sciencedirect.com/science/article/pii/S0167947321001705,9 September 2021,2021,Research Article,177.0
"Shin Yei Eun,Zhou Lan,Ding Yu","Biostatistics Branch, Division of Cancer Epidemiology and Genetics, National Cancer Institute, USA,Department of Statistics, Texas A&M University, USA,Department of Industrial and Systems Engineering, Texas A&M University, USA","Received 12 July 2020, Revised 26 August 2021, Accepted 29 August 2021, Available online 9 September 2021, Version of Record 15 September 2021.",https://doi.org/10.1016/j.csda.2021.107343,Cited by (4),"A functional data approach is developed to jointly estimate a collection of monotone curves that are irregularly and possibly sparsely observed with noise. In this approach, the unconstrained relative curvature curves instead of the monotone-constrained functions are directly modeled. Functional principal components are used to describe the major ==== of curves and allow borrowing strength across curves for improved estimation. A two-step approach and an integrated approach are considered for model fitting. The simulation study shows that the integrated approach is more efficient than separate curve estimation and the two-step approach. The integrated approach also provides more interpretable principle component functions in an application of estimating weekly wind ==== of a ====.",", ====, ====, and ====, ==== and ====.==== proposed to estimate the so-called relative curvature curve instead of a monotone curve directly. His approach has an advantage of converting the problem of estimating a constrained function into that of estimating an unconstrained function. Nevertheless, that approach is designed to estimate a single monotone curve while we aim to estimate a collection of monotone curves sharing similar shapes. In particular, we would like to borrow strength across different curves during estimation. For example, when a curve is only partially-observed as in ====(b), details of the curve when reaching the rated power output at high wind speed would be hardly recognized if the curve is estimated alone.====We thus aim at the joint estimation of a collection of monotone curves, rather than the one-by-one estimation. To that end, we make use of the concept of functional principal component analysis (fPCA), which is broadly used to represent multiple curves by a few key functions: a mean function and several leading principal component functions. The individual characteristics of each curve can also be preserved through principal component scores. By doing so, an incompletely observed curve such as ====(b) can borrow the information from entire data; therefore, its estimated curve would have a reasonable shape as other curves.====The traditional approach of fPCA is defined similarly as the classical multivariate principal component analysis (PCA) but merely a summation changes into an integration (====). See also ====, ====, ==== and ====. However, this traditional approach is limited to the case that all curves are completely observed at an equally-spaced grid. Even though one can project the data on a common grid and then apply the traditional approach, but it is not the best way to utilize the data. To overcome the drawbacks of traditional fPCA, ====, ====, ==== and ==== developed spline-based approaches for sparsely and irregularly sampled curves. On the other hand, ====, ==== and ==== suggested. While most fPCA techniques are developed for general curves without any shape constraints, we in this paper propose a fPCA model for monotone-constrained curves that has not been developed.====We study two approaches for the joint estimation of monotone curves via fPCA; we call them a ==== and an ==== approach, respectively. Both target the same structure in a functional model framework, however, the procedures for estimating unknown functions in the proposed model differ. The two-step approach simply performs existing two methods in a row; it first estimates a relative curvature for each monotone curve as suggested by ====, and then, the classical fPCA is applied to the estimated relative curvatures. On the other hand, the integrated approach is indeed a primary method we want to recommend, which estimates all of the spline coefficients in the proposed model simultaneously. Unlike the two-step approach, the integrated approach provides one unified algorithm.====The remainder of the paper is structured as follows. In Section ====, we review ==== to describe how to estimate a single monotone curve as well as its relative curvature and introduce the necessary background. In Section ====, we propose a fPCA model for a collection of monotone curves and develop two approaches to estimate model parameters. Section ==== presents a simulation study to compare the performances of several approaches: the Ramsay's approach, the two-step approach and the integrated approach. All approaches are applied to estimate power curves for a wind turbine in Section ====. The R code for producing the numerical results in the paper are available at ====.====We follow the computation of creating orthonormal basis function from ====. They provided details about the transformation from arbitrary basis functions to orthonormal. Here we explain briefly their techniques.",Joint estimation of monotone curves via functional principal component analysis,https://www.sciencedirect.com/science/article/pii/S0167947321001778,9 September 2021,2021,Research Article,178.0
"Wang Yijun,Wang Weiwei,Zhao Xiaobing","School of Statistics and Mathematics, Zhejiang Gongshang University, Hangzhou, Zhejiang Province, China,Collaborative Innovation Center of Statistical Data Engineering, Technology & Application, Zhejiang Gongshang University, Hangzhou, Zhejiang Province, China,School of Data Sciences, Zhejiang University of Finance and Economics, Hangzhou, Zhejiang Province, China","Received 28 November 2020, Revised 22 August 2021, Accepted 28 August 2021, Available online 9 September 2021, Version of Record 14 September 2021.",https://doi.org/10.1016/j.csda.2021.107346,Cited by (1),"Panel count data have been extensively discussed in the literature. In general, the existing approaches in modeling panel count data usually assume an ==== for the dependence of the conditional mean function on ==== are derived under some mild conditions. Some numerical simulations and an application of bladder cancer are carried out to confirm and assess the performance of the proposed model and approach."," event data (the exact occurrence times of the events can be acquired). The statistical analysis of such data has attracted considerable attention and various methods have been proposed.====; ====; ==== developed a joint model in which two latent frailty parameters were adopted to account for the correlations and proposed a three-step estimation approach. ==== proposed a robust joint model with an unspecified correlation link function. ====; ==== investigated univariate and multivariate panel count data with correlated observations by using semiparametric transformation models respectively. ==== proposed semiparametric transformation models with correlated observation times and follow-up times. ==== developed a robust estimation method for the semiparametric transformation models. ==== gave more comprehensive introductions of panel count data.==== employed polynomial splines to estimate the proportional hazards regression models with a flexible relative risk form. However, no related studies have been conducted on panel count data. Therefore, it is interesting to discuss the statistical analysis of the panel count data model with an unknown link function.====This paper proposes a two-step ====The rest of the paper is arranged as follows. In Section ====, the model is specified. In Section ====. Section ==== reports some numerical simulation results and an application of the bladder cancer data is illustrated in Section ====. Some concluding remarks are presented in Section ====.====Similar as ==== and ====, a simple lemma is stated for the first. The proof of the lemma is similar to that of ==== and ====, thus we omitted the proof here. ",Local logarithm partial likelihood estimation of panel count data model with an unknown link function,https://www.sciencedirect.com/science/article/pii/S0167947321001808,9 September 2021,2021,Research Article,179.0
Kim Kyongwon,"Department of Statistics, Ewha Womans University, 52, Ewhayeodae-gil, Seodaemun-gu, Seoul 03760, Republic of Korea","Received 9 March 2021, Revised 28 July 2021, Accepted 28 August 2021, Available online 8 September 2021, Version of Record 15 September 2021.",https://doi.org/10.1016/j.csda.2021.107344,Cited by (1),"The principal ==== pairs of variables in the network to evaluate ====. In the numerical experiment, our methods have competitive accuracy in both low and high-dimensional settings. Our methods are applied to the DREAM 4 challenge gene network dataset and they work well in high dimensional settings with a limited number of observations."," investigated a cancer genetic network by using one of the most famous graphical models called the Gaussian graphical model (GGM) based on RNA sequencing expression data. For more details about the graphical models in gene network biology, see ==== and ====. Furthermore, ==== and ==== presented a difference of brain network structure between ADHD and non-ADHD children based on the functional MRI dataset. ==== further developed a graphical model to analyze brain network characteristics of alcoholism based on the electroencephalography dataset. In social science, ==== applied a graphical model to infer the networks of the 109th U.S. Senate based on the voting record for three types of bills such as defense and national security, environment and energy, and healthcare. ==== also used a probabilistic graphical model to investigate the network information between social brands reputation and users.====We propose a new nonparametric and non–Gaussian statistical graphical model by combining methods of SDR to the nonparametric graphical models. A graph consists of a finite set of nodes ==== and set of undirected edges==== for convenience. The main purpose of a statistical graphical model is to estimate ==== among ==== possibilities.====Let ==== be a ====-dimensional random vector with each component corresponding to a node. The meaning of the statement ‘node ==== is not directly linked to node ====’ is that if one wants to go from node ==== to node ====, one must go through the rest of the nodes –that is, nodes in ====. This corresponds to the following Markovian-type probabilistic statement==== where ==== denotes ==== with its ====-th and ====-th components removed. For a comprehensive description of graphical models, see ====.==== can be explained as we cannot go from the first node to the second node without passing through the third node. It can further be interpreted that there is no direct edge between the first and second nodes. Because of this intuitive structure and interpretability, numerous methods for GGM have been developed, such as ====, ====, ====, and ====.==== such that ====. This can be seen as a semiparametric problem with the precision matrix as the parametric component and the copula transformations ==== as the nonparametric component. See ====, ====, and ====. This approach can be extended further to a class of ==== models (====). Another solution to refrain from the Gaussian assumption is using a fully nonparametric approach such as ==== and ====. Moreover, ==== introduced new statistical relation called additive conditional independence by using semigraphoid axioms. ==== and ====Recently, ==== introduced the sufficient graphical model (SGM) and presented the conjoined conditional covariance operator (CCCO) by applying nonlinear SDR (====) to a statistical graphical model.====The rest of the paper is organized as follows. In Section ====, we develop the basic settings for connecting SDR to statistical graphical models. In Section ====, we review two linear SDR methods. In Section ====, we introduce the principal graphical models with SIR and SAVE by incorporating the principal predictors from SIR and SAVE to CCCO. In Section ====, we develop methods for choosing tuning parameters and introduce comprehensive algorithms for the sample level estimators. In Section ====, we conduct simulation studies to compare the results of our methods with the existing methods. In Section ====, we apply our methods to the DREAM4 challenge gene network dataset. Section ==== concludes the paper with discussion. Some additional simulation studies are provided in the Supplementary material.====The following is the Supplementary material related to this article.",On principal graphical models with application to gene network,https://www.sciencedirect.com/science/article/pii/S016794732100178X,8 September 2021,2021,Research Article,180.0
"Ghosal Rahul,Ghosh Sujit K.","Department of Biostatistics, Johns Hopkins University, USA,Department of Statistics, North Carolina State University, Raleigh, NC, USA","Received 17 December 2020, Revised 16 August 2021, Accepted 16 August 2021, Available online 25 August 2021, Version of Record 1 September 2021.",https://doi.org/10.1016/j.csda.2021.107335,Cited by (4),Bayesian statistical inference for ,"The imposition of constraints in statistical estimation methods is often of practical interest and finds its application in several problems such as one-sided testing, order constrained or isotonic regression, monotone curve estimation, among many others (see ====; ====; ====), astronomical imaging (====). A detailed account for such estimation methods with constraints on parameter space can be found in ==== and ====; ====; ====; ====, ==== and ==== developed Bayesian methods for inference in linear models subject to linear inequality constraints. As a special case of general inequality constraints, inference for order constrained parameters has been studied by ====; ====; ==== among many others. However, many of the earlier work within the Bayesian framework are based on set specific inequalities (e.g., ordered restricted parameters) or equalities (e.g., sum to unity etc.) which limits the scope of a more general set of constraints that we consider in this article.====In this article, our contributions are three-fold: (i) we provide a unified framework for the construction of prior distribution supported on a subspace specified by an arbitrary number of linear equalities and inequalities within a linear model framework (also applicable in high-dimensional cases of ====), especially for the cases that involve equality constraint and/or when number of inequality constraints are larger than the dimension, e.g., when ==== truncated in the trapezoid specified by constraints ==== and ====.====, ====, ==== and ====). In particular, ==== used standard diffuse reference prior restricted to constrained parameter space. We propose to use a generalized truncated multivariate normal distribution (TMVN) prior (====; ====) for posterior inference in the inequality constrained GLM following a data augmentation approach (====The rest of the article is organized as follows. In Section ====, we use several simulated data studies to compare the performance of the proposed method to other available methods. In Section ====, the applications on real data are presented. We conclude with a discussion about some possible extensions of our work in Section ====.====The following is the Supplementary material related to this article.",Bayesian inference for generalized linear model with linear inequality constraints,https://www.sciencedirect.com/science/article/pii/S0167947321001699,25 August 2021,2021,Research Article,181.0
Kelter Riko,"Department of Mathematics, University of Siegen, Walter-Flex-Street 3, 57072 Siegen, Germany","Received 1 October 2020, Revised 26 July 2021, Accepted 26 July 2021, Available online 3 August 2021, Version of Record 13 August 2021.",https://doi.org/10.1016/j.csda.2021.107326,Cited by (5),"Hypothesis testing is a central statistical method in the biomedical sciences. The ongoing debate about the concept of statistical significance and the reliability of null hypothesis significance tests (NHST) and p-values has brought the advent of various Bayesian hypothesis tests as possible alternatives, which often employ the Bayes factor. However, careful calibration of the prior parameters is necessary for the type I error rates or power of these alternatives to be any better. Also, the availability of various Bayesian tests for the same statistical problem leads to the question which test to choose based on which criteria. Recently proposed Bayesian nonparametric two-sample tests are analyzed with regard to their type I error rates and power to detect an effect. Results show that approaches vary substantially in their ability to control the type I and II errors, and it is shown how to select the prior parameters to balance power and type I error control. This allows for prior elicitation and power analyses based on objective criteria like type I and II error rates even when conducting a Bayesian nonparametric two-sample test. Also, it is shown that existing nonparametric Bayesian two-sample tests are adequate only to test for location-shifts. Together, the results provide guidance how to perform a nonparametric Bayesian two-sample test while simultaneously improving the reliability of research.","The practice of null hypothesis significance tests (NHST) which use ====-values has been debated widely in recent times. The discussion about the concept of statistical significance goes on (====” era (====), there are no widely accepted solutions by now. Most problems of the widespread practice of declaring statistical significance of a result as the necessary requirement for a relevant scientific discovery are far from being new. However, as established research methodologies are not easy to overcome little has changed since the ASA statement about the context and purpose of ====-values (====; ====). On the other hand, multiple proposals have been made how to improve the overall situation (====), which range from simple options like applying stricter standards for declaring statistical significance (====) to more serious methodological shifts (====; ====; ====; ====). In the biomedical and cognitive sciences, Bayesian hypothesis tests are often proposed to improve the reproducibility of research (====; ====; ====; ====, ====, ====, ====, ====; ====).====-values remain the gold standard in the biomedical sciences (====).====; ====; ====), or the study is underpowered (====).====The more widespread use of Bayesian hypothesis tests, on the other hand, has been prevented by several practical problems, like prior elicitation, power analysis, type I error control and the availability of various indices for the significance and size of an effect (====; ====, ====). Still, considerable progress has been made in the last years on developing Bayesian hypothesis tests as counterparts to widely used frequentist nonparametric two-sample tests like the Mann-Whitney U test. One reason for this trend is that Bayesian hypothesis tests have various merits.====First, they adhere to the likelihood principle (====), which leads to several desirable consequences for practical data analysis and has led many researchers to advocate Bayesian data analysis instead of NHST and ====-values (====, ====, ====; ====).====However, Bayesian data analysis itself is complicated by suffering from several problems, which are highlighted by the following example from the biomedical sciences.",Power analysis and type I and type II error rates of Bayesian nonparametric two-sample tests for location-shifts based on the Bayes factor under Cauchy priors,https://www.sciencedirect.com/science/article/pii/S0167947321001602,3 August 2021,2021,Research Article,182.0
"Li Muyi,Zhang Yanfen","Wang Yanan Institute for Studies in Economics (WISE), Xiamen University, China,Department of Statistics and Data Science, School of Economics, Xiamen University, China,Key Laboratory of Econometrics (Xiamen University), Ministry of Education, China","Received 15 November 2020, Revised 7 July 2021, Accepted 8 July 2021, Available online 27 July 2021, Version of Record 2 August 2021.",https://doi.org/10.1016/j.csda.2021.107321,Cited by (3),"This article discusses diagnostic checking for vector ==== with uncorrelated but not independent innovations. In this situation, the multivariate portmanteau tests are severely over-sized due to the misspecification of critical values obtained from the ","Since the seminal work by ==== (hereafter BP) and ==== (hereafter LB) for univariate white noise checking, and ====, ====, ==== for their multivariate counterpart. The test statistics are defined respectively by==== and==== where ==== is the sample size, ==== is a fixed positive integer, ====, Tr==== is the transpose of a matrix ====. When ==== tends to infinity, ==== and ==== is not large enough, the LB test usually has better finite-sample performance than the BP test. Therefore, in the remaining of this article, we focus our discussion on ====.====When the innovations are independent and identically distributed (i.i.d.), the asymptotic distribution of ==== is well approximated by the ==== distribution with ====. However, the independence assumption has been revealed too restrictive since it precludes many forms of nonlinear dependence, see e.g. ====, ====, ==== and ====. Therefore, it is more reasonable to weaken the independent assumption on errors for covering more general dependence. However, in this situation, the BP and LB portmanteau tests should be dealt with more caution, since their asymptotic distribution may change a lot.==== distribution under the null hypothesis (see ==== for the multivariate case). The second is to derive the correct asymptotic distribution with the test statistics unchanged, which is followed by ==== and ==== for diagnostic checking on weak ARMA and weak VAR models, respectively. This has been also extended by ==== for goodness-of-fit test on structural VARMA models with uncorrelated but nonindependent errors.====When the error sequence is uncorrelated but not independent, ==== derived the correct asymptotic covariance matrix of residuals autocovariance (denoted by ==== should be a sum of weighted independent ====. However, a practical disadvantage is that it is very complicated to estimate the matrix ====. ====, ==== and ====In fact, the RW bootstrap is a variant of the wild bootstrap of ==== and was initially applied by ====, ==== and ====. In this article, we explore the (B)RW bootstrap to obtain a consistent estimator of ====The rest of the paper is organized as follows. Section ==== recalls multivariate portmanteau tests in ==== where the error term is relaxed to be a sequence of multivariate uncorrelated random vectors. Section ====. An illustrative application on real data is presented in Section ====. Section ==== concludes. All technical proofs are relegated to the appendix.====In this appendix, we give the proof of ====, ==== in the paper. Follow the notation in ==== and ====, we denote ==== as the expectation conditional on ==== and ==== (====) as a sequence of random vectors converging to zero (bounded) in probability conditional on ====. We sketch the proofs here: (i) referring to Theorem 1 in ====, we show the consistency of ==== and asymptotic normality of ==== in probability; (ii) extending Theorems 2.2 and 6.1 in ==== to multivariate cases, we achieve ====, ==== in this paper.====We first derive the asymptotic property of ====. Before that, we recall the notations:==== ==== By the definition,==== where",Bootstrapping multivariate portmanteau tests for vector autoregressive models with weak assumptions on errors,https://www.sciencedirect.com/science/article/pii/S0167947321001559,27 July 2021,2021,Research Article,183.0
"Jia Yichen,Jeong Jong-Hyeon","Department of Biostatistics, University of Pittsburgh, United States of America","Received 15 September 2020, Revised 13 July 2021, Accepted 15 July 2021, Available online 27 July 2021, Version of Record 29 July 2021.",https://doi.org/10.1016/j.csda.2021.107323,Cited by (15),.,"; ====; ====), loss function for predicting patient survival (====; ====; ====). ====; ====; ==== the quantile is generally defined as==== In practice, however, investigators would be more interested in associations between the quantiles of time-to-event distributions and potential predictors in a regression setting. Quantile regression, originally proposed by ====; ====; ====; ==== has developed an ==== package ====, and it is available on GitHub at ====.====In Section ====, we review the existing work on censored quantile regression. In Section ====, we present the explanation and implementation of our deep censored quantile regression. The proposed algorithm is assessed via simulation studies in Section ==== and illustrated with two breast cancer data sets in Section ====. Finally, we conclude our paper with a brief discussion in Section ====.====The following is the Supplementary material related to this article.",Deep learning for quantile regression under right censoring: DeepQuantreg,https://www.sciencedirect.com/science/article/pii/S0167947321001572,27 July 2021,2021,Research Article,184.0
"Kim Ahhyoun,Kim Hyunjoong","College of Business, Korea Advanced Institute of Science and Technology, South Korea,Department of Applied Statistics, Yonsei University, Seoul 03722, South Korea","Received 28 August 2020, Revised 12 July 2021, Accepted 17 July 2021, Available online 27 July 2021, Version of Record 30 July 2021.",https://doi.org/10.1016/j.csda.2021.107324,Cited by (3),"A new classification tree algorithm is presented. It has a novel variable ==== that can effectively detect interactions. The algorithm uses a look-ahead approach that considers not only the significance at the current node, but also the significance at child nodes to detect the interaction. It is also different from other classification tree methods in that it finds the splitting point using the odds ratio. To evaluate the ==== of the newly proposed tree algorithm, an empirical study of 27 real or artificial data sets is performed. As a result of the experiment, the proposed algorithm shows at least similar or significantly better performance than the well-known and successful ==== methods: Ctree, CART and CRUISE.","Classification trees have been positioned as a key technique in classification problem due to their unique and easy-to-interpret visualization of the fitted model. They are obtained by recursively partitioning the data space and fitting a simple prediction model within each partition (====). Each partitioning can be represented by a node in a tree (====), CART (Classification And Regression Trees; ====), C4.5 (====), FACT (Fast Algorithm for Classification Tree; ====), QUEST (Quick, Unbiased, Efficient, Statistical Tree; ====), CRUISE (Classification Rule with Unbiase Interaction Selection and Estimation; ====, ====), GUIDE (Generalized, Unbiased, Interaction Detection and Estimation; ====) and Ctree (Conditional Inference Trees; ====) are the ones which have received attentions.====; ====; ====). To avoid this problem, FACT, QUEST, CRUISE, and GUIDE use a two-step approach to separate the variable selection procedure from the split point selection procedure.====This article proposes a new classification tree algorithm, COZMOS (Chi-square and OZ ratio based Model for Optimal Splits), based on the idea of the two-step approach. It is a descendent of CRUISE, amplifying the strengths of CRUISE with several improvements. COZMOS adopts the general approach of variable selection in CRUISE, but is specialized in choosing the right variable for a given node when a pair of interacting variables is chosen as the most significant. It can be seen that the modified variable ==== is almost unbiased, powerful and, above all, intelligent in that it successfully discovers the intrinsic structure of the interacting variables. Additionally, COZMOS introduces a new measure of splitting criterion based on the odds ratio, and uses the exhaustive search method to find the best splitting point.====The rest of this article is organized as follows. Section ==== describes the brief details and some limitations of the original CRUISE method. Section ==== introduces COZMOS algorithm. To assess the performance, we compare the prediction accuracy of COZMOS with those of CART, Ctree and CRUISE using 27 real datasets. The experimental results are discussed in Section ====. Lastly, Section ==== concludes the study.",A new classification tree method with interaction detection capability,https://www.sciencedirect.com/science/article/pii/S0167947321001584,27 July 2021,2021,Research Article,185.0
"Park Jaewoo,Jin Ick Hoon,Schweinberger Michael","Department of Statistics and Data Science, Yonsei University, Republic of Korea,Department of Applied Statistics, Yonsei University, Republic of Korea,Department of Statistics, University of Missouri, Columbia, United States of America","Received 17 November 2019, Revised 19 May 2021, Accepted 17 July 2021, Available online 27 July 2021, Version of Record 30 July 2021.",https://doi.org/10.1016/j.csda.2021.107325,Cited by (5),-penalized nodewise logistic regressions.,"; ====; ====), spatial statistics (====; ====; ====; ====; ====; ====; ====; ====; ====; ====; ====A specific example is the branch of psychometrics concerned with network-based approaches to learning from educational assessment data (====; ====; ====; ====; ====; ====). For example, ====; ====; ====; ====), with a view to studying interactions among binary item responses in educational assessments. To estimate high-dimensional Ising models from educational assessment data, ==== followed the approach of ==== based ====-penalized nodewise logistic regressions can recover the true interaction structure with high probability, provided that certain assumptions are satisfied (see, e.g., ====, Theorem 1, p. 1295). Those assumptions include restricted eigenvalue and irrepresentability assumptions (see assumptions (A1) and (A2) of ====, p. 1294), which restrict the amount of dependence among relevant predictors and the amount of dependence between relevant and irrelevant predictors (see also ====; ====; ====; ====; ====). In practice, such assumptions are hard, if not impossible to verify and may be violated. In addition, quantifying the uncertainty about the estimated interaction structure and parameter estimators is challenging.====We propose an alternative approach to estimating Ising models based on Bayesian variable selection methods, which does not have such drawbacks although it does come with additional computational costs. We combine two approaches from the Bayesian literature on doubly-intractable posterior distributions: (1) the double Metropolis-Hastings algorithm (====) and (2) the stochastic search variable selection method (====) using spike and slab priors (====). We demonstrate the advantages of the proposed Bayesian approach compared with ====-penalized nodewise logistic regressions.====The remainder of our paper is organized as follows. In Section ====, we describe Ising models along with computational and statistical challenges arising from Ising models, and existing approaches for addressing them. In Section ====, we propose a Bayesian algorithm for estimating Ising models. In Section ====, we assess the performance of the method by using simulated data. In Section ====, we apply the Bayesian approach to three educational data sets.====The following is the Supplementary material related to this article.","Bayesian model selection for high-dimensional Ising models, with applications to educational data",https://www.sciencedirect.com/science/article/pii/S0167947321001596,27 July 2021,2021,Research Article,186.0
"Li Mengyan,Ma Yanyuan,Zhao Jiwei","Department of Mathematical Sciences, Bentley University, United States of America,Department of Statistics, The Pennsylvania State University, United States of America,Department of Biostatistics and Medical Informatics, University of Wisconsin-Madison, United States of America","Received 6 January 2021, Revised 11 July 2021, Accepted 12 July 2021, Available online 21 July 2021, Version of Record 9 June 2022.",https://doi.org/10.1016/j.csda.2021.107322,Cited by (2),Consider the regression setting where the response variable is subject to missing data and the ,") is crucial in statistical analysis with missing data. The propensity score model, i.e., the distribution of the missingness indicator conditional on all variables, is called ignorable if it does not depend on the missing values. Otherwise, it is called nonignorable, also known as missing not at random (====). Extensive literature exists on ignorable missing data (====; ====; ====; ====; ====; ====; ====), while nonignorable missingness is more challenging with less research on it.====The assumption on ignorable missingness can be violated in some applications. Specifically, in the ==== health record (EHR) data example that motivated our work, data are collected in a non-prescheduled fashion. Thus, data are only available when a patient seeks care or a physician orders care, therefore the visiting process is potentially reflective of a patient's risk category. In other words, it is most likely that the propensity score will depend on the missing value itself. In such situation, missing value dependent propensity score models are needed for handling nonignorable missingness.====Many earlier literatures model the propensity score parametrically, see ====; ====; ====; ====; ====; ====; ====; ====; ====; ====; ==== and ====). Second, because the relationship between the variable subject to missingness and the missingness indicator is far more difficult to grasp than the relationship between fully observed variables and the missingness indicator, it is likely more sensible to allow more flexibility and consider a complement modeling strategy. In other words, the missingness indicator nonparametrically depends on the variable subject to missing, while parametrically depends on the fully observed variables. This is the propensity model we propose in this work.====Regardless the propensity is modeled parametrically, nonparametrically or semiparametrically, nonignorable missing data problems encounter a universal identification issue. It is known (====; ====; ====) and the shadow variable approach (====; ====; ====; ====). Here, we adopt the shadow variable approach, where we assume the missingness depends on the variable subject to missingness and only part of the fully observed variables. The part that has no involvement in the propensity score model is termed shadow variable.==== procedure. We show that the proposed estimator is semiparametrically efficient.====In Section ====, we present our model, and devise estimators for the parameters of interest in the parametric parts by deriving efficient scores. Details of implementation and algorithm are given in Section ====. In Section ====, we examine the finite-sample performance of our method through simulation studies. The application of our method to the motivating data is presented in Section ====. The paper is concluded with some discussions in Section ====.",Efficient estimation in a partially specified nonignorable propensity score model,https://www.sciencedirect.com/science/article/pii/S0167947321001560,21 July 2021,2021,Research Article,187.0
"Zhao Yuna,Lin Dennis K.J.,Liu Min-Qian","School of Mathematics and Statistics, Shandong Normal University, Jinan 250358, China,Department of Statistics, Purdue University, West Lafayette, IN 47907, USA,School of Statistics and Data Science, LPMC & KLMDASR, Nankai University, Tianjin 300071, China","Received 26 January 2021, Revised 21 June 2021, Accepted 3 July 2021, Available online 15 July 2021, Version of Record 22 July 2021.",https://doi.org/10.1016/j.csda.2021.107320,Cited by (7)," (OofA-OAs), is proposed. It is shown that OofA-OAs are superior over any other type of fractional OofA designs for the predominant pair-wise ordering (PWO) model. The balance property of OofA-OAs is also developed. In addition, the capacity of OofA-OAs for estimating different models is investigated.","The order-of-addition (OofA) experiment aims at determining the optimal order for processing components in the experiment, which is essential in many areas. For example, in agriculture, ==== stated that “The order of addition of the lead acetate (before or after the pH adjustment) had a definite influence on the reaction. Higher recoveries were obtained by adjusting the pH after lead acetate addition.” The order of addition of reagents is critical in polymerase chain reaction and the sequence of drug administration impacts clinical outcomes (for example, ====). In genomics, different orders of adding taxa into the computer program yield different likelihoods of the fitted tree (for example, ====; ====). More applications can be found in ==== and references therein.====The study on OofA problem has increasingly aroused the attention of researchers in academe. ==== proposed the pair-wise ordering (PWO) model (as will be introduced in Section ====) which assumes that the responses of different orders depend on the pair-wise orders of components. ==== extended the PWO model by taking account of the higher-order interactions between PWO factors. With a different modeling perspective, ==== developed the component-position (CP) model which assumes that a component has different effects when it is processed at different positions in an order, and ====Considering ==== components, a full OofA design contains ====! different orders. In practice, performing an experiment by a full OofA design is usually unaffordable even for a moderately large ==== (for example, ==== resulting in ====, the OofA-OAs are optimal under a variety of commonly used design criteria including ====-, ====- and ====.-criteria. Based upon computer search, ==== found a small number of OofA-OAs with 12 or 24 runs for ====. ==== provided a closed-form construction method for OofA-OAs in ==== runs, where ==== for an even ==== and ==== for an odd ====. By employing balanced incompleted block designs, ==== found some OofA-OAs. The methods in ==== and ==== are lack of flexibility in design run size, what's more, the run sizes of their OofA-OAs are quite large. For example when ====, the run sizes of the OofA-OAs in ==== and ==== are 840 and 168, respectively. ==== and ==== respectively constructed a class of fractional OofA designs, called component-orthogonal arrays. The component-orthogonal arrays are ====-optimal for the CP model but may not be estimable under the PWO model. It is desirable that a fractional OofA design can have good performance for different models. Under the PWO model, ==== explored the construction of minimal-point OofA designs, and ==== generated highly efficient OofA designs via threshold accepting.====In this paper, we propose a systematic construction method for OofA-OAs. Compared to the existing construction methods, the new method enjoys three advantages: (i) it works for any design run size, provided that the OofA-OA exists, (ii) given the run size and the number of components, it is capable of finding many non-equivalent OofA-OAs, and (iii) it is user-friendly due to its elegant mathematical formulation. We address an important unresolved issue in the literature, a ====-optimal fractional OofA design is indeed an OofA-OA. It is further proved that any optimal fractional OofA design (in terms of ====-, ====-, ====.- or ====-optimalities) for the PWO model, must be an OofA-OA. The balance property of the OofA-OAs is also investigated. It is shown that the OofA-OAs have a perfect balance property. For example, after removing any ==== components from an OofA-OA, the resulting design has the 6 (3!=6) different orders appearing equally often. It is demonstrated that OofA-OAs can provide considerable relative ====-efficiencies (compared to their corresponding full OofA designs) for alternative models (such as CP model).====The rest of the paper is organized as follows. Preliminaries are given in Section ====. The construction method of OofA-OAs is proposed in Section ====. Section ==== explores the balance property of OofA-OAs and proves that, under the PWO model, ====-, ====-, ====.- or ====-optimal fractional OofA designs are OofA-OAs. The performance of OofA-OAs for the CP model is discussed in Section ====. Concluding remarks are given in Section ====. The proofs and some useful design tables are deferred to Appendix.====To differentiate the columns in ==== and ====, in the following proofs, we use ==== to denote the column of ==== corresponding to ==== in ====.",Optimal designs for order-of-addition experiments,https://www.sciencedirect.com/science/article/pii/S0167947321001547,15 July 2021,2021,Research Article,188.0
"Zhang Huaiyu,Wang Haiyan","Freddie Mac, Mc Lean, VA, United States of America,Department of Statistics, Kansas State University, Manhattan, KS, United States of America","Received 22 November 2020, Revised 16 June 2021, Accepted 1 July 2021, Available online 12 July 2021, Version of Record 19 July 2021.",https://doi.org/10.1016/j.csda.2021.107318,Cited by (4), of the test statistic was derived under ====. The new test provides more consistent results on random samples of the dataset.,"; ====). Classical Hotelling's ==== test is the popular tool for low dimensional settings. When the dimension is larger than the total sample size, Hotelling's ====). ==== also proved that the performance of Hotelling's ==== test would deteriorate as the dimension approaches the total sample size.====: the max of the absolute values of univariate two-sample ====-statistics, the sum of univariate two-sample ====-statistics, and the sum of the absolute values of univariate two-sample ==== and ====, large or small, including the scenarios where Hotelling's ==== is not well defined. Besides the permutation method, many other methods can be classified into two representative branches. One of them focuses on the distance metric embedded in Hotelling's ====. The singularity of ==== is the root of the challenge in the high-dimensional setting. To remedy this issue, modified distances were introduced to avoid computing ====; ====), diagonalized Mahalanobis distance (====; ====; ====), modified squared distances by removing sum of squared terms (====) and a scale-invariant version (==== to the area of high-dimensional tests. Through a random projection matrix, the data points are projected to a low-dimensional space, where Hotelling's ==== is well defined. The implementation of these tests usually depends on an ensemble of a large number of repetitions to overcome the weak performance of the individual random projection test. Variations of the random projection approach can be found in ====; ====; ====.====. This motivated us to develop a new test.====In this paper, we present our new test and demonstrate its advantages, theoretical properties, and its performance on simulated and real data. Our new test statistic is constructed based upon ====, where ==== are the univariate two-sample Welch's ====-statistics for each component of the ==== dimensions. We further standardize ==== by an estimate of the unknown standard deviation ====. Noting that ====, our estimation of ==== builds on the estimates of its elements ====. More details of the new test will be presented in Section ====.====The new test has the following advantages compared to a related test, the generalized component test (GCT), proposed by ====:====We applied the new test and several competing tests to identify differentially expressed GO terms with the acute lymphoblastic leukemia (ALL) dataset (====). The two molecular types in comparison were BCR/ABL and NEG. The BCR/ABL is a molecular type that is formed by the fusion of two genes, known as breakpoint cluster region (BCR) and v-abl Abelson murine leukemia viral oncogene homolog 1 (ABL1). It shows up in some patients with certain types of leukemia. The NEG represents the cytogenetically normal NEG B-cell ALL. If the multivariate means of a GO term are not equal under the two molecular types, then the GO term is considered being differentially expressed. Lack of ground truth makes the evaluation challenging for this task, but it is close to the situation in a real scientific study. In our evaluation strategy, we randomly split each group into two subsets, and repeat this with various random seeds 100 times. Denote ==== as the set of subjects from the BCR/ABL group and ==== as that from the NEG group. In the ====-th (i = 1,...,100) round, let ==== and ==== be a random partition of ====, and ==== and ==== be a random partition of ====. In this ALL dataset, the BCR/ABL group has 37 subjects (====) and the NEG group has 42 subjects (====). To keep the balance of sub-samples, we set the sizes ====, ====, and ====. Comparing two subsets within a molecular type, for example, ==== versus ====, can serve as the setting under the null hypothesis, and the between-type comparison, for example, ==== versus ====, can serve as the alternative hypothesis setting. Repeating the random partitioning allows us to evaluate the type I error and statistical power. Moreover, the GO terms identified by comparing ==== versus ==== should largely overlap with those identified by comparing ==== versus ====. This “consistency” is one of the major aspects we assess for the competing methods in this application.====The rest of the paper starts with introducing the new test and its theoretical properties in Section ====. Section ====The following is the Supplementary material related to this article.",A more powerful test of equality of high-dimensional two-sample means,https://www.sciencedirect.com/science/article/pii/S0167947321001523,12 July 2021,2021,Research Article,201.0
"Narci Romain,Delattre Maud,Larédo Catherine,Vergu Elisabeta","MaIAGE, INRAE, Université Paris-Saclay, 78350 Jouy-en-Josas, France","Received 6 July 2020, Revised 30 June 2021, Accepted 2 July 2021, Available online 12 July 2021, Version of Record 14 July 2021.",https://doi.org/10.1016/j.csda.2021.107319,Cited by (3),"Despite the recent development of methods dealing with partially observed epidemic dynamics (unobserved model coordinates, discrete and noisy outbreak data), limitations remain in practice, mainly related to the quantity of augmented data and calibration of numerous tuning parameters. In particular, as coordinates of dynamic epidemic models are coupled, the presence of unobserved coordinates leads to a statistically difficult problem. The aim is to propose an easy-to-use and general inference method that is able to tackle these issues. First, using the properties of epidemics in large populations, a two-layer model is constructed. Via a diffusion-based approach, a ",", ==== package POMP (====). Among these, we cite maximum iterated filtering (MIF: ====, ====, ====In this paper, we consider a different approach to deal with the presence of missing coordinates, discrete observations, and reporting and measurement errors. Our goal is to propose a useful and coherent ==== that allows key epidemic parameters to be estimated from imperfect observations from outbreaks.====A multidimensional Markov jump process describes the epidemic dynamics in a closed population of size ====. Using the large population framework, i.e., with ====, ====The derivation and accuracy assessment of Gaussian process approximation for stochastic epidemic models have previously been described in ====, along with maximum likelihood inference for parameters underlying epidemic dynamics. However, that study does not rely on Kalman filtering, nor does it consider noise in outbreak data. Computation of the approximate likelihood of the associated statistical model, as well as parameter estimation, performed via Kalman filtering recursion was proposed in ====For the sake of simplicity, we consider here an epidemic with homogeneous mixing in a closed population whose dynamics are described by a compartmental model, with each compartment containing individuals with identical health states. We focus on the simple SIR (susceptible - infectious - recovered) epidemic model characterized by a two-dimensional jump process, partially observed at regularly-spaced discrete times, with measurement errors. The approach can be easily extended to broader epidemic models observed with various sampling intervals.====The paper is organized as follows. In Section ==== we introduce the general framework and related inference issues, and propose the model approximation. Section ==== contains the main methodological developments of our paper: construction of the approximate log-likelihood, its computation based on Kalman filtering recursion, and the associated parameter estimation. In Sections ==== and ==== we assess the performance of our estimators on both simulated data and real data from an influenza outbreak in a British boarding school in 1978, and compare our results with those obtained using the MIF method. Section ==== contains a discussion and concluding remarks.====The sampling interval Δ is important in our method and we distinguish between two cases: “Small Δ” and “Moderate Δ”. We give below the dependencies on quantities of interest with respect to Δ.====Taylor expansions with respect to ==== at point ==== yield==== The following additional approximations, which simplify the analytic expressions, can be used in the state space equation:==== ====Computing the approximate log-likelihood ==== with Kalman filtering techniques requires computing the resolvent matrix Φ of the ODE system ====. When the time intervals between observations are too large (i.e., Δ is too large), we use the following approximation for matrix exponentials:==== where ====. This can however significantly increase computation times.",Inference for partially observed epidemic dynamics guided by Kalman filtering techniques,https://www.sciencedirect.com/science/article/pii/S0167947321001535,12 July 2021,2021,Research Article,202.0
"Mao Shanjun,Fan Xiaodan,Hu Jie","College of Finance and Statistics, Hunan University, Changsha, China,Department of Statistics, The Chinese University of Hong Kong, Hong Kong SAR, China,Department of Probability and Statistics, School of Mathematical Science, Xiamen University, Xiamen, China","Received 29 July 2020, Revised 16 June 2021, Accepted 16 June 2021, Available online 5 July 2021, Version of Record 12 July 2021.",https://doi.org/10.1016/j.csda.2021.107307,Cited by (1),"Tree-shaped datasets have arisen in various research and industrial fields, such as ","), Kendall correlation (====) and mutual information (====). All these statistics require each paired observation to be an independent and identically distributed (i.i.d.) observation (====). Thus, these classical statistics are not applicable if the observations have complex dependency. More specifically, this paper is concerned with tree-shaped datasets.====In this paper, a principled statistical model for tree-shaped datasets is proposed. The proposed methods are based on the need for modelling tree-shaped datasets which are emerging from various areas. For example, the measurements of the protein level of a specific labelled gene along the embryonic development constitute a binary tree structure (====; ====; ==== tested the relationship between tree structures, but their relation is between the samples' distributions of leave nodes and their corresponding latent group structures captured by the tree, which is not about the tree correlation that this paper is interested in. Hence, a new model and algorithm for measuring the correlation for tree-shaped dataset is proposed.====Note that a tree with a single observation at each node or a tree where the last observation of a parent node is not equal to the first observations of its child nodes are a special case of the tree-shaped dataset as defined above. The transformation can be done by simply copying the last observation of a parent node as the first observations of all of its child nodes. With similar transformation, we can also see that a regular time series or even an i.i.d. dataset are also special cases of the tree-shaped dataset.====The trend similarity between two tree-shaped datasets of the same tree structure is cumulated from the synchronized increase or decrease at matched locations of the tree. The correlation strength of variables at matched locations may change with the depth of the tree. As an example, gene expression measured on cell lineage tree of one individual organism follows a tree structure (====), is used to draw samples from the posterior.====The remaining content of the paper is organized as follows: the models of tree-shaped data are proposed in Section ====; a Bayesian method is presented in Section ==== and Section ====, respectively; Section ==== concludes the paper.====The following is the Supplementary material related to this article.",Correlation for tree-shaped datasets and its Bayesian estimation,https://www.sciencedirect.com/science/article/pii/S0167947321001419,5 July 2021,2021,Research Article,203.0
"Pircalabelu Eugen,Artemiou Andreas","UCLouvain, Institute of Statistics, Biostatistics and Actuarial Sciences, Voie du Roman Pays 20, 1348 Louvain-la-Neuve, Belgium,Cardiff University, School of Mathematics, Senghennydd Road 69, CF24 4AG Cardiff, Wales, UK","Received 10 June 2020, Revised 4 February 2021, Accepted 4 June 2021, Available online 29 June 2021, Version of Record 12 July 2021.",https://doi.org/10.1016/j.csda.2021.107302,Cited by (1)," the number of variables in the model is much larger than the available sample size ====, (ii) ==== is much larger than the number of slices ==== the model uses and (iii) ==== the number of projection vectors can be larger than the number of slices ====. The methodology is developed for the case of the sliced inverse regression model, but extensions to other dimension reduction techniques such as sliced average variance estimation or other methods are straightforward."," with ==== components. The approaches that each methodological framework uses, are at first sight quite different from each other. The PGMs framework gives importance to the identification of (separator) nodes which, when conditioned upon, contain all the necessary information to make other groups of nodes conditionally independent of each other. The SDR framework gives importance to the identification of linear/nonlinear combinations of nodes which, when conditioned upon, retain all the necessary information to make groups of nodes conditionally independent of each other. In the SDR literature, the SIR model (====) is one of the most popular and used techniques to achieve dimension reduction. Even though throughout the years many other methods have been introduced, SIR still remains one of the popular choices and a clear competitor to benchmark against.====The manuscript is structured as follows. In Section ==== we briefly present sufficient dimension reduction techniques and the probabilistic graphical modeling framework. In Section ==== the proposed method and estimation aspects are presented, while theoretical properties of the method are investigated in Section ====. A simulation study is presented in Section ==== and a real case analysis is presented in Section ====. We finish with a discussion on the method in Section ====.",Graph informed sliced inverse regression,https://www.sciencedirect.com/science/article/pii/S0167947321001365,29 June 2021,2021,Research Article,204.0
"Gaucher Solenne,Klopp Olga,Robin Geneviève","Département de Mathématiques d'Orsay, Université Paris-Saclay, Bâtiment 307 Rue Michel Magat, Orsay, France,ESSEC Business School, 3 Avenue Bernard Hirsch, Cergy, France,CNRS, LaMME, Université d'Évry Val d'Essonne, 23 boulevard de France, Évry, France,CREST, ENSAE, 5 Avenue Le Chatelier, Palaiseau, France","Received 17 November 2020, Revised 18 June 2021, Accepted 21 June 2021, Available online 25 June 2021, Version of Record 29 June 2021.",https://doi.org/10.1016/j.csda.2021.107308,Cited by (6), and prediction of the missing links. The method is also illustrated with an application in epidemiology and with the analysis of a political Twitter network. The algorithm is freely available as an R package on the Comprehensive R Archive Network.,"Networks are powerful tools to analyse complex systems: agents are represented as nodes, and pairwise interactions between agents are recorded as edges between these nodes. Examples of fields of applications include biology, where networks may be used to describe protein-protein interactions; ecology, where they may represent food webs ==== or spatial distributions in crop diversity networks ====; ethnology, where networks summarise relationships or trades between individuals or communities ====; ====. Those real-life networks are often modelled as realisations of random graphs or, equivalently, as noisy versions of more structured networks. In this setting, recovering the “noiseless” version of the graph, i.e., estimating the underlying probabilities of interactions between agents, is a key problem that has recently gained considerable attention (see, e.g., ====; ====; ====; ====). Most of the proposed methods are based on models describing the connectivity of the majority of nodes. However, in many examples, those models fail to describe networks containing a small number of outlier nodes with abnormal behaviour. Following ====, we define an outlier as “an observation that deviates so much from other observations as to arouse suspicion that it was generated by a different mechanism”.====. These outliers often exhibit connection patterns that differ from that of normal nodes: the authors of ==== show, for example, that spam attackers are often connected with numerous nodes in a random fashion, thus forming characteristic hubs. By contrast, the connections between regular nodes are more sparse and more structured: they may, for example, exhibit community structures. Identifying those malicious nodes is crucial to protecting users from the threat they represent. In the context of graphs obtained from survey data, anomalous behaviour may indicate that participants are providing false answers to distort public opinion on a subject ====; ====. In other cases, defaults of measurement instruments or fraudulent behaviours can lead to abnormal connectivity patterns. Finally, in contact networks, individuals with anomalous connection patterns may play an important role in the propagation of diseases, and their identification finds important applications in epidemiology (====; ====.====In addition, many real-life networks are polluted by missing data ====; ====. Indeed, complete exploration of all pairwise interactions between agents can be expensive and time consuming and requires significant effort. In social sciences, graphs constructed from survey data are likely to be incomplete due to no response or drop-out of participants. Online social network data are often obtained through crawling of user profiles; however, the gigantic size of these networks may drive analysts to prematurely stop this crawling, and work with a subsample of the network (====). Protein-protein interaction networks provide a blatant example of incompleteness, as the existence of each interaction must be tested experimentally, and most of these interactions have yet to be tested ====. When dealing with a partially observed network, being able to predict the probability of existence of non-observed edges is of particular interest and finds numerous applications, for example in biology ==== and ecology ====.==== for a review of these techniques. For instance, many algorithms based on trust propagation rely on the assumption that outlier nodes are not well connected with normal nodes ====; ====. Other algorithms, based on community structure, assume that outliers ====; ==== fail to be well connected to communities of normal nodes. However, it has been shown in ====, the authors aim to recover community structures when the majority of the nodes follow an assortative stochastic block model in the presence of arbitrary outlier nodes. However, their algorithm does not allow detection of these outlier nodes. Note that our problem is different, as we would like to estimate connection probabilities between nodes rather than recover community structures, and our assumptions on the random graph are more general.====On the other hand, estimation in networks with missing observations, and its application to link prediction has known a quite recent development. In ====, the authors consider the setting where non-existing edges can be erroneously recorded as observed (or existing edges recorded as not observed), both errors occurring at a fixed rate. More recently, ==== and ==== proposed algorithms to estimate the edge probabilities under different missing observations schemes, and ==== proposed a method for consistent community detection under several missing value scenarios. Both papers present convincing numerical experiments but lack theoretical guarantees.====Finally, our work is also closely related to recent developments in the field of robust matrix completion. Indeed, in our general model presented in Section ====, we assume that the matrix of connection probabilities can be decomposed as the sum of a low rank component (connectivity pattern of inliers) and that of a column-wise sparse component (non-zero columns corresponding to outliers). Our problem is to estimate the low-rank matrix in order to reconstruct the connectivity of inliers and to ====; ====; ====; ====; ====; ====; ====; ====. More recently, the problem of robust matrix completion with binary observations has been studied in ====; ====; ====In the present work, we present a new algorithm to detect the outliers and estimate the connection probabilities of the remaining nodes, which is robust to missing observations. For this algorithm, we provide both statistical and computational guarantees. In particular, in ====, we prove that under fairly general assumptions our algorithm achieves exact detection of the outliers. In ====. We also analyse the algorithm convergence complexity in ==== and show sub-linear convergence. In Section ====, we provide a simulation study with comparisons to state-of-the-art techniques, indicating that the proposed method has good empirical properties in terms of outlier detection and link prediction. Finally, we illustrate the performance of our method with two applications in epidemiology and social network analysis.====The proofs are presented as follows. First, we recall in Section ==== some results that will be used in our proofs. In Appendix ====, we provide the details of the ====. Appendix ==== is devoted to the study of the convergence of our algorithm. ==== is proved in Appendix ====, ==== is proved in Appendix ====, while in Appendix ====, we prove ====. ==== is proved in Appendix ====. Auxiliary Lemmas used throughout these sections are proved in Appendix ====.====To ease notations, we denote henceforth by ==== and ==== the estimation errors of ==== and ====.",Outlier detection in networks with missing links,https://www.sciencedirect.com/science/article/pii/S0167947321001420,25 June 2021,2021,Research Article,205.0
"Choi Taehwa,Kim Arlene K.H.,Choi Sangbum","Department of Statistics, Korea University, Seoul 02841, South Korea","Received 9 November 2020, Revised 11 March 2021, Accepted 12 June 2021, Available online 24 June 2021, Version of Record 30 June 2021.",https://doi.org/10.1016/j.csda.2021.107306,Cited by (0),"Double censoring often occurs in biomedical research, such as HIV/AIDS clinical trials, when an outcome of interest is subject to both left censoring and right censoring. It can also be seen as a mixture of exact and current status data and has long been investigated by several authors for theoretical and practical purposes. In this article, we propose the Buckley-James method for an accelerated failure time model under double random censoring. For the semiparametric inference, where the error distribution of the censored linear model is left unspecified, we develop an efficient EM-based self-consistency procedure to estimate the ==== and the unknown residual distribution function. ====, including the uniform consistency and weak convergence, are established for the proposed estimators. Simulation studies demonstrate that the proposed procedure works well under various ==== and outperforms the inverse-probability weighting method in terms of accuracy and efficiency. The method is applied to the HIV/AIDS study.","). The limited quantification of the assay makes the observed viral RNA copies reliable only when they are within this range and otherwise they are censored. In another example, screening of mammograms has left censoring, because of earlier occurrence of breast tumor before the first screening, while right censoring can occur due to undetected tumor at the end-of-study (====; ====; ====; ====; ====), and the two-sample comparison problem (====; ==== and ==== and ==== employed a linear mixed-effect model to study the progression of HIV infection markers under left censoring. For the scenarios where both left- and right-censoring variables are always known, ====, ====, ==== and ==== developed a martingale-based estimating function for doubly-censored data. More recently, ==== and ==== studied a shared frailty model for analyzing clustered and doubly-censored data.====In this article, we focus on censored linear regression analysis of doubly-censored data. Specifically, we generalize the Buckley-James estimation procedure (====; ====; ====) and least-squares estimations (====; ====; ==== and ====In the light of the foregoing findings, we develop efficient and reliable inferential procedures for linear regression analysis of doubly-censored data by modifying the Buckley-James estimating equation to accommodate left censoring in addition to right-censoring. The Buckley-James estimator is a root of a discontinuous estimating function, which may have multiple roots even in the limit since the convergence of the algorithm is not guaranteed. Following ====, we linearize the Buckley-James estimating function and solves the least-squares normal equation iteratively until convergence. A key step in implementing the proposed method is computation of the nonparametric maximum likelihood estimator (NPMLE) of the error distribution function in the AFT model, which can be done by iteratively solving the self-consistency equation (====). However, the self-consistency equations may fail to characterize the NPMLE, since a self-consistency estimate is not necessarily the NPMLE (====; ====). Motivated by ==== and ====The remainder of this paper is organized as follows. In Section ====, we demonstrate that the proposed algorithm works well under various censoring scenarios and locate the solution more efficiently than the IPCW approach. In Section ====, we apply the method to the AIDS clinical data. Section ==== provides some discussion and concluding remarks. All technical details are relegated to the web-based supplemental material. The R code for implementing our method is available at ====.====The following is the Supplementary material related to this article.",Semiparametric least-squares regression with doubly-censored data,https://www.sciencedirect.com/science/article/pii/S0167947321001407,24 June 2021,2021,Research Article,206.0
"Singh Satya Prakash,Davidov Ori","Department of Mathematics, Indian Institute of Technology Hyderabad, Telangana 502285, India,Department of Statistics, University of Haifa, Mount Carmel, Haifa 3498838, Israel","Received 12 August 2019, Revised 5 November 2020, Accepted 12 June 2021, Available online 23 June 2021, Version of Record 28 June 2021.",https://doi.org/10.1016/j.csda.2021.107305,Cited by (2),In a recent paper ====. The proposed designs are compared numerically to some well known existing designs and it is shown that the new designs require fewer experimental units to attain prespecified power. A thorough sensitivity analysis shows that the proposed designs are robust against possible misspecification of the parameters under the alternative and the order relation among the treatment groups.,"There are a wide variety of practical problems arising in different application areas in which one expects an ordering among ====). There are, of course, many other orderings; we briefly mention the simple tree order which arises when comparing several treatments with a control (==== and ====) and the umbrella order which may be used as an alternative to the simple order if high doses, or treatment levels, are associated with reduced efficacy or response. Many other order relations have been studied.====Although statistical methods for analyzing data under order restrictions are well developed, e.g., ==== or ==== survey previous developments and a provide a comprehensive and principled approach to the design of experiments under order restrictions. Specifically they address the issue of optimal experimental design in the context of the one–way analysis of variance (ANOVA) model==== where ==== is the response of ==== observation in ==== treatment group, ==== and ==== and the errors ==== are independent ==== and ====. The first is,==== where under the null, ====, i.e., ==== is the linear space in which all means are equal, whereas ==== all of which can be succinctly expressed as ==== for some, order specific, matrix ==== with elements ====, referred to as the restriction matrix. Note that the null in ==== is rejected if at least one strict inequality holds under the alternative.====The second type of problem considered by ==== is the union–intersection testing problem (cf., ==== and ====) formalized as==== where ==== is a set of pairs of indices corresponding to the treatment groups which we wish to compare and the individual hypotheses are==== Thus in contrast with the testing problem ====, the null in ==== is rejected if and only if all inequalities hold under the alternative. For example, in the context of the simple order with ==== groups ==== tests that both ==== ==== ==== hold, whereas in ==== only one of these inequalities suffices to reject the null.==== derived approximate maxi–min (MM) designs for ==== and approximate intersection–union test (IUT) based designs for ====. In both cases the set of feasible ==== designs is the simplex ==== where ==== is the proportion of observations allocated to the ==== treatment. Thus, in general, finding an approximate design involves maximizing an objective function with continuous arguments. We note that obtaining the approximate design is computationally simple in the case of MM–designs, i.e., in ====, and more complicated for IUT–designs, i.e., in ====. In practice, however, sample sizes are finite, and therefore exact and not approximate, designs must be used. The set of feasible ==== designs is ==== where ==== is the number of observations allocated to treatment ==== and ==== is a given total sample size. To fix ideas consider the approximate optimal design for the two sample problem where ====. It is not hard to see that if ==== is even then ==== is an exact optimal design and if ==== are optimal exact designs. Unfortunately, this simplicity is lost when ====. In fact, it is well known (====, cf. Chapter 12 in ====.====This paper proposed a new method for finding ==== most efficient exact design for the testing problems ==== and ====. In other words, the proposed methodology yields an exact design which maximizes the power and is therefore referred to as the most efficient exact design (MEED). The MEED is found using a two step procedure. First, given an optimal approximate design ==== and a total sample size ==== a collection of candidate designs denoted by ==== is generated. The collection ==== is provably typically small, at least relative to ====, and guaranteed to include the MEED. Next, a direct search on ==== yields the MEED. The goals of this paper are then twofold: (====) furnish both applied statisticians and experimental scientist with the tools for finding the MEED so they can plan more powerful studies for addressing their research problems; ==== further demonstrate the utility and benefit of designing experiments which utilize order restrictions; and ==== show that the designs are robust against some misspecifications.====The paper is organized in the following way. In Section ==== testing procedures for the hypotheses in ==== and ==== are described along with the approximate design selection criteria studied by ====. In Section ==== methods for obtaining efficient exact designs are provided. In particular we study the exact design problem in two scenarios. In the first the sample size is fixed in advance, whereas in the second, we search for an exact design which ensures a pre–specified power. In Section ====, numerical results are provided and the robustness of proposed designs against possible misspecification of ==== and the specification of the order restrictions are demonstrated. In Section ==== an illustrative example is provided. The selection of the MEED together with the practical advantages of using a design tailored for order are discussed. Section ==== provides a brief summary and discussion.",On efficient exact experimental designs for ordered treatments,https://www.sciencedirect.com/science/article/pii/S0167947321001390,23 June 2021,2021,Research Article,207.0
"Xu Kai,Zhou Yeqing","School of Mathematics and Statistics, Anhui Normal University, China,School of Mathematical Sciences, Tongji University, China","Received 5 February 2021, Revised 2 June 2021, Accepted 3 June 2021, Available online 18 June 2021, Version of Record 12 July 2021.",https://doi.org/10.1016/j.csda.2021.107301,Cited by (0),"A projection-averaging-based cumulative divergence to characterize the conditional mean independence is proposed. As a natural extension of ====, the new metric has several appealing features. It ranges from zero to one, and equals zero if and only if the conditional mean independence holds. It has an elegant closed-form expression that involves no tuning parameters, making it easy to implement. The sample estimator of new metric is ====-consistent under the conditional mean independence and root-====-consistent otherwise. A goodness-of-fit test for single-index models based on the variant of the proposed metric is further introduced, which generalizes the projected-based test of ====. The effectiveness of our proposals is demonstrated through simulation examples and a real application.","), Blum-Kiefer-Rosenblatt's coefficient (====), distance correlation (====), maximal information criterion (====), projection correlation (====), and ball correlation (==== ==== contribute to the conditional mean of response ====. If the conditional mean independence holds, that is, ==== almost surely, then there is no need to fit a regression model for the conditional mean of ==== given ====.====To detect the conditional mean independence, ==== to be large but finite. With moment restrictions on ====, the martingale difference correlation equals zero if and only if the conditional mean independence holds. In this sense, the martingale difference correlation can capture any type of conditional mean relationships. However, if the underlying distributions go against the moment conditions with potential outliers, the martingale-difference-correlation-based test may suffer from power loss, see ==== for more details. ==== and ==== introduced cumulative divergence as an important alternative to the martingale difference correlation. The zero cumulative divergence can also fully characterize the conditional mean independence between two univariate random variables. This rank-based measure does not require any moment conditions on the conditioning variable, and also keeps invariant to arbitrary monotone transformations of the conditioning variable. Although the cumulative divergence possesses such appealing properties, its scope of applications is pretty limited as it is only applicable in the univariate scenario.====The first goal of our paper is to generalize the cumulative divergence in a multivariate space. To accommodate arbitrary dimensional covariates, we modify the original cumulative divergence by averaging over all one-dimensional projections. The projection-averaging technique has been widely used to solve many other statistical problems. For instance, ==== proposed a projection correlation to characterize dependence between two random vectors. ==== developed a projected empirical process-based test for functional linear regression model. ==== used projection-averaging to introduce a two-sample testing procedure that is applicable in arbitrary dimensions. In this paper, we exploit the same principle in the context of testing and measuring conditional mean dependence. The new projection-averaging-based cumulative divergence not only inherits the desirable features of its univariate version, but also has its own advantages. It has an elegant closed-form expression that involves no tuning parameters, making it easy to implement. In its estimation, it utilizes the term ====, where ==== denotes ==== norm, and hence it is insensitive to the dimension of ====.====If covariates ==== contribute to the conditional mean of response ====, then a natural question is whether the underlying model structure is adequate to model the relationship between ==== and ====. A wrongly specified model structure would lead to misleading statistical analysis. Therefore, we further introduce a variant of projection-averaging-based cumulative covariance, which measures the conditional mean dependence of a random error term given covariates ==== constructed a data-driven Cramér-von Mises test. However, its power performance can be greatly influenced by the dimension of the covariates. ==== studied the asymptotical behavior of score-type test statistics. ==== designed a test for the multivariate response based on the empirical likelihood. ==== in high-dimensional data analysis. Meanwhile, the framework of our testing procedure also generalizes the projected-based test of ==== to a semiparametric regression setting that allows an unspecified link function. We show that the proposed test is consistent against any global alternatives and can detect the local alternatives distinct from the null at the parametric rate of ====.====The rest of this paper is organized as follows. Section ==== introduces the projection-averaging-based cumulative covariance and cumulative divergence. Their nice properties are also investigated. In Section ====, we propose a goodness-of-fit test for single-index models, and rigorously derive its properties under the null hypothesis and alternatives. The performances of our proposals are illustrated through simulations and a real data example in Section ====. We discuss the paper in Section ====. All technical proofs are given in the Supplement.====The following is the Supplementary material related to this article.",Projection-averaging-based cumulative covariance and its use in goodness-of-fit testing for single-index models,https://www.sciencedirect.com/science/article/pii/S0167947321001353,18 June 2021,2021,Research Article,208.0
"Castelletti Federico,Peluso Stefano","Department of Statistical Sciences, Università Cattolica del Sacro Cuore, Milan, Italy,Department of Statistics and Quantitative Methods, Università degli Studi di Milano-Bicocca, Milan, Italy","Received 28 September 2020, Revised 11 June 2021, Accepted 11 June 2021, Available online 18 June 2021, Version of Record 23 June 2021.",https://doi.org/10.1016/j.csda.2021.107304,Cited by (0)," known as essential graph (EG). Structure learning directly conducted on the EG space, rather than on the allied space of DAGs, leads to theoretical and computational benefits. Still, the majority of efforts has been dedicated to Gaussian data, with less attention to methods designed for multivariate ====. A ","), with directed acyclic graphs (DAGs) particularly suitable for many scientific problems, especially biological (====; ====; ====; ==== (====). While in some fields (e.g. genomics) such dependence relationships can be postulated ====), and therefore should have the same marginal likelihood, a requirement known as ====. DAGs in the same Markov equivalence class can then be represented by an essential graph (EG, ====, ====Clearly, structure learning of EGs always guarantees score equivalence, since it operates at level of equivalence classes. In this context, first efforts to the investigation of the EG space have been confined to small graphs (====), whilst more recently larger graphs were studied by ==== and ==== relies on the method of ==== to construct a parameter prior for Gaussian EGs, following the objective Bayes perspective of ==== and ==== for, respectively, Gaussian and covariate-adjusted DAG models. See also ==== provides an EG estimate using a greedy equivalence search (GES) algorithm based on additions and deletions of single edges, later modified by ====. Moreover, ==== proposed the PC algorithm, a constraint-based method which implements a sequence of conditional independence tests.====) are widely employed in many domains (====), to our knowledge the Bayesian literature on categorical EGs learning is narrow, limited to ==== and ====. The adoption of Bayesian scores by frequentist score-based methods can partially fill this gap, but clearly does not represent a fully satisfying solution. Still on the frequentist side, the PC algorithm of ====. ==== and ==== extensively cover categorical structure learning, but none of them at the level of equivalence classes. Also, the score-based algorithms outlined in ==== only learn discrete DAGs. Furthermore, with categorical data different prior specifications lead to different common scores, with crucial impact on the performance, and may easily compromise score equivalence.====In the present paper we propose a Bayesian method for structure learning of categorical EGs. Following the approach of ====, originally introduced for DAG models, we derive a closed-form expression for the marginal likelihood of an EG, therefore avoiding any issue related to lack of score equivalence or prior misspecification. Exploiting the Markov chain in ==== and developments in ====, a relative MCMC scheme on the EG space is constructed. Our method is fully Bayesian and therefore outputs a posterior distribution over the EG space, rather than the single model estimates provided by frequentist benchmarks such as the PC algorithm (==== on the space of DAGs, we directly score EGs by deriving a closed-form expression for the EG marginal likelihood, and adopt an MCMC scheme targeting the posterior over the space of Markov equivalence classes. The benefits of a Bayesian method for DAG model selection specifically targeted to EGs rather than DAGs are addressed from a theoretical perspective by ==== is extended to model selection of EGs in Section ====. Here we focus on the EG-driven likelihood decomposition, on the parameter prior induced by ==== is implemented on simulated data (Section ====), on a synthetic medical belief network and on US Congress voting records (Section ====). We finally discuss extensions to interventional categorical data and multiple datasets in Section ====.====A graph ==== is a pair ==== where ==== is a set of vertices (or nodes) and ==== a set of edges (or arcs). Nodes are associated to variables, while edges are used to represent direct interactions between variables. Let ====, ==== be two nodes. We say that ==== contains the directed edge ==== if and only if ==== and ====. If instead both ==== and ====, then ==== contains the undirected edge ====. Accordingly, we say that ==== is an undirected (directed) graph if it contains only undirected (directed) edges; in addition, ==== is ==== if it contains at least one directed edge.====Two vertices ==== are adjacent if they are connected by an edge (directed or undirected). In addition, we call ==== a ==== of ==== if ==== is in ==== and denote the neighbour set of ==== as ====; the common neighbour set of ==== and ==== is then ====. We say that ==== is a ==== of ==== and that ==== is a ==== of ==== if ==== is in ====. The set of all parents of ==== in ==== is then denoted by ====. A sequence of nodes ==== where ==== and ==== or ==== for all ==== is called a ====. A cycle is directed (undirected) if it contains only directed (undirected) edges; conversely we call it a partially-directed cycle. A graph with only directed edges is called a directed acyclic graph (DAG) if it does not contain cycles. For any subset ==== we denote with ==== the ==== of ==== induced by ====, where ====. A (sub)graph is complete if its vertices are all adjacent.====A partially directed graph with no partially-directed cycles is called a ==== (CG) or simply ==== (PDAG). For a chain graph ==== we call ==== ==== a set of nodes that are joined by an undirected path and denote the set of chain components of ==== by ====. A subgraph of the form ====, where there are no edges between ==== and ====, is called a ==== (or ====). The ==== of a graph ==== is the undirected graph on the same set of vertices obtained by removing the orientation of all its edges. Finally, a consistent extension of a PDAG ==== is a DAG on the same underlying set of edges, with the same orientations on the directed edges of ==== and the same set of ====-structures (====).",Equivalence class selection of categorical graphical models,https://www.sciencedirect.com/science/article/pii/S0167947321001389,18 June 2021,2021,Research Article,209.0
"Zhang Xiaoke,Xue Wu,Wang Qiyue","Department of Statistics, The George Washington University, Washington D.C., USA,Department of Computer Science, The George Washington University, Washington D.C., USA","Received 19 October 2020, Revised 27 May 2021, Accepted 7 June 2021, Available online 11 June 2021, Version of Record 16 June 2021.",https://doi.org/10.1016/j.csda.2021.107303,Cited by (4)," balancing methods are proposed to estimate the propensity score, which minimize the correlation between the treatment and covariates. The appealing performance of the proposed method in both covariate balance and causal effect estimation is demonstrated by a simulation study. The proposed method is applied to study the causal effect of body shape on human visceral adipose tissue.","; ====; ====; ====; ====; ====; ====).====To study the association between a functional predictor and a scalar response, which is a classic topic in FDA, functional regression is the most popular class of approaches (see survey papers, e.g., by ====; ====; ====; ====; ====; ====, ====; ====; ====). In contrast, we consider a functional variable as a treatment in observational studies.====In the literature on causal inference for observational studies, the causal effect estimation of a binary treatment is a classic topic and has been intensively studied (see overviews, e.g., by ====; ====; ====; ====; ====). In the past few decades, an increasing number of papers have appeared on multi-level categorical treatments or continuous treatments (e.g., ====; ====; ====; ====; ====; ====; ====; ====; ====; ====; ====). Recently, treatments of more complex forms have gradually received more attentions, such as multidimensional categorical treatments (e.g., ====; ====) and matrix treatments (e.g., ====The main contribution of this paper is twofold. First, the propensity score for functional treatments is properly defined, termed “functional propensity score”, to balance covariates. When a treatment is binary, multi-level categorical or continuous, the propensity score (====) or generalized propensity score (====; ====; ====) has been commonly used to balance covariates, which is defined in terms of the conditional mass/density function of the treatment given covariates. However, this way of defining propensity scores is inapplicable to functional treatments since the density function for a functional variable generally does not exist (====; ====; ====; ====; ====; ====; ====; ====; ====). Following the same idea, we seek functional propensity score estimates which can minimize the correlation between the FPC scores and covariates. Our methods generalize the parametric and nonparametric covariate balancing methods by ====; ====; ====), which is closely related to the covariate balancing measure.====The rest of the paper proceeds as follows. Section ==== provides the definition of the functional propensity score. In Section ====, two covariate balancing methods are proposed to estimate the functional propensity score, and the causal effect estimation by functional propensity score weighting is also introduced. Section ==== presents a simulation study to evaluate the performances of the two proposed methods with respect to covariate balancing and causal effect estimation accuracy. In Section ====, the proposed methods are applied to study the causal effect of body circumference on VAT. Section ==== concludes the paper.====The following is the Supplementary material related to this article.",Covariate balancing functional propensity score for functional treatments in cross-sectional observational studies,https://www.sciencedirect.com/science/article/pii/S0167947321001377,11 June 2021,2021,Research Article,210.0
"Rodwell D.T.,van der Merwe C.J.,Gardner-Lubbe S.","Department of Statistics and Actuarial Science, Stellenbosch University, South Africa","Received 23 July 2020, Revised 3 June 2021, Accepted 4 June 2021, Available online 9 June 2021, Version of Record 15 June 2021.",https://doi.org/10.1016/j.csda.2021.107299,Cited by (0)," is proposed. This technique, named CVA(H====), is showcased using the established mushroom data set, which contains a mix of categorical and ordinal variables. This novel method improves upon existing biplot construction in terms of ==== and class separation.",". This issue is further compounded in cases where the number of variables is larger than three. ====The following items will be discussed: the construction of PCA, CVA, and non-linear PCA biplots (also termed catPCA biplots) in section ====; thereafter, the new technique, referred to as CVA(H====), will be presented along with the mushroom data set, which will be used to construct the biplots. Thereafter, the various sets of biplots, namely, catPCA and CVA(H====) will be given together with accuracy metrics in section ==== to show that the new CVA(H====) technique improves on the existing non-linear PCA biplot; concluded in section ==== with a summary and discussion of areas for further research.",Categorical CVA biplots,https://www.sciencedirect.com/science/article/pii/S016794732100133X,9 June 2021,2021,Research Article,211.0
"Derek Tucker J.,Shand Lyndsay,Chowdhary Kenny","Sandia National Laboratories, PO Box 5800 MS 0829, Albuquerque, NM 87185, United States of America","Received 29 May 2020, Revised 31 May 2021, Accepted 3 June 2021, Available online 8 June 2021, Version of Record 11 June 2021.",https://doi.org/10.1016/j.csda.2021.107298,Cited by (3),"Functional data registration is a necessary processing step for many applications. The observed data can be inherently noisy, often due to measurement error or natural process uncertainty; which most functional alignment methods cannot handle. A pair of functions can also have multiple optimal alignment solutions, which is not addressed in current literature. In this paper, a flexible ==== Monte Carlo to be well-defined on the infinite-dimensional Hilbert space. This flexible Bayesian alignment method is applied to both simulated data and ==== to show its efficiency in handling noisy functions and successfully accounting for multiple optimal alignments in the posterior; characterizing the uncertainty surrounding the warping functions.",", ====, and ====. Functional data registration is also referred to as phase-amplitude separation, because the underlying goal of the procedure is to effectively distinguish phase (====-axis) from amplitude (====Early approaches to functional data alignment fall short in four major ways. (1) They lack the ability to characterize the uncertainty of the optimal warping function. (2) They assume the observed functions are naturally smooth and lack measurement error. (3) They do not consider more than one optimal alignment between a pair of functions. Finally, (4) Most methods do not use a proper distance as the objective function for registration.====Traditional approaches to functional data alignment, which are demonstrated in ====, ====, ====, and ====, are not flexible enough to characterize the uncertainty of the resulting optimal alignment solution. These approaches make the common assumption that the functions to be aligned are smooth, and thus they break down when a function exhibits too much noise. These methods usually rely on a derivative which is extremely noisy when the original functions are noisy, presenting additional computational challenges.====Recently, Bayesian frameworks have been proposed that can characterize the uncertainty of the warping function solution (====; ====; ====; ====, ====, and ==== and ====, to simplify the complicated geometry. We will also take advantage of this transformation and provide more details in Section ====Only the most recently proposed approach (====), has addressed the need to account for observed measurement error. There approach is for sparsely sampled functions where they propose a data-driven prior for inference and solely look at sparsely sampled functions and do not treat the dense problem. Previous approaches, Bayesian or otherwise, have assumed the observed functions to be aligned are naturally smooth. Although smoothness is a convenient assumption, it can be an invalid one in many practical applications. Recent work (====), utilized a fPCA construction to avoid overfitting when the data has additive amplitude variation. ====In this paper, we propose a hierarchical Bayesian approach to the registration of a pair of noisy functions on ====.====This paper is organized in the following way. Section ==== reviews pairwise functional registration in ====, the general challenges associated with it, and introduces the square-root velocity function representation and proper distance metric relied upon throughout this paper. Section ====, including our MCMC algorithm and multi-chain approach to the challenge of multiple alignments. In Section ==== Matlab package on GitHub.====.====In this section we provide the derivations to compute the derivative of the negative log-likelihood, ====. The negative log-likelihood is defined====To compute the directional derivative, let ==== and we will rewrite ==== for simplicity as====To find the directional derivative ====, we first, consider the sequence of maps ====, where ====. For the constant function ==== and a tangent vector ==== the differential of the first mapping at ==== is ====. For a tangent vector ====, the differential of the second mapping at ==== is ====, where ====. If we concatenate these two linear maps we obtain the directional partial derivative of ==== in a direction ==== as====We now can write the derivative of Φ in the direction of ==== as====Since ==== is an infinite-dimensional space, we can approximate the directional partial derivative by considering a finite-dimensional subspace of ====. Let us form a subspace of ==== using ====. We then can approximate the derivative using==== where ===='s are the ==== basis elements of the subspace.",Multimodal Bayesian registration of noisy functions using Hamiltonian Monte Carlo,https://www.sciencedirect.com/science/article/pii/S0167947321001328,8 June 2021,2021,Research Article,212.0
"Ghosh Santu,Ayyala Deepak Nag,Hellebuyck Rafael","Division of Biostatistics and Data Science, DPHS, Augusta University, Augusta, GA, USA,South Carolina Department of Public Safety, Columbia, SC, USA","Received 7 September 2020, Revised 8 May 2021, Accepted 9 May 2021, Available online 2 June 2021, Version of Record 8 June 2021.",https://doi.org/10.1016/j.csda.2021.107284,Cited by (1),", small n” setting. The asymptotic null distribution of the test statistic is derived and it is shown that the power of suggested test converges to one under certain alternatives when both ==== and ==== increase to infinity. Finite sample performance of the proposed test statistic is compared with other recently developed tests designed to also handle the “large ====, small ","; ====); medical image analysis (====), signal processing, astrometry and finance (====). Analysis of such high dimension, low sample size data sets present a substantial challenge, known as the “large ====, small ====” problem. One of the most prominent problems that researchers are interested in is making inferences on the mean structure of the population. However, many well known classical multivariate methods cannot be used to make inferences on the mean structures for large ==== small ====) breaks down for the two sample test when the dimension of the data exceeds the combined sample size.====Over the last few years, researchers have turned their attention to developing statistical methods that can handle the large ==== small ==== problem. A series of important works have been done on the two-sample location problem in the large ==== small ====-statistic faces in the large ==== small ====-statistic. However, one of the criticisms (====) of this method is the assumption of equal variance structures for the two populations, which is hard to verify for high-dimensional data. Addressing this shortcoming, ==== suggested an alternative method that does not assume the equality of variance-covariance matrices and removes the cross-product terms from the squared Euclidean norm of difference of sample means between the two populations. Another method proposed by ==== replaces the inverse of the sample covariance matrix in Hotelling's ====-statistic with the inverse of the diagonal of the covariance matrix. Modifications to this work (====, ====) have relaxed some of the assumptions, resulting in relatively similar test statistics with improved performance. Recently, ==== suggested a test procedure, known as generalized component test (GCT), that bypasses the full estimation of the covariance matrix by assuming that there is a logical ordering among the ==== components in such a way that the dependence between any two components is related to their displacement. In a development of another direction, ====-statistics of transformed data. Tests developed in (====; ====; ====; ====; ====; ====) are designed for testing dense but possibly weak signals, i.e., there are a large number of small non-zero means differences. Test suggested in ==== precision matrix, which is time-consuming for large ==== (====).====There is another set of test procedures (====, ====) recently proposed for testing the equality of means by using the notion of adapting the “sparsity level” of the signals. With such an idea, ==== developed an adaptive two-sample test for high-dimensional means. ==== further extended ===='s method under a U-statistics framework. These methods achieve high power against different types of signals.====Driven by the above two concerns of the test (====), we reconsider the problem of testing two mean vectors under sparse alternatives. We propose a new test based on the prepivoting approach. The concept of prepivoting is introduced by ====. Thus, the distribution of the prepivoted root is less dependent on population parameters than the distribution of the root. Consequently, approximate inference based on the prepivoted root is more accurate than approximate inference based on an original root. Pre-pivoting has not been considered previously in high-dimensional testing problems. A snapshot of the development of our proposed test method is as follows: we construct prepivoting marginally for all the ====The rest of paper is organized as follows. In Section ==== based on the prepivoting technique and study the limiting distribution null distribution of the test statistic. In this section, we also analyze the power of the suggested test. Simulation studies are presented in Section ==== and Section ====. Proofs and other theoretical details are relegated to the Appendix.====For two sequences of random variables ==== and ====, write ==== as ==== if ==== in probability as ====, write ====, if ==== is bounded in probability as ====.====To sketch the proof of ====, we need ====, ====, ====. ==== is given below. ==== ====: The heuristic justification heavily depends on the proof of ====. From the proof of ====, the expression of ==== is==== The expression for ==== is==== because ====. Since the order of convergence of ==== to 0 is same as ====, so on the basis of the proof of ====, we may conclude that the order of rate of convergence of the test statistic is ====, provided ====.",Two-sample high dimensional mean test based on prepivots,https://www.sciencedirect.com/science/article/pii/S0167947321001183,2 June 2021,2021,Research Article,213.0
"Wang Pei,Yin Xiangrong,Yuan Qingcong,Kryscio Richard","Dr. Bin Zhang Department of Statistics, University of Kentucky, Lexington, KY, United States of America,Department of Statistics, Miami University, Oxford, OH, United States of America","Received 22 May 2020, Revised 19 May 2021, Accepted 20 May 2021, Available online 26 May 2021, Version of Record 1 June 2021.",https://doi.org/10.1016/j.csda.2021.107285,Cited by (3), small ==== data are provided. The efficacy of the method is demonstrated by simulations and a real data example.," and a predictor vector ====. We seek to find a ==== dimension reduction matrix ==== such that ====, where ====. Here ==== means independence; that is, ==== is independent of ==== given ====. The subspace spanned by the columns of ==== is called a dimension reduction subspace (====; ====). Since ==== is not unique, the primary goal is to find the central subspace (CS), ====, which is the intersection of all such subspaces, if the intersection itself is a dimension reduction subspace (====). Such a CS exists and is unique under mild conditions (====; ====) and its dimension is called the structural dimension. Specific subspaces of interest are often developed. For instance, if the mean function ==== is of interest, then a matrix ==== such that ==== is called a mean dimension reduction matrix (====), and the subspace spanned by the columns of ==== is called a mean dimension reduction subspace. Similarly, the central mean subspace (CMS), ====, is defined as the intersection of all such mean dimension reduction subspaces if the intersection itself is a mean dimension reduction subspace (====). Other specific subspaces of interest include central moment subspace (====), central variance subspace (====), central informative predictor subspace (====) and central ====-subspace (====).====) and sliced average variance estimation (SAVE; ====), kernel inverse regression (KIR; ====), and directional regression (DR; ====); Forward methods include minimum average variance estimation (MAVE; ====) and sliced regression (SR; ====) and Kullback-Leibler distance (====; ====). Some of the proposed methods apply to a multivariate response. See ====, ====, ====, ====, ====, ==== and ====. A challenge in SDR is interpretation since all the original variables are involved in the reduced variables. To address this issue, ==== suggests adding a penalty to produce a sparse estimation. See also, ====. ==== propose two path procedures to solve large ==== small ==== problems. Different from the penalization methods, a screening approach aims to select the important variables only. See ====, ====, ====, ==== and ====.==== in the former method and by avoiding an optimization involving a nonlinear constraint in the latter method without sacrificing efficiency. In addition, the method has higher efficiency computationally compared to most of the forward methods which typically use nonparametric estimation. Our proposal works well not only for univariate response, but also for multivariate response. We further provide a method to estimate the structural dimension. By incorporating ====, ====, and ====, we develop a sparse SDR estimator. For large ==== small ==== data, we propose two methods by adapting the two-stage selection procedure of ====, and by using the sequential approach of ====.====The article is organized as follows. Section ==== provides the proposed method along with the estimations, algorithms and theoretical properties. Section ==== gives the details of sufficient variable selection. Section ==== develops two methods to deal with large ==== small ==== problems. Section ==== presents the simulation studies and a real data example. Section ==== concludes the article with a short discussion. All proofs and additional simulations are in the supplementary file.====The following is the Supplementary material related to this article.",Feature filter for estimating central mean subspace and its sparse solution,https://www.sciencedirect.com/science/article/pii/S0167947321001195,26 May 2021,2021,Research Article,214.0
"Acosta Jonathan,Alegría Alfredo,Osorio Felipe,Vallejos Ronny","Departamento de Estadística, Pontificia Universidad Católica de Chile, Santiago, Chile,Departamento de Matemática, Universidad Técnica Federico Santa María, Valparaíso, Chile","Received 28 June 2020, Revised 27 February 2021, Accepted 15 May 2021, Available online 21 May 2021, Version of Record 27 May 2021.",https://doi.org/10.1016/j.csda.2021.107282,Cited by (4),"The development of new techniques for sample size reduction has attracted growing interest in recent decades. Recent findings allow us to quantify the amount of duplicated information within a sample of spatial data through the so-called ==== (ESS), whose definition arises from the ==== of the full likelihood-based ESS while maintaining a moderate computational cost. A large dataset is analyzed to quantify the effectiveness and limitations of the proposed framework in practice.","The effective sample size (ESS) has been developed to quantify the number of independent and identically distributed observations within a sample of size ==== (====).====The literature on the ESS is substantial. While ==== provided a novel definition for spatial random fields with constant means, ==== addressed the ESS from a model selection perspective. ==== and ==== studied the ESS in specific contexts of importance sampling. ESS applications can be found, for instance, in ==== and ====, while ==== provided practical guidelines for ESS determination.====The ESS introduced by ==== and the references therein.====To circumvent the so-called “big ====” problem, we propose an alternative sample size reduction approach, which is fully based on block likelihood inference (====; ====). First, parameter estimation is carried out by means of the block likelihood estimation method; this accomplishes a trade-off between statistical and computational efficiency, where the inverses of small correlation matrices are involved. Second, a new notion of the effective sample size (named ====) comes from the Godambe information arising from this estimation framework. In particular, we focus on the “small blocks” method in ====, which performs better than other similar competitors (====; ====; ====). At the same time, the “small blocks” version generates a particularly amenable expression for ====. To illustrate the use of ====, as well as the simultaneous autoregressive model (see, e.g., ====The article is organized as follows. Section ==== briefly reviews the definition, some examples, and the main theoretical attributes of the traditional ESS. Section ==== introduces ==== and its properties. Section ==== discusses computational aspects that permit an efficient implementation of ==== for regularly-spaced locations. In Section ====, we calculate ====, the discrepancies between the ESS and ==== estimates obtained through simulation experiments are assessed. The computational performances of these approaches are also explored. Section ==== presents a real data application. Finally, Section ==== is a discussion of the main findings, and this includes problems to study in future research. For a neater exposition, the proofs of the main results are given in Appendix ====.====We now proceed with a point-to-point proof of ====.",Assessing the effective sample size for large spatial datasets: A block likelihood approach,https://www.sciencedirect.com/science/article/pii/S016794732100116X,21 May 2021,2021,Research Article,215.0
"Kang Yicheng,Shi Yueyong,Jiao Yuling,Li Wendong,Xiang Dongdong","Bentley University, Waltham MA, United States,China University of Geosciences, Wuhan, China,Wuhan University, Wuhan, China,Shanghai University of Finance and Economics, Shanghai, China,East China Normal University, Shanghai, China","Received 17 September 2020, Revised 22 April 2021, Accepted 7 May 2021, Available online 13 May 2021, Version of Record 19 May 2021.",https://doi.org/10.1016/j.csda.2021.107266,Cited by (1),Jump ,"). In quality control, a jump in a quality index of a product indicates that the production line could be out of control (====; ====). In finance, possible jumps in the exchange rate between Korean won and U.S. dollar during December 1997 have been identified by ====.====; ====; ====; ====). Some others adopt a direct approach that puts jump-preserving regression in the same framework as that of conventional nonparametric regression, without explicitly detecting jumps (e.g., ====; ====; ==== may be affected by the two countries' gross domestic products and interest rates. The jump in sea-level pressure in Bombay India noted in ==== and popularized by ====; ====, ====; ====; ====, …, ====:==== where each ==== (====) is piecewise continuous with possible jumps. The objective of our methodology is to estimate ==== and ====, if there is a single jump in the relationship between ==== and ==== and a single jump in the relationship between ==== and ====, then in the relationship between ==== and ====, there are two jump location curves that are parallel with the x-axis and the y-axis. In contrast, the jump location curves in the existing 2D JRA (e.g., ====) or the jump location surfaces in the existing 3D JRA (e.g., ====) could be much more flexible. On the other hand, the existing methods are difficult to generalize to higher-dimensional cases, since they require more intense observations in the design space which are often hard to obtain in certain applications.====, the finite-sample performance of our method is evaluated by simulated examples. An application to a real data is demonstrated in Section ====. Some concluding remarks are provided in Section ====. An R package that implements the proposed method and proofs of the theorems are included in the supplementary materials.====The following is the Supplementary material related to this article.",Fitting jump additive models,https://www.sciencedirect.com/science/article/pii/S0167947321001006,13 May 2021,2021,Research Article,216.0
"Wang Feifei,Zhu Yingqiu,Huang Danyang,Qi Haobo,Wang Hansheng","Center for Applied Statistics, Renmin University of China, Beijing, China,School of Statistics, Renmin University of China, Beijing, China,Guanghua School of Management, Peking University, Beijing, China","Received 26 August 2020, Revised 23 April 2021, Accepted 27 April 2021, Available online 29 April 2021, Version of Record 10 May 2021.",https://doi.org/10.1016/j.csda.2021.107265,Cited by (1),"One-shot-type (or divide-and-conquer) estimators have been widely used for distributed statistical analysis. However, their outstanding statistical efficiency hinges on two critical conditions. The first is the ==== condition, which requires that the sample sizes allocated to different Workers should be as comparable as possible. The second one is the ==== and ","Statistical analyses for massive data in distributed systems are becoming increasingly popular (====; ====; ====; ====). For distributed statistical analyses, so-called one-shot (OS) (or divide-and-conquer) estimators are commonly used (====; ====; ====; ====; ====). Here, we consider a distributed system with the standard Master–Worker-type architecture. Under this architecture, there should exist a central computer called the Master. The Master should be connected with all other computers, which are referred to as Workers. The idea of the OS estimator is to first obtain a local estimate by using local samples in each Worker. Then, different Workers produce different local estimates, which are reported to the Master to be simply averaged (====; ====; ====).====; ====). These properties make OS-type estimators both practically and theoretically appealing.====Nevertheless, the outstanding statistical efficiency of OS estimators hinges on two critical conditions. The first is a ==== condition. That is, the massive data should be distributed across Workers in a relatively uniform way. More specifically, the sample sizes allocated to different Workers should be as equal as possible (or at least of the same order). The second is a ==== condition. That is, the massive data should be distributed across Workers as randomly as possible. Since researchers often assume that WSs are distributed evenly and randomly across Workers (====; ====; ====; ====), these ==== and ==== conditions are assumed implicitly.==== is naturally split by time, which makes subsequent data operations practically convenient. However, the price paid for this structured storage is that the data distribution from different Workers can differ markedly. Indeed, the corresponding sample sizes can be incomparable.====In this context, how the violation of the two conditions affects the statistical performance of the OS estimators becomes a problem of great interest. Intuitively, when the ==== condition is violated, at least one Worker suffers a comparatively small sample size. Accordingly, the local estimates produced by these Workers would have substantially larger variability or even bias than the others. When the ==== and ==== conditions are crucial. The failure of either one can lead to seriously biased or even inconsistent OS estimates.====To overcome the challenges imposed by distribution ==== and ====Compared with a standard OS estimator, the OSUP estimator pays a price for obtaining the pilot sample. However, this computational cost is negligible in practice. By contrast, the gain obtained by the OSUP method is substantial. In particular, it leads to an estimator with superior statistical efficiency. Specifically, under very mild conditions, we show theoretically that the OSUP estimator can enjoy the same ====This study makes the following important contributions to the literature. First, we are among the first to study the issue of data distribution ==== and ====. We show both empirically and theoretically that the issue is crucial for distributed statistical analysis. Second, we develop a novel OSUP methodology that leads to an estimator with excellent statistical efficiency, without imposing any restrictive conditions about the data distribution of different Workers. As a byproduct, we systematically and theoretically investigate a number of important distributed estimators in terms of bias and variance. By doing so, the critical role played by distribution ==== and ==== can be better highlighted.====The rest of this paper is organized as follows. Section ==== discusses the ==== and ==== conditions empirically and theoretically. Section ==== concludes the paper.====In this section, we provide the detailed proof of the two propositions, where either the ==== condition or the ==== condition is violated. In both these cases, we adopt the logistic regression as an example to discuss the properties of ====. The log-likelihood function of the ====th Worker can be written as",Distributed one-step upgraded estimation for non-uniformly and non-randomly distributed data,https://www.sciencedirect.com/science/article/pii/S0167947321000992,29 April 2021,2021,Research Article,217.0
"Ferreira Marco A.R.,Porter Erica M.,Franck Christopher T.","Department of Statistics, Virginia Tech, Blacksburg, VA 24061, USA","Received 28 February 2020, Revised 31 March 2021, Accepted 24 April 2021, Available online 29 April 2021, Version of Record 4 May 2021.",https://doi.org/10.1016/j.csda.2021.107264,Cited by (0)," (which need to be computed for each value of the random effects variance parameter) in situations when many models need to be fitted. Three simulation studies are performed: the first simulation study shows improved performance in computational speed in estimation of the SGS algorithm compared to an algorithm that uses the spectral decomposition of the ====. Finally, the application of the novel SGS and SPM algorithms is illustrated with a spatial regression study of county-level median household income for 3108 counties in the contiguous United States in 2017.",") spatial random effects are used in a wide variety of fields such as economics, environmental science, and neuroscience. One of the most widely used spatial hierarchical models has intrinsic CAR (ICAR) random effects (====). The distribution of each ICAR random effect typically conditions on the ICAR effects of its neighbors. While this formulation has become ubiquitous in practice, two main difficulties arise when using ICAR spatial random effects. First, the joint density of ICAR effects is improper, which complicates inference on the spatial random effects. Second, the priors placed on the hyperparameters of the hierarchical model may unduly influence the analysis. To address these issues, ====, ==== for the analysis of genomic wide association studies. To the best of our knowledge, similar spectral decomposition transformations have not been used for the analysis of hierarchical models with ICAR spatial random effects.====; ====)) depends on the problem to be solved. Specifically, even though the computation of matrix spectral decomposition scales cubically with sample size, for a given neighborhood structure this decomposition needs to be computed only once to transform the data to the spectral domain, and then the SGS and SPM algorithm are applied directly to the spectral-domain data. In contrast, numerical decompositions for sparse matrices implemented in the R package INLA (the current fastest state-of-the-art) need to be performed for each different value of the random effects variance parameter. Thus, in situations when many models need to be fitted, the SPM algorithm may be substantially faster than INLA. For example, as shown in a simulation study in Section ====As shown in Section ====. Of note, the methods in ==== use a spectral decomposition without rewriting the model in the spectral domain, and so only achieve a reduction to ==== computational cost. We call the algorithm suggested by ==== the Spectral Decomposition of the Precision (SDP) algorithm. When compared to the SPD algorithm, our SGS algorithm reduces the computational time by 25.48% for datasets with 49 regions and 99.95% for datasets with 3600 regions. Therefore, the reduction in computational time is substantially more important for larger spatial datasets.==== possible models.====The remainder of the article is organized as follows. Section ==== reviews ICAR models including specification of priors. Section ==== proves the equivalency between the sum-zero constrained ICAR prior and the improper ICAR prior with centering on the fly. Section ==== reviews the hierarchical sum-zero constrained ICAR model. Section ==== proposes the fast and scalable SGS algorithm for posterior simulation. Section ==== presents the spectral posterior maximizer (SPM) approach to accelerate computations for large sample sizes. Section ==== presents three simulation studies: the first simulation study compares computational speed in estimation of the MCMC algorithms SGS and SDP; the second simulation study compares computational speed in estimation and model selection of SPM and INLA; and the third simulation study compares statistical properties of default INLA statistical procedures with those of SGS and SPM combined with reference priors. Section ==== provides a discussion and concluding remarks. For convenience of exposition, all proofs appear in Appendix ====.====The following lemma is useful for proving the equivalence between the sum-zero constrained ICAR prior and the improper ICAR prior with centering on the fly.",Fast and scalable computations for Gaussian hierarchical models with intrinsic conditional autoregressive spatial random effects,https://www.sciencedirect.com/science/article/pii/S0167947321000980,29 April 2021,2021,Research Article,218.0
"Polansky Alan M.,Pramanik Paramahansa","Department of Statistics and Actuarial Science, Northern Illinois University, De Kalb, IL 60115, USA","Received 29 January 2020, Revised 31 January 2021, Accepted 20 April 2021, Available online 28 April 2021, Version of Record 3 May 2021.",https://doi.org/10.1016/j.csda.2021.107263,Cited by (8)," model. The conditional iterative form of the algorithm insures that the calculations required to simulate an observed random network are relatively simple and does not require complicated models to be fit to an observed network. Bounds on the theoretical cohesiveness of the realized networks are established and empirical studies provide indications on more general properties of the resulting network, suggesting the types of applications where the process would be useful. The algorithm is used to generate networks similar to those observed in several examples.","Let ==== be a directed graph with a finite vertex set ==== and edge set ====. Without loss of generality assume that ==== for some ====. The edge set ==== is a finite collection of ordered pairs of the form ==== where ==== and ==== such that ====, so that we do not allow loops in ====. We will also assume that ==== is weakly connected. The graph ==== is completely characterized by the ==== adjacency matrix of ==== which has ==== element equal to ==== where ==== is the indicator function,==== With this type of representation, the study of graphs essentially simplifies to the study of these matrices. Note that because we do not allow for loops in the graphs, the diagonal elements of such adjacency matrices are all equal to zero. We will denote the set of all ==== binary matrices with all diagonal elements equal to zero as ====. More generally we will denote the set of all ==== binary matrices as ====. For the remainder of this paper we will refer to a graph ==== and its adjacency matrix interchangeably. Hence, for a graph ====, we will also denote the corresponding adjacency matrix as ==== with ==== element ==== symmetric binary matrices with all diagonal elements equal to zero as ====.====, Kolaczyk (====, Chapter 1), ====, and ====.==== (====, ====, ====) and ====. However, there are many fundamental issues which remain, mainly with respect to establishing reliable formal statistical methods such as constructing confidence regions and statistical tests of hypotheses for unknown network characteristics. Little progress, even from an asymptotic viewpoint, has been made in this area mainly due to the complex nature of the problems.==== that a random network has a certain specified level of connectivity. In this framework it is also of great interest to develop formal methods for statistical inference on these types of network parameters. This is the framework that will be directly considered in this paper.====A summary of much of the work with network models and statistical inference that has been developed can be found in ====. More recent research in the area of statistical inference on network graphs includes that of ==== establish some asymptotic properties of empirical graph moments as the graph size grows. ==== consider estimation strategies to perform model-based clustering on large networks. ==== consider inference from the likelihood framework, and develop inference for social network models based on information from adaptive network designs. Another likelihood approach is considered by ====. Some preliminary work on resampling in network graphs has been studied by ==== where two algorithms are considered. ==== use a hidden Markov nested effects model to temporally evolve signaling networks. This method is particularly focused to the case where connections between vertices are inferred from secondary sources.====The simulation of random graphs has been of interest for some time. Beyond the simple simulation of Erdős-Rényi type graphs, many methods are designed to simulate graphs with certain characteristics, such as degree distributions, small world properties, and Markov type properties. See ====, ====, and ==== for a complete background, and information of the application of these methods. Recent efforts include ==== who present a method for simulating graphs calibrated by the degree distributions of the vertices and the edges. These methods are useful if one is interested in the particular statistics that the simulation methods are designed to mimic. If creating graphs with a particular degree distribution is important, then the methods proposed in this paper may not be helpful as the degree distribution of a graph may only be partially dependent on its structure as indicated by motif frequencies.====Model based methods can be useful if the model can be fit successfully and if it is a relatively simple matter to simulate realizations based on the model. Perhaps the most frequently used general model for random graphs is the exponential random graph model (ERGM). A significant amount of research has been dedicated to this type of model, but specifying and fitting these models is a non-trivial task with significant problems related to model degeneracy and instability. See ==== and ====). The proposed simulation method avoids formal model fitting procedures, and only requires the relative frequencies of certain network substructures to be computed.==== develop a nonparametric approach based on sub-sampling and a network model proposed by ====. ==== implements a resampling algorithm using labeled snowball sampling with multiple inclusions. Again, the resampling algorithm does not produce entire network graphs, only sub-graphs.====The remainder of the paper is organized as follows. The proposed stochastic process is described in Section ====. Some bounds on network cohesion and degree distribution are established in Section ====. In Section ==== two examples of observed networks are discussed and the proposed process is used to generate new random networks that have a similar structure to the observed network. Section ==== considers the computation efficiency of three methods for generating networks through the use a benchmark study. Section ==== discusses some potential applications and limitations of the proposed process.",A motif building process for simulating random networks,https://www.sciencedirect.com/science/article/pii/S0167947321000979,28 April 2021,2021,Research Article,219.0
"Lee Jung Wun,Chung Hwan,Jeon Saebom","Department of Statistics, University of Connecticut, Storrs, CT 06269, USA,Department of Statistics, Korea University, 145 Anam-ro, Seongbuk-gu, Seoul 136-701, Republic of Korea,Department of Marketing Bigdata, Mokwon University, Republic of Korea","Received 10 September 2020, Revised 5 January 2021, Accepted 17 April 2021, Available online 24 April 2021, Version of Record 27 April 2021.",https://doi.org/10.1016/j.csda.2021.107261,Cited by (0), are less sensitive to the set of initial values as well as easier to obtain standard errors. We also address issues in MCMC such as label-switching problem with a dynamic data-dependent prior and ==== with a ,", ====; ====). These longitudinal versions of LCA use a set of repeatedly measured manifest items to track the transition or the path of latent class membership, but they can only deal with one latent class variable. Thus, LTA and LCPA are not suitable when we want to investigate the stage-sequential process of two or more latent class variables.====; ====; ====; ====Recently, the sequence of multiple latent class variables has been studied by ==== via multivariate latent class profile analysis (MLCPA), which can be considered as a multivariate version of LCPA. MLCPA incorporates multiple latent class variables in a longitudinal framework to investigate the dynamic change of associations among multiple latent class variables. MLCPA allows researchers to discover additional meaningful subgroups of individuals who share similar sequential patterns over time for several latent variables. ====), but a significant increase in the number of constraints on the parameters can lead to bias or misinterpretation.====; ====; ====; ====; ====).====The remainder of this paper is organized as follows. In sections ==== and ====, we describe an MLCPA with covariates and provide details on the Bayesian estimation strategy. We also present a numerical simulation study to assess the validity of the model identification and estimation using MCMC algorithm. In section ====, we apply an MLCPA to the data from NLSY97 to explore the relationship between depressive symptoms and substance use behaviors and their stage-sequential patterns. Concluding remarks are provided in section ====.====Let ==== be a set of parameters in MLCPA model.",Bayesian multivariate latent class profile analysis: Exploring the developmental progression of youth depression and substance use,https://www.sciencedirect.com/science/article/pii/S0167947321000955,24 April 2021,2021,Research Article,220.0
"Wang Kangning,Li Shaomin,Zhang Benle","School of Statistics, Shandong Technology and Business University, Yantai, China,Center for Statistics and Data Science, Beijing Normal University, Zhuhai, China,Guanghua School of Management, Peking University, Beijing, China","Received 14 June 2020, Revised 12 April 2021, Accepted 17 April 2021, Available online 22 April 2021, Version of Record 27 April 2021.",https://doi.org/10.1016/j.csda.2021.107262,Cited by (3),"Statistical analysis of massive data is becoming more and more common. Distributed composite ==== regression (CQR) for massive data is proposed in this paper. Specifically, the global CQR loss function is approximated by a surrogate one on the first machine, which relates to the local data only through their gradients, then the estimator is obtained on the first machine by minimizing the surrogate loss. Because the gradients of local datasets can be efficiently communicated, the communication cost is significantly reduced. In order to reduce the computational burdens, the induced smoothing method is applied. Theoretically, the resulting estimator is proved to be statistically as efficient as the global CQR estimator. What is more, as a direct application, a smooth-threshold distributed CQR estimating equations for variable selection is proposed. The new methods inherit the robustness and efficiency advantages of CQR. The promising performances of the new methods are supported by extensive numerical examples and real data analysis.","Nowadays, massive data sets are ubiquitous in many fields such as astronomy, economics, and industrial problems. Because such a huge dataset is hard to be efficiently dealt by one single machine, it is usually distributed and processed on many connected computers (====; ====).====A large number of methodologies and algorithms towards the distributed massive data analysis have been developed. The divide-and-conquer (DC) is one of the most important algorithms. It first obtains local estimators on each machine, then averages the local estimators to form the final estimator. For instance, ==== and ==== averaged the M-estimators and de-biased estimators, respectively; ==== defined an average for subspaces. For more references, one can see ====; ====; ====; ====; ==== and the references therein. Since only one round of communication is required in DC, the communication cost is significantly reduced. However, it might loss estimation efficiency in most occasions (====; ====). In order to improve the estimation efficiency, ====; ==== and ==== proposed the communication-efficient surrogate likelihood (CSL), and ====An important problem worthy of being noticed is the aforementioned methods are not robust since they are mainly based on mean regression. To improve estimation efficiency and achieve robustness, ==== developed composite quantile regression (CQR). Recently, CQR has been extended to many other commonly used models. For example, ==== and ==== proposed nonparametric and semiparametric CQR models, respectively; ==== and ==== developed weighted CQR, respectively; ==== and ==== extended it to the single index models. All of these results confirm its robustness and efficiency advantages.====Motivated by above considerations, we propose a communication-efficient distributed CQR for the distributed massive data. Specifically, we first approximate the global CQR loss function by a surrogate one on the first machine, which relates to the local datasets only through their gradients. Then the final estimator is obtained on the first machine, while other machines only need to calculate and transmit the local gradients. The communication cost is significantly reduced since the gradients can be easily communicated. Under mild conditions, the ==== of the proposed estimator is established, which shows that the resulting estimator is statistically as efficient as that obtained on the entire data set.====On the other hand, as a direct application, based on the new communication-efficient distributed CQR and the smooth-threshold estimating equations (====), we further propose a variable selection procedure for the massive data, which can also be realized on the first machine. Theoretically, we prove that the proposed variable selection method is consistent in variable selection and enjoys oracle property in estimation. Furthermore, the variable selection procedure also works well just as all the data were pooled on a single machine, since it inherits the good properties of the communication-efficient distributed CQR.====Also, ==== proposed CQR for the massive data, but their method is built upon DC strategy, which is completely different with ours. What is more, ==== and ====; ====), which leads to efficiency loss. Thus, the DC strategy based method in ====; ====; ==== and ====.====Furthermore, we would like to make a clarification here that although the proposed methods are built upon CQR (====) and borrows idea from the CSL method (====), it should be noted that it is not a trivial extension. Specifically, the new surrogate CQR loss function is completely different with the original CQR loss function in ====, and the smoothness condition about the loss function assumed in the CSL method (====; ====) to reduce the computational burdens, and build the statistical equivalence between the minimizer of the new surrogate CQR loss function and that of the smoothed version.====The remainder of this article is organized as follows. In Section ====, we present the communication-efficient distributed CQR, related algorithm and asymptotical properties. Section ==== presents the variable selection procedure. Section ==== presents experimental results and a real data application. Section ==== gives a brief discussion. Some additional simulation results and all the technical proofs are provided in the Appendix.",Robust communication-efficient distributed composite quantile regression and variable selection for massive data,https://www.sciencedirect.com/science/article/pii/S0167947321000967,22 April 2021,2021,Research Article,221.0
"Im Yunju,Tan Aixin","Department of Statistics and Actuarial Science, The University of Iowa, Iowa City, IA 52242, USA","Received 14 January 2020, Revised 7 April 2021, Accepted 12 April 2021, Available online 21 April 2021, Version of Record 3 May 2021.",https://doi.org/10.1016/j.csda.2021.107252,Cited by (2),"A class of Bayesian models tailored to regression problems, the conditional MFMs (cMFM), are described and studied. Computing for the cMFM is developed by extending the efficient ","The term “subgroup analysis” has been used to refer to at least two different types of analysis. One type of subgroup analysis concerns dividing subjects into subgroups based on their observed characteristics, and to find the differences among the subgroups and determine their statistical significance. An example is a study that tests whether the treatment effect differs from female to male. This paper concerns a second type of subgroup analysis, which is sometimes called “clustering”. The general goal is to identify ====This paper focuses on performing subgroup analysis in regression setups, which allows data to tell apart potential subgroups, such that subjects from different subgroups respond differently to a set of ====, sometimes after adjusting for the influence of a separate set of characteristics. We now use a hypothetical example to illustrate the potential benefits of using our subgroup analysis method.====Classical methods for subgrouping (clustering) often measure subjects' similarity based on the (Euclidean) distance between the points ==== and ==== for the ====th subject, and then cluster the ===='s that lead to patterns within groups that are both interpretable and simple. Take continuous responses for example, a possible choice of ==== is the Normal distribution for which the mean response is linear in the covariates, after appropriate transformations of the variables in case needed. Specifically, we write==== Here, ==== denotes the normal density function at ==== with mean ==== and variance ====. Now, let ==== denote the vector of ==== from ====. We say that individuals belong to the same group if their ==== values are equal. The distinct ==== values, say, ==== are called ====, where ==== denotes the number of groups present in the observed sample of size ====. In the example displayed by ====, there were indeed two sets of group-specific parameters used in data generation, ==== and ====.====One can easily extend model ==== to allow for additional covariates ==== that have homogeneous effects ==== on the response variable, by redefining ==== as==== In words, members from different latent subgroups respond differently to the covariates of interest ==== (e.g., treatments), after adjusting for other covariates ==== such as baseline characters of the patients. Note that ==== adopts a slightly more concise notation than ==== by absorbing the intercept terms and define ==== and ====. Also, ==== and ==== are allowed to have ==== components each, leading to ====-dimensional vectors ==== and ====.==== and ==== or ====, while using a penalty term to avoid overfitting, and to encourage individualized parameters to take on similar values to form groups. Specifically, define==== where ==== denotes the ==== norm, and ==== denotes a penalty function with some tuning parameter ==== can have equal values of ==== is that the within-group variabilities are essentially assumed equal, which may not be true over the entire population.==== and ====, the number of groups in the sample, is typically unknown, and one can simply treat it like other parameters and assign it a prior. In fact, a most natural solution is to work with the number of components in the population, ====, which could be different but no smaller than ==== (====). We will present Bayesian mixture models with priors on ====, which can further imply priors and posteriors of ==== (====, sec 3.1). When the prior on ==== is proper, the corresponding model is called a Mixture of Finite Mixtures (MFM) model. However, MFMs are not the most popular for subgroup analysis largely because they are hard to compute, as the dimension of the parameter space varies at different values of ====) because its special construction allows efficient computing algorithms. Nevertheless, the DPM sets ==== (in the prior, and hence in the posterior), which makes it a mis-specified model for many applications. Abundant theoretical and empirical studies have shown that the DPM leads to inconsistent posterior for ====. In fact, ==== can be substantially over-estimated even for small sample size. See, e.g. Miller and Harrison (====, ====, ====). These drawbacks make DPM less ideal for subgroup analysis.====Aside from choosing the mixing distribution such as the MFM or the DPM, another choice to make concerns the components to be mixed, specifically, whether to treat the covariates ==== gave a nice review of mixture models that model both ==== and ==== under the name “augmented response models”. To list a few such examples, ==== use the DPM of Multivariate Normal for the joint distribution of ==== and ====, from which the relationship between ==== and ==== is implied. ==== introduced the product partition model with covariates (PPMx) that allows a more general prior on subgroup structures that depend on ====. One type of models we include in our comparative study are mixtures of Multivariate Normal as shown in ====, they are the same models of ====. We call any mixtures (such as the DPM and the MFM) of ==== the ==== models, in contrast to mixtures of models ==== and ====, which we call the ==== models because the ===='s are fixed and conditioned upon in the latter cases. When mixed using the DPM, our conditional model, which we write as the cDPM, resides in the very general framework of Dependent Dirichlet Process (DDP) models (====). Also, see ====, which is similar to using the DPM over model ====. The cDPM model we study is slightly more general than the Linear DDP, in that the intercept and the variance of error terms are allowed to be heterogeneous, and that we also consider model ==== that allows each covariate to have either heterogeneous or homogeneous effects. Finally, note that the term “conditional models” should not be confused with the term “conditional algorithms”, the latter term is sometimes used to refer to a type of MCMC algorithms for Bayesian mixture models (====). See Section ==== for more discussions on the algorithms.====In light of the discussions above, this paper has the following objectives for subgroup analysis in regression problems. First, we investigate the use of MFM combined with models ==== or ==== that model the response conditionally upon the covariates. We call such models the conditional MFM (cMFM). We develop an efficient Markov chain Monte Carlo (MCMC) algorithm to compute the proposed cMFM, which is a straightforward extension of the algorithms of ==== that resembles, in structure and in cost, the Gibbs sampler and its split-merge improvement from ====, ====, ==== for the popular DPM. Secondly, we demonstrate versatile inferences that the cMFM and other Bayesian models can provide, notably the assessment of uncertainties, that is hard to obtain for frequentist methods. Thirdly, we compare several subgrouping methods for regression problems under different assumptions using simulation studies. Aside from the frequentist methods, we consider using the MFM and the classical DPM in the conditional model, and also the use of the MFM and the DPM in models that jointly model response and covariates. We call these Bayesian models the cMFM, the cDPM, the jMFM and the jDPM, respectively. Our study suggests the MFM models can better identify subgroups than their DPM counterparts. The study also shows the pros and cons of the conditional versus the joint modeling strategies. Briefly, the conditional models have good clustering performances in various setups, while the joint models are less robust to mis-specification of the distribution of the covariates and the error terms. In practice, joint models require users to specify the distribution for covariates, which can be challenging or inappropriate, say, when the covariates were indeed fixed by design. Yet these choices strongly influence subgrouping results, as the dimension of covariates is often higher than that of the response. On the other hand, the joint models are in principle more suitable than the conditional models in making predictions for new subjects, as they utilize information from the distribution of covariates for clustering. Still, prediction performance of the joint model vary depending on how well the covariates are modeled, and how much the clustering truly depends on the covariates.====Finally, as pointed out by a reviewer, the mixture models adopted in this paper can be regarded as introducing a discrete latent variable to account for heterogeneity. Note that models with continuous latent variables are also powerful tools to model heterogeneous populations. However, if finding subgroups is a main inference goal, the latter approach requires follow-up analysis, an example of which is doing cluster analysis following factor analysis in regression. Compared to using mixture models for clustering, the two-step clustering approach based on continuous latent variables is more complicated and makes it harder to assess the uncertainty of the results, but could be useful if the latent variables facilitate interpretation.====We structure the paper as follows. Section ==== motivates the Bayesian approach for subgroup analysis by showing a few types of inference it enables. Details of the proposed cMFM and other Bayesian mixture models for subgroup analysis are given in Section ====, with computing methods developed in Section ====. Section ==== contains simulation studies that compare the performance of different Bayesian mixture models and existing frequentist methods for subgroup analysis under different scenarios. Section ==== contains data analysis for a study on the relationship between plasma levels of beta-carotene and various factors. The paper ends with discussions and final remarks in Section ====.====The following is the Supplementary material related to this article.",Bayesian subgroup analysis in regression using mixture models,https://www.sciencedirect.com/science/article/pii/S0167947321000864,21 April 2021,2021,Research Article,222.0
"Ahfock Daniel,McLachlan Geoffrey J.","School of Mathematics and Physics, University of Queensland, Australia","Received 30 October 2020, Revised 3 March 2021, Accepted 15 April 2021, Available online 19 April 2021, Version of Record 23 April 2021.",https://doi.org/10.1016/j.csda.2021.107253,Cited by (0)," using the noisy labels compared to logistic regression using the ground-truth labels can then be derived. The main finding is that logistic regression can be robust to label noise when label noise and classification difficulty are positively correlated. In particular, when classification difficulty is the only source of label errors, multiple sets of noisy labels can supply more information for the estimation of a classification rule compared to the single set of ground-truth labels.","Many supervised learning algorithms operate on the assumption that the training set labels are the ground-truth labels. In practice, ground-truth labels may not be readily obtainable, and manual annotation is used to construct the training dataset (====; ====; ====). A primary concern is the robustness of an estimated classification rule with respect to label noise (====; ====). A closely related issue is that when each member of a group of experts provides a class assignment, the agreement is not necessarily unanimous. The situation when there is heterogeneity amongst the supplied labels is referred to as soft-labelling, as there is no definitive class assignment for each feature vector (====). Extracting the maximum amount of information from conflicting label sets is a challenging task in statistical machine learning (====; ====; ====; ====).====In the framework of model-based classification, the ground-truth labels are commonly treated as latent variables in a finite-mixture model (====; ====). The data generating process for a ====-component mixture model can be represented as a hierarchical model,==== where ==== gives the mixing proportions and ==== is a vector of component specific parameters. The latent variable ==== is considered the ground-truth label as the feature vector ==== is then sampled from the ==== given that ==== ====. We propose to treat the manual label ====.==== highlights that when the manual labels ====. In Section ==== we introduce a random-effects model for group labelling to account for imperfect knowledge of the posterior class-probabilities. In Section ==== presents simulation results regarding the asymptotic relative efficiency. In Section ==== we assess the proposed random effects model on the Gastroentology dataset introduced in ==== (a) and present the results of experiments on the Wisconsin breast cancer dataset. Finally, conclusions and directions for future work are given in Section ====.",Harmless label noise and informative soft-labels in supervised classification,https://www.sciencedirect.com/science/article/pii/S0167947321000876,19 April 2021,2021,Research Article,223.0
"Shi Jianwei,Qin Guoyou,Zhu Huichen,Zhu Zhongyi","Department of Statistics, Fudan University, Shanghai 200433, China,Department of Biostatistics, School of Public Health and Key Laboratory of Public Health Safety, Fudan University, Shanghai 200032, China,Department of Statistics, The Chinese University of Hong Kong, Hong Kong, China","Received 7 August 2020, Revised 8 April 2021, Accepted 10 April 2021, Available online 16 April 2021, Version of Record 26 April 2021.",https://doi.org/10.1016/j.csda.2021.107251,Cited by (3),"In the big data era, practical applications often encounter incomplete data. Current distributed methods, ignoring missingness, may cause inconsistent estimates. Motivated by that, a distributed algorithm is developed for ====-estimation with missing data. The proposed algorithm is communication-efficient, where only gradient information is transferred to the central machine. The parameters of interest and the nuisance parameters are simultaneously updated. Theoretically, it is shown that the proposed algorithm achieves a full sample performance after a moderate number of iterations. The influence of nuisance parameters on distributed ====-estimation is also investigated. Simulations via synthetic data illustrate the effectiveness of the algorithm. At last, the algorithm is applied to a real data set.","; ====). Then, communication efficiency often becomes a major bottleneck when the number of computers is large. These constraints make it necessary to develop methodologies for distributed data.====There has been a large amount of literature on distributed statistics. The distributed methods can be classified into two main categories (====). The first category of methods is called “one-shot” or “embarrassingly parallel”: local machines conduct estimation in parallel and send their results to the central machine, which then aggregates the information and outputs a final result. This method can adopt original non-distributed method directly, thus is applied in a wide range, covering ====-estimation (====; ====; ====), high-dimensional regression (====), nonparametric regression (====), Matrix Completion (====), principal component analysis (====), Heterogeneous data (====) and so on. However, they are restricted to the number of machines since the bias might not be lowered (====; ====).====Another category of methods belongs to Newton-type methods, mainly for distributed ====-estimation (====; ====; ====; ====, ====). The distributed Newton-type methods utilize the similarity between the data stored in different machines. Usually, the mean-squared errors of such distributed methods are similar to which is obtained based on the entire data after several iterations (====).====-estimation all assume data are complete. Simply ignoring incomplete observations will lose efficiency. Worse, it may cause an inconsistent estimator when data are not missing completely at random (====). There are many methods accounting for missing data, including maximum likelihood (====; ====; ====, ====). However, they require specification of the full likelihood or the distribution of missing data conditioning on observed data. For general distributed ====-estimation, it might not be satisfied.====, has a long history in statistics. Many studies (====; ====; ====; ====; ====), well suited for general ====-estimation. Current literature using the IPW method conducts analysis with full data in a single machine. It can not be directly adapted to the distributed setting.====The primary objective of this paper is to develop a communication-efficient method for distributed ====The contribution of this paper is twofold. First, we develop a distributed algorithm for ==== introduced by IPW method and the parameter vector of interest ====The rest of the paper is organized as follows. In Section ====, we develop the proposed method and demonstrate the algorithm in detail. Theoretic results are established in Section ====. Section ==== investigates the performance of our proposed method through simulations and real data analysis. In Section ==== we discuss several potential directions to extend our proposed approach. All the proofs can be found in Section ====.",Communication-efficient distributed ,https://www.sciencedirect.com/science/article/pii/S0167947321000852,16 April 2021,2021,Research Article,224.0
"Donelli Nicola,Peluso Stefano,Mira Antonietta","Università degli Studi di Milano-Bicocca, Department of Sociology and Social Research, Italy,Università della Svizzera italiana, Institute of Computational Science and Università dell’Insubria, Department of Science and High Technology, Switzerland,Università degli Studi di Milano-Bicocca, Department of Statistics and Quantitative Methods, via Bicocca degli Arcimboldi 8, 20126 Milan, Italy","Received 27 April 2020, Revised 27 March 2021, Accepted 28 March 2021, Available online 15 April 2021, Version of Record 23 April 2021.",https://doi.org/10.1016/j.csda.2021.107242,Cited by (1),Interactions among ==== of positive ," (MEMs). These models had been introduced by ==== to overcome some drawbacks associated with conventional workarounds used to model non-negative data, such as ignoring the non-negativity constraint or taking the logs of the observations. The advantages of modelling a non-negative process using distributions with support on the non-negative orthant are well described by ====. The univariate setting has been extended by ==== and ==== face non-negativity challenges, since restrictions to the positive orthant are not explicitly designed (====Models based on DPM already appeared in the econometric literature. For instance, DPM had been used by ==== to model volatility distribution or by ==== for financial returns. Along the same lines, ====, ==== applied it to Cholesky-type multivariate stochastic volatilities. Finally, an approach similar to the one proposed in this paper has been pursued in the context of multivariate GARCH models by ====. Nevertheless, there are some relevant differences between the approach described here and the one used in ====, how the implementation of moment restrictions in DPM models is still an open issue. For related applications of DPM in finance see for instance ====, ====.====The rest of the paper is organized as follows. In Section ====) and an empirical financial application on the interaction among volatility measures (Section ====). In Section ==== we conclude and propose further developments.",A Bayesian semiparametric vector Multiplicative Error Model,https://www.sciencedirect.com/science/article/pii/S0167947321000761,15 April 2021,2021,Research Article,225.0
"Liu Xiaoyu,Xiang Liming","School of Physical and Mathematical Sciences, Nanyang Technological University, Singapore","Received 16 July 2020, Revised 3 April 2021, Accepted 3 April 2021, Available online 14 April 2021, Version of Record 14 April 2021.",https://doi.org/10.1016/j.csda.2021.107248,Cited by (3),", and can achieve the optimal global convergence rate under some conditions. Simulation results demonstrate that the proposed estimator performs satisfactorily in finite samples. The application of the proposed method is illustrated by the analysis of smoking cessation data from a lung health study.","), and his/her time to smoking relapse was recorded between two adjacent annual visits without being observed exactly. It was observed that in the study a proportion of smokers had successfully quit smoking and never experienced the relapse after receiving intervention. To handle such data with a subgroup of “cured” subjects in the study population, a two-component mixture cure model is commonly applied by modeling the population through a mixture of a latency component for uncured (susceptible) subjects and an incidence component for cured (non-susceptible) subjects.====When the failure times of susceptible subjects are interval-censored, the existing literature on the mixture cure models often assumes a conventional survival model for the latency part, such as the proportional hazards (PH) model, proportional odds (PO) model, accelerated failure time (AFT) model (====; ====; ====) or their transformations (====; ====) assumes that the conditional cumulative hazard function of the failure time for the susceptible subjects takes the form of==== where ==== is the unspecified cumulative baseline hazard function, ==== and ==== on hazard progression and effects of covariates ==== and ==== are allowed to share some common components. Model ==== reduces to the conventional PH model when ====, the AFT model when ====, and the accelerated hazards (AH) model when ==== and ====, where the AH model can be written in terms of the hazard function as ==== with ==== being the baseline hazard function.====In a clinical trial for evaluating the treatment effect, where only one binary covariate is considered with ==== for the treatment group, ==== for the control group and ====, model ==== depicts the hazard functions for the control and treatment groups using ==== and ====, respectively. The treatment will accelerate (or decelerate) the hazard progression if ==== (or ====) and have no effect on the hazard progression if ====. After adjusting for the hazard progressions, ==== represents the logarithm of hazards ratio in the treatment group. Particularly when the treatment has no effect on the pace of hazard progression, i.e., ====, the treatment would increase (or decrease) the relative risk if ==== (or ====).==== and ==== developed the non-smooth estimating equations for the estimation of parameters. ==== to a more general class of GAH models as defined in ==== using a spline-based sieve maximum likelihood approach. For interval censored data, limited work has been done under the AH model recently. See ====The rest of the paper is arranged as follows. Section ==== describes a class of GAH cure models for interval-censored data. Section ==== provides details of the proposed sieve maximum likelihood estimation procedure and corresponding algorithm for implementation. Section ==== shows theoretical properties of the proposed estimator. Simulation studies and the analysis of smoking cessation data are presented in Section ==== and Section ====, respectively. Finally, Section ==== concludes the paper with some discussions. All technical proofs are given in the Appendix.====This section contains the proofs of ==== and Theorems ====, ====, ==== given in Section ====. Following the notation used in ====, throughout this appendix we denote by ==== the expectation of the function ====, and ==== the empirical process indexed by ====. Define ==== the loglikelihood for a sample of size one, where ====. Let ====, ====, and let ==== be a constant that may take different values. For all ====, ====.",Generalized accelerated hazards mixture cure models with interval-censored data,https://www.sciencedirect.com/science/article/pii/S0167947321000827,14 April 2021,2021,Research Article,226.0
Lambert Philippe,"Institut de Recherche en Sciences Sociales (IRSS), Méthodes Quantitatives en Sciences Sociales, Université de Liège, Place des Orateurs 3, B-4000 Liège, Belgium,Institut de Statistique, Biostatistique et Sciences Actuarielles (ISBA), Université catholique de Louvain, Voie du Roman Pays 20, B-1348 Louvain-la-Neuve, Belgium","Received 3 June 2020, Revised 8 April 2021, Accepted 8 April 2021, Available online 14 April 2021, Version of Record 19 April 2021.",https://doi.org/10.1016/j.csda.2021.107250,Cited by (3),Penalized B-splines are commonly used in ==== to describe smooth changes in a response with quantitative ,", ====), see R-packages ==== (====) and ==== (==== for early work on this with the four parameters of the stable distribution simultaneously modeled and ====. ==== and ==== relied on a (robustified) extended quasi-likelihood method.==== for early work using local polynomials and ==== with the references therein for some more recent work. These methods typically focus on the estimation of the conditional location and can only handle the estimation of the smooth effects of a very limited number of covariates. Additive models based on P-splines (====; ====; ====; ====) are preferred here for their excellent properties (====) and the possibility to handle a large number of additive terms. They are used to specify the joint effect of covariates on location and dispersion in the framework of the location-scale model, see Section ====. A nonparametric error distribution with an underlying smooth hazard function and fixed moments will be assumed for the standardized error term, see Section ====. In the absence of right-censoring, a location-scale model with a small number of additive terms and a quartile-constrained error density (instead of the hazard here) was considered in ==== and ====. The resulting estimation procedures are motivated using Bayesian arguments and shown to own excellent frequentist properties, see Section ==== and Appendix ====. They are extremely fast and can handle a large number of additive terms within a few seconds even with pure R code. The methodology is illustrated in Section ==== with the analysis of right- and interval-censored income data in a survey. Section ==== closes the paper with a discussion and research perspectives.====Conditionally on ==== and ====, one can obtain the following closed form expressions for the gradient ==== and Hessian ==== of the log posterior of the regression parameters ==== in ====:==== and==== with vectors ==== in ==== and diagonal matrices ====, ====, ==== in ==== defined below. Rewriting the error density as ==== where ==== and ====, we obtain the following expressions (depending on the censoring status of the response) for the aforementioned vectors and matrices:",Fast Bayesian inference using Laplace approximations in nonparametric double additive location-scale models with right- and interval-censored data,https://www.sciencedirect.com/science/article/pii/S0167947321000840,14 April 2021,2021,Research Article,227.0
"Lai Tingyu,Zhang Zhongzhan,Wang Yafei","Beijing University of Technology, Beijing, 100124, China,University of Alberta, Edmonton, AB, T6G 2R3, Canada","Received 11 October 2020, Revised 24 January 2021, Accepted 2 April 2021, Available online 9 April 2021, Version of Record 12 April 2021.",https://doi.org/10.1016/j.csda.2021.107246,Cited by (3),"A novel metric, called kernel-based conditional mean dependence (KCMD), is proposed to measure and test the departure from conditional mean independence between a response variable ==== given ==== is independent of ====, i.e. ==== is violated. Two real data examples are presented for the illustration of the proposed method.","Suppose that ==== and ==== and ====, respectively. We aim to test the hypothesis==== by which we assess whether the predictor ==== has a contribution to the mean of response ==== holds, pursuing a regression model for the conditional mean of ==== is needless. As mentioned in ==== and ====), model checking (====), and the applications in ====, ====, and ====.====The preceding test problem has been investigated by ==== for ==== and ====. Motivated by the distance covariance (==== given ====. ==== generalized MDD to the setting ==== and ==== and proposed partial martingale difference correlation to deal with the problem concerning ==== for some additional predictor ====. ==== proposed functional martingale difference divergence (FMDD), which extended MDD to the functional setting where ==== and ==== can be functional variables. ==== employed the idea of MDD to a high dimension scenario. The MDD method is shown powerful in many cases, and has many applications, see also ====; ====; ====.====As shown in ====, for ==== and ====, the martingale difference divergence can be approximated using pairwise covariances when ==== is large,==== where ==== and ==== is an independent copy of ====. This property is similar to the distance covariance (====). As discussed in ====, for testing dependence between two random vectors, the distance covariance has wonderful performance for monotone relationships, but may lose power for nonlinear dependencies; while the Hilbert-Schmidt independence criterion (HSIC) has opposite performance, that is, HSIC is slightly inferior to the distance covariance for monotone relationships but excel for various nonlinear dependencies. Noting that HSIC is a kernel-based method, this inspires us to seek a kernel-based method for conditional mean dependence.====We propose using the Hilbert-Schmidt norm of some operator to characterize the conditional mean dependence of ==== given ==== with the following steps. First, the condition of ==== is converted equivalently to a condition expressed with a family of finite signed measures on ==== consisting of functionals on ====, with a chosen kernel. Finally, a tensor operator from ==== to ==== is constructed and its Hilbert-Schmidt norm is used as a measure for the conditional mean dependence. We call it kernel-based conditional mean dependence (KCMD). Then, we construct a test based on KCMD to the hypothesis ==== is a special case of KCMD corresponding to the distance kernel described in ====. Furthermore, this tool can be applied to variable selection and model checking, as shown in Section ====, as well as other applications mentioned in Section ====. Through simulations, we find that the tests based on the KCMD measure have close performance for monotone relationships as that based on FMDD, but are superior for nonlinear relationships and relationships that the moment restriction ==== is violated.====The rest of the paper is organized as follows. In Section ====, we construct a test statistic based on the estimated KCMD for the conditional mean independence of ==== given ====, we explore the finite sample performance of the proposed tests by comparing with existing competing methods through simulations. In Section ==== we illustrate the applications of the proposed method in two real data examples. A discussion is given in Section ====. All the technical proof details are deferred to the appendix.====The proofs of ====, ====, ==== are of routine works, therefore we omit here; the proof of ==== can be done using the same paradigm as the proof of ====, thus it is also omitted. Note that ==== has a similar representation to the FMDD statistic in ====, the proofs of the asymptotic results can be carried out along the same lines as the proofs of the theorems in ====, but no strong moment conditions are needed here due to the boundedness of the kernel ====. For easy reading of the paper, we write down the details of the proofs in the following. They are deduced based on the limit theory of U-statistics.====We need some notations and Theorem 4 in ==== to finish the following proofs. Let ==== be a bootstrap statistic depending on ====. Define ==== if==== for any ====, where ==== denotes the conditional probability given ====. Similarly, we can define ==== as the conditional variance of ==== given ====. For convenience of reference, we list Theorem 4 in ==== as a lemma.",A kernel-based measure for conditional mean dependence,https://www.sciencedirect.com/science/article/pii/S0167947321000803,9 April 2021,2021,Research Article,229.0
"Wang Qin,Xue Yuan","Department of Information Systems, Statistics, and Management Science, The University of Alabama, Tuscaloosa, AL 35487, United States of America,School of Statistics, University of International Business and Economics, Beijing 100029, China","Received 2 May 2020, Revised 2 November 2020, Accepted 27 March 2021, Available online 8 April 2021, Version of Record 20 April 2021.",https://doi.org/10.1016/j.csda.2021.107241,Cited by (1),"Sufficient dimension reduction (SDR) is known to be a useful tool in data visualization and information retrieval for ====. Many well-known SDR approaches investigate the inverse conditional moments of the predictors given the response. Motivated by the idea of the aggregate dimension reduction, we propose an ensemble of inverse ==== to explore the central subspace. The new approach can substantially improve the estimation accuracy for the directions beyond the regression mean functions. A ladle estimator is proposed to determine the structural dimension of the central subspace. We further present two variable selection procedures to improve the ==== of the reduced variables. Both simulation studies and a real data application show the efficacy of the newly proposed method.","Since the seminal work of ==== and ==== be a univariate response and a ====-dimensional continuous predictors, respectively. The goal of SDR is to replace the ==== or ====. That is, SDR is to seek a subspace ==== with ==== such that ====where ====. The intersection of all such ==== if itself satisfies the above independence condition is defined as the ==== (CS), and is denoted by ====. The CS exists under very mild conditions (====, ====). The dimension of ====, denoted by ====, is called the structural dimension of the regression.====When the regression mean function ==== is of primary interest, ==== proposed the ==== (CMS), ====, as the smallest subspace ==== that satisfies ====
 ==== further generalized this idea to the central ====th moment subspace, ====, as ====As noted in ====, most regression models concern ====, the regression mean and variance functions. The central variance subspace, proposed by ====, is a special case of the central ====th moment subspace that targets on the regression variance function.====A main class of estimators of the CS is based on the inverse conditional moments of ====. Sliced inverse regression (SIR; ====) and sliced average variance estimation (SAVE; ====, while SAVE takes advantages of both the first ==== and the second ==== conditional moments. Under the so-called linearity and constant variance conditions, ==== can be recovered through the spaces spanned by the inverse mean and/or variance functions. Slicing the continuous response ==== is often used to facilitate the estimation. Despite the easy implementation in practice, both SIR and SAVE have their well-known limitations. SIR is known to fail if the regression is highly symmetric. ==== and ==== noted that the performance of SIR is not satisfactory when the CS is related to the regression variance function. Although SAVE is more comprehensive than SIR, its large sample properties and practical performance depends heavily on the number of slices (====). Furthermore, it has been noted that some relatively straightforward structure, such as a monotonic trend, that is manifested through ==== might be harder to detect for SAVE compared to SIR (====, ====). To overcome these limitations, the hybrid of various forms of the first two conditional moments was proposed, such as ====, ==== and ==== proposed an aggregate inverse mean estimator (knnSIR) that combines the local principal discriminant directions, which can break down the symmetric pattern and improve the estimation efficiency. Our practical experience showed that knnSIR also has difficulties in picking up the directions in the regression variance functions, similar to SIR, SAVE and DR. In this paper, we propose an ensemble of the local first two moments (ELF2M) estimator, which can effectively identify directions in both the regression mean and the variance functions. Compared to the aforementioned methods, ELF2M substantially improves the estimation accuracy with almost no increase in the computational burden in the models with heteroscedastic errors. The local aggregation inherited from knnSIR assures the ELF2M approach is more robust to the violation of the linearity condition. We further propose a ladle estimator for the determination of the structural dimension ====. A shrinkage procedure is incorporated to select important features for the interpretation of reduced variables.====The rest of this paper is organized as follows. Section ==== reviews SIR, SAVE as well as some of their hybrids. Section ==== introduces ELF2M, its estimation procedure and the ladle estimator for structural dimension. Section ==== to evaluate the efficacy of the proposed methods. Section ==== includes a real data application. Finally, Section ==== concludes the paper with a brief discussion.",An ensemble of inverse moment estimators for sufficient dimension reduction,https://www.sciencedirect.com/science/article/pii/S016794732100075X,8 April 2021,2021,Research Article,230.0
"Zhang Jing,Wang Qin,Mays D'Arcy","Department of Statistical Sciences and Operations Research, Virginia Commonwealth University, Richmond, VA 23284, United States,Department of Information Systems, Statistics, and Management Science, The University of Alabama, Tuscaloosa, AL 35487, United States","Received 20 August 2020, Revised 1 April 2021, Accepted 2 April 2021, Available online 8 April 2021, Version of Record 12 April 2021.",https://doi.org/10.1016/j.csda.2021.107247,Cited by (0), has been a significant feature in modern ," and ==== denote a univariate response and a ====-dimensional predictor vector respectively, the general goal is to explain the dependence between ==== and ====) when the ==== is large. Sufficient dimension reduction (SDR; ====; ====Let ==== be a dimension reduction subspace such that==== where ==== indicates independence and ==== in the standard inner product. Clearly, the dimension reduction subspace is not unique. ==== defines the intersection of all dimension reduction subspaces as the Central Subspace (CS) ====, and denotes the dimension of the CS as the structural dimension ====. When the conditional mean ==== is of primary interest, ==== proposed the central mean subspace (CMS) as the intersection of all subspaces satisfying ====.====. The most well known examples are the sliced inverse regression (SIR; ====) and the sliced average variance estimation (SAVE; ====). Both SIR and SAVE target the central subspace. The inverse regression approach is computationally efficient but generally requires certain probabilistic assumptions on ====. The forward regression approach studies the conditional distribution of ====. One popular method in this category is the minimum average variance estimation (MAVE; ====). MAVE estimates the central mean subspace ==== and ==== proposed density MAVE and Sliced Regression respectively, which extend the MAVE from the central mean subspace to the central subspace. ==== proposed an adaptive estimation procedure based on MAVE to improve the estimation efficiency when the error does not follow a normal distribution. ==== proposed a robust MAVE method (rtMAVE) where the least squares estimation is replaced by a local ====- or ====-estimation to guard against the influence of outliers. To take advantage of the efficient estimation of least squares formulation in the original MAVE, the rtMAVE requires a highly robust initial estimator, such as the one from the ====-regression as proposed in ====. In this study, we propose an alternative approach to achieve robust estimation for the central mean subspace. Motivated by the work of ==== and ====, ==== and ==== dealing with high leverage outliers in the predictor ====The rest of this paper is organized as follows. Section ==== provides a brief review of MAVE and the robust MAVE from ====. The details of our proposed regularized MAVE through penalized regression are presented in Section ==== and followed by the simulation studies in Section ====. In section ====, we apply our approach to the Boston Housing data. Section ==== concludes the paper with a short discussion. All the proofs are relegated to the Appendix.====C1. ==== is a stationary and absolutely regular sequence.====C2. ==== for all ====, ==== for all ====.====C3. The density function ==== of ==== has bounded fourth derivative and is bounded away from 0 in a neighborhood ==== around 0.====C4. The density function ==== of ==== has bounded derivative and is bounded away from 0 on a compact support.====C5. ==== has bounded, continuous derivatives.====C6. ==== and ==== have bounded, continuous third derivatives.====C7. ==== is a spherical symmetric density function with a bounded derivative. All moments of ==== exist.====The above conditions are imposed to facilitate the proof and similar to ====.",Robust MAVE through nonconvex penalized regression,https://www.sciencedirect.com/science/article/pii/S0167947321000815,8 April 2021,2021,Research Article,231.0
"Dong Ruipeng,Li Daoji,Zheng Zemin","International Institute of Finance, The School of Management, University of Science and Technology of China, Hefei, Anhui, 230026, China,Department of Information Systems and Decision Sciences, California State University, Fullerton, CA, 92831, United States of America","Received 15 August 2020, Revised 31 January 2021, Accepted 30 March 2021, Available online 7 April 2021, Version of Record 9 April 2021.",https://doi.org/10.1016/j.csda.2021.107243,Cited by (2),", we show that PEER enjoys nice sampling properties including consistency in estimation, prediction, and variable selection. Extensive simulation studies show that our proposal compares favorably with several existing methods in estimation accuracy, variable selection, and computation efficiency.","Multi-task learning has been widely used in various fields, such as bioinformatics (====; ====), econometrics (====, ====, ====, ====, ====, ====, ====, and the references therein. In particular, ==== and ====; ====), and ==== also considered extensions to incomplete outcomes, they did not provide the theoretical justification for the case with incomplete outcomes. In addition, the sequential steps in their procedure may result in the error accumulation. Alternatively, ====In this paper, we propose a new methodology of parallel integrative learning regression (PEER) for large-scale multi-task learning with incomplete outcomes, where both responses and predictors are possibly of high dimensions. PEER is a novel two-step procedure, where in the first step we consider a constrained optimization and use an iterative singular value thresholding algorithm to obtain some initial estimates, and then in the second step we convert the multi-response regression into a set of univariate-response regressions, which can be efficiently implemented in parallel.==== as well as the regression coefficient matrix, and accurately predict the multivariate response vector under mild conditions. To the best of our knowledge, there is no existing theoretical result on large-scale multi-response regression with incomplete outcomes. Our theoretical results are new to the literature.====The rest of this paper is organized as follows. Section ==== introduces the model setting and our new procedure PEER. Section ==== establishes non-asymptotic properties of PEER in high dimensions. Section ==== illustrates the advantages of our method via extensive simulation studies. Section ==== presents the results of a real data example. Section ==== concludes with some discussions. All the proofs are relegated to the Appendix.====To ease the presentation, we further introduce some notation which will be used later. Let ==== be the Frobenius inner product of two matrices ==== and ====. We also use ==== to denote the inner product of two vectors ==== and ====. For each ====, let ==== be the support of ==== where ==== is the ====th element of ====. In addition, define ====, where ==== is the estimation of ====. Hereafter we use ==== to denote a generic positive constant whose value may vary from place to place.====We first present two additional lemmas. These results can also be of independent interest. ====, ==== will be used in the proof of ==== while ==== will be used in the proof of ====. The proofs of all lemmas are provided in Appendix ====.",Parallel integrative learning for large-scale multi-response regression with incomplete outcomes,https://www.sciencedirect.com/science/article/pii/S0167947321000773,7 April 2021,2021,Research Article,232.0
"Kim Nam-Hwui,Browne Ryan P.","Department of Statistics and Actuarial Science, University of Waterloo, ON, Canada","Received 7 April 2020, Revised 29 March 2021, Accepted 1 April 2021, Available online 6 April 2021, Version of Record 8 April 2021.",https://doi.org/10.1016/j.csda.2021.107244,Cited by (1),"A finite mixture of factor analyzers is an effective method for achieving parsimony in model-based clustering. Introducing a penalization term for the factor loading can lead to sparse estimates. However, in the pursuit of sparseness, one can end up with rank-deficient solutions regardless of the number of factors assumed. In light of this issue, a new penalty-based method that can fit a finite mixture of sparse factor analyzers with full-rank factor loading estimates is developed. In addition, the extension of an existing penalized factor analyzer model to a finite mixture is introduced.","; ====; ====; ====. One such method is the use of factor analyzers and penalized likelihood, where the penalty is imposed on the factor loading to promote covariance sparsity. Contribution toward fitting a sparse factor analyzer includes (====; ====, ====; ====; ==== to a finite mixture of Gaussian factor analyzers for completeness of the literature. We will demonstrate both contributions' performance in real and simulated data settings.",In the pursuit of sparseness: A new rank-preserving penalty for a finite mixture of factor analyzers,https://www.sciencedirect.com/science/article/pii/S0167947321000785,6 April 2021,2021,Research Article,233.0
"Rodríguez Carlos E.,Walker Stephen G.","Department of Probability and Statistics, IIMAS-UNAM, Mexico,Department of Mathematics, University of Texas at Austin, USA","Received 29 July 2020, Revised 4 February 2021, Accepted 19 March 2021, Available online 3 April 2021, Version of Record 12 April 2021.",https://doi.org/10.1016/j.csda.2021.107230,Cited by (0),A novel analysis of the state space model is presented. It is shown that by modifying the standard recursive update it is possible to apply a ,"Let ==== be an observed process, whose realizations are assumed to be conditionally independent, given the states, i.e. ====and ====where we use the notation ==== and ====.====We assume for most of the paper that ==== and ====The two-stage model depicted in ====; image reconstruction in brain imaging methods such as electroencephalography, magnetoencephalography, e.g. ====; predictions for spread of infectious diseases, e.g. ====.====. Upon receiving ====The usual strategy to calculate ====, is based on a repeated application of a two-stage process:====From these steps, it is possible to recover ====From this all the current strategies for undertaking online prediction are based. The key is how to evaluate ==== from ====.====Numerous proposals have been made to provide algorithms that approximate the filtering densities. If the data can be modeled by a linear Gaussian SSM, it is possible to obtain exact analytic expressions to calculate the evolving sequence of filtering distributions of the type ====. Later, ====, ====, and ====. A popular algorithm under this setting is the auxiliary particle filter of ==== for the most recent tutorial covering recent methodological progress with particle filters.====, who obtained the recursive predictive density directly, circumventing the need for posterior integration. To achieve this with SSMs, we need to change the form of recursive update so we work with a different online version to ====. Indeed, we are unable to find a way to use copulas within the framework of ==== to ==== rather than for the usual update ==== to ====. Under this setting it is straightforward to include Gaussian, non-Gaussian elements and/or nonlinearity, obtaining competitive results with a straightforward algorithm. This strategy, is named the Copula Particle Filter (CPF). The motivation starts with the work of ==== who discussed online Bayesian prediction; our aim is to extend these ideas to the SSM setting.====The article is structured as follows. In Section ====, the CPF strategy is described and the algorithm is presented. Here we also describe how to deal with unknown parameters. In Section ==== we illustrate and compare our proposal against alternatives using simulated and real datasets. The article concludes with a discussion in Section ====.====The following is the Supplementary material related to this article. ",Copula Particle Filters,https://www.sciencedirect.com/science/article/pii/S0167947321000645,3 April 2021,2021,Research Article,234.0
"Liang Weijuan,Ma Shuangge,Lin Cunjie","Center for Applied Statistics, Renmin University of China, Beijing 100872, China,School of Statistics, Renmin University of China, Beijing 100872, China,School of Public Health, Yale University, New Haven, CT 06520, USA","Received 7 April 2020, Revised 21 September 2020, Accepted 23 March 2021, Available online 2 April 2021, Version of Record 16 April 2021.",https://doi.org/10.1016/j.csda.2021.107232,Cited by (1)," that involves moderate/high dimensional ==== has become common. Most of the existing analyses have been focused on estimation and variable selection, using penalization and other ","With the advancement of data collection techniques, data with a survival outcome and moderate/high dimensional ====To draw more definitive conclusions, inference is needed beyond estimation and selection. Being significantly more challenging, inference studies on survival data with moderate/high dimensional covariates under penalized (and other regularized) estimation have been limited. Examples include ====, ====, ====, ====, and others. As pointed out in ====, “====As summarized in ==== and other studies, existing inference techniques mostly take the fully conditional or pathwise conditional perspective. Under the fully conditional perspective, the goal is to quantify whether a covariate is independent of survival conditional on all other covariates (====). In contrast, under the pathwise conditional perspective, the goal is to quantify whether a covariate is independent of survival conditional on covariates that have already been included in the model (and concluded as relevant) (====, ====). More recently, the mFDR (marginal false discovery rate) strategy has been developed (====). Consider the schematic plot in ====. Among the three covariates, ==== is directly associated with survival and should be identified as significant. ==== is not associated and should be identified as insignificant. ==== may have some ambiguity: whether it is associated with survival depends on the status of ====. If ==== is not present, which is possible under the pathwise perspective, then ==== is significant. If ==== is present, as under the fully conditional perspective and some cases of the pathwise perspective, ==== is not significant. To avoid such ambiguity and draw “cleaner” conclusions, the goal of the mFDR technique is to screen out covariates like ====. For linear, generalized linear, as well as Cox and some other survival models, the theoretical validity and numerical advantage of the mFDR technique have been established (====, ====).====In this article, we consider survival data with moderate/high dimensional covariates. The goal is to further expand the scope of mFDR analysis and investigate its properties under the general transformation model. This is motivated by the appealing properties of mFDR and desirable robustness/flexibility of the transformation model for moderate/high dimensional data. We note that this research is more than a direct application of the mFDR technique. In particular, the existing mFDR studies are all based on M-estimation. By contrast, for the general transformation model, ====-estimation is needed, which is significantly more complicated than M-estimation. Accordingly, new and more challenging theoretical and numerical developments are needed. Overall, this study can provide a practically useful new inference approach for survival data with moderate/high dimensional covariates and is warranted beyond the existing literature.====As established in ====, ==== is consistent with ====. With Taylor expansion, ====
 ==== is a second-order degenerate ====-statistic, and it can be shown that ==== Under Condition (B0), we further have: ====and ====for ====. Together with ==== and ====, we have ====where ====.==== is a ====-statistic with kernel function ==== where ====. It can be rewritten as: ====We have ====, and ==== as ====. For the variance of ====, we have: ====Then ==== can be approximated by the U-statistic projection ====. Using the central limit theory of U-statistics (====), we have ====where ====. This completes the proof.  □",Marginal false discovery rate for a penalized transformation survival model,https://www.sciencedirect.com/science/article/pii/S0167947321000669,2 April 2021,2021,Research Article,235.0
"Wei Yuting,Wang Qihua,Duan Xiaogang,Qin Jing","Department of Statistics and Finance, University of Science and Technology of China, Hefei 230026, China,Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing 100190, China,Department of Statistics, Beijing Normal University,, Beijing 100875, China,National Institute of Allergy and Infectious Diseases, National Institute of Health3,, USA","Received 7 February 2020, Revised 22 October 2020, Accepted 6 March 2021, Available online 31 March 2021, Version of Record 12 April 2021.",https://doi.org/10.1016/j.csda.2021.107224,Cited by (1), given the covariable vector ==== is considered under the case where ,", ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ==== and ==== and ====. On the contrary, to the best of our knowledge, the literature on model selection with missing data is comparatively sparse because the complexity of missing data makes the direct application of the existing methods with complete data to incomplete data very challenging.==== proposed the predictive divergence for indirect observation models (PDIO) for model selection with incomplete data by taking the observed data log-likelihood as the goodness-of-fit part. In practice, sometimes a closed form of the observed data log-likelihood is infeasible because of intractable integrations. At this time, statisticians can seek help from the expectation–maximization (EM) algorithm (====) which is a popular technique for the missing data problems (====). ==== derived the observed data log-likelihood indirectly by using the EM algorithm. And then they employed this resulting observed data log-likelihood as the goodness-of-fit term of their proposed ==== criterion for model selection with missing data. Although the model selection by ==== criterion is consistent under some conditions, the practical application of ==== criterion is limited due to complex computations. For the consideration of computational simplicity, ==== further developed ==== criterion which is much easier for calculation than ==== criterion. However a potential drawback of ==== criterion, noted by ====, gives rise to the inconsistency of model selection. Concretely, such drawback of ==== criterion is that the candidate model, say ====, is used twice in the EM algorithm. At one time, model ==== further developed a procedure, named the E-MS algorithm, for model selection problem with missing data by drawing on the idea of the EM algorithm rather than the EM algorithm itself. As pointed out by ==== that the idea of their strategy extends the concept of parameters to include both the model and the parameters under the model, and thus allows the model to be part of the EM iterations.====Let ==== be the response variable and ==== the covariable vector. Both ==== and ==== are vectors while ====, where ====. However, as pointed out by ====, in many scientific areas, a basic task is to assess the simultaneous influence of several factors (covariates) on a quantity of interest (response variable). Particularly, ====. Therefore, in statistical analysis, researchers might only be interested in the selection of ==== rather than ====, where ==== given ====. In the following context, we use a generic label ==== to denote the true conditional probability model of ==== given ==== and ==== are available for each individual while ==== since ==== is missing.====This motivates us to propose new model selection criteria such that the consistency of model selection can be kept even if ==== or ==== is misspecified. Throughout this paper, we consider the case where ==== is missing at random (MAR) as in the literature, i.e., ====where ==== is the missing indicator variable of ====, taking value ==== if ==== is observed and ==== otherwise. Assume that a group of candidate parametric models are postulated for ==== and the number of the candidate models is finite. For candidate model ==== for ====, where ====). For candidate model ====, we denote the KL distance from ==== to ==== by ====where the expectation is taken with respect to ====. Since ====, the first term, is irrelevant to the candidate models, and hence the second term actually measures the closeness of ==== to ====. The larger the second term is, the closer ==== is to the probability function ====. We hence take ====, the second term, as the measure of how good the candidate model ==== fits the data and denote it by ====. Let ==== be the independent and identically distributed sample from ====. By the mean score estimating method (====), we obtain a mean score estimator of ====, denoted by ====, which is given by ====where the conditional expectation is taken with respect to ====. As what we mentioned previously, in practice, the parametric model for ==== is hard to be specified correctly. Even if a correctly specified parametric model for ==== were feasible, its form could be complicated which makes the corresponding method hard to be implemented because of the complicate integration calculation. Clearly, it is significant to develop robust model selection criteria to the misspecification of ====. Here, “robust” means that the model selection criterion is consistent even if the parametric model for ==== is misspecified.====Now let us state the basic idea of our proposed model selection criteria as follows. We first make a guess model for ==== and then calculate the mean score estimator ==== based on this guessed model. Next, we evaluate how biased this mean score estimator is from ==== and then get an inverse probability weighted estimator of this bias. With the estimator of the bias, we get a bias-corrected estimator of ==== which is denoted by ====. Finally, our proposed bias-corrected Kullback–Leibler distance (BCKL) criterion could be obtained by minimizing ====, of ==== can be obtained. Finally, the proposed ELBCKL criterion can be derived by minimizing ==== is misspecified.====The outline of the rest of the article is as follows. In Section ====, we develop BCKL criterion and ELBCKL criterion, and establish their theoretical properties in Section ====. In Section ====. Discussions are made in Section ====. All the technical details are delayed to ====, ====.====In this section, we give a brief description of how ==== criterion is calculated and the E-MS algorithm is implemented for our considered model selection problem. And meanwhile we give a more detailed illustration of the fact that, for our considered model selection problem, the consistency of the model selection by ==== criterion or the E-MS algorithm requires a correctly specified parametric model for ====.====In order to apply ==== criterion as well as the E-MS algorithm on our considered model selection problem, a parametric model, say ====, for ==== is required. With this parametric model, the ==== criterion in conjunction with BIC penalty for the problem considered in our paper is given by ====where ==== with ====and ==== being the estimator of ====. ==== is derived by the EM algorithm under model ==== and ==== is the complete-case-based MLE of ====. Denote ==== as log-likelihood of the observed data under model ==== which is equal to ====and let ==== be the maximizer of ==== with respect to ====. Then the goodness-of-fit part of ==== criterion, ====, is asymptotically equivalent to ==== if ==== is a correctly specified model for ====. Otherwise, the asymptotic equivalence fails which results in the inconsistency of the model selection by ==== criterion. Namely, ==== criterion is not robust to the misspecified parametric model for ====. Next, we devote ourselves to the illustration of the E-MS algorithm. From ====, we know that ==== is the mean score estimator of ==== based on the parametric model ==== where ==== is the second term of ==== in ====. From ====, one can see that if ==== is incorrect for ====, then ==== is a biased estimate of ==== and hence the model selection criterion with ==== being the goodness-of-fit part is inconsistent. The E-MS algorithm iteratively corrects this bias by treating ==== in ==== as a parameter and making it to be iterated. However, if ==== is misspecified, the E-MS algorithm still fails to possess the consistency of model selection. In other words, the E-MS algorithm is also not robust to the misspecified parametric model for ====. From formula ====, incorrectness of ==== or ==== leads to incorrectness of ====. This implies that our proposed BCKL criterion and ELBCKL criterion are robust to the misspecified parametric model for ==== or ==== since both criteria specify a model for ==== only. In light of this, we can say that, for our considered model selection problem, our proposals should be more reliable than ==== criterion and the E-MS algorithm.",Bias-corrected Kullback–Leibler distance criterion based model selection with covariables missing at random,https://www.sciencedirect.com/science/article/pii/S016794732100058X,31 March 2021,2021,Research Article,236.0
"Fan Caiyun,Lu Wenbin,Zhou Yong","School of Statistics and Information, Shanghai University of International Business and Economics, Shanghai 201620, China,Department of Statistics, North Carolina State University, Raleigh, NC 27695, USA,Key Laboratory of Advanced Theory and Application in Statistics and Data Science, MOE, Academy of Statistics and Interdisciplinary Sciences, East China Normal University, Shanghai 200062, China","Received 12 August 2019, Revised 24 December 2020, Accepted 16 February 2021, Available online 31 March 2021, Version of Record 16 April 2021.",https://doi.org/10.1016/j.csda.2021.107207,Cited by (1),"In censored linear regression, a key assumption is that the error is independent of predictors. We develop an ","), censored linear regression or accelerate failure time model (====) has been widely studied in survival data analysis. Many estimation methods were developed for censored linear regression, including Buckley–James estimator (====, ====), rank estimators (====, ====, ====, ====, ====) or the independence between the error and predictors in nonparametric regression (e.g. ====, ====, ====). Recently, ====Some regularity conditions that are assumed to prove the results in this paper are listing as follows.==== ==== ==== ==== Under condition(====1)–(====4), following the approach of ====, it can be easily shown that ====, where ====} for sufficiently small ====, and ==== is the asymptotic slope matrix of ====, defined in ====. This asymptotic linearity of ==== is essential to apply the method of ====. The estimation of ==== is similar.",Testing error heterogeneity in censored linear regression,https://www.sciencedirect.com/science/article/pii/S0167947321000414,31 March 2021,2021,Research Article,237.0
"Guo Chaohui,Lv Jing,Wu Jibo","School of Mathematical Sciences, Chongqing Normal University, Chongqing, 401331, China,School of Mathematics and Statistics, Southwest University, Chongqing, 400715, China,School of Mathematics and Big Data, Chongqing University of Arts and Sciences, Chongqing, 402160, China","Received 21 April 2020, Revised 20 March 2021, Accepted 20 March 2021, Available online 30 March 2021, Version of Record 7 April 2021.",https://doi.org/10.1016/j.csda.2021.107231,Cited by (0),. ==== including both extensive simulation studies and an empirical application are considered to verify the merits of our proposed approach.,"Let ==== be an independent identically distributed random sample from ====, where ==== is the response variable and ==== is the corresponding ==== given the ====-dimensional covariates ====, ====With rapid advances of computing power and other modern technology, our collected data frequently has an ultra-high dimensionality ==== that may diverge at nonpolynomial (NP) rate with the sample size ====, namely ==== for some positive constant ====. To the best of our knowledge, it is infeasible to obtain a satisfactory estimation of ====) or the spline method (====. For example, ==== introduced a sure independence screening (SIS) approach to screen out important predictors for linear regressions with Gaussian predictors and response, which can be seen as a milestone in ultra-high dimensional statistical analysis. ==== proposed a nonparametric independence screening approach for sparse ultra-high dimensional additive models to select variables by ranking a measure of the nonparametric marginal contributions of each covariate. ==== and ==== investigated feature screening methods based on the conditional correlation learning for sparse ultra-high dimensional varying coefficient models. More works have been done on ultra-high feature screening learning, see ====, ====, ====, ==== and ==== and references therein.====), ==== and ====, ====, ====, ====, ==== and ==== proposed a flexible semiparametric procedure called the “====” (MAMR) that is flexible for forecasting of time series. Under the framework of ultra-high dimensionality, ==== proposed a two-step semiparametric model averaging scheme by combining the popular feature screening technique and penalized MAMR, which can be regarded as an extension of ====. However, the aforementioned model averaging procedures are expected to be very sensitive to the outliers or/and heavy-tailed random errors, and then their efficiency may be significantly reduced for many commonly used non-normal errors. Since the seminal work of ====. In the third step, a penalized composite quantile model averaging marginal regression (PCQMAMR) is adopted to further select significant regressors and achieve a accurate prediction.====Compared with existing research findings, our work has the following innovations. On the one hand, our proposed NCQCS approach can robustly screen out truly important predictors, which can be taken as a useful extension of ==== and ====The rest of the article is organized as follows. In Section ====, we approximate ==== introduces the NCQCS procedure to reduce the dimension of the potential covariates and establishes its sure screening property. In Section ====, three simulation examples are used to verify the performances of feature screening and prediction of the proposed method. In Section ====, we apply the proposed method to analyze an ultra-high dimensional gene data. Some concluding remarks and discussion are provided in Section ====. Finally, technical proofs of the theoretical results are given in the Supplementary Material.====The following is the Supplementary material related to this article. ",Composite quantile regression for ultra-high dimensional semiparametric model averaging,https://www.sciencedirect.com/science/article/pii/S0167947321000657,30 March 2021,2021,Research Article,238.0
"Reynolds David,Carvalho Luis","Boston University, Department of Mathematics and Statistics, 111 Cummington Mall, Boston, MA 02215, United States of America","Received 8 September 2020, Revised 15 March 2021, Accepted 16 March 2021, Available online 27 March 2021, Version of Record 16 April 2021.",https://doi.org/10.1016/j.csda.2021.107229,Cited by (1),A novel approach to the problem of ,". FIM consists of discovering groups of items (itemsets) that are frequently purchased together by customers. Conventionally, the items that comprise frequent itemsets are assumed to have some inherent association.====Though this task was introduced for analyzing retail customer transaction data, it is now viewed more generally as a task of uncovering attribute values that systematically co-occur in a database. With this more general perspective, FIM has found diverse applications, such as bioinformatics (====Traditionally, FIM has been approached as an enumeration task. The goal is to enumerate all itemsets that meet some minimum support threshold, where the support of an itemset, ====, is the count of the number of transactions that contain ====. The difficulty of this task lies in the huge number of potential itemsets in the search space: if there are ==== items in a database, there are ==== were the first to achieve some efficiency in this task with their Apriori algorithm. Their algorithm identifies frequent individual items in the database and extends them to larger itemsets. That is, candidate itemsets of length ==== are generated from itemsets of length ====. The key observation for reducing the search space is that the support is a monotone measure. That is, for any itemsets X and Y such that ====, it follows that the support of ==== is at least as large as the support of ====. This has the implication that if an itemset is infrequent, all its supersets are also infrequent, and thus do not need to be explored. The Eclat algorithm (====), which produces the same output as the Apriori algorithm, is a depth first search algorithm that works on a slightly different representation of the transaction data that allows increased memory efficiency.====), H-Mine (====). However, as an enumeration problem, this limitation remains, especially for data in which the search space pruning techniques used by the aforementioned algorithms are less effective.====).====While these concise representations make the output more digestible, they do not necessarily increase the amount of useful information and they do not address difficult aspects of transaction data. Key among these is that co-purchases of items do not imply associations between them. When a transaction contains two items, say butter and soap, there is not necessarily a common motivation for the two purchases. Despite this lack of association, there may be many observed transactions with these two items. This large count of co-purchases (the output of FIM algorithms) may lead one to infer that this pair is associated when in fact there is no association, they both just happen to be popular items.====Our framework seeks to address this shortcoming by using a hierarchical model in which distinct sets of parameters control different aspects of transactions. In particular, we use distinct parameters to distinguish between an item’s propensity to be connected with other items in the association graph from its popularity. Additionally, we use Ewens distribution to control the number of ==== that are sampled in each transaction, with a separate set of parameters controlling the size (cardinality) of the sampled cliques. This allows us to distinguish between popularity and co-associations. For instance, returning to the previous example, although butter and soap may have a large number of co-occurrences, we may find from our graph estimate that the observed co-occurrence is driven by a set of common neighbors rather than association between the items.==== on chordal graphs, including some that are key to our inferential procedure, such as computing a minimum clique cover. This allows for an efficient, albeit approximate, inferential procedure.====The pseudo-code below gives the Apriori algorithm. The first pass of the algorithm simply counts item occurrences to determine the frequent 1-itemsets. A subsequent pass, pass ====, consists of two phases: first, the frequent itemsets in the previous step are used to generate candidate itemsets. The transaction database, ====, is then scanned and the support of each candidate is counted. Those that meet the user-specified minimum support are added to the set of frequent itemsets. ====This algorithm relies crucially on candidate generation. This is achieved by a sub routine consisting of a join step and a prune step. In the join step, say within pass ==== of the algorithm, new candidates, ====, consist of the union over pairs of itemsets from the prior pass, ====, that differ by only one item. In the prune step, delete all itemsets ==== such that some ==== subset of ==== is not in ====.====For example, let ====. In the join step, we obtain ====}. The prune step deletes ==== because the itemset ====.",Latent association graph inference for binary transaction data,https://www.sciencedirect.com/science/article/pii/S0167947321000633,27 March 2021,2021,Research Article,239.0
"Jones Matthew,Goldstein Michael,Randell David,Jonathan Philip","Department of Mathematical Sciences, Durham University, Lower Mountjoy, Stockton Road, Durham, DH1 3LE, UK,Shell Global Solutions International B.V., Grasweg 31, 1031 HW Amsterdam, Netherlands,Shell Global Solutions UK, Shell Centre, York Road, London, SE1 7NA, UK,Department of Mathematics and Statistics, Lancaster University, Lancaster, LA1 4YF, UK","Received 4 April 2020, Revised 5 March 2021, Accepted 13 March 2021, Available online 24 March 2021, Version of Record 17 April 2021.",https://doi.org/10.1016/j.csda.2021.107228,Cited by (0)," to propagate information efficiently between model components, lessening the computational demands associated with the inference. The proposed framework is illustrated through application to two examples: a model for the trajectory of an airborne projectile moving subject to gravity and ====, and a model for the coupled motion of a set of ringing bells and the tower which houses them.","An ==== (ODE) model implicitly defines a set of functions through specification of their derivatives with respect to a single input variable. ODE models are widely used in a range of different scientific fields: for example, in classical mechanics, Newton’s second law relates the forces acting on a collection of objects to their acceleration. The resulting ODE model can then be integrated in time to produce a description of the velocities and positions of the objects.====).====Recently, much effort has been devoted to development of methods which account for uncertainty about the solution of a system of differential equations. ====, ==== and later ==== lay some of the foundations for the field now known as probabilistic numerics. ====, ==== and ==== provide a review of the development of the field of probabilistic numerics, and highlight future research challenges. ==== and ==== are other useful reviews.====Authors following ==== propose probabilistic ODE solvers using Runge–Kutta means. ==== discuss convergence rates for Gaussian ODE filters. ==== investigate GP regression for linear multistep solution of ordinary differential equations, and provide proof of the convergence. ==== and ====. ==== apply probabilistic numerics to governing physical equations of industrial equipment, facilitating improved process monitoring.====Also relevant for the remainder of this article is the large literature on Bayesian uncertainty analysis for linking the output of complex computer models with the real-world systems that they are designed to represent. ==== and ====In this article, we develop a model which accounts for all important sources of uncertainty encountered when using a set of ODEs to represent the behaviour of a real-world system. The systematic variation of the numerical discrepancy as a function of the input state is accounted for, as is the structural discrepancy between the predictions generated from solution to the ODEs and the real-world system, along with any measurement error. Belief specification and inference for the model components can be carried out in either the fully probabilistic or Bayes linear settings; we consider both cases, favouring the computational simplicity of the Bayes linear analysis. We carry out an initial simulation for a limited number of points in the model input space, refining the solver grid and using the refined solution to quantify uncertainty in the numerical discrepancy as a function of the solver inputs. A ====The novel contributions of this article are: a) An inference framework for handling both numerical and structural discrepancies together in a coherent manner; b) Incorporation of systematic variation of the numerical discrepancy as a function of state, time-step and parameter setting; c) A scalable second-order (Bayes linear) framework for analysis in combination with a junction tree representation, enabling efficient local computation and therefore allowing application to larger problems.====The remainder of the article is structured as follows. In Section ====, we outline the structure of a general numerical scheme, and investigate the structure of the corresponding numerical discrepancy. Then, in Section ====, we introduce a general, graphical framework for linking uncertainties about each of the model components. In Section ====, we consider a more complex example, in which we account for discrepancies introduced through numerical solution of a coupled bell–tower model. Section ==== provides discussion and considers topics which could be the subject of future research in this area. Supplementary technical details are provided in the appendices.====This appendix summarises aspects of Bayes linear methodology required for the analysis performed in the main article. ==== details the extent of the prior specification that we must make, and details how our beliefs should be updated on learning the values of a set of quantities; ==== introduces the concepts of Bayes linear sufficiency and belief separation.",Bayes linear analysis for ordinary differential equations,https://www.sciencedirect.com/science/article/pii/S0167947321000621,24 March 2021,2021,Research Article,240.0
"Zheng Nan,Cadigan Noel","Centre for Fisheries Ecosystems Research, Fisheries and Marine Institute of Memorial University of Newfoundland, St. John’s, NL, Canada A1C 5R3","Received 14 August 2020, Revised 6 March 2021, Accepted 12 March 2021, Available online 23 March 2021, Version of Record 24 March 2021.",https://doi.org/10.1016/j.csda.2021.107227,Cited by (2),Measures of uncertainty are investigated for estimates and predictions using nonlinear mixed-effects models including state–space models in particular. These nonlinear mixed-effects models include fixed parameters and random effects. ====. In the ,") and have numerous applications in many fields including ecology, econometrics, engineering and environmental sciences (e.g. ====, ====). For example, state–space fish stock assessment models (====, ====, ====, ====, ====, ====) that integrate multiple sources of data related to stock productivity are increasingly being used and are considered by ==== to be an essential part of the next generation stock assessment package. Currently the most notable example is the SAM stock assessment package (====, ====) used by many working groups of the International Council for the Exploration of the Seas (ICES) (e.g. ====, ====). Recent versions of SAM are implemented in the Template Model Builder (TMB, ====) package within R (====, ====) can be even more easily implemented using the glmmTMB R package (====) which is built on TMB. Although the general focus of our research is on fish stock assessment model practice, the results of this paper are developed for a generic nonlinear mixed model framework.====The basic mixed-effects model and estimation approach are briefly described as follows, but more general reviews of linear mixed-effects models are provided by, for example, ====, ==== and ====, and additional details on nonlinear mixed-effects models as implemented very generically in TMB are available in ==== and ====. The random response data are collected in a ==== vector ==== and are assumed to have a multivariate probability density/mass function (pdf/pmf) ====, given values of the (====) vector of fixed-effects parameters ==== and the (====) vector of random-effects ====. The pdf of ==== is ====. In an integrated analysis setting the elements of ==== vector and do not develop additional notation for the responses which is consistent with the generic notation used in some of the literature (e.g. ====, ====, ====). The marginal distribution of ==== is ====where ==== are the elements of ====. For simplicity this ====-fold integral is denoted as ==== are those values ==== that maximize ====. Throughout this paper we use ==== to denote estimators. We can “estimate” ==== with the conditional mean ==== is unimodal and approximately symmetric about ==== then these estimates are equivalently those values ==== that maximize ==== when ====.====With ecological models, and in particular fish stock assessment models, the ==== parameters are usually not of direct interest and many of the ====, the random effects ====, and covariates are of direct interest. We generically denote such a function as ====. TMB provides standard errors of MMLE’s of ==== and also estimates of user-specified ====. This is explained further in Section ====. Both SAM and glmmTMB utilize this feature of TMB to calculate standard errors. ====, but the first Ref. (i.e. ====) does not mention the delta method while the second Ref. (i.e. ====) directly considered functions of only the random effects, ====. The frequentist variance refers to the variability of ==== derived from infinitely many data sets randomly drawn from ====. This will include repeated sampling of ==== from ==== and ==== from ====, we (2) provide a GDM approximation of ====, where ==== is a vector-valued function. We (3) clarify what is the statistical basis for the TMB GDM variance formula. We illustrate the utility of ====. We develop novel theoretical results without normal distributional assumptions for random effects, but we derive more amenable approximations for the normal distributional assumptions commonly found in statistical packages including TMB and ADMB (====, ====).====We first summarize the asymptotic properties of ==== which is used to derive ====. Since ====, a first-order Taylor’s series expansion of ==== at ==== gives ====
 ==== is commonly referred to in statistics as a score function. For any score function, under certain regularity conditions on the density function of the associated random variables, one can show that ====where ==== is the Fisher information matrix for ====. Eq. ==== is involved when showing that ====. For large sample sizes ====. If ==== is a normal distribution then ==== is exactly equal to ====. Hence, the approximation we use is ",Frequentist delta-variance approximations with mixed-effects models and TMB,https://www.sciencedirect.com/science/article/pii/S016794732100061X,23 March 2021,2021,Research Article,241.0
"Guadarrama María,Morales Domingo,Molina Isabel","Luxembourg Institute of Socio-Economic Research (LISER), Luxembourg,Operations Research Center, University Miguel Hernández de Elche, Spain,Department of Statistics, Universidad Carlos III de Madrid, Spain","Received 18 June 2020, Revised 10 March 2021, Accepted 10 March 2021, Available online 22 March 2021, Version of Record 3 April 2021.",https://doi.org/10.1016/j.csda.2021.107226,Cited by (1),None," describes the main SAE procedures until the publication date.====Statistical offices wish to publish regular statistics that are comparable and also stable over time, since highly unstable statistics have low credibility. By considering historical data in the estimation process and accounting for the potential time correlation, it is possible to obtain smoother estimates over time than those based solely on the cross-sectional data available at each time instant. Small area models accounting for time correlation have been previously considered by several authors. For example, at the area level, ==== considered a model with autocorrelated sampling errors. ==== and ==== studied models with time-varying random slopes, whereas ==== proposed a model with AR(1)-correlated area-time random intercepts. ==== proposed a slightly more complicated time correlated model, ====, ====, ==== and ==== introduced variants of the Rao–Yu model, ====, ====, ====, ====, ==== and ==== applied temporal area-level mixed models to poverty mapping, and, more recently, ==== and ====In the case of unit-level models, the literature is more scarce, perhaps because application of these models in practice requires historical unit-level information, which may not be always available, or not in a comparable form. Note that the methodology of the surveys is updated in some periods with possible changes in the survey design, questionnaire or even the survey collection mode, among others, making the surveys performed before and after the changes not comparable. Computational issues may also limit their applicability, since the size of the unit-level historical data sets to be processed can be really large. As examples of unit-level models, we can cite the space–time model employed by ====.==== proposed empirical best predictors (EBP) of non-linear parameters based on a unit-level model. Concretely, ====, ==== and ==== derived EBPs under two-fold NER models. The selection of the transformation is studied by ====. Instead of searching for an appropriate transformation to achieve normality, which might not be easy, another approach is to consider other distributions better suited for the data. The EBP methodology was extended by ==== and ==== to unit-level mixed models with skewed distributions or by ====This paper fills a gap in the current literature by introducing EBPs of domain parameters based on a unit-level temporal mixed model including area and area-time random effects with AR(1)-type time correlation. The paper is organized as follows. Section ==== introduces the proposed unit-level model for small area estimation in the presence of historical data. Section ==== deals with a particular class of domain parameters that includes poverty incidences, poverty gaps and average income, spelling out the corresponding EBPs for them. Section ====. Section ==== applies the proposed methodology to data from the Spanish survey on income and living conditions (SILC) from years 2004, 2005 and 2006, providing estimates of poverty incidences and gaps for women in the Spanish provinces. Section ==== draws some conclusions. ==== describes the derivations of the expressions in Section ==== and ==== includes detailed results from the application with Spanish SILC data and, finally, additional results are included in the Supplementary Material.",Time stable empirical best predictors under a unit-level model,https://www.sciencedirect.com/science/article/pii/S0167947321000608,22 March 2021,2021,Research Article,242.0
"Galvani Marta,Torti Agostino,Menafoglio Alessandra,Vantini Simone","Department of Mathematics, University of Pavia, Pavia, Italy,MOX, Department of Mathematics, Politecnico di Milano, Milano, Italy,Center for Analysis Decisions and Society, Human Technopole, Milano, Italy","Received 3 July 2020, Revised 21 February 2021, Accepted 28 February 2021, Available online 22 March 2021, Version of Record 12 April 2021.",https://doi.org/10.1016/j.csda.2021.107219,Cited by (6),"The problem of bi-clustering functional data, which has recently been addressed in literature, is considered. A definition of ideal functional bi-cluster is given and a novel bi-clustering method, called Functional Cheng and Church (FunCC), is developed. The introduced algorithm searches for non-overlapping and non-exhaustive bi-clusters in a set of functions which are naturally ordered in ==== through a non-parametric deterministic iterative procedure. Moreover, the possible misalignment of the data, which is a common problem when dealing with functions, is taken into account. Hence, the FunCC algorithm is extended obtaining a model able to jointly bi-cluster and align curves. Different simulation studies are performed to show the potential of the introduced method and to compare it with state-of-the-art methods. The model is also applied on a real case study allowing to discover the spatio-temporal patterns of a bike-sharing system."," and ==== for more details.====In this paper we consider the problem of bi-clustering functional data, a problem first theoretically introduced by ====In this paper we consider the problem in which each cell of the data matrix is a function and we would like to perform a bi-clustering of these functions to obtain similarity subgroups of rows and columns. To this purpose classical multivariate bi-clustering methods should be extended to deal with functional data. Many different methods have been proposed in the literature for clustering functional data, considering datasets where each observation is a function, see ==== for a complete survey on these models. In the bi-clustering framework, ==== and ==== both proposed a procedure which generalizes the classical latent block model (====) for multivariate data to the functional setting. These procedures are model-based and assume the existence of a latent-block structure in the data-matrix. ==== and ==== consider only the coefficients of the first functional PCA to represent each curve, therefore losing information, while ==== allow only for basis expansions as a smoothing procedure, while many other methods can be taken into account considering the different nature of the data at hand.====The aim of this work is to present a new flexible algorithm for the bi-clustering of functional data called ==== (FunCC) algorithm, extending the well known Cheng and Church algorithm, proposed by ====When dealing with functional data, another problem that has to be taken into account is the possible misalignment of the data (see for example (====) and ====) which acts as a confounding factor when trying to analyse the data. The problem of curve registration has been considered in literature by different authors. ==== consider self-modelling non-linear regression models to align curves, while ==== develop non-linear mixed effects models. Other works (see ====, ====, ==== and ====To show the benefits of the developed methodology, the FunCC algorithm is also applied on a real dataset, the bike sharing system (BSS) of Lyon. The aim is to provide useful information for the correct management of the service by discovering subgroups of stations and days with common operating patterns and highlighting potential issues of the BSS.====The paper is structured as follows: in Section ====. In Section ==== the more general case which allows for the functions registration step is presented. Different simulation studies are performed in Section ==== to underline the potential of the introduced algorithm and to compare it with state-of-the-art methods. In Section ==== the algorithm is applied on the BSS of Lyon. Conclusions are presented in Section ====. In ==== the original Cheng and Church procedure is reported.====Referring to the Cheng and Church model (====), given a data matrix ==== composed of ==== rows and ==== columns, a bi-cluster ==== is a set of rows ==== and a set of columns ==== such that each element ==== in the bi-cluster can be expressed as: ==== with ==== and ====, where ==== is the average value in the bi-cluster and ==== and ==== are respectively the residual value between row and column average value and the bi-cluster total average value ====. In detail:====The mean squared residual score of a bi-cluster ==== is expressed as: ====where ====. To find a set of bi-clusters in the data, a deterministic and greedy approach is employed returning as output bi-clusters having the maximal dimension in terms of number of rows and columns according to the minimization of a mean squared residual score ====. Specifically, a sub-matrix ==== is a bi-cluster if its ====-score is smaller than a given threshold ==== taken as input parameter by the algorithm. The aim of the algorithm is to find a maximal sub-matrix with a low H score. The Cheng and Church algorithm can be considered as a three-step procedure as expressed in Algorithm 3. Initially a nodes deletion step is performed removing rows and columns of the data matrix to minimize the mean squared residual ====. Then the result of the deletion is modified by adding nodes which do not impact on the score, obtaining as output the maximal bicluster below the chosen threshold ==== is identified. Details of Multiple and Single Node Deletion and Nodes Addition procedures are reported in Algorithm 4, 5 and 6. In detail for the Multiple Node Deletion step, Algorithm 4, a new parameter ==== is considered and the rows and columns with scores beyond a value identified by the threshold ==== are removed. This procedure is very fast but may return too much shrunk matrices. The Single Node Deletion step, in Algorithm 5, is instead a lower procedure deleting one node at the time. While ==== is bigger than the threshold ==== and other rows and columns can be removed, the algorithm proceeds evaluating the rows and columns scores and removing the row or the column with the higher score, i.e. the one which largely contributes to the score H.====After the deletion phase, the resulting bi-cluster may not be maximal; hence an addition step is performed, trying to add all the rows and the columns that do not increase the score H. In this step the procedure tries to add also the anti-correlated or inverted rows. This step was introduced because the original Cheng and Church algorithm was developed for gene expression data. An anti-correlated or inverted row in a gene expression data may indeed represent negatively regulated genes that are of interest when finding a bi-cluster.====Finally the algorithm is iterated without considering the results already found; to do so a masking procedure is performed. This masking procedure consists in replacing all the elements in the matrix already assigned to one bi-cluster with random values. This makes quite unlikely that already assigned elements would be reassigned to other bi-clusters, but it does not ensure it. ====
 ====
 ====
 ",FunCC: A new bi-clustering algorithm for functional data with misalignment,https://www.sciencedirect.com/science/article/pii/S0167947321000530,22 March 2021,2021,Research Article,243.0
Jin Lei,"Department of Mathematics and Statistics, Texas A&M University - Corpus Christi, Corpus Christi, TX 78412, USA","Received 16 July 2020, Revised 4 March 2021, Accepted 5 March 2021, Available online 18 March 2021, Version of Record 27 March 2021.",https://doi.org/10.1016/j.csda.2021.107223,Cited by (1),Statistical comparison of time series is useful for the detection of mechanical damage and many other real-world applications. New methods have been proposed to check whether two semi-stationary time series have the same normalized dynamics. The proposed methods differ from traditional methods in that they are based on the Laplace ,", ====, ====, ====, ====, ====, ====, ====, ==== and others. For comparing stationary time series that are dependent on each other, the methods include ====, ====, ==== and ====. In addition, methods such as ====, ==== and ==== can be used to compare non-stationary time series. Because it is easy to obtain a consistent variance estimate when a time series is stationary, there are no major differences between methods to compare the spectral densities or the normalized spectral densities for stationary time series. However, the situation could be different when the processes are non-stationary because the local normalization is complicated in general.====, ====; ====, ====). Recently, with the periodogram, ====, ==== have also proposed some semi-parametric Whittle likelihood approaches for complex statistical models with dependent errors. The traditional periodograms are related to ====, ====, ==== and ====In this paper, new methods based on the Laplace periodogram are proposed to compare two semi-stationary time series in terms of their normalized dynamics. The proposed methods can be useful for damage detection, especially when the vibration signals may not be stationary. If the variance of a process is not finite, the existing methods do not work because the traditional spectral density cannot be defined. By inheriting some of the nice properties of the Laplace periodogram, the proposed methods are capable of dealing with heavy-tailed processes whose variances may not exist. According to ====, the Laplace periodogram is connected to the zero-crossing information of a process. Therefore, the proposed methods naturally work for semi-stationary processes whose nonstationarity is only due to the time-varying magnitude. When two processes are not independent, it can be relatively difficult to obtain the likelihood functions. The proposed methods rely on the method of estimating equations and some empirical variance estimation so that they can be used to compare two dependent processes without the likelihood information.====The paper is organized as follows. Section ====. In Section ====, the proposed methods are applied for damage detection of a mechanical system based on vibration data. Concluding remarks are given in Section ====.",Robust tests for time series comparison based on Laplace periodograms,https://www.sciencedirect.com/science/article/pii/S0167947321000578,18 March 2021,2021,Research Article,244.0
"Wang Kangning,Li Shaomin","School of Statistics, Shandong Technology and Business University, Yantai, China,Center for Statistics and Data Science, Beijing Normal University, Zhuhai, China,Guanghua School of Management, Peking University, Beijing, China","Received 15 January 2020, Revised 5 March 2021, Accepted 6 March 2021, Available online 18 March 2021, Version of Record 23 March 2021.",https://doi.org/10.1016/j.csda.2021.107225,Cited by (22),"Modal regression is a good alternative of the mean regression and likelihood based methods, because of its robustness and high efficiency. A robust communication-efficient distributed modal regression for the distributed massive data is proposed in this paper. Specifically, the global modal regression objective function is approximated by a surrogate one at the first machine, which relates to the local datasets only through gradients. Then the resulting estimator can be obtained at the first machine and other machines only need to calculate the gradients, which can significantly reduce the communication cost. Under mild conditions, the asymptotical properties are established, which show that the proposed estimator is statistically as efficient as the global modal ====. What is more, as a specific application, a penalized robust communication-efficient distributed modal regression variable selection procedure is developed. Simulation results and real data analysis are also included to validate our method.","In the past decade, massive data has drawn dramatically increasing attention all over the world. Because the exceedingly large size of data often makes it impossible to handle all of them on a single machine, the dataset must be stored and processed on many connected computers. Thus it fails to deliver ==== by standard algorithms or statistical packages, which is largely due to the limited storage space in primary memory.==== blocks and fits the target model block by block, then averages the ==== fitted models to form a final one. For example, ==== averaged the M-estimators obtained by node machines; ==== averaged debiased estimators; and ==== defined an average for subspaces and computed it via eigen-decomposition. For more references about such methods, one can see ====, ====, ====, ====, ==== and ==== and the references therein. Although this approach is highly efficient in terms of communication, it might not achieve the best efficiency in statistical estimation in most occasions (====, ====, ====, ====, ====).====However, the aforementioned methods are mainly built on mean regression or likelihood framework, which are not robust and can be adversely affected by outliers or heavy-tail distributions. Modal regression, discussed in ====, ==== and ====, is a good alternative of the mean regression and likelihood method. It can achieve balance between robustness and high inference efficiency by choosing an appropriate tuning parameter. For more recent research about modal regression, one can see ====, ====, ====, ====, ====, ====, and so on.====The above considerations motivate us to develop a robust communication-efficient distributed modal regression for the distributed massive data, which can remedy the defects of the mean regression or likelihood-based methods. Specifically, we first approximate the global modal regression objective function by a surrogate one, which relates to the local datasets only through their gradients. Then the resulting estimator can be obtained by maximizing the surrogate global modal regression objective function at the first machine, and other machines only need to calculate gradients of local datasets. This can significantly reduce the communication cost. Under mild conditions, the asymptotical properties are established, which show that the proposed estimator is statistically as efficient as that obtained on the entire dataset.====In practice, we often have many variables, and some may be irrelevant. Thus, variable selection is necessary. Recently, shrinkage-type variable selection methods have seen increasing applications, which include but not limited to the nonnegative garrotte (====, ====), the LASSO (====), the adaptive LASSO (====), the bridge regression (====) and the SCAD (====). However, there are only a few papers about variable selection for the distributed massive data. For example, ==== proposed a DC penalized likelihood method; ==== investigated the distributed high-dimensional sparse regression by combining local debiased ==== estimates; and ==== revisited the same problem but further considered distributed testing and estimation methods in a unified likelihood framework. However, we can see that, these existing methods are also mainly built on mean regression or likelihood framework, which are not robust.====Based on the proposed robust communication-efficient distributed modal regression and the penalty functions, as a specific application, we further develop a robust communication-efficient distributed variable selection procedure, which can also be implemented at the first machine. Theoretically, the variable selection procedure works beautifully, including the consistency in variable selection and oracle property in estimation. By inheriting the properties of the robust communication-efficient distributed modal regression, the proposed variable selection procedure is also robust and works well just as all data were pooled on a single machine.====The rest of this paper is organized as follows. Section ==== introduces the robust communication-efficient distributed modal regression, related algorithm and asymptotical properties. The variable selection procedure is given in Section ====. Simulation results and real data analysis are reported in Section ====. Concluding remarks are discussed in Section ====. All the technical proofs are provided in ====.",Robust distributed modal regression for massive data,https://www.sciencedirect.com/science/article/pii/S0167947321000591,18 March 2021,2021,Research Article,245.0
"Qiu Tao,Xu Wangli,Zhu Liping","Center for Statistics and Data Science, Beijing Normal University, Zhuhai, 519087, China,Center for Applied Statistics, School of Statistics, Renmin University of China, Beijing, 100872, China,Center for Applied Statistics and Institute of Statistics and Big Data, Renmin University of China, Beijing, 100872, China","Received 5 August 2020, Revised 1 March 2021, Accepted 2 March 2021, Available online 16 March 2021, Version of Record 30 March 2021.",https://doi.org/10.1016/j.csda.2021.107218,Cited by (4),", ignoring the ==== of the random samples and the relations between data dimensions and sample sizes. Extensive simulations show that the power performance of the proposed test is encouraging compared with some existing methods.","Suppose ==== are two random samples drawn independently from a ====-dimensional distribution ====, for ==== respectively. Let ==== and ==== be the mean and the covariance of ====. In this article, we consider the problem of testing the hypothesis ==== where ==== and ==== are the sample means, and ==== (====). However, the classic Hotelling’s ==== test can be adversely affected, even when the sample dimension is larger than the sample size, if the sample covariance matrix is nearly singular (====).====To overcome the singularity problem in Hotelling’s ==== test, many studies have investigated the multivariate (“fixed ====”) and high-dimensional (“divergent ==== in ==== is replaced by a known quantity or another estimate. ==== and ==== constructed a test statistic by removing the sample covariance matrix, ====, in ====. ====, ====, and ==== propose a test statistic by replacing ==== with the inverse of the diagonal of ====. In these tests, asymptotic null distributions are derived when ====, ==== for some ====, or ====. Thus, these works all require that the dimension not be too large relative to the sample size. To allow for simultaneous testing of ultra-high-dimensional data, ==== propose a U-statistic that is constructed by removing the cross-product terms ==== and ==== in ====. This ensures the asymptotic null distribution is standard normal, regardless of the relationship between ==== and ====, when both the sample sizes and dimensions of the two random samples diverge to infinity. As stated above, all the aforementioned tests are essentially based on versions of Hotelling’s ==== test with diagonal estimators of ====).====, ====, ====, and ====. These methods apply Hotelling’s ==== and ==== are derived under the assumption of data normality. In addition, it has been observed in ==== that the ====-values of ==== obtained from the asymptotic null distribution are highly conservative for finite sample sizes. Furthermore, the asymptotic null distribution of ==== and ====, ====, ====, and ==== propose two-sample tests based on interpoint distance. These tests do not require the normal assumption nor the condition that special structures of the covariance matrix, and also do not need to set the test parameters. They prove that interpoint distance based tests are very effective in practice when analyzing high-dimensional data. Notably, most of these tests are combined tests and permutation approach is used to decide critical values. Furthermore,  ==== summarize a very general framework for these combined tests.====We propose a new two-sample test of ==== in a high-dimensional setting. This test involves randomly selecting a low-dimensional subspace of ==== and ==== when ====. Therefore, it is much more appealing as the data is deviated from normal distribution in many real applications. Further, no re-sampling procedure is required to approximate the asymptotic null distribution, which is computationally allowed in the proposed test to handle high- or ultra-high-dimensional data sets. Last but not least, the tests of ==== from the first class and ====.====. Extensive simulation studies are conducted in Section ==== to demonstrate the power performance of our proposed test and to compare it with many existing tests. The empirical studies indicate that our test outperforms competing tests in the parameter regimes anticipated by our theoretical results. Finally, we conclude the paper with a brief discussion in Section ====. All technical details are relegated to ====.====In the Appendix, we present the interpretations of Condition (C3) and proofs of ====, ====, ====.",Two-sample test in high dimensions through random selection,https://www.sciencedirect.com/science/article/pii/S0167947321000529,16 March 2021,2021,Research Article,246.0
"Kang Xiaoning,Wang Mingqiu","International Business College and Institute of Supply Chain Analytics, Dongbei University of Finance and Economics, Dalian, China,School of Statistics, Qufu Normal University, Qufu, China","Received 9 April 2020, Revised 20 February 2021, Accepted 2 March 2021, Available online 15 March 2021, Version of Record 22 March 2021.",https://doi.org/10.1016/j.csda.2021.107220,Cited by (4),. The merits of the proposed model are illustrated by the ==== and two genetic disease data.,"The ====Additionally, in traditional applications, the number of variables ==== is fixed and is much smaller than the sample size ====. Nowadays, because of the advanced data collection technologies, a large amount of high-dimensional data with large ==== and small ====, a sample of 128 females is examined to determine whether they have breast cancer based on ==== is close to ====. Even worse, it is not positive definite when ====. To accommodate the estimation of a covariance matrix in the high-dimensional cases, scholars make much effort as given in the literature. For example, ====, which is easy to implement and statistically meaningful. However, the estimation from the MCD depends on the variable orderings in the sense that different variable orderings would lead to different matrix estimates (====, ====). However, in most other scenarios, there is no pre-knowledge on the variable ordering before analysis, or the real data do not make sense with a variable ordering such as gene expressions, stock returns, medical data and industry data. Therefore, some works contribute to determine an ordering via certain criteria first, and then employ the MCD for estimating a covariance matrix. ====Another way to solve the variable ordering issue is the model averaging idea, which relaxes the variable ordering assumption. ==== considered an ensemble estimation by taking average on a set of covariance matrix estimates, which are obtained from a set of different variable orderings via the MCD approach. ==== extended ====’s ==== studied a sparse model averaging covariance estimation based on the MCD by imposing the ====The rest of paper is organized as follows. Section ==== reviews the latent variable version of the MCD technique. Sections ====, ==== detail the proposed model and provide the asymptotical convergence rate, respectively. The numerical simulation is conducted in Section ====. Analysis of two real data examples is presented in Section ==== before we conclude the paper in Section ====.",Ensemble sparse estimation of covariance structure for exploring genetic disease data,https://www.sciencedirect.com/science/article/pii/S0167947321000542,15 March 2021,2021,Research Article,247.0
"Zhang Tonglin,Lin Ge","Department of Statistics, Purdue University, 250 North University Street, West Lafayette, IN 47907-2066, USA,Department of Environmental and Occupational Health, University of Nevada Las Vegas, Las Vegas, NV 89154, USA","Received 9 August 2020, Revised 28 February 2021, Accepted 28 February 2021, Available online 10 March 2021, Version of Record 13 March 2021.",https://doi.org/10.1016/j.csda.2021.107217,Cited by (8),"Generalized ====-statistic as the dissimilarity measure, a generalized ==== is unknown, then the proposed method can be combined with generalized liformation criterion (GIC) to automatically select the best ==== for clustering. Both AIC and BIC are investigated as special cases of GIC. Theoretical and simulation results show that the number of clusters can be correctly identified by BIC but not AIC. The proposed method is applied to the state-level daily COVID-19 data in the United States, and it identifies 6 clusters. A further study shows that the models between clusters are significantly different from each other, which confirms the result with 6 clusters.","Generalized ====-means, including both ====-means and ====-means. The current research develops the method and uses it to group the patterns for the state-level daily confirmed cases of COVID-19 in the United States.====The outbreak of COVID-19 has become a worldwide ongoing pandemic since March 2020. According to the website of the World Health Organization (WHO), until January 31 2021, the outbreak has affected over ==== countries and territories with more that ==== million confirmed cases and ==== million deaths in the entire world. The most serious country is the United States. It has over ==== million confirmed cases and ====-means. We adopt the likelihood ratio or ====-statistic because it is induced by the standard uniformly most powerful unbiased (UMPU) test for exponential family distributions. Based on theory of the UMPU test, the proposed method should be more powerful than the convenient method based on ====-means directly on regression coefficients. This is confirmed by our simulation studies.====, it provides ==== clusters according to ==== centers. The ====-means can be replaced by any similarity or dissimilarity measure, leading to the generalized ====-means (====, ====). Because the choice of the dissimilarity measure is flexible, generalized ====-means can be combined with any divergence measure, including the UMPU test statistics.====), density-based clustering (====, ====). It can be further specified to ====-means (====, ====, ====, ====), ====-medians (====), and ====-modes (====), where ====-means is the most popular. To implement those, one needs to express observations of the data in a metric space, such that a distance measure can be defined. Several approaches have been developed to specify the distance measure. A review of these can be found in ====, p. 670.====Challenges appear in grouping daily patterns for the state-level COVID-19 data in the United States. Suppose that the daily patterns have been fitted by statistical models (e.g., GLMs) with the response as daily confirmed cases and explanatory variables as certain functions of time. The interest is to know whether models for individual states can be grouped into a few clusters. At least, two other methods can be used. The first is the direct usage of an existing clustering method on estimates of coefficients. A concern may arise because it is hard to address variability in estimates of coefficients. The second is the usage of mixture models, which often leads to EM algorithms for mixture structures (====). Here, we propose another method. We use a likelihood ratio or an ====-statistic as the dissimilarity measure in the generalized ====-means. Because they are formulated by the UMPU test, the resulting method should be more powerful than any other method theoretically. To verify this, we compare our method with the other two methods by simulation studies. We find that our method has lower clustering object error (OE) rates than our competitors.====We propose our method based on a known ==== at the beginning. When ==== is unknown, we use GIC to select the best ====-means on the slopes only, leading to two clusters. Based on our intuition, we believe that the unsaturated clustering problem can also be carried out by mixture models with EM algorithms. Because our method is developed based on the UMPU test, it should be more powerful than any other methods.====The article is organized as follows. In Section ====, we propose our method. In Section ====, we study theoretical properties of our method. In Section ====, we evaluate our method with the comparison to a few previous methods by simulation studies. In Section ====, we implement our method to the state-level COVID-19 data in the United States. In Section ====, we provide a discussion.",Generalized ,https://www.sciencedirect.com/science/article/pii/S0167947321000517,10 March 2021,2021,Research Article,248.0
"Huang Xianzheng,Zhang Hongmei","Department of Statistics, University of South Carolina, Columbia, SC 29208, USA,Division of Epidemiology, Biostatistics, and Environmental Health, School of Public Health, University of Memphis, Memphis, TN 38111, USA","Received 12 March 2020, Revised 19 February 2021, Accepted 24 February 2021, Available online 8 March 2021, Version of Record 20 March 2021.",https://doi.org/10.1016/j.csda.2021.107209,Cited by (1),"-values to quantify the statistical significance of the tests. Operating characteristics of these testing procedures are investigated using synthetic data in simulation experiments. Additionally, the proposed methods are applied to flow cytometry data from a designed experiment, and data of bile acids from an observational study in the Alzheimer’s Disease Neuroimaging Initiative.",", ====, ====, ====, ====Applications of Bayesian networks include profiling gene maps in genetics studies, predicting a treatment outcome in the medical field, and conducting financial analysis in econometrics, among many other examples. In particular, identifying differential Bayesian networks is of great interest to many domain scientists. For example, comparing a cellular signaling network associated with a diseased population and its counterpart network associated with a healthy population can provide insight on the impact of the disease on the concert work of relevant cells. In this article, we develop methods to infer whether or not a Gaussian Bayesian network is differential between two populations. The study presented here distinguish from existing relevant works in at least three aspects. First, we test differential directed networks as opposed to differential undirected networks, with a sizable collection of existing literature on the latter yet very limited research on the former. For instance, ==== tested multivariate two-sample means associated with graphs of known structures using Hotelling’s ====-tests. ==== extended their work in order to globally test differentiation of undirected graphs. ==== also developed methods for testing differentiations of undirected graphs based on precision matrices. ==== studied associations of undirected networks with a feature of interest. ==== developed a general framework for testing differential connectivity between two undirected networks, which aims to qualitatively compare structures of two precision matrices instead of quantitatively comparing entries of them. This highlighted feature in ==== relates to the second aspect that makes our methods stand out from existing works, which is that our proposed testing procedures can test differentiation in regard to solely graph structure, or strengths of associations between variables, or both. Third, the proposed inference procedures are applicable to data from an observational study or from designed experiments. Drawing inference for a Bayesian network can be more challenging than for an undirected network because a Bayesian network can be identified based on data from an observational study only up to a Markov equivalence class (====, ====, ====, ====, ====To prepare for methodology development, we first formulate regression models that characterize a Bayesian network in Section ====. Section ==== presents an algorithm for inferring a network based on penalized score equations. Test statistics based on quadratic inference functions are proposed in Section ==== to estimate the null distribution of the test statistics. Operating characteristics of the testing procedures are investigated via simulation experiments reported in Section ====. The proposed methods are applied to two real-life applications in Section ====. Lastly, we summarize the contribution of the current study and discuss follow-up research agenda in Section ====.",Tests for differential Gaussian Bayesian networks based on quadratic inference functions,https://www.sciencedirect.com/science/article/pii/S0167947321000438,8 March 2021,2021,Research Article,249.0
"Kirkby J. Lars,Leitao Álvaro,Nguyen Duy","School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, GA 30318, United States,Department of Mathematics, University of A Coruña, Campus de Elviña s/n, 15071, A Coruña, Spain,CITIC research center, University of A Coruña, Campus de Elviña s/n, 15071, A Coruña, Spain,Department of Mathematics, Marist College, Poughkeepsie, NY 12601, United States","Received 26 May 2020, Revised 9 February 2021, Accepted 12 February 2021, Available online 5 March 2021, Version of Record 17 March 2021.",https://doi.org/10.1016/j.csda.2021.107202,Cited by (7),"A general and efficient nonparametric density estimation procedure for local bases, including B-splines, is proposed, which employs a novel statistical Galerkin method combined with basis ","Nonparametric density estimation is an important research area in ====, ====, ====, ====, ====, ====, ====, ====, among many other important works. Some early reviews of the field are provided by ==== and ====, as well as the more recent work (====), change-point detection (====).====, which introduces a now state-of-the-art procedure for kernel density estimation, which we will use as a benchmark for numerical comparison due to its impressive and robust performance. In addition to kernel density estimators, orthogonal sequence estimators have also been widely adopted, including ====, ====, ====, ====, ====, ==== and more recently ====, ====. Similar approaches using wavelet estimators are also common, see ====, ====, ====, ====, ==== and ====.====In many applications, the ==== nature of orthogonal sequence estimators becomes a limitation, and ==== density estimators using uniform B-splines have been proposed in ====, ==== and ====. These estimators are ==== in the sense that each B-spline basis element is compactly supported, and the effect of any individual data point on the estimator is highly localized to a small set of neighboring basis elements. Related spline-based techniques have also been studied, such as logsplines (====, ====, ====, ====), smoothing splines (====, ====), penalized B-splines (P-splines) (====), and shape-constrained splines (====).====, ====, ====, ====, ====). For example, in ====, the authors derive closed-form expressions for various portfolio risk measures, such as value at risk (VaR) and expected shortfall, due to the tractability==== of the B-spline basis. Similarly, in ====, ====, ====). The advantage of using such bases in many applications such as these is facilitated by the tractability of the B-splines. Advances in estimation methodology are therefore valuable from both a theoretical and a practical perspective.====Galerkin methods are well-known and powerful techniques for solving (partial) differential equations, see ==== and ====, where ==== is the sample size and ==== the number of basis elements. By comparison, the complexity of least-squares is ==== to form the Gram matrix ====, and ==== to compute the LU (or Cholesky) factorization, at a total cost of ====. The Empirical Characteristic Function (ECF) based approach of ==== requires ==== to compute the ECF, and ==== to compute the coefficients, at a total cost of ====.====, which allows us to efficiently determine the bandwidth. Note that, as shown in ====, the optimal number of linear basis elements (as determined by the theoretically optimal bandwidth) grows at a rate of ====, which is dominated by ====, and similarly for other bases. Hence, the overall complexity is just ====, see for example ====, ==== and ====. Especially for large samples, the cost savings can be substantial. In summary, the contributions of this work are as follows:====The remainder of the paper is organized as follows: Section ==== introduces the Galerkin density estimator, and provides several examples using B-spline bases. Section ==== establishes the main theoretical results, including the unbiasedness and consistency properties of the estimator, and asymptotic normality is also demonstrated. Section ==== is concerned with the bandwidth selection of the proposed estimator. By utilizing a closed-form expression for the dual basis, we are able to derive the least squares cross validation formula in closed-form, which allows us to efficiently determine the optimal bandwidth. Various numerical experiments are considered in Section ====. Section ==== concludes the paper.",Nonparametric density estimation and bandwidth selection with B-spline bases: A novel Galerkin method,https://www.sciencedirect.com/science/article/pii/S0167947321000360,5 March 2021,2021,Research Article,250.0
"Park Seyoung,Lee Eun Ryung","Department of Statistics, Sungkyunkwan University, Republic of Korea","Received 25 September 2020, Revised 21 December 2020, Accepted 12 February 2021, Available online 4 March 2021, Version of Record 19 March 2021.",https://doi.org/10.1016/j.csda.2021.107204,Cited by (1),", as a function of the quantile level, ====, where ==== is the quantile region of interest and ==== is a domain of ====, and utilize composite quantile regression to estimate the parameters. Furthermore, we develop constrained composite quantile regression to provide a more efficient estimate in case the null hypothesis is not rejected. We show that the proposed test admits normal approximations. Using simulation and real data analysis, we demonstrate the superiority of the proposed test over other tests designed for a finite number of quantile levels.",", ====). In quantile regression analysis, it is important to assess the significance of regressors to make rational decisions regarding their effects on the response. In this respect, Wald- and score-type tests (====), and a score-type test that considers multiple quantiles to assess the significance of regressors (====) have been proposed.====, ====, ====, ====, ====, and ==== and ====.====VC quantile regression assumes that, for a given quantile, ====, ====Here, ==== is the ====th conditional quantile of response ====, given the ====, and an index variable, ====; ====. Furthermore, ==== is a ====-dimensional unknown functional vector that depends on the quantile level, ==== and ====, ====, ====, ====, ====).====). ==== considered a general hypothesis-testing problem using a rank-reducible structure under fixed ====-asymptotics in VC quantile regression. Most related studies considered a single quantile level, ====, or a finite number of quantile levels of interest when testing with the VC quantile regression model. For testing over an interval of quantiles, ==== exploited either Wald or the Rankscore process under fixed ====-asymptotics in the linear quantile model. ==== proposed a score test for a given quantile region in high dimension. To the best of our knowledge, inference on a regional quantile, i.e., an interval of quantile levels, for VC quantile regression has not been considered thus far.====The incorporation of an interval of quantile levels rather than a single or, more generally, finite quantile levels in the VC quantile model is advantageous in certain respects. First, the testing becomes more powerful by using information across quantile levels (====). Second, it is important in empirical research to test over lower or upper quantiles rather than at specific quantile levels, such as ====In particular, we consider the following general null hypothesis: ====for the VC quantile regression ====, when the vector, ====, is either a known or unknown function of ==== and ====. Here, ==== is a given ==== matrix with full rank, where ==== and ==== are allowed to increase with ====. In this study, we considered the following three cases: (1) ====; (2) ====; and (3) ==== for some specified or unspecified ====, ====, and ====, respectively. By using different cases and different matrices for ====, various tests can be handled in the VC quantile model, e.g., the significance of specific variables or parallel relationships between variables. For example, the aforementioned cases, (1), (2), and (3), include testing whether the model can be explained by a linear model, a linear quantile model, and a VC linear model against a VC linear quantile model, respectively.====We consider regional quantile, that is, an interval of quantile levels, requiring different technical arguments from the existing approaches for a single quantile level to show the theoretical properties of the proposed test. In particular, the Bahadur representation (====, ====) cannot be directly adopted in this study, because a score function, ====, for which we need to solve ====, does not exist. In contrast, the objective function in our settings depends on selected quantile levels, ====, in the considered region, ====, of quantiles, and ==== varies by a choice of these quantile levels, where ==== can increase with ==== as well. Because high dimensions with increasing ==== and ==== setting can be impractical, we focus a finite ==== and ==== case in the main manuscript for its practical utility. However, the technical proofs in the supplementary materials still hold for the increasing case.====Compared with the most-related literature (====) using regional quantile, the proposed test can handle a more general null hypothesis by considering whether the coefficients functions in VC models have some specified/unspecified structures over a range of quantiles. On the other hand, ==== tests only whether the coefficients in the linear quantile model are zero over a range of quantiles. For example, the proposed test can handle a problem whether some coefficient functions are some unspecified non-zero constant functions over a range of quantiles in the VC model.====The remainder of this paper is organized as follows. We introduce the proposed test with its theoretical properties in Section ====. In Section ====, we present the evaluation of the finite sample performance of the proposed test via simulation. In Section ====. Technical details are deferred to the Supplementary Materials.====The following is the Supplementary material related to this article. ",Hypothesis testing of varying coefficients for regional quantiles,https://www.sciencedirect.com/science/article/pii/S0167947321000384,4 March 2021,2021,Research Article,251.0
"Zhang Qingzhao,Ma Shuangge,Huang Yuan","Department of Statistics and Data Science, School of Economics, Xiamen University, China,Wang Yanan Institute for Studies in Economics, Xiamen University, China,Department of Biostatistics, Yale University, United States","Received 30 December 2019, Revised 1 November 2020, Accepted 2 November 2020, Available online 4 March 2021, Version of Record 19 March 2021.",https://doi.org/10.1016/j.csda.2021.107210,Cited by (2),The Gaussian ,", ====, ====). To reduce bias caused by the ==== penalty, ==== apply the nonconvex SCAD penalty. Other existing methods include the neighborhood lasso selection (====), neighborhood Dantzig selection (====), penalized D-trace loss method (====), constrained ==== minimization (====), and others. We refer to ==== for a comprehensive overview of sparse estimators.====, and note that similar examples arise in many other fields.====The idea of integrating multiple data sources and learning parameter homogeneity/heterogeneity goes beyond the two examples above and has been studied extensively using the penalized methods, for example by ====, ====, ====, and others. A popular way to integrate information from multiple datasets is meta-analysis. Some meta-analysis approaches operate on summary statistics, while the individual participant data meta-analysis approaches directly analyze individual level data (====). The random-effects model has been proposed to handle heterogeneity in magnitudes, but so far there has been a lack of work on promoting sign consistency. Although some individual participant data meta-analysis approaches can handle high-dimensional data, our literature review suggests that the penalization technique has not been commonly adopted. For example, ==== apply fixed-effects and random-effects models on the C-index, and ==== propose to use a weighted sum of gene expressions for dimension reduction. We note that although the object of estimation in the aforementioned studies is not a precision matrix, the settings and considerations are expected to be similar for precision matrices.====In recent studies, the joint estimation of multiple precision matrices has been investigated. One framework is to incorporate a target matrix such that the estimation can be encouraged to shrink to this target matrix (====), or update the precision matrix repeatedly over time with prior information available (====). Another framework, which is the framework we adopt, is to jointly analyze multiple raw datasets without any accessible target matrix or prior information. Building on the maximum penalized likelihood, ==== assume a common structure and encourage estimation of multiple networks towards this common structure. ==== propose a hierarchical penalization approach to jointly estimate multiple graphical models. ==== examine the joint graphical lasso using group lasso and fused lasso penalties. Along this line, ==== take a Laplacian penalization approach. ==== develop a weighted constrained ==== minimization approach. With all these methods, a popular strategy is to treat the elements corresponding to the same edge in multiple datasets as a group, and regularization is imposed on the (norms of) group parameters. Despite considerable successes, a common limitation shared by most of the existing studies is a lack of consideration of the relationships across datasets (matrices). In data analysis, it is often sensible to expect that the same edges have the same signs in different datasets although the magnitudes can be different.====In this study, we conduct the joint estimation of multiple precision matrices under high-dimensional settings. Motivated by the above discussions, our goal is to promote sign consistency across precision matrices while flexibly allowing for conflicting signs. The rest of the article is organized as follows. In Section ====, we develop a new joint estimation approach to promote sign consistency. An effective computational algorithm is developed, and the theoretical properties are rigorously established. In Section ====, we conduct extensive simulations and compare our approach with the competing alternatives. Two data examples are analyzed in Section ====. The article concludes with discussions in Section ====. Proofs and additional numerical results are presented in the Supplementary File.====The following is the Supplementary material related to this article. ",Promote sign consistency in the joint estimation of precision matrices,https://www.sciencedirect.com/science/article/pii/S016794732100044X,4 March 2021,2021,Research Article,252.0
"Huang Shih-Ting,Xie Fang,Lederer Johannes","Department of Mathematics, Ruhr-Universität Bochum, 44801 Bochum, Germany","Received 22 October 2020, Revised 16 February 2021, Accepted 16 February 2021, Available online 28 February 2021, Version of Record 9 March 2021.",https://doi.org/10.1016/j.csda.2021.107205,Cited by (3),"Ridge estimators regularize the squared Euclidean lengths of parameters. Such estimators are mathematically and computationally attractive but involve tuning parameters that need to be calibrated. It is shown that ridge estimators can be modified such that tuning parameters can be avoided altogether, and the resulting estimator can improve on the prediction accuracies of standard ridge estimators combined with cross-validation.","High-dimensional estimators typically minimize an objective function that contains again two functions: a data-fitting function to ensure a good fit to the data and a penalty function to leverage additional information. Popular data-fitting functions are least-squares and negative log-likelihood; popular penalty functions are ==== (lasso) (====) and ==== (ridge) (====). The weighting between data-fitting and penalty function is finally determined by a tuning parameter. This tuning parameter needs to be calibrated in a way that suits the estimator (such as lasso or ridge), data, and application at hand.====Known calibration schemes such as cross-validation (====, ====), stability selection (====, ====), and adaptive validation (====, ====, ====) require two steps: compute the estimators or surrogates of them for a range of tuning parameters and then apply a rule to select among those candidate estimators. We now focus on the ridge estimators and pose the question of whether the calibration of their tuning parameters can instead be integrated into the estimation process directly.====), which shows that replacing ====-regularization by ====-regularization can make estimators amenable to recent techniques in high-dimensional theory, especially KKT-type proofs as considered in ==== (trex) (====, ====, ====, ====), which proposes a way to integrate tuning parameter calibration into lasso-type estimators. However, while both of these lines of research focus on regularized least-squares in linear regression, we demonstrate that an inherent calibration of the ridge parameter is possible for a wide range of data-fitting functions and models.====We make three main contributions:",Tuning-free ridge estimators for high-dimensional generalized linear models,https://www.sciencedirect.com/science/article/pii/S0167947321000396,28 February 2021,2021,Research Article,253.0
"Zhong Wei,Wang Jiping,Chen Xiaolin","MOE Key Lab of Econometrics and Wang Yanan Institute for Studies in Economics, Xiamen University, Xiamen, Fujian, 361005, China,Department of Statistics in School of Economics and Fujian Key Lab of Statistics, Xiamen University, Xiamen, Fujian, 361005, China,School of Statistics, Qufu Normal University, Qufu, 273165, China","Received 6 March 2020, Revised 12 October 2020, Accepted 16 February 2021, Available online 24 February 2021, Version of Record 15 March 2021.",https://doi.org/10.1016/j.csda.2021.107206,Cited by (4),"Feature screening has become an indispensable ==== tool for ultrahigh dimensional data analysis. This article introduces a new model-free marginal feature screening approach for ultrahigh dimensional survival data with right censoring. The new procedure could be used for survival data with both ultrahigh dimensional categorical and continuous ====. Motivated by Cui et al. (2015), a censored mean variance index (cMV) is proposed to measure the dependence between a survival outcome and a categorical covariate. Then a slice-and-fuse method is exploited to modify the cMV index adaptive to continuous covariates. The sure independence screening based on the censored mean variance index (cMV-SIS) is proposed to identify the important covariates for ultrahigh dimensional data with censored survival outcomes. It enjoys many appealing merits inherited in the mean variance index. It is model-free and thus robust to ====. It is also robust to heavy tails and outliers in covariates. Moreover, the sure screening properties are theoretically investigated for both categorical and continuous covariates under some mild technical conditions. Extensive numerical simulations and a real data example have demonstrated the competitive performances of the proposed feature screening method.","), SCAD (====), adaptive LASSO (====), MCP (====) and so on. However, as noted by ====, computation burden inherent in those methods causes simultaneous problems of computational expenditure, statistical accuracy and algorithmic stability, which make them hard to be applied directly to ultrahigh dimensional data where the number of covariates is much larger than the sample size. One computational efficient solution is the marginal feature screening, which was pioneered by ==== and developed substantially in more recent years. ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ==== among many others.====In ultrahigh dimensional survival data, the outcome is time to an event subject to right censoring such that the screening methods for complete data could not be exploited directly. Recently, marginal independence feature screening approaches have been extended to the survival context. ==== and ==== investigated the screening procedures for Cox’s model, respectively. ==== proposed a semiparametric feature screening method for a class of single-index hazard models. To relieve the model structure constraints, some model-free survival feature screening approaches have been proposed. ==== developed a quantile-adaptive screening means by ranking the ==== norm of estimated marginal functional relationships. ==== developed a robust screening method based on the inverse probability-of-censoring weighted Kendall’s ====. ==== extended the SIRS (==== presented two kinds of robust screening procedures based on distance correlation. ==== developed an integrated powered density method by comparing the differences in the powered covariate-stratified density functions. ==== further extended the classic forward regression to Cox’s model, where important variables are selected sequentially according to the increment of partial likelihood with an EBIC stopping rule. ==== introduced a model-free ====-norm learning method, which provides an integrative framework for detecting predictors with various levels of impact on censored outcome data. One may refer to ==== for a recent review of marginal independence screening methods for survival data with ultrahigh dimensional covariates.====Most of the aforementioned survival screening procedures implicitly assume that the covariates are continuous. However, ultrahigh dimensional survival data with categorical and continuous covariates both exist in practice. Meanwhile, the heavy tails and outliers of continuous covariates are also commonly encountered. Motivated by these facts, we propose a new robust marginal feature screening approach for survival data capable of dealing with both of ultrahigh dimensional categorical and continuous covariates. For example, single nucleotide polymorphism (SNP) variables are discrete while gene expression level variables are continuous in genetical studies. We propose a new censored mean variance index (cMV) to measure the dependence between a categorical covariate and a survival outcome with right censoring. For the continuous covariates, a slice-and-fuse method is adopted as follows. We partition the support of a continuous covariate into multiple slices, and then define the cMV index between the new discretized variable and the response. Then, we repeat this procedure for a certain number of times based on different partitions. Finally, we fuse these cMV indices to generate the final censored mean variance index for the continuous covariate. This slice-and-fuse idea has also been used by ==== and ====The rest of this article is organized as follows. In Section ====, we introduce our methodologies for ultrahigh survival data with both categorical and continuous covariates. Sure screening properties are presented in Section ====. Section ====. Technical proofs are included in ====.====Let ==== be the probability space that underlies all the random variables in the paper, where ==== is the sample space, ==== is the ====-algebra and ==== is the probability measure. Denote by ==== to be a subset of ==== under which ====, ====. According to the theorem in Section 2.3.1 of ==== and Theorem 3.1 of ====, we have that ====. In the following proofs, we consider events with the intersections of ====, which will not be stated explicitly.",Censored mean variance sure independence screening for ultrahigh dimensional survival data,https://www.sciencedirect.com/science/article/pii/S0167947321000402,24 February 2021,2021,Research Article,254.0
"Zhang Yuyang,Schnell Patrick,Song Chi,Huang Bin,Lu Bo","Division of Biostatistics, College of Public Health, The Ohio State University, Columbus, OH, USA,Division of Biostatistics and Epidemiology, Cincinnati Children’s Hospital Medical Center, Department of Pediatrics, College of Medicine, University of Cincinnati, Cincinnati, OH, USA","Received 27 May 2020, Revised 26 January 2021, Accepted 26 January 2021, Available online 23 February 2021, Version of Record 13 March 2021.",https://doi.org/10.1016/j.csda.2021.107188,Cited by (2),Inferring ,"). However, the methods for identifying and estimating subpopulation causal effects are relatively less developed. Heterogeneous subpopulation effects are ubiquitous in many studies, as individuals with different characteristics may respond differently to a certain intervention (====). A major challenge in subgroup analysis is that the subgroup structure is often unknown in advance. Ad-hoc analysis of subgroup effect may lead to misleading findings and more principled data-driven subgroup analysis strategies are desirable (====).====A traditional string of literature on subgroup effects focuses on effect modification as interaction terms in pre-specified models (====, ====, ====, ====).====). It extends the classification and regression tree (CART) (====) by modifying the tree splitting criterion to maximizing the mean squared subgroup treatment effect. The subgroup effects are estimated using inverse propensity score weighting to control confounding (====). Causal inference tree (CIT) proposes a facilitating score to split the tree, where subjects with similar facilitating scores should have similar propensity scores and treatment effects (====). Since the facilitating score is related to both treatment assignment and potential outcomes, a parametric likelihood function needs to be specified in the inference. The virtual twins (VT) method aims to find the individual treatment effect then apply the classification tree to identify the subgroup structure (====Matching is another popular way of using propensity score for confounding adjustment in observational studies (====, ==== use matching design and the classification tree to examine effect modification with a focus on design sensitivity for potential unmeasured confounding. One reason of limited use of matching design in subgroup analysis is that it is hard to ensure the matching quality in every subgroup in practice, especially when there are many subgroups. ====In this paper, instead of focusing on balancing covariate distributions in known subgroups, we develop a novel approach, i.e. matching tree (MT), as a principled data-driven strategy for subgroup identification, when subgroup indicators are unknown. Individual subjects from different treatment groups are first matched to remove confounding. The tree-based method is then applied to within-pair outcome differences to identify the subgroup structure, with trimming strategy to prevent overfitting. The matching based estimator is unbiased for subgroup treatment effect estimation among people with similar characteristics as the treated subjects, which is also robust to model misspecification. The resulting subgroup structure may be further utilized by content expert to improve the intervention strategies. The paper is organized as follows. Section ==== provides justifications for matching based subgroup estimator and details for matching tree algorithms. Section ==== presents two simulation studies. Section ==== applies the proposed method to study the potential subgroup effect of the timing of Tobramycin use on chronic infection among pediatric CF patients. Section ==== ends the paper with discussions on limitation and potential extensions.====This proofs of ==== and ==== follow the idea of ==== where they proved the strong ignorability for the general population. We extend their results to subpopulations using a similar approach. The proofs are included here for completeness.",Subgroup causal effect identification and estimation via matching tree,https://www.sciencedirect.com/science/article/pii/S0167947321000220,23 February 2021,2021,Research Article,255.0
"Li Rui,Reich Brian J.,Bondell Howard D.","Department of Statistics, North Carolina State University, Raleigh, NC 27695, USA,School of Mathematics and Statistics, University of Melbourne, Parkville, VIC 3010, Australia","Received 6 July 2020, Revised 9 February 2021, Accepted 13 February 2021, Available online 22 February 2021, Version of Record 17 March 2021.",https://doi.org/10.1016/j.csda.2021.107203,Cited by (19),"Due to their flexibility and ====, machine-learning based ",", ====, ====, ====), population and demographic studies (====), transportation and traffic analysis (====, ====) and energy forecasting (====, ====, ====), or prediction intervals (====, ====), we aim to directly estimate the full conditional distribution, as other quantities can be directly extracted from it.====There are a number of approaches to estimate the conditional distribution of the target quantity ==== given the input vector ====. A major class of methods estimates the density functions ==== and ====) to obtain the conditional distribution estimate ====. This approach is limited by the dimensionality of the input space ====, ====, ====, ====, ====). Another popular approach is to approximate the distribution of interest by a parametric distribution family or mixture of such distributions, such as a mixture of Gaussians (====, ====, ====, ====), which obtains the full conditional distribution from the empirical distribution in the aggregated tree leaves. A boosting approach to this problem was discussed in ====.==== falls into each bin given the input covariates ==== and ====. Finally, these ensemble member estimates are combined by the ensemble approach to obtain a smoother and more stable density estimator.====Similar partition ideas have been proposed in sliced inverse regression (SIR) (====The paper is organized as follows: In Section ====, we describe the model set up. In Section ====, we examine approaches and loss functions that can be used in the multi-class classification stage and in Section ====. In the simulation study, we thoroughly examine the effect of number of bins, different binning strategies as well as different loss functions. In Section ====, the method is illustrated using the aforementioned solar energy example where we demonstrate superior performance to quantile random forests. We conclude with the discussion in Section ====.====In this appendix, we provide the proof for ====. This proof is related to the proof of histogram consistency as in Theorem 6.11 of ====, with the consideration of the bias and variance from the classification model. Let ==== denote ====, ==== denote ==== and ==== denote ====. Without the loss of generality, we assume the range of ==== is ====. ==== denotes the number of equally sized, consecutive and non-overlapping bins. ==== denotes the bin width, and we have ====. Let ==== be the target conditional density, and ==== be equally sized, consecutive and non-overlapping bins on the true density support ====. ====, such that ====. Then we have ==== where ==== is between ==== and ====. ==== is the middle point of interval ====.====The bias term ==== for the density estimator is following: ====Integrate ==== over the interval ====: ==== where ====.====Integrate ==== over the range of ====: ====If the model bias ====, we can achieve ==== as ====.====For the variance part: ====If the model variance ====, then ==== as ====.",Deep distribution regression,https://www.sciencedirect.com/science/article/pii/S0167947321000372,22 February 2021,2021,Research Article,256.0
"Li Ting,Song Xinyuan,Zhang Yingying,Zhu Hongtu,Zhu Zhongyi","School of Statistics and Management, Shanghai University of Finance and Economics, China,Department of Statistics, The Chinese University of Hong Kong, Hong Kong,Academy of Statistics and Interdisciplinary Sciences, East China Normal University, China,Department of Biostatistics, University of North Carolina at Chapel Hill, United States of America,Department of Statistics, Fudan University, China","Received 6 May 2020, Revised 29 January 2021, Accepted 30 January 2021, Available online 14 February 2021, Version of Record 26 February 2021.",https://doi.org/10.1016/j.csda.2021.107192,Cited by (6),Classical clusterwise linear regression is a useful method for investigating the relationship between scalar predictors and scalar responses with heterogeneous variation of regression patterns for different subgroups of subjects. This paper extends the classical clusterwise linear regression to incorporate multiple functional predictors by representing the functional coefficients in terms of a functional principal component basis. We estimate the functional principal component coefficients based on M-estimation and ,"To detect potential cluster-level heterogeneity in functional linear models, we consider the clusterwise functional linear regression model to deal with the group-specific regression patterns for functional data. Given a scalar response ==== and ==== smooth random predictor processes ==== on a compact support ==== that are square integrable, we have the model as follows: ====where ==== is the intercept, ==== is the index set for the ====th group and ==== and the number of groups ==== are often unknown.====Model ====, ==== and ==== combined ====-means algorithm with classical estimation procedures, which enjoy the advantages of conceptual simplicity and computational efficiency. ====, ==== and ====, ==== and ==== remains relatively undeveloped in the literature with some exceptions. ====, ==== and ====The focus of this paper is to introduce a computational efficient and robust estimation method for model ==== without assumptions on the underlying distribution, and to provide throughout theoretical investigations with respect to both the coefficient estimators and the estimator of the group number. By adopting the functional principal component analysis, we are able to deal with the infinite-dimensional functional coefficients. We propose a general approach to estimate the functional principal component coefficients based on M-estimation and ====Compared with the existing literature, we make several major contributions. First, the proposed ====. To the best of our knowledge, this work is the first to derive the consistency result for model selection by the Bayesian information criterion and the rates of convergence for parameter estimators in the context of clusterwise functional linear models or functional mixture models. In spite of their high importance, these consistency results are relatively rare even for classical clusterwise linear regression models (====). Fourth, the proposed method is applicable to intermittent and noisy trajectories at the price of a more complex theoretical investigation. Meanwhile, it is flexible by choosing different criterion functions according to various interests, and the theoretical results obtained are applicable to a general loss function. Although conceptually similar to ====, their method is restricted to least-squares loss function with perfectly observed functional variables and their paper lacks theoretical results.====, ====, ==== and ====. Furthermore, the proposed method is conceptually different from the curve-based clustering or classification methods, such as in ====, ==== and ====. These methods classify trajectories directly, whereas the proposed method clusters the functional processes and the scalar response together according to possibly different regression patterns with an unknown group membership.====The rest of this paper is organized as follows. Detailed estimation method can be found in Section ====. In Section ====, we establish the consistency of the Bayesian information criterion and convergence of the set of estimators. Section ==== demonstrates the practicality of the proposed method through finite sample simulation studies. We apply our method to a real dataset obtained from the ADNI study and to a dataset from an experiment on medfly fecundity in Section ====. All the proofs can be found in the supplementary material.====The following is the Supplementary material related to this article. ",Clusterwise functional linear regression models,https://www.sciencedirect.com/science/article/pii/S0167947321000268,14 February 2021,2021,Research Article,257.0
"Silva Ivair R.,Duczmal Luiz,Kulldorff Martin","Department of Statistics, Federal University of Ouro Preto, Ouro Preto, MG, Brazil,Department of Statistics, Federal University of Minas Gerais, MG, Brazil,Division of Pharmacoepidemiology and Pharmacoeconomics, Department of Medicine, Harvard Medical School and Brigham and Women’s Hospital, Boston, MA, USA","Received 13 July 2020, Revised 18 January 2021, Accepted 20 January 2021, Available online 12 February 2021, Version of Record 18 February 2021.",https://doi.org/10.1016/j.csda.2021.107185,Cited by (4),". In addition, its performance is illustrated on simulated and real data of birth defects in New York State.","), for understanding the spatial distribution of tree species in botany (====) and, as emphasized by ====, for disease outbreak detection in epidemiology, like the study of clusters of dengue contamination in the Brazilian Amazon (====), and breast cancer mortality (====), among many others.====According to ==== and ====, the methods for spatial clusters detection can be classified as ‘general’ or ‘focused’. With the general type, the search is performed over the whole map, and the size and location of a potential cluster is not defined prior to starting the analysis. With focused methods, the search is done only in a target sub-region of the map established prior to running the analysis. There is a number of powerful methods of the general type, like e.g. the elliptic spatial scan statistic, (====), the simulated annealing strategy, (====), Tango’s statistic, (====), the scan type method, (====), and the circular scan-type test introduced by ====, which hereinafter shall be referred simply by scan test. The scan test has proven to be a powerful tool for identifying spatial clusters even when the true cluster is highly irregular, (====, ====).====). As an extension of the work by ====, information from multiple data sources of disease surveillance is incorporated to achieve more coherent spatial cluster detection using tools from multi-criteria analysis (====). ==== evaluate the ====) was developed to avoid detecting excessively large clusters by absorbing insignificant neighbors with non-elevated risks. ==== proposed a spatial scan statistic for matched case-control data considering the correlations between the matched pairs. ==== developed a spatial scan statistic for detecting multiple-cluster in generalized linear models.====After one identifies the most likely cluster through the scan statistic, the next step is to infer the magnitude of relative risks inside that cluster. For point estimation, ====), a formal method for constructing a confidence interval for the relative risk of a detected cluster is still an open challenge.====. All of these properties are proved under analytical arguments.====). The results indicate that the proposed method produces informative intervals.====. Section ==== describes the method. Section ==== contains the demonstrations of properties (i) to (iii) above. Section ==== provides a numerical study of the expected interval length. Section ==== presents an example of application to analyze the data of birth defect data in New York State, USA, for the years 2005 to 2009. Section ==== closes the paper with the main conclusions.",Confidence intervals for spatial scan statistic,https://www.sciencedirect.com/science/article/pii/S0167947321000190,12 February 2021,2021,Research Article,258.0
"Lim Alejandro,Chiang Chin-Tsang,Teng Jen-Chieh","University of California, Berkeley, USA,Institute of Applied Mathematical Sciences, National Taiwan University, Taipei, Taiwan, ROC,Data Science Degree Program, National Taiwan University, Taipei, Taiwan, ROC","Received 4 September 2020, Revised 29 December 2020, Accepted 12 January 2021, Available online 12 February 2021, Version of Record 24 February 2021.",https://doi.org/10.1016/j.csda.2021.107181,Cited by (1),"Since the inception of the FIRST Robotics Competition (FRC) and its special playoff system, robotics teams have longed to appropriately quantify the strengths of their designed robots. The FRC includes a playground draft-like phase (alliance selection), arguably the most game-changing part of the competition, in which the top-8 robotics teams in a tournament based on the FRC’s ranking system assess potential alliance members for the opportunity of partnering in a playoff stage. In such a three-versus-three competition, several measures and models have been used to characterize actual or relative robot strengths. However, existing models are found to have poor ==== due to their imprecise estimates of robot strengths caused by a small ratio of the number of observations to the number of robots. A more general regression model with latent clusters of robot strengths is, thus, proposed to enhance their predictive capacities. Two effective estimation procedures are further developed to simultaneously estimate the number of clusters, clusters of robots, and robot strengths. Meanwhile, some measures are used to assess the ==== of competing models, the agreement between published FRC measures of strength and model-based robot strengths of all, playoff, and FRC top-8 robots, and the agreement between FRC top-8 robots and model-based top robots. Moreover, the stability of estimated robot strengths and accuracies is investigated to determine whether the scheduled matches are excessive or insufficient. In the analysis of qualification data from the 2018 FRC Houston and Detroit championships, the predictive ability of our model is also shown to be significantly better than those of existing models. Teams who adopt the new model can now appropriately rank their preferences for playoff alliance partners with greater ==== than before.","For Inspiration and Recognition of Science and Technology (FIRST) is an international youth organization which sponsors the FIRST Robotics Competition (FRC), a league involving three-versus-three matches, as dictated by a unique game released annually. The FIRST entry on ==== provides a more detailed description of the organization and its history. Winning an individual match in the FRC requires one set of three robots, an alliance, scoring more points than an opposing alliance. Since its founding by Dean Kamen and Woodie Flowers in 1989, an increasing number of teams have designed robots according to each year’s specific FRC game mechanics. The FRC has since ballooned to a pool of close to 4000 teams in 2018 playing in local and regional tournaments for a chance to qualify for the world championships of more than 800 teams. In turn, starting in 2017, FIRST divided its world championships into two championship tournaments in Houston and Detroit with about 400 robots playing in each.====Important to this article’s terminology and used frequently among scouters and tournament staff alike are the competing “red” and “blue” alliances to refer to the aforementioned sets of three robots. Those interpreting raw footage of matches rely heavily on the correspondingly colored bumpers of the robots in their data collection. Additionally, a robotics “team” refers to a group of people who have constructed, managed, and maintained a “robot”, which refers to the actual machine playing in a match.====With the never-changing three-versus-three format arose questions posed by many FRC strategists over the years in analyzing past tournaments and their alliance selection phases: Which robots carried their alliance, and which robots were carried by their alliance? How might one use the answers to the first questions to predict hypothetical or upcoming matches?====Over the years, FRC games have gone through a variety of changes and tasks that form a key part of competition. For 2017, tasks that earned points during a match included having the robot move across a line during the autonomous period, delivering game elements from one location to another using the robot, and having the robot climb a rope (====). Generally, tasks will be similar to these regardless of the year, though the game elements, delivery locations, and terrain will often change each year. In addition to similar tasks, there are some fundamental aspects that have not changed from year to year. Games across the years have followed the same match timing format of 15 s of autonomous control, in which robots are pre-programmed to operate in ways that score points, followed by a period of remote control, also called the teleoperated period, lasting at least two minutes. ==== has provided a more detailed video guide of how points were scored for the 2017 FRC game, and also provides a similar video guide at the start of each season.====There also exist numerous regulations and loopholes to those regulations that affect the relationships of results between different tournaments. Before 2020, ideally, the same robot played in different tournaments would be expected to have the same scoring ability across those tournaments. However, analysts often consider robots of the same team from different tournaments independent from each other, since although FIRST implemented a “Stop Build Day” in which teams could no longer make significant changes to their robots, there were still loopholes that allowed for a team’s robot to significantly improve in between tournaments. One such loophole allowed wealthier teams to build a secondary robot for testing new components, which could be added on during pre-tournament maintenance. As of 2020, the Stop Build Day rules have been lifted so that all teams can work on improvements more equitably in between tournaments. This development means that in future years, one can predict the performance difference of a robot between tournaments to be even more drastic than in recent history, thus reducing the reliability of previous tournament data in predicting the performance of a robot in future tournaments. Because of these developments and the history of inconsistency between tournaments, for the purposes of modeling, the same team’s entry played in different tournaments will be considered a completely different robot. The model presented in this article is, thus, limited to the data from the qualification rounds to predict the outcomes of the playoff rounds from the same tournament. It must also be noted that most maintenance that occurs in between rounds does not have a significant additional positive impact on the robot’s performance, since most of the scoring ability is inherently designed outside of competitive play.====As of the 2018 competitions, each championship site is composed of six divisions. Each division runs a mini-tournament to determine a division champion alliance. The six division champion alliances further advance to the Einstein Division, a round-robin group whose top two alliances play a best-of-three to decide a site champion alliance. In some years, each site champion alliance plays the other at another gathering called the “Festival of Champions”. This scheme somewhat parallels the Major League Baseball’s (MLB) National and American Leagues before the introduction of interleague play in that championship sites, championship divisions, and regions for qualifying for the world championships segregated teams from all over the world into their own pools of interaction, with the Festival of Champions serving as a counterpart to the MLB’s ====.====As in all tournaments throughout a season, matches in each division mini-tournament are divided into a qualification stage and a playoff stage, which are similar to a microcosm of a regular season and a postseason in major sports leagues on the scale of only two days and with far more teams. In the qualification stage, teams’ robots are assigned by a predetermined algorithm into matches of six split into three in a blue alliance against another three in a red alliance. ==== found similar types of data in the National Basketball Association (NBA), where the contributions of players are assessed through home team scores and away team scores at different shifts, which are defined to be periods of play between substitutions when ten players on the court is unchanged, during a match. Such a concept is also applicable in the National Hockey League (NHL).====Within the context of an individual game, the FRC also shares many similarities with traditional sports. Basketball is a game that could be broken into separate units such as possessions, in which one team has control of the ball and attempts to score by putting the ball in a basket, while the other team defends said basket. The above description defines a possession with a scoring method, a defense, and an offense. The FRC’s analog to the NBA’s possession is a cycling feature in which a robot may use the same method or set of actions to score repeatedly over the course of a match. Like the structure of a basketball possession, each of these cycles involves an offensive unit attempting to score with a specific action, and, sometimes, a defensive unit, which seeks to deter or defend from opponent scoring attempts. Robots on the same alliance may work in tandem within each cycle or in parallel, often running multiple independent cycles with the hopes of maximizing scoring. In both cases, a successful score for the offense is seen positively for offensive players and negatively for defensive players. That the FRC allows the possibility of simultaneous, independent cycles should not lessen the similarity between a cycle and a possession, since each cycle may be modeled the same way. The relation between cycles in the FRC and possessions in the NBA allows individual contributions of robots in a FIRST match to be analyzed like the individual contributions of players. However, unlike the FRC, players in the NBA are not randomly assigned to shifts, and if a shift in the NBA is comparable to an individual match in the FRC, for any given NBA shift within a game, players of different teams cannot be teammates in traditional sports, whereas in the FRC, former teammates often end up facing each other in later matches on the same day.====Especially worth mentioning is that for the FRC, since matches and playing field setup are standard without significant opportunity for fan interaction, there is no need to take into account a home-court advantage effect. A defensive component is also unnecessary in our model formulation. Since the designed robots are memory-less, scores of different matches are reasonably assumed to be mutually independent. There also exists the case in traditional sports where certain players may have so much synergy that a team simply does not provide the opportunity, and thus the data, for the hypothetical shifts where said players are separated. For the qualification stage of an FRC tournament, a predetermined schedule algorithm, as detailed in the “match assignment” section of the 2017 game manual (====It has been observed that within an alliance for a specific match, there is at least some robot on the winning alliance that contributed, possibly more than the others, to the win and at least some robot on the losing alliance that did not pull its weight, possibly more than the others, in the loss. For example, Team 254 of the 2018 Hopper Division went undefeated in its qualification matches, and Team 4775 of the same division failed to win a match in qualification per The Blue Alliance’s online database (====). These, together, provides a motivation to estimate robot strengths to determine which robots are contributing the most to wins and losses. We aim in this article to develop a model that utilizes the estimated strengths of individual robots to predict the outcome of any match.====Official ranking of FRC teams within the qualification stage involves a system of ranking points (RP). The kickoff event’s revealing of game rules introduces two in-game objectives, the completion of which is rewarded with one RP each. Additionally, winning a match will net a team two RPs while losing produces no additional RP. Pre-playoff rankings are determined according to the total RP earned in the qualification stage with the top-8 teams in terms of RP being guaranteed a spot in the playoffs. These top-8 teams are given the ability to form playoff alliances with an additional three members for the playoff stage, one of which serves as a substitute, preserving the three-versus-three game structure. When there are ties within the RP system, the average score (AS) measure is used to break RP ties.====Unlike the qualification stage, playoff stage alliances are determined by teams during the alliance selection phase. It is during the alliance selection phase that the pool of teams within a particular tournament draft themselves into playoff alliances that they each believe have the best chances of winning in the playoff stage. It is after this phase that the new, team-selected alliances take full control of their own results, independent of randomness in having carried or being carried in the qualification stage. The alliance selection phase, which takes place immediately after the end of qualification and directly affecting the upcoming playoff, also marks the transition between when RP is king (qualification) and when in-game scores become the ultimate goal (playoff). These reasons are why alliance selection is the most game-changing and consequential portion of an entire tournament. Subsequently, there are some teams that design their robots specifically to complete the in-game objectives, important to obtaining a higher RP ranking, with the hopes of guaranteeing a spot in the playoffs and having more control over alliance selection. Teams that choose to implement this design strategy sometimes sacrifice scoring ability, which is more important during the playoff stage. However, sometimes this tradeoff for alliance selection control is understandable, since such teams usually invest more than others into scouting and analysis of the competition and would thus have an information advantage in making alliance selections.====The alliance selection period is like the free agency period of the NBA. This is when players and managements, aside from in-season trades, negotiate the bulk of partnerships in, for non-rebuilding teams, hopes of capturing the best odds for the following season. It is important to note that unlike traditional sports leagues, the FRC does not have an analog to team-rebuilds within a tournament, so all alliance selections must be done with only the end goal of a championship in mind. Similarly, robotics teams, will negotiate with each other during the alliance selection phase to form the best alliance of robots for the playoff rounds. Synergy comes into play within both the free agency period and alliance selection, as management of an NBA team would not be looking to hypothetically field a team of five centers, nor would a FIRST team necessarily look to form a partnership with a team whose robot is optimized for the same scoring method. The best alliances tend to have each robot perform a different job, so as to maximize scoring opportunity, or minimize the other alliance’s scoring opportunity if one of the jobs is general defense, depending on that year’s game. Thus, NBA management and FRC strategists are comparable in that they both seek well-roundedness, as well as saturated ability in their quests for championships. Other traditional sports’ free agencies also exhibit the same desires and goals from teams.====In the playoff stage, the eight alliances play three of their four robots in each match in a best of three matches knockout manner until a champion is declared. As the FRC continues to progress, so have questions from participants on improving the quality of alliance selection and decision making. Since the goal of playoff matches is actual points rather than RP, many decide not to rely on the RP system to make decisions regarding the alliance selection. Teams throughout the years have created new measures in order to assess actual or relative robot strengths. Some widely used measures in robotics forums (e.g. ====, ====, ====, ====) include the offensive power rating (OPR), the winning margin power rating (WMPR), and the calculated contribution to winning margin (CCWM), among others. In one such investigation of these measures, using simulated and past FRC and FTC tournaments, ====In the 2018 FRC Houston and Detroit championships, there were 112 to 114 matches involving 67 to 68 robots for divisions in the qualification stage. The corresponding ratios of the number of observations to the number of robots range from 3.28 to 3.40 for the OPR model and from 1.64 to 1.70 for the WMPR model. The analysis of such paired comparison data also reveals no strong evidence to support the use of the WMPR model. These considerations motivate us to explore some possible avenues to enhance their predictive capacities. With the consideration of latent clusters of robot strengths, the proposed model can be regarded as the dimension reduction of the parameter spaces in the OPR and WMPR models. One crucial issue is how to estimate the number of clusters, clusters of robots, and robot strengths. The major aim of our proposal is to assess robot strengths in a more accurate and precise manner and help highly ranked teams assess potential alliance members for the opportunity of partnering in the playoff stage. To the best of our knowledge, there is still no research devoted to studying the latent clusters of individual strengths in team games.====The rest of this article is organized as follows. Section ==== outlines existing measures and models for robot strengths. In Section ====, we propose a more general regression model with latent clusters of robot strengths, develop two effective estimation procedures, and present some agreement and stability measures. In Section ====, our proposal is applied to the 2018 FRC Houston and Detroit championships. Conclusion and discussion are also given in Section ====.====Let ==== and ====. For the OPRC model, we derive that ==== In light of the above result, ==== has the form ====with ==== As for the WMPRC model, the properties in equation (8.5) of ==== and ==== enable us to have ====It follows that ",Estimating robot strengths with application to selection of alliance members in FIRST robotics competitions,https://www.sciencedirect.com/science/article/pii/S0167947321000153,12 February 2021,2021,Research Article,259.0
"Bouchouia Mohammed,Portier François","Télécom Paris, Institut Polytechnique de Paris, 19 Place Marguerite Perey, 91120 Palaiseau, France","Received 6 March 2020, Revised 25 January 2021, Accepted 25 January 2021, Available online 11 February 2021, Version of Record 24 February 2021.",https://doi.org/10.1016/j.csda.2021.107191,Cited by (1),A statistical ====.,"As the world witnesses the negative effects of traffic congestion, including pollution and economically ineffective transportation (====, ====, ====), achieving smart mobility has become one of the leading challenges of emerging cities (====, allowing for the identification of road traffic determinants.====The proposed methodology addresses two important issues regarding ====. First, in contrast with a traditional ==== analysis (====), the number of vehicles using the network almost vanishes during the night. Hence, in terms of probabilistic dependency, the road traffic between the different days is assumed independent. Such a phenomenon is referred to as “====) where the road network “regenerates” at the beginning of each new day. Second, the size of the road network, especially in urban areas, can be relatively large compared to the number of observed days. This implies that the algorithms employed must be robust to the well-known ==== (====) setting in which the features are numerous.====For each time ==== of day ====, we denote the vector of speeds registered in the road network by ====.====Hence ====, ==== and ==== stand for the number of sections in the network, the number of days in the study, and the number of time instants within each day. Inspired by the ====, the proposed model is similar to the popular ==== (VAR) model, as described in the econometric literature (====). The one difference is that it only applies within each new day due to the regeneration property.====We therefore consider the following linear regression model, called regenerative VAR, ====with ==== and ====. This model is used to predict the next value ====. The parameter ==== encodes for the influence between different road sections, and the parameters ==== account for the daily (seasonal) variations.====The approach taken for estimating the parameters of the regenerative VAR, ==== and ====, follows from applying ordinary least-squares (OLS) while penalizing the coefficients of ==== using the ====-norm just as in the popular LASSO procedure (====); see also ====, ====, ==== for reference textbooks. The estimator is computed by minimizing over ==== and ==== the following objective function ====where ==== and ====, ====, ====, ====, ====, ====, ==== and ====. The aforementioned references advocate for the use of the LASSO or some of its variants in time-series prediction when the dimension of the time series is relatively large. Other variable selection approaches in VAR models, but without using the LASSO, are proposed in ====.====). From an estimation point of view, two key aspects are related to the regenerative VAR model: (i) the regression output is a high-dimensional vector of size ====, ====). Another issue with regards to the regenerative VAR model is that of model switching, which occurs when the use of two different matrices ====From a theoretical standpoint, we adopt an asymptotic framework which captures the nature of usual road traffic data in which ==== and ==== are growing to infinity (==== might be larger than ====) whereas ==== is considered to be fixed. We first establish a bound on the predictive risk, defined as the normalized prediction error, for the case ====. This situation corresponds to ====. The order is in ==== and therefore deteriorates when the value of ====, claiming that each line of ==== has a small number of a non-zero coefficient (each section is predicted based on a small number of sections), we study the regularized case when ====. In this case, even when ==== is much larger than ====, we obtain a bound of order ==== (up to a logarithmic factor), and the eigenvalue condition is alleviated as it concerns only the eigenvalues restricted to the ==== variables. Finally, these results are used to demonstrate the consistency of a cross-validation change point detection procedure under regime switching.====From a practical perspective, the regenerative VAR, by virtue of its simplicity, contrasts with past approaches, mostly based on ====, that have been used to handle road traffic data. For instance, in ====, ==== are used to extract spatial and temporal information from the data before predictions. In ====, ==== (MLP) and ==== and ====. Three advantages of the proposed approach are the following:====To demonstrate the practical interest of the proposal, the data used is concerned with the urban area of a French city, Rennes, made of ==== days, ==== road sections and ==== time instants (from ==== pm to ==== pm) within each day (see ====). Among all the considered methods, including the classical baseline from the time series analysis as well as the most recent neural network architecture, this is the regime switching model that yields the best performance.====The outline is as follows. In Section ====, we present the main theoretical results of the paper that are bounds on the prediction error of ====. Section ==== investigates the regime switching variant. A comparative study including different methods applied to the real data presented before is proposed in Section ====. A simulation study is conducted in Section ====. All the proofs of the stated results are gathered in ====.====Before starting the proofs, we specify some useful notations. For any matrix ====, the ====-norm is denoted by ==== and the ====-norm is given by ====. The ====th column (resp. ====th line) of ==== is denoted by ==== (resp ====). Both are column vectors.",High dimensional regression for regenerative time-series: An application to road traffic modeling,https://www.sciencedirect.com/science/article/pii/S0167947321000256,11 February 2021,2021,Research Article,260.0
"Luati Alessandra,Novelli Marco","Department of Statistical Sciences, University of Bologna, Italy","Received 19 December 2019, Revised 15 January 2021, Accepted 15 January 2021, Available online 10 February 2021, Version of Record 17 February 2021.",https://doi.org/10.1016/j.csda.2021.107183,Cited by (1), showing that our formulation outperforms the original one both in small and in large samples.,", ====, ====) and superconducting systems (====, ====, ====). The novelty lies in the development of the indirect or generalized measurement, which avoids the so-called wave-function collapse by extending the measurement process to an auxiliary meter-system and then performing the measurement only on the latter (====, ====, ====, ====, ====, ====). The theoretical background which lies behind the experimental setting considered in the paper can be envisaged in the class of explicit-duration hidden Markov models (EDHMM). As a matter of fact, the conventional Hidden Markov Models (HMM), used for example to model quantum systems under no perturbation (====), are based on the Markovian short-memory assumption and cannot capture the complex dynamics induced by the external force.====, ====), for multinomial data (====, ====, ====, ====, ====).====There are two main contributions in this article. First, to the best of our knowledge, this is the first attempt to model the dynamics of an open quantum system by means of the EDHMM. Second, we take into account the presence of sparse data (lack of information) by introducing a nonparametric kernel estimator for discrete duration distributions. In addition, in order to reconstruct the hidden dynamics of the system, a ==== which is robust against the underflow problem is used.====The remainder of the paper is organized as follows. Explicit duration HMM are discussed in Section ====. Section ==== addresses the estimation issues, including the kernel estimator along with its properties and the Viterbi algorithm used for state reconstruction. Section ==== introduces and discusses the experimental setup. Section ==== concludes the paper. Additional results are presented in ====.====See ====, ====, ====, ====, ====.",Explicit-duration Hidden Markov Models for quantum state estimation,https://www.sciencedirect.com/science/article/pii/S0167947321000177,10 February 2021,2021,Research Article,261.0
"Batool Fatima,Hennig Christian","Department of Statistical Science, University College London, Gower Street, London WC1E 6BT, United Kingdom,Dipartimento di Scienze Statistiche “Paolo Fortunati”, Universita di Bologna, Bologna, Via delle belle Arti, 41, 40126, Italy","Received 22 May 2020, Revised 22 November 2020, Accepted 26 January 2021, Available online 10 February 2021, Version of Record 25 February 2021.",https://doi.org/10.1016/j.csda.2021.107190,Cited by (47),"The Average Silhouette Width (ASW) is a popular cluster validation index to estimate the number of clusters. The question whether it also is suitable as a general objective function to be optimized for finding a clustering is addressed. Two algorithms (the standard version OSil and a fast version FOSil) are proposed, and they are compared with existing ","), and it has been argued that there is no universally best approach, and that the cluster analysis approach needs to be chosen taking into account what kinds of clusters are required, which depends on domain knowledge and the aim of clustering (====, ====).====Many cluster analysis methods such as ====-means (====) and Partitioning Around Medoids (PAM; ====) are defined by optimizing an objective function over all partitions of the data set into a fixed number of ==== clusters. The objective function formalizes the quality of the clustering. The PAM objective function, for example, sums up distances of all observations to the centre of the cluster to which they were assigned, and a good clustering is one for which this is small. Many of these objective functions are not suitable for finding an optimal number of clusters ====, because they will automatically improve if ==== is increased due to more degrees of freedom in optimization.====Therefore, so-called cluster validation indexes that can be meaningfully optimized over ==== are often used together with such partitioning clustering methods in order to find an optimal ====. Many such indexes have been proposed in the literature (see ==== and ====). Some of these are for fixed ==== equivalent to objective functions of partitioning methods such as ====-means, e.g., the Calinski and Harabasz index (====). Some others are not in this way connected to a specific partitioning method. One popular such index is the Average Silhouette Width (ASW) (====). The ASW achieved overall very good results in the extensive simulation study of ====. In ==== it is suggested for finding the number of clusters with PAM, but in fact its definition is not directly connected to any specific partitioning method, and it can be seen as a general distance-based approach to assess the quality of a clustering.==== but also for fixed ==== in order to integrate the problem for fixed ==== and the problem of finding the best ====. This idea is explored here. We treat the idea with an open mind and do not attempt to suggest that optimum ASW clustering is an optimal clustering method in any other sense than optimizing the ASW (which can be seen as desirable on its own terms); rather what we do is to show both potential and problems with this approach. The problems are also relevant to the use of the ASW for just choosing ====, for which it is of widespread use despite a far from comprehensive evaluation and theoretical basis. To our knowledge, up to now using the ASW also for choosing a clustering with fixed ==== has only been explored by ====, where a modification of the PAM algorithm called PAMSil was proposed that looks for a local medoid-based optimum of the ASW. ====, where the ASW was originally introduced, mentions a possibility of its optimization for finding a clustering in a side remark. Exploring the new clustering method based on the ASW, we also to some extent explore strengths and weaknesses of the popular use of the ASW as a method to estimate the number of clusters.====In Section ==== we introduce optimum ASW clustering and propose two algorithms for it. In Section ==== we show that the ASW fulfils some axioms that have been proposed for clustering quality measures in ====. In Section ==== we run an extensive simulation study to explore the performance of optimum ASW clustering compared to other well established clustering methods. There is also an experiment regarding outliers. Section ====.====The following is the Supplementary material related to this article. ",Clustering with the Average Silhouette Width,https://www.sciencedirect.com/science/article/pii/S0167947321000244,10 February 2021,2021,Research Article,262.0
Wiens Douglas P.,"Mathematical & Statistical Sciences, University of Alberta, Edmonton, Canada T6G 2G1","Received 14 June 2020, Revised 23 January 2021, Accepted 25 January 2021, Available online 5 February 2021, Version of Record 18 February 2021.",https://doi.org/10.1016/j.csda.2021.107189,Cited by (3),Methods for the construction of dose–response designs are presented that are robust against possible ==== and mislabelled responses. The ==== of the predictions. Both sequential and adaptive approaches are studied. Finite sample simulations and examples illustrate the gains to be made by adaptivity.,". ==== noted the sensitivity of locally optimal dose–response designs to perturbations of the local parameters, and to the chosen model for the continuous response. This avenue of investigation extended ==== and was continued by ====. ==== designed for robustness against an infinite dimensional, nonparametric neighbourhood of such departures from the fitted link. ==== proposed to optimize a loss function after averaging over a finite set of competing models, differing with respect to, for instance, the assumed parameter values, or the assumed link.====For some recent work in this area see as well ====, ====, ==== and ====.====Perhaps closest to the approach to robustness advocated in this article is ====, who adopted a logistic link, allowing for errors in the specification of the linear component. These model errors were allowed to range over a certain – rather sparse – neighbourhood, and the – locally optimal – design was chosen to minimize the average loss over this neighbourhood. In contrast, in the current work we allow an arbitrary link function, a much richer class of model alternatives, allow for labelling errors, and derive ==== and ====, ==== designs. A rough description of our sequential approach is that, given an ====, followed by a simulation study in Section ====. This study compares various estimation, weighting, and design schemes, and illustrates the gains to be made by our adaptive, robust designs over sequential but non-adaptive methods, and over random designs.====Our work in this article is partially motivated by the following study, to which we shall return in Section ====. ==== investigated the age of menarche (onset of menstrual bleeding) in a large number of Warsaw girls. Here ‘age’ plays the role of dose and the response is ==== or ==== as the respondent reported that menarche had or had not occurred, when asked at age ====. ==== found that the age of menarche can be misreported, which we interpret as a mislabelled response, up to 5% of the time. ==== found age of menarche to be highly, and negatively, correlated with Body Mass Index (====), which in turn (====) is negatively correlated with smoking patterns. ==== to ==== we fit a model relating age to ",Robust designs for dose–response studies: Model and labelling robustness,https://www.sciencedirect.com/science/article/pii/S0167947321000232,5 February 2021,2021,Research Article,263.0
Zuo Yijun,"Department of Statistics and Probability, Michigan State University, East Lansing, MI 48824, USA","Received 8 June 2019, Revised 19 January 2021, Accepted 20 January 2021, Available online 27 January 2021, Version of Record 8 February 2021.",https://doi.org/10.1016/j.csda.2021.107184,Cited by (1),"Notions of depth in regression have been introduced and studied in the literature. The most famous example is Regression Depth (RD), which is a direct extension of location depth to regression. The projection regression depth (PRD) is the extension of another prevailing location depth, the projection depth, to regression. The computation issues of the RD have been discussed in the literature. The computation issues of the PRD and its induced median (maximum depth estimator) in a regression setting, never considered before, are addressed. For a given ==== exact algorithms for the PRD with cost ====
 (====) and ====
 (==== and ==== are proposed. Here ==== is a number defined based on the total number of ====; ==== is the total number of unit directions ==== utilized; ==== employed; ==== is the total number of replications. Furthermore, as the second major contribution, three PRD induced estimators, which can be computed up to 30 times faster than that of the PRD induced median while maintaining a similar level of accuracy are introduced. Examples and simulation studies reveal that the depth median induced from the PRD is favorable in terms of robustness and efficiency, compared to the maximum depth estimator induced from the RD, which is the current leading regression median.","Notions of location depth have been introduced and extensively studied in the literature over the last three decades. Depth notions have found applications in diverse fields and disciplines (see ==== for a review). Among others (Simplicial depth (====), Zonoid depth (====, ==== (====, ====)) and Spatial depth (====) (popularized by ====) and the projection depth (PD) (====, ====) (thoroughly studied in ====), both of which are in the spirit of the projection-pursuit scheme.====One naturally wonders if the depth notion can be extended to a regression setting. Regression depth (RD) of ==== is the most famous example, which directly extends HD to regression, whereas projection regression depth (PRD), induced from ==== and introduced in ====, is an extension of the PD to regression.====where ==== denotes the transpose of a vector, and random vector ====
 (==== and ==== are in ====. Let ====. Then ====. We use this model or ==== interchangeably depending on the context.====The maximum depth estimator induced from the RD, ====, can asymptotically resist up to 33% (====) (whereas the one from the PRD, ====, can resist up to 50% (====)) contamination without breakdown, in contrast to the 0% of the classical LS estimator. An illustration of these facts is given in ====, where the data set is given in Table 9 of Chapter 2 from ====By modifying the P-estimate of ====: ====
 ====where ==== stands for the distribution of the p-dimensional random vector ====, ====, ====, ==== will be restricted to the univariate regression functional of the form ==== could be a univariate location functional that is location, and scale (or called affine) equivariant; ==== does not depend on ==== and ====, see Z18a.====It is not difficult to see that ==== and ==== and the projection depth function ====
 (====), respectively.====Examples of ==== in ====. Examples of ==== in ====.====For the consideration of robustness, in the sequel, ==== is fixed and it is the pair ====, unless otherwise stated. Hereafter, we write ==== rather than ====. For this special choice of ==== and ==== such that ====
 We have ====and ====Applying the min–max (or max–min) scheme, we obtain the maximum (deepest) ==== (also denoted by ====) w.r.t. the pair ====
 ====When a sample ==== of ==== is given, an empirical distribution ==== based on ==== is obtained. Replacing ==== above by ==== we obtain all empirical versions.====, and in ====. The computation issues of the PRD and the ==== are the two main goals of this article. The third goal is to introduce several PRD induced estimators which can be computed much faster than that of the ====.====The rest of this article is organized as follows. Section ==== presents the computation problem and addresses the exact and approximate computation algorithms for the UF====, and equivalently for the PRD====. Furthermore, theoretical results are established and exact and approximate algorithms are presented along with abounded examples. Section ==== is devoted to (i) the computation of the ====, (ii) examples of the exact computation of the PRD as well as the approximate computation of the ====, and (iii) comparisons of performance between the ==== against leading competitors such as LS, ====, and ltsReg. Section ==== investigates the finite sample relative efficiency of the ====. Brief concluding comments end Section ==== and the article.====The following is the Supplementary material related to this article. ",Computation of projection regression depth and its induced median,https://www.sciencedirect.com/science/article/pii/S0167947321000189,27 January 2021,2021,Research Article,264.0
"Cappozzo Andrea,Greselin Francesca,Murphy Thomas Brendan","Department of Statistics and Quantitative Methods, Milano-Bicocca University, Italy,School of Mathematics & Statistics and Insight Research Centre, University College Dublin, Ireland","Received 21 July 2020, Revised 18 January 2021, Accepted 18 January 2021, Available online 26 January 2021, Version of Record 8 February 2021.",https://doi.org/10.1016/j.csda.2021.107186,Cited by (1),"The problem of identifying the most discriminating features when performing supervised learning has been extensively investigated. In particular, several methods for variable selection have been proposed in model-based classification. The impact of outliers and wrongly labeled units on the determination of relevant predictors has instead received far less attention, with almost no dedicated methodologies available. Two robust variable selection approaches are introduced: one that embeds a robust classifier within a greedy-forward selection procedure and the other based on the theory of ==== and irrelevance. The former recasts the feature identification as a model selection problem, while the latter regards the relevant subset as a model parameter to be estimated. The benefits of the proposed methods, in contrast with non-robust solutions, are assessed via an experiment on synthetic data. An application to a high-dimensional classification problem of contaminated spectroscopic data is presented."," (====, ====, ====, ====, ====), bioinformatics (====), genomic (====, ====, ====). Nonetheless, the impact that outliers and wrongly labeled units cause on the efficient determination of discriminant variables has received far less attention. Indeed, the presence of attribute and class noise can heavily damage a classifier performance (====), and most variable selection methods rely on the implicit assumption of dealing with an uncontaminated training set.====In order to overcome this limitation, the present paper proposes two approaches for robust variable selection in model-based classification: one that embeds a robust classifier, recently introduced in the literature, in a greedy-forward stepwise procedure for model selection (Section ====). Both procedures rely on impartial trimming (====): an appealing technique for robust parameter estimation in which no model assumption is a-priori required for the noise component. By leaving the anomalous units unmodeled, great flexibility is achieved and thus very heterogeneous contamination patterns can be effectively dealt with.====The remaining of the article is structured as follows. Section ==== formally characterizes the problem of variable selection in model-based discriminant analysis. In Section ====: they are the main contributions of the present manuscript. Section ==== is devoted to the comparison of several feature selection procedures within two simulation studies in an artificially contaminated scenario. Section ==== presents a high-dimensional discrimination study where our robust variable selection proposals are successfully applied to a chemometrics contest. Section ==== concludes the paper by highlighting some remarks and future research directions. Technical issues and computational details for the two novel methods are respectively deferred to ====, ====.====In this section we retrieve the ML estimates for the grouping and no grouping structures in the robust stepwise greedy-forward approach (Section ====), by means of the spurious outliers model specification.",Robust variable selection for model-based learning in presence of adulteration,https://www.sciencedirect.com/science/article/pii/S0167947321000207,26 January 2021,2021,Research Article,265.0
"Chabert-Liddell Saint-Clair,Barbillon Pierre,Donnet Sophie,Lazega Emmanuel","Université Paris-Saclay, AgroParisTech, INRAE, UMR MIA-Paris, 75005, Paris, France,Institut d’Études Politiques de Paris, France","Received 4 March 2020, Revised 14 January 2021, Accepted 14 January 2021, Available online 26 January 2021, Version of Record 3 February 2021.",https://doi.org/10.1016/j.csda.2021.107179,Cited by (5), at stake.,"The statistical analysis of network data has been a hot topic for the last decade. The last few years witnessed a growing interest for multilayer networks (see ====, ====, ====). A particular case of multilayer networks is multilevel networks where each level is a layer and an affiliation relationship represents the inter-layer. Multilevel networks are used across many fields such as sociology (====) or environmental science (==== and the maintenance of social inequalities can fall within the scope of this approach (====). Following ====, one might think that these two types of interactions (between individuals and between organizations) are interdependent, the individuals shaping their organizations and the organizations having an influence on the individuals. We aim to propose a statistical model for multilevel networks in order to understand how the two levels are intertwined and how one level impacts the other.====In what follows, a multilevel network is defined as the collection of an inter-individual network, an inter-organizational network and the affiliation of the individuals to the organizations. Besides, we assume that the individuals belong to a unique organization. Such a dataset is studied by ====, some researchers in cancerology being the individuals and their laboratories the organizations. ==== deal with another dataset concerned with the economic network of audiovisual firms and the informal network of their sales representatives during a trade fair. This latter dataset will be analyzed in this paper.====In the last years, the Stochastic Block Model (SBM developed by ====, ==== propose an SBM for multiplex networks and ==== an SBM for time-evolving networks. In this paper, we propose an SBM suited to multilevel networks (MLVSBM).",A stochastic block model approach for the analysis of multilevel networks: An application to the sociology of organizations,https://www.sciencedirect.com/science/article/pii/S016794732100013X,26 January 2021,2021,Research Article,266.0
"Lee Kuo-Jung,Feldkircher Martin,Chen Yi-Chi","Department of Statistics and Institute of Data Science, National Cheng Kung University, Tainan, Taiwan,Vienna School of International Studies (DA), Vienna, Austria,Department of Economics, National Cheng Kung University, Tainan, Taiwan","Received 9 July 2020, Revised 12 January 2021, Accepted 12 January 2021, Available online 26 January 2021, Version of Record 3 February 2021.",https://doi.org/10.1016/j.csda.2021.107180,Cited by (0)," and with high-dimensional data. Finally, the framework is applied to cross-sectional data investigating early warning indicators. The results reveal two distinct country groups for which estimated effects of vulnerability indicators vary considerably.","Many empirical problems in economics involve high-dimensional data where the number of predictors/covariates (====) is large relative to the number of observations (====). As such, ====). Furthermore, these large-scale data are often characterized by a significant degree of heterogeneity as they may arise from different sources.====Recently, considerable interest has been put on shrinkage methods and variable selection. These methods allow us to significantly decrease the number of covariates; see, e.g., ====, ====, ====. Relatedly, different subsets of covariates may only be relevant for different sub-populations. ====, ====). Our algorithm deals with an unknown number of components and applies variable selection within each component of the mixture model. Overall, our approach marks a significant improvement over existing alternatives such as the reversible jump algorithm of ====, which is not well defined when ====, small ==== problem, and consider an alternative prior to the commonly used ====-prior – the latter which avoids potential posterior multimodality due to strong collinearity (====). Third, it has been found that misspecification in Gaussian FMR models may lead to spurious groupings when mixture components slightly deviate from a normal distribution (====). Thus, our FMR model is robustified by the use of a student ====-distributions following ====The paper is organized as follows: Section ==== briefly reviews the statistical treatments of variable selection in FMR models, with a particular focus on identifying the number of mixture components. Sections ====, ==== describe our proposed Bayesian sparse variable selection approach for high-dimensional FMR and the required RJMCMC algorithm. Section ==== conducts simulation studies to evaluate the performance of the proposed RJMCMC in a number of different scenarios. This section also provides a sensitivity analysis with respect to alternative choices of prior distribution. Section ==== concludes.====The following is the Supplementary material related to this article. ",Variable selection in finite mixture of regression models with an unknown number of components,https://www.sciencedirect.com/science/article/pii/S0167947321000141,26 January 2021,2021,Research Article,267.0
"Mirfarah Elham,Naderi Mehrdad,Chen Ding-Geng","Department of Statistics, Faculty of Natural & Agricultural Sciences, University of Pretoria, Pretoria, South Africa","Received 11 July 2020, Revised 11 January 2021, Accepted 11 January 2021, Available online 24 January 2021, Version of Record 5 February 2021.",https://doi.org/10.1016/j.csda.2021.107182,Cited by (7),"Mixture of linear experts (MoE) model is one of the widespread statistical frameworks for modeling, classification, and clustering of data. Built on the normality assumption of the error terms for mathematical and computational convenience, the classical MoE model has two challenges: (1) it is sensitive to atypical observations and outliers, and (2) it might produce misleading inferential results for ====. The aim is then to resolve these two challenges, simultaneously, by proposing a robust MoE model for model-based clustering and discriminant censored data with the scale-mixture of normal (SMN) class of distributions for the unobserved error terms. An analytical expectation–maximization (EM) type algorithm is developed in order to obtain the maximum likelihood parameter estimates. Simulation studies are carried out to examine the performance, effectiveness, and robustness of the proposed methodology. Finally, a real dataset is used to illustrate the superiority of the new model.","-component MRM (====, ==== given the ====-dimension explanatory vector ==== is ====where ==== stands for the pdf of normal distribution with location and scale parameters ==== and ====, ====, ==== is the ==== the model parameters set is ====. Bear in mind that the mixing proportion with the constraint ====, is in fact ==== indicates from which component each subject has arisen. Recently, the classical MRM ==== has found appealing applications in many fields, such as business, marketing, and biological studies, see ====, ==== and ==== to name a few. It has also been extended to accommodate heavy-tail and/or skew distributed data. In this regard, ==== proposed an MRM by replacing ==== in ==== introduced an MRM by assuming that the components have log-concave densities and developed two EM-type (====) algorithms to obtain the maximum likelihood (ML) parameter estimates. Moreover, ==== extended the mixture models based on the scale-mixture of SN (SMSN) class of distributions (====) into the regression context.====Built up from the MRM formulation, the MoE model (==== be the response variable, ==== and ====. Instead of considering constant mixing component in model ====, the MoE model assumes that ====, and is known as a gating function. For instance, extending the MRM ====, the pdf of the normal-based MoE (MoE-N) is ====where for the gating parameters ==== with ====, ====and the model parameters set is ====. It should be emphasized that ==== and ==== can be exactly or partially identical. Since the introduction of the MoE-N model, considerable amount of contributions have been produced to overcome its potential deficiency in analyzing skew and heavy-tail distributed data. See for instance the works by ==== and ====, ==== on proposing the Laplace, Student’s-==== and skew-==== MoE models, respectively.====In many practical situations, such as economic and clinical studies, medical research and epidemiological cancer studies, the data are collected under some imposed detection limits. It might lead to incomplete data with different types of interval, left and/or right-censored responses. In this regard, censored regression model with the normality assumption for the error terms, known as Tobit model, was constructed by ====, ==== presented the nonlinear and linear censored regression models to overcome the problem of atypical observations. ==== also proposed censored linear regression model with the SMSN class of distributions to accommodate asymmetrically distributed censored datasets. Moreover, mixture of censored regression models based on the Student’s-==== model and on the SMN class of distributions was proposed by ==== and ==== as a flexible approach for modeling multimodal censored data with fat tails.==== is also approximated by an information-based approach. To illustrate the computational aspects and practical performance of the proposed methodology, a real-data analysis and several simulation studies are presented.====The remainder of the paper is organized as follows. Section ==== briefly reviews the SMN class of distributions. Model formulation and parameter estimation procedure of the MoE-SMN-CR model are presented in Section ====. Five simulation studies are conducted in Section ==== by analyzing wage-rates dataset. Finally, we conclude the paper with a discussion and suggestions for future work in Section ====.==== For the uncensored data ====, we have ====. Therefore, the only necessary conditional expectation ==== for the considered models can be computed as follows.==== In the censored cases, we have ====. For the sake of notation, let ====Therefore, the necessary conditional expectations ====, ====, and ==== for the considered models can be computed as follows. ====
 ====
 ==== where ====In the following, the closed forms of ==== and ==== for the special cases of SMN class of distributions are presented.",Mixture of linear experts model for censored data: A novel approach with scale-mixture of normal distributions,https://www.sciencedirect.com/science/article/pii/S0167947321000165,24 January 2021,2021,Research Article,268.0
"Gangloff Hugo,Courbot Jean-Baptiste,Monfrini Emmanuel,Collet Christophe","ICube - CNRS UMR 7357, Université de Strasbourg - CNRS, Illkirch, France,GEPROVAS, Strasbourg, France,IRIMAS, UR 7499, Université de Haute-Alsace, Mulhouse, France,SAMOVAR, Télécom SudParis, Institut Polytechnique de Paris, Palaiseau, France","Received 28 August 2020, Revised 9 January 2021, Accepted 9 January 2021, Available online 21 January 2021, Version of Record 3 February 2021.",https://doi.org/10.1016/j.csda.2021.107178,Cited by (4),Modeling strongly ,"). Popular approaches to solve this problem include clustering-based methods (====), graph-based methods (==== is hidden. When strong spatially-correlated noise corrupts the image, classical approaches reach their limit and new dedicated models need to be considered to improve the accuracy of the segmentation.====, ====, ====, ====) in which a third auxiliary process is added to improve the modelization possibilities, such as non-stationarities in the model parameters.==== GMRF, ==== these studies consider a discriminative probabilistic model (====, ====, ====We start by deriving the general probability distribution that defines a GPMF. We then propose four instances of the GPMF model whose expressiveness varies, in terms of ====The outline of the paper is the following. We first describe the new model, its core equations and main properties (Section ====). Then we illustrate the successive generalizations made with GPMF by studying models which are special cases of the GPMF (Section ====). We develop a parameter estimation procedure to solve the problem of unsupervised segmentation (Section ====). Finally, the models are successfully evaluated on synthetic and on real world images linked with a medical application (Section ====).====
 ====
 ==== We now show that if ==== is a GPMF, then its Gibbs distribution is necessarily of the form of Eq. ====. If ==== is a GPMF with respect to ====, it is then a PMF with respect to the same neighborhood and, thanks to the Hammersley–Clifford theorem we can write that: ====On the other hand, if we want to meet the second condition of the GPMF definition, we need to ensure that ==== is the density of a multivariate Gaussian function. Thus, there exists a SPD matrix ==== such that: ====Then, using: ====we get Eq. ==== by Eqs. ==== and ====. Now note that since ==== is SPD, ==== can be written as a positive semidefinite quadratic form in the variables ====, such that: ====
 ==== and ==== are polynomial function of ==== variables where terms involving a mix of ==== and ==== variables can be found, but terms with ==== variables alone cannot be found. Moreover there is the constraint that the RHS of Eq. ==== is a positive semidefinite quadratic form in the ==== variables.====Using Eq. ==== as well as the result of the multivariate Gaussian integral “backwards”, Eq. ==== becomes Eq. ====. We finally rearrange Eq. ==== into Eq. ====. In Eq. ==== it is clear that the RHS does not depend on ==== then so does the LHS. Thus, on the LHS, the terms containing ==== variables in ==== must simplify with terms in ====. Therefore, we are able to deduce the constraints on the terms of ==== that we are looking for:====We introduced ==== and ==== to be potential functions which only involve ==== variables.====This concludes the necessity part of the demonstration.==== Let us first show that a factorization of the form of Eq. ==== using Eq. ==== or, equivalently, that a Gibbs distribution whose energy is given by Eq. ==== is a PMF. It is straightforward to see that ====. Moreover, from the conditional formulation we have clearly that, ====, ====. These first two points show that ==== is a PMF with respect to ====.====We now need to show that ==== is the density of a GMRF. Using the energy formulation, we show using Bayes theorem that: ====In the previous equations, we remove all the ==== where the ==== variables do not play a role. The remaining terms are ==== and ====, which are assumed to be positive semidefinite forms in ====. Thus, by definition, there exists ==== and ==== SPD matrices of size ==== such that: ====Note that ==== and ==== can depend on ==== but we omit it in notations for clarity. Note also that ==== is a diagonal matrix. Since ====, ==== is a SPD matrix since it is the sum of two SPD matrices. Finally, using the result of the multivariate Gaussian integral we find Eq. ====. The latter equation shows that ==== given ==== is a GMRF, which is the second element of the GPMF definition and concludes the sufficiency part of the demonstration.",Unsupervised image segmentation with Gaussian Pairwise Markov Fields,https://www.sciencedirect.com/science/article/pii/S0167947321000128,21 January 2021,2021,Research Article,269.0
Ueki Masao,"School of Information and Data Sciences, Nagasaki University, 1-14 Bunkyo, Nagasaki 852-8521, Japan","Received 3 February 2020, Revised 28 December 2020, Accepted 28 December 2020, Available online 19 January 2021, Version of Record 3 February 2021.",https://doi.org/10.1016/j.csda.2021.107168,Cited by (1)," had a high power regardless of the simulation scenarios. Applied to a group-wise analysis in real genome-wide association study data from Alzheimer’s Disease Neuroimaging Initiative, the proposal gave a higher association signal than the existing methods.",", ====, ====). Similarly, a genome-wide environment interaction study explores the interaction effect of each genetic variant and environment factor pair (====). Those tests examine the effects under a given alternative model represented by a few parameters (e.g. a single genetic variant is independently associated with disease), but they might be insufficient to describe complex phenomenon. To extract new findings with minimal prior knowledge, more complex models would be needed.====Alternative approaches include group-wise analysis for pre-defined groups of variables. For example, in genome-wide association studies, nearby variants are grouped and tested separately for each group (====, ====, ====, ====, ====). Similarly, a multi-voxel test called a searchlight mapping is proposed in functional magnetic resonance imaging data analysis (====). However, test is underpowered if the saturated model is excessively redundant compared to the genuine structure. The power can be gained by custom-made tests using prior knowledge but is limited if the knowledge is incomplete. The incompleteness is often unavoidable when exploring various candidate variables. Data-adaptive approaches have been proposed in genomic studies (====, ====, ====, ====, ====, ====, ====, ====, ====, ====). However, these approaches often require complicated null distribution calculation either analytically or computationally, or otherwise are only applicable to low-complexity models. It is helpful if there is a framework that fits existing highly data-adaptive procedures to a hypothesis test without both custom-made modification and complicated null distribution computation.====This paper develops a flexible data-driven test procedure for conditional mean, directly applicable to various existing statistical models that bridge between low and high complexity models via a tuning parameter as in penalized regression. The test is based on the model that maximizes the Yanai’s generalized coefficient of determination (====, ====) generalized to any modeling procedure. It is proportional to the covariance between a response variable and its predicted value divided by the square root of the generalized degrees of freedom (====, ====, ====, ====, ====, ====, ==== without complicated null distribution computation using a significance threshold for the saturated model (or the largest model in the sequence). Since it is simple and simulation-free in computing ====-value, the proposed method is suitable for effect discovery in high-dimensional data which requires a large number of tests.====Through simulation studies for group-wise test problems, the proposed test adapted to the lasso (====), ridge (====, ====). Applied to a group-wise analysis for a real genome-wide association study data from Alzheimer’s Disease Neuroimaging Initiative (ADNI), the proposed test showed a higher association signal at the known risk variant than the existing methods.",Testing conditional mean through regression model sequence using Yanai’s generalized coefficient of determination,https://www.sciencedirect.com/science/article/pii/S0167947321000025,19 January 2021,2021,Research Article,270.0
"Hintz Erik,Hofert Marius,Lemieux Christiane","Department of Statistics and Actuarial Science, University of Waterloo, Canada","Received 29 November 2019, Revised 7 December 2020, Accepted 4 January 2021, Available online 15 January 2021, Version of Record 23 January 2021.",https://doi.org/10.1016/j.csda.2021.107175,Cited by (2),"Efficient algorithms for computing the distribution function, (log-)density function and for estimating the parameters of multivariate normal variance mixtures are introduced. For the evaluation of the distribution function, randomized quasi-Monte Carlo (RQMC) methods are utilized in a way that improves upon existing methods proposed for the special case of normal and ==== package ==== (version ====0.0.4).","The multivariate normal and (Student) ==== for more details.====Formally speaking, we say that a random vector ==== follows a ====, denoted ====, if, in distribution, ====where ==== denotes the ====, ==== for ==== is the ==== (a covariance matrix), and ==== (where ==== denotes the identity matrix), which we can think of as the mixing variable; see, for example, ====. Note that ====, hence the name of this class of distributions. This implies that if ====, then ====, and if ====, then ==== and ====). Furthermore, note that in the latter case with ==== (so when the components of ==== are uncorrelated) the components of ==== are independent if and only if ==== is constant almost surely and thus ==== is multivariate normal; see ====. The multivariate ==== distribution is obtained by letting ==== have an inverse-gamma distribution. In what follows we focus on the case ==== in which ====; other decompositions of ==== into ==== for some ==== can be obtained from the eigendecomposition or singular-value decomposition.==== based on the Cholesky factor ==== of ====.====In contrast, evaluating multivariate distribution functions (such as the normal and the ====) is a difficult, yet important problem that has gained much attention in the last couple of decades; see, for instance, ====, ====, ====, ==== and ==== as well as references therein for a discussion of the estimation of multivariate normal and ==== for the evaluation of truncated multivariate ==== distributions. To further illustrate how challenging this problem is, we note that the ==== package ==== (one of the most widely used packages according to reverse depends, see ====) and other ==== packages do not even provide functionality for evaluating the distribution function of the well-known multivariate ==== distribution for non-integer degrees of freedom ====.==== quantile function of ====.====Providing algorithms for the above tasks for a more general family of distributions than what currently exists in the literature is one of the main contributions of this work.====The algorithm we propose to efficiently evaluate the joint distribution function of a normal variance mixture (including the case when ==== is singular) is obtained by generalizing methods by A. Genz and F. Bretz to evaluate the distribution function of the multivariate normal and ==== distribution. In particular, we generalize a variable reordering algorithm originally suggested by ==== and adapted by ====, ==== respectively include the discussion of RQMC methods and the tasks of evaluating the joint distribution function.====Regarding the joint density function of ==== follows an inverse-Burr distribution. Since our goal is to provide algorithms that work for any normal variance mixture, an efficient algorithm to approximate the joint (log)-density function of ==== is needed. We tackle this by proposing in Section ==== can be estimated efficiently.====This flexible algorithm turns out to be a key ingredient for the task of parameter estimation. Here our contribution is to propose an algorithm that is general enough to handle any normal variance mixture with bounded density function, as explained in Section ====. More precisely, we employ an ECME (“Expectation/Conditional Maximization Either”) algorithm, which is a likelihood-based fitting procedure developed in ====. This procedure requires repeated evaluations of the log-density function of ====, which is one of the reasons why efficient algorithms for the latter are important when this density does not have a closed form.====. This section also includes a detailed investigation of why the reordering algorithm works well with RQMC methods, as well as a data analysis with real-world financial data.====All presented algorithms are available in our ==== package ==== (in particular, via ====, ====, ==== and ====; see also ====) and the conducted simulations are reproducible with the demo ====; see ====.====To the best of our knowledge, none of the four aforementioned tasks have been discussed in the literature in such generality where the only requirement is to have a computationally tractable quantile function for the mixing variable ====. By specifying the latter, methods developed in this paper (and the implementation in ====) can be used to perform standard modeling tasks for multivariate normal variance mixtures well beyond the case of a multivariate ==== distribution. To demonstrate this, a real financial data set is analyzed using an inverse-gamma, a Pareto and an inverse-Burr mixture at the end of Section ====.====If ==== is positive semidefinite with rank ====, the resulting singular normal variance mixture can be estimated by applying results described in ====, who developed an accurate method to evaluate the distribution function of a multivariate normal distribution with singular correlation matrix ====, see also (==== Section 5.2) for more details.====Let ==== with ==== for ====, ====, that is, ==== is lower triangular with some diagonal elements being zero; see ==== for an algorithm to compute such ==== which uses a numerical tolerance to determine zero-entries. After permutations and scalings (that must also be applied to ==== and ====), ==== shall have the following form where “====” denotes an entry that can be zero or non-zero: ====Note that ====. Define ==== with ====. As demonstrated in ====, ==== can then be written in a similar fashion as in ====: ====Note that the ====-dimensional integral still has ==== active constraints: For variable ====, the ==== constraints ==== for ==== need to be satisfied simultaneously so that the limits in ==== are given by ====This idea can be generalized to singular normal variance mixtures. Proceeding as in Section ==== one obtains ====where ==== The RQMC methods described in Section ==== can be applied to the problem in this form to estimate ====. The main difference is that the dimension of the problem in the singular case is given by the rank ==== as opposed to the dimension ==== of the normal variance mixture.","Normal variance mixtures: Distribution, density and parameter estimation",https://www.sciencedirect.com/science/article/pii/S0167947321000098,15 January 2021,2021,Research Article,271.0
"Lu Jun,Lin Lu,Wang WenWu","College of Liberal Arts and Sciences, National University of Defense Technology, Changsha, China,School of Statistics, Shandong Technology and Business University, Yantai, China, Yantai, China,School of Statistics, Qufu Normal University, Qufu, China","Received 8 June 2020, Revised 21 October 2020, Accepted 5 January 2021, Available online 14 January 2021, Version of Record 25 January 2021.",https://doi.org/10.1016/j.csda.2021.107176,Cited by (1),"This paper proposes a new screening procedure for the ultrahigh dimensional data with a categorical response. By exploiting the group structure among predictors, a new partition-based screening approach is developed via the ==== ==== (RKHS) embeddings in the maximum mean discrepancy framework. Consequently, the new method is able to identify the influential group of predictors that may be overlooked by the marginal screening methods. Moreover, by using the RKHS embedding, the new ranking index has a very simple form, and thus can be evaluated easily. As a by-product, the new method is model-free without specifying any relationship between the predictors and the response. The sure screening property of the proposed method is proved and the effectiveness of the new method is also illustrated via ==== and a real data analysis.",", these variable selection methods could face some challenges. The most immediate problem is the unbearable computational cost incurred by the ultrahigh dimensionality, it will not only heavily slow down the computing speed of the algorithm but also result in unstable solutions. Besides, the ultrahigh dimensionality also destroys the regular conditions of these variable selection methods, such as the uniform uncertainty principle condition in Dantzig selector (====).====To address the above problems, ==== proposed the concept of sure independence screening (SIS), which calculates the marginal utility of a predictor as a rough measure to rank the predictors. Since then, numerous literature has been proposed in various statistical models. ====, ====, ==== and ==== and ====, ====, ====, ====, ==== and the reference therein. From our perspective, we are in favour of the use of model-free screening methods at two-fold: (a) it avoids the risk of model misspecification, and (b) it brings feasibility for further statistical analysis, which means that once we get a low-dimensional predictor after the screening, we could try several models to fit the data and pick out the best one.====In this paper, we develop a new feature screening method for ultrahigh dimensional data with a categorical response, where the categorical response usually represents the label of a subject. This kind of data is frequently encountered by biostatistician who work on multi-class problems. Regarding the feature screening for such kind of data, several pieces of literatures have made some progress. The following are some typical relevant works. ==== put forward a screening procedure called fused Kolmogorov filter (FKL). ==== proposed a new categorical-adaptive variable screening by integrating the difference between the conditional distribution function of predictors and the unconditional ones (CAS). All the methods mentioned above are proposed based on the marginal idea that handles the predictors separately without considering the possible grouping structure. In some applications, however, some influential predictors showing similar functionality could be marginally negligible but combined as a group would have a nontrivial effect on the outcome variable. For instance, the genome-wide association studies (GWAS) have observed that the arise of some diseases may be affected by the synergy of several genes together, instead of a single one. In such situations, leveraging the group information could be helpful to the feature screening.====Motivated by the above observations, we propose a partition-based screening method which is effective to detect the group having active predictors. We achieve this goal by employing a very general framework called maximum mean discrepancy (MMD), which is firstly proposed by ==== and recently is further studied by ====The rest of the paper is organized as follows. Section ==== is the main methodological development of the newly proposed screening method, the theoretical guarantees are also provided in this section. Section ==== presents some discussion on our method. Technical proofs are postponed to the Appendix.====The following is the Supplementary material related to this article. ",Partition-based feature screening for categorical data via RKHS embeddings,https://www.sciencedirect.com/science/article/pii/S0167947321000104,14 January 2021,2021,Research Article,272.0
"He Yizeng,Kim Soyoung,Kim Mi-Ok,Saber Wael,Ahn Kwang Woo","Division of Biostatistics, Medical College of Wisconsin, Milwaukee WI 53226, USA,Department of Epidemiology and Biostatistics, University of California, San Francisco CA 94143, USA,Division of Hematology and Oncology, Medical College of Wisconsin, Milwaukee WI 53226, USA","Received 30 May 2020, Revised 18 December 2020, Accepted 29 December 2020, Available online 14 January 2021, Version of Record 8 February 2021.",https://doi.org/10.1016/j.csda.2021.107167,Cited by (4)," penalties to select important variables at both group and within-group levels for competing risks data. The proposed method is applied to ==== transplantation data to personalize the graft source choice for treatment-related mortality (TRM). While the existing medical literature attempts to find a uniform solution ignoring the heterogeneity of the graft source effects on TRM, the analysis results show the effect of the graft source on TRM could be different depending on the patient-specific characteristics.","In a single treatment decision setting, a treatment regime is a rule that assigns a treatment to a patient based on his/her observed characteristics. The assignments are evaluated by a health outcome of interest, where we assume, without loss of generality, that a smaller value implies a greater treatment benefit. Accordingly, a treatment regime is evaluated by the average health outcome computed at a population level as if all patients were treated as dictated by the treatment regime. Thus, the goal of statistical analysis regarding the personalized treatment assignment is to identify the optimal regime which maximizes treatment benefits at the population level or yields the smallest average outcome value.====). The impact of the choice of graft source on competing risks outcomes such as treatment-related mortality (TRM) is inconclusive, where TRM is death without experiencing relapse and relapse is a competing risk. For example, ==== reported PB had a lower TRM rate in the early period, but a higher TRM rate at 5 years after transplant. On the other hand, ==== concluded BM leads to a lower TRM rate compared with PB. The increased TRM risk with PB is usually mediated by an increased risk of graft vs. host disease (GVHD). However, GVHD is also a surrogate measure of graft vs tumor effect (GVT) which is of paramount importance to reduce relapse risk. In patients with high-risk disease (for example, refractory leukemia at the time of HCT), having a robust GVT is critically important, since in these patients treatment failure post HCT is usually mediated by relapse. So even though GVHD is increased with PB (and hence higher TRM risk), relapse risk is usually decreased. The risks and benefits from personalized decision making are very different among patients who have the favorable-risk disease, where relapse risk is not a major concern. For such patients, it is important to identify approaches to reduce TRM. It is of great clinical interest to study which graft source works better based on the patients’ characteristics at the time of transplant (====).====, ====, ====, ====). However, the patient’s characteristics often exhibit group structures. For example, as shown in Table 14 in Section ==== of the Supplementary Material, many covariates examined in ==== for a comprehensive review), but little when the optimal treatment regime is concerned.====As for literature on the optimal treatment regime for survival outcomes, ====, ====, and ==== proposed nonparametric estimators for the cumulative incidence function (CIF) of dynamic treatment regimes. ====To address the limitations of the current literature on competing risks outcomes, we propose a doubly robust method for identifying the optimal treatment regime with a bi-level variable selection procedure. Our contributions are three folds. First, we revise and adapt the method of ==== to the competing risks setting based on the CIF. The proposed doubly robust method protects against model misspecification of either the propensity score or the outcome regression. Second, we propose new adaptive ==== to personalize the graft source choice to reduce the TRM incidence in the population. Although clinicians are interested in identifying the more beneficial graft source choice between BM and PB for the population in HCT, existing literature has been inconclusive. Our analysis provides a different perspective on this important decision: individualizing the choice of graft source for HCT.====The following is the Supplementary material related to this article. ",Optimal treatment regimes for competing risk data using doubly robust outcome weighted learning with bi-level variable selection,https://www.sciencedirect.com/science/article/pii/S0167947321000013,14 January 2021,2021,Research Article,273.0
"Sun Dayu,Zhao Hui,Sun Jianguo","Department of Statistics, University of Missouri, Columbia, MO 65211, USA,School of Statistics and Mathematics, Zhongnan University of Economics and Law, Wuhan 430070, China","Received 25 May 2020, Revised 18 October 2020, Accepted 12 December 2020, Available online 11 January 2021, Version of Record 22 January 2021.",https://doi.org/10.1016/j.csda.2020.107161,Cited by (2),"A great deal of literature has been established for ==== can be observed completely or at the same observation times for the response variable, and the observation process is independent of the response variable completely or given covariates. As pointed out by many authors, in practice, one may face the situation where the response variable and covariates are observed intermittently at different ",", ====Some literature has been developed for regression analysis of synchronous longitudinal data with informative observation processes and among others, early work includes ==== and ====, ==== and ==== provided an estimating equation-based method when the follow-up time may be informative too. In addition, among others, ==== and ====, ==== discussed the same problem considered here. However, they only considered the situation where the response variable of interest represents some cumulative counts, a special case of the longitudinal variable to be investigated here. Also the methods developed by these authors require the assumption that covariates can be completely observed, while the method proposed below allows for the covariates to be observed only at some discrete time points. In summary, it does not seem that there exists an established approach that can allow both the asynchronicity and the informative observation process. As pointed out by many authors, when they exist, the analysis that ignores either the asynchronicity or the informative observation process could result in biased results and misleading conclusions. In the following, we will present a class of general and flexible models and an estimating equation-based approach that can deal with both issues.====The remainder of the paper is organized as follows. In Section ====, the estimating equation-based approach described in Section ==== for the synchronous case will be generalized to asynchronous longitudinal data situations with the use of the kernel weighting technique similar to that used in ==== and also in the section, the bandwidth selection for kernel weighting is discussed. Some results obtained from an extensive simulation study are presented in Section ==== and indicate that the proposed method works well in practical situations. In Section ====, we apply the proposed approach to the HIV longitudinal data described above and Section ==== contains some discussion and concluding remarks.====The following is the Supplementary material related to this article. ",Regression analysis of asynchronous longitudinal data with informative observation processes,https://www.sciencedirect.com/science/article/pii/S0167947320302528,11 January 2021,2021,Research Article,274.0
"Allassonnière Stéphanie,Chevallier Juliette","Centre de Mathématiques Appliquées, Écoles polytechnique, Palaiseau, France,Équipe Maasai, Inria Sophia-Antipolis, Université Côte d’Azur, Nice, France,Centre de Recherche des Cordeliers, Université Paris-Descartes, Paris, France","Received 22 April 2020, Revised 9 December 2020, Accepted 11 December 2020, Available online 9 January 2021, Version of Record 8 March 2021.",https://doi.org/10.1016/j.csda.2020.107159,Cited by (13),The expectation–maximization (EM) algorithm is a powerful computational technique for ,"Although the expectation–maximization (EM) algorithm (====, ====) is a very popular and often efficient approach to ==== likelihood (or ====, ====), in which a Monte Carlo implementation of the expectation in the expectation-step is carried out. Although the MCEM algorithm provides an elegant solution, it requires a constant increase in the amount of simulated data throughout the algorithm and often a large number of simulations (====, ====).====In an alternative way, ====. Essentially, the SAEM algorithm obtains an increasingly accurate approximation of the expectation by computing a weighted average of the empirical mean and all the approximations from previous iterations. Hence, all the simulated data contribute to approximate the expectation. A decreasing sequence of step-sizes allows benefiting more and more from these preceding iterations while placing less and less emphasis on the last, more imprecise Monte Carlo approximation. This method is proved to converge toward stationary points of the log-likelihood, without increasing the sample size at each iteration (====). Moreover, under certain conditions on the likelihood, the convergence toward local ==== is guaranteed (====). One hope is that in addition to avoiding saddle points and circumventing the computation of the expectation, introducing randomness may enable to escape local ====. However, this is neither yet theoretically proved nor numerically illustrated in the literature.====The EM algorithm has a long and rich history, with a vast literature (====, ====), and it is still under development (====). We focus here on the stochastic variants of the EM algorithm and, more precisely, on its stochastic approximation. The SAEM is still attracting significant research interest, for example, concerning the choice of the step-sizes and the appropriate stopping rule (====). Moreover, its numerical efficiency has been demonstrated in several situations, such as inference in hidden Markov models (====).====, ====, ====, ====), resulting in the MCMC–SAEM algorithm. MCMC methods require Markov chains with millions of iterations until they converge to their stationary distribution (====). One of the main advantages of the MCMC–SAEM algorithm is that the Markov chain does not need to reach the stationary distribution for the MCMC–SAEM to converge toward local ====. Indeed, the stochastic approximation allows counterbalancing these short Markov chains. Thus, the underlying idea behind MCMC–SAEM is not only to use Markov chains to sample under the conditional distribution but also to ask to what extent an inaccurate simulation will degrade the convergence of the SAEM algorithm. The MCMC–SAEM algorithm is particularly well suited to studying non-linear mixed-effect models (====) and is constantly improving. For example, improvements have been proposed to consider missing data (====) or speed its convergence (====, ====). When the posterior distribution is not analytically available, one can sample from an approximation of the posterior distribution, ====) for a review). In this sense and to overcome the sampling issue, ==== do not provide any theoretical guarantee of its convergence.====Moreover, despite appealing features, the limit position of the SAEM algorithm can strongly depend on its initialization. To avoid convergence toward local ====, ====, at each iteration ==== of the SAEM algorithm, they consider the “false” model in which the noise variance is equal to ====, where ==== is a positive sequence of temperatures that decreases slowly toward 0. Therefore, the bigger ==== is, the more the likelihood of the model is flattened, and the optimizing sequence can escape easily from local ====, and a fortiori, from saddle points. The simulations gave good results, but there was no theoretical guarantee for this procedure. Based on the same idea, ==== has proposed using the simulated-annealing process as a “trick” to better initialize the SAEM algorithm. This initialization scheme is implemented in the ==== software and gives impressive results on real data (====, ====, ====).==== and the ABC–SAEM algorithms (====), to cope with intractable or difficult sampling. We refer to this new algorithm as the approximated-SAEM.====This general framework allows us to build a procedure, in the spirit of the simulated annealing version of the SAEM (====), to favor convergence toward global ====) raised.====The paper is organized as follows: Section ==== briefly recalls the SAEM algorithm and provides details on the theoretical guarantees surrounding its convergence. In Section ====, we introduce our new stochastic approximation version of the EM algorithm, namely the approximated-SAEM, and prove the convergence of this algorithm toward critical points under similar assumptions to those of the initial SAEM. By its similarity with the proof of the convergence of SAEM, our proof highlights the non-binding nature of the different hypotheses and thus the great applicability of our algorithm. In particular, under the same assumptions about the model’s likelihood as those introduced by ====, it converges toward local ====. For the sake of brevity, the evidence of the convergence of the approximated-SAEM is postponed in ====. We then introduce the tempering-SAEM and provide a theoretical study of its convergence toward stationary points. We also give a heuristic to its convergence toward “less local” ====. Section ==== is dedicated to experiments. The first application we consider is the ====.====The demonstration consists of applying Theorem 2 of ====; in order to make our article more self-contained, we start by recalling it (====). In particular, Assumptions ==== in the demonstration of the convergence theorem refer to their hypotheses. Moreover, since it is used in our demonstration, we also recall Lemma 2 from the same paper (====).====Lemma 2 of ==== ensures the existence of a Lyapunov function for our algorithm.",A new class of stochastic EM algorithms. Escaping local maxima and handling intractable sampling,https://www.sciencedirect.com/science/article/pii/S0167947320302504,9 January 2021,2021,Research Article,275.0
"Dyckerhoff Rainer,Mozharovskyi Pavlo,Nagy Stanislav","Institute of Econometrics and Statistics, University of Cologne, Albertus-Magnus-Platz, 50923 Köln, Germany,LTCI, Telecom Paris, Institut Polytechnique de Paris, 19 Place Marguerite Perey, 91120 Palaiseau, France,Faculty of Mathematics and Physics, Charles University, Sokolovská 83, Praha 8, CZ-186 75, Czech Republic","Received 17 July 2020, Revised 21 November 2020, Accepted 28 December 2020, Available online 9 January 2021, Version of Record 25 January 2021.",https://doi.org/10.1016/j.csda.2020.107166,Cited by (4),Data depth is a concept in ==== that measures the centrality of a point in a given data cloud in ,", the depth assigns to any point of the space ==== a real number, often scaled to ====, ====. For more information on applications of data depth we refer to surveys ==== and ====.====) – one of the most important depth notions in the literature – is an NP-hard problem (====), and the only exact existing algorithm for computation of the projection depth (====) is still very slow. For this reason, theoretical developments on the data depth are accompanied by a substantial body of literature on its computation, which still contains a number of open problems. Since exact computation of certain depth notions can come at a very high computational cost (see, e.g., ==== for the halfspace depth and ==== described a class of depths which satisfy the weak projection property. Out of the existing variety, these can be calculated in a universal way by minimizing depth in univariate projections. For this, in each direction, only the univariate depth of ==== observations should be computed, which often has computational time complexity only ====. This class of depths constitutes the focus of the current work. Among the depths that satisfy the projection property, Mahalanobis (====), zonoid (====), halfspace (====), projection (====) and asymmetric projection (====) depth are considered here.====Several authors have already applied approximation techniques to the (sample) depth computation, notably for the halfspace depth and projection depth. Purely random methods seem most intuitive and have been used, e.g., by ==== for the halfspace depth and ==== for the projection depth. More sophisticated procedures were proposed as well. ==== minimize univariate halfspace depth after projecting data onto directions based on a random combination of sample points. ==== points from the sample, and then employ a brute-force approximation of the halfspace depth in these projections. ==== accelerate the problem of approximation of the halfspace depth of many points (and of the sample itself) w.r.t. a sample by preliminary sorting the data in all projections. ==== use the Nelder–Mead algorithm (==== run in ====) for approximation of the projection depth.====In this work, a systematic experimental approach is used to study the behavior of the approximation of the sample depth by minimizing it on univariate directions. For minimization of the univariate depth, we contrast eight ==== in an extensive simulation study. As algorithms we considered: (i) random search, (ii) grid search, (iii) refined random search, (iv) refined grid search, (v) random simplices, (vi) simulated annealing, (vii) coordinate descent, (viii) Nelder–Mead. Since the performance of the algorithms depends on chosen parameters, we start by fine-tuning of the algorithms. After this, the algorithms are compared in different settings to provide a trustworthy conclusion.====Throughout this paper we use the following notation. Vectors are notated with bold letters, e.g., ====. ==== denotes the set of non-negative real numbers. The ==== is denoted by ====. The transpose of a vector ==== is denoted by ====. The ====, or shortly ====, and ==== or ==== denotes the origin in ====. For a set ==== we denote by ==== the uniform distribution on the set ====, and ==== and ==== are denoted by ====. By ==== we mean equality in distribution.====The following is the Supplementary material related to this article.====Supplementary materials to this article include:====
 ====
 ",Approximate computation of projection depths,https://www.sciencedirect.com/science/article/pii/S0167947320302577,9 January 2021,2021,Research Article,276.0
Sundararajan Raanju R.,"Department of Statistical Science, Southern Methodist University, Dallas, TX 75275, USA","Received 22 June 2020, Revised 23 December 2020, Accepted 27 December 2020, Available online 6 January 2021, Version of Record 16 January 2021.",https://doi.org/10.1016/j.csda.2020.107164,Cited by (2),"Dimension reduction techniques for ==== are derived. Finally, a consistent test on the cross-spectrum of pairs of components is used to find the desired segmentation into the lower-dimensional subseries. The numerical performance of the proposed method is illustrated through simulation examples and an application to modeling and forecasting wind data is presented.",", ====, ==== and imposing structural restrictions on the VAR model is another approach in Chapter 9 of ====.====Principal component analysis (PCA) for time series is another well-known method for dimension reduction. The classical dynamic PCA for time series from Chapter 9 in ==== expresses the observed multivariate series as a two-sided moving average of an uncorrelated vector process. This process is called the principal component series and has a diagonal spectral matrix with no spectral coherence between any two components. Chapter 9 of ====-variate series. This ====, ====, ==== consider a similar setup but without considering the lagged dependence. ====, the component series are fitted with univariate autoregressive (AR) models with order chosen by AIC and the residuals from the fit are considered as the pre-whitened series. This task of fitting AR models to processes having strong periodicities or strong moving average components can be difficult as it can potentially lead to large AR model orders.====In Section ====. We compare the performance of our method with competitors using simulation examples in Section ====. We discuss here the advantages of our spectral domain method over its time domain counterparts in models that have strong periodic components and/or strong moving average components. In Section ====.====The proof of ==== follows from an application of Theorem 8.1.10 and Corollary 8.1.11 of ==== that are on the perturbation of invariant subspaces. We first state those two results in the following lemma. We denote ==== as the ==== norm of the matrix ====.",Principal component analysis using frequency components of multivariate time series,https://www.sciencedirect.com/science/article/pii/S0167947320302553,6 January 2021,2021,Research Article,277.0
"Cao Jian,Keyes David E.,Turkiyyah George M.","CEMSE Division, Extreme Computing Research Center, King Abdullah University of Science and Technology, Thuwal 23955-6900, Saudi Arabia,Department of Computer Science, American University of Beirut, Beirut, Lebanon","Received 4 January 2020, Revised 17 December 2020, Accepted 24 December 2020, Available online 6 January 2021, Version of Record 25 January 2021.",https://doi.org/10.1016/j.csda.2020.107165,Cited by (1),"-dimensional covariance matrix is ====, where ",") and the covariance block between two separable (==== and ==== studied the SKP and the diagonally loaded SKP representations, respectively, concluding their suitability for estimating spatio-temporal covariance structures due to their higher energy concentration in the first few principal components than the low-rank representation. Nonetheless, the spatial dimensions in ==== and ====), in up to one million dimensions, which is new in the literature. Notice that we use the term ‘dimension’ to interchangeably refer to the number of spatial locations and the number of rows/columns of a matrix (block), which in fact, have a corresponding relationship introduced in Section ====.====The prototype of the SKP representation is the nearest Kronecker product (NKP) problem defined in ====, ====, with ==== terms, ====. Here, we denote the matrix under approximation by ====. It is different from the original work because we focus on covariance matrices. Given predefined dimensions of ==== and ====) and in fact, ==== largest singular values is sufficient, hence reducing the complexity of constructing the SKP representation from ==== to ==== for the ====-dimensional matrix ====. In this paper, we introduce an ====), suitable for spatial covariance matrices from large 2D grids. The impression of ‘large’ when describing the matrix dimension is usually supplemented by the research topic, for example, ==== studied low-rank tensor approximation targeting matrices of ==== dimensions while here, we consider dimensions above ==== as large.====The paper is organized as follows: Section ==== introduces an ACA-based algorithm for constructing the SKP representation, which amounts to a block variant of the classic ACA (====). Section ==== discusses the quasi-optimal strategy for indexing the 2D spatial locations and choosing the dimensions of the Kronecker factors, ==== and ====, to minimize the memory footprint. In Section ====, a Cholesky factorization algorithm under the SKP representation is proposed together with its complexity analysis as a prototype for the potential linear algebra operations under the SKP representation. Section ==== discusses the storage efficiency and the computational limitations of the SKP representation. A list of symbols containing the variables necessary for the pseudo-algorithm of the Cholesky factorization is provided in the ====.====List of symbols containing the variables necessary for the pseudo-algorithm of the Cholesky factorization.",Sum of Kronecker products representation and its Cholesky factorization for spatial covariance matrices from large grids,https://www.sciencedirect.com/science/article/pii/S0167947320302565,6 January 2021,2021,Research Article,278.0
"Yuan Quan,Liu Binghui","Key Laboratory for Applied Statistics of MOE and School of Mathematics and Statistics, Northeast Normal University, China","Received 4 March 2020, Revised 19 October 2020, Accepted 15 December 2020, Available online 30 December 2020, Version of Record 7 January 2021.",https://doi.org/10.1016/j.csda.2020.107163,Cited by (9),"Maximizing modularity is a widely used method for community detection, which is generally solved by approximate or greedy search because of its high complexity. In this paper, we propose a method, named MSM, for modularity maximization, which reformulates the modularity maximization problem as a subset identification problem and maximizes the surrogate of the modularity. The surrogate of the modularity is constructed by replacing the discontinuous indicator functions in the reformulated modularity function with the continuous truncated ",", ====, ====), information networks (====, ====), etc. In many cases, the network units can be divided into groups with the property that there are many edges between the units in the same group, but relatively few edges between the units in the different groups. Such type of groups are viewed as communities, which are often associated with important structural characteristics of a complex system.====) and the Lernighan–Lin algorithm (====, ====) and the modularity optimization methods (====, ====, ====, ====), the degree-corrected stochastic block models (DCSBMs) (====, ====) and the latent space models (====). In addition, some methods are designed to deal with the community detection problem with overlap (====, ====, ====, ====), that is, the nodes in the network may belong to more than one community.====, ====). Under the modularity framework, maximizing the modularity function is the key problem, which is actually a NP-hard problem (====). The earliest algorithm for maximizing the modularity function is the GN algorithm (====), which is a greedy algorithm. To reduce some useless operations of the GN algorithm in situation of sparse networks, ==== proposed the CNM algorithm. These two algorithms are based on hierarchical search, while some follow-up algorithms are established based on spectral optimization. For example, ==== rewrote the expression of modularity as the eigenspectrum of the modularity matrix, and then proposed the EIGN algorithm based on this expression. In addition, there are some methods to optimize modularity based on block model (====).====These algorithms are approximate optimization of modularity, which try to find a proper balance between community detection accuracy and computational efficiency. Besides, there are many useful strategies for approximate optimization, some of which attempted to relax the binary membership assignment to a continuous version to ease the optimization (====, ====). In particular, ==== function proposed by ====. By drawing on the idea of ==== and ====, we reformulate the community detection problem as a subset identification problem, which is solved by maximizing the surrogate of modularity. Then, the proposed method is named as Maximizing the Surrogate of Modularity, which is written as MSM for short. Specifically, the surrogate of modularity is constructed by replacing the discontinuous indicator functions in the reformulated modularity function with the continuous truncated ====The rest of this paper is organized as follows. We elaborate the definition of modularity and the proposed algorithm in Section ====. Then, we present the simulation results of the proposed algorithm and some related algorithms in Section ====, followed by some real data analyses in Section ====. Finally, we conclude this paper in Section ====.",Community detection via an efficient nonconvex optimization approach based on modularity,https://www.sciencedirect.com/science/article/pii/S0167947320302541,30 December 2020,2020,Research Article,279.0
"Du Mingyue,Li Huiqiong,Sun Jianguo","Department of Applied Mathematics, The Hong Kong Polytechnic University, Hong Kong, China,Department of Statistics, Yunnan University, Kunming, 650091, China,Department of Statistics, University of Missouri, Columbia, MO, 65211, USA","Received 14 February 2020, Revised 15 October 2020, Accepted 9 December 2020, Available online 27 December 2020, Version of Record 31 December 2020.",https://doi.org/10.1016/j.csda.2020.107157,Cited by (2),"In this paper, we discuss ==== of censored failure time data when there exist missing ","). However, sometimes one may face or has to deal with nonignorable missing, meaning that the missing may depend on both the observed and missing values (====This work was motivated by an Alzheimer’s Disease study, the Alzheimer’s Disease Neuroimaging Initiative (ADNI), which is a longitudinal multi-centre study designed to develop clinical, imaging, genetic, and ====In addition to the study described above, missing data occur in many other areas such as longitudinal follow-up studies and sample survey and also in many forms in terms of missing parts and missing mechanism (====, ====, ====, ==== described many such nonignorable examples. In the case of interval-censored data, a general example of nonignorable missing covariates occurs when there exist some internal covariates or correlated longitudinal covariates (====, ====).====As mentioned above, many methods have been proposed in the literature for regression analysis of censored failure time data with missing covariates (====, ====, ====, ====, ====, ====, ====), under the semiparametric linear transformation model, a class of flexible models that includes many commonly used models as special cases (====, ====, ====).====The remainder of this paper is organized as follows. In Section ====, we will first describe some notation and the models and some assumptions that will be used throughout the paper. A two-step estimation procedure is then proposed in Section ====, for the implementation of the proposed estimation procedure, a novel EM algorithm is developed with the use of Poisson variables in the data augmentation part. Section ==== presents some results obtained from a simulation study conducted for the assessment of the finite sample properties of the proposed method, and they suggest that the approach works well for practical situations. In Section ====, we apply the proposed approach to the AD study described above and Section ==== contains some discussion and concluding remarks.====In this Appendix, we will sketch the proof for the consistency and asymptotic normality of the proposed estimators described above by employing the empirical process theory and nonparametric techniques. Define ====, and ==== for a function ====, a probability function ==== and a sample ====. For the proof, we need the following regularity conditions.====(A1) Assume that ====, ====, and there exists a positive constant ==== such that ====. Also the union of the supports of ==== and ==== is contained in the interval ==== with ====.====(A2) The transformation function G is twice continuously differentiable on ==== with ==== and ====.====(A3) The set of covariates ==== has bounded support.====(A4) The bandwidth ==== satisfies that ====, ==== and ====.====(A5) The function ==== is continuously differentiable up to order ==== in ====, with the first derivative being strictly positive, and satisfies ==== for some positive constant ====.====(A6) If ==== for all ==== with probability 1, then ==== for ====, ==== and ====.====First we will prove the consistency and for this, we will verify the conditions of Theorem 5.7 of ====. Let ==== denote the functions whose total variation in ==== are bounded by a given constant. Then the class of functions ====is a convex hull of functions ====, so it is a Donsker class. Furthermore, ====is bounded away from zero. Therefore, ==== belongs to some Donsker class due to the preservation property of the Donsker class under Lipschitz-continuous transformations. Then we can conclude that ==== converges in probability to 0 as ====.====Now we verify that another condition of Theorem 5.7 of ==== also holds. That is, for any ====, we have ====Note that this condition is satisfied if we can prove the model is identifiable. According to condition (A6) and similar arguments to the proof of Theorem 2.1 of ====, we can show the identifiability of the model parameters. Now, by Theorem 5.7 of ====, we have ====, which completes the proof of consistency.====Before proving the asymptotic normality, we will need to establish the convergence rate. For this, we will first define the covering number of the class ==== and establish a needed lemma.====To establish the convergence rate, for any ====, define the class ==== with ====. Following the calculation of ====, we can establish that ==== with ====, where ==== denotes the bracketing number (see the Definition 2.1.6 in ==== with respect to the metric or semi-metric d of a function class ====. Moreover, some algebraic calculations lead to ==== for any ====. Therefore, by Lemma 3.4.2 of ====, we obtain ====where ====. The right-hand side of ==== yields ====, where ==== is a positive constant. Then ==== is a decreasing function, and ====. According the theorem 3.4.1 of ====, we can conclude that ====.====Now we prove the asymptotic normality of ====. Following the proof of Theorem 2 in ====, one can obtain that ====where ==== is the score function for ====, ==== is the score function along this submodel ====. This implies that the influence function for ==== is exactly the efficient influence function, so that ==== converges to a zero-mean normal random vector whose covariance matrix attains the semiparametric efficiency bound (==== p. 65).",Regression analysis of censored data with nonignorable missing covariates and application to Alzheimer Disease,https://www.sciencedirect.com/science/article/pii/S0167947320302486,27 December 2020,2020,Research Article,280.0
"Zhang Hongmei,Huang Xianzheng,Han Shengtong,Rezwan Faisal I.,Karmaus Wilfried,Arshad Hasan,Holloway John W.","Division of Epidemiology, Biostatistics, and Environmental Health, School of Public Health, University of Memphis, Memphis, TN, USA,Department of Statistics, University of South Carolina, Columbia, SC, USA,Joseph J. Zilber School of Public Health, University of Wisconsin, Milwaukee, WI, USA,School of Water, Energy and Environment, Cranfield University, Cranfield, Bedfordshire, UK,Clinical and Experimental Sciences, Faculty of Medicine, University of Southampton, Southampton, UK,David Hide Asthma and Allergy Research Centre, Isle of Wight, UK,Human Development and Health, Faculty of Medicine, University of Southampton, Southampton, UK","Received 17 June 2020, Revised 23 October 2020, Accepted 7 December 2020, Available online 26 December 2020, Version of Record 30 December 2020.",https://doi.org/10.1016/j.csda.2020.107156,Cited by (2)," sites (CpG sites), the proposed approach is further examined on its ability to detect network differentiations. Findings from theoretical assessment, simulations, and real data applications support the efficacy and efficiency of the proposed method for network comparisons.","In the following, we provide a derivation of the approximated full conditional posterior of ====, ==== It is an approximation of the posterior distribution of ==== conditional on ==== and variance components, ==== and ====, for a given graph ordering ==== with ==== being a collection of indicators representing inclusion or exclusion of parental nodes at each node. The justification laid out in this session follows in spirit the justification of Bayesian Information Criterion in ====. The conditional posterior probability of ==== conditional on ==== and the variance components for a given ordering is ==== The last proportionality is due to the choice of a non-informative prior in our work, ====.====In the following, we omit the dependence on ==== and the variance components for notation simplicity, but it needs to be clear that all the derivations are conditional on ====. The distribution of ==== conditional on ==== and ==== is, ====
 where ==== denotes regression coefficients of connected edges at node ==== under ====, and ==== with ==== an identity matrix and ==== known and large to formulate a non-informative but proper prior distribution for ====.====Take the natural logarithm transformation of the likelihood function ==== and perform Taylor expansion at ====, a consistent estimator of ==== such that ====. We have, for a large ====, ==== where “====” denotes “asymptotically equal to”, and ====is the sample Fisher information matrix.====Exponentiate both sides, ==== which gives ====For the integration in ====, ==== where ==== is a constant representing the normalizing constant for the prior of ====, ==== is a constant combining ==== and terms not involving ====, ====, and ==== is the number of parents of node ====.====Recall that ==== is the variance in the prior distribution of ==== and chosen to be large to construct a non-informative but proper prior for ====. When the sample size ==== is large, information in the data dominates the priors, ====We thus have ==== where ====. ==== is a constant, since ==== converges as ==== and based on assumption (3), ====. Note that ==== is ==== defined in Eq. ==== in the main text under the Bayesian context. In a Gibbs sampler, ==== is represented by posterior samples of ====.====The same derivation applies to the calculation of ====, which gives ==== where ==== is constant, and, as above, ==== is equivalent to ==== defined in Eq. ==== in the main text.====Now we have, ==== where ====. The last approximation is due to ==== being bounded as ====, conditional on the following assumptions, (1) ====, and ==== are in the order of ==== and ====, (2) ==== and ==== approaches to infinity in the same speed, and (3) ==== and ==== as ====. We denote the approximated conditional posterior of ==== as ==== with ==== acting like a penalty determined by sample size and conditional on edges of inferred graphs.",Gaussian Bayesian network comparisons with graph ordering unknown,https://www.sciencedirect.com/science/article/pii/S0167947320302474,26 December 2020,2020,Research Article,281.0
"Hashemi Farzane,Naderi Mehrdad,Jamalizadeh Ahad,Bekker Andriette","Department of Statistics, Faculty of Mathematical Sciences, University of Kashan, Kashan, Iran,Department of Statistics, Faculty of Natural & Agricultural Sciences, University of Pretoria, Pretoria, South Africa,Department of Statistics, Faculty of Mathematics and Computer, Shahid Bahonar University of Kerman, Kerman, Iran","Received 7 August 2019, Revised 23 November 2020, Accepted 12 December 2020, Available online 19 December 2020, Version of Record 26 December 2020.",https://doi.org/10.1016/j.csda.2020.107162,Cited by (5),". An efficient and computationally tractable EM-type algorithm is adopted for computing the ==== by presenting a hierarchical representation of the proposed model. Finally, the efficiency and advantages of the proposed novel methodology are demonstrated through both simulated and real benchmark datasets."," be a set of ==== independent and identically distributed (====) random vectors followed by a ====-dimensional continuous distribution. The FA model can then be formulated as ====where ==== denotes the ====-variate normal distribution with mean vector ====, ====, and the symbol ‘==== is a location vector, ==== is the matrix of factor loadings, ==== with ==== being the latent variables called ====, ==== denote the model errors called ====, and ==== where ====. It can be seen from ==== that ====, ==== and ====.====The multivariate normality assumption for the factors of the model ==== provides a mathematically as well as computationally tractable method to investigate the complex correlations between the variables under consideration (====). However, the robustness of the model against atypical observations is often criticized in relation to real-world problems (====, ====, ====, ====). In this regard, the interest in skew distributions provide a platform for a robust extension of the FA model. For instance, ==== proposed a factor model characterized by skew-normally (====) distributed factors. ==== proposed a new generalization of the rSNFA and student-==== FA (====FA; ====) models by assuming the restricted multivariate skew-====) are equivalent to the classical versions, proposed by ==== and ====, after appropriate re-parameterization. The rMSN model belongs to the class of mean-mixture of normal (MMN) distributions. Recently, ==== extended the MMN method to obtain models that not only have an equal number of parameters, but are also more flexible than the rMSN or rMST distributions. Specifically, a ====-dimensional random vector ==== is said to have an MMN distribution if it can be generated through the linear stochastic relationship ====where ====, and ==== is an arbitrary random variable. It is obvious that model ==== is a symmetrically distributed model. However, a more flexible and skew-type one can be obtained based on the assumption that ==== in ==== follows any asymmetric distribution, preferably a positive support model such as the truncated-normal, exponential and gamma distributions. Alternatively, the MMN distribution might also belong to the class of skew-elliptical models (====) if, for example, one considers that ==== follows the truncated-normal model. Proposing any non-elliptical as well as non-symmetric distribution (e.g. the exponential and gamma models) for the mixing random variable ====, ==== would lead to a skew non-elliptically contoured distribution. By introducing two new special cases of the MMN model, ====-normal (====With respect to the mentioned properties of ====, the objective of this paper is to propose a new factor model by assuming the MMN distribution for the factors. The proposed hierarchical representation enables the development of an expectation–maximization (EM; ====) type algorithm for computing the maximum likelihood (ML) estimates of parameters. In the rMST-based models, especially rSTFA, it is known that the rMSN-based models are obtained as the degree of freedom tends to infinity. A simulation study in Section ==== shows that the proposed model outperforms both rSNFA and rSTFA models when the degree of freedom increases. The mathematical and computational efficiency of the presented methodology, namely the finite sample properties and outperformance in dealing with the highly skewed data, are also verified. Finally, two real-world datasets provide a comparative analysis of the performance of the new factor model compared with some existing FA models.====The layout of the paper is as follows. In Section ====, the MMN model formulation and some of its characteristics are presented. Section ==== presents the formulation of the MMN factor analysis (MMNFA) model along with its parameter estimation. Three simulation scenarios are conducted in Section ==== to investigate the performance of the model and to study the finite sample properties of the proposed EM-based estimators. The usefulness of the proposed method is illustrated in Section ==== by analyzing two real datasets. Finally, discussion and suggestions for future work follow. Some technical details and additional information are provided in the Online Supplement.====The following is the Supplementary material related to this article. ",A flexible factor analysis based on the class of mean-mixture of normal distributions,https://www.sciencedirect.com/science/article/pii/S016794732030253X,19 December 2020,2020,Research Article,282.0
"Shi Jianhong,Zhang Yujing,Yu Ping,Song Weixing","School of Mathematics and Computer Sciences, Shanxi Normal University, Linfen, 041000, China,Department of Statistics, Kansas State University, Manhattan, KS 66506, United States of America","Received 9 October 2019, Revised 11 December 2020, Accepted 12 December 2020, Available online 19 December 2020, Version of Record 24 December 2020.",https://doi.org/10.1016/j.csda.2020.107158,Cited by (4)," modal regression models when the ==== are prone to measurement errors. Large sample properties of the proposed estimator, including the consistency and ====, have been thoroughly investigated. Simulation studies and real data applications have been conducted to evaluate its robustness to potential outliers and its effectiveness in reducing bias caused by the measurement error."," and ====, ==== was eventually refined in ==== discussed the same estimation procedure independently of ====, additionally developing the breakdown point theory and providing a data-driven bandwidth selector. Recently, ====The above-mentioned literature assumes that all variables in the regression models are precisely observed. However, in real applications, some variables cannot be measured precisely due to various reasons, see ====’s estimator by assuming that the measurement error is normally distributed, and the estimating function is entire with respect to the predictors. In fact, the corrected score estimators proposed in ==== and ==== discussed the modal regression using a mixture of classical and deconvolution kernel estimators for the joint density of the response and predictors.==== in the linear setup. The paper is organized as follows: The parametric modal regression model and the SIMEX estimation procedure will be introduced in Section ====; large sample properties will be discussed in Section ====; simulation studies and real data applications are conducted in Section ==== to evaluate the finite sample performance of the proposed SIMEX estimator. All the proofs of the main results are deferred to the ====.==== and ==== and ==== is denoted by ====, the density function of ==== is denoted by ==== given ==== is denoted by ====. For a generic parametric function ====, where ==== is the argument and ==== is the parameter, possibly multidimensional, the first two derivatives of ==== with respect to ==== are denoted by ==== and ====, and the first three derivatives of ==== with respect to ==== are denoted by ====, ====, and ====, respectively. For any vector or matrix ====, we use ==== to denote ====, where ==== is the transpose of ====. For the sake of simplicity, the multi-fold integral sign will be denoted by a single integral sign, and for a ====-dimensional vector ====, ====.====This section contains the proofs of all the main results from Section ====. For the sake of simplicity, we only present the proofs for the univariate ====. The extension to ====-dimensional covariates is straightforward, except for some notational complexities. Note that in the univariate case, ====.",SIMEX estimation in parametric modal regression with measurement error,https://www.sciencedirect.com/science/article/pii/S0167947320302498,19 December 2020,2020,Research Article,283.0
"Qiu Zhiping,Chen Jianwei,Zhang Jin-Ting","Research Center of Applied Statistics and Big Data, Huaqiao University, Xiamen, China,School of Statistics, Huaqiao University, Xiamen, China,Department of Statistics and Applied Probability, National University of Singapore, Singapore,Department of Mathematics and Statistics, San Diego State University, USA","Received 27 October 2019, Revised 1 August 2020, Accepted 13 December 2020, Available online 19 December 2020, Version of Record 26 December 2020.",https://doi.org/10.1016/j.csda.2020.107160,Cited by (11), consistencies are established. Simulation studies show that the proposed two tests generally have higher or not worse powers than some existing competitors. A real data application illustrates the proposed tests.,", ====, ==== and ==== for principal component analysis; ==== and ====, ====, and ==== for linear regression models, among others.====In recent decades, a number of significance tests are considered for univariate functional data. Examples include ====, ====, ====, ====, ====, ====, ====, and ==== among others. In particular, for analysis of variance (ANOVA) for univariate functional data, a number of works have been done. ====-test is usually time-consuming, and even when the pointwise ====-tests have been developed.  ==== proposed an ====-norm based global test whose test statistic is obtained via integrating the numerator of the pointwise ====-test statistic over time. ==== further showed that the null distribution of the ====-norm based test is asymptotically a ====-type mixture.  ==== proposed a test based on the sample means of the curves. ==== developed a global test whose test statistic is obtained via integrating the pointwise ====-test statistic over time. ==== studied an ====-norm based global testing procedure for multilevel structured functional data. On the other hand, ====-test statistic over time. This supremum test is further studied by ==== and is called the ==== test. Moreover, some other interesting testing procedures have also been developed for the ANOVA problem of functional data, e.g., ==== developed a pseudo likelihood ratio testing approach; ====, ==== proposed some new tests based on basis expansion technique; ====, ==== and ==== considered functional mixed-effects models; ====, ==== and ====.====All of the above tests are for univariate functional data. In recent years, more and more interests are attracted to multivariate functional data that have more than one function observed for a statistical unit. For example, ====, ==== and ==== considered principal components analysis; ====, ====, ==== investigated cluster analysis; ==== studied canonical analysis; ====, ====, ==== and ====, ====, ====, and ====In this article, we consider a two-sample problem for multivariate functional data. Our motivation example is the Canadian weather dataset which is described in ==== in detail. It is available in R package ====. In this dataset, there are ==== Canadian weather stations involved. For each weather station, the temperature and precipitation were recorded daily and were averaged over the years from ==== to ==== so that there are one temperature curve and one precipitation curve over a whole year (365 days) available for each of the ==== Canadian weather stations. There are ==== weather stations in Eastern Canada and ==== in Western Canada. Of interest is to check if the mean temperature curves and the mean precipitation curves of the eastern weather stations and the western weather stations are equal. This is a two-sample problem for multivariate functional data.====Mathematically, a two-sample problem for multivariate functional data can be described as follows: Let ==== denote a ====. Let ====, ====, denote the two multivariate functional samples. Assume that ====where ====, and ==== denote the mean vector functions and the common covariance matrix function, respectively. Moreover, the observed vector functions in the two multivariate functional samples are assumed to be independent. Of interest is to test the equality of the two mean vector functions ====The above two-sample problem can be tested using the tests described in ====. However, we shall propose two new global tests which generally have higher or not worse powers than those of ==== have. In addition, some theoretical analysis of our global tests is also presented.====The remainder of this article is organized as follows. In Section ====, the two global tests for the two-sample problem ==== for multivariate functional data are proposed. Their asymptotic random expressions under the null and some local alternative hypotheses are derived. Two methods for approximating the null distributions of the test statistics are described. Section ====. Section ==== presents some concluding remarks. Technical proofs of the main results are given in the ====.",Two-sample tests for multivariate functional data with applications,https://www.sciencedirect.com/science/article/pii/S0167947320302516,19 December 2020,2020,Research Article,284.0
Wu Jianhong,"Shanghai Normal University, Shanghai 200234, China","Received 14 June 2020, Revised 27 November 2020, Accepted 28 November 2020, Available online 17 December 2020, Version of Record 22 December 2020.",https://doi.org/10.1016/j.csda.2020.107153,Cited by (3),"This paper considers the estimation of high dimensional factor model with multiple threshold-type regime shifts in factor loadings. Firstly, the number of thresholds is determined by comparing the number of factors in the adjacent ====, the consistency of these estimators can be obtained. ==== results demonstrate that the proposed method has desirable performance in finite samples. A real data analysis is carried out for illustration.","In the last decade, there has been an increasing interest in high dimensional factor models with structural changes. Most studies have focused on the determination of break date in factor models with a single change. A brief review is given as follows. ==== developed a shrinkage estimation method to determine the number of pre-break and post-break factors as well as break date, and obtained the consistency of the estimated break fraction. ==== converted the problem of detecting the break date in factor loadings into estimating the change point in the second moment of pseudo-factors, and further showed the estimator of break date is stochastically bounded. ==== considered the least squares estimator of the break date and showed its consistency for both large and small breaks under certain conditions. When the time dimension of samples is sufficiently large, multiple breaks are often found in real applications. ==== extended ====’s ==== work to the case with multiple structural changes. They first estimated the break dates for any given number of breaks, and then determined the correct number of breaks by a sequence of tests. In contrast, ==== and ====.  ==== studied high dimensional factor models with a single threshold and showed that the resulting estimator of the threshold is consistent or even super-consistent under certain conditions. ==== considered threshold factor models for high-dimensional time series and proposed procedures for the estimation of loading spaces, the threshold value and the number of factors, where the factors were allowed to be non-stationary. Similarly to the cases with structural breaks driven by time points, multiple regime shifts driven by the threshold principle often happen when the time dimension is sufficiently large. As far as we know, in the existing literature there has been no theoretical study on high dimensional factor models with multiple thresholds.====In this paper, we consider the estimation of parameters in a high-dimensional factor model with multiple threshold-type regime shifts in factor loadings. In light of the method of ====, we first divide the sample into many subgroups based on the observed threshold variable and convert the problem of determining of the number of thresholds to that of determining the number of subgroups with a single threshold. Then, we adopt the method of ==== for more details. We can also adopt the linearity test of ====The rest of this paper is organized as follows. Section ==== introduces the model and states some assumptions needed for the theoretical results. Section ==== gives the estimators of thresholds, factors and loadings, as well as the corresponding asymptotic properties. Section ==== reports Monte Carlo simulation results. Section ==== gives a real data analysis. Section ==== gives a discussion on future directions. The proofs of theoretical results are relegated to ====. Throughout this paper, for a matrix ====, rank====, tr====, ====, respectively. Let ==== be the space spanned by column vectors of ====. The number of elements in the discrete set ==== is denoted by ====. The integer part of a real number ==== is denoted by ====. Let ==== be the indicator function, and let ==== and ==== denote the ====-dimensional vector with all elements being one, respectively.",Estimation of high dimensional factor model with multiple threshold-type regime shifts,https://www.sciencedirect.com/science/article/pii/S0167947320302449,17 December 2020,2020,Research Article,285.0
"Xiao Qian,Xu Hongquan","Department of Statistics, University of Georgia, Athens, GA 30602, USA,Department of Statistics, University of California, Los Angeles, CA 90095, USA","Received 8 March 2020, Revised 6 December 2020, Accepted 8 December 2020, Available online 14 December 2020, Version of Record 23 December 2020.",https://doi.org/10.1016/j.csda.2020.107155,Cited by (6),"In modern pharmaceutical studies, treatments may include several drugs added sequentially, and the drugs’ order-of-addition can have significant impacts on their efficacy. In practice, experiments enumerating all possible drug sequences are often not affordable, and appropriate statistical models which can accurately predict all cases using only a small number of experimental trials are required. A novel mapping-based universal Kriging (MUK) model and its simplified variant are proposed for analyzing such order-of-addition experiments with blocking. They can provide accurate predictions and have robust performances under various ====. The MUK model can also incorporate available domain knowledge to enhance its interpretation. The superiority of the proposed methods is illustrated via a real five-drug experiment on lymphoma and two simulation examples.","In pharmaceutical sciences, there are quite a few studies on the order-of-addition effects where the sequence of arranging drug components plays a significant role to affect the response. In an experiment analyzing protein function within an in-vitro transport system (====), the sequence of actions for two components was found to be significant on the response. In another drug combination experiment on oral cancer (====), the authors showed that the sequence of adding three drugs (bortezomib, camptothecin, and doxorubicin) played a vital role in ensuring the efficacy of treatments. We can also see the order-of-addition effects in many other scientific disciplines, such as chemical science (====), political science (====), bio-chemistry (====), food science (====), nutritional science (====), manufacturing (====) and behavioral science (====).====To show the characteristics of order-of-addition experiments, we illustrate an example in ==== where three anti-tumor drugs, denoted as A, B and C, were added into cell cultures either sequentially or simultaneously. There are in total ==== possible sequences for arranging the three drugs which were added one by one every six hours. The responses were the percentages of tumor inhibition measured 12 h after the addition of last drug. The researchers found that adding drugs following the sequence ==== would achieve the best response among all possible arrangements including the case of all drugs added together. Such an order-of-addition experiment is different from a crossover trial (====). The former only measures the endpoint response after all drugs are added, and the drug effects are believed to be dependent on their orders of addition. While, the latter measures the response after adding each drug (3 responses per run in this example), and each drug has a fixed effect which may carry over to the next period but does not depend on its order of addition.====In drug combination studies, some researchers would judiciously decide the drug sequences (without doing any experiment) based on the drug mechanisms and the corresponding pathway information of diseases, which may not be possible in many cases where the mechanisms are unknown (====). Thus, the order-of-addition experiments are often needed. There are ==== possible sequences for arranging ==== drugs, and an experiment enumerating all of them may not be affordable in practice (====). Appropriate statistical models are needed for accurately predicting the outcomes of all sequences using only a small number of runs. The current literature on modeling order-of-addition experiments is limited, and only two types of linear models are proposed: the pair-wise ordering (PWO) model (====, ====, ====) and the component-position (CP) model (====). Linear models have their advantages in modeling order-of-addition problems, such as clear interpretations. Yet, their prediction accuracy may not always be satisfactory; see Sections ====, ==== for examples.====, ====, ====, ====, ====). Specifically, in a drug combination experiment on lung cancer, the researchers showed that Kriging model can provide more accurate predictions than some popular linear and non-linear models (====, ====). Standard Kriging models can only take quantitative inputs, and some recent research extend their applications to both quantitative and qualitative inputs (====, ====, ====).====.====The remainder of this paper is organized as follows. In Section ====, we review current methods for modeling order-of-addition experiments. In Section ====, we first introduce the application of the UK model, then develop the MUK model, and finally illustrate the model estimations. In Section ==== we present a real data analysis on lymphoma treatment and in Section ==== we include two simulation studies to show the superiority of the proposed methods. Section ==== concludes and discusses some future research.====The following is the Supplementary material related to this article. ",A mapping-based universal Kriging model for order-of-addition experiments in drug combination studies,https://www.sciencedirect.com/science/article/pii/S0167947320302462,14 December 2020,2020,Research Article,286.0
"Zhou Ping,Yu Zhen,Ma Jingyi,Tian Maozai,Fan Ye","School of Applied Sciences, Beijing Information Science and Technology University, Beijing, China,School of Statistics, Renmin University of China, Beijing, China,School of Statistics and Mathematics, Central University of Finance and Economics, Beijing, China","Received 31 December 2019, Revised 22 November 2020, Accepted 4 December 2020, Available online 13 December 2020, Version of Record 24 December 2020.",https://doi.org/10.1016/j.csda.2020.107154,Cited by (1),", diverging ====. The assumption on the number of clients is more relaxed than that of the AEE estimator and the proposed method is thus more practical for real-world applications. Simulations and a case study demonstrate the satisfactory finite-sample performance of the proposed estimator.","In modern times, large-scale data sets have become increasingly common and they are often stored across multiple machines. Since communication cost between machines is considerably higher than the cost of reading data and conducting local statistical analysis (====; ==== and references therein). These algorithms perform parallel estimation in each client and transmit the local results to the server for an aggregated estimator (====; ====; ==== assume ==== is the number of clients and ==== is the sample size. However,==== point out that the assumptions on the divergence rate of ==== in the existing distributed estimators by one round of communication are too restrictive to be in accordance with the common practice whereby a huge number of clients are in use. ====. Based on an averaging estimator, the one-step estimator adds a single Newton–Raphson update with one additional round of communication. Provided that the averaging estimators are ====. In this paper, we will focus on GLMs, the MLE of which is a special case of M-estimators. It is worth pointing out that nearly all existing distributed estimators used to surrogate the MLE (not penalized MLE), such as ====, ==== and ==== under a weak assumption on ====.====As far as we know, limited research has been focused on the MLE for GLMs in the “large ====, diverging ====” framework, although related work does exist. The pioneering studies of the “large ====, diverging ====” framework is on M-estimators, which includes ====, ====, ==== and ====. It is worth mentioning that ==== increases with the sample size ==== under a relatively strong assumption, i.e., ====. ==== establishes the consistency of the GEE estimator when ====. In this paper, we first show the asymptotic efficiency of the MLE in GLMs under the assumption of ====. Based on the same assumption on the divergence rate of ====, the AEE estimator and the proposed one-step estimator are then shown to enjoy the same asymptotically efficiency. Through simulations and a case study, we demonstrate that the proposed one-step method outperforms existing distributed estimators with one round of communication, including the simple average method and the AEE method, when the number of clients is relatively large.====The main contributions of this paper are as follows.====First, we establish the asymptotic efficiency of the MLE in GLMs under a relaxed assumption ====. To accomplish this goal, we follow the theoretical techniques of the proof in ==== with adaptation to the GLMs and make an additional assumption, ====, as in ====.====Second, we also study the asymptotic properties of the AEE estimator under ====. Its asymptotic efficiency requires ==== to be ====, which is a restrictive constraint and thus limits its widespread application. This motivates us to develop a new distributed estimator with a more relaxed assumption on ====.====Third, we extend the one-step estimator in a fixed ==== setting (====) to the case of increasing dimensions and propose a new one-step distributed estimator. The proposed method reduces computation by updating the average estimator with a single round of Fisher-scoring iteration instead of Newton–Raphson iteration used in ====. We prove that the new one-step estimator shares the same asymptotic properties as the global MLE under ====. Compared with the AEE estimator, the new one-step estimator enjoys asymptotic efficiency under a more relaxed assumption on the number of clients, i.e., ====. Simulations verify this result by showing that the proposed one-step method has greater advantages over the simple average estimator and the AEE estimator as the data are distributed across more clients.====The rest of this article is organized as follows. Section ==== presents the basic notations used in this paper. In Section ====, we provide the distributed estimators in GLMs and establish their asymptotic efficiency in the “large ====, diverging ==== to show the finite sample performance of the proposed method. Conclusions are presented in Section ====. Technical details and more simulation results are relegated to the ====.",Communication-efficient distributed estimator for generalized linear models with a diverging number of covariates,https://www.sciencedirect.com/science/article/pii/S0167947320302450,13 December 2020,2020,Research Article,287.0
"Bansal Prateek,Krueger Rico,Graham Daniel J.","Transport Strategy Centre, Department of Civil and Environmental Engineering, Imperial College London, UK,Transport and Mobility Laboratory, Ecole Polytechnique Fédérale de Lausanne, Switzerland","Received 7 July 2020, Revised 16 October 2020, Accepted 1 December 2020, Available online 13 December 2020, Version of Record 30 December 2020.",https://doi.org/10.1016/j.csda.2020.107152,Cited by (6),None,"Spatial count data models are widely used in disciplines such as ecology, epidemiology, geography, regional science as well as transportation planning and engineering to explain and predict non-negative integer-valued outcome variables such as species and disease counts, patenting and innovation activities as well as crime and accident rates in geographically distinct entities such as local government areas, census tracts or traffic analysis zones (e.g. ====, ====, ====, ====, ====, ====).==== and ==== (====). While ==== captures the systematic correlation across neighbouring spatial units. In spatial count data models, unobserved spatial heterogeneity is operationalised through the inclusion of random link function parameters (====, ====, ====). Ignoring these spatial effects may result in biased parameter estimates and inaccurate inference due to higher type-I error (====, ====, ====). However, accounting for spatial heterogeneity and dependence also renders the estimation of spatial count data models computationally expensive.====, ====, ====). MCMC methods guarantee asymptotically exact inference, but succumb to three important limitations, namely computationally intensive estimation, high storage costs for the posterior draws, and difficulties in assessing convergence (====).====To address the bottlenecks of MCMC in the estimation of spatial econometric models, ====).====, ====, ====, ====, ====VB methods have been introduced for the estimation of non-spatial count data models and of linear spatial models. Yet, no VB method exists for the estimation of spatial count data models. Several studies present VB methods for variants of count data models, but none of the proposed approaches accounts for spatial dependencies between units (====, ====, ====, ====). ====, ==== and ==== devise VB methods for the estimation of models with spatial dependence; however, the proposed methods are limited to linear models with continuous outcome variables.====In this paper, we propose a VB method for the fast estimation of a spatial count data model, which accommodates both spatial heterogeneity and dependence. To be specific, we consider a negative binomial (NB) model with random link function parameters and a matrix exponential spatial specification of spatial dependence (====). To address the non-conjugacy of the NB model, we also adopt the Pólya-Gamma data augmentation (PGDA) technique in the proposed inference method. PDGA introduces auxiliary latent variables into the models. Conditional on these variables, the NB likelihood of the observed counts is translated into a heteroskedastic Gaussian likelihood, which admits closed-form conjugate posterior updates for nearly all model parameters. Only a few studies employ the PGDA technique in VB estimation (====, ====, ====, ====, ====).====We organise the remainder of the paper as follows. In the subsequent section, we formulate the considered spatial negative binomial model, and in Section ==== further compares VB and MCMC in estimating youth pedestrian injury counts in the census tracts of New York City. The findings of this empirical application corroborate the insights derived from the simulation study. Conclusions and avenues for future research are presented in Section ====.====To obtain the conditional posterior distribution of the dispersion parameter ==== in MCMC, we follow the strategy adopted by ====. We represent the negative-binomial-distributed count variable as follows: ====Thus, the conditional posterior update of ==== is: ====Since the posterior update of ==== is conditional on ====, we also update the conditional posterior of ==== using the following equation: ====
 ====
 ",Fast Bayesian estimation of spatial count data models,https://www.sciencedirect.com/science/article/pii/S0167947320302437,13 December 2020,2020,Research Article,288.0
"Huang Hengzhen,Chen Xueping","College of Mathematics and Statistics, Guangxi Normal University, Guilin 541004, China,Department of Statistics, Jiangsu University of Technology, Changzhou 213001, China","Received 10 June 2020, Revised 24 September 2020, Accepted 19 November 2020, Available online 28 November 2020, Version of Record 10 December 2020.",https://doi.org/10.1016/j.csda.2020.107150,Cited by (4),"Preclinical experiment on two-drug combination is a stepping stone to multi-drug combination studies. ==== have been proposed in the literature to test the presence of synergism between the combined drugs. However, a design that is efficient for synergy testing is not necessarily desirable for dose–response modeling and the latter is important for future development on drug interaction analysis. This work proposes an experimental design, called a compromise design to meet the dual requirements on synergy testing and dose–response modeling. The key idea of the design is to spread the design points uniformly on a pair of design regions where synergy testing and dose–response modeling are respectively carried out. Simulations and two illustrative examples are given to demonstrate the usefulness of the compromise design. In the illustrative examples, the good balance of the proposed design is visualized by 2-D projections of the design points. The simulation results indicate that the compromise design performs satisfactorily in terms of both testing power and model prediction accuracy.","Drug combination studies have been widely applied in disease treatment such as cancer and HIV due to their potential for improved effectiveness and inhibited drug resistance at lower, less toxic doses and the need to move new therapies rapidly into clinical trials (for examples, see, ====, ====, ====, ====, ==== and ====, ====, ====, ====, ==== and ==== and ==== and ==== studies, ==== proposed to use the ==== (====) for two drug combination experiments. ==== and a series of subsequent papers (====, ====, ====, ====, ====) have proved that the uniform design is a globally optimal design in the sense that it is a 3-D design for the experiment and it maximizes the minimum power of the lack-of-fit test to detect synergism between the combined drugs. The general model framework developed by ==== will be used in this paper. In the rest of this opening section, we will show that there are two design regions, where synergy testing and dose–response modeling are respectively carried out, should be jointly taken into account under the model framework. This will be pointed out in Section ====. As demonstrated by many researchers, dose–response modeling in preclinical experiment is important for future development on drug interaction analysis (for examples, see, ====, ====, ====, ====). However, a motivating example will be given in Section ==== to demonstrate that there is a conflict in meeting synergy testing and dose–response modeling from the viewpoint of experimental design.====The ====-parameter ==== function is defined as follows (====): ====where ==== is the dose of a drug (input), ==== is the effect (output), and ==== and ==== are the parameters. The above equation can be re-written as ====, a monotone function of ====, having a linear relationship with ==== via ====The above transformation is well-defined since the parameters ==== and ==== are respectively the lower and upper asymptotes of the dose–effect. It is easy to see that Eq. ==== is essentially the same as given in Eq. ====.",Compromise design for combination experiment of two drugs,https://www.sciencedirect.com/science/article/pii/S0167947320302413,28 November 2020,2020,Research Article,289.0
Fermanian Adeline,"Sorbonne Université, CNRS, Laboratoire de Probabilités, Statistique et Modélisation, 4 place Jussieu, 75005 Paris, France","Received 10 January 2020, Revised 15 October 2020, Accepted 14 November 2020, Available online 25 November 2020, Version of Record 8 December 2020.",https://doi.org/10.1016/j.csda.2020.107148,Cited by (18), to ,"), with more than a hundred new datasets. Sequential data are characterized by the fact that each sample consists of an ordered array of values. The order need not correspond to time, for example, text documents or DNA sequences have an intrinsic ordering, and are, therefore, considered as sequential. Besides, when time is involved, several values can be recorded simultaneously, giving rise to an ordered array of vectors, which is, in the field of time series, often referred to as multidimensional time series. To name only a few domains, market evolution is described by financial time series, and physiological variables (e.g., electrocardiograms, electroencephalograms) are recorded simultaneously in medicine, yielding multidimensional time series. Finally, smartphone and GPS sensors data, or character recognition problems, present both spatial and temporal aspects. These high-dimensional datasets open up new theoretical and practical challenges, as both algorithms and statistical methods need to be adapted to their sequential nature.====, ==== and ====), with the limitation that they become more complicated and harder to fit.==== and ==== provide introductions to the area. ==== give an account of recent advances. In particular, longitudinal functional data analysis is concerned with the analysis of repeated observations, where each observation is a function (====, ====). The data arising from this setting may be considered as a set of vector-valued functions with correlated coordinates, each function corresponding to one subject and each coordinate corresponding to one specific observation.====Although these various disciplines work with sequential data, their goals usually differ. Typically, ==== is concerned with predicting future values of one observed function, whereas (longitudinal) functional data analysis usually collects several functions and is then concerned with the prediction of another response variable. However, all these methods rely on strong assumptions on the regularity of the data and need to be adapted to each specific application. Therefore, modern datasets have highlighted their limitations: a lot of choices, in basis functions or model parameters, need to be handcrafted and are valid only on a small-time range. Moreover, these techniques struggle to model multidimensional series, in particular, to incorporate information about interactions between various dimensions.====), combined with a 1-nearest neighbor algorithm. ====. It has been rediscovered by ==== and ==== give a recent account, focuses on developing a new notion of paths to make sense of evolving irregular systems. Notably, ====, which is an unknown function of a random path ====, rough path theory suggests that the signature is a relevant tool to describe ====.====The signature has recently received the attention of the machine learning community and has achieved a series of successful applications. To cite some of them, ==== used the same approach for character recognition, and ==== investigated its use for the detection of bipolar disorders, and ==== for human action recognition. For introductions to the signature method in machine learning, the reader is referred to the work of ==== and to ====.====However, despite many promising empirical successes, a lot of questions remain open, both practical and theoretical. In particular, to compute signatures, it is necessary to embed discretely sampled data points into paths. While authors use different approaches, this embedding is only mentioned in some articles, and rarely discussed. Thus, the purpose of this paper is to take a step forward in understanding how signature features should be constructed for machine learning tasks, with a special focus on the embedding step. The article is organized as follows.====These empirical results are based on three recent datasets, in different fields of application. One is a univariate sound recording dataset, called Urban Sound (====), whereas the others are multivariate. One has been made available by ====, and consists of drawing trajectories, while the other is made up of 12 channels recorded from smartphone sensors (====). They are each of a different nature and present a variety of lengths, noise levels, and dimensions. In this way, generic and domain-agnostic results are obtained. The code is available at ====.",Embedding and learning with signatures,https://www.sciencedirect.com/science/article/pii/S0167947320302395,25 November 2020,2020,Research Article,290.0
"Brown Paul T.,Joshi Chaitanya,Joe Stephen,Rue Håvard","Department of Mathematics and Statistics, The University of Waikato, Hamilton 3240, New Zealand,CEMSE Division, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia","Received 6 December 2019, Revised 8 November 2020, Accepted 11 November 2020, Available online 21 November 2020, Version of Record 10 December 2020.",https://doi.org/10.1016/j.csda.2020.107147,Cited by (2),"Recently, it has been shown that the shape of a marginal distribution can be more accurately and efficiently captured using a set of low ==== (LDS) points compared to standard grid points. This suggests that the use of LDS could improve the ==== to marginal ","where ==== is the marginal function of the ====th element of the variable ====, and ==== is the variable ==== excluding the ====), these sampling methods may not necessarily be computationally efficient (====).====Initially, INLA used grid points to evaluate the joint posterior and to compute marginal hyperparameter posterior distributions (====, ==== for details of how to compute these points). The numerical integration-free method (NIFA), a method bypassing numerical integration altogether, has achieved further computational gains. However, the CCD and NIFA strategies do have the drawback of only allowing for unimodal approximations. For details on both of these strategies, see ====. Recently, a new method called Bayesian inference using sparse grid quadrature evaluation (BISQuE) (====) uses sparse grids (====) to explore the hyperparameter space and estimate marginal posteriors. BISQuE is similar to INLA as a Bayesian inference framework for hierarchical models, though not necessarily just for latent Gaussian models.====An alternative strategy for hyperparameter exploration and estimation, developed by the authors of ====). Mixture models, Markov switching models, as well as some spatial and spatio-temporal models are known to lead to multimodal likelihoods, see ====, ====, ==== and ====. INLA can be used to estimate multimodal posteriors, but this requires fine grids (====). It was shown in ==== that the LDS-StM was able to capture multimodal shapes using a fraction of the points than fine grids.====Although LDS-StM performed better than grids in a general setting, the method itself could not be implemented within INLA. Issues arose when choosing the degree of the polynomial. If a higher degree was needed (such as with the approximation of skewed, or multimodal distributions), the resulting matrices were sometimes ill-conditioned, and therefore some computation was inaccurate. They also encountered issues with Runge’s phenomenon (====), where oscillations would form in the tails of the approximations.====, ==== and ====. The methods presented may also be used in other grid-based methods such as BISQuE as well, though this will require further investigation.====This paper is organised as follows. In Section ====, we give some necessary background including on LDS points and quasi-Monte Carlo (QMC) integration, INLA, and posterior marginalisation using grid-based methods. Section ==== gives an overview of the LDS-StM algorithm. In Section ====, we also develop the two modifications LDS-QA and LDS-CX for implementation in INLA. We illustrate how our new algorithms work and compare their performance to existing INLA approaches in Section ====. Finally, we close with our concluding remarks in Section ====.====Orthogonal projections are used to project points in high dimensions onto a two-dimensional space. Note that in practice this simply amounts to a selection of matrix columns. However, for precision and to avoid ambiguity, we supply the technical details below.====Let ==== be an ==== dimensional function that is evaluated at ==== unique points ====, where each ====, is an ====-====
 ====. These points along with the function evaluations, that is ==== are ====-==== that can be expressed in matrix form as ====To estimate the ====th marginal ====, we orthogonally project ==== for ==== on the ====th marginal axis to obtain ====, ====where ==== is a projection matrix and ==== has size ====, and is a unit basis vector for ==== with the ====th entry in the first column and the ====th entry in the second column as one, all the remaining entries are zeros.====For example, if ==== and ==== then, ====and ",A novel method of marginalisation using low discrepancy sequences for integrated nested Laplace approximations,https://www.sciencedirect.com/science/article/pii/S0167947320302383,21 November 2020,2020,Research Article,291.0
"Fanjul-Hevia Arís,González-Manteiga Wenceslao,Pardo-Fernández Juan Carlos","Departamento de Estatística, Análise Matemática e Optimización, Universidade de Santiago de Compostela, Spain,Departamento de Estatística e Investigación Operativa and Centro de Investigacións Biomédicas (CINBIO), Universidade de Vigo, Spain","Received 11 October 2018, Revised 19 October 2020, Accepted 20 October 2020, Available online 19 November 2020, Version of Record 5 December 2020.",https://doi.org/10.1016/j.csda.2020.107146,Cited by (2),Comparing the accuracy and the behaviour of different diagnostic procedures is one of the main objectives of the Receiver Operating Characteristic (ROC) curve analysis. Along with the diagnostic variables it is usual to observe other ,"One of the main concerns of any medical study is the correct diagnosis of the patients. This problem is, in fact, a problem of classification: doctors want to determine whether their patients have a certain illness or not. In order to classify them into two populations (healthy or diseased) they often use a diagnostic marker, a variable that is usually considered to take higher values for the diseased individuals. However, in any diagnostic procedure some of the patients will be misclassified. The sensitivity (the ability of classifying a diseased patient as diseased) and the specificity (the ability of classifying a healthy individual as healthy) are two quantities that are employed to measure the two types of errors that can be made.====The Receiver Operating Characteristic (ROC) curve is a statistical tool that is used in this context to analyse the discriminatory capability of a diagnostic variable. It is constructed by representing the sensitivity against the complement of the specificity. Here we will consider two continuous random diagnostic variables, ==== and ====, representing the diagnostic variable in the diseased and the healthy populations, respectively. One usual way of representing this curve is by the formula ====where ==== is the cumulative distribution function of ==== and ====. The ROC curve is usually followed by global summaries of its discrimination capability, such us the well-known Area Under the Curve (AUC) or the partial Areas Under the Curve (pAUCs). For more information on the construction and estimation of ROC curves we refer to the books of ==== and ====.====In this paper we contribute by proposing a non-parametric test for comparing a conditional version of ==== to be discussed below. Our paper thus contributes to the statistical literature on the comparison of the accuracy of diagnostic procedures; see ====, ====).====, ====, ====, ====, ====) but, despite being one of the most powerful methods to compare ROC curves when one dominates the others at all points, it fails to reject the cases in which the curves cross each other. Other methods compare the difference among the ROC curves themselves (====, ====, ====, ====, ====, ====Along with the diagnosis variable it is usual to have some other covariates. It is important to take this information into account, because the diagnostic capability of a marker can change with the value of a covariate. By considering the conditional ROC curve it is possible to analyse how the performance of the maker is affected by this extra information. For a fixed value ====, where ==== is the support of the covariate ==== at hand, the conditional ROC curve is defined by ====where ==== is the cumulative distribution function of the diseased population conditioned to the value ====, and ==== is the conditional quantile function of the healthy population.====The possible effect of the covariates on the ROC curves (discussed, for example, in ====) should then be incorporated in the analysis when comparing different methods of diagnosis. ROC curves that seem equivalent could be different when conditioned to the value of a certain covariate, or the other way around. To our better knowledge, little has been done in the literature to address testing problem associated to conditional ROC curves. Some authors have dealt with the related issue of determining whether the considered covariate has a significant effect on the ROC curve (====, ====), but nothing has been done for the comparison of two or more conditional ROC curves.====In this paper we propose a non-parametric methodology to compare two or more ROC curves conditioned to the value of one continuous covariate in the context of independent populations. In Section ==== we present the new methodology. It includes a background about the estimation of the conditional ROC curve and a bootstrap algorithm for approximating the distribution of the proposed test statistic. The results from a simulation study are shown in Section ====. A problem with real data is displayed in Section ==== to illustrate the procedure, followed by some conclusions.====Here we present the assumptions and proofs needed for the theoretical results presented in this article. In the following lines, ==== and ==== are the cumulative distribution function and the density function of the covariate ====, and ==== is the density function of the error ====, with ==== and ====. In addition, the superscript ==== for ==== will represent the first, second or third derivative of a function.",A non-parametric test for comparing conditional ROC curves,https://www.sciencedirect.com/science/article/pii/S0167947320302371,19 November 2020,2020,Research Article,292.0
"Peng Heng,Xie Chuanlong,Zhao Jingxin","College of Finance and Statistics, Hunan University, Changsha, China,Hong Kong Baptist University, Hong Kong, China,Huawei Noah’s Ark Lab, Hong Kong, China,Wisers AI Lab, Hong Kong, China","Received 28 December 2019, Revised 15 September 2020, Accepted 24 October 2020, Available online 18 November 2020, Version of Record 8 December 2020.",https://doi.org/10.1016/j.csda.2020.107126,Cited by (0),", a proper trade-off between statistical efficiency and computation simplicity may improve the finite-sample performance.","Let ==== be an output variable, and let ====be input vectors with ====-length and ====-length respectively. The semi-varying coefficient model is in the form of ====where ==== is the index variable, ====, and ==== is a smooth function. A special case of the semi-varying coefficient model is the varying coefficient model (====), in which ====In the following, we review some related work about estimating ==== and ==== via local or global smoothing methods.====We start with the varying coefficient model in ====. If ====, ==== have the same degree of smoothness, ==== proposed an estimation method with smoothing splines. ==== and ==== developed another global smoothing method based on polynomial splines. By choosing multiple smoothing parameters, their method works well when ====, ==== have different degrees of smoothness. On the other hand, since the varying coefficient model is locally approximated by simple linear models, the kernel-based local smoothing estimators are also popular in the literature. ====. The bias and variance of this one-step estimator achieve ==== and ==== when all ==== possess the same degree of smoothness. However, ==== pointed out that if this assumption does not hold, the optimal rate (====, the bias of the two-step estimator is of ==== and the variance is of ====.====As to the semi-varying coefficient model in ==== will turn the problem into a varying coefficient model. Then the remains can be solved by the methods we have mentioned above. ==== suggested to consider ==== as functional e.g. ====, and then take the average with respect to ====. The bias of their estimator is of order ====. We notice that this estimator is developed from a local estimator, which implies that the global property of ==== in ==== is not fully utilized. Then in ====, a profile least-square estimator was put forward. This estimator also has a bias ==== and a variance ====. Besides, ==== have shown that unlike ====, ==== presented a semi-local least squares estimator. The constant vector ==== is estimated globally while the functional ones are estimated locally. ==== have shown that their estimator has bias of ==== and the variance is ====. Alternatively, the general series method can also be applied to semi-varying coefficient models, see ====.====Naturally, we are also interested in the test problem that whether a coefficient is really varying. The researchers have investigated many kinds of difference between the null and the alternative hypothesis to get the test statistics and the corresponding critical values. ====, ==== and ==== proposed the generalized likelihood ratio (GLR) tests and illustrated the idea with a varying coefficient model in detail. They have proved that the GLR tests are optimal and follow the Wilk’s phenomena.====However, a growing concern of the computation cost has caused a vast number of studies to develop fast algorithms. The estimators mentioned above need loads of computational work. What is worse, for model checking problem, one has to fit all ====, both under the null hypothesis and the alternative. If the bootstrap is also used to determine the rejection region, the computation burden will be even heavier. On the other hand, the room for improving the estimation efficiency is quite limited. The optimal rate of the two-step estimator is already ==== is bounded by the semiparametric information matrix. Thus, the excessive pursuit for the estimation efficiency may gain little but make the method complicated and time-consuming. Therefore, a proper trade-off between the efficiency and the computational burden should be taken into consideration to improve the performance of the statistical methods. Works about this topic seem scant. We make attempts to fill this void in this paper.====We derive a local average method to estimate the varying coefficients. The main idea of our method is to regard the function ==== as piecewise constant so that we can piece wisely estimate ==== via the least squares estimator. We call the proposed method as Local Average Estimate. In the following, we show that though the proposed method is simple and rough, it provides a good base for further inference. The local average estimator has three advantages. First, it sharply lightens the computation burden. The local linear or quadratic estimator estimates the value of ==== at a given point of ====. So one have to repeat the estimation procedure many times to depict the function ====. Our method locally transforms the original model into a simple linear model, and directly estimate the mean values of ====The proposed method introduces many parameters to model varying coefficient functions. Intuitively, it may over-fit the varying coefficients. Thus our strategy challenges the common suggestions about bias–variance trade-off. In this paper, we prove that though the variance of our method is large, its bias is small enough to build further inference procedures. In Section ====, we introduce the local average estimator under varying coefficient models and semi-varying coefficient models and investigate the asymptotic properties. Furthermore, based on local averaging, we propose three tests in Section ====. An important advantage of the proposed tests is that they can focus on a specific coefficient and avoid redundant computation caused by estimating nuisance coefficients.",Fast inference for semi-varying coefficient models via local averaging,https://www.sciencedirect.com/science/article/pii/S0167947320302176,18 November 2020,2020,Research Article,293.0
"Gijbels Irène,Karim Rezaul,Verhasselt Anneleen","Department of Mathematics and Leuven Statistics Research Center (LStat), KU Leuven, Belgium,Department of Statistics, Jahangirnagar University, Bangladesh,Data Science Institute, Interuniversity Institute for Biostatistics and statistical Bioinformatics, Universiteit Hasselt, Belgium","Received 10 February 2020, Revised 28 September 2020, Accepted 27 October 2020, Available online 18 November 2020, Version of Record 7 December 2020.",https://doi.org/10.1016/j.csda.2020.107129,Cited by (3)," quantile regression imposes often too restrictive assumptions. Nonparametric regression avoids making distributional assumptions, but might have the disadvantage of not exploiting distributional modelling elements that might be brought in. A semiparametric approach towards estimating conditional quantile curves is proposed. It is based on a recently studied large family of asymmetric densities of which the location parameter is a quantile (and not a mean). Passing to conditional densities and exploiting local likelihood techniques in a multiparameter functional setting then leads to a semiparametric estimation procedure. For the local ==== the asymptotic distributional properties are established, and it is discussed how to assess finite sample bias and variance. Due to the appealing semiparametric framework, one can discuss in detail the bandwidth selection issue, and provide several practical bandwidth selectors. The practical use of the semiparametric method is illustrated in the analysis of maximum winds speeds of hurricanes in the North Atlantic region, and of bone density data. A simulation study includes a comparison with nonparametric local linear quantile regression as well as an investigation of robustness against miss-specifying the ==== part.","Classical regression focuses on estimation of the conditional mean function ==== of a response ==== given a set of ====. A vast literature dealing with estimation of ====Conditional mean estimation focuses only on the average effect of the response ==== given ==== (with ====) of ==== given ==== is denoted and defined as ====, with ==== the cumulative distribution function of ==== given ====. See ==== and ==== for ====. The conditional quantile of ==== given ==== coincides with the minimizer of ==== with respect to ====, where ==== is the so-called check function. Henceforth ==== is such that ==== is minimal.====For estimating a conditional quantile ==== one can rely on parametric, semiparametric or nonparametric approaches. In linear quantile regression ====, with ==== the ====-dimensional column vector, and with ==== the column vector (of dimension ====, and estimated parameters are those for which this empirical quantity is minimized. See also Section ====. Linear or, more generally, parametric quantile regression can be insufficient, in case no appropriate form for the conditional quantiles can be put forward. Consider data on maximum wind speeds in hurricanes occurring in the North Atlantic region during the period 1971 and 2017. ====(b) (produced using the proposed semiparametric quantile estimation method) reveals however that also decreases are noticeable, even for several quantile curves, during that period. As discussed in Section ==== the semiparametric estimation method led to considerably smaller prediction errors than when using a nonparametric estimation method (see ====). This example simply illustrates the specific merits of the semiparametric approach that is presented in this paper.====A particular point of attention when estimating conditional quantile curves is that of the non-crossing property. By definition, for ====, the conditional quantile curves satisfy ====, for all ==== in the domain of the random vector ====).====In this paper we contribute with an appealing semiparametric method to estimate conditional quantile curves. As a starting point we rely on a very broad family of asymmetric densities, recently studied in ====. Within this large family of densities, with index-parameter ====
 (====), the location parameter (say ====) coincides with the ====th quantile of the distribution. It therefore is called the quantile-based family of asymmetric densities (QBA densities). This family provides a very advantageous framework, since many probabilistic properties, and a detailed study of estimators and their behaviour (with explicit expressions for asymptotic variance–covariance matrices) were established in ====. Moreover, the family is of a location-scale type. A density in the family is symmetric if and only if ====. Based on this family we consider a class of asymmetric conditional densities that involves an unknown location function ==== and an unknown scale function ====. For a given member of the family of conditional densities (constituting the parametric component), one can produce a localized version of the log-likelihood, locally modelling both the unknown location and scale function (the nonparametric components) via polynomials (of possibly different degree). This results into a local polynomial likelihood type of problem, but under nonstandard working conditions (i.e. non-differentiability), due to the quantile-based setting. Only in case ==== we are back to standard working conditions. The advantages of this particular semiparametric approach are:====In the present paper we restrict to a univariate covariate setting (i.e. ====) although extension to a multivariate setting is methodologically rather straightforward, as is briefly discussed in Section ====.====The paper is further organized as follows. After a very brief recall of parametric and nonparametric approaches to conditional quantile estimation in Section ====, we present our semiparametric local likelihood estimation type approach in Section ====. Section ====. Section ==== is devoted to the adaptation of the method to account for some additional unknown parameter (function). The finite-sample performance of the semiparametric procedure is investigated via a simulation study in Section ====. Real data applications in Section ==== illustrate the use of the proposed semiparametric estimation method. Proofs of all theoretical results are provided in the Supplementary Material. This material also contains new asymptotic results for nonparametric local polynomial quantile regression and optimal bandwidth choice for it. These results fill some gap in the literature. The Supplementary material further presents some additional results from the simulation study. The discussed semiparametric estimation method is implemented in the R package ==== (====).====The following is the Supplementary material related to this article. ",Semiparametric quantile regression using family of quantile-based asymmetric densities,https://www.sciencedirect.com/science/article/pii/S0167947320302206,18 November 2020,2020,Research Article,294.0
"Lee Keunbaik,Lee Chang-Hoon,Kwak Min-Sun,Jang Eun Jin","Department of Statistics, Sungkyunkwan University, Seoul 03063, South Korea,Division of Pulmonary and Critical Care Medicine, Department of Internal Medicine, Seoul National University Hospital, Seoul 03080, South Korea,Department of Internal Medicine, Healthcare System Gangnam Center, Healthcare Research Institute, Seoul National University Hospital, Yeoksam-Dong, Gangnam-gu, Seoul 06236, South Korea,Department of Information Statistics, Andong National University, 1375 Gyeongdongro, Andong 36729, South Korea","Received 4 November 2019, Revised 8 November 2020, Accepted 9 November 2020, Available online 18 November 2020, Version of Record 2 December 2020.",https://doi.org/10.1016/j.csda.2020.107144,Cited by (5), (ARMACD) to explain the correlations between responses at each ==== fatty liver disease.,"). NAFLD is diagnosed by the presence of fatty liver detected using ultrasonography, with none of the other possible etiological factors associated with chronic liver disease — significant alcohol consumption, positivity test results for hepatitis B surface antigen or antibodies against hepatitis C virus, and other known etiologies of chronic liver disease (====). The NAFLD data were collected from 11,892 subjects among 20,112 Korean adults following a routine health screening. Then subjects with NAFLD were matched with those without NAFLD using the propensity scores (1:1 ratio). Several variables including the subject’s pulmonary functions (forced expiratory volume in 1-second (FEV1) and forced vital capacity (FVC)) and body mass index (BMI) were evaluated. Since they were measured regularly at least three years follow-ups, the response variables represented longitudinal outcomes. In this paper, we analyze the data collected from the NAFLD study.====Most multivariate longitudinal data analysis included separate univariate longitudinal data. In a separate analysis, the ==== can be explained. However, the separate univariate longitudinal data analysis cannot explain other correlations between multivariate outcomes — the ==== and the ====’s (====) and random effects covariance matrix (====). However, these covariance matrices have limitations in explaining the various correlations required by multivariate longitudinal data analysis (====) to explain the various correlations.==== and ==== employed the MCD to decompose a covariance matrix into generalized autoregressive matrix (GARM) and innovation covariance matrix (ICM). The GARM explains the correlations within separate responses and the cross-correlations between responses over time, and the ICM explains the correlations between responses at each time point. To model the ICM, ==== employed matrix logarithmic covariance modeling (====), and ==== used another MCD. ==== and ==== apply unnatural ordering of the responses to use a second MCD. In ====, the ensuing parameters derived from the matrix logarithmic covariance are difficult to estimate or interpret. ==== also employed the MCD to model the covariance matrix and modeled the resulting ICM using the enhanced ====’s (====). Recently, ==== proposed autoregressive moving-average Cholesky decomposition (ARMACD) to model the covariance matrix in linear models for univariate longitudinal data. The ARMACD combines the MCD and moving-average Cholesky decomposition (MACD) to create a more flexible decomposition of the covariance matrix and provides for a variety of structures in the covariance matrix with a small number of parameters (====). ====’s (====) models to accommodate multivariate longitudinal data with many replications using the idea of the ARMACD. To our best knowledge, no studies have modeled the multivariate longitudinal data with many replications. Therefore, we aim to address this gap.====The rest of this paper is organized as follows. In Section ====, we propose linear models with covariance matrix for multivariate longitudinal data using the ARMACD and HD. Sections ====, ====’s (====) models using simulation studies. Section ==== analyzes the motivating data using our proposed models; finally, Section ==== concludes this paper.==== where ==== with ==== being the ==== zero matrix with ====th element ====, ==== ==== with ====1. The derivatives of ==== with respect to ====, ====, ==== and ==== have the following respective forms: ==== The expectations of these are clearly zero because ====.====In order to examine the other partial derivative of the log likelihood, we can rewrite the loglikelihood ==== as follows: ====Then we take a derivative with respect to ==== to obtain ====Now taking another derivative with respect to ====, we obtain ====Since ==== is independent of ====, the expectation of ==== is zero. Similarly, ==== is zero.====We also take a derivative of ==== with respect to ====. Then we obtain ====where ==== with ==== and ==== for ====. Now taking another derivative with respect to ====, we obtain ====where ====.====Since ==== consists of ====, ==== and ==== are independent. Then ====Derivative of ==== with respect to ==== is given by: ====Similar to ====, ====2. Let ==== be the contribution of the ====th observation to the log likelihood function given by ====, and let ==== and ==== denote the mean and variance of ==== when ====, respectively. Then ==== where ==== and ====.====It follows from the compactness of the parameter space (C2) and the boundedness of the covariates (C3) that there exists a real number ==== such that ==== for all ====. Thus, we have ====. Thus, by Kolmogorov’s Strong Law of Large Number, we obtain ====Notice that the above constant ==== is independent of ====. Based on the compactness of these parameter spaces and the following of a similar argument as that of ====, the convergence in ==== is uniform in ====. Moreover, it can be shown that ==== is equicontinuous in ====. Because the parameter space ==== is compact, and by the condition (C3), it can be shown that ==== converges to a finite limit such that ====Then by ====, the foregoing convergence is uniform in ==== and the limit ==== is continuous in ====. Hence, by ====, we obtain ====uniformly in ====. Because ==== is compact, ==== is uniformly continuous in ====.====Since ==== is in ====, we have ====for all ====. That is, ==== has a uniform maximum at ====. Because ==== is continuous and ==== is compact, ==== is bounded away from its maximum for any ==== bounded away from ====. That is, for ====, there exists ==== such that ====for ====.====Now we claim the strong consistency of ==== to ==== using a contradiction. Suppose that there is a set of positive probability where ==== does not converge to ====. For each ==== in the set, there exists a subsequence ==== and a limit point ==== in ==== such that ====. Because ==== produces a maximum for every ====, we have ====Then by uniform convergence and continuity of the limit, for this ==== we obtain that ====but this contradicts ====, and thus we conclude that ==== is strongly consistent for ====.====3. The proof of part 3 is essentially the same as that of Theorem 2 in ====. Note that Theorem 2 of ==== also relies on results in ====, and requires several regularity conditions for asymptotic normality. These are all satisfied from (C2)–(C4).",Analysis of multivariate longitudinal data using ARMA Cholesky and hypersphere decompositions,https://www.sciencedirect.com/science/article/pii/S0167947320302358,18 November 2020,2020,Research Article,295.0
Zhang Hong-Fan,"Wang Yanan Institute for Studies in Economics, Xiamen University, China","Received 22 May 2020, Revised 19 September 2020, Accepted 9 November 2020, Available online 18 November 2020, Version of Record 2 December 2020.",https://doi.org/10.1016/j.csda.2020.107145,Cited by (3), optimization. A specific algorithm to implement the estimation procedure concerning the choice of the instruments is provided. ==== of the estimators are also established. Simulated experiments show that the proposed estimation method performs well in finite samples. Application to the National Longitudinal Survey of Young Men data illustrates the proposed model and method in analyzing the returns to schooling.," and ====.====, ====, ==== and many others included. ====In this article, we are concerned with the following partially linear single-index model in which the exogenous regressors enter the nonparametric function but the endogenous regressors are left in the linear part, that is ====where ==== is the response, ==== is the error term, ==== is the exogenous regressors satisfying ==== almost surely, ==== is the endogenous regressors that may be correlated with ====, leading to ====. The parameters ==== and ==== are the two vectors, with ==== satisfying ====
 (==== denotes the Euclidean norm) and ==== (the first component of ====), and the link function ==== is an unknown function satisfying some certain smooth conditions. Further, there exists an instrumental variable ====
 ==== but is uncorrelated with ====. Summarizing these conditional moments yields ====where ==== is an unknown ==== matrix and the error term ==== may be correlated with ==== inducing the endogeneity of ====. The matrix ==== is assumed to be non-random with full row rank ====, or to be random but independent of the data ====. For the latter case, we also assume given the data, ==== has the full rank ====. Compared with the earlier mentioned semiparametric endogenous models, the nonparametric function ==== can still influence ==== in a linear fashion. Due to the endogeneity of ====, conventional estimation methods for the partially linear single-index model tend to produce inconsistent estimators. Thus, the main objective of this article is to develop new estimation method which can consistently estimate the parameter vectors ==== and ====, and the link function ==== under the ====, ==== setup.====Investigation of the partially linear single-index model with endogenous regressors in the linear part dates from the studies of the return to schooling (====, ====). Suppose ==== is the log of wages, ==== is the year of schooling (denoted by ====), ====), ====, ====, and ====). ==== considered the linear regression model ====where the coefficient ==== can be interpreted as the return to schooling. Since schooling is not randomly assigned so that ==== is endogenous, Card used proximity to a four year college as an instrumental variable to perform a two-stage least-squares (2SLS) estimation. An interesting finding is that the 2SLS estimate of ==== typically exceeds the ordinary least-squares (OLS) estimate. ==== attributed this result to the restrictive linear functional form and considered a partially linear model by putting all the exogenous variables into a nonparametric function, that is ====The authors showed the estimate of ==== is more precise than the 2SLS estimate by using their proposed estimation method. However, the estimation efficiency of ==== in ==== has to suffer from the “curse of dimensionality”. In this article, to achieve dimension reduction purpose, we will model this data by the partially linear single-index form in ==== aiming to give more precise estimates of the nonparametric function and the coefficient of the ====.====Let us return to the estimation topic. In the absence of endogeneity, model ==== has been studied extensively in the literature. For example, ==== employed the profile least-squares procedure incorporating some penalization approaches to simultaneously estimate parameters and select variables. ====) to get efficient, constructive and practicable estimators under the more general time series setting. However, all these methods mentioned above cannot be directly applicable if there are endogenous regressors.====We highlight the MAVE method of ==== in two aspects. Firstly, the MAVE method has a simple algorithm with explicit solutions available in each iteration. Secondly, the commonly used bandwidth in univariate function regression suffices to guarantee the root-==== consistency of the parameter estimator, thus undersmoothing the nonparametric function is not needed. In this article, the MAVE method is generalized to deal with the estimation of the partially linear single-index model with endogenous regressors. We propose using the generalized moments equation to obtain the estimates of the nonparametric components and the parameter vectors. In each iteration, different choices of the instrumental variables are considered. We call the resulting algorithm the Iterative GMM algorithm, and prove that it is also convergent to achieve the parameter estimators. As the Iterative GMM method is proceeded in the same manner of MAVE, it inherits all the advantages of MAVE.====The rest of this article is organized as follows. In Section ====, we provide a detailed description of the Iterative GMM estimation procedure, including the choices of the instrumental variables and its corresponding algorithm. Section ====. Section ==== applies our model and method to estimate the return to schooling, followed by a conclusion in Section ====. Technical conditions and proofs are given in the ====.====We make the following assumptions to drive the asymptotic results of the estimators. ====In the sequel, we further assume ==== and ==== with ==== for ease of derivations. The basic tools are the two lemmas, ====, ==== in the end, which are given in ====. These two lemmas concern the convergence rate of the kernel random sequences and double-sum kernel random sequences. The proofs of these lemmas are omitted. We state the main proofs of ==== and ==== by following the similar clues in ====. Let ====, ==== and ====. Let ==== and ====. Let ====, ====, ====, ==== and ====. By the condition on ====, there is ==== and ====. Let ====. Suppose ==== is a matrix, ==== or ==== means each element of ==== is ==== or ==== almost surely. We abbreviate ==== and ==== as ==== and ====, respectively. By (A.6), conditional on the data, ==== is fixed in each realization. Thus, in the followings, ==== is regarded as a constant matrix.",Iterative GMM for partially linear single-index models with partly endogenous regressors,https://www.sciencedirect.com/science/article/pii/S016794732030236X,18 November 2020,2020,Research Article,296.0
"Wang Lei,Zhao Puying,Shao Jun","School of Statistics and Data Science, Nankai University, Tianjin, PR China,Department of Statistics, Yunnan University, Kunming, PR China,Key Laboratory of Advanced Theory and Application in Statistics and Data Science-MOE, School of Statistics, East China Normal University, Shanghai, PR China,Department of Statistics, University of Wisconsin-Madison, Madison, USA","Received 12 December 2019, Revised 15 October 2020, Accepted 2 November 2020, Available online 12 November 2020, Version of Record 27 November 2020.",https://doi.org/10.1016/j.csda.2020.107142,Cited by (1),To estimate distribution functions and ==== of a response variable when the data having nonignorable nonresponse and the dimension of ,"Missing data exist in many statistical problems. Let ==== be the study variable or response that is subject to nonresponse, and let ====. Let ==== be the response status indicator for ====, where ==== if ==== is observed and ==== is called the propensity of missing data. When the data are missing at random (MAR) or ignorable (====), i.e., the propensity depends only on observed values ====, there exist many well-established methods in the literature, see ====, ====, ====, ====, ==== and the references therein. However, in practice, missing data are often nonignorable, i.e., the propensity depends on ==== regardless of whether ==== is observed or missing. In the presence of nonignorable nonresponse, estimation of population parameters in ==== is rather challenging, since some parameters are not identifiable (====, ====) without any further restrictions on the propensity, and estimators based on the MAR assumption may have large biases. Over the past few years, nonignorable missing data problems have received a lot of attention in the literature; see ====, ====, ====, ====, ==== and ====.====Estimation of the distribution functions and quantiles of the response ====. For nonignorable missing responses, ==== proposed several robust nonparametric/semiparametric estimators based on the exponential tiling model (====). Recently, ==== required external data to estimate the tilting parameter and only the exponential tiling model is considered by ==== and ====, and hence these existing methods have limited scope of application. The second issue is that the kernel regression estimation by ==== and ====The existence of nonignorable missing responses and high dimensional covariates and their impacts on the inference about distribution functions and quantiles of ==== motivate us to find a valid and efficient estimation approach. To the best of our knowledge, this problem has not previously been investigated. In this paper, different from ==== and ==== can be decomposed as two parts, ====, with a ==== unrelated to propensity conditioned on ====, that is, ====. Such a covariate ==== is used to create sufficient estimating equations for estimating the propensity and ensure that the propensity is identifiable, and is referred to as an instrument. Using an instrument to identify the population parameters can be found in ====, ====, ====, ====, ==== and so on. A discussion of how to find an instrument is given in Section ====When the dimension of covariates is not low, routinely applying nonparametric kernel regression is impeded by the curse of dimensionality and may result in large bias and efficiency loss (e.g., ====, ====, ====). To address such an issue in the nonignorable missing data problems, we apply the sufficient dimension reduction (SDR) technique (see, ====, ====) in the kernel estimation of propensity as well as the conditional distribution function. For a general response ====, if information contained by ==== about ==== is summarized by ====, i.e., ====
 , and ==== with dimension lower than the dimension of ====, then nonparametric regression of ==== on ==== is more efficient than that based on ====. Here, ====), the sliced average variance estimation (SAVE; ====), the minimum average variance estimation (MAVE; ====), the fusion-refinement procedure (====), the semiparametric method (====) and so on. See ==== for a review of the most recent advances in SDR methods. When the response is MAR, ==== and ==== studied the dimension-reduced estimation for distributions and quantiles. However, to the best of our knowledge, the dimension reduction strategy under nonignorable missing data has not yet been considered. In this paper, we use SDR to estimate such a matrix ==== based on the appropriate ==== and ====-consistent estimator ==== is used. It can be shown that the proposed estimators obtained by using ==== and ====The rest of the article is organized as follows: We consider three dimension-reduced semiparametric estimators for distribution functions and establish their asymptotic normality based on a general semiparametric propensity in Section ====. The corresponding dimension-reduced estimators for quantiles are proposed in Section ====. Results from some simulation studies in Section ==== show that the proposed dimension-reduced estimators work well. Section ==== analyzes a real-data set for illustration. All technical details are provided in ====.",Dimension-reduced semiparametric estimation of distribution functions and quantiles with nonignorable nonresponse,https://www.sciencedirect.com/science/article/pii/S0167947320302334,12 November 2020,2020,Research Article,297.0
"Wang Bingling,Zhou Qing","Department of Biostatistics, University of California, Los Angeles, USA,Department of Statistics, University of California, Los Angeles, USA","Received 14 July 2020, Revised 29 September 2020, Accepted 2 November 2020, Available online 12 November 2020, Version of Record 2 December 2020.",https://doi.org/10.1016/j.csda.2020.107141,Cited by (1),. Extensive numerical comparisons show that the proposed algorithms outperform existing DAG learning methods in identifying causal graphical structures. The practical application of the methods is illustrated by learning causal networks for combinatorial binding of transcription factors from ChIP-Seq data.,", ====, ====, ====, ====, ====).==== and the Fast Causal Inference (FCI) algorithm by ====, ====, search the space of graphs for an optimal structure using greedy, local, or some other search strategies.====, ==== causes ==== or vice versa. In terms of DAGs, we are considering either ==== or ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ==== showed that the true causal DAG is identifiable assuming non-Gaussian errors under linear SEMs and proposed a linear non-Gaussian acyclic model (LiNGAM) for causal structure learning. ====. ==== proposed a post-nonlinear (PNL) causal model under which one can distinguish the cause from effect and investigated conditions for identifiability of the model. See ==== and ==== for recent reviews of relevant works.====A key ingredient in the above methods is the use of general independence tests to determine the causal directions (====, ====, ====, ==== so that the corresponding SEM is ====, where the noise ==== is independent of the causal parent ====. If ==== such that ==== and that ==== is independent of ==== onto ==== is independent of ==== by design. Thus, advanced and complex test procedures such as the Hilbert–Schmidt Independence Criterion (HSIC) (====), a kernel-based independence test, is often used in this approach. To estimate a causal DAG on many variables, a sequence of such independence tests is usually performed by these methods to identify a causal ordering and the parent set of each variable.====In this paper, we restrict our attention to non-invertible causal relations between variables in a DAG, which has not been explored in the literature. In the bivariate case, we develop a novel method to identify the causal direction by test for non-invertibility of ====, ====), epidemiology (====, ====) and social sciences (====, ==== may cause a change of the binding of another TF ====. The causality of the binding activities among a set of TFs may be reviewed by learning a DAG from their binding data, in particular, ChIP-Seq data. There could be nonlinear relations in this problem, which reflects the complexity in combinatorial gene regulation. We will apply our method to ChIP-Seq data to demonstrate its use in scientific discovery.====The remainder of this paper is organized as follows. We start with introducing our bivariate non-invertible SEM and test of causal direction in Section ====. We then incorporate this method into structure learning of causal networks with both linear and nonlinear SEMs in Section ====. Section ==== evaluates the performance of the proposed algorithms under different simulation settings and compares with other competing DAG learning methods. Section ==== presents an application to ChIP-Seq data for the construction of a TF binding causal network. The paper concludes with discussions in Section ====. In the supplementary material, we provide some technical details of our algorithms and additional numerical results.====The following is the Supplementary material related to this article. ",Causal network learning with non-invertible functional relationships,https://www.sciencedirect.com/science/article/pii/S0167947320302322,12 November 2020,2020,Research Article,298.0
Wang Yihe,"Department of Statistics, University of Illinois at Urbana-Champaign, IL, 61820, USA","Received 2 April 2020, Revised 27 October 2020, Accepted 29 October 2020, Available online 9 November 2020, Version of Record 20 November 2020.",https://doi.org/10.1016/j.csda.2020.107130,Cited by (4)," from the data itself is proposed. Furthermore, the proposed procedure is free of tuning parameters and performs well in simulations and in a multiple stock price prediction problem.","We study linear regression for multiple outcomes, where the number of outcomes ====). To be precise, let ==== be an observed ==== be the ====th row of ====. Throughout the main text we assume that ====, but in ==== we present simulation results when ====. Let ==== be the corresponding ==== outcome matrix and ==== be the ====th outcome of the ====th observation. We assume that the observed data follow the linear model ====for ==== and ====, where ==== is a ==== and are independent across both observations ==== and outcomes ====, though we consider dependent outcomes in simulations in ====. Our goal is to predict, for a newly observed ====, the corresponding ==== outcome vector ====. This model assumes that the design matrix is the same for all outcomes ====; Section ==== discusses extending the approach to designs that change with ====.====) and multiple brain networks (====). It is also especially important in modern genomics. For example, has been used to train models that predict the expression levels of tens of thousands of genes using genotype data (====, ====.==== outcome separately. This is straightforward to implement but ignores potential relationships between the outcomes. Indeed, even when outcomes are independent, it has been shown that information can be borrowed across them to reduce overall estimation error (====, ====). Most existing methods accomplish this by assuming a common structure across the parameter vectors ==== of the outcomes ====; see Section ==== for more details.==== outcomes. Section ==== reviews relevant literature and Section ==== describes the proposed method in detail. Simulation results are provided in Section ==== and an application to a multivariate stock price prediction problem is given in Section ====. Conclusions and discussions are presented in Section ====. Our method is implemented in the R package ==== and available at ====.====In order to show this, let ==== be the optimal decision rule defined in ==== and ==== be any other separable rule. Then the compound risk ==== is ====This is because ====, ==== and ====. The last term is zero because ==== divided by 2 is ",A nonparametric empirical Bayes approach to large-scale multivariate regression,https://www.sciencedirect.com/science/article/pii/S0167947320302218,9 November 2020,2020,Research Article,299.0
"Eckardt Matthias,González Jonatan A.,Mateu Jorge","School of Business and Economics, Humboldt Universität zu Berlin, Berlin, Germany,Department of Mathematics, University Jaume I, Castellón, Spain","Received 16 February 2020, Revised 21 October 2020, Accepted 2 November 2020, Available online 9 November 2020, Version of Record 25 November 2020.",https://doi.org/10.1016/j.csda.2020.107139,Cited by (4)," with additional quantitative marks. The proposed ==== is defined through partial ==== characteristics; it is highly computationally efficient and reflects the conditional similarity amongst sets of spatio-temporal sub-processes of either points or marked points with identical discrete marks. Two applications, on crime and forestry data, are presented.","Spatio-temporal point patterns, where a finite set of pairs of ==== with ==== and ==== are the point location and the time of occurrence of the ====th event, respectively, have become ubiquitous in various scientific fields, such as infectious disease epidemiology (====), the study of tornado events (====), fire dynamics (====) or seismography (====, ====-function (====), Ripley’s ====, ====, ====, ====). Marked spatio-temporal point patterns, where additional qualitative (yielding so-called multitype or multivariate point processes) or quantitative information is available for each pair ====, have not been covered much in the literature. Thus there is an increasing need for efficient statistical techniques allowing for the investigation and analysis of such type of spatio-temporal point processes. For a general review on different spatio-temporal point process statistics and models commonly used at present, we refer the interested reader to ====.====This paper aims to contribute to the analysis of multivariate spatio-temporal point processes, where locations and times for a set of different types of points, such as a collection of distinct tree species, are under study. In particular, we consider multivariate marked spatio-temporal point processes where both qualitative and quantitative marks are available for each single pair ====.====At present, two different approaches can be identified in the literature focusing on quantitatively marked spatio-temporal point processes where a potentially time-varying real-valued mark is attached to each single point location. One strand of the literature, mainly applied in the field of spatio-temporal earthquake research, was covered by ====, ====, ==== and ==== amongst others. They use ==== (==== at point location ==== conditional on the history of the complete spatio-temporal process up to time ==== (see ====). Under any such specification, the magnitude of the earthquake is often included as a mark.====, ====, ====, ====, ====, ====, ====, and ==== discussed so-called ==== to model the evolution of a quantitatively marked spatial point pattern over equidistant steps in time such as the diameter at breast height (DBH) value for a set of trees recorded over consecutive times.====While the above specifications consider the analysis of quantitatively marked spatio-temporal point patterns, the investigation of cross-characteristics through marked versions of the spatio-temporal reduced second-order moment measure and Ripley’s ====-function has just recently been discussed by ====. Unlike the classical second-order summary characteristics such as the spatio-temporal ====-function describes the expected number of further space–time points of type ==== from an arbitrary space–time point of type ==== of the process, given that the points in question have space and time separation ==== and ==== of distinct type or between points of type ==== and any alternative type in the multivariate process including ====, respectively. Originating in the purely spatial case, these two types of point process characteristics for qualitatively marked processes investigate the pairwise distances between the point locations of two distinct component patterns or between the point locations of one component and those of any alternative patterns.====, ==== and ====, this paper introduces different partial point process characteristics in the frequency domain which express the interrelation between pairs of component processes that remains after the effect of all remaining processes has been removed. In addition, adopting the ideas of classical multitype point process characteristics, a new dot-type spectrum, the ====, is introduced which reflects the linear interrelation of one component and any alternative patterns included.====To the best of our knowledge, the treatment of a combination of discrete with quantitative marks in a context of spatio-temporal point processes is new. If we also consider partial characteristics, we go a step further from existing literature. Finally, the extension of a spatial dependence graph model to the spatio-temporal context is also new.====The remainder of the paper is structured as follows. Section ==== provides some background on the main characteristics of point processes in the spatio-temporal domain. Section ==== develops the main results of the spectral analysis for spatio-temporal point processes. Then, Section ==== presents the spatio-temporal dependence graph model. Applications to crime data and forestry are developed in Sections ====, ====. The paper ends with some conclusions.====The following is the Supplementary material related to this article. ",Graphical modelling and partial characteristics for multitype and multivariate-marked spatio-temporal point processes,https://www.sciencedirect.com/science/article/pii/S0167947320302309,9 November 2020,2020,Research Article,300.0
Liebscher Eckhard,"University of Applied Sciences Merseburg, Department of Engineering and Natural Sciences, D-06217 Merseburg, Germany","Received 19 February 2020, Revised 2 November 2020, Accepted 3 November 2020, Available online 9 November 2020, Version of Record 2 December 2020.",https://doi.org/10.1016/j.csda.2020.107140,Cited by (2),A new multivariate extension of Kendall’s dependence coefficient tailored for use in ,"Let ==== and ==== are the regressor (predictor) variables. In applications of regression analysis, the question arises how strong the dependence of the response variable ==== on the regressor vector ==== can be approximated by a strictly increasing function of ====, where the distribution of ==== is regarded as fixed. In the literature on non-linear regression analysis, multiple correlation has been addressed in two main directions to our knowledge. The first one is connected with the ACE-algorithm by ====. Unfortunately, in the case ====, no multiple correlation measure is available as output of the algorithm. On the other hand, in ==== (see ====, Chapter 5). Since the copula is invariant under strictly increasing transformations (see ====, Theorem 2.4.3), this advantage carries over to the dependence coefficients. As it is shown below, the Kendall regression coefficient equals one precisely in the case where ==== is monotonously increasing function of ==== having a strong influence on ==== (see Section ==== for an example).====Multivariate extensions of Spearman’s rho and Kendall’s tau are investigated in several papers. In contrast to our approach, however, they are defined for a single random vector. ==== introduced in his paper a general class of multivariate dependence measures related to Kendall’s tau and Blomqvist’s beta. ==== give a good survey of approaches until 2010. For dimension greater than 2, the multivariate Kendall’s tau ==== of ==== (similarly the multivariate Spearman’s rho) gives value one precisely in the case where each variable is a monotonously increasing function of any other variable (cf. ====In ==== an interesting property of the multivariate Kendall’s tau is proved. ==== introduced a multivariate trimmed Kendall’s tau. In ====. In ====For computations in R, the author created an R package depcoeff which is available on CRAN. The simulations presented in this paper were performed using the functions provided by this package.====The paper is organized as follows: In Section ====, we introduce the Kendall regression coefficient and give some of its properties. Section ==== is devoted to the estimation of the coefficient introduced in Section ====, we discuss two extensions of the Kendall regression coefficient. The first one measures the dependence as a function which is increasing in some variables and decreasing in the remaining variables. The second extension uses a splitting of the regressor domain. Then the dependence is measured separately on the split regions. The simulation studies of Sections ====, ====.",Kendall regression coefficient,https://www.sciencedirect.com/science/article/pii/S0167947320302310,9 November 2020,2020,Research Article,301.0
"Li Junlan,Wang Tao","Department of Bioinformatics and Biostatistics, Shanghai Jiao Tong University, China,SJTU-Yale Joint Center for Biostatistics and Data Science, Shanghai Jiao Tong University, China","Received 22 May 2020, Revised 17 September 2020, Accepted 28 October 2020, Available online 7 November 2020, Version of Record 20 November 2020.",https://doi.org/10.1016/j.csda.2020.107131,Cited by (3),"Categorical responses cause no conceptual complications for dimension reduction in regression, but the performance of some methods may suffer in this context and hence supervised dimension reduction in practice must recognize the nature of the response. Using a continuous latent variable to represent an unobserved response underlying the ==== is developed for carrying out ====. Simulated examples and an application to a dataset concerning the identification of handwritten digits are presented to compare the performance of the proposed method with that of existing methods.","). One approach, called sufficient variable selection, is driven by the goal of identifying a subset of predictors that exhibit the strongest effects and assumes implicitly that only a few coordinates in the predictor vector are relevant to the response (====). The other approach, akin to principal component analysis and known as sufficient dimension reduction (SDR; ====).====where ==== means equal in distribution. Here, ==== is the response variable, ==== is the predictor vector, and ==== with ====. The column space of ====, ====, is called a dimension-reduction subspace. Under mild conditions, the intersection of all dimension-reduction subspaces is also a dimension-reduction subspace, called the central subspace and denoted by ==== (====), which is the identifiable target of estimation in SDR. We use ==== to denote the dimension of ====.====There is an extensive literature on SDR (====). The first methods are sliced inverse regression (SIR; ====), sliced average variance estimation (SAVE; ====), and directional regression (DR; ====). SIR is based on the first conditional moment ====, and SAVE and DR use ==== and the second conditional moment ====. In these methods conditional moments are typically estimated by slicing the response. ==== proposed a slicing-based method called likelihood acquired directions (LAD), and showed that it is more accurate than all dimension-reduction methods based on the same population foundations as SIR and SAVE.====LAD postulates a model for the regression of ==== on ====. Model-based SDR is relatively new. When ==== is normally distributed, ==== and ==== introduced a class of principal fitted components (PFC) models for supervised dimension reduction and obtained minimal sufficient linear reductions. Assuming that the predictors are conditionally independent given the response, ==== extended PFC to continuous, categorical, or mixtures of continuous and categorical predictors. More recently, ==== extended model-based inverse regression to elliptically contoured predictors, and ==== developed SDR methods for regressions in which ==== assumes implicitly that the response variable is quantitative. Nevertheless, the aforementioned methods can be applied to categorical responses as well. ==== is multivariate normal, ====). As such, the implementation of SDR methods in practice must recognize the nature of the response.====In this paper we investigate SDR in binary response regression. ==== and ==== demonstrated the unsatisfactory performance of commonly-used SDR methods when the response is binary on a number of illustrative examples. To address this issue, ==== proved that the central subspace based on the binary response ==== instead of ====) to estimate ====. More recently, motivated by principal support vector machines (====) for dimension reduction in regression, ==== proposed principal weighted support vector machines (PWSVM) for SDR. PWSVM is similar in spirit to probability-enhanced SDR methods. However, rather than estimating ==== to represent an unobserved response underlying the binary response. To induce the dependence between ==== and ====, we specify a logit model for ==== and a PFC model for ====. We obtain the minimal sufficient linear reduction for ====The remainder of this paper is organized as follows. In Section ==== we review the PFC model. In Section ====, we evaluate the performance of our proposal on a simulation study. Section ==== is an application of the proposed method to the pen-based handwritten digit recognition dataset (====). Section ==== contains the discussion.====The following is the Supplementary material related to this article. ",Dimension reduction in binary response regression: A joint modeling approach,https://www.sciencedirect.com/science/article/pii/S016794732030222X,7 November 2020,2020,Research Article,302.0
"Liu Yang,Ruppert David","Department of Statistics and Data Science, Cornell University, United States of America,School of Operations Research and Information Engineering, Cornell University, United States of America","Received 28 May 2020, Revised 22 October 2020, Accepted 25 October 2020, Available online 4 November 2020, Version of Record 10 November 2020.",https://doi.org/10.1016/j.csda.2020.107128,Cited by (1),"A novel approach is proposed for density estimation on a network. Nonparametric density estimation on a network is formulated as a nonparametric ==== local regression estimate is used to model the change in slope. The special case of local piecewise linear regression is studied in detail and the leading bias and variance terms are derived using weighted least squares theory. The proposed approach will remove the bias near a vertex that has been noted for existing methods, which typically do not allow for discontinuity at vertices. For a fixed network, the proposed method scales sub-linearly with sample size and it can be extended to regression and varying ==== on a network. The working of the proposed model is demonstrated by simulation studies and applications to a dendrite network dataset.",". In order to carry out analyses of those events, researchers need a range of specific techniques.====One of the frequently demanded tasks is density estimation on a network, but few statistical methods had been developed to address this need until recently. A natural first attempt at analyzing such data is to take the kernel density estimate (KDE) on the one-dimensional real line ====and apply it directly to network data by defining ==== as the network distance, where ==== is any location on the network, and ==== are the observed data locations. ====, where ==== is the bandwidth. The network distance is defined as the shortest path distance between two points on a network. However, under this approach, estimate ==== does not conserve mass. This happens because that the induced kernel ====-neighborhood of a vertex, and so ==== is not a probability density. As a result, it will overestimate the true density. ==== summarized widely used methods for density estimation on a network. Many published papers, such as ====, mentioned by Okabe et al. computed a kernel density estimate on a network, but most have used ====.====, ====, ====) seem to produce reasonable results in applications. However their methods suffer from a lack of theoretical grounding, and their computational cost is high (see Section ====). Furthermore, ESCK does not allow for discontinuity at vertices, which is found in many applications, such as traffic network, and the example in ==== and Section ====. Although ESDK produces discontinuous density at vertices, the estimation in the small neighborhood of vertices always uses data from all edges, which can lead to large bias. This is demonstrated in Case I of the simulation study in Section ====.====More recently ==== proposed a diffusion density estimator (DE) on networks using the connection between kernel smoothing and diffusion (====). DE is based on numerical solution of the heat equation, and it is not only faster than ESDK and ESCK, but also provides a sound statistical rationale and helps establish theoretical properties. The estimate is asymptotically unbiased with rate ==== away from the boundary region, where ==== of KDE. Moreover, similar to ESCK, DE does not allow for discontinuity at vertices, which consequently, can lead to large bias.====To motivate our research and illustrate its contribution, we introduce the dendrite data collected by the Kosik Lab, UC Santa Barbara, and first analyzed by ==== and ====. In this example, the events are dendritic spines, which are of clinical importance. Cognitive disorders such as ADHD and autism may result from abnormalities in dendritic spines, especially the number of spines and their maturity (====). The events on the network are the locations of 566 spines observed on one branch of the dendritic tree of a rat neuron, as shown in ====. The density of the spines shows clear discontinuity at vertices A, C and D; see ====, ====, ==== and ====. We will need a density estimation method that allows for multiple levels of smoothness at vertices — discontinuous, continuous with discontinuous derivative, and continuous with continuous derivative. We show that our proposed method will remove the bias that has been noted for existing methods such as (====) and ==== due to their inflexibility at vertices. KDE, ESDK, ESCK and DE’s inflexibility arises because the smoothness of the density function at a vertex needs to be decided before choosing an estimator: if discontinuity is desired at a vertex, KDE is applied to each edge, otherwise ESDK, ESCK or DE should be applied. In contrast, we propose a data-driven approach and determine the smoothness of the density function at a vertex by statistical testing. In fact, estimation at vertices is a major contribution of this paper.",Density estimation on a network,https://www.sciencedirect.com/science/article/pii/S016794732030219X,4 November 2020,2020,Research Article,303.0
"Lee Hangsuck,Ha Hongjun,Lee Taewon","Department of Actuarial Science / Mathematics; Sungkyunkwan University, 25-2, Sungkyunkwan-Ro, Jongno-Gu, Seoul 03063, Republic of Korea,Department of Mathematics; Saint Joseph’s University, 5600 City Avenue, Philadelphia, PA 19131, USA,Division of Applied Mathematical Sciences; Korea University, Sejong Campus, 2511, Sejong-ro, Sejong City 30019, Republic of Korea","Received 12 November 2019, Revised 19 October 2020, Accepted 22 October 2020, Available online 2 November 2020, Version of Record 17 November 2020.",https://doi.org/10.1016/j.csda.2020.107125,Cited by (0), (mUDD) and ," in the associated single decrement (SD) tables where ====Once the assumption for multiple decrements is chosen, an important question arises: Is it possible to convert probabilities of decrement to absolute rates of decrement and vice versa? In practice, we do not observe absolute rates of decrement but collect empirical probabilities of decrement. It is especially important to convert probabilities of decrement to absolute rates of decrement. If some risk factors that result in failure are removed because of development of a new vaccine or medical improvements or a set of particular decrements causes failure in a specific situation, for instance, the corresponding absolute rates become the basis for constructing a new multiple-decrement table. It is well known how easy it is to transform probabilities of decrement into absolute rates of decrement under a simple uniform distribution of decrement in the MD context. It is also true that one can quickly obtain probabilities of decrement from given absolute rates of decrement on the same premises.====There are many contributions to one-way conversion from absolute rates of decrement or probabilities of decrement under a specific decrement assumption (e.g., see ====, ====, ====, ====). In ====Another contribution of this paper is that we derive implicit solutions for a general form of absolute rates of decrement under the SD assumption. We provide formulae as the main results in ====, which yield absolute rates from the given probabilities of decrement. It is noted that the absolute rates obtained through our formulae are not readily available due to the difficulty of calculating competing probabilities among decrements. In general, it is not practical to find a desired absolute rate of decrement in an analytical form except for simple cases (e.g., when there are only two decrements); this is discussed in ====). ==== displays the main subjects of this paper by contrasting the results of the study in ====.====The remainder of the paper is structured as follows: Section ==== introduces necessary notations used throughout this paper and provides fundamentals of the subject; Section ==== explains how to obtain absolute rates form given probabilities of decrement under the general SD assumption; Section ==== offers the conversion algorithm of probabilities to absolute rates and numerical implementations to validate of the algorithm under the sCDD assumption; and, finally, Section ==== concludes the paper. All proofs are given in the ====.",Decrement rates and a numerical method under competing risks,https://www.sciencedirect.com/science/article/pii/S0167947320302164,2 November 2020,2020,Research Article,304.0
"Xu Yang,Zhao Shishun,Hu Tao,Sun Jianguo","Center for Applied Statistical Research and College of Mathematics, Jilin University, Changchun, China,School of Mathematical Sciences, Capital Normal University, Beijing, China,Department of Statistics, University of Missouri, Columbia, MO, USA","Received 22 January 2020, Revised 10 October 2020, Accepted 10 October 2020, Available online 26 October 2020, Version of Record 9 November 2020.",https://doi.org/10.1016/j.csda.2020.107115,Cited by (4),"Variable selection for failure time data with a cured fraction has been discussed by many authors but most of existing methods apply only to right-censored failure time data. In this paper, we consider variable selection when one faces interval-censored failure time data arising from a general class of generalized odds rate mixture cure models, and we propose a penalized variable selection method by maximizing a derived penalized likelihood function. In the method, the sieve approach is employed to approximate the unknown function, and it is implemented using a novel penalized expectation–maximization (EM) algorithm. Also the ",", ==== proposed the smoothly clipped absolute deviation (SCAD) penalty and ==== developed the adaptive LASSO (ALASSO) penalty. The SCAD penalty is a nonconcave function on ====Interval-censored failure time data arise when the failure time of interest is known or observed only to belong to some intervals instead of being observed exactly. Such data commonly occur in many areas including clinical studies, epidemiology researches, and sociological surveys. It is apparent that interval-censored data include right-censored data as a special case. An example of interval-censored data arose from the 2003 Nigeria Demographic and Health Survey concerning the childhood mortality (====). In the study, the health status of women in the reproductive age and their children were collected. In particular, the death time of child was observed exactly if the death occurs within the first two months of birth and after that, the information on the mortality was collected through interviewing the mothers of the children. Thus, only interval-censored data are available for the death time in general (====).====To deal with the failure time data with a cured subgroup, one common type of method is the mixture model approach, assuming that the underlying population is a mixture of cured and uncured subpopulations. In the method, the cure rate and latency survival function of the uncured subjects are commonly modeled separately. Among other models, one commonly used model for the situation is the proportional hazards mixture cure (PHMC) model and several methods have been proposed for inference about it based on interval-censored data. For example, ==== discussed the fitting of the GORMC model to interval-censored data.====Several authors have discussed variable selection for interval-censored failure time data. For example, ==== considered the problem under the frame of the proportional hazards (PH) model with a piecewise constant baseline hazard function and developed an EM algorithm to determine the proposed estimators. ==== discussed the situation wherein the failure time of interest follows a general class of semiparametric transformation models.====A few mixture model-based methods are available for variable selection when one faces failure time data with a cured subgroup. For example, ==== and ==== presented some semiparametric methods for the case of right-censored data, and ====The remainder of the paper is organized as follows. In Section ====, after introducing some notation and the GORMC model along with some commonly used penalty functions, we will present the proposed penalized variable selection approach. Section ==== discusses the implementation of the proposed method and the development of a novel penalized EM algorithm. In Section ==== presents some results obtained from a simulation study conducted to assess the finite sample performance of the proposed approach, which indicate that it works well in practical situations. Section ==== applies the proposed methodology to the children’s mortality data discussed above, and some discussion and concluding remarks are given in Section ====.====In this appendix, we will give the expressions of the conditional expectations used in the E-step. For the conditional expectation of different random variables, such as ==== and ====, one may need to discuss that individuals are in different situations, when the subject ==== is left-censored, the conditional probability mass function of ==== for the truncation ==== can be written as ====where ==== and the ==== is the value of ==== evaluated at ====. This is denoted as ==== conditionally follows the zero-truncated negative binomial distribution (ZTNB), i.e. ==== ZTNB====. Similarly, when the subject ==== is interval-censored, we have ==== ZTNB====, where ====, is the conditional probability for the truncation ====. Thus, the conditional expectations of ==== and ==== can be written as ====Then, using the fact ==== and ====, we have ====Similarly, ==== is the value of ==== with ==== evaluated at ====.====Finally, the conditional expectations of ==== and ====, given the observed data and the current parameter updates ====, can be written as ==== where ==== and the ==== is the value of ==== evaluated at ====.",Variable selection for generalized odds rate mixture cure models with interval-censored failure time data,https://www.sciencedirect.com/science/article/pii/S0167947320302061,26 October 2020,2020,Research Article,305.0
"Goto Satoshi,Takagishi Mariko,Yadohisa Hiroshi","Big Data Strategy Office, SoftBank Corp., Japan,Graduate School of Engineering Science, Osaka University, Japan,Department of Culture and Information Science, Doshisha University, Japan","Received 15 December 2018, Revised 9 October 2019, Accepted 17 October 2020, Available online 22 October 2020, Version of Record 20 November 2020.",https://doi.org/10.1016/j.csda.2020.107123,Cited by (0)," for analyzing time-varying relational count data. The first model, the dynamic Poisson infinite ",", ====, ====, ====).====.====), and it becomes difficult to interpret clusters across time points since cluster structures are inconsistent over time. On the contrary, clustering using a time-varying structure (====, ====) can help detect changes in relationships due to specific events.====, ====, ====), which extends the hidden Markov model (HMM) based method (====, ====) to relationship data, is especially useful since it can automatically determine the number of clusters from the data. The dIRM method can handle relational data containing either a zero or a one (essentially whether a relationship exists). However, in practice, we often obtain relational count data. For example, if each element of a data matrix represents how many times two products (corresponding to the row and column of the matrix) are simultaneously purchased, this data can be considered to be relational count data between the products. Since count data contain more information than binary data, it is preferable to use a model that can handle relational count data when we have count information instead of only zeros and ones.====Based on the foregoing, in this paper, we propose two new clustering methods for relational count data under time changes. First, we extend the Poisson infinite relational model (PIRM) (====), a clustering method for relational count data that does not consider clustering relationships that change over time, to be able to handle time changes to a model. We call this first model the dynamic Poisson infinite relational model (dPIRM). Second, we propose a dPIRM that can handle data containing many zeros, which we call the dynamic zero-inflated Poisson infinite relational model (dZIPIRM). The latter extension is especially important for count data, as zero-inflated count data are often obtained in practice, especially when the interval between observation times is short (====, ====, ====).====The remainder of the paper is organized as follows. In Section ====, we begin by considering a clustering model of time-varying relational count data without zero-inflated characteristics. Next, we propose a clustering model for zero-inflated time-varying relational count data. In Section ====, we illustrate how our proposed models can handle numerically simulated data. In Section ====, we show how the two proposed clustering methods for relational data may be used with real data. Finally, Section ==== concludes.====In this Appendix, we show proofs for Eq. ==== and ====. First, Eq. ==== can be derived as follows.====Next, proof of ==== is derived as follows.",Clustering for time-varying relational count data,https://www.sciencedirect.com/science/article/pii/S0167947320302140,22 October 2020,2020,Research Article,306.0
"Gerber Florian,Nychka Douglas W.","Department of Applied Mathematics and Statistics, Colorado School of Mines, Golden, CO, USA","Received 29 May 2020, Revised 4 October 2020, Accepted 5 October 2020, Available online 22 October 2020, Version of Record 2 November 2020.",https://doi.org/10.1016/j.csda.2020.107113,Cited by (7), is fitted to a scientifically relevant canopy height dataset with 5 million observations. Using 512 processor cores in parallel brings the evaluation time of one covariance parameter configuration to 1.5 minutes.,"An important benefit of the rapid advances in computing, data storage, and remote sensing is the availability of large spatial and space–time datasets, which help to address substantial scientific questions. Such data are relevant in weather and climate applications but also contribute to a better understanding of processes on the Earth’s surface (====). A hallmark of many of these datasets is the large number of often irregularly spaced spatial observations, which poses statistical as well as computational challenges and motivates our study.====, ====) or a latent field connected to the data (====). Although there exists a mature methodology for such models and their application to large datasets, the ever-increasing amount of data remains challenging and motivates current research (====, ====). However, only a few methods can exploit the capabilities of current high-performance computing infrastructures, and this study is an advance in that direction.==== ==== ====Our parallel CV approach takes advantage of the well-known ==== in spatial prediction (",Parallel cross-validation: A scalable fitting method for Gaussian process models,https://www.sciencedirect.com/science/article/pii/S0167947320302048,22 October 2020,2020,Research Article,307.0
Ciuperca Gabriela,"Institut Camille Jordan, Université Claude Bernard Lyon 1, 69622 Villeurbanne, France","Received 7 December 2018, Revised 3 July 2020, Accepted 5 October 2020, Available online 14 October 2020, Version of Record 23 October 2020.",https://doi.org/10.1016/j.csda.2020.107112,Cited by (4),"In many application areas, the problem of the automatic variable selection in a linear model with asymmetric errors is encountered, when the number of ==== diverges with the sample size. For this high-dimensional model, the penalized ==== and compared with the adaptive LASSO quantile estimator. The proposed estimation method is also applied to real data in genetics.",", under assumption that the first moments of ====). Readers can find in ====, ====).====In application fields (genetics, chemistry, biology, industry, finance), with the development in recent years of storage and/or measurement tools, we are confronted to study the influence of a very large number of variables on a studied process. That is why, we consider in the present work the following linear model: ==== and ==== its true value (unknown). The size ==== of ==== can depend on ==== but the components ==== do not depend on ====, for any ====. The vector ==== contains the values of the ==== explanatory deterministic variables and ==== the values of response variable for observation ====. The values ==== are known for any ====. Throughout the paper, all vectors are column. If ==== is very large, in order to find the explanatory variables that significantly influence the response variable ====, ====For model ====, let the index set of the non-null true parameters, ====Since ==== is unknown then, the set ==== is also unknown. We assume, without reducing generality, that ==== and its complementary set is ====, with ====. Hence the first ==== explanatory variables have a significant influence on the response variable and the last ==== variables are irrelevant. Thus, the true parameter vector can be written as ====, with ==== a ====-vector with all components zero. The number ====.====For a vector ==== for its subvector containing the corresponding components of ====. For ====, we denote by ==== the ====-vector with the components ====, ====. We also use the notation ==== or ==== for the cardinality of ====.====In order to find the elements of ====, one of the most used techniques is the adaptive LASSO method, introduced for ==== fixed by ==== by penalizing the squares sum with a weighted ==== penalty. This type of parameter estimator is interesting if it satisfies the oracle properties, i.e. the two following properties occur:====In order to distinguish between different types of adaptive LASSO estimators, we will use the term “adaptive LASSO LS-estimator” to refer to the minimizer of the LS sum penalized with adaptive LASSO.====Let us give some papers from very rich literature that consider the adaptive LASSO LS-estimator when ==== depends on ====: ====, ==== and ====. If the moments of the errors do not exist or the distribution of ==== presents outliers, then the LS framework is not appropriate. One possibility is to consider the quantile model with the adaptive LASSO penalty. The recent literature is also very rich: ====, ====, ====, ==== and ====, ====, to give just a few examples. As stated before, the non-differentiability of the loss function for quantile method complicates the theoretical study and its computational implementation, which is a very important aspect in high-dimensionality. Let us mention another work of ==== which is also devoted to the high dimensional regression in absence of symmetry of the model errors and which proposes a penalized Huber loss but in which the sparsity of the robust approximate LASSO estimator is not studied. The robust approximate LASSO estimator is consistent with the same convergence rate as the optimal rate under the light tail situation.====In the present paper we consider the expectile loss function for a high-dimensional model. In order to introduce the expectile method, for a fixed ====, let us consider the function ==== of the form ====For the error and the design of model ==== we make the following basic assumptions.====The errors ==== satisfy the following assumption:====While, the design ==== satisfies the following assumption:==== and ==== its largest and smallest eigenvalues, respectively. Let us consider ==== the generic variable for the sequence ====. Assumption (A1) is commonly required for the expectile models, see ====, ====, ==== and ====, ====, ==== and ====). Other assumptions will be stated about design in the following two sections, depending on the size ==== which varies in turn with ====.====Quite in general, it is wise to use the expectile method when the moments of ==== exist but its distribution is asymmetric. For ====, we get the classical method of least squares.====For model ====, consider the expectile process ====and the one with LASSO adaptive penalty: ====The adaptive weights ==== will be defined later depending on the size of ==== with respect to ====. The tuning parameter ==== controls the overall model complexity. Hence, we should choose ==== and ==== such that ==== for non-null parameters and ==== for null coefficients. In order to automatically detect the null and non-zero components of ====, we proceed in a similar way as for the adaptive LASSO LS-estimation introduced by ====, and we consider the adaptive LASSO expectile estimator of ====: ====The components of ==== are ====. Similarly to ====, let us define the index set: ====with the non-zero components of the adaptive LASSO expectile estimator.====The estimator ==== will satisfy the ==== if:====For ==== fixed, the properties of the estimator ==== have been studied by ==== towards ==== is of order ==== and that ==== satisfies the oracle properties. The case where ==== is fixed has also been studied by ====, where consider a penalized linear expectile regression with SCAD penalty function and obtain a ====-consistent estimator with oracle properties. In the present paper we assume that ==== depends on ====, more precisely, ====, with the constant ====. The size ==== and the set ==== can also depend on ====.====The case when ==== depends on ==== was also considered in ==== by considering the SCAD penalty for the expectile process. They propose an algorithm that converges, with probability converging to one as ====, to the oracle estimator after several iterations. Always for ==== depending on ====, for ==== sub-Gaussian errors, ==== penalize the expectile process with LASSO or nonconvex penalties. They find the convergence rate of the penalized estimator, propose an algorithm for finding this estimator and implement the algorithm in the R language in package ====. The paper of ==== introduces several approaches depending on selection criteria and shrinkage methods to perform model selection in semiparametric expectile regression.====Let us give some general notations. For a vector ====, we denote its transpose by ====, by ====, ==== and ==== the ====, ====, ==== norms, respectively. The number ==== of the explanatory variables and ==== of the significant variables can depend on ====, but for convenience, we do not write the subscript ====. Throughout the paper, ====, which value may differ from one formula to another.====In order to study the properties of the adapted LASSO expectile estimator ====, we introduce the following functions, using the same notations as in ====: ==== The paper is organized as follows. In Section ====, with ====. We obtain the convergence rate of the ==== and the oracle properties. A similar study is realized in Section ====, when ====. In Section ====, a simulation study and an application to real data are presented. All the proofs are relegated in Section ====.",Variable selection in high-dimensional linear model with possibly asymmetric errors,https://www.sciencedirect.com/science/article/pii/S0167947320302036,14 October 2020,2020,Research Article,308.0
Desgagné Alain,"Département de mathématiques, Université du Québec à Montréal, Montréal (Québec), H3C 3P8, Canada","Received 16 September 2019, Revised 5 October 2020, Accepted 6 October 2020, Available online 14 October 2020, Version of Record 23 October 2020.",https://doi.org/10.1016/j.csda.2020.107114,Cited by (3),"Linear regression with normally distributed errors – including particular cases such as ANOVA, Student’s ","Linear regression assuming normally distributed errors – including particular cases such as ANOVA, Student’s ====-direction). These outliers can have a negative impact on the estimation of the regression and scale parameters and hence on the whole inference. In particular, the detection of outliers can be affected by the masking and swamping effects, which refer to unidentified outliers and falsely identified outliers respectively.==== distributions. ====-estimators (====) computed with the iteratively re-weighted least squares method, ====-estimators (====) and REWLS estimator (====) are three examples of WLS. Several robust methods using other approaches have also been proposed, e.g., ====-estimators (====), least median of squares (LMS, ====) and least trimmed squares (LTS, ====) estimators, to name the most popular. In their review, ==== concluded that the REWLS and ====-estimators perform the best by far in the estimation of regression coefficients, achieving high efficiency simultaneously in the absence and the presence of outliers.====. Second, the outlier region and tail behavior are determined automatically, based on the proportion of outliers given by the weights of the mixture, which means no tuning constant to choose, for example, the number of degrees of freedom for the Student’s-==== distribution. Third, the N–FLP is a super heavy-tailed distribution with logarithmic decay for maximum robustness.====Our second key contribution consists in adapting the well-known expectation–maximization (EM) algorithm for the estimation of the regression model with N–FLP errors. In particular, the N–FLP estimator of the regression coefficients is a WLS estimator, and a connection with ==== and REWLS estimators. The N–FLP estimators go in the same direction by simultaneously offering high efficiency in the absence and presence of outliers, with a significant gain in robustness compared to ==== and REWLS estimators. In particular, the N–FLP estimators reach the largest desired breakdown point of 50% (see Section ====).====The remainder of the paper is structured as follows. In Section ====, the N–FLP mixture is introduced. In Section ====, we describe the adapted EM algorithm that leads to the N–FLP estimators of the linear regression model. Outlier detection, connection with ====-test. In Section ==== concludes the paper.====The following is the Supplementary material related to this article. ====
 ","Efficient and robust estimation of regression and scale parameters, with outlier detection",https://www.sciencedirect.com/science/article/pii/S016794732030205X,14 October 2020,2020,Research Article,309.0
"Wang Qihua,Su Miaomiao,Wang Ruoyu","Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing 100190, China,University of Chinese Academy of Sciences, Beijing 100049, China","Received 8 April 2020, Revised 1 October 2020, Accepted 2 October 2020, Available online 13 October 2020, Version of Record 17 October 2020.",https://doi.org/10.1016/j.csda.2020.107111,Cited by (2),"Imputation and the inverse ==== of the proposed estimator is equal to the semiparametric efficiency bound established by Hahn (1998, Econometrica, pp 315–331) when both the selection probability function and the outcome regression function are the functions of their assumed models, respectively. The finite sample properties of the proposed estimator are evaluated by simulation studies and the proposed method is illustrated by a real data analysis.",", ====, ====, ==== and ====. The idea of imputation is to impute missing response values by fitted response values. See, e.g. linear regression imputation in ====, ====, ====; nonparametric imputation in ==== and ====, semiparametric regression imputation in ==== and fractional imputation for survey sampling in ==== and among others. However, parametric inverse probability weighting and imputation require a specification of the selection probability function and the outcome regression model respectively. And consistency fails when the selection probability function is misspecified for the former and the outcome regression for the latter. Therefore, the augmented inverse probability weighting (AIPW) is a more advanced approach, where both the selection probability and the outcome regression are modeled. The resulting estimator is consistent if either model is correctly specified, which achieves the so-called “double robustness” property. See, e.g. ====, ====, ====, ====, ====, ====, ====, ====, ====, ==== and ==== proposed a double balancing core estimator for response mean whose consistency can be attained under mild assumptions without the requirement that either model is correct. Therefore, as is mentioned by the authors, the double balancing core estimator has an “extended double robustness” property.====, ==== and ====. The multiple robust method of ====, ==== propose a multiple robust imputation estimator in survey sampling framework.====All the proposed multiple robust methods in literature require that the assumed models include a correctly specified model, which is still difficult in practice. To develop an estimator more robust than the multiple robust estimator, we propose a beyond multiple robust method by focusing on the estimation of the response mean based on the augmented inverse probability weighting technique. This method defines a ==== as long as either the true selection probability model or the true outcome regression model can be presented as a function of the corresponding assumed models. Clearly, it is a weaker requirement than that the assumed models contain a true model, and hence the proposed method is more robust than the multiple robust methods mentioned above.====The proposed beyond multiple robust estimator of the response mean is proved to be asymptotically normal when one of the true models is a function of its assumed multiple models, and attains the semiparametric efficiency bound established by ====The remaining of this paper is organized as follows. In Section ==== by simulation studies and an application on real data. A discussion is presented in the last section to end this paper. The technical details and additional simulations are relegated to the supplementary material.",A beyond multiple robust approach for missing response problem,https://www.sciencedirect.com/science/article/pii/S0167947320302024,13 October 2020,2020,Research Article,310.0
"Yang Yi,Guo Yuxuan,Chang Xiangyu","Center of Intelligent Decision-Making and Machine Learning, School of Management, Xi’an Jiaotong University, China,School of Statistics, Renmin University of China, China","Received 14 October 2019, Revised 17 September 2020, Accepted 24 September 2020, Available online 13 October 2020, Version of Record 9 November 2020.",https://doi.org/10.1016/j.csda.2020.107107,Cited by (3), classification functions for a ,", ====), medical diagnosis (====, ====) and face recognition (====, ====). In these practical applications, the costs of different types of misclassification errors could be vastly different (====). Cost-sensitive learning, unlike the regular cost-insensitive learning, takes the varying costs associated with misclassifying examples into considerations. It aims at minimizing the total misclassification cost instead of errors, which is of more practical significance.====In the past twenty years, cost-sensitive learning has attracted much attention from researchers. Studies in this field mainly fall into three categories. The first category weights the data space on the basis of translation theorem (====). This kind of approach modifies the distribution of the training set with regard to misclassification cost, so that the distribution of examples is made biased towards the high-cost classes, and a cost-insensitive classifier is then applied. The second class of techniques utilizes the Bayes risk theory to assign each example to the class which has the lowest expected cost (====, ====, ====).====, ====, ====, ====, ====).====To overcome the disadvantages of the existing multi-class cost-sensitive classifiers mentioned above, this paper proposes a new angle-based cost-sensitive multicategory classification framework. Using the simplex coding to construct a (====)-dimensional decision function vector for ====-class problems under the angle-based framework (====, ====, ====, ====), the proposed classification method treats all classes in a simultaneous fashion without the sum-to-zero constraint. Thus, the computational complexity can be highly reduced. To this end, we first extend the notion of Fisher-consistency defined in ==== to cost-sensitive multicategory classification problems using angle-based formulations. Then, we propose a family of angle-based loss functions that are justified to be Fisher-consistent for cost-sensitive multicategory learning. To demonstrate the usefulness and effectiveness of the proposed framework, two new cost-sensitive multicategory boosting algorithms are derived as concrete examples. We verify their performance by comparing them with previous multiclass boosting algorithms in both simulated and real-data experiments. The results show that the proposed methods yield competitive performance compared with other boosting algorithms in both cost-insensitive and cost-sensitive scenarios.====The rest of the paper is organized as follows: Section ==== presents a brief review of cost-sensitive learning and the angle-based classification framework. The reason why the existing angle-based multicategory classification framework could not be generalized directly to the cost-sensitive version is also discussed. In Section ====, we define the Fisher consistency of angle-based loss functions for cost-sensitive multicategory classification. A family of angle-based loss functions which are Fisher-consistent are then proposed. Section ==== describes two novel cost-sensitive multicategory boosting algorithms based on the proposed loss functions. In Section ====.====The Lemma 1 of ==== is presented as follows:",Angle-based cost-sensitive multicategory classification,https://www.sciencedirect.com/science/article/pii/S0167947320301985,13 October 2020,2020,Research Article,311.0
"Gu Lijie,Wang Suojin,Yang Lijian","Soochow College & School of Mathematical Sciences, Soochow University, Suzhou 215006, China,Department of Statistics, Texas A&M University, College Station, TX 77843, USA,Center for Statistical Science & Department of Industrial Engineering, Tsinghua University, Beijing 100084, China","Received 17 December 2019, Revised 14 September 2020, Accepted 22 September 2020, Available online 9 October 2020, Version of Record 23 October 2020.",https://doi.org/10.1016/j.csda.2020.107106,Cited by (1)," in order to demonstrate the usefulness of each of these methods. As an illustration, the proposed SCB is applied to the Old Faithful geyser data for testing the error distribution.","Consider the following nonparametric regression model with random design: ====in which ====, are independent identically distributed (i.i.d.) copies of  ====, and ====, are i.i.d. random errors independent of ====, satisfying ====, ==== and cumulative distribution function (cdf) ====.====Due to its flexibility, nonparametric regression model ==== proposed an estimator of the error distribution based on non-parametric regression residuals with weak convergence, leading to prediction intervals and goodness-of-fit tests. ====. For more applications and tests about the error distribution in a semi- or nonparametric regression model, see ====, ====, and ====.====Our goal is to construct a smooth simultaneous confidence band (SCB) for the error distribution ====-values. ====, ====, and ==== investigated the SCB for the unknown nonparametric regression function ==== using kernel and spline methods. For recent theoretical developments and applications of SCBs in various contexts, see, for instance, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ==== and ====.====In model ====, if the regression function ==== were known, one could easily compute the well-known empirical distribution function (EDF) of the errors, ====and the accompanying infeasible nonsmooth SCB for ==== could be constructed as ====where ==== is the ====th percentile of the Kolmogorov distribution function ====. Clearly, ==== is a discontinuous step function regardless of ==== being continuous or discrete. To remedy this deficiency of ====, ==== proposed a smooth monotone polynomial spline estimator for the cdf. ==== proposed a kernel distribution estimator (KDE) ==== as follows: ====in which ==== with bandwidth ====. Then, according to ====, the infeasible smooth SCB for ==== could be constructed as ====The SCBs in ====, ==== are termed “infeasible”, as one observes only ====, not ====, so that ====, ==== are both infeasible since the regression function ==== is unknown. A standard approach is to use a two-step procedure to construct an estimator of the error distribution ==== given below.====Step 1: Estimate the regression function ==== of ==== where ====
 ==== is a rescaled kernel function with bandwidth ====.====Step 2: Replace the unobservable errors ==== by the regression residuals ====to estimate ==== by a plug-in KDE ==== or EDF estimator ==== as follows: ====and ====In order to obtain the asymptotic distribution of the global estimation error to be used for constructing SCBs, we refer to the weak convergence of the residual-based empirical process. As a leading method in the existing literature for constructing SCBs for the error distribution in nonparametric regression, ==== proved that the empirical process ====We adopt the main result of ==== to obtain asymptotically equivalent i.i.d. representation for the process ====, from which a comprehensible proof can be derived to show that ==== converges in distribution to a Gaussian process; see ====, ==== in Section ====.====For the implementation of the SCBs, ====, ====.====The rest of the paper is organized as follows. In Section ====, we state the main theoretical results which establish the smooth SCB for the error distribution ==== based on KDE ====. Section ==== describes the actual steps to implement the SCB. Section ==== contains the simulation results and a real data analysis is illustrated in Section ====. Section ==== provides some concluding remarks. Some technical proofs are given in the ====.====Throughout this Appendix, denote ==== as ====, and ==== as ====, where ==== is a constant. We use ==== (or ====) to denote sequences of random variables of order ==== (or ====) in probability. We also define ==== as the supremum norm of a function ==== on domain ====.",Smooth simultaneous confidence band for the error distribution function in nonparametric regression,https://www.sciencedirect.com/science/article/pii/S0167947320301973,9 October 2020,2020,Research Article,312.0
"Huang Youjun,Pan Jianxin","Mathematical College, Sichuan University, Chengdu 610065, China,Department of Mathematics, The University of Manchester, Manchester M13 9PL, UK","Received 30 March 2020, Revised 30 September 2020, Accepted 30 September 2020, Available online 8 October 2020, Version of Record 15 October 2020.",https://doi.org/10.1016/j.csda.2020.107110,Cited by (4), on the mean and within-subject correlation coefficients.," considered a random effect model for longitudinal binary data. ==== developed an intercept random effect model with bridge distribution, and ====The latter was investigated by using generalized estimation equation (GEE) methods, see, e.g., ====, ====). In addition, ==== proposed a joint GEE approach to model the mean and covariance. However, their method only applies to longitudinal continuous data and is not appropriate for longitudinal binary data. ==== further extended the methods to general multivariate discrete and continuous responses. ==== considered an alternating logistic regression method for dealing with computational infeasibility when the cluster size is large.====. In Section ====, simulation studies are conducted to assess the performance of the proposed approach under various scenarios. For illustration, the approach is applied to the analysis of two real data examples in Section ====. Some concluding remarks are provided in Section ====.",Joint generalized estimating equations for longitudinal binary data,https://www.sciencedirect.com/science/article/pii/S0167947320302012,8 October 2020,2020,Research Article,313.0
"Barrientos Andrés F.,Canale Antonio","Department of Statistics, Florida State University, 214 Rogers Building (OSB), 117 N. Woodward Ave. Tallahassee, FL, 32306-4330, USA,Department of Statistical Sciences, University of Padova, via C. Battisti 241, 35121, Padova, Italy","Received 18 April 2020, Revised 11 August 2020, Accepted 20 September 2020, Available online 7 October 2020, Version of Record 23 October 2020.",https://doi.org/10.1016/j.csda.2020.107104,Cited by (2),"Regression models are widely used statistical procedures, and the validation of their assumptions plays a crucial role in the data analysis process. Unfortunately, validating assumptions usually depends on the availability of tests tailored to the specific model of interest. A novel ====."," formulations tailored to test specific aspects of the model. For example, the Shapiro–Wilk (====) and the  ==== procedures test for normality and the Reset test of ==== propose a test to check whether the lack-of-fit comes from the incorrect parametric or nonparametric modeling of the regression function. ==== propose a test for globally testing the four assumptions of Gaussian linear regression models (i.e., linearity, homoskedasticity, uncorrelatedness, and normality). Despite addressing different aspects jointly, ====’s test still lacks generality as it is tailored to a specific class of models.====In this article, we aim to provide a global approach for testing the goodness-of-fit of general regression models. To this end, we present a novel ==== procedure applicable to a broad class of regression models whose response variable is univariate and continuous. The proposed approach departs from the ideas motivating standard approaches and exploits a suitable transformation of the response variable and a Bayesian nonparametric predictor-dependent mixture model.====There are very few Bayesian nonparametric contributions that propose general goodness-of-fit tests and that are applicable in a wide set of situations (see ==== for a review). Furthermore, the majority of such works do not consider the regression framework, but rather focus on proposing tests for predictor-independent densities (====, ====, ====, ====, ====, ====, ====). To our knowledge, the only works focusing on regression models are those by ==== and ====, ====). ==== proposes a method to approximate a calibrated version of the Bayes factor between a parametric model and a Dirichlet process mixture model alternative. Unfortunately, both of these proposals are difficult to implement when the goal is to test the fit of several regression models, as they require the analyst to derive or approximate the marginal likelihood of each model.====). Failure to detect differences between the two models suggests the model of interest fits the data well. Frequentist goodness-of-fit tests that rely on analysis of residuals, commonly defined as the response minus an estimate of the corresponding conditional mean (====, ====), require a full characterization of the residuals’ distribution. Unfortunately, such characterization is available only in a few cases (e.g., when the response is normally distributed).====, ==== proposes a more general approach for defining the residuals of a regression model using Rosenblatt’s transformation (see ====). These residuals, referred to as universal residuals, take values in ==== and, under a correct model specification, are uniformly distributed. Universal residuals represent a powerful tool for defining Bayesian nonparametric goodness-of-fit tests. In fact, some Bayesian goodness-of-fit tests employ a simplified version of universal residuals (e.g., ====, ====). Consistent with these approaches, our proposal exploits the fact that, under correct model specifications, universal residuals are not only uniformly distributed but also independent from the predictors. We propose using a Bayesian nonparametric approach to model universal residuals conditionally on predictors and look for deviations from both the uniformity and independence assumptions jointly. We assess these deviations in terms of the Bayes factor. Specifically, we use a mixture model based on predictor-dependent stick-breaking mixtures (====, ====, ====, ====, ====, ====). This class of models satisfies appealing properties in terms of flexibility (====) and large sample behavior (====, ====).====The rest of the paper is organized as follows: In Section ==== we describe Rosenblatt’s transformation and its relation to universal residuals. Then, we introduce our Bayesian goodness-of-fit test and discuss its properties. Section ====. Section ==== summarizes our findings and provides some directions for future work. All the proofs of our results are reported in ====.",A Bayesian goodness-of-fit test for regression,https://www.sciencedirect.com/science/article/pii/S016794732030195X,7 October 2020,2020,Research Article,314.0
"Marra Giampiero,Farcomeni Alessio,Radice Rosalba","Department of Statistical Science, University College London, Gower Street, London WC1E 6BT, UK,Department of Economics and Finance, University of Rome “Tor Vergata”, Via Columbia 2, 00133 Roma, Italy,Business School, City, University of London, 106 Bunhill Row, EC1Y 8TZ London, UK","Received 26 February 2020, Revised 31 August 2020, Accepted 5 September 2020, Available online 6 October 2020, Version of Record 6 October 2020.",https://doi.org/10.1016/j.csda.2020.107092,Cited by (0)," package, with an implementation of our approach, is freely available on CRAN.","Survival data are encountered in many applications and since the pioneering work of ====). For more examples of interval-censored data, in various fields, we refer the reader to ==== and ====. The presence of interval-censored observations does not rule out other types of censoring. In fact, it is perfectly possible for some patients to have experienced the event of interest before the first screening or, alternatively, to reach the end of the trial without ever experiencing it, thus generating left- and right-censored observations, respectively. In many cases, furthermore, it might be additionally possible to precisely measure the time to event for some subjects, therefore having additionally uncensored observations. We refer to this situation as ==== censoring (====, ====).====, ====, ====). Cirrhotic patients, due to compromised liver functionality or as a side effect of treatment, are additionally oftentimes immunodepressed and hence at higher risk of infections. Our data consist of ==== cirrhotic patients who were admitted to Policlinico Umberto I hospital in Rome, Italy, between 2009 and 2017. Of these, none was infected at admission, none was taking antibiotics, and none was scheduled for (nor had) major surgery during the hospital stay. The endpoint is a composite one, where an event is defined as the occurrence of an infection or death before hospital discharge. Times were recorded from admission. The main scientific questions with the data at hand revolve around the possibility of an increased risk of infection or death due to the use of catheterism, paracentesis, and overcrowding of the ward. We would like to model the effect of these binary predictors after non-parametrically adjusting for the effect of MELD, a score summarising the progression of liver failure. Indeed, a clearly non-linear effect of MELD will be discovered, indicating that a simple polynomial effect of this predictor would lead to misleading inference. Clearly, these data provide uncensored time-to-event in case death (before infection) is observed, and right-censored data if no event occurs before hospital discharge. Furthermore, in case an infection is observed, the event time is only known to have occurred between the last and current assessments (usually within a time span of 12 to 48 hours), therefore having also interval-censored event times.====, ====), although an efficient implementation can be found in ====. There are works which proposed estimating flexible survival models under mixed censoring and in the following we mention the perhaps most relevant to this paper. Recent articles include ==== proposed a sieve maximum likelihood two-step estimation procedure based on polynomial splines for the accelerated hazards model. ==== introduced an EM algorithm to estimate proportional hazards (PH) models that estimate covariate effects parametrically, and use monotone splines to approximate the cumulative baseline hazard function. The literature on survival modelling is vast and some interesting developments and ==== implementations are discussed in ==== and ====, and many models incorporated in the ==== package. These, however, do not allow for either mixed censoring or flexible baseline and covariate effects via penalised regression splines.====Building on ====, ====, however, as opposed to our proposal, these authors impose monotonicity via a penalty term and, as they point out, their algorithm requires improvements when it comes to multidimensional smoothing parameter estimation. In order to facilitate the use of the developments in this article in industry and academia, as well as enhance reproducible research, our methods are available within the ==== package (====) for the ==== (====) software.====The rest of the paper is organised as follows: Model formulation and parameter estimation are discussed in Sections ====, ==== , with further details provided in Section ====. A simulation study is presented in Section ====, and the results obtained by applying the proposed modelling framework to real data are discussed in Section ====. Section ==== concludes the paper with some directions of future research.====This section contains the analytical expressions of the score and Hessian of the model’s log-likelihood. Recall that the structure of Eq. ==== implies the presence of four main components, that is ====Exploiting this fact, the gradient and Hessian are reported according to the type of censoring considered to ease their readability. To simplify the notation, we use ====, ==== and ====, where ==== is adopted whenever the equality holds both for ==== as for ====. To simplify the notation further, we also present the results for a single ====th observation.",Link-based survival additive models under mixed censoring to assess risks of hospital-acquired infections,https://www.sciencedirect.com/science/article/pii/S0167947320301833,6 October 2020,2020,Research Article,315.0
"Mestre Guillermo,Portela José,Rice Gregory,Muñoz San Roque Antonio,Alonso Estrella","Universidad Pontificia Comillas, Escuela Técnica Superior de Ingeniería ICAI, Instituto de Investigación Tecnológica, Madrid, Spain,Universidad Pontificia Comillas, Facultad de Ciencias Económicas y Empresariales ICADE, Madrid, Spain,Department of Statistics and Actuarial Science, University of Waterloo, Canada,Universidad Pontificia Comillas, Escuela Técnica Superior de Ingeniería ICAI, Departamento de Matemática Aplicada, Madrid, Spain","Received 17 February 2020, Revised 25 September 2020, Accepted 25 September 2020, Available online 5 October 2020, Version of Record 16 October 2020.",https://doi.org/10.1016/j.csda.2020.107108,Cited by (10), norm of the lagged ,", ==== and ==== provide excellent introductions to the key topics of functional data and its applications.====If these functional data are collected sequentially over time, it is natural to expect a time dependence between functional observations. This motivates the consideration of functional time series (FTS); see e.g. ==== and ====To present the main ideas, let ==== denote an observed stretch of length ==== of a functional time series. Here we assume that each observation ==== whose sample paths are in ====. We could consider instead FTS taking values in more general function spaces, but proceed with ==== in this presentation given the data applications we present below.====where ==== denotes a sequence of independent and identically distributed random functional processes with zero mean, and ==== is an operator of the ==== past values of the time series and the ====, ====, ====), the functional ARHX model (====, ====), the functional moving average model (====), the functional ARMA model (====), the SARMAHX model (====, ====) and the functional ARCH model (====) all follow the general equation ====.====For example, ====, ==== illustrate these tools for a FTS composed of daily electricity price profiles from the Spanish electricity market in 2014 (====). In particular, ==== shows the rainbow plot for the price profiles time series. Increases or changes in trend of the daily price profiles are clearly visible in the summer months, as well as in January (red curves). It is however difficult to decipher from this plot the nature of the serial dependence between the curves. ==== for a formal definition. It can be seen by analyzing these carefully that the electricity prices for hours 12 to 18 are correlated with past curves, and that hours 19 to 22 are less influenced by the former price curves. These graphical tools provide some insights on the trend and outliers of the FTS, but in terms of describing the nature of serial dependence between the curves are lacking in many respects, especially in that they are difficult to interpret at a glance.====While these kinds of identification methods have been well addressed in the literature of scalar time series, the same cannot be said for FTS. An analog of the autocorrelation function for FTS was proposed in ==== for the purpose of quantifying conditional heteroscedasticity in functional data, but its use in FTS model selection has not been explored. Further, to our knowledge, the notion of partial autocorrelation with functional data has not been explored to date, nor how such summary information could be used in FTS model building.====This paper proposes methods based on the ==== and ====. These models generalize the aforementioned functional linear models, allowing the inclusion of both autoregressive and moving average effects as well as incorporating the seasonal behavior present in the data. As such, identifying the serial correlation structure of the data is of utmost importance to fitting the SARMAHX model.====The paper is organized as follows: Section ==== presents the main contributions and the practical implementation of the proposed identification procedure. Section ==== contains the results of a Monte-Carlo simulation study of the proposed methodology. Demonstrations and applications of these model identification procedures are presented in ====, where two real-world datasets are explored. Section ==== gives some concluding remarks, summarizes the results of the methodology developed in the previous sections, and points to some directions for future research. ==== contains details of the main technical results.",Functional time series model identification and diagnosis by means of auto- and partial autocorrelation analysis,https://www.sciencedirect.com/science/article/pii/S0167947320301997,5 October 2020,2020,Research Article,316.0
"Pun Chi Seng,Hadimaja Matthew Zakharia","School of Physical and Mathematical Sciences, Nanyang Technological University, Singapore","Received 5 December 2019, Revised 18 September 2020, Accepted 21 September 2020, Available online 30 September 2020, Version of Record 6 October 2020.",https://doi.org/10.1016/j.csda.2020.107105,Cited by (7),A self-calibrated direct ==== based on ,", plays an important role. From economics and finance to biology and health sciences, the estimated precision matrix has found its place in many applications and literature, be it the direct estimation of ==== (==== is part of the solution (==== is less than the number of observations ==== is often much larger than ====Under the high-dimensional setting, the sample covariance matrix is singular and consequently, there is no pivotal estimate for ====. To address the singularity of a large covariance matrix estimate, various high-dimensional statistical techniques have been developed under different assumptions, such as ==== with banding, ==== and ==== with thresholding, ==== with tapering, etc. Although one can take the inverse of the aforementioned estimators to obtain an estimate of ====, it has been shown to be superior to estimate ==== directly, especially when the desired statistical solution relies on only ====. In the literature, the estimators for ==== are mostly based on penalized likelihood maximization and its variants. For example, ====, ====, ==== and ==== propose a Lasso-type estimator, called Glasso, based on ====-regularized log-determinant divergence; see ==== for the details of Lasso. To avoid the eigen-decomposition of the determinant term, ==== propose to use a D-trace (quadratic) loss and prove its consistency. ==== and ====) for solving the Glasso and ====-regularized D-trace loss for precision matrix estimation, respectively. Another cluster of prevailing approaches estimates ==== column by column, such as ==== with a linear programming approach, ==== with their CLIME, ==== with their scaled Lasso, and ====, where ==== is the inverse of the common covariance matrix and ==== is the difference of the expected mean vectors between two classes. ==== show that when ==== propose a relaxed version of this rule, where the naïve independence rule is applied to the variables chosen by thresholding. ==== apply the thresholding technique to estimate a sparse precision matrix and constructs a sparse LDA. However, these separate estimations in essence are not better than a direct estimation approach with linear programming (LPD) as claimed in ====. The authors take advantage of an important observation that the Fisher rule depends on ==== only through ====. A direct estimation of ==== and is further developed in ==== and ====. Recently, the LPD is extended to an adaptive version of LPD (AdaLDA) in ==== and a dynamic LPD in ====.====With the similarity between CLIME and LPD, we can unify these two frameworks. Thus, this paper is focused on an ====-estimator of a generic form ==== in high dimensions. A highly related study is ==== that establish a unified framework for the norm-regularized ====-estimators. The interests of estimating parameters of the form ==== are beyond the aforementioned applications and include portfolio selection; see ====, ====, ====, ==== and ====.==== present a survey of tuning parameter selection. There are two main issues of tuning parameter: (1) inappropriate parameterization as documented in ====; (2) the theoretically optimal tuning parameter depends on the unknown parameters. Due to the second issue and the desire to perform well out-of-sample, cross-validation (CV) becomes a common approach to determine the optimal value of the tuning parameter. However, CV is computationally costly and tends to overfit; see ==== and ====, both of which are computationally efficient. Towards the goal of tuning-free, square-root Lasso in ====, scaled Lasso in ====, and TREX in ==== adapt to factors of the theoretically optimal tuning parameter, ====, where ==== is standard deviation of the noise, ==== is the noise vector, ==== is ==== norm. Square-root Lasso and scaled Lasso re-scale the Lasso with respect to ====. TREX, which adapts to ====, is completely tuning-free in its formulation, but the modified loss function is no longer convex. Both square-root Lasso and the scaled Lasso have a constant factor for effective tuning parameter, while TREX fixes its corresponding factor ==== as ==== to achieve tuning-free. However, it is mentioned in a recent work (====) by the authors of TREX that a data-dependent choice of ==== may improve the accuracy. Moreover, both square-root Lasso and TREX are not easy to be extended to the direct estimation of precision matrix and LDA.====In this paper, we propose a method called DECODE (Descent-based Calibrated Optimal Direct Estimation) that is an ====. It is computationally efficient as it only needs a few iterations of ==== directly and thus its applicability can be extended to linear discriminant analysis and many other statistical problems. In this paper, we will present DECODE for precision matrix and linear discriminant analysis.====The contribution of this paper is threefold. First, we introduce the DECODE framework for the applications in precision matrix estimation and linear discriminant analysis in high dimensions. It inherits the merits of ACLIME in ==== and AdaLDA in ==== and extends the scaled Lasso in ==== to a regularized ====-estimator. Second, we establish statistical error bounds for both applications, where the algorithm of DECDOE will only involve a scale-free parameter (====) instead of a data-sensitive tuning parameter. The bounds are of the same orders as in ==== and ====. Third, we conduct extensive simulation and empirical studies to compare DECODE with CLIME, Glasso, and EQUAL (for fast implementation of ====) for precision matrix estimation and graphical model selection and with LPD, NLDA, and GLDA for classification. The DECODE performs well in not only simulation studies but also in two well-known cancer classification problems.====The remainder of this paper is organized as follows. Section ==== constructs the DECODE as an estimator of a general vector of the form ====. We also present some theoretical properties of DECODE. The subsequent two sections show two applications of DECODE. Section ==== presents some numerical results from our simulation and real-data studies. Here, we see the performance of DECODE in both applications and explore the behavior of our estimator. Finally, we conclude the paper and discuss future possible works in Section ====. All proofs are contained in ====.====To end this section, we define some notations and definitions that are used in the following sections. For a ====-dimensional vector ====, we define the ==== norm by ====, where ==== is the indicator function of event ====; the ==== norm by ====; the ==== norm by ====; the weighted ==== norm by ==== for a ====; the ==== norm by ====. For a matrix ====, we define the elementwise ==== norm ====, the matrix ==== norm ====; the elementwise ==== norm by ====, the spectral norm ====. ==== is a ==== as its ====th column. For two sequences of real numbers ==== and ====, define ==== for ==== if there exists a constant ==== such that ====; define ==== if ====.",A self-calibrated direct approach to precision matrix estimation and linear discriminant analysis in high dimensions,https://www.sciencedirect.com/science/article/pii/S0167947320301961,30 September 2020,2020,Research Article,317.0
"Wu Runxiong,Chen Xin","Department of Statistics & Data Science, Southern University of Science and Technology, Shenzhen 518055, China","Received 24 March 2020, Revised 7 September 2020, Accepted 8 September 2020, Available online 28 September 2020, Version of Record 15 October 2020.",https://doi.org/10.1016/j.csda.2020.107089,Cited by (2), codes implementing our methods and scripts for regenerating the numerical results are available at ====.," be a univariate response and ==== be a ==== predictor vector, SDR aims to find a ==== matrix ==== such that ====where ====
 denotes the statistical independence. The column space of ==== satisfying ==== is called a dimension reduction subspace. Under mild conditions (====, ====), the intersection of all the dimension reduction subspaces exists and is unique. In this case, if the intersection itself is also a dimension reduction subspace, we call it the central subspace (====, ====) for the regression of ==== on ==== and denote it by ====. Note that the dimension of ==== denoted by ==== is usually much smaller than the original predictor’s dimension ====. Thus, we reduce the dimensionality of the predictor space. The primary interest of SDR is to find such central subspace ====.====Since the introduction of sliced inverse regression (SIR; ====) and sliced average variance estimation (SAVE; ====), many methods have been proposed for estimating the basis of ====, including inverse regression (IR; ====), directional regression (DR; ====), minimum average variance estimation method (MAVE; ====), sliced regression (SR; ====), ensemble approach (====), integral transform method (====), Kullback–Leibler distance based estimator (====), likelihood based method (====), and semiparametric approach (====), etc.==== and ==== proposed a method using distance covariance (DCOV; ====, ====) for estimating the central subspace ====, ====, ==== and the sample ==== are not too large, but optimization is often computationally difficult for moderately high dimensional settings. Another method that seems to work is to use the Matlab package ==== by ==== of the proposed algorithm over the Stiefel manifold. Third, we extend our method to sufficient variable selection based on distance covariance. Simulation studies show our algorithm is ten to hundred times faster than the methods relying on SQP algorithm.====A toy example is given to visualize what SDR does and to see the performance of our algorithm and the competitor’s. In this example, we generate 800 independent copies one time from ====where ====
 ==== is generated from uniform distribution over interval ====, ==== and ==== is a standard normal error. In the following figure, we can see how the first two SDR components recover a circle pattern. Our algorithm (MMRN, see details in a later chapter) is about 20 times faster than the competitor (see ====).",MM algorithms for distance covariance based sufficient dimension reduction and sufficient variable selection,https://www.sciencedirect.com/science/article/pii/S0167947320301808,28 September 2020,2020,Research Article,318.0
"Zhu Kailun,Kurowicka Dorota,Nane Gabriela F.","DIAM, Delft University of Technology, Delft, The Netherlands","Received 9 March 2020, Revised 9 September 2020, Accepted 10 September 2020, Available online 28 September 2020, Version of Record 16 October 2020.",https://doi.org/10.1016/j.csda.2020.107091,Cited by (8),". It is shown in the simulation that the performance of the heuristic is comparable to the D-vine based approach. Furthermore, it is explained how to extend the heuristic into a situation when more than one response variable are of interest. Finally, the proposed R-vine regression is applied to perform a stress analysis on the manufacturing sector which shows its impact on the whole economy.",").====). In ====A more flexible copula model, called regular vine copula, is introduced in ====. There are ==== regular vine structures for ==== elements as proved in ====. One can choose any of these structures for the data. However, most of the time the heuristic introduced by ====, which is also implemented in the package ==== (====) in ====, is used.====For some vine structures the conditional distribution of the response variable given the covariates can be estimated directly from the vine rather than through integration. This is possible when the response variable appears in the conditioned set of the edge in the last tree of the vine. Under this requirement, the regular vine copula model has been used in ==== to estimate the conditional distribution of IQ given duration of breastfeeding and other covariates.====In ====, a D-vine based forward regression procedure has been introduced. The vine structure is fixed to be a D-vine and the order of variables in the D-vine, as introduced in ==== is the so called simplified vine where all conditional copulas do not directly depend on the conditioning variables. More about the simplifying assumption can be found in ==== and ====. In ==== and ==== different heuristics based on computing some correlation measures to choose the vine structure in the regression setting have been explored.====The contribution of this paper is an extension of the method presented in ==== to allow other structures than the D-vine in estimation of the conditional distribution. Our goal is to test whether more flexibility in the choice of vine structure (which is more computationally intensive) can lead to significantly better copula regression model. We propose a general approach of how the vine structure can be obtained during the forward selection such that the conditional distribution can be estimated in the analytic form. The approach is motivated by the results and algorithms presented in ====. Furthermore, we propose a new heuristic R-vine based forward selection procedure which is different than the tree-wise maximization of correlation in ==== during extension. Our new heuristic maximizes the correlation ’globally’ when constructing the vine structure.====Moreover, we consider in this paper the case when more than one response variables are of interest. Our heuristic of choosing a vine structure has been extended and allows to build the joint conditional distribution of multiple response variables given the covariates. The conditional distribution of two response variables obtained by our heuristic is given in an analytic form but in the case of more than two requires integration.====, ====. However, none of these lends itself to be applied in the case where many covariates are of interest. They are simply too computationally intensive and not scalable to higher dimensions. It is still an open question how to alleviate the effect of the simplifying assumption.====This paper is organized as follows: a brief introduction of regular vines along with C-vine and D-vine is in Section ====. Besides, the main tool for constructing the vine structure called vine binomial tree and its properties are introduced in the same section. In the end of that section we explain which requirement is necessary such that the conditional distribution can be estimated without integration. A review of the D-vine based forward regression is presented in Section ====. Furthermore, we propose our heuristic for the R-vine based forward selection procedure in Section ====. A comparison of the performance for different regression models based on one example data set is shown in Section ====. In Section ==== a simulation study to compare the D-vine and R-vine forward selection methods is presented. Moreover, in Section ====, our heuristic is generalized to allow estimating joint conditional distribution of more than one response variable. To improve the R-vine regression model in Section ==== a method introduced in ====, that proposes to search for several vines having 2 sampling orders in common with the initial one, is modified and applied. The real data analysis is in Section ==== and the conclusion can be found in Section ====.",Simplified R-vine based forward regression,https://www.sciencedirect.com/science/article/pii/S0167947320301821,28 September 2020,2020,Research Article,319.0
"Song Yunquan,Liang Xijun,Zhu Yanji,Lin Lu","College of Science, China University of Petroleum, Qingdao 266580, PR China,Zhongtai Securities Institute for Financial Studies, Shandong University, Jinan 250014, PR China","Received 28 August 2019, Revised 3 May 2020, Accepted 7 September 2020, Available online 28 September 2020, Version of Record 6 October 2020.",https://doi.org/10.1016/j.csda.2020.107094,Cited by (13),.,"In many fields including spatial econometrics and endemiology, the data used is spatial dependent data. To deal with these types of data, spatial regression models are widely studied. Among them, the spatial autoregressive (SAR) model ====is widely studied with ==== and has attracted widespread attention (====, ====). As a spatial weights matrix is usually constructed from geographical or economic information to characterize the spatial dependence, the candidates for the spatial weights matrix might not be unique. There exist two types of methods to select the spatial weights matrix ====. The first type of methods select the spatial weights matrix from a set of alternative ones. ==== selects the true spatial weights matrix using GMM estimates. It is suggested to use a non-nested J-test for testing a null SAR model against a set of alternative models with different spatial weights matrices. ==== suggest a modification of Kelejian’s J-test that uses available information in a more efficient way, and ==== extend it to a panel data setting. The second type of methods estimate a weighting matrix by averaging different spatial weights matrices. A model averaging procedure was proposed in ==== to reduce estimation error. This type of methods overcome the difficulty that the true spatial weights matrix is not among the candidates.====), smoothly clipped absolute deviation (SCAD, ====), and adaptive LASSO (====). Due to the spatial dependence, the above penalized methods could be used directly in the variable selection of SAR model.====As the classical variable selection methods are heavily affected by intense noise and outliers, a number of robust approaches have been proposed. Many studies adopt Huber’s loss function (====). As Huber’s method has limitations in terms of efficiency, ====, which is widely used in boosting algorithm (====). For instance, the parameter of the linear model ==== can be estimated by minimizing ====, where ==== represents the residual of the ====th observation, and ==== controls the degree of robustness and efficiency. For a large ====, ==== is small, observations with large values of ==== will result in empirical losses near 1.0 and therefore have a small impact on the estimation. Hence, a smaller ==== would limit the influence of outliers on the estimators. A choice of ==== is proposed in ====), and composite quantile regression estimator (====).====, there are a great bulk of literature contributed on variable selection and model selection for the SAR models. ==== used stochastic search variable selection (SSVS) priors to deal with the problem of variable selection in the SAR models, which avoids the complex calculation of marginal likelihoods in BMA. Recent work by ====. Although Bayesian methods have made great progress in variable and model selection for the SAR models, there are some difficulties to assess the quality of priors and choose proper priors for users in applications. On the other hand, the classical non-Bayesian penalized methods, such as LASSO and SCAD, have got success in the classical linear regression models. However, there is only some initial focus on non-Bayesian methods for SAR model. To the best of our knowledge, ==== studied variable selection of the SAR model via LASSO method. ==== developed a penalized quasi-maximum likelihood method for simultaneous model selection and parameter estimation, ==== employed the naive least squares for the estimation of unknown parameters in SAR models.====These methods, however, are affected to outliers in finite samples. In fact, the outliers or intense noise bring in challenges in parameter estimation and variable selection for SAR model. In the regression setting, the robustness of the resulting estimators heavily depends on the choice of the loss function. For dealing with outliers in spatial ====, it is natural to consider employing robust loss functions. However, this research map is far from trivial. The main challenge comes from the spatial dependence, which is usually characterized by the spatial weight matrix. As the weight matrix itself usually could not be accurately estimated, or even has a large deviation from the true value, it is troublesome to deal with the inherent large noise at the same time.====where ====, ====, ==== is a penalty term, ==== is the exponential squared loss function: ====. Here, ==== is tuning parameter controlling the degree of robustness. A small ==== could limit the impact of an outlier on the estimators, but it would also reduce the sensitivity of the model. While it seems natural to construct the variable selection model of ==== is a variable to be solved, and the exponential squared loss function is nonconvex, the empirical loss term is an essentially a structured nonconvex function with respect to two block of variables, ==== and ====. Moreover, as many of the penalty terms, such as the Lasso or adaptive Lasso penalty, are nondifferentiable, the objective function of ==== is a nonconvex, non-differentiable and block-structured function.====In this work, we presented a robust variable selection method for spatial autoregressive based on the exponential squared loss function and the adaptive lasso penalty. The method could select important predictors and, simultaneously, estimate the ====. The main contributions of this work are as follows.====The organization of this paper is as follows. In Section ====, we introduce the SAR model with independent and identical distribution errors, and present the penalized exponential squared loss method under adaptive Lasso penalty function. In Section ====, we propose an efficient algorithm to complete the variable selection procedure. Some simulations are carried out to examine the finite sample performance in Section ==== and an example of application is presented in Section ====. we conclude the research in Section ====.====.====The main abbreviations and notations used in this work are as follows.",Robust variable selection with exponential squared loss for the spatial autoregressive model,https://www.sciencedirect.com/science/article/pii/S0167947320301857,28 September 2020,2020,Research Article,320.0
"Wang Zhanfeng,Noh Maengseok,Lee Youngjo,Shi Jian Qing","Department of Statistics and Finance, Management School, University of Science and Technology of China, Hefei, China,Department of Statistics, Pukyong National University, Busan, Republic of Korea,Department of Statistics, Seoul National University, Seoul, Republic of Korea,Department of Statistics and Data Science, College of Science, Southern University of Science and Technology, Shenzhen, China,School of Mathematics, Statistics & Physics, Newcastle University, Newcastle, UK","Received 3 August 2019, Revised 24 June 2020, Accepted 8 September 2020, Available online 24 September 2020, Version of Record 29 September 2020.",https://doi.org/10.1016/j.csda.2020.107093,Cited by (2),The ,". The conditional mean ==== is often used to fit ====, based on the model ====, where ==== is an error term. Let ==== be an unknown mean function of ====. Then, the nonparametric concurrent regression model is rewritten as ====To estimate the function ====, this paper considers a process regression model ====where ==== is a random function, and ==== is an error process. In model ====, ==== can be treated as a nonparametric random function and thus this model can be called a nonparametric random functional regression model. Without loss of generality, we can set mean function of ==== to be 0 in ====. We will focus our discussion in this paper on functional batch data. More explanation will be given later.====When ==== and ==== is called the Gaussian process regression (GPR) model. GP can also be treated as a prior process of the unknown function ====, ====. The random error ====). Recent developments include GPR analysis for batch data (====) and GPR for single-index model (====). However, it is well-known that the GPR model is susceptible to outliers. To overcome this problem, one way is to replace Gaussian random error ====). Furthermore, ==== can be modeled by heavy tailed processes such as t-process (TP); for example, ==== used a simple TP; and ==== used other heavy-tailed processes such as Slash process and contaminated-normal process. TP has been used frequently in many different areas to build a robust model, for example, ==== and ==== used a TP to build a multi-task learning model, and ==== is usually defined jointly with random errors ====  under the same scale parameter (see e.g. ====, ====, ====. For example, the extended t-process regression (eTPR) model (====) assumes that ==== and ==== in ==== and the distribution assumption of ==== is not justified for many applications. In addition, as we shall discuss in Section ====, prediction from the eTPR model tends quickly to that from the GPR model as sample size grows. We can overcome this drawback  by modeling the regression function ==== separately from the distribution of random errors, where the latter is usually linked to the distribution of the response variable.====In our real data example of motor learning, we are interested in studying how performance changes over time for patients with stroke and dementia while they play the game. There are 24 young subjects with age range between 20 and 35, and 26 old adults with ages between 52 and 80 years. For each subject, ==== mean distances (meandist) between cursor and target in the tracking phase are recorded against time. About half of the subjects (young or old) are randomly allocated to one group with a single instruction (single treatment group), and the others are given double instructions (double treatment group). One of the aims of the project is to compare the treatment effects between two groups. Let ==== stand for two groups, and ==== denote ====th subject in the ====th group. Then the model can be expressed as ====, where ==== is the meandist and ====. We are interested in comparing ====, the treatment effect functions for two groups. The distribution of the random errors ==== is linked to the distribution of ====. A general and flexible way is to treat the following two issues independently: one issue is to model ==== and the other is to make assumption about the distribution of ====. Hence, this paper studies a general robust t-process regression model in which ==== and ==== are defined separately. We name this general model as gTPR. This is a very flexible model. When ==== and ====The gTPR models, however, involve intractable integrations in the calculation of predictive mean and variance. We use the idea of h-likelihood method to develop an efficient algorithm. The h-likelihood was first introduced by ==== as an extension of the classical likelihood to random unknowns, and extended to wider range of applications (====, ====, ====, ====). It is particularly an efficient implementation method for uni-mode distributions.====For GPR models, statistical properties such as information consistency of the estimator of unknown ==== have been studied, e.g. ====, ==== and ====The remainder of the paper is organized as follows. Section ====. In Section ====, the gTPR model is defined for batch data and an efficient estimation procedure is proposed. Numerical studies and real examples, including detection of outlying curves, are given in Section ====. We conclude in Section ====. All the proofs are presented in the Supplementary materials.====The following is the Supplementary material related to this article. ",A general robust t-process regression model,https://www.sciencedirect.com/science/article/pii/S0167947320301845,24 September 2020,2020,Research Article,321.0
"Hees Katharina,Nayak Smarak,Straka Peter","Department of Statistics, TU Dortmund University, Dortmund, Germany,National Australia Bank, Melbourne, Australia,School of Mathematics and Statistics, UNSW, Sydney, Australia","Received 20 September 2019, Revised 14 September 2020, Accepted 14 September 2020, Available online 19 September 2020, Version of Record 23 October 2020.",https://doi.org/10.1016/j.csda.2020.107096,Cited by (3),"In many complex systems studied in statistical physics, inter-arrival times between events such as solar flares, trades and neuron voltages follow a heavy-tailed distribution. The set of event times is fractal-like, being dense in some time windows and empty in others, a phenomenon which has been dubbed “bursty”. A new model for the ","Time series displaying temporally inhomogeneous behaviour in terms of the occurrence of events have received strong interest in the recent statistical physics literature (====, ====, ====, ====, ====, ====, ====, ====). They have been observed in the context of earthquakes, sunspots, neuronal activity and human communication (see ====, ==== for a list of references) (====). Such time series exhibit high activity in some ‘bursty’ intervals, which alternate with other, quiet intervals. Although several mechanisms are plausible explanations for bursty behaviour (most prominently self-exciting point processes by ==== and renewal Hawkes processes, e.g. ====, ====, ====, ====).====Often a magnitude, or mark can be assigned to each event in the renewal process, such as for earthquakes, solar flares or neuron voltages. The Peaks-Over-Threshold model (POT, see e.g. ====). Then as one increases the threshold ====, the Poisson process is thinned, i.e. its intensity decreases ==== with ====
 (see e.g. ====).====As will be shown below, in the heavy-tailed waiting time scenario threshold crossing times form a ==== (====, ====), and hence the fractional Poisson process generalizes the standard Poisson process. Again as the threshold size ==== increases and the threshold crossing probability ==== decreases, the fractional Poisson process is thinned: The scale parameter of the Mittag-Leffler inter-arrival times of threshold crossing times increases, but ====; see the Theorem below.====Maxima of events which occur according to a renewal process with heavy-tailed waiting times have been studied under the names “Continuous Time Random Maxima process” (CTRM) (====, ====, ====, ====), “Max-Renewal process” (====, ====, ====), and “Shock process” (====, ====, ====, ====, ====, ====). The existing literature focuses on probabilistic results surrounding these models. In this work, however, we introduce a method of inference for this type of model, which is seemingly not available in the literature.====We review the marked renewal process in Section ====. We give a statistical procedure to estimate model parameters via stability plots in Sections ====, ====, but first we need to discuss inference for the Mittag-Leffler distribution in Section ==== as well as a Likelihood-ratio test to guide the choice, whether a Mittag-Leffler or an exponential distribution fits better to the inter-exceedance times. A simulation study of the effectiveness of our statistical procedure is given in Section ====. In Section ==== we apply our method to a real dataset. In Section ====. For all statistical computations we have used ====
 (====) and the package ====
 (====.",Statistical inference for inter-arrival times of extreme events in bursty time series,https://www.sciencedirect.com/science/article/pii/S0167947320301870,19 September 2020,2020,Research Article,322.0
"Watanabe Chihiro,Suzuki Taiji","Graduate School of Information Science Technology, The University of Tokyo, Tokyo, Japan,Center for Advanced Intelligence Project (AIP), RIKEN, Tokyo, Japan","Received 12 March 2020, Revised 6 July 2020, Accepted 10 September 2020, Available online 19 September 2020, Version of Record 29 September 2020.",https://doi.org/10.1016/j.csda.2020.107090,Cited by (4),"Latent block models are used for probabilistic biclustering, which is shown to be an effective method for analyzing various ==== sets. However, there has been no statistical test method for determining the row and column cluster numbers of latent block models. Recent studies have constructed statistical-test-based methods for stochastic block models, which assume that the observed matrix is a square ==== of the test statistic and measuring the test accuracy.","Block modeling (====, ====), customer–product transactions (====), congressional voting (====), document–word relationships (====), and gene expressions (====). Latent block models or LBMs (====) are used for probabilistic biclustering of such relational data matrices, where rows and columns represent different objects. For instance, suppose that a matrix ==== represents the relationship between users and movies, where entry ==== is the rating of the ====th movie by the ====th user. In LBMs, we assume a regular-grid block structure behind the observed matrix ====; i.e., both rows (users) and columns (movies) of matrix ==== are simultaneously decomposed into latent clusters. A block is defined as a combination of row and column clusters, and entries of the same block in matrix ====An open problem in using LBMs is that there has been no statistical criterion for determining the numbers of row and column clusters. Recently, statistical-test-based approaches (====, ====, ====) have been proposed for estimating the cluster number of stochastic block models (SBMs) (====). In regard to the LBM setting, no statistical method has been constructed to determine row and column cluster numbers.====Aside from the test-based methods, several model selection approaches have been proposed based on cross-validation (====) or an information criterion (====, ====, ==== random matrix (====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====). Here, we assume that each entry ==== of matrix ====, which is given by ==== (which is computed by the original matrix ====, its block-wise mean ==== and standard deviation ====) follows a distribution with a sub-exponential decay. From the result in ====, the normalized maximum eigenvalue of ==== converges in law to the Tracy–Widom distribution with index ====, under the above sub-exponential condition. Based on this result, we constructed a goodness-of-fit test for a given set of row and column cluster numbers of an LBM, using the maximum singular value of matrix ====, which is an estimator of the matrix ====. We proved that under the null hypothesis (i.e., observed matrix ==== consists of a given set of row and column cluster numbers), the proposed test statistic ==== converges in law to the Tracy–Widom distribution with index ====
 (====). We also showed that under the alternative hypothesis, test statistic ==== increases in proportion to ==== is a number proportional to the matrix size (====, ====).====The proposed method solves the limitations of other model selection approaches. (1) Our statistical test method enables us to obtain knowledge about the reliability of the test results. When testing a given set of row and column cluster numbers, we can explicitly set the probability of Type I error (or false positive) as a significance level ====. (2) Unlike the other model selection methods, the proposed method does not depend on the clustering algorithm as long as it satisfies the consistency condition (Section ====). It only uses the output of a clustering algorithm to test a given set of cluster numbers; there is no need to modify the test method according to the clustering algorithm. (3) The proposed test method requires relatively small computational complexity. It does not require the MCMC procedure or partitioning into the training and test data sets. For these reasons, the proposed test-based method can be widely used for the purpose of knowledge discovery.====The next sections consist of the detailed explanation of the proposed test method for LBMs. In Section ====, we describe the proposed goodness-of-fit test and its theoretical guarantee with the assumptions required for the problem setting. Next, we briefly review the related works and their differences from the proposed method in Section ====. The main results are presented in Section ====, we experimentally demonstrate the effectiveness of the proposed test method by showing the asymptotic behavior of the test statistic and calculating the test accuracy. We discuss the results and limitations of the proposed method in Section ==== and conclude the paper in Section ====.====Let ==== and ====, respectively, be the row and column sizes of the ====th ==== block, and ====, ====, and ====, respectively, be the ====th ==== blocks of matrices ====, ====, and ====. Here, we prove the following lemma:",Goodness-of-fit test for latent block models,https://www.sciencedirect.com/science/article/pii/S016794732030181X,19 September 2020,2020,Research Article,323.0
"Gressani Oswaldo,Lambert Philippe","Institute of Statistics, Biostatistics and Actuarial Sciences (ISBA), Université catholique de Louvain, Voie du Roman Pays 20, B-1348, Louvain-la-Neuve, Belgium,Institut de Recherche en Sciences Sociales (IRSS), Méthodes Quantitatives en Sciences Sociales, Université de Liège, Place des Orateurs 3, B-4000, Liège, Belgium","Received 21 March 2020, Revised 1 September 2020, Accepted 3 September 2020, Available online 11 September 2020, Version of Record 19 September 2020.",https://doi.org/10.1016/j.csda.2020.107088,Cited by (9),Generalized ,", ==== and ==== provide a complete and comprehensive treatment of GAMs, emphasizing on semiparametric methods and penalized regression splines.====There exist a large variety of regression splines in the literature for modeling the smooth terms in a GAM, for instance P-splines (====), thin plate splines (====), O’Sullivan penalized splines (====) or adaptive splines (====As MCMC techniques can be subject to poor chain convergence and tend to carry a heavy computational burden, ====, ==== and ==== among others. Taken separately, P-splines and INLA have made an impressive impact in the statistical community and initiated a flourishing literature in diversified domains (see e.g. ====; ====), yet a few references attempted to unify the strength of both approaches. ====In the present article, we borrow some ideas from INLA and combine them with P-splines to design the Laplace-P-spline (LPS) methodology, a novel unified approach for approximate Bayesian inference in generalized additive models. Although INLA is a well-tailored approach for making inference in a variety of statistical models, there is room for further computational improvements when considering the specific class of GAMs. In particular, the use of ====The remainder of the article is outlined as follows. In Section ====, the Bayesian Laplace-P-spline generalized additive model is formulated and the Laplace approximation to the conditional posterior of regression and spline parameters is derived. Section ==== with comparisons against a popular benchmark method. Section ==== closes the paper with concluding remarks and sketches future research prospects.====This appendix provides in full detail the analytical derivations of the gradient and Hessian associated to the (log-) posterior of the log penalty vector: ==== where for notational convenience, we define ====.====To obtain the gradient of ====, the partial derivatives of the latter quantity with respect to ==== are required. The partial derivative of Term I in ==== can be obtained using Jacobi’s formula: ==== where ==== is a (symmetric) block diagonal matrix defined as: ====Derivation of Term II with respect to ==== simply equals the scalar ====: ====Partial derivatives of Term III and Term IV are obtained using: ==== For Term III, recall that the trace is invariant under cyclic permutations: ==== For Term IV we use the chain rule and obtain: ==== The partial derivative of Term V is obtained as follows: ==== With regard to the derivative of Term VI we have: ==== For notational convenience we define ====. From all the above intermediate results for Terms I-VI, the gradient ==== has the following entries: ====First, we focus on the diagonal entries. The derivative of Term VII is: ==== Let us derive the intermediate result: ==== Partial differentiation of Term VIII yields: ==== and using intermediate result ====, one obtains for Term VIII: ==== For Term IX, we have: ==== Using ==== and intermediate result ==== we have for Term IX: ==== The partial derivative of Term X is obtained as follows: ==== Partial differentiation of Term XI gives us: ==== Finally derivation of Term XII gives us: ====Using the differentiation results for Terms VII–XII, the diagonal elements of the Hessian of ==== are: ==== Regarding the off-diagonal components, note that for index ==== we have for Term VII: ==== Let us define ==== and consider the following intermediate result: ==== Result ==== can be used to obtain the differentiation of Term VIII: ==== To derive Term IX, we also use result ====: ==== Partial differentiation of Term X goes as follows: ==== Partial differentiation of Term XI gives us: ==== Finally, using the above results, the off-diagonal elements ====; ==== and ==== of the Hessian of ==== are: ==== To summarize, the gradient and Hessian entries of ==== are:==== ==== ==== ====:==== ====, ==== ====:==== ====, ==== ====, ====: ==== To assess the accuracy of the above gradient and Hessian equations associated to ====, we have implemented a procedure in ==== that compares the analytical results with the numerical derivatives of ==== obtained with the ==== and ==== functions of the ==== package at 50 randomly selected points ==== with ==== and the response generated from a Poisson distribution. Numerical and analytical derivative results turn out to be very similar, a clear indication that the derived analytical results are accurate.",Laplace approximations for fast Bayesian inference in generalized additive models based on P-splines,https://www.sciencedirect.com/science/article/pii/S0167947320301791,11 September 2020,2020,Research Article,324.0
Fiebig Ewelina Marta,"Technische Universität Berlin, Fakultät VII - Wirtschaft und Management, Institut für Volkswirtschaftslehre und Wirtschaftsrecht, FG Ökonometrie und Wirtschaftsstatistik, Sekr. H 57, Straße des 17. Juni 135, 10623 Berlin, Germany","Received 6 January 2020, Revised 1 September 2020, Accepted 1 September 2020, Available online 9 September 2020, Version of Record 8 October 2020.",https://doi.org/10.1016/j.csda.2020.107087,Cited by (0),"A new procedure is proposed for selecting the input value for the adaptation bandwidth ==== in nonparametric Gaussian regression via Propagation–Separation approach. Since ==== in a data-driven way. Its performance is evaluated via simulations. The results are very convincing: Cross-validation is a very transparent selection procedure that provides for each sample a “tailor-made” input value for ==== and allows a successful identification of the underlying regression function. As the sample size increases, the accuracy of estimation improves in the sense that the estimates approach the true regression function. In addition, cross-validation accounts for the weighting functions and other parameter values used within the Propagation–Separation method by adjusting ==== accordingly, which results in very robust estimates."," and ====. Unlike the conventional kernel regression, PS is structurally adaptive, does not suffer from the curse of dimensionality, and is able to detect and preserve edges. It is an extension of the Adaptive Weights Smoothing (AWS) by ==== the greatest local neighborhood ==== in which the unknown model function ==== is constant or can be well approximated by a constant value ====. By doing so, the PS procedure puts no restrictions on the shape of the neighborhoods ====, nonparametric density estimation and classification (====). Other applications can be found, for example, in ====, ====, ====, ====, ====, and ====.====The big challenge in applying the iterative PS method lies in finding an appropriate input value for ==== – the ====). The opposite happens if the function appears to change locally (====). The decision between propagation and separation, which is at the heart of the structurally adaptive PS approach, is entirely controlled by the tuning parameter ====. If ==== is too small, a false alarm of a changing regression function may result, and estimation efficiency via propagation is forgone. On the contrary, if ==== is too large, a change in the regression function may not be detected, resulting in too large estimation window and thus biased estimates. Although ==== is at the heart of the trade-off between flexibility (bias) and precision (variance), so far no convincing method has been developed for a data-driven choice of this parameter. This severely impedes the practical application of the procedure, despite its highly desirable statistical properties.====There exist two theoretical proposals for choosing ==== and ====, respectively. Strictly speaking, the PC formulated in ==== is a simplified version of the PC from ====. We explain these criteria in more detail in Section ====. What we want to emphasize here, however, is that both PCs do not depend on the data at hand, and are designed to safeguard against selecting too small value of ==== in case of a globally constant regression function. That means, they are based on an artificial and purely theoretical setup which is unlikely to ever be given in practice. Furthermore, the formal proof of the PC formulated in ==== is based on an additional assumption (called there “Assumption S0”), which in reality will not hold true due to the iterative structure of the PS algorithm. We discuss this in more detail in Section ====.====In this paper we propose a data-driven selection procedure for the input value of ====. Particularly, we take advantage of the fact that the adaptation bandwidth plays in the PS approach the same role as the window width ==== (the ====) in the conventional nonparametric regression — that is, it is the key tuning parameter of the procedure which is at the bias–variance trade-off and has to be determined by the analyst. Therefore, our method is based on the idea of Cross-Validation (CV). Although in the standard kernel regression CV is regarded as a very successful data-driven selection procedure for the globally optimal value of bandwidth (====), it has not yet been analyzed in the PS framework in the context of ====). This is why we mostly focus on simulations as an analytical tool. We do so by considering Gaussian regression under different structural shapes of the model function ==== and different values of the error variance ====. Our results are very promising (Sections ====, ====). They reveal that the proposed CV approach is a very transparent selection procedure that provides for each sample a “tailor-made” input value for ==== and allows a successful identification of the underlying regression function. It works particularly well when ==== is low relative to the contrast of ====. However, even if the noise level is high, the CV-based solution approaches ==== when the sample size, ==== proposed by ==== is used for each sample. According to our analysis, this is because this value is closely related to the size of the support of the ==== ==== when changing this kernel.====In ==== the PS algorithm considered in this paper is also referred to as ==== PS approach. This emphasizes that the ==== of the procedure is omitted here. The memory step was introduced by the authors in the original version of the algorithm to provide certain stability of estimates during the iteration process, as well as to allow for a proof of some theoretical properties of the method (==== Theorem 5.7; Theorem 5.8; Theorem 5.9). The idea behind this step was to compare in each iteration and at each location the new estimate with the previous one. In case of a significant difference, the new estimate was replaced by a weighted average of the two estimates under consideration. However, neither the necessity nor the positive impact of the memory step could be confirmed in applications (====, ====, ====, ====, ====). Furthermore, the study by ==== has shown that the PS approach preserves its most important properties – that is, propagation and separation – also without the memory step. In particular, because they are independent of this step and result solely from the adaptivity of the method. That is why we focus here on the simplified version of the PS approach. However, for the sake of brevity, we drop the word “simplified” throughout the paper.====The remainder of this article is structured as follows. Section ==== defines the Gaussian regression model – which stands in focus of this research – and describes how the PS algorithm works within this framework. It also gives an overview of the existing theoretical proposals for choosing ==== as well as points out their strengths and weaknesses. The new approach for choosing ==== – the CV algorithm – is presented in Section ====, which begins with a brief description of the necessary notation, then continues with the description of the algorithm. Section ==== presents results of CV for a set of Gaussian random samples and discusses the most important findings. Section ==== demonstrates how well the new procedure performs in comparison to the PC. In Section ==== some properties of the new method are analyzed. Finally, Section ==== concludes and gives an outlook on further research. Some additional illustrations and an insight into how the CV algorithm performs when applied to functions that do not meet the structural assumption are provided in the ====, ====, respectively.====See ====, ====.",On data-driven choice of ,https://www.sciencedirect.com/science/article/pii/S016794732030178X,9 September 2020,2020,Research Article,325.0
"Park Gunwoong,Kim Yesool","Department of Statistics, University of Seoul, Seoulsiripdaero 163, Dongdaemun-gu, Seoul 02504, Republic of Korea","Received 11 March 2020, Revised 26 August 2020, Accepted 26 August 2020, Available online 6 September 2020, Version of Record 17 September 2020.",https://doi.org/10.1016/j.csda.2020.107084,Cited by (4), is the number of nodes and ==== is the maximum degree. It is also shown that the proposed algorithm requires ,", ====). However, learning the graph structure of a causal or directional network only from observations is a notoriously difficult task owing to non-identifiability and non-polynomial computational cost.====Recently, a number of fully identifiable classes of directed acyclic graphical (DAG) models, as well as their learning algorithms, have been developed: For example, ====, ====, ====, ====, ====, ====, ==== relax the assumption of linearity, and provide learning approaches for nonlinear ANMs where each variable is determined by a non-linear function of its parents and an error term. ====, ====, ====, ====, ====, ====, ====, ==== further develop Gaussian linear SEMs learning approaches when error variances are unknown heterogeneous.====In this paper, the design of a novel algorithm for learning high-dimensional Gaussian linear SEMs with heterogeneous error variances, as discussed in ====, is presented. It is proven that the proposed algorithm has the sample bound ==== where ==== is the number of nodes and ==== is the maximum degree. We acknowledge that it is not the first theoretical result for recovering ==== Gaussian linear SEMs. In fact, ==== establish the consistency of learning high-dimensional sub-Gaussian linear SEMs with the sample bound ==== when error variances are heterogeneous. Furthermore, for the same error variance setting, ====, ==== respectively provide the sample complexities of their algorithms, ==== and ====, where ==== is the predetermined upper bound of the in-degree. We provide a detailed comparison in terms of both computational and sample complexities in Sections ====, ====.====Aside from the differences in sample complexity, it is important to note that when error variances are heterogeneous, the identifiable linear SEMs discussed in ====). Hence, to the best of our knowledge, this is the first theoretical result that applies to the high-dimensional Gaussian linear SEMs discussed in ====.====We demonstrate through simulations and a real human cell signaling data application involving multivariate Gaussian data, that the proposed algorithm performs well in terms of recovering directed edges. In the simulation study, we consider high-dimensional and sparse settings where the number of nodes is ==== and the in-degree of graphs is ====. Furthermore, our algorithm is compared to the state-of-the-art uncertainty scoring (US) (====), greedy DAG search (GDS) (====), linear structural equation model learning (LISTEN) (====), the top-down search (TD) (====), PC (====), and greedy equivalence search (GES) (====) algorithms.====The remainder of this paper is structured as follows: Section ==== summarizes the necessary notations and problem settings. Section ==== explains Gaussian linear SEMs and Section ==== discusses their identifiability conditions. In Section ====, we introduce a new Gaussian linear SEM learning algorithm. Sections ====, ==== compares the proposed algorithm to the related works in terms of sample complexity. Sections ====, ==== evaluate our methods and state-of-the-art algorithms using synthetic and real data.",Learning high-dimensional Gaussian linear structural equation models with heterogeneous error variances,https://www.sciencedirect.com/science/article/pii/S0167947320301754,6 September 2020,2020,Research Article,326.0
"Zhou Lin,Tang Yayong","College of Mathematics, Sichuan University, Chengdu, Sichuan, 610064, China","Received 17 August 2019, Revised 22 July 2020, Accepted 25 July 2020, Available online 4 September 2020, Version of Record 6 October 2020.",https://doi.org/10.1016/j.csda.2020.107056,Cited by (1)," can be used to accelerate the EM algorithm. Essentially, this method is an adjustment of the AEM algorithm, and it usually achieves a faster convergence rate than the AEM algorithm by sacrificing a little simplicity. The convergence of the APX-EM algorithm, includes a global convergence result for this method under suitable conditions, is discussed. This method is illustrated for factor analysis and a random-effects model.","The EM algorithm formulated by ====, ==== and ====. This has resulted in the development of methods to accelerate the EM algorithm. ==== classified them into three groups: ==== and ==== accelerators. Pure accelerators are those that require only the EM algorithm for their implementation, the QN1 algorithm devised by ==== is a pure accelerator. EM-type accelerators do not actually use the EM algorithm but do use things related to the EM algorithm such as its ====-function, there are numerous accelerators belonging to this group, including: the ECM algorithm of ====, the ECME algorithm of ====, the AECM algorithm of ==== and so on. Finally, hybrid accelerators require both the EM algorithm and other problem-specific quantities such as the log-likelihood and its gradient, for instance, the AEM algorithm of ==== and the DAAREM algorithm of ==== both are hybrid accelerators.====The AEM algorithm suggested by ====, where ==== and ==== is the Hessian of the ====-function ==== viewed as a function of ==== and evaluated at ====
 (==== is actually the linearly PNCG algorithm described by ==== which maximizes the complete-data log-likelihood as a function of the working parameter within each EM iteration. The so-called PX-EM algorithm is an EM-type accelerator, and the underlying statistical principle is to perform a ‘covariance adjustment’ to correct the M step. Its advantage is a certain amount of gain in speed of convergence if the expansion is appropriately chosen. To further understand why the PX-EM algorithm can work so well, ==== discussed the nonidentifiability of expanded parameters in the observed-data model and took a look at PX-EM algorithm from the point of view of efficient data augmentation.====As the PX-EM algorithm is the same as the plain EM algorithm to the expanded model, each PX-EM step can also be viewed as a generalized gradient defined by a matrix ====, and we discovered that the difference between ==== and ==== is ====, where ==== is the auxiliary parameter and ==== are the corresponding covariance matrices of the MLE in the expanded model. This discovery prompted us to propose a hybrid accelerator, called accelerated PX-EM (APX-EM) algorithm, which can often make AEM faster. More specifically, we use a PX-EM step instead of an EM step as a generalized gradient and then apply the linearly PNCG algorithm with the preconditioner ====. The APX-EM algorithm can be applied to many areas with only simple modifications of AEM algorithms, we will attempt to show by examples that the APX-EM algorithm usually converges faster than the AEM algorithm.====The article is organized as follows. Section ==== gives a brief review of the AEM algorithm and the PX-EM algorithm. Section ==== provides the basic theory of the APX-EM algorithm, including the relationship of the generalized gradient algorithm and the linearly PCNG algorithm, an explanation of why we can view each PX-EM step as a generalized gradient and a result of global convergence about the APX-EM algorithm. Section ==== demonstrates the APX-EM algorithm for a confirmatory factor analysis and a random-effects model. Finally, Section ==== concludes with a short discussion.",Linearly preconditioned nonlinear conjugate gradient acceleration of the PX-EM algorithm,https://www.sciencedirect.com/science/article/pii/S016794732030147X,4 September 2020,2020,Research Article,327.0
"Chaoubi Ihsan,Cossette Hélène,Marceau Etienne,Robert Christian Y.","Université Laval, Québec (Qc), Canada,CREST, Paris, France,Université Claude Bernard Lyon 1, Lyon, France","Received 4 March 2020, Revised 13 August 2020, Accepted 15 August 2020, Available online 3 September 2020, Version of Record 12 September 2020.",https://doi.org/10.1016/j.csda.2020.107071,Cited by (0)," within-block copulas and asymmetric between-block pair-copulas. For the specific case of two-level trees, dependence properties of pairs are investigated, and a full estimation procedure is proposed for the tree structure and parameters of the hierarchical copulas. The efficiency of the procedure is illustrated through three simulation examples and a study with two real datasets.","Introduced by ==== -dimensional expression given by ====where ==== and ==== is a continuous and non-increasing function called the ==== of the copula. It satisfies ==== and ====, and it is strictly decreasing on ====. The function ==== is the inverse of ==== (with ====). Moreover, ==== defines an Archimedean copula in any dimension ==== if and only if ==== is completely monotone (see ====). From Bernstein’s Theorem (see ====), ==== has to be the Laplace–Stieltjes transform (LST) of a strictly positive rv. This type of copulas has the advantages of having explicit expressions, as well as capturing various dependence structures and being easily used in high dimensions (see, e.g., ====, ====, ==== and ==== of ====, we have that ====. In practice, this can be a major drawback.====To allow non-exchangeability, hierarchical Archimedean copulas (HAC) have been developed. ==== discusses the first approach to construct such copulas, leading to the well-known nested Archimedean copulas, obtained by nesting Archimedean copulas into each other. Further research has been done on this construction technique, see, e.g., ==== and ====. As pointed out notably by ====, the verification of the sufficient nesting condition within these copulas is not always an easy task when generators belong to different Archimedean families hence leading to less flexible structures. Note that the authors in Section 3 of ==== discuss this aspect in detail and provide sufficient nesting conditions. Recent developments on other construction techniques have successfully suppressed the need for this condition to be verified. For example, ==== proposed to construct HACs using Lévy subordinators, while ==== and random sums with common counting rvs in ==== . A detailed comparison of these two construction approaches is discussed in Section 4.2 of ====. Although this family of copulas offers very rich and complex structures of dependence, it can typically not be used for data as represented in ====Characterized by their Pickands dependence function, extreme value copulas aim to represent the limiting dependence structure of componentwise block maxima under appropriate normalizations. An extreme value copula is asymmetric under the condition of an asymmetric Pickands dependence function. Introduced by ====, bivariate Archimax copulas may also create asymmetric dependence. They include both the extreme value copulas and the Archimedean copulas. A multivariate extension of Archimax copulas was proposed by ==== and investigated in depth by ====. Finally, ====In this paper, we introduce a hierarchical copula that allows to have a block exchangeability structure, in which within-block copulas are Archimedean and between-block copulas are non-exchangeable (in particular pairs that belong to different blocks are asymmetric). Inspired by ==== proposed a popular mechanism for generating bivariate asymmetric copulas through products of copulas (not necessarily Archimedean). We refer to ==== for a review on the construction of asymmetric bivariate dependence structures (measures of asymmetry and tests of symmetry are also presented and discussed). A multivariate extension of ====’s multiplicative procedure was proposed in ====. We observe such types of products of copulas with our model for between-block copulas.====The remainder of the paper is organized as follows. In Section ====, we present the construction approach of the proposed family of hierarchical copulas. We introduce notations and provide general families of exchangeable and non-exchangeable copulas of blocks of leaves within the tree structure of our proposed family. In Section ====, we investigate the special case of two-level hierarchical copulas. We first present and discuss several specific families of copulas. Then we provide sufficient conditions for stochastic comparison of pair-copulas. In Section ====, we propose a full estimation procedure that is able to determinate the tree structure of a two-level hierarchical copula and to produce estimates of the parameters through a bottom-up composite likelihood approach. The full estimation efficiency is illustrated through three examples with simulated data and a study with two real datasets.",Hierarchical copulas with Archimedean blocks and asymmetric between-block pairs,https://www.sciencedirect.com/science/article/pii/S0167947320301626,3 September 2020,2020,Research Article,328.0
"Ke Rui,Lu Wanbo,Jia Jing","School of Economics, Hefei University of Technology, Hefei, China,School of Statistics, Southwestern University of Finance and Economics, Chengdu, China,School of Economics, Anhui University, Hefei, China","Received 7 July 2019, Revised 23 May 2020, Accepted 30 August 2020, Available online 3 September 2020, Version of Record 8 September 2020.",https://doi.org/10.1016/j.csda.2020.107086,Cited by (0),This paper considers a residual-based approach to diagnose the adequacy of both the univariate and vector multiplicative error model (MEM). Two residual-based ,"The research on the microstructure of securities markets relies on the dynamic analysis of a large number of non-negative time series. The multiplicative error model (MEM) proposed by ==== and its multivariate version, the vector MEM (VMEM) proposed by ====, are particularly suited to model such non-negative time series. The autoregressive conditional duration (ACD) model and the conditional autoregressive range (CARR) model are special cases of the MEM. These models have been successfully adopted for modeling and forecasting many non-negative financial data, such as the financial duration (====), price range (====), trading volume (====), absolute return (====), realized volatility (====) and trading intensities (====).====Since the family of MEMs has been widely applied to analyze various non-negative time series in financial market, diagnostic checking the adequacy of the MEMs becomes an important issue in model evaluation. A number of diagnostic tests have been proposed in the recent literature, and they can be divided into three categories (====, ====, ==== distribution under the null hypothesis. Therefore, ====, ====). The second category of tests checks the distribution of the innovation term. ==== consider an alternative nonparametric test by comparing a consistent nonparametric density estimator directly with a parametric density under the null hypothesis. Recently, ==== suggest several goodness-of-fit tests based on the cumulative distribution function and exponential transforms of the innovation distribution. The last category of tests checks the functional form of the conditional mean function. As stressed by ====), generalized spectral derivative-based tests (====, ====). In addition, ==== and ==== also consider the diagnostic checking of a given MEM having a Markov structure.====However, all of the above mentioned tests are only suitable for univariate MEMs, there are few diagnostic tests available for the VMEM in the literature. The first is the Box–Pierce–Ljung type multivariate portmanteau test (====, ====), test conditional asymmetry (====) and fractional cointegration (====The rest of the paper unfolds as follows. In Section ====, we propose the univariate and multivariate version of residual-based statistics and derive their asymptotic distribution. Section ==== contains the results of the Monte Carlo simulations to evaluate the finite sample performance of the residual-based test. An empirical example is considered in Section ====. Section ==== concludes the paper.",Evaluating multiplicative error models: A residual-based approach,https://www.sciencedirect.com/science/article/pii/S0167947320301778,3 September 2020,2020,Research Article,329.0
"Byrd Michael,Nghiem Linh H.,McGee Monnie","Department of Statistical Science, Southern Methodist University, TX, 75206, USA,Research School of Finance, Actuarial Studies, and Statistics, Australian National University, ACT 2601, Australia","Received 31 March 2020, Revised 25 August 2020, Accepted 26 August 2020, Available online 3 September 2020, Version of Record 25 November 2020.",https://doi.org/10.1016/j.csda.2020.107085,Cited by (3),"A framework for determining and estimating the conditional pairwise relationships of variables in high dimensional settings when the observed samples are contaminated with measurement error is proposed. The framework is motivated by the task of establishing gene regulatory networks from ==== studies, in which measurements are taken for a large number of genes from a small sample size, but often measured imperfectly. When no measurement error is present, this problem is often solved by estimating the ==== under ==== constraints. However, when measurement error is present, not correcting for it leads to inconsistent estimates of the precision matrix and poor identification of relationships. To this end, a recent iterative imputation technique developed in the context of missing data is utilized to correct for the biases in the estimates imposed from the contamination. This technique is showcased with a recent variant of the spike-and-slab Lasso to obtain a point estimate of the precision matrix. Simulation studies show that the new method outperforms the naïve method that ignores measurement error in both identification and estimation accuracy. The new method is applied to establish a conditional gene network from a microarray dataset.","Estimating the precision matrix is a difficult task when the number of observations ==== is often much less than the dimension of the features ==== (====). The common approach is to assume that the precision matrix is sparse (====, we instead focus on the direct likelihood approach. The direct likelihood approach optimizes the full likelihood function with an element-wise penalty on the precision matrix; common examples being graphical lasso (====), CLIME (====), and TIGER (====).====There are many practical issues associated with Gaussian graphical models, such as hyperparameter tuning (====), missing data (====), and repeated trials (====), which practitioners need to adjust for a successful analysis. We address another practical issue that is often involved with the microarray studies, measurement error. In fact, microarray studies tend to have noisy measurements because of technical variations resulting from sources such as sample preparation, labeling, and hybridization (====), but, to our knowledge, has not yet been well studied in the context of Gaussian graphical models, especially in high dimensional settings.====We propose a Bayesian estimator to correct for measurement error in estimating a sparse precision matrix; our new method extends the optimization procedure of ====, referred to as BAGUS. While directly incorporating the estimate of the uncontaminated variable is possible, we find the incorporation of the imputation–regularization technique of ====).====The following is the Supplementary material related to this article. ",Bayesian regularization of Gaussian graphical models with measurement error,https://www.sciencedirect.com/science/article/pii/S0167947320301766,3 September 2020,2020,Research Article,330.0
"Davis Casey B.,Hans Christopher M.,Santner Thomas J.","Department of Statistics, The Ohio State University, United States of America","Received 1 April 2019, Revised 29 July 2020, Accepted 23 August 2020, Available online 1 September 2020, Version of Record 29 September 2020.",https://doi.org/10.1016/j.csda.2020.107083,Cited by (5)," extension of a global-trend plus local-trend model is proposed that also allows measurement errors. In contrast to the original CGP model, the new Bayesian CGP model introduces a weight function to allow the total process variability to be apportioned between the large- and small-scale processes. The proposed prior distributions ensure that the fitted global mean is smoother than the local deviations, a feature built into the CGP model. The log of the process variance for the Bayesian CGP is modeled as a Gaussian process to provide a flexible mechanism for handling variance functions that vary across the input space. A ==== Monte Carlo algorithm is proposed that provides posterior estimates of the parameters for the Bayesian CGP. It also yields predictions of the output and quantifies uncertainty about the predictions. The method is illustrated using both analytic and real-data examples.",", ====, ====).====An important early approach to prediction and UQ of an unknown ==== regarded ====, ====, ====, ====). Let ====, ====, denote a “training data” set of ====Our interest is in prediction and UQ of functions ====which was originally considered by ==== and also by ====; henceforth ==== is referred to as the BJX function. ==== plots the BJX function. The points in ==== indicate the value of ==== at the ==== training data inputs used by Ba and Joseph (2012) for prediction. The BJX function is not consistent with a draw from a stationary process because it has three behavior paradigms. For small ====, ==== can be described as having a relatively flat global trend with rapidly-changing local adjustments. For intermediate ====, ==== increases rapidly and smoothly, with few local departures. For large ====, ==== has a relatively flat global trend with minor local adjustments.====Several methodologies exist for inference from data generated by non-stationary ==== such as ====. Perhaps the most widely-used approach is universal kriging (UK) (e.g., ====), which assumes the function ==== can be viewed as a draw from a (non-stationary) GP ====where ==== is a stationary Gaussian process with zero mean, process variance ====, and positive definite correlation function ==== so that ==== has covariance ====Throughout this paper the notation ==== is that ==== describes large-scale ==== trends while ==== describes small-scale deviations from the large-scale behavior. A special case of UK is ordinary kriging which assumes ==== has constant mean. ==== and ==== provide details about model ====, methods for estimating model parameters, prediction methodology for test data inputs, and uncertainty quantification of the predictions.====Continuing the BJX example, the top row of ==== shows point predictions of ==== for the constant- and cubic-mean UK predictors computed at a 0.01 grid of test locations; the predictor interpolates the 17 training data locations (a “nugget” term was not included). While the constant- and cubic-mean predictors and uncertainty bands are similar for ====, differences can be seen when ====. Reversion to the global mean is evident for the constant-mean predictor by the dips in the predictor between training data inputs, while the cubic-mean predictor exhibits a “bump” near ==== that is driven by reversion to the estimated cubic mean function. The 95% prediction intervals based on the cubic mean are shorter than those based on the constant mean, however both sets of intervals are unreasonably wide when ====. Intuitively, the UQ intervals should be short where ==== is essentially flat.====While UK has proved useful in many applications, it has several limitations. The requirement that the regression functions be known (or adaptively selected from a pre-defined collection of regression functions) proves difficult in some applications. In addition to bias due to potential misspecification of the regression functions, standard prediction intervals under UK do not account for uncertainty in the selection of the regression functions. From a computational perspective, entertaining a large class of potential regression functions may result in a large selection problem, necessitating a combinatorial search over a large space. Finally, the UK local adjustment model described above assumes a trend-stationary Gaussian processes while in many applications, even if the zero mean function is appropriate, the unknown ==== may exhibit non-stationary behavior due to a non-constant variance function. Ignoring these potential features of ==== in the assumed process model may result in both poor prediction and inaccurate uncertainty quantification.====Other methods for emulating ====). In contrast, to avoid the need to pre-specify a specific form of non-stationarity in the covariance function, ==== uses latent Gaussian processes to construct a Gaussian process product model that is intended to capture variability in the amplitude of the response function.==== provide another attractive alternative that uses latent Gaussian processes to emulate non-stationary behavior. Their composite Gaussian process (CGP) model avoids specification of the mean regression functions by writing the generating GP ==== as the sum ====where, conditionally on model parameters ====, ==== and ==== are independent GPs such that ====In ====, ==== represents a smooth process that captures any global trend in ====, while ==== represents a less-smooth process of local adjustments required to capture the function ====. The relative smoothness of the ==== and ==== processes is controlled by the parameters of the correlation functions ==== and ====. By replacing the regression term in ==== with the more flexible ==== process, model ==== is able to flexibly adapt to rather arbitrary large-scale ==== features. By allowing the variance of the local process ==== to depend on ====, the model ==== allows for the amount of local adjustment to vary smoothly across the input space.====The bottom row of ==== displays the predicted BJX function and 95% UQ limits under the TGP model (left panel, fit using the ==== package of ====, ====, ====) and the CGP model (right panel, fit using the ==== package of ====). The CGP predictions appear more accurate than do predictions under the other methods, especially when ====. The CGP global predictor (dashed blue line) is nearly identical to the overall predictor (red line) over most of the input space; it is only for small ==== that the local adjustment is non-negligible. A visual comparison indicates that the CGP UQ limits are comparable to or wider than the other methods for small ====. For large ====, the CGP UQ limits are somewhat narrower than those for the other methods, but still indicate a large amount of uncertainty about the function in a region where it is essentially flat.==== by integrating over uncertainty in the unknown model parameters.====After the BCGP model is introduced in Section ====, Section ==== describes the computational algorithm we have developed for prediction and uncertainty quantification. Section ==== performs prediction and UQ for a collection of analytic and real-data examples that illustrate features of the BCGP overall predictor and the BCGP predictor of the global and local trends.",Prediction of non-stationary response functions using a Bayesian composite Gaussian process,https://www.sciencedirect.com/science/article/pii/S0167947320301742,1 September 2020,2020,Research Article,331.0
"Hébert Florian,Causeur David,Emily Mathieu","Agrocampus Ouest, CNRS, IRMAR - UMR 6625, F-35000 Rennes, France","Received 9 November 2018, Revised 14 July 2020, Accepted 21 August 2020, Available online 29 August 2020, Version of Record 7 September 2020.",https://doi.org/10.1016/j.csda.2020.107082,Cited by (1),"In global testing, where a large number of ==== than existing methods."," for fANOVA and ==== for GWAS issues) reflects the difficulty of identifying a method that would show a good detection performance in a wide scope of situations. As reported by ====, ====, ====However, the most popular whole-interval or whole-region testing methods, both in fANOVA and in GWAS, are based on simple aggregations of pointwise test statistics, not especially designed to be optimal under dependence. For example, ==== suggest using the maximum absolute pointwise test statistics, which turns out to be analogous to the famous ==== procedure, proposed by ==== to test for the relationship between genotypes of a given set of Single Nucleotide Polymorphisms (SNPs) and a case/control group membership in the context of GWAS. A functional F-type test statistic based on the squared L====–norm of the vector of pointwise test statistics is also introduced by ====, whereas similar weighted or unweighted L====-norm statistics are recommended by many authors (====, ====, ====) for GWAS issues.====The choice of an appropriate method to aggregate pointwise test statistics falls into the general context of global testing as defined by ====. The former paper focuses on the impact of the sparsity rate of the association signal on the choice between the L====-norm based test statistics of standard Analysis of Variance and the higher criticism (====) in a wide variety of correlation patterns. The former higher criticism (HC) test statistics can be viewed as a Kolmogorov–Smirnov type distance between the standardized empirical distribution of the pointwise p-values and the theoretical uniform null distribution. If the pointwise test statistics are assumed to be independent and in the so-called Rare-and-Weak paradigm, defined by conditions on the amplitude and sparsity rate of the signal, ==== show that HC reaches the optimal detection bounds obtained by ====.====It is commonly observed that, whatever the aggregation method, detection performance for a given association signal is affected by dependence across pointwise test statistics. A growing number of studies therefore suggest that signal detection procedures are improved by aggregating decorrelated pointwise test statistics, as for instance in ==== and ==== for HC and ==== introduce Correlation-Adjusted t-scores based on a James–Stein shrinkage estimate of correlations.====However, as discussed in ====, ==== in the GWAS context). Indeed, arguing that decorrelation may generate noise and weaken the signal, ==== introduce the generalized higher criticism (GHC) procedure in which aggregation is performed on the raw pointwise test statistics and the impact of dependence is accounted for by an ==== scaling of the HC statistic. Observing that the detection performance of the GHC procedure highly depends both on the pattern of dependence across pointwise statistics and on the sparsity rate of the signal, ==== propose to combine it with the maximum of the absolute pointwise test statistics and a weighted L====-norm statistic in an omnibus testing approach. While the former test shows good detection performance in the simulation setup proposed in ====, it raises limitations in terms of power and computational cost induced by the two-step Monte-Carlo calculation of the ====-value.====In Section ====Section ==== introduces a class of global test statistics defined as linear combinations of squared decorrelated pointwise tests. A specific choice of linear coefficients gives the sum of squared pointwise test statistics, which ignores dependence in aggregation, whereas another choice gives the Hotelling’s t-square test, which on the contrary accounts for dependence by introducing a preliminary whitening step. Other choices of linear coefficients leading to alternative dependence handling strategies, a procedure searching for an optimal choice of the linear coefficients is proposed. Finally, a comparative study of the proposed global testing procedure with a variety of alternative methods is conducted in Section ====, based on simulations under various assumptions on the dependence across explanatory variables and in the context of two genetic association studies.====The following is the Supplementary material related to this article. ",An adaptive decorrelation procedure for signal detection,https://www.sciencedirect.com/science/article/pii/S0167947320301730,29 August 2020,2020,Research Article,332.0
"Chen Tianbo,Sun Ying,Li Ta-Hsin","King Abdullah University of Science and Technology (KAUST), Statistics Program, Thuwal 23955-6900, Saudi Arabia,IBM Watson Research Center, NY, United States of America","Received 8 January 2019, Revised 10 August 2020, Accepted 12 August 2020, Available online 29 August 2020, Version of Record 9 September 2020.",https://doi.org/10.1016/j.csda.2020.107069,Cited by (9), (CNN) to classify smoothed quantile periodogram of earthquake data and achieve a higher accuracy than a similar classifier using ordinary periodograms.,", ====, ====, ====, ====, ====, and ====.====The quantile periodogram was inspired by the quantile regression (==== used the quantile periodogram to detect and estimate hidden periodicity from noisy observations when the noise distribution was asymmetric with a heavy tail on one side, while ordinary periodograms are less effective in handling such noise. ==== applied the quantile periodogram to Dow Jones Industrial Average and showed the advantages of quantile periodograms when handling time-dependent variance.====In traditional spectral analysis, the ordinary periodogram needs to be smoothed to serve as an estimator of the underlying ordinary (or power) spectrum. Many methods have been proposed to do so. One classical method is to apply a nonparametric smoother to the periodogram across frequencies. ==== discussed several periodogram smoothing techniques include moving-average smoothing. ====, ====. ==== and ==== used the Yule–Walker method to estimate the parameters.==== applied the ==== showed that the quantile spectrum can be estimated consistently by a window smoother of the quantile periodogram.====The ordinary spectrum is a critical feature of Gaussian time series, and it is often used to classify or cluster time series. For example, ==== trained a large, deep CNN to classify 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set. For accurate classification, a good estimation of the quantile spectrum is crucial. The quality of the estimator directly affects the classification performance.====Motivated by the function approximation capability of the autoregressive (AR) spectrum (====The rest of the paper is organized as follows. In Section ====, we introduce the quantile periodogram and the proposed estimation procedure. The simulation study is performed in Section ====, and the time series classification of earthquake waveforms using both ordinary and quantile periodograms is described in Section ====. We conclude and discuss the paper in Section ====.====The following is the Supplementary material related to this article. ====
 ",A semi-parametric estimation method for the quantile spectrum with an application to earthquake classification using convolutional neural network,https://www.sciencedirect.com/science/article/pii/S0167947320301602,29 August 2020,2020,Research Article,333.0
"Zilber Daniel,Katzfuss Matthias","Department of Statistics, Texas A&M University, USA","Received 7 January 2020, Revised 12 August 2020, Accepted 23 August 2020, Available online 28 August 2020, Version of Record 7 September 2020.",https://doi.org/10.1016/j.csda.2020.107081,Cited by (21),"Generalized ==== for GGPs is proposed, which combines a ","), but the same model has more recently also been referred to as a generalized GP (GGP) (e.g., ==== for a recent review of deterministic techniques and ==== for a comparison of MCMC and expectation propagation. ==== argue that variational methods and expectation propagation suffer from underestimated and overestimated posterior variances, respectively. Here, we consider the Laplace approximation (e.g., ====, ====), a classic technique for integral evaluation based on second-order Taylor expansion. ==== show numerically that the Laplace approximation can be a practical and accurate method for GGP inference.====). Low-rank approaches (e.g., ====, ====, ====, ====, ====) have limitations in the presence of fine-scale structure (====, ====, ====, ====, ====, ====) have gained popularity due to their accuracy and flexibility. In particular, a class of highly promising GP approximations (====, ====, ====, ====, ====, ====There are also a number of existing methods for large non-Gaussian datasets modeled using GGPs. A popular approach is to combine a low-rank GP with an approximation of the non-Gaussian likelihood, as the dimension reduction inherent in the low-rank approximation carries through even to the non-Gaussian case. ==== use variational inference to obtain the posterior and select a set of conditioning points for their low-rank approximation. Some methods of dimension reduction, including random sketching (e.g., ====) and projection, offer theoretical guarantees and can be combined with MCMC methods for the analysis of non-Gaussian data (e.g., ====, ====), but are still subject to the limitations of low-rank methods. ==== develop state–space models for one-dimensional non-Gaussian time series, which can perform inference in linear time and memory using a set of knots in time, a form of low-rank approximation. Alternate priors (e.g., log gamma priors for count data, ====Similar to what we shall propose, some authors have combined a sparse-precision approach with a non-Gaussian approximation. A prominent example is ==== proposed to apply the GP approximation of ==== to a latent GP, but did not provide an explicit algorithm for large non-Gaussian data. While both ==== and ==== matrix is not linear in ====, but rather ==== in two dimensions (==== Thm. 6), and at least ==== in higher dimensions. In the Gaussian setting, this scaling problem can be overcome by applying a Vecchia approximation to the observed data (====) or to the joint distribution of the observed data and the latent GP (====).====, ====). By modeling the joint distribution of pseudo-data and GP realizations at each iteration, our VL approach can ensure sparsity and guarantee linear scaling in ==== for any dimension, overcoming the scaling issues of the sparse-matrix approaches mentioned above.====To our knowledge, we provide the first explicit algorithm extending and applying the highly promising class of general-Vecchia GP approximations to large non-Gaussian data. We believe it to be a useful addition to the literature due to its speed, simplicity, guaranteed numerical performance, and wide applicability (e.g., binary, count, right-skewed, and point-pattern data). In addition, as shown in ====, the general Vecchia approximation includes many popular GP approximations (e.g., ====, ====, ====, ====, ====, ====, ====) as special cases, and so our VL methodology also directly provides extensions of these GP approximations to non-Gaussian data.====The remainder of this document is organized as follows. In Section ====, we review the Laplace approximation and general Vecchia. In Section ====, ====, we study and compare the performance of VL on simulated and real data, respectively. Some details are left to the appendix. A separate Supplementary Material document contains Sections S1–S6 with additional derivations, simulations, and discussion. The methods and algorithms proposed here are implemented in the ==== package ==== (====) with sensible default settings, so that a wide audience of practitioners can immediately use the code with little background knowledge. Our results and figures can be reproduced using the code and data at ====.====The desired Newton–Raphson update has the form ====As shown in Section ====, we have ==== and ====. Using an idea similar to iterative weighted least squares (Section 2.5, ====), we can premultiply the variable ==== by the Hessian to combine terms, and then rearrange and pull out the prior mean. Dropping the iteration subscript of ==== for ease of notation, we can write ==== as ==== where ====.====Now consider the posterior mean in the case of a Gaussian likelihood ==== with a conjugate Gaussian prior, ====. Employing a well-known formula, we have ====Thus, we have ====, the posterior mean under the assumption of Gaussian pseudo-data ====.",Vecchia–Laplace approximations of generalized Gaussian processes for big non-Gaussian spatial data,https://www.sciencedirect.com/science/article/pii/S0167947320301729,28 August 2020,2020,Research Article,334.0
"Feng Long,Zhao Ping,Ding Yanling,Liu Binghui","Key Laboratory for Applied Statistics of MOE and School of Mathematics and Statistics, Northeast Normal University, China,School of Science, Changchun Institute of Technology, China","Received 28 November 2019, Revised 24 April 2020, Accepted 16 August 2020, Available online 27 August 2020, Version of Record 4 September 2020.",https://doi.org/10.1016/j.csda.2020.107070,Cited by (2),"In the study of panel regression, current existing cross-sectional dependence tests are mainly based on the normal assumption. However, in practical applications, the normal assumption is usually not valid, which weakens the usability of the tests. To develop more testing tools suitable for nonnormal panel data, we extend the rank-based framework of U-statistics to panel regressions, and derive their asymptotic null distributions respectively as ====. The results of some simulation results and a real data analysis demonstrate the superiority of the proposed tests, especially their robustness to deviation from normality.",", ====, ====). On this ground, to deal with potential cross-sectional dependence, some researchers tend to employ a preliminary test for the existence of cross-sectional dependence and adjust the inference procedures according to the outcome of the pretest (====, ====).====There are a lot of studies on testing the cross-sectional dependence in the literature of spatial econometrics (see ====, ====). Originally, ==== and small cross-sectional dimension ====. When ==== is fixed and ====, the LM==== test is asymptotically Chi-square distributed with ==== degrees of freedom under the null hypothesis of no cross-correlation. However, as mentioned in ====, when ==== is large relative to ====, LM==== is severely oversized.====To amend the size distortions of LM====, ==== proposed a scaled version of LM====, written as CD====, which is asymptotically distributed as ====, under the null hypothesis of no cross-correlation, with ==== first, and then ====. However, for finite ==== increases, the incorrect centering is likely to be accentuated. To this end, ==== accordingly. The resulting bias-adjusted LM test, written as LM====, is asymptotically normal as ==== followed by ==== and small ====.====Using an alternative approach, ==== proposed a cross-sectional dependence test, written as CD, based on the residual Pearson correlation coefficients rather than their squares. It has correct empirical size in finite samples, even when ==== is much larger than ====, but has low power when the correlation coefficients roughly sum up to zero. This may occur when the errors are generated from a factor model, where the loadings average to zero. ==== analyzed the implicit null hypothesis of the CD test, which is given by weak dependence rather than by independence of errors between cross-sections. In addition, ==== proposed a ‘directed’ test, written as ====, based on the information matrix equality that is available for large ==== situation. On this ground, in this paper we will develop a series of rank-based procedures for testing the error cross-sectional dependence, which can be robust to deviation from normality.====Specifically, the proposed tests belong to the nonparametric testing framework that often leads to robustness to deviation from normality (see ====). A large number of high-dimensional nonparametric testing methods have been developed recently (see ====), which were designed to test mutual dependence between the entries of a high-dimensional random vector. The hypothesis test studied in this paper for the cross-sectional dependence can be viewed as an extension of the mutual dependence hypothesis test. To the best of our knowledge, no high-dimensional nonparametric testing method for cross-sectional dependence in panel data models has been proposed so far.====Our tests are established following the framework of U-statistics proposed by ====, which covers a variety of well-known rank correlations, including Kendall’s tau proposed by ====, a dominating term of Spearman’s rank correlation coefficient (rho) proposed by ==== and the t==== correlation proposed by ====. By resampling, we get a lot of subdatasets from the original stock dataset, based on which we can conduct multiple tests separately and then sum up all the results. These results suggest that the proposed methods are much more competitive in testing the cross-sectional dependence between the stock returns. This is probably due to the observation that the error distributions of the stock returns seriously deviate from normal distribution.====In summary, in this paper, to deal with potential cross-sectional dependence of panel data, we propose using some rank-based tests for cross-sectional dependence before performing regular inference procedures directly on data. Compared with the widely used cross-sectional dependence tests based on normal distribution theory, the proposed tests have prominent power advantages. One potential disadvantage of these tests is that they need to pay more computing cost than many existing cross-sectional dependence tests based on normal distribution theory, as the statistics are more complex. This extra computational cost is usually not necessary when processing normally distributed data, because in such situation the existing cross-sectional dependence tests based on normal distribution theory often have good enough power performance, and the proposed tests cannot get much more power gain. Hence, the most suitable case of applying the proposed tests to is the high dimensional panel data with non-normally distributed errors.====The rest of the paper is organized as follows. We introduce the methodology of the proposed cross-sectional dependence tests in heterogeneous panel data models in Section ====, including some recent developments, the proposed methods and their asymptotic distributions under the null hypothesis. The simulation results of the proposed tests in comparison with some existing methods are demonstrated in Section ====, followed by a real data analysis in Section ====. Then, we reach a conclusion of this paper in Section ==== and relegate the technical proof to the ====.",Rank-based tests of cross-sectional dependence in panel data models,https://www.sciencedirect.com/science/article/pii/S0167947320301614,27 August 2020,2020,Research Article,335.0
"Zhang Haixiang,Wang HaiYing","Center for Applied Mathematics, Tianjin University, Tianjin 300072, China,Department of Statistics, University of Connecticut, Storrs, Mansfield, CT 06269, USA","Received 10 January 2020, Revised 12 August 2020, Accepted 14 August 2020, Available online 27 August 2020, Version of Record 2 September 2020.",https://doi.org/10.1016/j.csda.2020.107072,Cited by (13),"With the development of modern technologies, it is possible to gather an extraordinarily large number of observations. Due to the storage or transmission burden, big data are usually scattered at multiple locations. It is difficult to transfer all of data to the central server for analysis. A distributed subdata selection method for big data linear regression model is proposed. Particularly, a two-step ==== strategy with optimal subsampling ==== of the proposed estimator are established. Simulation studies and an illustrative example about airline data are provided to assess the performance of the proposed method."," presented a communication-efficient surrogate likelihood framework for distributed statistical inference problems. ==== studied the divide and conquer method for cubic-rate estimators under massive data framework. ==== proposed an online updating method that could incorporate new variables for big data streams. ==== proposed an online updating approach for testing the proportional hazards assumption with big survival data.====Another popular method is the subsampling approach, where the basic idea is to select a subsample for the purpose of statistical inference. ==== proposed an algorithmic leveraging-based sampling procedure. ==== and ==== provided a novel information-based optimal subdata selection approach. ====The remainder of this article is organized as follows: In Section ====, we provide an optimal sampling criterion with the focus on developing subsampling probabilities and allocation sizes, which minimize the asymptotic mean squared error of the resultant estimator. Then we develop a two-step algorithm to approximate the optimal subsampling procedure. In Section ====, we conduct an extensive simulation study to verify the effectiveness of our method. An application to airline data is presented in Section ====. In Section ====, we provide concluding remarks and future research topics. All proof details are given in ====.====Below, we give the proofs for ==== ====
 ====. Note that from Cauchy–Schwarz inequality Condition (C.2) implies that ====, and from Hölder’s inequality, Condition (C.3) implies that ==== and ====. We first establish the following lemma.",Distributed subdata selection for big data via sampling-based approach,https://www.sciencedirect.com/science/article/pii/S0167947320301638,27 August 2020,2020,Research Article,336.0
"Fan Xinyan,Zhang Qingzhao,Ma Shuangge,Fang Kuangnan","School of Statistics, Renmin University of China, China,School of Economics, Xiamen University, China,The Wang Yanan Institute for Studies in Economics, Xiamen University, China,Department of Biostatistics, Yale University, United States of America","Received 17 February 2020, Revised 2 August 2020, Accepted 7 August 2020, Available online 21 August 2020, Version of Record 28 August 2020.",https://doi.org/10.1016/j.csda.2020.107066,Cited by (0),"Network construction has been heavily exploited in ====. In many cases, connections between a large portion of variables are of minimal importance. As such, partial graphs have played an important role in network construction. Due to the existence of a multiplicative normalization constant, the existing construction approaches may bear high computational cost. To reduce the ====, the conditional score matching for high-dimensional partial ==== is proposed. This approach is uniquely advantageous by being not influenced by the multiplicative normalization constant. An effective computational algorithm is developed, and it is shown that the computational complexity of the proposed method is less than that of those in the literature. Statistical properties are established, and two extensions are explored to incorporate more information and accommodate more general distributions. A wide spectrum of simulations and the analysis of a breast cancer gene expression dataset demonstrate competitive performance of the proposed methods.",", ====, ====.====In many applications, connections between a large portion of nodes in a network are of minimal importance. For example, in genetic studies, data may be collected on gene expressions and their regulators for the same subjects. In this case, we can be mainly interested in modeling connections between gene expressions and between gene expressions and their regulators, but with no interest in connections between regulators. Mathematically, suppose that two random vectors ==== and ==== are jointly normally distributed with mean zero and covariance matrix ====. The inverse covariance matrix can be partitioned into ====Here we are interested in ==== and ==== but not ==== and ====. The first strategy estimates the full ====. A representative approach is graphical Lasso, of which the objective function is the penalized negative log-likelihood function, ==== where ==== is the sample covariance of ==== and ==== is a tuning parameter. The second strategy estimates ==== and ==== via the penalized log-likelihood of ==== given ==== (====, ====, ====, ====). The objective function is ==== where ====, ==== and ==== and ==== for graphical Lasso and ==== for the second strategy), the computational complexities of graphical Lasso and the second strategy are ==== and ==== (====), respectively. Moreover, some related studies (====, ====) estimate ==== without estimating ====.====To eliminate the influence of the multiplicative normalization constant, Hyvärinen (====) proposes a divergence between two distributions, called the score matching loss. ==== and ====, which is less than that of the alternative methods when ====. Second, statistical properties of our approach are rigorously established whereas some existing studies have focused on methodological development only. Specifically, the elementwise ==== bound between the estimates and true parameters is provided. Under mild conditions, the estimates are proved to recover the correct signs of the true parameters. Last but not least, we further make two extensions to expand the applicability of the proposed method. The first is that we jointly analyze multiple datasets to incorporate more information and propose the integrative conditional score matching model (iCSM). In addition, we adapt our model to accommodate more general distributions and propose the rank-based conditional score matching model (rCSM).====The rest of this article is organized as follows: The CSM approach is developed in Section ====, where the computational algorithm and statistical properties are established. Two extensions are developed in Section ====. Concluding remarks are presented in Section ====, and additional technical details are provided in the ====.====This Appendix includes three parts: Appendix A introduces the derivation of Algorithm 1; Appendix B provides the proof of ====; and Appendix C provides the algorithm of iCSM.",Conditional score matching for high-dimensional partial graphical models,https://www.sciencedirect.com/science/article/pii/S0167947320301572,21 August 2020,2020,Research Article,337.0
"Baek Changryong,Gates Katheleen M.,Leinwand Benjamin,Pipiras Vladas","Department of Statistics, Sungkyunkwan University, 25-2, Sungkyunkwan-ro, Jongno-gu, Seoul, 03063, Republic of Korea,Department of Psychology and Neuroscience, UNC at Chapel Hill, CB#3270, Davie Hall, Chapel Hill, NC 27599, USA,Department of Statistics and Operations Research, UNC at Chapel Hill, CB#3260, Hanes Hall, Chapel Hill, NC 27599, USA","Received 20 April 2020, Revised 29 July 2020, Accepted 11 August 2020, Available online 20 August 2020, Version of Record 2 September 2020.",https://doi.org/10.1016/j.csda.2020.107067,Cited by (3),The problem of testing for the equality of ,", ====, with a large number ==== of univariate component series ====, referred to as high-dimensional time series (HDTS) data, have been collected and studied in a number of modern applications. For example, HDTS are prevalent in fMRI studies where a component series represents a BOLD signal at a particular brain location of an individual (e.g. ====). HDTS have been drawing ever greater attention in Economics and Finance, where individual series could represent stocks or other assets, a range of macroeconomic indicators, and so on (e.g. ==== and ====). Other application areas include Environmental Sciences (e.g. ====, ==== and ====), Business (e.g. ====), Genetics (e.g. ==== and ====), etc. Analysis of HDTS does not preclude the situation where the observations ==== are independent across time, and thus should build upon and extend the approaches available in the independent setting.====Assuming that two independent HDTS ==== and ====, ====A number of approaches to testing the equality of ACVFs of ==== and ==== are considered in this work. On one hand, note that ACVF at a lag is just a suitable mean of the product of the demeaned series and its demeaned, lag-shifted copy. The problem of testing for the equality of ACVFs at that lag is then a special case of testing for the equality of high-dimensional means, placed in the context of time series data. The latter problem has been studied by a number of authors, including ====, ====, ==== and ====We shall examine and discuss the suggested methods in the context of several canonical models of HDTS, to which we shall refer as sparse, factor and combined. Sparse models have ACVFs with a large number of zero entries. The opposite is the case for factor models, which can thus also be thought of as dense models. Combined models possess features of both sparse and factor models. A similar distinction among the models of ==== is made in related literature. It is also expected that some of the proposed methods will work better for one type of model or another (e.g. the considered sup-tests seem more appropriate for sparse models), and is indeed the case in our context, as will be documented through simulations below.====We apply the proposed methods to an fMRI data set. The data set concerns individuals in induced emotional states (anxiety, anger), as well as the rest state. The basic question is whether there are differences in ACVFs (or ACFs) across these states for a particular individual and across the populations of individuals. We also note that in ACVF testing, the dimension of the series involved is of the order ====, where ==== is the dimension of the original HDTS, since even just considering the covariance (the ACVF at lag 0), there are ==== cross product series. In the application study, in particular, we work with ====, for which ==== is fairly large.====). For lower-dimensional vector time series, this problem has been studied in ====, ====, ====, ====, ====). The focus here is on (global) testing rather than estimation.====The rest of the paper is structured as follows: In Section ====, we start by describing the models of interest. In Section ====, we gather, discuss and compare the various approaches to testing the equality of ACVFs of two independent HDTS. Testing across two populations is also considered in that section. Section ==== contains a simulation study and Section ==== includes an application. Section ==== concludes the paper. More technical proofs and discussion are moved to ====, ====, ====.====In this section, we specify the exact forms of the models (sparse and factor) that are used in the simulations in Section ====. In all cases, we assume for simplicity that the series has zero mean.==== A HDTS model with a sparse ACVF can be introduced in a number of ways. We work with the following model. Suppose a ==== series ==== follows a vector moving average (VMA) model of order ==== as in ====where ==== are i.i.d. ==== and ==== are ==== matrices (and should not be confused with ==== in ====). To have both a sparse ACVF and temporal dependence, we shall take ====for real-valued ====’s (and such that the corresponding univariate MA model is invertible) and a sparse covariance matrix ====. For the latter, we construct ==== as ====where ==== is ====, ==== and the second choice is made in ==== for ==== different pairs ====, ====, with ==== viewed as a sparsity parameter.====Note that, for ====, ====with a matrix ==== having zero entries, except ==== pairs ==== for which the second choice of ==== was made, in which case ====. Both matrices ==== and ==== are sparse. Furthermore, the weights ==== are generated as ====A suitable choice of ==== and ==== can give larger or smaller correlations. In our simulations, we take ====, ====.====The ACVF of the VMA model ==== with ====–==== is given by ====with ====.==== Here, we work with a ==== series ==== defined as ====where ==== is a ==== matrix of loadings, ==== are i.i.d. ==== vectors that are independent of ====, and ==== follows a vector autoregressive (VAR) model of order ====, ====where ==== are i.i.d. ====. Further choices for ====, ====, ==== and ==== will be made below.====For the model ====–====, its ACVF ==== is given by ====where ==== is the ACVF of the factor process. When ====, the relation ==== connects the covariance matrices of ==== and ==== as ====For example, a special case of the models ====–==== that was considered in our simulations takes an even ====, ====, ====, ====where ====, ====, ====where ==== and ====. Furthermore, ==== is taken as ====In the simulations in Section ====, we take ====, ====. By ====, note that ====Then, by ====, ====, ====, the covariance matrix of ==== can be written as ====The matrix ==== will be selected as a diagonal matrix to make ==== a correlation matrix, that is, in view of the preceding relation, as ====A few words on the interpretation of the model with ====–====. Because of the loading structure ====, one thinks of the subsets of component series ==== and ==== as two “communities”, since they load through ==== the factor series ==== and ====, respectively. By choosing properly ==== and ==== in ====, the loadings can be made stronger or weaker. The choice of ==== is made to ensure that the model has temporal dependence (stronger as ==== increases) and has its covariance given in ====. The parameter ==== can be thought as that of overlap in the following sense. When ====, the matrix ==== with ==== is block-diagonal, with the covariance matrices of the two blocks corresponding to the two communities. But when ====, there is some “overlap” in dependence between the communities through the off-diagonal blocks. Note also that ==== explains the term “dense” model, since both ==== and ==== are not sparse in our construction.",Two sample tests for high-dimensional autocovariances,https://www.sciencedirect.com/science/article/pii/S0167947320301584,20 August 2020,2020,Research Article,338.0
"Zeng Yaohui,Yang Tianbao,Breheny Patrick","Department of Biostatistics, University of Iowa, United States of America,Department of Computer Science, University of Iowa, United States of America","Received 12 December 2019, Revised 3 August 2020, Accepted 4 August 2020, Available online 19 August 2020, Version of Record 4 September 2020.",https://doi.org/10.1016/j.csda.2020.107063,Cited by (8),The lasso model has been widely used for model selection in ,"where ==== is the ==== response vector, ==== is the ==== feature matrix, ==== is the coefficient vector, and ==== and ==== to denote the Euclidean (====) norm and ==== norm, respectively.====, ====, ====, ====). Efficiently solving the lasso model is therefore of great significance to statistical and machine learning practice.====Over the past years a number of efficient algorithms have been developed for solving the lasso (====, ====, ====, ====, ====, ====, ====, ====). With the evolving era of Big Data, however, it is increasingly common to encounter large-scale, ultrahigh-dimensional data sets. The increased number of features and observations in these data sets present added challenges to solving the lasso efficiently.====One idea for reducing computation time is to drop certain features from the analysis prior to fitting the lasso. As a result, the dimensionality of the feature matrix – and hence the computational burden of the optimization – will be substantially reduced. This idea, known as ====, has been around for a long time, but was first studied formally by ====.==== of the lasso (formally defined in Section ====) within a compact region ====. Then given a feature ====, its coefficient estimate ==== is guaranteed to be 0 if ==== (====). The pioneering work in this direction is the SAFE rule developed by ====. The smaller the region ====, the more features will be discarded and more efficiency gained; this has motivated other more powerful rules such as the EDPP, Dome, and Sphere tests, all of which shrink ==== according to different strategies (====, ====, ====, ====).====A separate line of research has sought to develop “strong” rules that are more powerful at discarding features than safe rules and for which violations are unlikely, but possible. This idea was initially proposed by ====, who developed ==== (SSR) based upon the Karush–Kuhn–Tucker (KKT) conditions for the lasso problem along with an assumption of “unit-slope” bound. The main idea is that we are still solving the original optimization problem, but we can skip certain calculations that are likely to be unnecessary, thereby reducing computational burden. However, because it is possible for these rules to incorrectly discard active features, a post-convergence KKT checking step is required in order to guarantee the correctness of the solution.====In this paper, we propose combining safe and strong rules, yielding ==== (HSSR) for discarding features in lasso-type problems. The key of HSSR is to incorporate ====Although this idea is relatively simple, we consider it to be novel for two primary reasons. First, the existing literature is firmly divided and for the most part published in entirely different types of journals: most of the research on safe rules has appeared in machine learning and computer science journals, while the research on strong rules has appeared in statistics journals. Most of what has been written gives the impression that these are two irreconcilable and mutually exclusive approaches to improving efficiency. We show here that this is not the case – the two types of rules can be combined in a relatively straightforward manner. Second, the degree of efficiency gained by combining these rules is rather surprising, at least to us. In many cases, the hybrid rules are more than the sum of their parts, providing much greater gains in efficiency when combined than using either type of rule alone.====The main contributions of this research include:====In this paper we assume without loss of generality that the response vector ==== is centered so that the intercept term is dropped from the lasso model. We further assume the feature vectors ==== are centered and standardized to have unit variance: ====for ====.====Standardization is a typical ==== in fitting lasso models since: (1) it ensures that the penalty is applied uniformly across features with different scales of measurement; (2) it often contributes to faster convergence of the optimization algorithm; (3) as we will see in following sections, it simplifies feature screening rules and thus reduces computation complexity.====The rest of the paper is organized as follows. Section ==== reviews the two categories, strong rules and safe rules, upon which our work is built. We propose our new hybrid screening strategy in Section ====, we extend SSR-BEDPP to the elastic net and group lasso problems. Section ==== compares the performance of our rules with existing ones via extensive numerical experiments on synthetic and real data sets for both the standard lasso and the group lasso problems and conclude with some final remarks in Section ====. Proofs of theorems are given in the ====.",Hybrid safe–strong rules for efficient optimization in lasso-type problems,https://www.sciencedirect.com/science/article/pii/S0167947320301547,19 August 2020,2020,Research Article,339.0
"Machado Robson J.M.,van den Hout Ardo,Marra Giampiero","Department of Statistical Science, University College London, Gower Street, London WC1E 6BT, UK","Received 17 October 2019, Revised 3 July 2020, Accepted 1 August 2020, Available online 19 August 2020, Version of Record 26 August 2020.",https://doi.org/10.1016/j.csda.2020.107057,Cited by (4),"Continuous-time multi-state Markov models can be used to describe transitions over time across health states. Given longitudinal interval-censored data on transitions between states, ",", ====). For a wide range of applications, however, the risks of moving across states depend on the current state and on time. In this case, a non-homogeneous Markov assumption is assumed to model the multi-state process.====, ==== can be used to estimate parameters. This estimation includes a smoothing (or penalty) parameter that balances smoothness of the fitted hazard across the whole time range against fidelity to the data.====A penalised maximum likelihood estimation for a progressive three-state model is developed in ====. Estimation is performed with an algorithm which uses analytical derivatives of the penalised log-likelihood. The smoothing parameters are selected using a grid search with cross-validation. In this case, models have to be fitted for every combination of smoothing parameters defined by the grid. ====). ====-spline basis functions placed equidistantly. However, the log-likelihood is maximised without penalisation. ==== proposed a penalised likelihood method to estimate semiparametric multi-state models with splines. The smoothing parameters are selected by using grid search. Even though the method is general and allows for backward transitions, it can become burdensome for applications that involve multiple penalties. Therefore, the methods available in the literature cannot fully address the problem of estimating multi-state models with splines for interval-censored data as they are not feasible for many applications.====In the presence of interval censoring, specific methods are needed to fit time-dependent multi-state models. For progressive processes with a limited number of states, unknown transition times can be integrated out; see, for example, ====, ====, and ====. For more complex processes, particularly those with backward transitions, a piecewise-constant approximation to the time dependency can be adopted; see, for example, ====, ====, and ====. As mentioned above, ==== is an exception, as he fits time-dependent models to interval-censored data using direct numerical solution to the Kolmogorov Forward differential equations.====In this paper, we propose a new efficient method to estimate multi-state models with splines for interval-censored data. A Markov process framework is used to formulate the models. Hazards are specified with splines to allow for flexible modelling over time. Estimation is undertaken using a penalised likelihood approach. Given a piecewise-constant approximation to the hazards, the Fisher scoring algorithm presented in ====). The fitted multi-state model with splines can be used for flexible modelling of time dependency, but also to check parametric specifications.====Section ==== introduces the data on cardiac allograft vasculopathy (CAV). In Section ====, the hazard models with splines are defined and the likelihood function is derived. Section ==== comprises the main methodological work; it defines the penalised likelihood function and discusses how the smoothing parameters are estimated along with the model parameters. A simulation study in Section ==== shows that the proposed method works and also illustrates some effects of interval censoring. Section ==== presents the main application which is an analysis of the CAV data, and Section ==== briefly presents an additional analysis of a five-state process. Section ==== is the concluding discussion. Two appendices provide technical details additional to Section ====.====For easy reference, we derive the parametrisation of the model-parameter estimators as in ====. A first-order Taylor expansion of ==== about the current fit ==== is given by ====where ==== and ====. Let us define ====. A new fit ==== is obtained by taking the right-hand side of Eq. ==== to be zero ==== Therefore, the new fit for the parameter estimator can be expressed as ====where ==== with ====.",Penalised maximum likelihood estimation in multi-state models for interval-censored data,https://www.sciencedirect.com/science/article/pii/S0167947320301481,19 August 2020,2020,Research Article,340.0
"Ito Tsubasa,Sugasawa Shonosuke","Center for Spatial Information Science, The University of Tokyo, 5-1-5 Kashiwanoha, Kashiwa, Chiba, 277-8568, Japan,M&D Data Science Center, Tokyo Medical and Dental University, Japan","Received 20 March 2020, Revised 6 August 2020, Accepted 11 August 2020, Available online 19 August 2020, Version of Record 25 August 2020.",https://doi.org/10.1016/j.csda.2020.107068,Cited by (0)," to compute the region, thereby the proposed method can be easily carried out in practical applications. The effectiveness of the proposed method is demonstrated through simulation studies and an application to meta-analysis of screening test accuracy for alcohol problems.","). In this meta-analysis, the summary statistics in each study are two primary correlated outcomes of diagnostic, sensitivity and false positive rate (====), and we are typically interested in summary receiver operating characteristic curve. Moreover, DTA from different sources for studies are generally heterogeneous due to various factors, which should be adequately addressed to avoid underestimation of statistical errors and misleading conclusions (====, ====).====In the bivariate random-effects meta-analyses, standard inference methods depend on large sample approximations for the number of studies synthesized, for example the extended DerSimonian–Laird methods (====, ====, ====) and restricted maximum likelihood (REML) estimation (====, ====). Recently, several refined methods have been proposed to improve confidence intervals in multivariate meta-analysis. For example, ==== developed improved confidence intervals in network meta-analysis using Bartlett-type corrections, and ==== and ==== and ==== considered higher order likelihood inference in the univariate meta-analysis, which cannot be directly applicable to more complicated multivariate meta-analysis. As more general approaches, ====, and demonstrate that the proposed method shows quite reasonable empirical coverage than REML while the computational cost in both methods are almost identical. We also demonstrate the proposed method through an application to meta-analysis of screening test accuracy for alcohol problems.====This paper is set out as follows. In Section ====, we describe the proposed confidence region under bivariate random-effects models. In Section ====, we numerically demonstrate the proposed confidence region together with existing methods through extensive simulation studies and an application with real dataset. We conclude with a short discussion in Section ====. R code implementing the proposed method is available at GitHub repository (====).====The following is the Supplementary material related to this article. ",Improved confidence regions in meta-analysis of diagnostic test accuracy,https://www.sciencedirect.com/science/article/pii/S0167947320301596,19 August 2020,2020,Research Article,341.0
"Ju Xiaomeng,Salibián-Barrera Matías","Department of Statistics, University of British Columbia, Canada","Received 22 January 2020, Revised 7 August 2020, Accepted 9 August 2020, Available online 18 August 2020, Version of Record 2 September 2020.",https://doi.org/10.1016/j.csda.2020.107065,Cited by (9),"Gradient boosting algorithms construct a regression predictor using a ==== show that, when no atypical observations are present, the robust boosting approach works as well as the standard gradient boosting with a squared loss. Furthermore, when the data contain outliers, the robust boosting estimator outperforms the alternatives in terms of prediction error and variable selection accuracy."," and predictions for future observations, even when the training data set may contain atypical observations. These “outliers” need not be “extreme” or aberrant values, but might simply be points following a different model from the one that applies to the majority of the data. It is well known that classical methods can provide seriously misleading results when trained on such heterogeneous or contaminated data sets, and thus obtaining robust alternatives is of immediate practical importance.====. Fewer options are available for robust non-parametric regression methods, and generally they have not been developed as thoroughly as the linear ones. Among them we can mention: robust locally weighted regression (====, ====), local M-estimators (====), and the proposals in ====, ====, and ====, ====. However, selecting appropriate bandwidths for each of the components of an additive model can be computationally prohibitive when a moderate or large number of explanatory variables are involved.====Most robust boosting algorithms in the literature were designed for classification problems (====, ====, ====, ====, ====). Previous proposals to robustify regression boosting methods replaced the squared loss with the absolute value function, or with Huber’s loss.  ==== introduced several ways to robustify boosting, primarily focusing on linear regression. One of their methods (==== proposed variations of random forests that use the median to aggregate trees and make the tree splits. ==== established a connection between random forests and locally weighted regression. They suggested a forest-type regression framework which includes ====’s (====) quantile random forest as a special case. It is also applicable to other loss functions, such as Huber’s or Tukey’s.====Our main concern with these proposals is that they use robust loss functions that either may yield estimators with low efficiency (e.g. the ====, ====). It is easy to see that this changing scale estimator may not work well, since the scale can be overestimated in early iterations and this might result in observations with large residuals not being considered outlying. In fact, our numerical experiments confirm that this is the case even in relatively simple settings. To address this problem, we propose a robust boosting method that directly minimizes a robust scale of the residuals, and hence does not need to compute an auxiliary residual scale estimator. Although in principle our approach can be used with any scale estimator, gradient boosting uses the partial derivatives of the objective function. Hence, in this paper we focus on minimizing an M-estimator of scale, for which we can compute the corresponding gradient. This approach can be seen as an extension of S-estimators (====) to this class of non-parametric regression estimators. Moreover, this robust boosting estimator can be used (along with its associated robust residual scale estimator) as the initial fit for a boosting M-type estimator using Huber’s or Tukey’s loss. As in the case of linear models, this second stage is expected to result in a robust estimator with higher efficiency (less variability) than the S-estimator.====The remainder of this paper is organized as follows: Section ==== reports the results of our simulation studies comparing our method with previously proposed robust boosting algorithms, while results obtained on benchmark data are discussed in Section ====. Finally, Section ==== summarizes our findings and conclusions.====The following is the Supplementary material related to this article. ",Robust boosting for regression problems,https://www.sciencedirect.com/science/article/pii/S0167947320301560,18 August 2020,2020,Research Article,342.0
"Bagkavos Dimitrios,Ioannides Dimitrios","Department of Mathematics, University of Ioannina, 45110, Greece,Department of Economics, University of Macedonia, 54006, Greece","Received 8 October 2019, Revised 5 August 2020, Accepted 6 August 2020, Available online 17 August 2020, Version of Record 25 August 2020.",https://doi.org/10.1016/j.csda.2020.107064,Cited by (2)," optimal bandwidth rule for integrated derivative products. The ==== of all methodological contributions are quantified analytically and discussed in detail. Three real ==== illustrate the benefits of the proposed methodology in practice. Finally, numerical evidence is provided on the finite sample performance of the proposed technique with reference to benchmark estimates.","Let ==== denote a continuous lifetime variable with cumulative distribution function (c.d.f.) ==== and ====. This approach enables the development, also in Section ====, of local polynomial estimates for integrated c.d.f. derivative products of any arbitrary order. These are useful on their own right since they are necessary for the implementation of automatic bandwidth selectors, in estimation of population characteristics, statistical distance measures and in a variety of other settings.====, quantified analytically in Section ====, ensure efficient estimation of the functionals as opposed to using conventional kernel smoothers. A subsequent advantage thus results by their utilization in developing (in Section ====) a solve-the-equation AMISE-optimal plug-in bandwidth rule applicable to all estimates proposed here. The rule is built as a direct extension of the corresponding density estimation bandwidth selector for complete data proposed in ====Section ====First recall that for ====, by ====, ",Fixed design local polynomial smoothing and bandwidth selection for right censored data,https://www.sciencedirect.com/science/article/pii/S0167947320301559,17 August 2020,2020,Research Article,343.0
"Bak Kwan-Young,Jhong Jae-Hwan,Lee JungJun,Shin Jae-Kyung,Koo Ja-Yong","Department of Statistics, Korea University, Seoul 02841, Republic of Korea,Department of Information Statistics, Chungbuk National University, Chungcheongbuk-do 28644, Republic of Korea,LG Display AI/BigData Analysis Team, 245, LG-ro, Wollong-myeon, Paju-si, Gyeonggi-do, 10845, Republic of Korea","Received 6 August 2019, Revised 1 August 2020, Accepted 1 August 2020, Available online 15 August 2020, Version of Record 16 September 2020.",https://doi.org/10.1016/j.csda.2020.107060,Cited by (1),We study a penalized logspline density estimation method using a total variation penalty. The B-spline basis is adopted to approximate the logarithm of density functions. Total variation of derivatives of splines is penalized to impart a data-driven knot selection. The proposed estimator is a bona fide density function in the sense that it is positive and integrates to one. We devise an efficient coordinate descent algorithm for implementation and study its ==== sense. We also propose a logspline method for the ,"We consider a problem of nonparametric density estimation based on a random sample. Let ====. Suppose that ==== is a random sample from the distribution of ====. The goal is to estimate ==== based on ====. To this end, we develop a penalized logspline density estimation method, approximating the logarithm of ==== using the B-spline basis and regularizing the log-likelihood function with a total variation penalty.==== and well studied in both, computational and theoretical aspects, by ====, ====, ====, ====, ==== and ==== and ====, ==== and ====, ====). For other studies on the log-density method, see ==== and references therein.====, ====, ====, ====, ====). However, few studies have focused on data-driven knot selection via penalizations under the logspline density estimation framework.====Total variation penalties frequently appear in density estimation studies. ==== adopt the total variation of the first derivative of the logarithm of density as a penalty. ====, ==== and ====. While these studies adopt the total variation of densities or their derivatives, in this paper, we utilize the total variation of the derivatives of the spline function. Our approach has an advantage that we do not need to pre-specify the form of the true density function ==== such as constant or linear as in ==== to compute the total variation penalty and estimates.====Theoretical studies on log-density estimation have been undertaken by several researchers. ==== and ==== norms. ====Even though theoretical aspects of ==== penalization have extensively been studied in the regression set-up (e.g. ====, ====, ====, ====, ====), relatively less attention has been drawn to penalized density estimation. Among them, ==== obtain estimators for densities by minimizing a ==== provide similar oracle inequalities for estimates obtained from ==== penalty under an adaptive Danzig constraint. In terms of total variation penalties, some theoretical properties of regression spline estimators have been developed by ==== and ====. However, few studies have been undertaken to provide theoretical justification of density estimation with total variation penalty.====The rest of this paper is organized as follows. Section ====. In Section ====, we analyze the convergence property of the coordinate descent algorithm. Theoretical results consisting of the oracle inequality and rate of convergence are presented in Section ====. Section ====, ====.==== states that ==== lies in the same region generated by ==== as ==== does.====We now give proofs of ==== and ====.",Penalized logspline density estimation using total variation penalty,https://www.sciencedirect.com/science/article/pii/S0167947320301511,15 August 2020,2020,Research Article,344.0
"Chau Thi Tuyet Trang,Ailliot Pierre,Monbet Valérie","IRMAR-INRIA, University of Rennes, Rennes, France,Univ Brest, CNRS, LMBA - UMR 6205, Brest, France","Received 19 September 2018, Revised 7 June 2020, Accepted 4 August 2020, Available online 13 August 2020, Version of Record 2 September 2020.",https://doi.org/10.1016/j.csda.2020.107062,Cited by (5),"State–space models are ubiquitous in the statistical literature since they provide a flexible and interpretable framework for analyzing many time series. In most practical applications, the state–space model is specified through a ==== for parametric state–space models. It is proposed to combine two of these techniques, namely the Stochastic Expectation–Maximization (SEM) algorithm and ==== (SMC) approaches, for non-parametric estimation in state–space models. The performance of the proposed algorithm is assessed though simulations on toy models and an application to environmental data is discussed."," provide a natural framework to study time series with observational noise in environment, economy, computer sciences, etc. They have a wide range of applications in data assimilation, system identification, model control, change detection, missing-data imputation (see e.g. ====). The general SSM which is considered in this paper is defined through the following equations, ==== describes the time evolution of the latent process ====. The operator ==== links the latent state to the observations ====. The random sequences ==== and ==== model respectively the random components in the dynamical model and the observational error. Throughout this paper, we make the classical assumptions that ==== is known (typically ====) and that ==== and ==== and ==== where ====In this paper, we are interested in situations where the dynamical model ====.====Such ==== were originally introduced in ==== and ==== for data assimilation in oceanography and meteorology. In these application fields, a huge amount of historical data sets recorded using remote and in situ sensors or obtained through numerical simulations are now available and this promotes the development of data-driven approaches. It was proposed to build a non-parametric estimate ==== of ==== using the available observations and plug this non-parametric estimate into usual filtering and smoothing algorithms to reconstruct the latent space ==== given observations ====. Numerical experiments on toy models show that replacing ==== by ==== leads to similar results if the sample size used to estimate ==== is large enough to ensure that ==== is “close enough” to ====. Some applications to real data are discussed in ====.====). In ====, better results were obtained with a slightly more sophisticated estimator known as local linear regression (LLR) in statistics (====) and constructed analogs in meteorology (====) or sparse regression (see ====In the above mentioned references, it is generally assumed that a sequence ==== of “perfect” observations with no observational error is available to estimate the dynamical model ====. However, in practical applications, only a sequence ==== of the process ==== with observational errors is given to fit the model. The main contribution of this paper is to propose a method to build non-parametric estimate of ==== in this context. A simple approach would consist in “forgetting” the observational errors and computing directly a non-parametric estimate based on the sequence ==== but this may lead to biased estimates. This is illustrated in ==== obtained with the toy SSM defined as ====with ====. The left plot shows the scatter plot ==== for a simulated sequence ==== of the latent process ==== and the corresponding non–parametric estimate ==== based on this sample which is reasonably close to ====. The right plot shows the scatter plot ==== of the corresponding sequence with observation noise. Note that ==== is obtained by adding a random noise to ==== and this has the effect of blurring the scatter plot by moving the points both horizontally and vertically. It leads to a biased estimate of ====). One of the classical approaches to reduce the bias is to introduce instrumental variables which help to get information about the observational error. This approach has been adapted to linear first order auto-regressive models in ==== and further studied in ====. ====This paper discusses an extension of the algorithms proposed in ==== to non–parametric SSMs where the non-parametric estimate of ==== is updated at each iteration of the EM recursions using the trajectories simulated from the smoothing algorithms. It permits to correct sequentially the bias in the estimate of ==== for time series.====The paper is organized into the following sections. The estimation of the parametric component using EM recursions is introduced in Section ====. Then this algorithm is extended to estimate both the parametric and non-parametric components in Section ====. Simulation results obtained on a toy model (Lorenz-63) are presented in Section ====. Then, Section ==== discusses an application to oceanographic data, where the algorithms are used to impute missing wave data given noisy observations. The paper ends with some concluding remarks in Section ====. All the codes of the proposed approach used for numerical experiments in this paper are available on ====.",An algorithm for non-parametric estimation in state–space models,https://www.sciencedirect.com/science/article/pii/S0167947320301535,13 August 2020,2020,Research Article,345.0
"Liu Tianqing,Yuan Xiaohui,Sun Jianguo","Center for Applied Statistical Research and School of Mathematics, Jilin University, Changchun 130012, China,School of Mathematics and Statistics, Changchun University of Technology, Changchun, Jilin 130012, China,Department of Statistics, University of Missouri, USA","Received 25 April 2019, Revised 12 December 2019, Accepted 1 August 2020, Available online 12 August 2020, Version of Record 16 September 2020.",https://doi.org/10.1016/j.csda.2020.107061,Cited by (2),"Missing data occur in almost every field and a great deal of literature has been established for the analysis of missing data with different types of missing mechanisms and under various models. Nonignorable missing data can be analyzed using nonparametric transformation models, which has not been discussed in the literature. In particular, assume that the conditional ==== can be written as the product of separate unknown functions of the response variable and ","Let ==== and ==== denote a response variable and a ==== the subvector of ==== formed by the components in ==== which may suffer missing. Define the response indicator function ==== if ==== is observed and 0 otherwise. It will be assumed that the components in ==== with ====, are often discussed (====). They are (i) missing completely at random if ====, (ii) missing at random if ====, and (iii) nonignorable missing if ==== depends on ====. It is well-known that the analyses with the missing at random assumption will be much easier than with the nonignorable missing assumption, while for the data with nonignorable missing, the use of the methods derived under the missing at random assumption could result in serious estimation bias and incorrect conclusions.====A great deal of literature has been established on the analysis of nonignorable missing data (====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====). In particular, ==== gave a comprehensive review of the existing literature on the nonignorable missing.====, ==== and ==== follows this direction. In particular, ==== proposed a general UMDM assumption that ==== can be written as the multiplier of an unknown ====-only function and an unknown ====-only function. This general UMDM assumption is very flexible and includes many specific scenarios. For example, if one considers the case of ==== having missing values and ==== fully observed, i.e., the missing response case, then the missing at random assumption is a special case of this general UMDM assumption. Moreover, this general UMDM assumption can also allow the covariates ==== to have missing values, and both response ==== and covariates ==== to have missing values. In the following, we explore how to extend rank-based estimation methods to general transformation regression models with nonignorable missing data under this general UMDM assumption.====Many authors have discussed different types of transformation regression models and developed various estimation methods. For example, ==== proposed a distribution-free maximum rank correlation estimator by using Kendall’s ====, and ==== gave a similar monotone rank estimator. Also ====The remaining of the paper is organized as follows. In Section ====. Also in Section ====, a random weighting resampling scheme is given to approximate the distribution of the proposed WR estimators. Section ==== gives some results obtained from a simulation study conducted to assess the performance of the proposed method and they indicate that the method works well for practical situations. In Section ====, an application is discussed and Section ====.====In this appendix, we will sketch the proof of the asymptotic results in ====, ==== and provide the justifications of ==== and Eq. ====. We will use ==== and ==== to denote respectively the density and distribution functions of ====. Then, the conditional density of ==== given ==== is given by ====, where ====.====Let ==== denote an observation from the distribution ==== on the set ====. For each ==== in ==== and each ==== in ====, define ====where ==== and ====. Write ==== for the ====th partial derivative operator of the function ==== with respect to ====, and let ====
 Let ==== and ==== denote the support of ==== and ====, respectively. To establish all the large-sample properties in this paper, we require the following conditions:",Weighted rank estimation for nonparametric transformation models with nonignorable missing data,https://www.sciencedirect.com/science/article/pii/S0167947320301523,12 August 2020,2020,Research Article,346.0
Grömping Ulrike,"Beuth University of Applied Sciences, Berlin, Germany","Received 7 November 2019, Revised 27 July 2020, Accepted 1 August 2020, Available online 11 August 2020, Version of Record 27 August 2020.",https://doi.org/10.1016/j.csda.2020.107059,Cited by (2),"Regular ==== with 2-level factors are among the most frequently used experimental plans. In many cases, designs should be blocked for dealing with inhomogeneity of experimental units. At the same time, the research question at hand may imply a focus on specified sets of two-factor interactions, while it is not justified to assume negligibility of other low order effects. An algorithm is provided for blocking a regular fraction into – possibly small – blocks while keeping specified two-factor interactions clear from confounding with main effects or other two-factor interactions. The proposed algorithm is implemented in the R package ==== and combines an ==== algorithm by the author with an automated implementation of a recent proposal for blocking fractions by hand.","Regular fractional ==== designs with 2-level factors are frequently used. If inhomogeneity of experimental units has to be accounted for, they are typically conducted in blocks. At the same time, the research question at hand may imply a focus on specified sets of two-factor interactions, while it is not justified to assume negligibility of other low order effects. For example, in experiments with control factors and noise factors, particular emphasis may be on the estimation of interactions between control factors and noise factors, while not assuming that interactions within each group of factors are negligible. Two-factor interactions (2fis) that are not confounded with main effects or other 2fis are called “clear”.====Blocking fractional factorial 2-level designs can be a challenging task, if the blocked design must keep a user-specified set of 2fis clear. Some authors have worked on this: For example, ==== provided upper and lower bounds for the number of clear 2fis and proposed a construction method for blocked designs with many clear 2fis. ==== proposed an algorithm for constructing blocked resolution IV designs with maximum number of clear 2fis and provided a catalogue of such designs with up to 64 runs. A recent article by ==== (see ==== for an earlier version of that package). Accommodation of estimability requirements – for blocked or unblocked designs – uses the estimability algorithm that was described in ====, which makes use of the R package ==== (==== (see Section ====.====Section ====. Section ==== describes the algorithm and its implementation in the package ====. The final discussion points out limitations and opportunities for future extensions. Supplementary material provides details on the fractions used in this paper, an overview of relevant R functions in the R package ====, R code for the examples from Sections ====, ==== and an additional worked example that accommodates user-specified custom generators.====The following is the Supplementary material related to this article. ",An algorithm for blocking regular fractional factorial 2-level designs with clear two-factor interactions,https://www.sciencedirect.com/science/article/pii/S016794732030150X,11 August 2020,2020,Research Article,347.0
"Junker Robert R.,Griessenberger Florian,Trutschnig Wolfgang","Department of Biology, Evolutionary Ecology of Plants, Philipps-University Marburg, Karl-von-Frisch-Strasse 8, 35043 Marburg, Germany,Department of Biosciences, University of Salzburg, Hellbrunnerstrasse 34, A-5020 Salzburg, Austria,Department of Mathematics, University of Salzburg, Hellbrunnerstrasse 34, A-5020 Salzburg, Austria","Received 19 March 2020, Revised 27 July 2020, Accepted 1 August 2020, Available online 9 August 2020, Version of Record 25 August 2020.",https://doi.org/10.1016/j.csda.2020.107058,Cited by (18)," or Spearman’s ==== is equally dependent on a random variable ==== as vice versa. A copula-based, hence scale-invariant dependence measure called ==== overcoming the just mentioned problem was introduced in 2011. ==== attains values in ====, it is 0 if, and only if ==== and ==== are independent, and 1 if, and only if ==== for ==== is used both, to perform a simulation study illustrating the small sample performance of the estimator as well as to estimate the directed dependence between some global climate variables as well as between world development indicators."," and ==== largely ignore this key aspect. In fact, classical ‘dependence’ measures like Pearson’s ====, Spearman’s rank correlation ==== or Kendall’s ==== are applicable only in specific situations (linearity or monotonicity/concordance) and are by construction symmetric and hence undirected, insinuating that the random variable ==== is equally dependent on the random variable ==== as vice versa.====As a matter of fact, also in many real-life situations dependence between random variables is asymmetric (see, e.g., ====, ====, ====, ====). Considering, for instance, only symmetric dependence measures may lead to wrong risk assessments in stock market trades (see ====), or to inaccurate gene network reconstructions (see ==== and the references therein). Moreover, analyzing networks via Pearson’s ==== or Spearman’s ====).====In the course of an extensive literature research we found additional, more sophisticated dependence measures, including, e.g., distance correlation ==== (see ====), the maximal information coefficient (MIC) (see ====, ====, ====), Linfoot correlation (see ==== (see ====). These methods detect independence in the sense that the measure ==== fulfills ==== if, and only if ==== and ==== are independent and are, to a certain extend, capable of quantifying symmetric non-linear dependence. Nevertheless, none of these approaches assigns maximum dependence exclusively to pairs ==== of random variables in which ==== (but not necessarily vice versa). The mutual-information based method MIC assigns maximum dependence to ==== if ==== is a measurable function of ==== or vice versa, in which case we have ====. The reverse implication, however, does not hold, i.e., we can have ==== although neither ==== is a measurable function of ==== nor vice versa (see ====). According to Rényi’s axioms (see ==== a different value than the pair ====. ====, e.g., depicts a ====-shaped sample of size ====. In this case knowing ==== strongly improves the predictability of ==== – we gain a lot of information about ==== by observing ==== – whereas the information gain in the other direction is significantly smaller.====Ignoring asymmetry in dependence means ignoring valuable information. To the best of the authors’ knowledge up to now there exist only two measures ==== quantifying the dependence of all pairs of continuous random variables ==== which fulfill the following five natural requirements:====These measures are ==== introduced by Trutschnig in 2011 (see ====) and the so-called measure of regression dependence ==== introduced by Dette et al. in 2013 in ====, which can also be regarded as the ====-version of ====. In this paper we focus on ====, the results, however, can directly be translated to ====. ==== is constructed via the Markov kernel (conditional distributions) of the (unique) copula ==== underlying the pair ==== and is defined as the so-called ====-distance of ==== to the product copula ==== describing independence. In fact, letting ==== denote Markov kernels of the bivariate copulas ====, the ====-distance is given by ====and ==== is defined as ==== whereby ==== is just a normalizing constant. In other words, we have ====For proofs of the fact that ==== fulfills the requirements (R1)–(R5) and for additional information on ==== and ==== we refer to ====, in this paper we focus on how to estimate ====.====Considering that ==== for ==== (short for quantification of asymmetric dependence). Using qad we then estimate the directed dependence between some global climate variables as well as between world development indicators, and perform a simulation study illustrating the small sample performance of our estimator.====The rest of the paper is organized as follows: Notation and preliminaries are reviewed in Section ====. Section ==== recalls the definition of an empirical checkerboard copula, sketches the idea underlying the estimator ==== and provides the main theoretical result of the paper, i.e., strong consistency of ====. Section ==== the afore-mentioned dependence between global climate variables and between world development indicators is estimated and illustrated in Section ====. Finally, Section ==== provides an outlook and completes the paper.====The following example shows that the maximal information coefficient (MIC) can be maximal also outside the situation of complete dependence.",Estimating scale-invariant directed dependence of bivariate distributions,https://www.sciencedirect.com/science/article/pii/S0167947320301493,9 August 2020,2020,Research Article,348.0
"Freund Fabian,Siri-Jégousse Arno","Crop Plant Biodiversity and Breeding Informatics Group (350b), Institute of Plant Breeding, Seed Science and Population Genetics, University of Hohenheim, Fruwirthstrasse 21, 70599 Stuttgart, Germany,Departamento de Probabilidad y Estadística, IIMAS, UNAM. Apartado Postal 126 01000 México, CDMX, Mexico","Received 11 January 2020, Revised 17 July 2020, Accepted 20 July 2020, Available online 30 July 2020, Version of Record 19 November 2020.",https://doi.org/10.1016/j.csda.2020.107055,Cited by (6),"Modeling genetic diversity needs an underlying genealogy model. To choose a fitting model based on genetic data, one can perform model selection between classes of genealogical trees, e.g. Kingman’s coalescent with ==== or multiple merger coalescents. Such selection can be based on many different ==== measuring genetic diversity. A ","Modeling genetic diversity of a sample of ==== present-day individuals is a key tool when reconstructing the evolutionary history of populations or scanning the genome for regions under selection, see e.g. ====. For a genetic region without recombination, a standard approach is to model the genealogical tree and the mutations along its branches as random processes. The genealogy thus is given by a random tree with ====Several genealogical tree models have been proposed (see ==== for a summary). The most widely used model, Kingman’s ====-coalescent, is a strictly bifurcating random tree (====). It approximately describes the genealogy of a sample from a fixed-size population under neutral evolution if offspring numbers per parent are not too variable, e.g. if the population is described by a (haploid or diploid) Wright–Fisher model or a Moran model with fixed size ====, see ====. More precisely, the ====-coalescent is the limit (in distribution) of the genealogies in the discrete populations as ====, if one treats ==== generations as one unit of time in the coalescent tree (which has a continuous time scale). Here, ====-coalescent as limits of genealogies from Wright–Fisher models with varying population size and/or population structure are available, see e.g. ==== and ====. These variants still are strictly bifurcating trees. Barring recombination, both haploid and diploid populations are well-captured by Kingman’s ====-coalescent and its variants.====However, some populations will feature genealogies not well approximated by Kingman’s ====-coalescent and its bifurcating variants. Certain modes of rapid selection, affecting the whole genome, should lead to the Bolthausen–Sznitman ====-coalescent, e.g. see ====, ==== and ====. This genealogy model likely produces multifurcating genealogical trees. Even a succession of hard selective sweeps may cause a multifurcating genealogy (though not the same model), see ====.====Generally, one can consider the genealogy in an arbitrary Cannings model, i.e. in a fixed-size discrete-time population model where offspring sizes are exchangeable among parents. Then, a whole class of multifurcating trees, the ====- or ====-====-coalescents, emerges as possible weak limits of the genealogies for ====. Again, to reach these limits time has to be rescaled as for Kingman’s ====-coalescent, see ====. Non-Kingman ====-coalescents arise if the distribution of offspring size per parent is variable enough, see ====. They have been shown to explain the genetic diversity of several maritime species better than Kingman’s ====-coalescent and its variants, see e.g. ====, ==== or ====. They seem to capture well the concept of sweepstake reproduction, where one individual may produce a considerable part of the population by chance. Two specific haploid models proposed for this are leading to the Eldon–Wakeley-====-coalescents, see ====, and to the Beta-====-coalescents, see ====. However, for diploid populations, the multiple merger mechanism is modified from its haploid version, see ====. Other mechanisms may also lead to these coalescent limits, e.g. ==== argues that Beta-====-coalescents may describe genealogies in viral populations where super-spreaders tend to infect a large number of individuals. Again, variants for varying population sizes in the pre-limit models are available, see ====, ====, ==== and ====. For further discussion where multiple merger genealogies may arise, see the reviews ==== and ====.====For cancer cells, (one-locus) Beta-====) were reported, which were much lower than the SFS-based classification errors from ====. The discrepance of error rates may come from slightly different hypotheses, a different approach to mutation rates, a different ABC approach and/or the use of additional summary statistics.====Motivated by this drop in error rates, our goal is to investigate which statistics are best suited to distinguish different classes of ====-coalescents. Additionally to the statistics from ====, we consider some further common statistics as well as a new statistic, the smallest allele frequency among non-private mutations observed in one individual.====These statistics are used in an ABC framework. A simulation-based method is a reasonable choice since the distributional characteristics of the diversity statistics used are not fully understood, see e.g. ==== that performs well with many, potentially uninformative statistics and that measures the importance of each statistic to distinguish between the hypotheses.====The following is the Supplementary material related to this article. ",The impact of genetic diversity statistics on model selection between coalescents,https://www.sciencedirect.com/science/article/pii/S0167947320301468,30 July 2020,2020,Research Article,349.0
"Burgard Jan Pablo,Krause Joscha,Schmaus Simon","Department of Economic and Social Statistics, Trier University, Universitätsring 15, 54296 Trier, Germany","Received 10 July 2019, Revised 4 July 2020, Accepted 6 July 2020, Available online 18 July 2020, Version of Record 10 September 2020.",https://doi.org/10.1016/j.csda.2020.107048,Cited by (1),Spatial dynamic ,", ==== and ====, as well as ====. Microsimulations are often conducted according to a basic procedure. First, a base population as a synthetic replica of the system of interest is constructed. In practice, this may be either artificially generated data or real-world observations from administrative records and surveys (====). Next, multiple parameters that characterize the system in its initial state are altered in scenarios. The alterations are designed to target important properties of the system in light of the research objectives. The effects of the alterations are projected into future periods, such that every scenario initializes an individual branch in the system’s evolution. After a given number of periods (simulation horizon), the branches are compared giving insights on important dynamics and dependencies within the system (====).====There are different types of microsimulations. They mainly differ with respect to the manner in which the mentioned alterations are projected. An important distinction is between static and dynamic microsimulations (====). Static microsimulations are characterized by the constancy of unit characteristics over time. When constructing the synthetic replica, every unit is provided with a set of characteristics that determines its behavior and interaction with other units. In static microsimulations, these characteristics do not change over the simulation horizon. Only specific simulation inputs are altered, depending on the research objectives. Examples of static microsimulation models can be found in ==== as well as ====. Dynamic microsimulations, on the other hand, are characterized by stochastic changes of unit characteristics (state transitions) over time. Since a unit’s behavior within the synthetic replica is driven by its respective characteristics, the interactions between units are also subject to temporal variation. Examples of dynamic microsimulation models can be found in ==== and ====. If the dynamic microsimulation is time-discrete, state transitions can only appear periodically at distinct points in time (====).====Hereafter, we focus on dynamic microsimulations with discrete time. In particular, we look at microsimulations in socio-economic research, where polytomous variables are of interest. This conceptual delimitation differentiates the topic from other fields where corresponding simulations are also used, such as particle physics (====, ====, ====).====, ====, ====, ====). Thus, if the survey data lacks in regional detail, methodological adjustments are required.====In this paper, we discuss so-called alignment methods (====, ====, ====, ====Two alignment methods are studied for this purpose. The first method is called logit scaling and was originally proposed by ====. It is an ex-post approach based on iterative proportional fitting (====). After the initial model parameter estimation has been performed, the transition probability estimates under the model are adjusted sequentially until they reproduce the external benchmarks. ====, ====, ====Both methods are described and discussed in theory. Next, they are tested in simulation experiments to evaluate their performances in a controlled environment. And finally, both methods are applied to labor forceprobability estimation based on the German Microcensus 2012 (====). We find that the inclusion of aggregated regional benchmarks allows for the recovery of local micro-level transition dynamics despite a lack in regional detail. The remainder of the paper is organized as follows. In Section ====, the basic methodology is described. This includes the presentation of a suitable statistical framework, the multinomial logit model, as well as logit scaling as a standard method for alignment. Section ==== introduces constrained maximum likelihood as an alternative alignment approach. Section ==== contains the simulation experiments, while Section ==== encloses the application. Section ==== closes with some conclusive remarks. Note that a preprint of this paper has been published as working paper by ====.====The following is the Supplementary material related to this article. ",Estimation of regional transition probabilities for spatial dynamic microsimulations from survey data lacking in regional detail,https://www.sciencedirect.com/science/article/pii/S0167947320301390,18 July 2020,2020,Research Article,350.0
"Wang Shuying,Wang Chunjie,Wang Peijie,Sun Jianguo","School of Mathematics and Statistics, Changchun University of Technology, Changchun, 130012, China,Center for Applied Statistical Research, School of Mathematics, Jilin University, Changchun, 130012, China,Department of Statistics, University of Missouri, Columbia, MO, 65211, USA","Received 3 March 2019, Revised 20 November 2019, Accepted 25 November 2019, Available online 28 November 2019, Version of Record 2 December 2019.",https://doi.org/10.1016/j.csda.2019.106891,Cited by (6),"The additive hazards model is one of the most commonly used model in ==== of failure time data and many estimation procedures have been developed for its inference under various situations (Kalbfleisch and Prentice (2002); Lin and Ying (1994); Sun (2006)). In this paper, we consider a situation, case ==== of the resulting estimators are established. A simulation study is conducted to assess the finite sample performance of the proposed method and suggests that it works well for practical situations. Also the method is applied to an AIDS study that motivated this study.",", ====, ====, ====). In this paper, we consider a situation, case ==== interval-censored data with informative interval censoring, that often occurs in practice such as medical follow-up studies but has not been discussed much in the literature due to the difficulties involved. For the problem, a sieve maximum likelihood approach will be developed====Interval-censored failure time data have recently attracted a great deal of attention (====, ====). By interval censoring, we usually mean that instead of being observed exactly, the failure time of interest is known only to belong to a window or an interval. It is apparent that interval-censored data include right-censored data as a special case (====) and they can occur naturally in any follow-up studies such as clinical trials. Also they can have various forms such as case ==== interval-censored data, which are also often referred to as current status data (====, ====, ====). For the situation, each study subject is observed only once and the failure time of interest is either left- or right-censored. In the following, we will consider a much more general form, case ====, ====).====Many procedures have been proposed for the analysis of interval-censored data when the interval censoring or censoring mechanism is non-informative (====, ====, ====, ====, ====, ====). There also exist some estimation procedures when the interval censoring is informative (====, ====, ====, ====, ====, ====) and the latent or frailty model approach (====, ====, ====, ====, ====, ====, ====). In particular, ====, ==== proposed some estimating equation-based methods for case ====The remainder of this paper is organized as follows. We will begin in Section ==== with introducing some notation and assumptions that will be used throughout the paper. In particular, we will employ the latent variable approach to describe the correlation between the failure time of interest and the observation process and present the resulting likelihood function. Section ==== will give some results obtained from an extensive simulation study conducted for the assessment of the finite sample performance of the proposed method and they indicate that it seems to work well for practical situations. In Section ====, the proposed approach is applied to a set of real data arising from an AIDS study and Section ==== contains some discussions and concluding remarks.====In this appendix, we will sketch the proofs for ====, ====, ====. Throughout the following proofs, we denote ==== and ====. Let ==== represent a generic constant that may vary from place to place. We first present the regularity conditions required.",Estimation of the additive hazards model with case ,https://www.sciencedirect.com/science/article/pii/S0167947319302464,28 November 2019,2019,Research Article,363.0
"Zhang Ting,Wang Lei","School of Statistics and Data Science, LPMC & KLMDASR, Nankai University, Tianjin, 300071, PR China","Received 20 December 2018, Revised 3 August 2019, Accepted 12 November 2019, Available online 25 November 2019, Version of Record 2 December 2019.",https://doi.org/10.1016/j.csda.2019.106888,Cited by (9),None,". Moreover, in real data analysis, the QR model may include many irrelevant covariates, especially for high-dimensional covariates. In this case, it is important to find which covariates are relevant for prediction, both for better interpretation of the model, and for better efficiency of the estimator. Due to their computational efficiency and statistical stability compared to information criterion based methods, penalization or shrinkage based variable selection methods, including Lasso (====), SCAD (====), adaptive Lasso (====), Dantzig selector (====) and many others, have attracted lots of attention.====The majority of existing methods consider the estimation of QR model based on fully observed data, while little knowledge is available on analyzing general QR estimation under missing data. When the data are missing at random (MAR), dropping observations with missing response data will result in a serious loss of efficiency and badly biased QR estimators. However, in practice the data are often missing not at random (MNAR), i.e., the propensity depends on response regardless of whether the response is observed or missing. In the presence of nonignorable nonresponse, estimation is a great challenge, since some parameters are not identifiable (====). Methods of handling nonignorable missing data can be very different from those for ignorable missing data, see ====, ====, ====, ====, ==== and ====. With nonignorablemissing data, ==== and ====) method for QR estimation based on nonignorable missing covariates.====The rest of this paper is organized as follows. After presenting the parametric propensity and instrument approach, we define the SWEL based QR estimator and investigate its statistical properties in Section ====. In Sections ====, ====, we introduce the PSWEL estimator and algorithm for variable selection. Simulation studies are given in Section ====. Section ==== analyzes the AIDS Clinical Trials Group Protocol 175 data for illustration. Some discussions can be found in Section ====. All technical details are provided in ====.====This section provides conditions and the proofs of ====, ====, ====.====Denote ====, ====, ====. Next, we establish part (ii). Taking derivation of ==== and ==== about ==== and ==== at ==== respectively, we obtain ==== Expanding ==== and ==== about at ==== yields ====
 where ====. As ==== with probability tending to 1, we consider the components ==== and ====. Then we immediately derive that ====with ====where ==== is the first d sub-vector of ====, ==== is the top ==== sub-matrix of ====, and ==== is the top ==== sub-matrix of ====. Additionally, ==== is the top ==== sub-matrix of ====, and ==== is the top ==== sub-matrix of ====. Note that ====, we know that ====. Therefore, ====where ====. Furthermore, under condition (C8), for ====, ====, ==== (a is the constant in the SCAD penalty), we have ====, which implies ====. Under condition (C7), we have ==== and ====, and ====, ==== is the corresponding ==== sub-matrix of ====. By Slutsky’s theorem and the Central Limit theorem, we have ====, where ====.",Smoothed empirical likelihood inference and variable selection for quantile regression with nonignorable missing response,https://www.sciencedirect.com/science/article/pii/S0167947319302439,25 November 2019,2019,Research Article,364.0
"Im Jongho,Morikawa Kosuke,Ha Hyung-Tae","Department of Applied Statistics, Yonsei University, Seoul, South Korea,Graduate School of Engineering Science, Osaka University, Osaka, Japan,Department of Applied Statistics, Gachon University, Sungnam, South Korea","Received 21 March 2019, Revised 13 October 2019, Accepted 1 November 2019, Available online 23 November 2019, Version of Record 3 December 2019.",https://doi.org/10.1016/j.csda.2019.106882,Cited by (0),. Numerical examples with various simulated and real datasets demonstrate the superiority of the proposed estimator.,", ====, ====, ==== for the theory of orthogonal polynomial expansions, and ====, ====, ====, ====, ====, ==== for series density approximation as an application in distribution theory.====, ====, ====, ====, ====, ====) and a moment-matching method (e.g., see ====). A modification of the orthogonal polynomial expansion for removing the negativity problem was discussed in econometric theory by ==== for estimating model parameters using maximum likelihood. ====, ====, ==== proposed a closed-form transition density expansion for a multivariate affine jump–diffusion process.==== density (i.e., non-negative and integrable to 1).====This study proposes least squares-type estimation for series expansion, the semi-parameters of which are, as suggested by ====.",A least squares-type density estimator using a polynomial function,https://www.sciencedirect.com/science/article/pii/S0167947319302373,23 November 2019,2019,Research Article,365.0
"Liu Juxin,Ma Yanyuan,Johnstone Jill","Department of Mathematics and Statistics, University of Saskatchewan, Saskatoon, Canada,Department of Statistics, Pennsylvania State University, University Park, USA,Department of Biology, University of Saskatchewan, Saskatoon, Canada","Received 6 October 2018, Revised 5 November 2019, Accepted 7 November 2019, Available online 22 November 2019, Version of Record 3 December 2019.",https://doi.org/10.1016/j.csda.2019.106887,Cited by (1),"Field studies in ecology often make use of data collected in a hierarchical fashion, and may combine studies that vary in sampling design. For example, studies of tree recruitment after disturbance may use counts of individual seedlings from plots that vary in spatial arrangement and sampling density. To account for the multi-level design and the fact that more than a few plots usually yield no individuals, a mixed effects zero inflated ","Zero-inflated Poisson (ZIP) regression is a popular approach to handle excessive number of zeros for count data (e.g. ====, ====). Application of these models has been expanding rapidly in the field of ecology, where counts of individuals are a key response variable of interest (====, ====, ====); development of these types of analyses is an active research area in ecology (====, ====, ====). Model structures may need to account for a number of challenging aspects of ecological data, such as hierarchical sampling designs, uneven sampling effort, unknown detection error, and limited knowledge of the various sources of zero counts in the data (====).====Zero-inflated models are often applied in ecology to assess the relationship of environmental, spatial, and other covariates with response data that are in the form of counts or abundances. Usually the ecological question aims to assess the number, relative influence, and statistical significance of parameters in the model, with the aim of improved understanding of ecological processes or prediction of ecological responses to variations in covariates (====). Because of the high variance and over-dispersion associated with zero-inflated datasets, evaluation of model fit can be a challenging and sometimes overlooked aspect of ecological analyses. Many ecological studies simply resort to comparing output from multiple different model structures to select the best form of model for the data at hand (e.g. ====, ====, ====). Standard assessments of model fit may be poorly behaved under the conditions of zero-inflated models and do not appear to be routinely applied to provide robust assessments of model performance (====, ====).====In this paper we investigate the problem of assessing model goodness of fit using a case study of seedling recruitment after fire (====Suppose ==== is the count of new trees for the ====th plot in the ====th site, where ====. We consider the following regression model, given the site-specific random effects, ====The parameters ==== and ==== are modeled as in ====. Specifically, suppose the covariates associated with ==== are ==== and the covariates associated with ==== are ====. Let ==== and ==== and ==== be ==== where the random effects ==== and variance–covariance matrices ==== and ====, respectively. If ====, ==== and ==== are independent and have normal distributions with mean 0 and variance ==== and ====, respectively. If ====, it means no random intercepts in the Poisson component (ZI component). Please note that we propose the test procedure in a general model setting in Section ====.==== extended the work by ==== checked whether it is appropriate to introduce random effects in a ZIP or ZIB model. However, to our best knowledge, there is no formal test procedure in the literature for the general goodness-of-fit of the model ==== that we are considering in this paper.====The remaining part of this paper is outlined as the following: We discuss the parameter estimation for a ZIP model with normal random effects in both the Poisson regression part and the zero-inflation part in Section ====. We then construct the test statistic based on the cumulative sum of the residuals in Section ====. In Section ====, we also establish the theoretical properties of the proposed test procedure. Two sets of simulation studies are conducted in Section ==== to check the performance of the proposed test procedure. In this section, we also analyze the seedling data described in ====, which initiated this work originally. We provide some discussions and conclude the findings in Section ====. Further technical details are provided in ====.",A goodness-of-fit test for zero-inflated Poisson mixed effects models in tree abundance studies,https://www.sciencedirect.com/science/article/pii/S0167947319302427,22 November 2019,2019,Research Article,366.0
"Feng Long,Zhang Xiaoxu,Liu Binghui","School of Statistics and Data Science, LPMC and KLMDASR, Nankai University, China,Key Laboratory for Applied Statistics of MOE and School of Mathematics and Statistics, Northeast Normal University, China","Received 10 December 2018, Revised 2 November 2019, Accepted 12 November 2019, Available online 21 November 2019, Version of Record 28 November 2019.",https://doi.org/10.1016/j.csda.2019.106889,Cited by (6),.,"Nowadays, high-dimensional or even ultra-high dimensional data are becoming increasingly available in many fields of application, as data collection technology evolves very quickly. Here the dimensionality is high when the number of variables is larger than the sample size, while it is ultra-high when the number of variables is one or several orders of magnitude larger than the sample size, as pointed out in ====In this paper, we consider the high-dimensional two-sample location testing problems. Let ====, ====, be i.i.d. copies of two independent random vectors respectively, which obey ====where ====, ==== are the symmetry centers and ====There is a lot of existing literature on this problem. As the dimension of the variables is assumed to be fixed and the sample size can grow to infinity, the most familiar method for the two-sample location problem is the Hotelling’s ==== test, with the test statistic: ====where ==== and ==== are the corresponding sample means and ====On this ground, ==== proposed a test statistic by replacing the Mahalanobis norm in the Hotelling’s ==== test statistic with the Euclidian norm, which is based on ==== by replacing ==== in ==== with ====, the ==== proposed a modified test statistic (abbreviated as CQ hereafter) as follows ====by removing ==== from ==== extended the results of ==== to unequal covariance matrices.==== where ====, ==== and ====, ==== and ==== respectively. Here ==== where ==== is the covariance matrix of the ====th sample, ==== is the covariance matrix of the ====th sample excluding ==== and ==== is the covariance matrix of the ====th sample excluding ==== for each ==== and each ====. Unfortunately, this test is not shift-invariant.====The above modified Hotelling’s ==== tests generally have very good performance for data from normal distributions, but deteriorate quickly when the data deviate from normality especially in high dimension situation, hence would perform extremely poorly for heavy-tailed distributions. For this reason, as mentioned in ==== and ==== proposed a high-dimensional nonparametric multivariate test based on spatial signs, and ====For example, ==== proposed a scalar-invariant test statistic based on spatial signs (abbreviated as SS hereafter) ====with the spatial sign function ==== for each ====, where ==== and ==== are the corresponding estimations of the location vectors and the scatter matrices respectively. Here ==== and ==== can be obtained by using the “leave-one-out” samples, ====Unfortunately, as a result of the additional bias caused by estimating the location parameters, this method can only allow the dimension of variables to be at most the square of the sample sizes. In addition, ==== proposed a two-sample spatial rank test (abbreviated as CC hereafter): ====which can deal with ultra-high-dimensional data, but is not invariant under scalar transformations. These motivate us to establish a new spatial rank testing method for the high-dimensional data, which could avoid estimating the location parameters in the construction of test statistic, hence be available for ultra-high-dimensional data. Essentially, the proposed high-dimensional spatial rank test is a reworking of ==== (CC) (====) by embedding the process of re-scaling and leveraging the leave-out strategy in ==== to remove bias, which can be invariant under scalar transformations and just has the power to deal with ultra-high-dimensional data, allowing the dimension to grow almost exponentially with the sample sizes.====Specifically, we first estimate the scale of each variable by spatial-rank-based procedures; then on this basis we construct the high-dimensional spatial rank test via the leave-out method as in ==== and ====. By embedding the estimated scales in the test statistic, we aim to treat all the variables in a “fair” way. Unlike the spatial-sign-based methods, there is no need for the proposed test to estimate the location parameters for spatial ranks. As a result, the bias could be avoided, which makes it available even as the dimension grows almost exponential with the sample sizes.====The rest of the paper is organized as follows. We introduce the proposed high-dimensional spatial rank test for high-dimensional data in Section ====. The numerical performance of the proposed test is demonstrated in Section ====. Finally, we conclude this paper in Section ==== and relegate the technical proofs to Supplementary Material.====The following is the Supplementary material related to this article. ",A high-dimensional spatial rank test for two-sample location problems,https://www.sciencedirect.com/science/article/pii/S0167947319302440,21 November 2019,2019,Research Article,367.0
"Zhang Likun,Castillo Enrique del,Berglund Andrew J.,Tingley Martin P.,Govind Nirmal","Department of Statistics, Pennsylvania State University, State College, PA 16801, USA,Department of Industrial and Manufacturing Engineering, Pennsylvania State University, State College, PA 16801, USA,Netflix Inc., Los Gatos, CA 95032, USA","Received 4 March 2019, Revised 4 November 2019, Accepted 5 November 2019, Available online 20 November 2019, Version of Record 27 November 2019.",https://doi.org/10.1016/j.csda.2019.106885,Cited by (0),New methodology is presented for the computation of ==== confidence intervals from massive response data sets in one or two ,"Here we adapt the “bag of little bootstraps” (BLB) method by ====, ====), the required MCMC computations are not amenable to the analysis of the massive data sets we intend to study. The main advantage of BLB is that it operates on a modern parallel and distributed architecture while preserving the statistical efficiency of the bootstrap. Another methodological objective of this paper is to propose a new algorithm to automate the selection of the smoothing parameter in penalized quantile regression splines. The existing criteria for choosing the tuning parameter that controls the smoothness of the functional estimate generally rely on the calculation of the effective degrees of freedom of the quantile fit, which we found to be intractable and problematic; see Section ==== for details. Our proposed criterion, CVB (cross-validation for binned data sets), based on a cross-validation technique integrated within BLB, is well-behaved and selects a smoothing parameter that achieves a desirable balance between the smoothness of the fitted function and the fidelity to the observed data points.====. As the original motivating problem from Netflix, further described in ====), respectively. These data sets feature the required scale, with each RCP yielding more than 12.6 million data points, while the spatial nature permits for a demonstration of our methodology with two covariates.====The remainder of this paper is organized as follows. Section ==== describes the Netflix use case. Section ==== introduces the nonparametric quantile smoothing spline model. Model fitting is via a penalized objective that controls the smoothness of the fitted quantile function, and the selection of the penalization parameter is a modification of the Multifold Cross-Validation approach of ====. Section ====), and Section ==== discusses a new criterion and the selection of the smoothing (penalization) parameter given the distributed nature of the “bag of little bootstraps” method. Coverage analyses of the performance of our proposed BLB–CVB method on various simulated functions and surfaces are presented in Section ====. Finally, the methodology is used to compare open source climate model predictions in Section ====.====In this section, we illustrate the Delaunay triangulation for a specific set of covariates ====. For simplicity, we generate 10 covariates on the unit square ==== for ====. To create the Delaunay triangulation ====, we need to ensure that the interior of the circumcircle of each triangle in ==== contains no points in ====. Package ==== (====) provides an efficient and numerically stable algorithm to produce Delaunay triangulation. See ==== for the Delaunay triangulation and its circumcircles for the 10 covariates.",Computing confidence intervals from massive data via penalized quantile smoothing splines,https://www.sciencedirect.com/science/article/pii/S0167947319302403,20 November 2019,2019,Research Article,368.0
"Zhang Tingting,Sun Yinge,Li Huazhang,Yan Guofen,Tanabe Seiji,Miao Ruizhong,Wang Yaotian,Caffo Brian S.,Quigg Mark S.","Department of Statistics, University of Virginia, United States of America,Division of Biostatistics, University of Virginia, United States of America,Department of Neurology, University of Virginia, United States of America,Department of Biostatistics, Johns Hopkins University, United States of America","Received 20 December 2018, Revised 3 August 2019, Accepted 26 September 2019, Available online 15 November 2019, Version of Record 3 December 2019.",https://doi.org/10.1016/j.csda.2019.106847,Cited by (3), methods are developed to estimate the model parameters of the proposed ODE model and to identify clusters of strongly connected brain regions. The proposed ODE model and Bayesian method are applied to iEEG data collected from a patient with medically intractable epilepsy and used to examine the patient’s brain networks before the seizure onset.,"The human brain is a network system, where brain regions, as network nodes, constantly interact with each other. The directional effect exerted by one brain region over another is referred to as directional connectivity and corresponds to a network edge in the brain network. Identifying connected brain regions and mapping the human brain network help us understand the mechanism of the brain as well as its normal and abnormal functions. In this article, we model the directional connectivity of the human brain and identify connected brain regions using intracranial electrocorticography (iEEG) data, ==== measurements of many regions’ neuronal activities.====, ====, ====, ====, ====, ====, ====). ====).====, ====, ====, ====), and has attracted much attention in researching the brain’s functional organization. As such, the proposed model leads to scientifically meaningful network results. We refer to the ODDM with the cluster structure as the modular ODDM (MODDM).====The MODDM, like many other statistical models, is an approximation of the complex system under study, and thus, has a discrepancy from the underlying true mechanism of the brain. As the model ====The rest of the manuscript is organized as follows. Section ==== introduces the MODDM. In Section ====, and apply the MODDM to analyze a real iEEG study in Section ====. Section ==== discusses analysis results and future research directions.",Bayesian inference of a directional brain network model for intracranial EEG data,https://www.sciencedirect.com/science/article/pii/S0167947319302026,15 November 2019,2019,Research Article,369.0
"Pan Yuqing,Mai Qing","Microsoft, Redmond, WA 98052, United States of America,Department of Statistics, Florida State University, Tallahassee, FL, 32306, United States of America","Received 6 February 2019, Revised 3 November 2019, Accepted 4 November 2019, Available online 15 November 2019, Version of Record 3 December 2019.",https://doi.org/10.1016/j.csda.2019.106884,Cited by (5),Differential ==== is an important statistical problem with wide applications. Many statisticians focus on binary problems and propose to perform such analysis by obtaining sparse estimates of the difference between ,", ====, ====, ====, ====, ====). Differential network analysis aims to detect changes of networks under different conditions. Consider variables ====. In differential network analysis, it is often assumed that ====where ====. Within the level ====, ====. Then ==== at Level ====. Our goal is estimating the differences among ====’s. It is often assumed that the differences are sparse, indicating that most pairs ==== have the constant interactions in all the levels, while we strive to identify the pairs with different interactions across levels (====, ==== e.g).====An intuitive approach to differential network analysis is to first estimate ====, and then take the differences among the estimated ====’s. The estimation of ==== can be performed either individually (====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====) or jointly (====, ====, ====, ====). However, this approach requires stronger assumptions than we desire. In order to estimate ==== in high dimensions by the above methods, we need to assume that each ==== is sparse. This assumption can be stringent in practice. For example, ==== and ==== and ====. If both of them are sparse, then apparently their difference ==== has to be sparse as well. However, when ==== is sparse, we do not necessarily have sparse ==== and ====. By directly assuming that ==== is sparse, we can avoid sparsity assumptions on ==== and ==== individually.====Consequently, efforts have been spent on direct estimation of the differential networks (====, ====, ====, ====). These methods directly estimate the differential networks, without imposing assumptions on the individual precision matrices. Thus, they are particularly suitable when we are reluctant to make sparsity assumptions on each network, as is the case in the transcriptional networks. These methodological developments greatly enrich the literature of differential network analysis. New statistical theories have been derived for these methods, and researchers have applied them on real datasets to obtain new findings.====However, the computation of differential network analysis remains a critical issue. Some of the existing methods can be demanding on the storage, while others have room for improvement on the computation efficiency. Note that we aim to estimate parameters of dimensions ====, where ==== can be very large in practice. Therefore, it is essential to accompany the methods with algorithms that scale well with ====. To this end, we propose an algorithm with cheaper and faster computation for the two promising differential network analysis methods in ==== and ====. Complexity analysis shows that the improvements are particularly significant when the truth is very sparse. Moreover, most existing methods focus on binary problems, but our algorithm easily accommodates multiple network problems.====, ====, ==== we briefly review differential network analysis. In Section ==== contains the application of the SMORE algorithm in binary differential network analysis. The extension to multiple network problems is discussed in Section ====. We review QDA and consider its high-dimensional extension in Section ====. In Sections ==== & ====, we present numerical studies on simulated and real data. A discussion is given in Section ====. All the technical proofs are relegated to the ====, ====, ====, ====.====We describe the Dantzig algorithm proposed by ==== for completeness. The Dantzig algorithm solves ==== as follows. ",Efficient computation for differential network analysis with applications to quadratic discriminant analysis,https://www.sciencedirect.com/science/article/pii/S0167947319302397,15 November 2019,2019,Research Article,370.0
"Liu Mengque,Zhang Qingzhao,Fang Kuangnan,Ma Shuangge","School of Journalism and New Media, Xi’an Jiaotong University, China,School of Economics, Xiamen University, China,Wang Yanan Institute for Studies in Economics, Xiamen University, China,Department of Biostatistics, Yale University, United States of America","Received 5 February 2019, Revised 31 October 2019, Accepted 2 November 2019, Available online 13 November 2019, Version of Record 18 November 2019.",https://doi.org/10.1016/j.csda.2019.106883,Cited by (4),"The finite mixture of regression (FMR) model is a popular tool for accommodating ====. In the analysis of FMR models with high-dimensional ====, it is necessary to conduct regularized estimation and identify important covariates rather than noises. In the literature, there has been a lack of attention paid to the differences among important covariates, which can lead to the underlying structure of covariate effects. Specifically, important covariates can be classified into two types: those that behave the same in different subpopulations and those that behave differently. It is of interest to conduct structured analysis to identify such structures, which will enable researchers to better understand covariates and their associations with outcomes. Specifically, the FMR model with high-dimensional covariates is considered. A structured penalization approach is developed for regularized estimation, selection of important variables, and, equally importantly, identification of the underlying covariate effect structure. The proposed approach can be effectively realized, and its statistical properties are rigorously established. Simulation demonstrates its superiority over alternatives. In the analysis of cancer ====, interesting models/structures missed by the existing analysis are identified.",", ====, ====.====, ====). In the literature, a relevant study is ====.====In the aforementioned and other high-dimensional studies, the focus of variable selection has been on distinguishing between important covariates and unimportant ones. Comparatively, there has been much less attention paid to the critical question of “====. Homogeneous and heterogeneous covariates have significantly different implications. Homogeneous covariates describe the shared properties (i.e., “commonality”) of all subjects, whereas heterogeneous covariates determine the mixture of subjects and their differences. As such, identifying the structure of covariate effects can significantly advance our understanding of covariates and their associations with the response. In the literature, of relevance are a handful of recent studies on the structure of covariate effects in cure rate models (====). Penalization has been adopted for variable selection, estimation, and identification of the covariate effect structure. Cure rate models differ significantly from FMR models. More importantly, in the existing cure rate model studies, statistical properties have not been established.====In this article, we consider heterogeneous data with high-dimensional covariates that can be described using FMR models. Like the literature, we conduct regularized estimation and variable selection. Significantly advancing from the literature, our objective also includes identifying the underlying structure of covariate effects, that is, distinguishing the homogeneous covariates from the heterogeneous ones. This effort can greatly advance our understanding of covariates and their relationships with the response variable. Although sharing a related scheme with the recent cure rate model studies, the modeling, proposed approach, and computation in our study are significantly different. In addition, statistical properties are rigorously established, which can provide important insights into the proposed method as well as other high-dimensional FMR models. With an intuitive formulation, solid statistical basis, and satisfactory numerical performance, this study can provide a useful new venue for analyzing commonly encountered heterogeneous data.====The rest of the article is organized as follows. In Section ====, we describe the proposed method, its computational algorithm, and the statistical properties. Simulation in Section ==== and data analysis in Section ==== demonstrate the competitive practical performance of the proposed method. A brief discussion is provided in Section ====. Technical details and additional numerical results are provided in ====.====The following is the Supplementary material related to this article. ",Structured analysis of the high-dimensional FMR model,https://www.sciencedirect.com/science/article/pii/S0167947319302385,13 November 2019,2019,Research Article,371.0
"Deresa Negera Wakgari,Van Keilegom Ingrid","ORSTAT, KU Leuven, Naamsestraat 69, B-3000 Leuven, Belgium","Received 8 December 2018, Revised 2 October 2019, Accepted 22 October 2019, Available online 4 November 2019, Version of Record 12 November 2019.",https://doi.org/10.1016/j.csda.2019.106879,Cited by (10),"In ==== observations are often right censored and this complicates considerably the analysis of these data. Right censoring can have several underlying causes: administrative censoring, loss to follow up, competing risks, etc. The (latent) censoring times corresponding to the latter two types of censoring are possibly related to the survival time of interest, and in that case this should be taken into account in the model. A unifying model is presented that allows these censoring mechanisms in one single model, and that is also able to incorporate the effect of ==== on these times. Each time variable is modeled by means of a transformed linear model, with the particularity that the error terms of the transformed times follow a ","). Second, subjects might be lost to follow up or might drop out from the study. For this type of censoring, one needs to be cautious, as the reason for the drop-out might be indirectly related to the survival time. For example, this relation can be negative when people drop out because they are feeling healthy and decide to no longer follow a treatment. This means that those who are censored would have a longer expected survival time after drop-out since they are on better health conditions. On the other hand, we can see a positive relation when people are too ill to be treated any longer. This means that those who are censored due to their severe health conditions would have a smaller expected survival time since they are at higher risk of experiencing the event soon after drop-out. Assuming that the drop-out time is stochastically independent of the survival time would therefore lead to biased results in this case (====, ====). Finally, a third common type of right censoring is caused by competing risks, namely the occurrence of another event which prevents the occurrence of the event of interest. Often, these mutually exclusive competing events are dependent. For instance, the occurrence of cancer and cardiovascular diseases share some common risk factors, like fat intake and general fitness. Considering competing risks as being independent, would therefore be an unrealistic assumption. Note that in this paper when we talk about ‘independent censoring’, we mean ‘stochastically independent censoring’ (i.e. the survival time is stochastically independent of the censoring time). So, our definition of independent censoring differs from the one commonly used in the multiplicative intensity model discussed, for instance, in Section III.2.2 in the book of ==== and in Section 2.2.8 in the book of ====.====From the above discussion it is clear that censoring can have several underlying driving forces, and that one should take these into account. In fact, each of the three types of censoring described above, requires a different approach. In this paper we present a unifying model that allows these censoring mechanisms in one single model, and that is also able to incorporate the effect of ====) has been studied extensively (see e.g. ====, ====, ====, ====, among others). Despite its popularity and the fact that a lot of theory and software has been developed for inference in this model, a drawback of these models is that the cumulative incidence function, which is very important in the context of competing risks, cannot be interpreted in an easy way in these models. Therefore, ==== and ==== and ==== and for a less detailed but useful treatment see ====, ==== and ====, ====).====Compared with those under stochastically independent censoring, methods for competing risks analysis under dependent censoring are less well developed. In this paper, we will emphasize on the methodological developments under the latter assumption. According to ====, the joint distribution of unobserved failure times is not identifiable from the joint distribution of the follow-up time and the cause of failure (identified minimum) when we have dependent competing risks. Therefore, to identify the joint distribution of unobserved failure times, we need extra information about their dependence. ==== and ====, ====, ==== and ====). Even when the functional form of the underlying failure times is known, the joint distribution may or may not be identified, i.e., the parameter values may not be determined by the distribution of the identified minimum (see, for example, the ==== model given in ====). Motivated by these papers, ====Our paper is organized as follows. In Section ==== we perform a detailed simulation study to show that our proposed model works well for finite samples. Section ====, whereas the ==== contains the proofs of the identifiability results of Section ====.",A multivariate normal regression model for survival data subject to different types of dependent censoring,https://www.sciencedirect.com/science/article/pii/S0167947319302348,4 November 2019,2019,Research Article,372.0
"Paci Lucia,Consonni Guido","Department of Statistical Sciences, Università Cattolica del Sacro Cuore, Milan, Italy","Received 12 March 2019, Revised 19 October 2019, Accepted 26 October 2019, Available online 2 November 2019, Version of Record 12 November 2019.",https://doi.org/10.1016/j.csda.2019.106880,Cited by (11),An objective ,", ====, ====, ====). Typically the structure of the underlying graph is unknown and need to be estimated on the basis of the available data: this is referred to as (graph) structural learning.====In the standard setting data arise in the form of multivariate independent and identically distributed observations, or merely independent observations as in covariate-adjusted graphical models (====, ====). In some cases however, data arise as multivariate observations collected over time; this requires enlarging the notion of graph to encompass both contemporaneous dependencies among variables as well as dynamic relationships over time; see ====.====, ====, ====), environmental sciences (====), neuroscience (====) and genomics (====). VAR models involve a large numbers of parameters, as soon as the dimension of the response variable is moderately high, leading to unstable inferences and inaccurate forecasts. A natural way to overcome this difficulty is to introduce evidence-backed parsimony constraints on the parameter space, and this is where graphical modeling kicks in.====VAR models can be naturally represented by graphs, with directed edges reflecting the autoregressive structure over time while undirected edges describe the contemporaneous interactions among variables. In this paper we focus on the undirected component of such graphs, and describe an objective Bayes methodology to learn dependencies among variables having adjusted for the recursive component of the time series. Specifically, we assume that the error ====Graphical VAR models have received lately some attention.  ====The paper is organized as follows. Section ==== contains some background material on the VAR model, as well as the required notions of objective Bayes model selection, in particular the fractional Bayes factor approach. In Section ==== presents a simulation study, while Section ====). Finally we conclude with a brief discussion in Section ====.====The fractional prior in ==== is obtained by combining the noninformative prior ==== with a fraction ==== of the likelihood in ====, that is ====Multiplying and dividing by ====, we obtain ====which is the kernel of the Matrix Normal–Inverse Wishart ====, where ====, ====, ==== and ====.",Structural learning of contemporaneous dependencies in graphical VAR models,https://www.sciencedirect.com/science/article/pii/S016794731930235X,2 November 2019,2019,Research Article,373.0
"Han Bo,Wang Xiaoguang","School of Mathematical Sciences, Dalian University of Technology, Dalian, Liaoning 116024, China","Received 1 March 2019, Revised 13 August 2019, Accepted 12 October 2019, Available online 23 October 2019, Version of Record 31 October 2019.",https://doi.org/10.1016/j.csda.2019.106874,Cited by (1),Case-cohort and nested case-control designs are widely used strategies to reduce costs of ,") and nested case-control (====). For this methodology, one common means is to assign different denominator weights in the pseudo/partial-likelihood, e.g., ==== and ==== for CC designs and ====. Several survival models have also been studied with the IPW method for CC and NCC designs, such as the proportional odds model (====) and the accelerated failure time model (====).==== and ==== developed the maximum likelihood estimation for the Cox model under two cohort designs. The efficient estimation was extended to a more general class of transformation models by ==== in two-phase cohort studies. The maximum likelihood estimation for the accelerated failure time model under CC and NCC designs was investigated by ====.====In clinical cohort studies, there may exist a number of cured subjects who remain event free of interest. Such as the subjects of Wilms tumor in Stage IV, after the surgery, the four-year survival is ==== as reported by ====. For the esophageal cancer, namely the esophageal squamous cell carcinoma, as described in ====, the disease-free survival rate is 85.0% among the subjects at the median of zinc concentration. Hence it is of great clinical importance to pay attention to a cure fraction among subjects when analyzing the survival data in cohort studies and disease prevention trials.====Following the tradition of statistical research, two types of cure rate models are commonly investigated to accommodate a cure fraction occurring in clinical experiments, namely the mixture cure model (====) and the non-mixture cure model (====, ==== and ==== and many others. The latter models the entire population directly and has a biological motivation presented in ====. The non-mixture cure model has been used in numerous studies, including the study of the prostate cancer (====), the study of the oropharynx cancer (====), the study of the smoking cessation (====), the study of the rectal cancer (====), among others. Statistical tools in the presence of a cure fraction for cohort designs are not well developed. Appropriate statistical procedures to take into account the cured subgroup of clinical experiments would be of great value for assessing the effects of suspect exposures on the outcomes of interest precisely.==== and ====The rest of the article is organized as follows. Section ==== describes the semiparametric non-mixture cure model and the pseudo-maximum likelihood estimation with the sieve method. The asymptotic properties of the estimators are presented in Section ====. Section ==== develops the EM algorithm to calculate the estimators. Simulation studies are executed to evaluate the finite sample performance of the proposed methods in Section ====. The analysis for the Wilms tumor dataset is provided in Section ====. Section ==== presents the concluding remarks. Detailed proofs are provided in ====.====In this appendix, the detailed proofs for ====–==== are provided. First, we present a necessary auxiliary lemma as follows.====The proof of ==== can be easily completed by the definition of the covering number presented in ====.",Semiparametric estimation for the non-mixture cure model in case-cohort and nested case-control studies,https://www.sciencedirect.com/science/article/pii/S0167947319302294,23 October 2019,2019,Research Article,374.0
"Tian Yuzhu,Song Xinyuan","School of Mathematics and Statistics, Northwest Normal University, LanZhou, PR China,Department of Statistics, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong","Received 1 March 2019, Revised 10 October 2019, Accepted 12 October 2019, Available online 23 October 2019, Version of Record 1 November 2019.",https://doi.org/10.1016/j.csda.2019.106876,Cited by (5),None,", ====, ====) for recent reviews and further comprehensive studies on QR).====QR is traditionally solved using approaches, such as interior algorithm (====), Majorize-Minimization (MM) algorithm (====), and the Expectation–Maximization (EM) algorithm (==== considered Bayesian spatial QR models; ==== considered Bayesian QR analysis of censored dynamic panel data; ==== considered Bayesian Tobit QR for single-index models; ====), adaptive LASSO (====), smoothly clipped absolute deviation (SCAD; ====), elastic-net (Enet; ====), bridge penalized regression (====, ====, ====), and ====-norm penalization (====With the rapid development of Bayesian statistical computation, many penalized regularization methods can be conveniently conducted in a Bayesian framework. For example, ==== proposed Bayesian LASSO (BLASSO) regression; ==== considered Bayesian adaptive LASSO (BALASSO) QR; ==== discussed Bayesian doubly adaptive Enet LASSO for VAR shrinkage; ==== studied Bayesian bridge regression; ==== discussed Bayesian tobit QR with ==== penalty; ==== studied Bayesian bridge QR with a fixed penalty index ====; and ==== considered Bayesian bridge regression using ====-norm regularization. However, different penalized methods present different statistical properties and variable selection efficiency. ==== showed that ====
 (====In the present work, we are interested in Bayesian bridge-randomized QR based on ====
 (====) regularizer. For bridge QR regression with ====-norm regularization, penalty exponent ==== is deemed as a parameter that can be estimated in Bayesian framework. For example, a Beta family prior is forced on this penalty exponent ====The remainder of this paper is organized as follows. Section ==== presents the considered model and working likelihood. In Section ====, we establish a Bayesian hierarchical model of bridge-randomized penalized QR and we consider its adaptive version. Section ==== applies the proposed methodology to a real-life study. Finally, Section ==== concludes the paper.",Bayesian bridge-randomized penalized quantile regression,https://www.sciencedirect.com/science/article/pii/S0167947319302312,23 October 2019,2019,Research Article,375.0
"Xue Liugen,Zhang Jinghua","College of Applied Sciences, Beijing University of Technology, Beijing 100124, China","Received 15 January 2019, Revised 11 September 2019, Accepted 16 October 2019, Available online 23 October 2019, Version of Record 7 November 2019.",https://doi.org/10.1016/j.csda.2019.106877,Cited by (6),"In this paper, we study the empirical likelihood for a partially linear single-index model with a subset of ","Consider the partially linear single-index model (PLSIM) ====where ==== is a scalar response variable, ==== and ==== are ==== and ==== and ==== are two unknown vectors, ==== is an unknown smooth link function, and ==== is a random error with ==== almost surely. The restriction ====Suppose that ==== is an independent and identically distributed sample from ====. Consider the vector ==== formed by shuffling the elements of ==== such that ====, a ====-dimensional non-null vector with ====, is observed for all ====’s, while ==== contains elements for which observations may or may not be available for some ====’s. Let ==== if all values contained in ==== are observed, and ==== otherwise. Throughout this paper, we assume that ==== and ==== is identically distributed, and let the missing data are missing at random (MAR), that is ====where ====In this paper, we consider the following four missing data scenarios: (i) data missing from all of the covariates but fully observed for the response; (ii) data missing from the response only; (iii) data missing from the response and a subset of covariates; (iv) data missing from a subset of covariates only. We do not consider the case of data missing from all of the covariates as well as the response, because we require a non-null ==== in order to be able to estimate ====For missing data, some authors carry out research. ==== developed an adjusted empirical likelihood approach to inference for the mean of the response variable, and proved the nonparametric version of Wilks’ theorem for the adjusted empirical log-likelihood ratio by showing that it has an asymptotic standard chi-squared distribution. ==== and ==== proposed a unified empirical likelihood approach to missing data problems and explore the use of empirical likelihood to effectively combine unbiased estimating equations when the number of estimating equations is greater than the number of unknown parameters, the proposed method can achieve semiparametric efficiency if the probability of missingness is correctly specified. The related works have ====, ====, ====, ====, ====, ====. It is worth mentioning that all of the above cited references were to handle either missing covariates or missing response, but not both.====For the single-index models under no missing data, many authors have carried out research. ====, ====, ====, ====, ====. For the single-index models under missing data, ==== and ==== investigated the empirical-likelihood-based inference for the construction of confidence intervals and regions of the parameters of interest in the single-index models with missing covariates at random, and the proposed inverse probability weighted-type empirical likelihood ratio is asymptotically standard chi-squared. However, the above mentioned references do not have to handle missing response and/or partially missing covariates.====In this paper, we are interested in estimating ====, ==== and unknown function ====, as well as in constructing confidence regions of ==== for model ====. Two empirical log-likelihood ratios for ==== are constructed. It is shown that each of two ratios is asymptotically chi-squared. We also construct a class of estimator for ====, ==== and ====. We prove that our methods yield asymptotically equivalent estimators for ==== and ====-consistency. We also prove that the estimator of ==== has the asymptotic normality and uniformly convergence rate. The proposed results can be used directly to construct the confidence regions of ====. The proposed methods have the following features and points. (a) The resulting empirical likelihood ratio has asymptotically chi-squared distribution by using the centered covariates and the imputation for missing values, this achieves the bias-correction for the empirical likelihood ratio function; (b) Because of our bias correction, the existing data-driven algorithm is valid for selecting an optimal bandwidth to estimate ==== and its derivative, undersmoothing for estimator of ==== is avoided; (c) The proposed methods can handle both missing responses and missing covariates simultaneously, this extends the applied scope in practice. We need to point out that the empirical likelihood method introduced by ====The rest of this paper is organized as follows. Section ==== gives methodology for constructing the empirical likelihood ratios and estimators for ====, ==== and ====. Section ==== develops some asymptotic properties of the proposed methods. Section ==== reports the results of some simulation studies and a real data example. Section ==== is the concluding remarks. The proofs of main results are relegated to the appendix.====In this appendix, we prove ====, ====, ====, ==== only. The proofs of ====, ==== are similar to the proofs of Theorems 1, 2 and 4 in ====, respectively, and hence we omit their proofs. The following ==== is useful for proving these theorems. The proof of ==== is given in the supplement material.",Empirical likelihood for partially linear single-index models with missing observations,https://www.sciencedirect.com/science/article/pii/S0167947319302324,23 October 2019,2019,Research Article,376.0
"Borrajo M.I.,González-Manteiga W.,Martínez-Miranda M.D.","Department of Statistics, Mathematical Analysis and Optimisation, Universidade de Santiago de Compostela, Spain,Department of Statistics and Operations Research, Universidad de Granada, Spain","Received 23 March 2019, Revised 11 October 2019, Accepted 14 October 2019, Available online 22 October 2019, Version of Record 26 October 2019.",https://doi.org/10.1016/j.csda.2019.106875,Cited by (3),The bias–variance trade-off for inhomogeneous point processes with ,", ====); epidemiology, (====, ====); astronomy, (====, ====); forestry, (====); seismology, (====, ====, ====, ====). An overview of general theory on point processes and some classical applications can be found in ====, ====, ====, ==== and ====.====, ==== and ====, or pseudolikelihood procedures, see for example ==== proposed a semiparametric intensity estimator and ==== and ====, ====, ==== for an earlier full description of the problem), meanwhile in the context of point processes it has received less attention. ==== showed the equivalence, for one-dimensional Cox processes, between ==== have addressed the bandwidth selection problem for ====’s kernel estimator suggesting a simple bandwidth estimate which does not use any model assumptions about the underlying process. Unfortunately this bandwidth choice is not designed to optimally balance the bias–variance trade-off of the intensity kernel estimator and then one cannot expect accurate intensity estimates in practice.====. The main difference between them is that marks are necessarily linked to the events, while covariates provide information about the whole observation region. ==== developed a kernel intensity estimator assuming that the intensity depends on some observed continuous covariates through an unknown continuous function. ==== proposed a slightly different estimator based on the same idea. Regarding the bandwidth parameters, ==== suggested a simple but computationally intense cross-validation method to choose the bandwidth for his estimator. And ==== did not specifically address the bandwidth selection problem for his estimator but used in practice the common rule-of-thumb borrowed from density estimation (see ====).====In this paper we revisit theoretically and empirically the traditional bias–variance trade-off for the case of inhomogeneous point process with covariates. First we construct a theoretical framework to guarantee the consistency of a new kernel intensity estimator based on covariates. The estimator is defined by using the relationship between density and intensity functions in a similar way to ====. Second we address the problem of bandwidth selection for the new kernel estimator. We suggest three new data-driven bandwidth selectors: a simple rule-of-thumb based on assuming normality and inspired on ====’s; a bootstrap estimator derived from a consistent resampling procedure; and an extension of the bandwidth selector proposed by ==== to the case of intensity depending on covariates. For simplicity we define the resampling procedure assuming that the underlying process is Poisson but other approaches are possible if specific information is available. Similarly we prove the consistency of both the kernel intensity estimator and the resampling procedure in the Poisson case. This Poisson assumption nevertheless seems not to be restrictive in practice as our empirical studies will show.====The paper accomplishes both theoretical and practical aspects but to make it easier to follow we have deferred most of the technical details to the appendices. Our theoretical developments are to the best of our knowledge new in the field. Although several methods already exist in the context of this paper, the theory is sparse in many ways. Particularly no theoretical bias–variance expressions, and hence no ad-hoc bandwidth selectors, have been previously developed for point processes with covariates. The practicality of our proposals is supported by simulations and a data analysis. Our simulations show a good performance of our proposals, which are competitive with other existing procedures. Simulations also show that our proposals perform well even when simulating from non-Poisson processes such as log-Gaussian Cox and Neymann–Scott processes, and considerably better than the currently existing methods in these contexts.====The theoretical framework we have set up in the paper is defined for one single one-dimensional covariate. In practice it is of interest to consider several covariates. The relationship between the intensity and the density function can be also considered to define a multivariate extension of our intensity estimator. Moreover this link opens a promising way of solving problems of practical interest in spatial statistics, just by bringing into spatial statistics what has been done for decades in the density context (see for example ==== and ====, as well as ====, for a recent revision on this topic). This paper fully describes this link in the one-dimensional case and details briefly the multivariate extension.====The rest of the paper is organised as follows. In Section ====, we briefly review kernel intensity estimation for point process with and without covariates. Section ==== is devoted to set up the theoretical framework for kernel intensity estimation with covariates and to develop asymptotic theory. In Section ==== a new resampling procedure is proposed based on smooth bootstrap and its consistency is shown. Section ==== describes the three new bandwidth selection methods. An extensive simulation study is carried out in Section ==== to analyse the finite-sample properties of the new proposals for Poisson and non-Poisson point processes, including a comparison with other existing competitors. Section ====, and finally some conclusions are drawn in Section ====. All computations have been performed in R. The wildfire data set and the R code with original functions to reproduce the application are provided as supplementary material.====Following ==== and ==== we develop the following result that allows us to establish the model for the transformed point processes in the covariate space:====The relationship between ==== and ==== can be extended to the expected number of events through this result adapted from ====:====Applying this result to the unnormalised spatial cumulative distribution function ==== we have: ==== Deriving with respect to ====, we get ====.====We can now rewrite the relationship between the original spatial point process intensity and the transformed one through an integral, which also implies that the expected number of events in the corresponding region is the same in both processes: ==== Hence, we can finally have a one-dimensional inhomogeneous Poisson point process.",Bootstrapping kernel intensity estimation for inhomogeneous point processes with spatial covariates,https://www.sciencedirect.com/science/article/pii/S0167947319302300,22 October 2019,2019,Research Article,377.0
"Selosse Margot,Jacques Julien,Biernacki Christophe","Laboratoire ERIC, 5 Avenue Pierre Mendès France, 69500 Bron, France,Université Lumière Lyon 2, 86 Rue Pasteur, 69007 Lyon, France,Université de Lille, UFR de Mathématiques, Cité Scientifique, 59655 Villeneuve d’Ascq Cedex, France,INRIA, 40, av. Halley, Bât A, Park Plaza, 59650 Villeneuve d’Ascq, France","Received 11 October 2018, Revised 7 October 2019, Accepted 7 October 2019, Available online 18 October 2019, Version of Record 26 October 2019.",https://doi.org/10.1016/j.csda.2019.106866,Cited by (13),"The importance of clustering for creating groups of observations is well known. The emergence of high-dimensional data sets with a huge number of features leads to co-clustering techniques, and several methods have been developed for simultaneously producing groups of observations and features. By grouping the data set into blocks (the crossing of a row-cluster and a column-cluster), these techniques can sometimes better summarize the data set and its inherent structure. The Latent Block Model (LBM) is a well-known method for performing co-clustering. However, recently, contexts with features of different types (here called mixed ==== sets) are becoming more common. The LBM is not directly applicable to this kind of data set. Here a natural extension of the usual LBM to the “Multiple Latent Block Model” (MLBM) is proposed in order to handle mixed type data sets. Inference is performed using a Stochastic EM-algorithm that embeds a ====, and allows for missing data situations. A ","Among the most famous co-clustering techniques, the Non-negative Matrix Tri-Factorization consists in factorizing the ==== data matrix ==== into three matrices ==== (of size ====), ====
 (====), ====
 (====), with the property that all three matrices have non-negative elements, see for instance ==== by ==== is achieved by minimizing the error function ====, with the constraints ====, meaning that all elements of ====, ==== and ==== are greater than ====, and ==== represents the ==== matrix: an element ==== of ==== summarizes the observations belonging to row-cluster ==== and column-cluster ====. Despite the non-negative property of the matrices, it is not always easy to interpret the resulting matrices. For example, matrices ==== and ====) can be used for model selection purposes, including the choice of the number of co-clusters. This technique has proved its efficiency in co-clustering several types of data: continuous (====), nominal (====), binary (====), ordinal (====), and functional (====, ====). However, when the variables are inherently correlated in a row-cluster, this model is not suitable. To overcome this issue, the authors of ==== for a complete survey).====However, none of these techniques were developed in a co-clustering framework. To the best of our knowledge, the only work to co-cluster heterogeneous data is ====) is adapted to the proposed model in order to select the number of row-clusters and column-clusters.====Co-clustering techniques can be seen as an efficient alternative method to the selection of variables thanks to its parsimony, especially in very high dimensions. In addition, it can produce interpretable sets of variables since it can group redundant variables or noisy variables. In this way, a first, naive answer is to manually select the informative blocks, but ==== alternatively define a model that automatically distinguishes the informative blocks for textual data sets. For mixed data, variable selection is more challenging. ==== perform clustering while incorporating variable selection and this method can produce homogeneous row-clusters. However, compared to co-clustering, it does not provide interpretable column-clusters, which may be essential for the summary of the data set, in particular with a high number of variables.====The paper is organized as follows. Section ==== gives an overview of the LBM to help understanding of this paper. Then, it proposes an extension to a new LBM version that allows heterogeneous data sets. Section ==== proposes an algorithm for model inference, based on a Stochastic Expectation Maximization (====, a description of the different types of data that can be taken into account with this method is given, and formulas for model inference are presented. Section ==== assesses the efficiency of the proposed method on simulated data while Section ==== provides a conclusion.",Model-based co-clustering for mixed type data,https://www.sciencedirect.com/science/article/pii/S016794731930221X,18 October 2019,2019,Research Article,378.0
"Arellano-Valle Reinaldo B.,Azzalini Adelchi,Ferreira Clécio S.,Santoro Karol","Departamento de Estadística, Pontificia Universidad Católica de Chile, Santiago, Chile,Dipartimento di Scienze Statistiche, Università di Padova, Italy,Departamento de Estatística, Universidade Federal de Juiz de Fora, Juiz de Fora, Brazil","Received 26 April 2019, Revised 26 September 2019, Accepted 30 September 2019, Available online 14 October 2019, Version of Record 24 October 2019.",https://doi.org/10.1016/j.csda.2019.106863,Cited by (11),"In the context of ====, the true unobservable ==== are commonly assumed to have a normal distribution. This assumption is replaced here by a more flexible two-piece normal distribution, which allows for asymmetry. After setting-up a general formulation for two-piece distributions, we focus on the case of the normal two-piece construction. It turns out that the joint distribution of the actual observations (the multivariate ",".====In our development, we adopt a classical approach to these MEMs, where the response variable ==== is a ====-dimensional random vector (====) and is related linearly to a single explanatory variable ====, we observe ====, where ==== is a random measurement error. The assumption ==== is there to avoid unidentifiability problems, or equivalently to avoid the assumption that ==== is known.====This formulation within the MEMs literature has already a long history. ==== used it for the comparison of four combinations of two instruments and two operators for measuring vital capacity. For examples of the use of MEMs in the medical field, see e.g. ====. Examples in agriculture are presented in ====; examples in psychology and education are presented by ====.==== and the latent variable ====. However, the normality assumption can be too restrictive and may suffer from lack of robustness against departure from the distributional assumption, with important impact on the inferences; on this issue, see ====, ==== and ====. Consequently, ==== have shown the advantage of adopting the skew-normal (SN) distributional assumption for the latent variable ==== later; see also the work of ====.====In the present paper, we develop a new class of flexible MEMs based on a two-piece normal (TPN) distribution in the form presented by ====; the TPN and other asymmetric distributions will be summarized in the next section. After that, in the core part of the paper, we assume that ==== follows a TPN distribution, while the measurement errors and model errors are assumed to be normally distributed. The different assumption on the distribution of ==== is the basic element of difference with respect to the formulation of ",A two-piece normal measurement error model,https://www.sciencedirect.com/science/article/pii/S016794731930218X,14 October 2019,2019,Research Article,379.0
"Taylor Simon A.C.,Sherlock Chris,Ridall Gareth,Fearnhead Paul","Department of Mathematics and Statistics, Lancaster University, Lancaster LA1 4YF, UK","Received 24 March 2019, Revised 25 July 2019, Accepted 21 September 2019, Available online 14 October 2019, Version of Record 24 October 2019.",https://doi.org/10.1016/j.csda.2019.106845,Cited by (1)," structure that we create for the former and an enforced, approximate, conjugate structure for the latter. A simulation study demonstrates the accuracy of our method, and inferences are consistent across two different datasets arising from the same rat tibial muscle.","Motor unit number estimation (MUNE) is a continuing challenge for clinical neurologists. Determination of the number of motor units (MUs) that operate a particular muscle provides important insights into the progression of various neuromuscular ailments such as amyotrophic lateral sclerosis (====, ====), and aids the assessment of the efficacy of potential therapy treatments (====).====A MU is the fundamental component of the neuromuscular system and consists of a single motor neuron and the ==== whose contraction it governs. Restriction to a MU’s operation may be a result of impaired communication between the motor neuron and muscle fibres, or abnormality in their function. A direct investigation into the number of MUs via a biopsy, for example, is not helpful since this only determines the presence of each MU, not its functionality.==== measurement of functioning MUs. The effect on the muscle may be measured by recording either the minute variation in muscle membrane potential or the physical force the muscle exerts (====). The methods developed in this article are applicable to either type of measurement. Since our data consist of whole muscle twitch force (WMTF) measurements we henceforth describe the response in these terms. In a healthy subject, the stimulus–response curve is typically sigmoidal (====), illustrating the smooth recruitment of additional MUs as the stimulus increases; however, the relatively low number of MUs in a patient with impaired muscle function may manifest within the stimulus–response relationship through large jumps in WMTF measurements.====MUNE uses the observed stimulus–response pattern to estimate the number of functioning MUs. Techniques for MUNE generally form two classes: the average and comprehensive approaches. The most common averaging approach is the incremental technique of ====). This occurs when two or more MUs have similar activation thresholds such that different combinations of MUs may fire in reaction to two identical stimuli. Consequently, the incremental technique tends to underestimate the average MUTF and hence overestimate the number of MUs. A number of improvements both experimentally (e.g. ====, ====) and empirically (e.g. ====, ====) have been proposed to try to deal with the alternation problem but, despite these improvements, each method oversimplifies the data generating mechanism and there is no gold-standard averaging approach; ==== and ==== provide thorough discussions on these approaches to MUNE.====A desire for a more complete model for the data generating mechanism motivated the comprehensive approach to MUNE in ====, which proposed three assumptions:====From these assumptions, ==== proposed a set of similar statistical models each of which assumed a different ====In a subsequent paper, ====) to sample from the MU-number posterior mass function directly. However, its implementation is highly challenging with slow and uncertain convergence particularly when the muscle has many MUs. This is partly attributed to difficulty in defining efficient and meaningful transitions between models, with transition rates found to be 0.5%–2% (====). The between-model transition rate was improved in ====, the principal inference targets are separate estimates of the marginal likelihood for a range of neuromuscular models, each assuming a different number of MUs.====The paper proceeds as follows. Section ==== presents the neuromuscular model of ==== for a fixed number of MUs and defines the priors for the model parameters. Section ==== describes the SMC-MUNE method itself. Section ==== assesses the performance of the SMC-MUNE method for ==== simulated datasets. Closer examination of cases where the point estimate of the number of MUs was incorrect revealed two classes of error; an example in each is investigated in detail. Section ==== presents a comparison of the SMC-MUNE method against the RJMCMC algorithm for the simulated datasets where the truth is known. Section ==== applies the SMC-MUNE method to data (collected using the method in ==== concludes the paper with a discussion on the effectiveness of SMC-MUNE and of potential avenues for improvement.====Evidence for specifying the upper bound ==== is taken from ==== where, for a Gaussian excitability curve, the coefficient of variation of a random variable whose cdf is given by the excitability curve was estimated to be 1.65%. With the log–logistic curve this corresponds to ====. Given that ====, we deduce that ====. The limitations of the study of ====, commented on by ====, indicate that a larger bound may be required than initially suggested, which was why sensitivity of MUNE to ==== was investigated in Sections ====, ====.====After assimilating the baseline observations ====, both ==== and ==== are known (and known to be small) with considerable certainty. Thus, given that we also know ====, approximating ==== as ==== and considering ==== to be a point mass at ==== is reasonable. Furthermore, the prior for ==== does not need to be set until just before the observation ==== is assimilated. Given the tight posterior for ==== at this juncture it is, therefore, possible to incorporate the knowledge that ==== into the vague prior for ==== (which is conceptually similar to specifying an initial joint prior on ==== and ====). Letting ====
 denote the posterior median of ==== at time ====, tuning parameters ==== and ==== are chosen such that ==== is desired. Given that ====where ==== is the cdf of a ==== variable evaluated at ====. In practice we specify the prior for ==== by defining a small ==== and then solving ==== for ====.",Motor unit number estimation via sequential Monte Carlo,https://www.sciencedirect.com/science/article/pii/S0167947319302002,14 October 2019,2019,Research Article,380.0
"Walder Adam,Hanks Ephraim M.","Pennsylvania State University, University Park, United States of America","Received 4 March 2019, Revised 27 September 2019, Accepted 30 September 2019, Available online 11 October 2019, Version of Record 28 October 2019.",https://doi.org/10.1016/j.csda.2019.106861,Cited by (6)," is proposed, along with conjugate samplers for LMAs with georeferenced and areal support. A ==== of SGLMMs with LMAs and GRFs is conducted over multiple data support and response types.","LMAs have received sporadic attention over the past decade as alternatives to GRFs (====, ====, ====). ==== to the case of Type-G Matérn random fields of which the LMA is a special case. In Section ====, we provide a summary of this extension for the symmetric LMA. Following ====, the LMA can similarly be expressed as a conditionally sparse Gaussian random field through the introduction of auxiliary data. We also provide insights for handling the computational issues associated with MCMC implementation.==== explored the LMA for geostatistical data with Gaussian responses. Though the discrete space model was claimed to be analogous, no further exploration was considered. ==== provided a Bayesian implementation of the graph trend filtering (GTF) estimates of ==== for SGLMMs. Our model can easily be implemented in place of any Gaussian conditionally autoregressive (CAR) or simultaneously autoregressive (SAR) model. In Section ====, we propose a novel MCMC implementation of our Bayesian hierarchical model for discrete space SGLMMs.====. Given the ease of implementation, and familiarity of inference, LMA models can be useful alternatives to GRF models.====The paper is organized as follows: In Section ==== we provide background material needed to develop our models. In Section ==== we discuss finite element approximations for continuous space LMAs. We also provide details related to the numerical issues involved with fitting LMAs via MCMC. In Section ==== we detail our discrete space LMA model and its relation to the GTF estimates of ====. In Section ==== we consider four datasets on which the LMA model is compared to its GRF counterpart. We conclude with a discussion in Section ====.====The following is the Supplementary material related to this article. ",Bayesian analysis of spatial generalized linear mixed models with Laplace moving average random fields,https://www.sciencedirect.com/science/article/pii/S0167947319302166,11 October 2019,2019,Research Article,381.0
"Yang Jing,Tian Guoliang,Lu Fang,Lu Xuewen","Key Laboratory of Computing and Stochastic Mathematics (Ministry of Education), College of Mathematics and Statistics, Hunan Normal University, Changsha, 410081, China,Department of Statistics and Data Science, Southern University of Science and Technology, Shenzhen, 518055, China,Department of Mathematics and Statistics, University of Calgary, Calgary, AB T2N 1N4, Canada","Received 28 February 2019, Revised 5 September 2019, Accepted 7 October 2019, Available online 11 October 2019, Version of Record 17 October 2019.",https://doi.org/10.1016/j.csda.2019.106867,Cited by (2)," is also presented for implementation. Finally, some simulation studies and two real data analysis are conducted to confirm the merits and theoretical findings of the novel method.","-covariate ==== that can capture most information about the relationship between the response variable ==== and covariate ====. Specifically, the SIM is defined by ====where ==== is the random error term satisfying ==== almost surely, ==== is an unknown link function, ==== is a ====-dimensional unknown single-index coefficient such that ==== and the first non-zero component is positive for model identification purpose.==== have been proposed by many researchers. ==== and ==== initially introduced an average derivative estimation (ADE) method, which was then improved by ====. Besides, ====. Other related references include ====, ====, ==== and ====.==== has attracted considerable efforts for robust estimation of SIM. Specifically, ==== considered a modified quantile version of MAVE method, and ==== consistency of the resulted estimator. ==== proposed a pseudo-profile likelihood approach for estimation and test of single-index quantile regression models. For more recent articles, readers are recommended to ====, ====, ==== and the references therein. It is worthy noting that although the QR method is robust, it may suffer from efficiency when there exists no outlier or the error distribution is normal. In particular, the above mentioned literatures about SIM were built on the idea of MAVE, which essentially is an iterative process between the index parameter and nonparametric function, thus the computation task will be heavy especially when the dimension of ==== is relatively high.====Recently, ==== investigated a new estimation procedure based on the local modal regression (LMR) in nonparametric model. Further, ==== and ==== extended the LMR approach to the single-index models and partially linear single-index models. ==== and ====The rest of this paper is organized as follows. In Section ====, we present the new estimation methodology by integrating the ideas of LMR and OPG. In Section ====, we establish the asymptotic properties of the developed estimators and further discuss the optimal choices of tuning parameters. In Section ====. All the technical proofs are collected in the ====.====To prove the theoretical results, the following lemmas will be frequently used in the sequel.====The detailed proofs of ====, ==== can be referred to ==== and ====, respectively.====Firstly, we consider the second term ====. Note that ==== Applying the Taylor expansion to ==== at the point ====, ==== can be expressed as ====Further rewrite ==== in the term ====, the above equality is equal to ====where ==== and ====. On the other hand, it follows from ==== that ====Therefore, based on ====, conditions (C1), (C4)–(C5) and the assumption that the initial value ====, we have ====Next, let us focus on the third term ====. Denote by ==== and ====, it follows from Theorem 2.1 of ==== that ====Hence, ==== holds due to the bandwidth assumption ====. Then we have ====
 Taking into account of conditions (C4), (C6) and the fact that ====, we have ====Consequently, by condition (C1) and ====, we can obtain ====Furthermore, it is easy to verify that ==== by conditions (C1)–(C3) and (C6). Then, combine the results of Eqs. ====, ====, ====, ==== leads to ====Recall that ====, it follows from ==== and Lemma 6.7 in ==== that ====where ====.====Now, we introduce the following defined operator ====: For any function ==== with ====, ====Let ====, then by conditions (C1), (C2) and (C4), we have ====Denote by ====it follows from condition (C2), ==== and Lemma 6.7 in ==== that ====Let ==== and ====. Assume that we start with ==== in the iteration, it is easy to verify that ==== holds based on the assumption on ====. Therefore, from Eq. ====, we have ==== where ====. Let ====, it follows that ====Notice that ==== and ====, we have ====, which indicates ==== and ==== hold. Hence, we can obtain that ====This means ====. Considering that the estimator ==== in the next iteration is the eigenvector of the matrix ====, then we can obtain from the conclusion of Lemma 3.1 in ==== that ====. That is, ====. Consequently, the asymptotic normality of ==== can be easily derived as ==== by the central limit theorem. This completes the proof.",Single-index modal regression via outer product gradients,https://www.sciencedirect.com/science/article/pii/S0167947319302221,11 October 2019,2019,Research Article,382.0
"Yi Fengting,Tang Niansheng,Sun Jianguo","Yunnan Key Laboratory of Statistical Modeling and Data Yunnan University, Kunming 650091, PR China,Department of Statistics, University of Missouri, Columbia, MO, 65211, USA","Received 24 January 2019, Revised 25 September 2019, Accepted 27 September 2019, Available online 11 October 2019, Version of Record 17 October 2019.",https://doi.org/10.1016/j.csda.2019.106848,Cited by (3),"Interval-censored failure time data often occur in many areas and their analysis has recently attracted a great deal of attention. On the other hand, most of the existing literature for them can only deal with time-independent ","Interval-censored failure time data often occur in many areas and their analysis has recently attracted a great deal of attention (====, ====Among others, one early reference on time-dependent covariates was given by ====, which discussed the relationship between the survival time of AIDS patients and their CD4 cell counts. ====, ====, ====, ====, ====). Of course, depending on the purpose of the analysis, one may take the joint modeling approach that models the longitudinal variable conditional on the failure time variable (====, ====, ====, ====, ====), or employ the approach that models the longitudinal and failure time variables together (====, ====, ====, ====, ====, ====, ====, ====, ====).====For either regression analysis of failure time data with time-dependent covariates or joint analysis of longitudinal and failure time data, it is apparent that a key is to take into account the relationship between the longitudinal and failure time variables such as biomarkers and disease onsite times and the heterogeneity of and measurement errors on the longitudinal variables. Among others, a common approach for this is to employ latent variables to connect the models for the longitudinal and failure time variables. For example, ====,==== and ==== adopted this approach and developed some two-stage and likelihood estimation procedures, respectively. More specifically, ==== and ==== considered the joint analysis under the proportional hazards (PH) model for the failure time of interest, while ==== discussed the joint analysis under the accelerated failure time model with the covariate following a linear mixed-effects model.====The remainder of the paper is organized as follows. We will first introduce in Section ==== the notation and models that will be used throughout the paper and then present the resulting likelihood function. In particular, we will employ the linear mixed-effects model for the time-dependent or longitudinal covariates and the PH model for the failure time of interest. In Section ====, for estimation, the maximum likelihood estimation procedure will be derived with the unknown function estimated in the completely nonparametric manner. For the determination of the proposed estimators, we will develop a MCEM algorithm that involves only low-dimensional parameter estimation in each iteration and thus is much more stable and faster than the algorithm given in ====. The idea discussed here applies to more general situations and some comments on this will be given below. Section ==== presents some results obtained from an extensive simulation study conducted to assess the finite sample performance of the proposed estimation approach, and they suggest that it works well in practice. In Section ====, the approach is applied to a set of longitudinal and failure time data arising from an AIDS clinical trial that motivated this study, and Section ==== contains some discussion and concluding remarks.====Let ==== with ====, ==== and ====, and ==== and ==== denote the true values of ==== and ====, respectively. Define ====
 ====and ====Also let ====
 ====
 ====, and ====, where ==== and ====. To prove the strong consistency of ====, we need the following regularity conditions and a lemma given below.==== The true values ==== and ==== belong to ==== with ==== being an interior point of ====, where ==== is a compact set of ==== with ==== and ==== is a set of cumulative distribution functions ==== with ====, ==== for some ==== and ====. Also assume that the cumulative hazard function ==== is continuously differentiable over ====.==== Given ==== and ====, the failure time ==== and the examination process are independent, depends only on ==== and ====. Given ====, the examination time ====’s have a continuous density function with the support being an interval ==== with ====.==== There exists an integer ==== such that ==== and ====. Moreover there exist a positive ==== such that ==== for each ====.==== The supports of both ==== and ==== are bounded.==== Assume that ====, and if there exists a constant vector ==== with probability 1 ====, then we have ====.",Regression analysis of interval-censored failure time data with time-dependent covariates,https://www.sciencedirect.com/science/article/pii/S0167947319302038,11 October 2019,2019,Research Article,383.0
"García-Ródenas Ricardo,García-García José Carlos,López-Fidalgo Jesús,Martín-Baos José Ángel,Wong Weng Kee","Departamento de Matemáticas, Escuela Superior de Informática, Universidad de Castilla la Mancha, 13071–Ciudad Real, Spain,Universidad de Navarra, ICS, Campus Universitario, 31080–Pamplona, Spain,Department of Biostatistics, University of California, Los Angeles, USA","Received 9 October 2018, Revised 1 August 2019, Accepted 21 September 2019, Available online 10 October 2019, Version of Record 17 October 2019.",https://doi.org/10.1016/j.csda.2019.106844,Cited by (18),- and ,", ==== or ====, to mention a few.====High dimensional models are of increasing interest because they reflect studies more realistically and aided by increase in computer power. For ==== to construct optimal computer experiments. Such algorithms have also been used to provide optimal estimates for investigating efficacy of dual lung cancer screening by chest X-ray and sputum cytology (====) and estimating parameters in a nonlinear mixed PK/PD model for a pharmaceutical application (====) are excellent algorithms for solving optimization problems over a discrete search space.==== compared deterministic algorithms, and ==== compared among metaheuristic algorithms. The scope of our work is therefore broader and more ambitious in that we compare performances across different types of algorithms and ascertain whether nature-inspired metaheuristic algorithms or their hybrids tend to outperform deterministic algorithms, on average, for searching optimal designs.====The main contributions in this paper are:====In Section ====, we review the statistical background, different types of designs and various design criteria. Section ==== discusses different types of algorithms for optimization and they include exact methods and nature-inspired metaheuristic algorithms, and concludes with a subsection on how the latter may be hybridized for better performance. Section ==== presents a summary of our results and general recommendations concerning the choice of algorithms for generating optimal experimental designs.",A comparison of general-purpose optimization algorithms for finding optimal approximate experimental designs,https://www.sciencedirect.com/science/article/pii/S0167947319301999,10 October 2019,2019,Research Article,384.0
"Shen Pao-sheng,Hsu Huichen","Department of Statistics, Tunghai University, Taiwan, ROC","Received 9 May 2019, Revised 29 September 2019, Accepted 2 October 2019, Available online 9 October 2019, Version of Record 11 October 2019.",https://doi.org/10.1016/j.csda.2019.106862,Cited by (8),"Doubly truncated data arise when a failure time ==== is observed only if it falls within a subject-specific, possibly random, interval ====, where ==== and "," and ==== denote the calendar times of the occurrence of the first and second event, respectively. Under double truncation, individuals are selected into the sample if and only if their second events occur within the calendar time interval ====, i.e., ====. Let ==== denote the lifetime of interest. Let ==== and ====, where ==== denote the left and right truncation times, respectively. Let ==== be a ====. Under double truncation, ==== is observed if and only if ====, i.e., ==== is doubly truncated by ==== and ====. ==== highlights all the different times for doubly truncated data described above.==== proposed local constant and local linear kernel-type estimators for nonparametric regression with a doubly truncated response. ==== proposed estimators based on estimating equations (EE). However, the EE estimator suffers from large estimation variance. Frank and ==== investigated non-parametric estimation for a linear regression model under double-truncation. For doubly truncated data, based on the density function of observed lifetimes and the random sample size, ====) has often been used in the analysis of survival time and related data. Under the PH model, ====. ====. ==== extended the pseudo likelihood approach to semiparametric transformation models. ====All the above methods require the assumption that ==== is independent of ====. Specifically, the approaches of ==== and ==== rely on consistency of the estimator of ====, which requires the independence assumption. Similarly, the pseudo likelihood approaches of ====, ==== and ====, which also requires the independence assumption. In practice, the assumption of independence can be violated. For example, for the age at onset of the Alzheimer’s diseases, factors such as depression and stress are associated with delayed study entry. Since these factors are correlated with survival, the left truncation time ==== and survival time ==== are not independent. In literature, there exist many statistical methods available for analyzing survival data with dependent left-truncation (e.g. ====, ====, ====, ====, ====, ====, ====, ====) and dependent right-truncation (====). Recently, ==== and ==== are independent given ====. They proposed an expectation–maximization (EM) algorithm to obtain the conditional maximum likelihood estimators (cMLE) and established their asymptotic properties. Their simulation study demonstrates that the cMLE performs well.====In this article, we consider fitting semiparametric transformation regression models to doubly truncated data. In Section ====, simulation studies are conducted to investigate the performance of the cMLE. In Section ====, the proposed method is illustrated using an AIDS dataset.====Suppose that ==== takes the possible values ====, the marginal likelihood ==== can be written as ====where ==== are ==== discrete data of ====’s from the subgroup ====. For ====, let ====Then ====Thus, when ==== is discrete the marginal likelihood ==== can be treated as a product-multinomial likelihood (====, ====), i.e. any ==== that satisfies ====maximizes ====. Given estimators ==== and ====, let ====and ====It follows that ====. Hence, ==== is satisfied, i.e., ==== is maximized. Note that the maximum value of the marginal likelihood remains a constant ==== for any ==== and ====. Hence, it suffices to maximize ====. The proof is complete.",Conditional maximum likelihood estimation for semiparametric transformation models with doubly truncated data,https://www.sciencedirect.com/science/article/pii/S0167947319302178,9 October 2019,2019,Research Article,385.0
"Santitissadeekorn Naratip,Lloyd David J.B.,Short Martin B.,Delahaies Sylvain","Department of Mathematics, University of Surrey, Guildford, Surrey, UK,Department of Mathematics, Georgia Institute of Technology, Atlanta, GA, USA","Received 27 February 2019, Revised 27 September 2019, Accepted 27 September 2019, Available online 8 October 2019, Version of Record 21 October 2019.",https://doi.org/10.1016/j.csda.2019.106850,Cited by (8)," is achieved by the filtered intensity. The potential of using filtered intensity to improve police patrolling prioritisation is also tested. By comparing with the prioritisation based on MLE-derived intensity and historical frequency, the result suggests an insignificant difference between them. While the filter is developed and tested in the context of urban crime, it has the potential to make a contribution to data assimilation in other application areas.","A conditional intensity process (as defined in ====) model is a powerful tool for investigating and predicting count data (either in the form of the number of events in a given time period or a time-series of occurrence times) commonly found in a wide variety of applications such as seismology (====, ====), epidemic disease outbreaks (====, ====), and urban crime (====). In the context of urban crime, a typical intensity process is the Hawkes process (====, ====, ====), which employs the fact that the Hawkes process can be considered as a branching process with immigration (====). Once the MLE parameters are obtained, the intensity at any time ==== can be deterministically computed given the history of event times up until ==== while the projection of the intensity into the future must be stochastically simulated via one of several possible techniques (====, ====.==== given the history of observations up to and including time ==== – for the Hawkes process or other conditional intensity processes and investigates its application to urban crime data analysis and prediction. Developing filtering algorithms to help track and quantify uncertainty in real-time for intensity process models of count data could have impacts on a range of applications such as influenza forecasting. It also offers distinct advantages over the typical (non-filtering) MLE approach, where a large batch of data is used all at once instead of sequentially assimilating one observation at a time. One such advantage is the ability of a filtering algorithm to dynamically track model parameters over time. Nonetheless, it was shown in ====A step in this direction was recently carried out by ==== and is allowed to vary stochastically with time. We believe that this consideration may be more useful to some real-world data than MLE, which only estimates static parameters. An extended Poisson–Kalman filter (ExPKF) is derived following a similar idea of the extended Kalman filtering framework applied to the Poisson likelihood function and requires the computation of the gradient and Hessian of the underlying intensity models but can be applied to multivariate intensity models.====, ====).====One of the key goals for modelling crime and analysing crime data is to give an insight into an effective strategy of reducing crimes. Several data-driven approaches to crime prediction have attempted to incorporate predictive policing into their patrolling strategies so that a limited number of officers can be most efficiently allocated at the right place and time. For example, recent randomised field-trial conducted with police departments has shown positive results for some regions in UK and United States (====) in helping patrol officers to identify “at the moment” hotspots. Here the term hotspot roughly refers to the region in which the crime rate is relatively high compared to its neighbours. The model used by ==== is the Epidemic Type Aftershock Sequence (ETAS) model (====, ====) where the model parameters are estimated by MLE via the EM algorithm. Motivated by this application, we also carry out an experimental study below to compare the goodness-of-fit as well as predictive skill of the ExPKF and MLE in the context of patrol prioritisation in a neighbourhood of Chicago.====The Appendix provides a brief description of the MATLAB codes (version R2018a) used for Sections ====, ====, ==== in this work as well as the (anonymised) data. The step-by-step explanation can be found in the scripts that run these codes.",Approximate filtering of conditional intensity process for Poisson count data: Application to urban crime,https://www.sciencedirect.com/science/article/pii/S0167947319302051,8 October 2019,2019,Research Article,386.0
"Lai Yuanhao,McLeod Ian","Western University, London, Ontario, Canada","Received 19 September 2018, Revised 25 September 2019, Accepted 26 September 2019, Available online 5 October 2019, Version of Record 11 October 2019.",https://doi.org/10.1016/j.csda.2019.106849,Cited by (3), inputs.,"The class prediction problem with ====-dimensional input ==== and output variable ====, where ====, is considered. For each class ====, ==== is a ====-multivariate random vector generated from the multivariate probability distributions ====. The family of component-wise distance-based discriminant rules is defined by, ====where ==== is a test input, ==== is the ====th marginal distribution of ====, and ==== is the distance between ==== and ====
 (====, ====, ====). The optimal prediction is ==== where ==== is the mean of ====, ====, ====). When the input ====, where ====, ==== and ====, often has better performance. ==== Section 4.1), is used to estimate the parameters ==== or ==== for ==== and ====.====). The Tukey mean difference plot (==== p.21) was invented to compare data from such distributions. ==== extended MC to the quantile-based classifier (QC) defined by ====, where ==== is the ====-quantile of ==== for ==== and ====is the quantile distance function (====, ====). When ====, QC reduces to MC. ==== showed that the QC can provide the Bayes optimal prediction with skewed input distributions. The usefulness of QC was demonstrated by simulation as well as an application (====). An R package which implements the centroid, median and quantile classifiers is available (====).====Although QC is effective for discriminating high-dimensional data with heavy-tailed or skewed inputs, it suffers from the restriction of assigning each variable the same importance, which limits its effectiveness when there are irrelevant extraneous inputs. Another limitation for QC and the median centroid classifier with high dimensional data may be noise accumulation. ==== proved that the centroid classifier may perform no better than random guessing due to noise accumulation with high dimensional data.====Our proposed ensemble quantile classifier (EQC), presented in Section ====, is a flexible regularized classifier that aims to overcome these two limitations and provides better performance with high-dimensional data, asymmetric data or when there are many irrelevant extraneous inputs. We introduce the binary EQC for discriminating observations into one of two classes and then extend it to situations with more than two classes. In ==== of Section ====, it is shown that sample loss function of EQC converges to the population value when the sample size increases. In Sections ====, ====, the improved performance of EQC is demonstrated by a simulation study and an application to text categorization.====A random variable ==== is said to follow the asymmetric Laplace distribution, denoted as ====, if its probability density function has the form, ====where ====, ==== and ==== respectively are the location, the scale and the skewness parameters.====Let ==== and ==== be the prior probabilities of ==== and ====. If ==== and ==== consist of independent asymmetric Laplace distribution with parameters ==== and ====, then the Bayes decision boundary becomes ==== with, ====where for ====, ====Since ==== is also the ====-quantile of an asymmetric Laplace distribution, if we let ====, Eq. ==== will become the ==== of EQC in Eq. ==== with ==== and ==== for ====. If ====’s are rescaled by its standard deviation ==== first, then the ==== will become ====Therefore, we can see that the decision boundary given by the EQC is the Bayes decision boundary in this special case while QC cannot be if ==== is not homogeneous.",Ensemble quantile classifier,https://www.sciencedirect.com/science/article/pii/S016794731930204X,5 October 2019,2019,Research Article,387.0
"Han Ningren,Ram Rajeev J.","Massachusetts Institute of Technology, 77 Massachusetts Avenue, Cambridge, MA 02139, USA","Received 24 November 2018, Revised 21 September 2019, Accepted 24 September 2019, Available online 1 October 2019, Version of Record 15 October 2019.",https://doi.org/10.1016/j.csda.2019.106846,Cited by (8),A two-stage algorithm based on ,"The ability to directly probe the vibrational and ====Extracting concentration or quantity information pertaining to certain chemicals of interest from a complex mixture spectrum is one of the central themes of modern chemometrics research. ====), principle component regression (PCR) (====) among others (====Another complication typically associated with Raman spectrum processing is baseline estimation and correction (====, ====). A number of algorithmic techniques exist to facilitate automatic baseline estimation and correction (====, ====, ====, ====, ====, ====). However, as mentioned in ====, most of these methods aim at estimating the baseline signal alone without jointly estimating the peak signals, which may bring the potential risk of introducing bias and errors from these separate steps into the estimation.====, ====, ====, ==== and ====, we provide the statistical framework and computation procedure for our two-stage algorithm, where the first stage is used to learn the peak information for the pure target analyte spectrum and the second stage is for quantifying its concentrations in mixtures. In Section ====With Gibbs sampling, ==== can be updated with an inverse-gamma distribution as ====
 ==== can be updated as ====Denoting ==== as the maximum likelihood (ML) estimation of ==== and ==== as the squared residue of the ML estimation, we define ====With this definition, our posterior for ==== and ==== can be updated as ====and ",Bayesian modeling and computation for analyte quantification in complex mixtures using Raman spectroscopy,https://www.sciencedirect.com/science/article/pii/S0167947319302014,1 October 2019,2019,Research Article,388.0
"McCloud Nadine,Parmeter Christopher F.","Department of Economics, The University of the West Indies at Mona, Kingston, Jamaica,Department of Economics, Miami Business School, 5250 University Drive, Miami, FL, 33146, United States of America","Received 12 December 2018, Revised 22 September 2019, Accepted 25 September 2019, Available online 30 September 2019, Version of Record 9 October 2019.",https://doi.org/10.1016/j.csda.2019.106843,Cited by (7),"The ==== for mixed, continuous, and discrete densities. Simulations validate the theoretical contributions. Several empirical examples demonstrate the usefulness of the method suggesting that calculating the effective number of parameters of a ==== maybe useful in interpreting differences across estimators.",", ====, ====). These approaches, following ====Imagine now a researcher wishes to know the degrees of freedom stemming from their density estimate. Take for example ====. Here there are two parameters and as ==== changes the curvature of the density of ==== changes. But, regardless of the value of ====, there are always ==== degrees of freedom. If they were instead to estimate the density of ====Here we argue that what might be viewed as the appropriate hat matrix for the standard kernel density estimator is not an ideal candidate to use for computing its effective number of parameters or, subsequently, the implied degrees of freedom. In light of this limitation, we transform the kernel density estimator to resemble a local constant ==== and consequently draw on the extant literature on calculation of the hat matrix in nonparametric regression to construct an appropriate hat matrix. This simple trick allows easy estimation of the effective number of parameters used by the kernel density estimator.====We show that the speed at which the bandwidth decays to zero impacts, as would be expected, the number of effective parameters in the model, essentially quantifying smoothness of the kernel density estimate through degrees of freedom. Additionally, we demonstrate theoretically that the trace of our implied kernel density hat matrix converges in ==== to a multiple of the cardinality of the support of the data. Simulations are provided that validate these theoretical claims. Lastly, two empirical examples provide practical insights from the proposed approach both between parametric and nonparametric estimates and across nonparametric estimates with differing bandwidths.====Why might knowledge of the effective number of parameters be important for practitioners? This metric could assist in understanding how well a density fits the data relative to a parametric density, or it might serve as a useful comparison between two different nonparametric density estimators using different bandwidth selection devices. Knowledge of the degrees of freedom could serve as a baseline for selection across a set of density estimates or prove useful in understanding the power properties of competing tests.==== who modify an initial kernel density estimate, known as the carrier density, with a parametric exponential scaling function. The addition of this parametric scaling function “…allows the nonparametric smoother to use a substantially greater window width without badly degrading the overall fit to the data.” ====. As our theory will demonstrate, this larger bandwidth for the carrier density will result in fewer estimated effective parameters. ====’s (====The idea of converting a nonparametric density estimator to allow comparison with traditional parametric methods has also received attention. ==== estimation of the parameters of a regression model but produces asymptotically efficient estimates as well (see also ====, ====). ==== implement an idea similar for nonparametric regression. Our approach to calculating the effective number of parameters could be deployed here as well to determine how different the nonparametric density is (in terms of parameters) to a user-specific parametric density.====The following is the Supplementary material related to this article. ",Determining the Number of Effective Parameters in Kernel Density Estimation,https://www.sciencedirect.com/science/article/pii/S0167947319301987,30 September 2019,2019,Research Article,389.0
"Rodríguez Carlos E.,Núñez-Antonio Gabriel,Escarela Gabriel","Department of Probability and Statistics, IIMAS-UNAM, Mexico,Department of Mathematics, Universidad Autónoma Metropolitana (Iztapalapa), Mexico","Received 17 January 2019, Revised 17 September 2019, Accepted 19 September 2019, Available online 27 September 2019, Version of Record 1 October 2019.",https://doi.org/10.1016/j.csda.2019.106842,Cited by (8),"Clustering complex circular phenomena is a common problem in different scientific disciplines. Examples include the clustering of directions of animal movement in the wild to identify migration patterns, and the classification of angular positions of meteorological events to investigate seasonality fluctuations. The main goal is to develop a novel methodology for clustering and classification of circular data, under a ","Directional data arise from observations that can be represented as unit vectors in a ====-dimensional space, with the sample space being the unit sphere with dimension ====. Circular data are a particular case of directional data when directions can be represented in a ====-means as in ====). For the inference of an unknown number of components, ==== used reversible jump MCMC (====, ====) used a birth–death process, and ==== proposed an alternative method requiring that the component parameters can be integrated out of the model analytically. Recently, ====.====, a proposal is to undo the label switching deterministically, finding permutations of the parameters that minimize a loss function (e.g. ==== and ====). This can be regarded as a partition problem, where the latent allocations define partitions over the observations; here, an optimal partition has to be chosen from a sample of partitions (e.g. ====).====Several procedures have been proposed for the clustering of linear data, see e.g. ==== and ====; however, cluster analysis for circular data has not been fully developed, despite its potential implementation in various disciplines (e.g. ====, ====, ====, and ====, such as the unit circle, the complexity increases. On the one hand, following a non-parametric approach, the difficult part is to define coherent measures of distance that allow the grouping of circular data, and these data have a periodic behavior. On the other hand, if we adopt a model-based perspective, we need to use distributions defined over the unit circle, and these always come with some challenges. A general approach has been simply to adapt procedures for linear data to the analysis of circular data (e.g. ====); nevertheless, such an approach is ill-specified since it fails to consider the inherent topology of the unit circle.====In this paper, we devise a methodology for clustering circular data under a Bayesian mixture modeling setting. First, we set a finite mixture model with an unknown number of components, assuming that each component follows a projected normal distribution defined on the unit circle. Second, we perform model selection by jointly making inferences about the parameters of the mixture and the number of components. Third, we select the model with highest posterior probability, and use a deterministic relabeling strategy to recover ==== for its mixture components. Finally, we find estimates of both the posterior classification probabilities for the draws of observations and the densities for each cluster.====The rest of this paper is organized as follows. In Section ====, we present the model and describe some of its characteristics. Our MCMC strategy to sample from the posterior distribution of parameters for each component of the mixture, as well as for the number of components, is described in Section ====; here, we first deal with the case of a fixed number of components and then move to the trans-dimensional case. In Section ====, we discuss an approach to deal with the label switching problem, which is based on the data and a deterministic relabeling strategy. In Section ====, we illustrate and compare our methodology with other alternatives using simulated data, and we also analyze two real datasets. We conclude with a discussion and an outline of future work.====The following is the Supplementary material related to this article. ",A Bayesian mixture model for clustering circular data,https://www.sciencedirect.com/science/article/pii/S0167947319301975,27 September 2019,2019,Research Article,390.0
"Duan Jin-Chuan,Fulop Andras,Hsieh Yu-Wei","NUS Business School, Risk Management Institute and Department of Economics, National University of Singapore, Singapore,Department of Finance, ESSEC Business School, France,Department of Economics and Dornsife Institute of New Economic Thinking, University of Southern California, USA","Received 8 October 2018, Revised 30 August 2019, Accepted 17 September 2019, Available online 25 September 2019, Version of Record 5 October 2019.",https://doi.org/10.1016/j.csda.2019.106841,Cited by (3),"A data-cloning ==== algorithm is proposed as a general-purpose, global optimization routine for the ==== of models with latent variables. In the SMC==== phase, the method first marginalizes out the latent variable(s) by applying one layer of SMC at a fixed parameter value and then searches for the optimal parameters through another layer of SMC. The data-cloning phase is deployed to ensure global convergence by dampening multi-modality and to reduce the Monte Carlo error associated with SMC. This new method has broad applicability and is massively parallelizable through leveraging modern multi-core CPU or ==== computing.",", among many others) or Simulated Annealing (SA), are often adopted in practice. Further details regarding SA can be found in ====—the joint (posterior) distribution of the parameters of interest ==== and the latent variables ====—via MCMC.====In this paper, we propose a data-cloning ====, ====) and financial time series (====). Instead of sampling from ====, they propose to sample the joint distribution of the parameter ==== and ==== ==== of the latent variables, ====. The name data-cloning comes from the fact that their algorithms require computing the product of ==== conditional densities ====, where ==== is the observed data. This “cloned” likelihood can be understood as independently repeating another ==== experiments in which all the realizations happen to be the same ====. This extended target has a marginal distribution of ==== that is proportional to the likelihood raised to the power of ====. Therefore, when ==== is sufficiently large, the target distribution concentrates itself around the sample MLE and “flatten” all other local maxima in a way akin to SA. Sampling ==== is therefore equivalent to performing (global) optimization.====The key to data-cloning’s success relies on an effective sampler for the extended target ==== is more robust than single-chain MCMC methods, its efficiency still critically hinges on using reasonably efficient MCMC kernels over the extended state–space to move particles, which may be hard to come by in general.====To overcome the practical difficulty associated with sampling the joint density of parameters and latent variables, the recent progress on Particle MCMC (====) and ====
 (====, ====, ====Motivated by these concurrent developments, we devise a data-cloning ==== method by incorporating data cloning into the density-tempered ==== sampler of ====. Specifically, we sequentially sample from a sequence of targets with marginal distributions proportional to the likelihood raised to the power of ====. When ==== (no cloning), our algorithm is essentially that of ====The remainder is organized as follows. Section ==== provides an overview of the proposed method. Section ==== details the algorithm, and Section ==== concludes with additional discussions on simulation techniques to further improve the computational efficiency.",Data-cloning SMC,https://www.sciencedirect.com/science/article/pii/S0167947319301963,25 September 2019,2019,Research Article,391.0
Spezia Luigi,"Biomathematics & Statistics Scotland, Craigiebuckler, Aberdeen, AB15 8QH, United Kingdom","Received 28 September 2018, Revised 21 August 2019, Accepted 14 September 2019, Available online 23 September 2019, Version of Record 3 October 2019.",https://doi.org/10.1016/j.csda.2019.106840,Cited by (4), are available it is worthwhile selecting the subsets of variables which might affect most each row of the ====. A ," developed a HMM with a non-homogeneous Markov chain (NH-MC), whose transition probabilities depended on multivariate Normal exogenous variables, to analyse the precipitation occurrence in different sites. That model was modified by ====, ====, and ==== to include precipitation amounts, ==== to generate runoff scenarios, and ==== and ====, whereas ====The logit approach is currently the most popular for non-homogeneous hidden Markov chains (NH-HMCs) and was adopted, among others, by ==== to model US industrial production, ==== for rainfall amounts, ==== for portfolio defaults, ==== for the US three-month treasury bill rates, ==== for employment growth rates, ==== for relative sea surface elevation, ==== for a two-pillar Phillips curve for the Euro area, ==== for precipitation amounts, ==== for air temperatures, ==== for the US industrial production.====, ====, and ====As an alternative to the previous approaches, NH-HMCs were obtained by ==== and ====One of the major issues when dealing with NH-HMCs is the selection of the exogenous variables that affect the dynamics of the hidden states. ==== compared four different techniques to select the covariates affecting each row of the time-varying transition matrices: Stochastic Search Variable Selection of ====, ====) and ecological (====, ====) models including NH-HMCs. ==== selected a single subset of covariates affecting all transition probabilities by a method based on the reversible jump MCMC algorithm of ====.====In this paper a novel evolutionary Monte Carlo (EMC) algorithm is proposed for the selection of exogenous variables affecting the different rows of the transition matrices in a NH-HMM. EMC is an MCMC method which processes a population of chains in parallel, with a different temperature attached to each chain. EMC was introduced by ==== for drawing samples (also called individuals) from binary distributions. ====), i.e. mutation (update of a single individual), crossover (partial update of two individuals) and exchange (full update of two individuals), and performing simulations at various temperatures, as in simulated annealing (====EMC can be included in the class of population MCMC (PMCMC), as defined by ====, who compared the performance of random walk Metropolis, genetic, and PMCMC algorithms. Other significant contributions to EMC and PMCMC methods are as follows: EMC was generalized by ====; four moves for the EMC were developed by ====, who also introduced a strategy for designing the temperatures; a review of PMCMC methods was presented by ====, who subsequently proposed a trans-dimensional PMCMC (====); PMCMC methods were used in model choice by ==== and ====; a PMCMC called Distributed EMC was introduced by ====; PMCMC algorithms to generate multiple history matched models in reservoir modelling were presented by ====.====The plan of the paper is as follows. NH-HMMs are introduced in Section ====; Section ==== describes the EMC algorithm for variable selection; Section ==== presents the results of various simulated examples, where the covariates were either uncorrelated or correlated, with increasing levels of correlations; an application to ozone dynamics is illustrated in Section ====, where results about covariate selection, choice of the number of hidden states, parameter estimation, hidden chain reconstruction, and classification are shown; a discussion finalizes the paper.",Bayesian variable selection in non-homogeneous hidden Markov models through an evolutionary Monte Carlo method,https://www.sciencedirect.com/science/article/pii/S0167947319301951,23 September 2019,2019,Research Article,392.0
"Duarte-López Ariel,Pérez-Casany Marta,Valero Jordi","Data Management Group (DAMA - UPC), Spain,Department of Computer Architecture, Technical University of Catalonia, Barcelona, Spain,Department of Statistics and OR, Technical University of Catalonia, Barcelona, Spain","Received 18 March 2019, Revised 9 September 2019, Accepted 10 September 2019, Available online 18 September 2019, Version of Record 24 September 2019.",https://doi.org/10.1016/j.csda.2019.106838,Cited by (4), describing the interaction between members of a given platform have been fitted. The results have been compared with the fits obtained through other bi-parametric distributions.," uses the PL distribution to fit the tail of data sets related to: the number of copies of books sold in the US from 1895 to 1965; the populations of US cities; or earthquake magnitudes. Additional examples from computer science are presented in the review by ====, where the authors use a PL distribution to adjust several Internet measures such as YouTube video popularity and web access, among others. In addition, the analysis by ====. Other practitioners have developed mechanisms that mimic scenarios where the distribution appears (see ====, ====). In general, the PL distribution is considered appropriate when one observes data distributed over orders of magnitudes and extreme values are not rare, which is quite common in nature.====Most of the references cited above fit the PL distribution in the tail of the data, since it is not able to adjust the top-concave pattern drawn by the first values in the support of the variable. For ====, the PL usually emerges for values greater than a given threshold that divide “the Gaussian and Paretian worlds”. This philosophy is also pointed out in ==== and ====. The real existence of PL distributed data was analyzed in the recent work by ====, where the authors conclude that only a small amount of the analyzed data sets are appropriately fitted by a PL distribution.==== and renamed by Douglas in 1971 to ==== and ====, where ==== is Poisson distributed, the PSS is defined as the distribution of the sum of ==== independently and identically distributed (i.i.d.) copies of ==== compares the performance of several PSSs used to model citation data. For an application to insurance data, see ====The new discrete family of probability distributions is denoted by ==== (Zipf–PSS). The Zipf–PSS is a bi-parametric distribution that depends on the ==== parameter of the Zipf and the ====, ====). Some important properties related to the model are proved, for instance: the linearity of the tail. The Zipf–PSS distribution is implemented in the R-package ==== (====), which is available at CRAN.====The proposed model is shown to be suitable through the analysis of a case study where the Zipf–PSS distribution is used to fit several degree sequences related to the independent annual networks of the MathOverflow community. The fit of the Zipf–PSS is compared with those achieved by the negative binomial (NB) and the discrete Weibull (DW) (====The rest of the article is organized as follows. Section ==== introduces the Zipf distribution and the concept of PSS, then defines the notion of regularly varying function required to prove the tail linearity of our model. Section ==== defines the Zipf–PSS distribution and analyzes its main properties. Section ==== presents the analysis of several real degree sequences. Finally, Section ==== gives some recommendations for using the Zipf–PSS distribution in random network generation.",The Zipf–Poisson-stopped-sum distribution with an application for modeling the degree sequence of social networks,https://www.sciencedirect.com/science/article/pii/S0167947319301938,18 September 2019,2019,Research Article,393.0
"Ma Yingying,Lan Wei,Zhou Fanying,Wang Hansheng","School of Economics and Management, Beihang University, China,School of Statistics and Center of Statistical Research, Southwestern University of Finance and Economics, China,Guanghua School of Management, Peking University, China","Received 30 March 2019, Revised 20 July 2019, Accepted 22 August 2019, Available online 18 September 2019, Version of Record 23 September 2019.",https://doi.org/10.1016/j.csda.2019.106833,Cited by (3), to ,").====To account for network dependency, a spatial autoregressive (SAR) model with covariates is employed; see, e.g., ====, ====, ====, and ====, ====, ====, ====, and ====.==== to ====The rest of the article is organized as follows. Section ====. The article is concluded with a short discussion in Section ====. All the technical details are relegated to ====.====This ==== includes two parts, which introduced the proofs of ====, ====, respectively.",Approximate least squares estimation for spatial autoregressive models with covariates,https://www.sciencedirect.com/science/article/pii/S0167947319301884,18 September 2019,2019,Research Article,394.0
Gaigall Daniel,"Leibniz University Hannover, Institute of Probability and Statistics, Welfengarten 1, 30167 Hannover, Germany","Received 31 March 2019, Revised 30 June 2019, Accepted 31 August 2019, Available online 16 September 2019, Version of Record 19 September 2019.",https://doi.org/10.1016/j.csda.2019.106837,Cited by (3),The Rothman–Woodroofe symmetry test statistic is revisited on the basis of independent but not necessarily identically distributed ====. The distribution-freeness if the underlying distributions are all symmetric and continuous is obtained. The results are applied for testing symmetry in a meta-analysis random effects model. The consistency of the procedure is discussed in this situation as well. A comparison with an alternative proposal from the literature is conducted via simulations. Real data are analyzed to demonstrate how the new approach works in practice.,", ====, ====, ====, ====, ====, and ====, for instance. If the underlying distribution is continuous, the Rothman–Woodroofe symmetry test statistic serves a distribution-free testing criterion for the null hypothesis of symmetry which is also consistent if the sample size tends to infinity, see ====. Moreover, the Rothman–Woodroofe symmetry test statistic has the useful property of invariance under reflecting all data at zero. Of course, there exist other tests for the treatment of the nonparametric testing problem of symmetry if the underlying random variables are independent and identically distributed, see, e.g., ====, ====, and ====.====We apply the Rothman–Woodroofe symmetry test statistic to independent but not necessarily identically distributed random variables. Estimating and especially testing problems on the basis of independent but not necessarily identically distributed random variables are also under consideration in ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, and ====. An alternative expression of the Rothman–Woodroofe symmetry test statistic is derived in Section ====. Although we do not deal with identically distributed data, it turns out that the test statistic is distribution-free if the underlying distributions are all symmetric and continuous, see Section ====Meta-analysis is a generic concept for combining and interpreting results of different studies. The topic is frequently treated in literature, see ==== for a broad overview. Common models in meta-analysis are fixed effect models and random effects models. Fixed effect models assume homogeneity across the studies, where random effects models take heterogeneity into account. Random effects models have many applications, especially in medical research, see, e.g., ====, ====, ====, and ====, ====, and ====The type of data acquisition in meta-analysis is categorized in individual participant data approaches and aggregate data approaches. Individual participant data approaches make use of raw data from all studies, where aggregate data approaches use summary data published in study reports. An obvious approach for dealing with individual participant data from different studies would be to use a test applicable for independent and identically distributed data in each study, and combine the results with the help of a multiple testing procedure. In fact, this method has two disadvantages. At first, the well-known multiple comparisons problem arises if the number of studies is large. Second, the whole approach is not applicable if the classification of the data in the studies is unknown.====We apply the Rothman–Woodroofe symmetry test statistic and our general results for testing symmetry on the basis of individual participant data in a meta-analysis random effects model, see Section ====. This approach is also applicable if the classification of the data in the studies is unknown. Moreover, the usage of a single test circumvents the multiple comparisons problem. The consistency of the procedure is discussed in the case that some of the sample sizes of the different studies or the number of studies tend to infinity, see Section ====. In Section ====, simulations compare the test with a procedure in ====, suitable for the meta-analysis as well. Results obtained indicate an advance under the null hypothesis and yield a varied picture under alternatives. An analysis of real data in Section ==== demonstrates how the approach works in practice. Thereby, results of different antidepressant clinical trials are pooled for the investigation of the effect of the intake of a drug on the degree of a depression. Each trial consists of a drug and a placebo arm. Finally, Section ==== discusses our methodology and results. This section includes also an outlook for the treatment of the testing problem of symmetry about an unknown center in the meta-analysis random effects model.",Rothman–Woodroofe symmetry test statistic revisited,https://www.sciencedirect.com/science/article/pii/S0167947319301926,16 September 2019,2019,Research Article,395.0
"Fouskakis Dimitris,Ntzoufras Ioannis,Perrakis Konstantinos","Department of Mathematics, National Technical University of Athens, Zografou Campus, Athens 15780, Greece,Computational and Bayesian Statistics Lab, Department of Statistics, Athens University of Economics and Business, Greece,Department of Mathematical Sciences, Durham University, Durham, UK","Received 17 December 2018, Revised 30 August 2019, Accepted 4 September 2019, Available online 12 September 2019, Version of Record 3 October 2019.",https://doi.org/10.1016/j.csda.2019.106836,Cited by (1),"The power-expected-posterior (PEP) prior is an objective prior for Gaussian linear models, which leads to consistent model selection inference, under the M-closed scenario, and tends to favour parsimonious models. Recently, two new forms of the PEP prior were proposed which generalize its applicability to a wider range of models. The properties of these two PEP variants within the context of the normal linear model are examined thoroughly, focusing on the prior dispersion and on the consistency of the induced ====. Results show that both PEP variants have larger variances than the unit-information ====-prior and that they are M-closed consistent as the limiting behaviour of the corresponding marginal likelihoods matches that of the BIC. The consistency under the M-open case, using three different ==== scenarios is further investigated.","The determinant of the CR-PEP prior covariance matrix is ====Based on the matrix determinant Lemma (==== p.416), which states that ==== for any square invertible matrices ==== and ====, we can write ==== as ==== Using repeatedly the matrix determinant Lemma on the last term of ==== yields ==== Note that the transition from ==== to the following equation is due to the fact that ====, since ==== for any sub-matrix ==== of ====. From ====, ==== we have that ",Variations of power-expected-posterior priors in normal regression models,https://www.sciencedirect.com/science/article/pii/S0167947319301914,12 September 2019,2019,Research Article,396.0
"Bianco Ana M.,Rodrigues Isabel M.","Facultad de Ciencias Exactas y Naturales, Universidad de Buenos Aires and CONICET, Argentina,CEMAT and Department of Mathematics, Instituto Superior Técnico, Universidade de Lisboa, Portugal","Received 15 June 2018, Revised 13 August 2019, Accepted 14 August 2019, Available online 7 September 2019, Version of Record 17 September 2019.",https://doi.org/10.1016/j.csda.2019.106827,Cited by (1), value sensitivity to the presence of outliers.," subjects for ==== to assign them to treatment and control. The comparison may be done in different settings. For example, the goal can be to compare the mean of two the populations or in more complex situations, the interest may be to check the equality between the parameters of the regression model assumed for each population under study. In this sense, it could be of interest to make out if the effect of certain risk factor on the response is of the same magnitude in both populations under consideration.====, ====, ==== given the covariates is ====, where ==== with ====, ====, ====, ====, ====, ====, ====, ==== and ====. We also refer to ====, ==== and ==== recommend the use of the redescending weighted ====estimators defined in ==== and indicate the ==== function allowing to compute them. They also mention the available implementations in ==== of some other proposals, such as those given by ====, ==== and ====. In addition, ==== provides a description of several robust testing procedures and their implementation in ====, when dealing with one logistic population.====In this paper, we go further and consider the situation of two populations following model ====As a motivating example we consider the very well known food-stamp data set, which is a benchmark in the robust literature of logistic regression and was first considered by ====. A relevant characteristic of this data set is that there is little overlap and for worse, some of the overlapping points are suspicious of being outliers. In fact, observation 5 is an isolated design point and ==== suggest that also case 66 is somewhat outlying, while ==== also suspect from observation 137. The data consist of 150 randomly selected persons from U.S. citizens, where the response ====Taking into account that the results obtained for one sample cannot be directly extrapolated to two samples coming from different populations, in this paper we introduce a robust Wald-test based on robust weighted estimators to test equality between the regression parameters of two populations. As mentioned above, this problem has not been previously considered and several estimators can be used to construct the Wald statistic. Considering the remarkable performance of redescending weighted ====estimators (====) for the one-sample problem, described among others in ====, in this paper, we construct our robust Wald statistic from the ==== and also a weighted version of the density power divergence estimators defined in ====. On the other hand, the latter case corresponds to the estimators defined in ====, for the one-sample problem.====The paper is organized as follows. In Section ====, for the one population setting, we remind the definition of the robust estimators given for the logistic model. In Section ====, a robust Wald-type test is constructed to compare the parameters between populations. Among other results, we show that the test statistic is asymptotically ==== distributed under the null hypothesis of no population effect. Besides, we derive its asymptotic behaviour under root==== alternatives. The robustness of the tests is investigated through its influence function in Section ====. The results of a Monte Carlo study are summarized in Section ====, while the food-stamp data set is analysed in Section ====. Proofs are relegated to ====. A supplementary file available online contains some additional Tables and Figures.",Robust Wald-type methods for testing equality between two populations regression parameters: A comparative study under the logistic model,https://www.sciencedirect.com/science/article/pii/S0167947319301744,7 September 2019,2019,Research Article,397.0
"Flórez Alvaro Jóse,Alonso Abad Ariel,Molenberghs Geert,Van Der Elst Wim","I-BioStat, Universiteit Hasselt, B-3590 Diepenbeek, Belgium,I-BioStat, KU Leuven, B-3000 Leuven, Belgium,Janssen Pharmaceuticals, B-2340 Beerse, Belgium","Received 22 January 2019, Revised 19 August 2019, Accepted 23 August 2019, Available online 3 September 2019, Version of Record 9 September 2019.",https://doi.org/10.1016/j.csda.2019.106834,Cited by (2),"When assessing surrogate endpoints in clinical studies under a causal-inference framework, a simulation-based sensitivity analysis is required, so as to sample the unidentifiable parameters across plausible values. To be precise, ==== need to be sampled with only some of their entries identified from the data, known as the matrix completion problem. The positive-definiteness constraints are cumbersome functions involving all matrix entries, making this a challenging task. Some existing algorithms rely on sampling and then rejecting invalid solutions. A very efficient algorithm is built on previous work to generate large correlation matrices with some a prior fixed elements. The proposed methodology is applied to tackle a difficult problem in the surrogate marker field, namely, the evaluation of multivariate, potentially high-dimensional, surrogate endpoints. Whereas existing methods are limited to very low-dimensional surrogates, the new proposal is stable, fast, shows good properties, and is implemented in a user-friendly and freely available R package.","In the causal-inference framework, one frequently fits models with an only partially identifiable set of parameters ====, i.e., there is a subset of ==== that cannot be estimated from the data. A possible solution to this problem is to impose untestable restrictions, e.g., based on expert knowledge, for the unidentifiable parameters to estimate the model. Alternatively, one can conduct a sensitivity analysis to assess how the fitted model and conclusions based there upon change as the unidentifiable parameters vary across plausible values. The latter option is taken by ==== and ====. Their approach rests upon computing the ICA across a set of randomly generated correlation matrices, taken to mean sampling from the collection of all symmetric positive semi-definite matrices (PSD) of a given dimension, and with unit diagonal, but, importantly, while keeping the estimable values fixed. This so-called matrix completion problem is non-trivial.====The PSD constraint involves all values of a correlation matrix, and therefore, its random generation is very challenging. However, several algorithms have been presented. ==== proposes a method based on a transformation of partial correlations, later extended by ====. The parameterization in terms of partial correlations and its application on the completion problem are also presented by ==== and ====, respectively. ====, ====, ====, among others. Although many of these approaches would target PSD matrices, we do prefer a positive-definite (PD) constraint because of the operations we need to perform on the so-resulting matrices, involving inversion of these as a whole and sub-matrices thereof.====In this paper, we build on previous work and evaluate through simulations various algorithms to generate random correlation matrices with fixed entries in the multiple surrogates assessment. Joe’s algorithm is generalized by conveniently rearranging the fixed elements of the correlation matrix and leads to excellent performance, in particular also in terms of speed and the ability of handle high-dimensional matrices. On the other hand, the adaptation of the method proposed by ==== is cumbersome. Another alternative is to simulate a pseudo-correlation matrix and to find the nearest correlation matrix, according to some metric, keeping the identifiable values fixed. Some of these adjustments for non-positive-definiteness can be found in ==== and ====. Furthermore, the methodology is applied to solve a difficult problem in the surrogate marker field, namely, the evaluation of multivariate surrogate endpoints, as well as in a different, high-dimensional context.====The structure of the manuscript is as follows. Section ==== presents the methodology for assessing multiple surrogates. In Section ====, various algorithms to generate unrestricted random correlation matrices are introduced. Section ==== describes the algorithms to generate random correlation matrices with some of their values fixed. A simulation study to compare the methods is executed in Section ====. A motivating experiment on mice (the transPAT study) is presented and analysed in Section ====. Section ==== is reserved for final remarks.====The following is the Supplementary material related to this article. ",Generating random correlation matrices with fixed values: An application to the evaluation of multivariate surrogate endpoints,https://www.sciencedirect.com/science/article/pii/S0167947319301896,3 September 2019,2019,Research Article,398.0
"Zhao Yi,Lindquist Martin A.,Caffo Brian S.","Department of Biostatistics, Johns Hopkins Bloomberg School of Public Health, United States of America","Received 21 August 2018, Revised 28 August 2019, Accepted 29 August 2019, Available online 3 September 2019, Version of Record 6 September 2019.",https://doi.org/10.1016/j.csda.2019.106835,Cited by (29)," imaging study, illustrating its ability to detect biologically meaningful results related to an identified mediator.",", ====, ====, ====, ====, ====, ====, ====, ====, ====). During the past decade, methods for dealing with multiple mediators have attracted increasing attention (====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====). Most of these methods are designed for dealing with relatively low-dimensional data. With the emergence of modern technologies (e.g., high-throughput technologies in omics studies and neuroimaging technologies), data sets with a large number of variables are being collected. However, methods for conducting mediation analysis with high-dimensional mediators are limited. Motivated by a genetics study, ==== proposed a principal component analysis (PCA) based approach for reducing the high-dimensional gene expression mediators to a series of marginal mediation problems. Incorporating a regularized regression for the outcome model, ==== introduced an independent screening approach for high-dimensional mediation analysis.====In the field of neuroimaging, studies on the impact of brain mediators on cognitive behavior are becoming increasingly popular (====, ====, ====, ====, ====, ====, ====). ==== presented an early attempt at addressing neurological images as mediators, though the analysis was conducted on univariate summaries extracted from the multivariate images. Recently, ==== proposed a mediation analysis approach that transforms the high-dimensional mediator candidates into independent directions of mediation (DMs). ==== recently proposed a general mediation model under the LSEM framework to account for the causal dependencies between the mediators and introduced a lasso-type penalty to regularize the mediation pathway effects to achieve simultaneous mediator selection and mediation effect estimation.====). ====, ====, ====).====In this study, we propose a sparse principal component of mediation (SPCM) approach to perform high-dimensional mediation analysis. This approach has two stages: the first performs a PCA for high-dimensional mediation using the method proposed in ====, ====). In this study, the fused lasso (====), as a special case of the generalized lasso (====This paper is organized as follows. Section ==== introduces the PCA based mediation approach for multiple mediators proposed in ====. In Section ====, we present the sparse principal component of mediation approach. Section ==== summarizes the simulation results. We apply our proposed method to a task-based fMRI study in Section ====. Section ==== summarizes this paper with discussions.====The following is the Supplementary material related to this article. ",Sparse principal component based high-dimensional mediation analysis,https://www.sciencedirect.com/science/article/pii/S0167947319301902,3 September 2019,2019,Research Article,399.0
Song Junmo,"Department of Statistics, Kyungpook National University, 80 Daehakro, Bukgu, Daegu, 41566, Korea","Received 30 January 2019, Revised 20 August 2019, Accepted 21 August 2019, Available online 2 September 2019, Version of Record 4 September 2019.",https://doi.org/10.1016/j.csda.2019.106832,Cited by (14),This paper deals with the problem of testing for ,", ====, ====, and ====. Statistical testings such as parameter change test and specification test have also investigated by some authors. See, for example, ==== and ====.====In this study, we are concerned with change point problem in diffusion processes. It is well known that ignoring changes can lead to false inference. Hence, change point problem has received a great deal of attention from researchers and practitioners. See the recent review paper by ====. For diffusion processes, ====, ====, ====, and ==== and ====.====. In the latter cases, on the other hand, various robust methods for reducing the effect of outliers have been developed. For an overview on this area, we refer the reader to ====. In this study, we deal with the deviating observations from the latter point of view.====As is widely recognized, statistical inference such as estimation and testing are unduly influenced by outliers. Recently, ==== and ====The rest of the paper is organized as follows. In Section ====, we introduce the residual-based CUSUM test and the MDPDE for diffusion processes. Then, we propose a robust CUSUM test for parameter change and derive its asymptotic null distribution. In Section ====, we conduct a simulation study to investigate the finite sample performance. Section ==== illustrates a real data application to KOSPI200 volatility index. Section ==== concludes and technical proofs are given in Section ====.",Robust test for dispersion parameter change in discretely observed diffusion processes,https://www.sciencedirect.com/science/article/pii/S0167947319301872,2 September 2019,2019,Research Article,400.0
"Li Xiaoxia,Tang Niansheng,Xie Jinhan,Yan Xiaodong","Yunnan Key Laboratory of Statistical Modeling and Data Analysis, Yunnan University, Kunming 650500, PR China,School of Economics, Shandong University, Jinan 250100, PR China","Received 7 October 2018, Revised 15 August 2019, Accepted 18 August 2019, Available online 28 August 2019, Version of Record 2 September 2019.",https://doi.org/10.1016/j.csda.2019.106828,Cited by (7),"This paper addresses the feature screening issue for ultrahigh-dimensional data with responses missing at random. A novel nonparametric feature screening procedure is developed to identify the important features via the conditionally imputing marginal ==== correlation. The proposed nonparametric screening approach has several desirable merits. First, it is nonparametric without assuming any regression form of predictors on response variable. Second, it is robust to outliers and heavy-tailed data. Third, under some ====, it is shown that the proposed feature screening procedure has the sure screening and ranking consistency properties. Simulation studies evidence that the proposed screening procedure outperforms several existing model-free screening procedures. An example taken from the ==== diffuse large-B-cell lymphoma study is used to illustrate the proposed methodologies."," is growing exponentially with the sample size ====, but only a small number of predictors have a significant effect on response variable. Due to the simultaneous challenges of computational expediency, statistical accuracy and algorithmic stability (====), the smoothly clipped absolute deviation (====), and the adaptive LASSO (==== presented a feature screening procedure for linear models and generalized linear models based on the marginal empirical likelihood ratio. The aforementioned model-based feature screening procedures behave satisfactorily only when the underlying models are correctly specified, but they may not perform well when the posited fitting model is misspecified.====It is well-known that it is quite challenging or almost impossible to specify a correct model for ultrahigh-dimensional data in many practical applications. To this end, some model-free feature screening approaches have been developed in recent years. For example, ==== presented a sure independent ranking and screening (SIRS) procedure to identify the significant predictors. ==== developed a SIS procedure based on the distance correlation (DC), which is not robust to heavy-tailed data with extreme values. ==== presented a robust feature screening procedure based on the Kendall ==== correlation. ==== proposed a sure feature screening procedure based on the fused Kolmogorov filter together with the slicing technique, which is computationally time-consuming. ==== gave a mean–variance screening (MVS) procedure for the ultrahigh dimensional discriminant analysis problem, which is only available for categorical response and continuous predictors. ==== extended the MVS procedure of ====. There is considerable literature on variable selection for regression models with missing data. For example, see ====, ====, ====, ====, ==== and ====, among others. However, the aforementioned literature focuses on low-dimensional regression models with missing data. Recently, it has been recognized that there is an increasing need to develop some feature screening method to identify the significant predictors in the analysis of ultrahigh-dimensional data. For example, ==== extended ====’s (==== proposed a model-free feature screening procedure based on the adjusted sure independent ranking and screening utility (SIRS) (====) via the inverse probability weighted (IPW) approach. ==== correlation.====The rest of this paper is organized as follows. In Section ====, we introduce the ASRC screening procedure in the presence of missing responses at random. Section ==== establishes the sure screening and ranking consistency properties under some regularity conditions. Simulation studies are conducted to investigate the performance of the proposed screening procedure in Section ====. A brief discussion is given in Section ====. Technical details are presented in ====.",A nonparametric feature screening method for ultrahigh-dimensional missing response,https://www.sciencedirect.com/science/article/pii/S0167947319301756,28 August 2019,2019,Research Article,401.0
"Caimo Alberto,Gollini Isabella","Technological University Dublin, Ireland,University College Dublin, Ireland","Received 19 November 2018, Revised 2 August 2019, Accepted 2 August 2019, Available online 14 August 2019, Version of Record 21 August 2019.",https://doi.org/10.1016/j.csda.2019.106825,Cited by (18),"A new modelling approach for the analysis of weighted networks with ordinal/ polytomous dyadic values is introduced. Specifically, it is proposed to model the weighted network connectivity structure using a hierarchical multilayer exponential random ==== ","). In many empirical contexts these relationships have a strength associated with their edges (====). The nature of the variation in the strength of an edge between two nodes may be determined by a variety of aspects depending on the application context; for example, the amount of traffic flowing along connections in transportation networks (====), and interactions in cellular and genetic networks (====).====, ====).====, ====); the multi-valued curved ERGMs (====); ERGMs for inference on networks with continuous edge values (====, ====); the Geometric/Poisson reference ERGMs for ordinal/count networks (====); and the rank-order-edge ERGMs (====).====, we review the main features of ERGMs. In Section ==== we show how multilayer graphs can be used to represent weighted network structures. In Section ==== we introduce the multilayer ERGM approach. In Section ==== we generalise the modelling framework using a Bayesian hierarchical modelling approach and we propose to extend the approximate exchange algorithm (==== for a recent review). In Section ====, we test our methodology on simulated data and, in Section ====.",A multilayer exponential random graph modelling approach for weighted networks,https://www.sciencedirect.com/science/article/pii/S0167947319301720,14 August 2019,2019,Research Article,402.0
"Ni Lyu,Fang Fang,Shao Jun","School of Data Science and Engineering, East China Normal University, China,Key Laboratory of Advanced Theory and Application in Statistics and Data Science - MOE, School of Statistics, East China Normal University, China,Department of Statistics, University of Wisconsin - Madison, United States","Received 13 December 2018, Revised 30 July 2019, Accepted 3 August 2019, Available online 13 August 2019, Version of Record 14 August 2019.",https://doi.org/10.1016/j.csda.2019.106824,Cited by (7),Most existing feature screening methods assume that data are fully observed. It is quite a challenge to develop screening methods for incomplete data since the traditional missing data analysis techniques cannot be directly applied to ultrahigh dimensional case. A two-step model-free feature screening procedure for ultrahigh dimensional ,"With the rapid development of modern technology, collecting ultrahigh dimensional data is quite common nowadays in many areas such as bioinformatics, medical research, imaging, finance, and social sciences. Traditional variable selection methods cannot deal with ultrahigh dimensional data very well. In the seminar work of ====) and SCAD (====) can be applied in the second stage. Many feature screening methods have been developed for a continuous response in the past decade, see, for example, ====, ====, ====, ====, ==== and many others. Meanwhile, some feature screening procedures were proposed for a classification problem. ==== further improved PC-SIS to an adjusted Pearson Chi-Square statistic based screening method (APC-SIS). Also, ==== proposed an information gain sure independence screening procedure (IG-SIS). ==== proposed a screening procedure based on conditional information entropy for binary classification.====, ====). Some variable selection methods have been developed for the missing data problem. For example, the IC====, the penalized observed likelihood in ==== and the penalized validation criterion in ====. For high dimensional case, ==== proposed a penalized pairwise pseudo likelihood method for variable selection with missing data. Nevertheless, these variable selection methods with missing data were developed with specific models or fixed-dimensional covariates. So they cannot be directly applied to ultrahigh dimensional case especially in a model-free manner. As a result, the development of model-free feature screening methods is critical to ultrahigh dimensional data analysis in the presence of missing data.====It is quite a challenge to develop feature screening methods with missing data and limited work has been done in the literature. When the response variable is missing at random (====), ==== proposed a screening procedure based on the inverse probability weighted screening index estimates. ==== be a categorical response, ==== be a categorical and fully-observed covariate, and ==== be a categorical and partially-observed covariate, for ==== and ====. The covariate dimensions ==== and ==== can both be pretty large in the framework of ultrahigh dimensional data. Denote ==== and ====. The true model follows the definition in ==== and is denoted as ====, where ==== is the smallest subset of ==== that satisfies ====and ==== denotes conditional distribution. The covariates in ====, where ==== is the number of elements in set ====. For each partially-observed covariate ====, let ==== be the missing indicator with ==== if ==== is observed and ==== if ==== is missing. We assume that the missing ==== data mechanism depends on the response ==== and the fully-observed covariate set ====, i.e., ====The covariate set ==== in ==== has a size ====. Since ==== and ==== are fully observed, the missingness mechanism ==== is missing at random (MAR); see, e.g., ====. Note that we do not impose any parametric assumption on the propensity function ====.====Our proposed method consists of two steps. In the first step we apply existing screening methods such as APC-SIS (====) to ==== to obtain an estimator ====. In the second step, based on ==== and Assumption ====), IG-SIS (====) and APC-SIS (====). Theoretically, we show the sure screening property and variable selection consistency of the proposed method. Empirically, we investigate the finite sample performance by simulation studies and a real data example. To the best of our knowledge, this is the first paper dealing with feature screening with missing covariates. Although the proposed method is mainly discussed with categorical covariates, in the simulation study we also consider a situation where continuous and categorical covariates appear simultaneously.====The rest of the paper is organized as follows. Section ==== describes the proposed screening method in detail and proves its sure screening property and variable selection consistency. The selection of the tuning parameters is also discussed. Sections ====, ==== present simulation studies and a real data example, respectively. Some concluding remarks are given in Section ====. A sketch of the proof is given in the Appendix and more technical details are in a supplementary material.====By applying Bernstein’s inequality, we can obtain the following lemma.",Feature screening for ultrahigh dimensional categorical data with covariates missing at random,https://www.sciencedirect.com/science/article/pii/S0167947319301719,13 August 2019,2019,Research Article,403.0
"Ma Huijuan,Zhao Wei,Zhou Yong","Key Laboratory of Advanced Theory and Application in Statistics and Data Science, Ministry of Education, China,Academy of Statistics and Interdisciplinary Sciences, East China Normal University, Shanghai 200062, China,Department of Biostatistics and Bioinformatics, Emory University, Atlanta, 30322, USA,Academy of Mathematics and Systems Sciences, Chinese Academy of Sciences, Beijing, 100190, China","Received 8 September 2018, Revised 19 April 2019, Accepted 6 August 2019, Available online 13 August 2019, Version of Record 30 August 2019.",https://doi.org/10.1016/j.csda.2019.106826,Cited by (3),The mean ," with finite expectation at time ==== is defined as ====. It is often of interest to analyze the mean residual life function in many applications. For example, a customer may be interested in knowing how much longer his or her computer can be used, given that the computer has worked normally for ==== firstly proposed the proportional mean residual life model, which has been studied by several authors. The proportional mean residual life model, or the Oakes–Dasu model, is specified by ====where ==== is the mean residual life corresponding to the ====-vector covariates ====, ==== is some unknown baseline mean residual life when ====, and ====. ==== used counting process theory to develop another semiparametric inference procedures for the proportional mean residual life model, which was generalized to tackle case-cohort data (====). ==== and ==== proposed the additive mean residual life model ====, and discussed various estimation methodologies with or without right censoring. ==== proposed a more general family of transformed mean residual life model ====, which can include the proportional mean residual life model and the additive mean residual life model as special cases. ==== proposed a flexible class of semiparametric mean residual life models ====, where some effects may be time-varying and some may be constant over time. However, the majority of existing literature studied mean residual life models with right-censored data.==== studied the Cox proportional hazards (PH) model under length-biased sampling data. This model with length-biased and censored data (====, ====, ====) has been studied further by ==== and ====. ==== and ====Recently, ==== and ==== studied proportional mean residual life models with length-biased and right-censored data. But for more complicated biased sampling data, few papers can be found to estimate unknown quantities in mean residual life models. Biased-sampling data (====) arises naturally in complex surveys. For example, in large-scale population-based surveys with multi-stage sampling, the complex design results in a set of probability weights for each subject. ==== considered a pseudo-partial likelihood approach for proportional hazards (PH) model under a general biased-sampling scheme. ==== assumed the following density function for the observed data subject to biased-sampling ====where ====, ==== is a known nonnegative weight function, ==== is the normalized constant that renders ==== to be a real density function. The special case of ==== involves length-biased sampling when ====. However, for the general biased-sampling data, we only observe the biased variable and endow it with the density function ====, the truncated variable is completely latent or missing. That is, biased sampling is more general than length-bias sampling that contains truncated variable information.====In this article, we propose a broad class of regression models for the MRLF of targeted unbiased failure time data that takes the form ====where ==== is a prespecified transformation function, ==== is some unknown function, ==== and ==== are defined similarly as before. Some important features of this model are outlined next.====It is clear that the proposed model ==== defines a broad class of models through the link function ====, which allows the model to be useful. The choices of ==== and ==== yield the additive mean residual life model and proportional mean residual life model, respectively. It is useful to consider the class of Box–Cox transformation, where ==== is given by ====, thus if we insist that ==== lies in the family of Box–Cox transformation but allow ==== to be estimated, this would provide a way in model selection between the additive mean residual life model and other models contained by the Box–Cox family. That is to say, our model could be useful in model selection.====On the other hand, we should be careful about the choice of an appropriate link function ====. Practically, the choice may be based on prior data or the desiring interpretation of the regression parameters, and this choice will also affect the interpretation of the function ====. Theoretically, there are some requirements for choosing an appropriate ====, which will be discussed in Section ====. We also give a model diagnostic method in Section ====, which could be easily used in practice to assess the adequacy of the model.====The remainder of this paper is organized as follows. Section ====, and a general inference procedure based on estimating functions is proposed. Then we extend the method to the situation where the censoring time depends on ====. The large sample properties of the resulting regression coefficients estimates are also given in this section. We present a model diagnostic method in Section ==== to assess the adequacy of model. Section ==== is devoted to simulation studies to examine the finite sample properties of the regression parameter estimators. In Section ====, we apply the methodology to two real-life datasets: the Channing House Data and the CSHA Data. The outline of proofs is provided in ==== finally.====Following are the outline of proofs, please read Supplementary Materials for the details. Recall that ",Semiparametric model of mean residual life with biased sampling data,https://www.sciencedirect.com/science/article/pii/S0167947319301732,13 August 2019,2019,Research Article,404.0
"Yu Ting-Hung,Tsai Henghsiu,Rachinger Heiko","Institute of Statistical Science, Academia Sinica, Taiwan, ROC,Department of Applied Economics, Universitat de les Illes Balears, Spain","Received 5 October 2018, Revised 3 May 2019, Accepted 31 July 2019, Available online 12 August 2019, Version of Record 29 August 2019.",https://doi.org/10.1016/j.csda.2019.106823,Cited by (5)," (AMLE) based on approximating the log-likelihood function of the observations is proposed. Both the drift and the ==== are allowed to be either linear or non-linear. In order to choose the most appropriate among these four possibilities, three information criteria are employed. Further, a ==== can help to determine whether threshold effects are present. Via simulations, the finite sample performance of the proposed AMLE is compared to an alternative quasi-likelihood estimator and the finite sample performance of the information criteria as well as the likelihood ratio test are studied. Finally, the efficacy of our approach is demonstrated with two financial time series.","), also known as the Vasicek model (====), and the Cox–Ingersoll–Ross (CIR) model (==== and ====, and the threshold CIR process (====, ====), respectively. The threshold OU process and the threshold CIR process (to be defined in Section ==== for a recent survey of this literature.====, ====, and ====. See also ====, ====, ====, ==== and ====. For the former, the diffusion function is often assumed to be a constant. For example, ====For the problem of parameter estimation of threshold diffusion processes, see ====, ====, and ==== for CTAR models, and ====, ====, ====, ==== and ==== which are therefore not applicable here. The drift terms of these CTAR or CTARMA models are all piecewise linear while the diffusion terms are piecewise constant. Furthermore, all of these papers, except for ====, assume discrete-time data.====More recently, ====, ====).====The continuous-time process is often modeled by stochastic differential equations. The log-likelihood function of discrete-time data obtained from a nonlinear continuous-time process is generally intractable. In this paper, we propose to approximate the stochastic differential equation by a difference equation using Euler’s method (====The rest of the paper is organized as follows. In Section ====, we briefly review the threshold diffusion process and describe the procedure of computing the AMLE in details. We, further, propose a likelihood ratio (LR) test and an approach based on information criteria (IC) for model specification in Section ====. Sections ====, ====. We also analyze the robustness of the proposed method in Section ==== and evaluate the performance of the model specification procedures in Sections ====, ====. The proposed method is applied to analyze two financial time series data in Section ====. Finally, Section ==== concludes.",Approximate maximum likelihood estimation of a threshold diffusion process,https://www.sciencedirect.com/science/article/pii/S0167947319301707,12 August 2019,2019,Research Article,405.0
"Yan Mei,Kong Efang,Xia Yingcun","School of Mathematical Science, University of Electronic Science and Technology, China,Department of Statistics and Applied Probability, National University of Singapore, Singapore","Received 3 June 2018, Revised 18 July 2019, Accepted 20 July 2019, Available online 9 August 2019, Version of Record 12 August 2019.",https://doi.org/10.1016/j.csda.2019.106818,Cited by (0), and the censoring variable ==== and ,"Consider a general model in which the univariate response variable ==== depends on the ==== vector of predictors ====, ====, ====, ====, ====, ====). In other words, it is assumed that there exists some ==== matrix ====, with ====, such that the distribution functions of ==== given ====, are identical to that of ==== given ====, i.e. ====Or, equivalently that ==== and ==== are statistically independent given ====, written as: ====The subspace spanned by the columns of ====, denoted as ====, is called a sufficient dimension-reduction (SDR) subspace; the intersection of all SDR subspaces, if still a SDR subspace itself, is referred to as the central subspace (CS), denoted as ====. Let ==== denote the orthonormal matrix such that ====. We call the columns of ====, the dimension reduction directions and the dimension of ====, the structural dimension.==== (either the event time or the censoring time) is not always observed, many famous models fall into the afore-described general framework of dimension reduction. Below are a few popular examples.====, vector of parameters ====, and a random error ====, formulation as such could still be too restrictive for the purpose of sufficient dimension reduction, as ==== merely implies that ====for some unknown function ==== and some random ‘error’ ====, which is independent of ==== but with unspecified distribution; see also ==== and ====. The methodology in this paper is aimed directly at the most general set-up of ====.==== denote the (scalar) response variable, which is usually taken to be the event time, and ====, the censoring variable (time). The observations are collected in the form of a triple ====, where ====and ==== is the indicator function. In this paper, we work under the assumption that ====This assumption is quite common in existing literature, and is crucial for the validity of many statistical methods in survival analysis, such as the Kaplan–Meier estimator as well as the partial likelihood method.====In this paper, semi-parametric structures of ==== are assumed to hold for both the event variable ==== and the censoring variable ====. Specifically, there exist two orthonormal matrices ==== and ====, such that ==== based on the following observation ====where ==== stands for direct sum between two vector spaces. Note that ==== and ====; see its proof given in ====. Some existing methods might be applied to find ====, but no methods are available to identify the individual central subspaces. In fact, we will see later on that the estimation of ==== could be done more efficiently than those of the other two. In this paper, we start with the estimation of ==== and ====, and then go on to derive an estimate of the joint space ====.====Methodologies developed for general dimension reduction for censored data are few and far between. Among them, some impose restrictive assumptions to facilitate discussions on the technicalities, while others suffer from poor efficiency. For example, ==== and ==== applied the method of ==== to examine a special case where it is required that ====. ==== considered a similar set-up for micro-array data with restrictions imposed on the design of ====, as well as on the link function. Moreover, the estimation efficiency is often unsatisfactory; see the discussions in ==== and ====. ==== investigated the single-index model for censored data via the Kaplan–Meier transformation. ==== developed a method based on the properties of the hazard density functions, but found it inefficient when the censoring variable also depends on ====. ====, ====, ====, ====, ====, ====, and ====, in one unified algorithm. We end this section with results on conditions under which the estimation problem is ‘well-posed’, i.e. the observed ==== contains sufficient information to recover the three central subspaces. Indeed, it is not difficult to come up with examples where ==== or ====; namely, the amount of information contained in observations ==== is not enough to fully recover ==== or ====. Here ==== is defined in exactly the same sense as in ====, i.e. it is the smallest space generated by the columns of matrix ==== given X, is the same as when given ====.==== ==== ====According to ====, this is defined as ====where ====, and ==== is a sequence of weights adding up to one. In ====, it is noted that the use of a Nadaraya–Watson type of weights leads results in a local K–M estimator with too large a bias, especially with high dimensional ====, and thus a higher order local polynomial weights (kernel) should be used. Specifically, suppose ==== has partial derivatives up to order ==== with respect to ====, and we define the following equivalent polynomial (kernel) weight ====where ====, ====, ==== is the ==== vector ====, ==== is a kernel function and ==== is a smoothing parameter, both possibly different from those used in ====. ==== proved that ====, ==== jointly define a local K–M estimator with a bias of order ====. However, if the empirical RDW weight ==== of ==== is derived from this ==== of ====, then the fact that ==== is a step (discontinuous) function by nature, renders the asymptotics of the solution to ==== intractable. See, ==== for more in-depth discussion on this matter and the relevant facts in empirical processes. The solution they suggested is the use of the following smoothed local K–M estimator ====where ==== is a univariate symmetric kernel function and ==== is yet another associated smoothing parameter. Little difference results from this additional smoothing, though, and indeed their asymptotic representation are nearly identical (====). With a slight abuse of notation, we still denote this smoothed local K–M estimator by ====, from which the empirical RDW weights ==== are henceforth derived.====Let ==== be a positive integer and ====, a bounded set in ====. For some constants ==== and ====, we say a function ==== belongs to ====, if ====where ====, a vector of nonnegative integers, ==== denotes the greatest integer smaller than ====, and ==== stands for the Euclidean norm of a vector. ====, is essentially the class of functions on a bounded set in ====, that possess uniformly bounded partial derivatives up to order ==== and whose highest partial derivatives are Lipschitz of order ====. In nonparametric smoothing, it is standard practice to assume the function of interest possesses certain degree of smoothness so as to justify the use of local polynomial smoothing (approximation). ==== also has its significance in empirical processes (====).====We make the following assumptions, in which the order of smoothness ====, will be specified later, and ==== stands for a generic positive constant whose value might vary from assumption to assumption.====[A1] and [A2] are standard assumptions in nonparametric polynomial smoothing; under [T1], we could consider, for any ====, ====, the ====-th order Taylor approximation of ====, ====where ==== and ====; the approximation error is such that ====uniformly in ==== and ==== in any compact region of ====. [T2] is a nonparametric extension of condition A3 in ====, to ensure the identifiability of the model. [T2] and [T3], together with the below two conditions are assumed for the local K–M estimator ==== to be of sufficient accuracy and is an element of ====.====For any ====, let ====
 ====For notational simplicity, we shall write ====, ==== as ====, ====, ==== and ====, respectively, and write ====Theoretical study of the asymptotics of the solution vector to ==== hinges on the establishment of its Bahadur representation (====). For matrices, constructed according to ==== using these solution vectors, the derivation of their asymptotics further requires the so-called ‘uniform’ Bahadur representation: the remainder term in the representation converges to zero at a speed uniform in ==== and in ==== for some small enough ====.====Write ==== as ====. The following lemma concerns the uniform Bahadur type representations for ==== and ====.",Quantile based dimension reduction in censored regression,https://www.sciencedirect.com/science/article/pii/S0167947319301653,9 August 2019,2019,Research Article,406.0
"Liu Jin,Ma Yingying,Wang Hansheng","School of Statistics and Data Science, LPMC & KLMDASR, Nankai University, China,School of Economics and Management, Beihang University, China,Guanghua School of Management, Peking University, China","Received 4 October 2018, Revised 25 March 2019, Accepted 15 July 2019, Available online 8 August 2019, Version of Record 13 August 2019.",https://doi.org/10.1016/j.csda.2019.106815,Cited by (1),None,", ====), econometrics (====, ==== becomes very large, the sample covariance matrix is singular and does not perform well (====, ====).====), thresholding (====), and penalized likelihood estimation (====, ====, ====, ====, ====, ====, ====). More detailed introduction for the covariance estimation can be found in ====, ====, and ====.====Another increasingly popular approach is to model the covariance matrix with prior network information (====, ====). This method is especially useful when the dimension of the covariance matrix is high but the sample size is limited. For example, in a mobile phone user network, there are ==== mobile phone users and the corresponding covariance matrix is defined as the variation of the monthly call duration (====). For such type of data, the sample size is usually very small and the dimension ==== could be very large. In order to estimate the response covariance matrix, ==== proposed a covariance regression model, which has fully utilized the prior network structure information among different mobile phone users. An extension work with more auxiliary information can be found in ====.====The aforementioned network based methods are very useful in modeling covariance matrix, but they require that the covariance matrix does not change over time. This assumption is too restrictive and could be violated in many practical applications. For instance, in financial markets, the optimal portfolio allocation for the ====th day/month may not be optimal for the ====, ====, ====, ====, ====) and varying coefficient models (====, ====, ====, ====, ====, ====Our contribution lies in three aspects. First, we propose a new semiparametric regression model for the response covariance matrix estimation, which has fully utilized the network information. Second, we obtain a nonparametric KS estimator and establish its ====The rest of the article is organized as follows. Section ==== introduces the semiparametric regression model for response covariance matrix. The estimation method is also presented. Section ==== studies asymptotic properties of the proposed estimator. A BIC-type model selection method is also proposed. Section ==== provides simulation studies and an empirical example. We conclude with a brief discussion in Section ====. All technical proofs are relegated to ====, ====, ====, ====.====To establish the theoretical results in Section ====, the following two technical lemmas are considered. ==== gives the brief explanation to prove that the response covariance matrix ==== is positive definite for any time point ====. ==== establishes the asymptotic normality of random innovation under dependence data. Throughout this Appendix, denote ==== as a generic positive constant, which can take different values at different places.",Semiparametric model for covariance regression analysis,https://www.sciencedirect.com/science/article/pii/S0167947319301628,8 August 2019,2019,Research Article,407.0
"Sarkar Shuchismita,Zhu Xuwen,Melnykov Volodymyr,Ingrassia Salvatore","Bowling Green State University, Bowling Green, OH, USA,The University of Alabama, Tuscaloosa, AL, USA,University of Catania, Catania, Italy","Received 5 October 2018, Revised 27 July 2019, Accepted 27 July 2019, Available online 6 August 2019, Version of Record 13 August 2019.",https://doi.org/10.1016/j.csda.2019.106822,Cited by (25),"Finite mixture modeling is a popular technique for capturing heterogeneity in data. Although the vast majority of the theory developed in this area up to date deals with vector-valued data, some recent advancements have been made to expand the concept to matrix-valued data, for example, by means of matrix ====. Unfortunately, matrix mixtures tend to suffer from the overparameterization issue due to a high number of parameters involved in the model. As a result, this may lead to problems such as overfitting and mixture order underestimation. One possible approach of addressing the overparameterization issue that has proven to be effective in the vector-valued framework is to consider various parsimonious models. One of the most popular classes of parsimonious models is based on the ==== of ====. An attempt to generalize this class and make it applicable in the matrix setting is made. Estimation procedures are thoroughly discussed for all models considered. The application of the proposed methodology is studied on synthetic and real-life data sets.",", ====), skew-==== (====, ====, ====), normal-inverse Gaussian (====), and generalized hyperbolic (====) mixtures.==== associated with ==== rows, ==== covariance matrix ==== related to ==== columns, and ==== mean matrix ====. A mixture with matrix normal components was first introduced by ====. ====, where they considered four matrix distributions including the matrix variate skew-====One serious concern related to matrix mixture models is the potentially high number of parameters. The overparameterization issue is well-documented for multivariate (====, vector-valued) mixture models. It often occurs due to a large number of variables ==== as each covariance matrix has ====, ====), ====, for the ====th component, ==== is given by ====, where ====, ====, and ==== located on the main diagonal. Geometrically, ==== reflects the volume, ==== determines the orientation, and ==== indicates the shape of the ====th component. This decomposition offers remarkable flexibility in the choice of volume, shape, and orientation of mixture components. Different combinations of these three characteristics lead to fourteen parsimonious models presented in ====.====Section ==== provides necessary preliminaries on finite mixture modeling in vector- and matrix-valued settings. Section ==== describes proposed parsimonious models and parameter estimation in every case. Section ====.====The following is the Supplementary material related to this article. ",On parsimonious models for modeling matrix data,https://www.sciencedirect.com/science/article/pii/S0167947319301690,6 August 2019,2019,Research Article,408.0
"Zhao Yaqing,Bondell Howard","Department of Statistics, North Carolina State University, Box 8203, Raleigh, NC 27695-8203, United States,School of Mathematics and Statistics, University of Melbourne, Peter Hall Building, VIC 3122, Australia","Received 14 May 2018, Revised 23 July 2019, Accepted 30 July 2019, Available online 4 August 2019, Version of Record 7 August 2019.",https://doi.org/10.1016/j.csda.2019.106821,Cited by (13),"Penalized regression can improve prediction accuracy and reduce dimension. The generalized ==== problem is used in many applications in various fields. The generalized lasso penalizes a ====, and it is shown to be both accurate and efficient compared to previous work."," norm or ==== norm of the coefficients are well-known as ridge regression (====) and lasso (least absolute shrinkage and selection operator) regression (====, ====). Let ==== be a vector of responses, ==== be a matrix of predictors and ==== be a vector of coefficients. Then ridge and lasso regression can be expressed as ====, ====, respectively: ==== where ==== is a tuning parameter specifying how much the coefficients are penalized. Ridge regression achieves an appropriate trade-off between model bias and variance in linear regression, which improves prediction accuracy, especially when ====.====Along with achieving a variance-bias balance, lasso also produces solutions of sparse structures by shrinking some parameters to exactly zero, which provides better interpretable models in many cases. In data analysis, lasso achieves variable selection by selecting the important predictors and discarding the unimportant ones. There exists a number of lasso solvers, for example, the most widely used R packages ====
 (====) and ====
 (====) are based on LARS (Least Angle Regression) algorithm (====) and pathwise coordinate descent (====), respectively. Among these algorithms, the lars algorithm provides the complete solution path for the lasso problem by taking advantage of the fact that solutions of lasso are piecewise linear with respect to ====, ====) is also a path tracking algorithm which can be considered as an extension of lars. There are a variety of lasso-type extensions, such as the fused lasso (====) and adaptive lasso (====) among others. ==== discussed a generalized lasso problem, which is defined as: ====where ==== is a matrix that allows us to specify the desired structural/geometric property of a problem. The choice of ==== leads to different forms of lasso type problems and have broad applications, for example, fused lasso (====), trend filtering (====), network lasso (====), and others.====In this paper, we focus on the generalized lasso problem and propose a new algorithm to solve these types of problems. The main contribution of this algorithm is that it transforms the generalized lasso to a regular lasso so that we allow for the use of existing popular lasso solvers, like lars and glmnet, without requiring the penalty matrix ====). We show that our new approach obtains the full solution path more accurately and efficiently than existing methods.====The sections are organized as follows: in Section ====, we discuss some current algorithms to solve the generalized lasso problems. We also review existing methods related to lasso estimator inference. In Sections ====, ====, respectively, we show our algorithm to solve the problem and describe how to construct confidence interval for the generalized lasso estimator. Section ==== in Section ====. Finally, we show a real data example in Section ====.",Solution paths for the generalized lasso with applications to spatially varying coefficients regression,https://www.sciencedirect.com/science/article/pii/S0167947319301689,4 August 2019,2019,Research Article,409.0
"Jokiel-Rokita Alicja,Topolnicki Rafał","Faculty of Pure and Applied Mathematics, Wrocław University of Science and Technology, Wybrzeże Wyspiańskiego 27, 50-370 Wrocław, Poland","Received 27 September 2018, Revised 23 July 2019, Accepted 24 July 2019, Available online 31 July 2019, Version of Record 6 August 2019.",https://doi.org/10.1016/j.csda.2019.106820,Cited by (4),"A ==== of the ROC curve based on the Lehmann family of distributions is an alternative to the popular binormal model. A special case of this model is the Bi-Weibull model. New estimators of the unknown model parameter and consequently the ROC curve from the Lehmann family are presented, and their properties are proved. The accuracy of the proposed estimators is compared with the accuracy of a known estimator based on the partial likelihood method. The conclusion that some of the new estimators perform generally better than their competitor is made.","The receiver operating characteristic (ROC) curve describes the performance of a diagnostic test, which classifies individuals into one of two categories. It is defined as a plot of the true positive rate (TPR) against the false positive rate (FPR), or sensitivity versus 1-specificity, for various threshold values.====, ====, ====, ====, ====, ====, ====, ====, ====, ====; and recently, ====).==== proposed an alternative semiparametric model to the binormal one which leads to the family of ROC curves, called Lehmann family. The ROC curves from this family have a simple analytic form.====More precisely, let ==== and ==== be the test results from a non-diseased population and a diseased population, respectively. Let ====, and ==== — a continuous cdf of the random variable ====. The ROC curve is defined as a plot of ==== versus ==== for ====, or equivalently as a plot ====, against ====, for ====.====A special feature of the ROC curve is that it is invariant to any increasing transformation of the data, i.e., if ====, and ====, for some increasing transformation ====, then the ROC curve corresponding to the distribution functions ==== and ==== is the same as the ROC curve corresponding to the distribution function ==== and ==== of the random variables ==== and ====, respectively.====Let ==== and ==== and ====, respectively (maybe after some transformation ====). ==== proposed adopting the following semiparametric relationship ====in the ROC curve context. A relationship between cumulative distribution functions analogous to ==== was originally proposed by ====. Assumption ==== corresponds to the proportional hazards assumption of the form ====, where ==== and ==== are hazard functions of the random variables ==== and ====, respectively.====It can be easily shown that under assumption ==== the ROC curve has a very simple analytic form ====regardless of the unknown form of the survival function ====, and it is substantially different from the binormal ROC curve given by ==== where ====, ==== are unknown parameters.====.====This paper deals with the estimation of the Lehmann ROC curve. To the best of our knowledge, estimation of the Lehmann ROC curve was considered only by ====The paper is organised as follows. In Section ==== we recall the estimator proposed in ====. In Section ==== we consider estimators of the Lehmann ROC curve based on AUC estimators — all estimators discussed therein are newly proposed. In Section ==== results of a simulation study are presented, which was conducted in order to compare performance of all considered estimators, especially in the case of small sample sizes. Based on the simulation results the form of an estimator introduced in Section ==== is justified. Real data are examined in Section ====. Section ==== contains conclusions and some prospects.",Estimation of the ROC curve from the Lehmann family,https://www.sciencedirect.com/science/article/pii/S0167947319301677,31 July 2019,2019,Research Article,410.0
"Yu Weichang,Azizi Lamiae,Ormerod John T.","School of Mathematics and Statistics, University of Sydney, Australia,ARC Centre of Excellence for Mathematical and Statistical Frontiers, The University of Melbourne, Parkville VIC 3010, Australia","Received 22 November 2018, Revised 15 July 2019, Accepted 18 July 2019, Available online 29 July 2019, Version of Record 6 August 2019.",https://doi.org/10.1016/j.csda.2019.106817,Cited by (5),"Variable selection and classification are common objectives in the analysis of high-dimensional data. Most such methods make distributional assumptions that may not be compatible with the diverse families of distributions data can take. A novel ==== nonparametric ====. By an application to some simulated and publicly available real datasets, the proposed method exhibits good performance when compared to current state-of-the-art approaches.",", ====).====Drawbacks of traditional formulations of DA for high-dimensional data analysis include the lack of variable selection options, and restrictive distributional assumptions. Variable selection techniques are important because they overcome the gradual accumulation of estimation errors as the number of variables increases (====, ====, ====, ====).====While solutions such as monotonic transformations (see for example ====, ====) and finite mixture modelling for the group-conditional distributions (====, ====).====, ====, ====). However, the density may be undersmoothed in regions of the domain space where there are few observations.==== that has been incorporated into several DA models (see for example ====, ====). One alternative has been proposed by ====An alternative nonparametric prior is the Pólya tree (====, ====, ====, ====, ====, ====). In particular, ==== proposed a Bayesian nonparametric analogue of the multiple samples test that demonstrated superior results to a similar model (====) by assigning a Pólya tree to each of the unknown population distributions. ====Our approach builds on the previous works described above, but differs in several ways related to the use of the Pólya tree prior. In ====, the Pólya tree is assigned as a hyperprior to unknown Gaussian means of the data, whereas in our model the Pólya tree is assigned as the prior of the unknown group-conditional distributions that the variables are drawn from. ==== assigned a Pólya tree prior on the joint group-conditional distribution of the variables instead of having ==== univariate Pólya tree priors, as is the case with our model. Both of these papers have been proposed for low dimensional data analysis in contrast to our approach that was specifically designed to handle situations in which the number of variables (====) is greater than the sample size (====). Furthermore, we have simplified the model for high dimensional datasets by assuming independence between variables and assigned a separate Pólya tree prior to the group-conditional distribution of each variable.====In Section ====, we will elaborate on the posterior inference of our model, and the heuristic interpretation of our resultant classification rule. This will be followed by a short Section ==== that discusses the setting of a hyperparameter in our model. In Section ====, we compare our proposed model with existing options in simulated, and gene expression datasets. Circumstances that lead to good performance of the model will also be discussed. Finally, we will suggest possible extensions, and conclude in Section ====.====We shall present the derivation in the case where we have a single new observation ==== and describe the generalisation to multiple new observations towards the end. Following ====, the updates for the parameter ==== may be computed as ====The expression involves an expectation of the log Bayes factor ====where ====
 ==== is the set of all binary representations of length ====, ==== is the indicator function, ==== is the Beta function, ==== is the number of group ==== observations in the partition-subset ==== and ====.====Clearly, the expectation in Eq. ==== involves nonlinear functions of ==== and ====. Therefore, we shall utilise some approximations to get around this. We may use Taylor’s expansion about ==== to approximate ====
 and, for small ====, a Stirling’s approximation to approximate the Beta functions ==== Hence, the update equation for ==== may be approximated as ====where ====The sum to infinity in the above expression is computationally tractable as the subset counts decreases as we go further down the layers of the tree. In particular, there exists a constant ==== such that either ==== or ==== for all ====
 ====
 ==== and ====. Hence, we may rewrite the log Bayes factor as ====Similarly, we use ==== to derive the update for ==== as ==== and if ==== is large, then ====where the number of observations in groups 1 and 0 are ==== and ==== respectively, the vector ==== of size ==== is such that the ====th element is ==== the ==== prefix of a vector denotes element-wise ====, the binary representation ==== denotes the first ==== branching directions taken by ==== and the number ==== is such that ==== for all ==== and ====. Note that the approximation in Eq. ==== follows from Taylor’s approximation.====We may extend the classification rule in Eq. ==== to ==== new observations by simply replacing each ==== with ====, where the ====th element of ==== is ==== the number ==== is such that ==== for all ==== and ==== and the rest of the notations used have been explained in Section ====.==== In our numerical examples, we truncate the Pólya tree priors at layers ==== for all ====. This allows us to account for the details at higher resolution of the group-conditional distributions as ==== increases (====).",Variational nonparametric discriminant analysis,https://www.sciencedirect.com/science/article/pii/S0167947319301641,29 July 2019,2019,Research Article,411.0
"Liu Yongxin,Zeng Peng,Lin Lu","Zhongtai Securities Institute for Financial Studies, Shandong University, Jinan, China,Department of Mathematics and Statistics, Auburn University, Auburn, United States,School of Statistics, Shandong Technology and Business University, Yantai, China,School of Statistics, Qufu Normal University, Qufu, China","Received 17 March 2018, Revised 19 July 2019, Accepted 20 July 2019, Available online 28 July 2019, Version of Record 1 August 2019.",https://doi.org/10.1016/j.csda.2019.106819,Cited by (5),"In many application areas, prior subject matter knowledge can be formulated as constraints on parameters in order to get a more accurate fit. A generalized ",", where ==== is a ====-dimensional vector of predictors and ==== is the response. Quantile regression estimates an intercept ==== and a vector of coefficients ====where ====, ==== is the check function defined by ====and ==== is the indicator function. Refer to ==== for a comprehensive discussion on the theory, algorithms, and applications of quantile regression.====Some recent studies on quantile regression have proposed several methods on analyzing high-dimensional data. ==== studied quantile regression with a ====-norm penalty. They developed an efficient algorithm to compute the solution path of ==== and derived a formula for degrees of freedom, which is used to assess the effective dimension of the fitted model. ==== considered quantile regression with a SCAD penalty and proved that the estimates have an oracle property. ==== considered a scaled ====-penalized quantile regression (====-QR) in high-dimensional sparse models. They studied the theoretical properties of the estimators and showed that ====-QR is consistent at the near oracle rate under general conditions. ==== applied the penalized quantile regression for dynamic panel data. They showed that the penalty term reduces the dynamic panel bias and increases the efficiency of the estimators.==== and ====. ====. ==== discussed a non-parametric quantile regression using P-splines which assumes that the estimated quantile function is monotonically increasing. However, there is no existing literature on regularized quantile regression with linear constraints to the best of our knowledge.====To this end, this article will study a regularized quantile regression with linear constraints. Consider estimating ==== and ==== by solving the following optimization problem, ====where ==== is a tuning parameter, ==== is the ====-norm of a vector, ====, ====, ====, ====, and ==== are constant matrices or vectors specified by users according to assumptions or subject matter knowledge in application. Problem ==== becomes the ====-norm quantile regression in ==== when ==== in ==== yield adaptive lasso, fused lasso, or other variants. The linearly constrained generalized lasso with squared-error loss has been studied by ====, ==== and ====, etc. But they are not robust compared with quantile regression in the presence of noise and outliers.====Problem ==== are connected with the constraints and the penalty, which makes the problem more difficult than the existing ====-norm quantile regression. The main contribution of this paper lies in the following aspects.====The rest of the paper is organized as follows. Section ==== derives the KKT conditions of the optimization problem and describes the properties of the solution. Section ==== studies the entire solution path of the parameters as a function of ====. In Section ====, a formula of degrees of freedom is obtained using Stein’s unbiased risk estimation (====). Simulation studies in different scenarios and two real data examples are conducted to illustrate the application of the proposed method in Sections ====, ====, respectively. Some conclusion remarks are included in Section ====. All technical proofs of lemmas and theorems are postponed to ====.====Notation convention: ==== is the response vector. ==== is the ====th row of ====. For an index set ====, ==== is the matrix obtained by selecting the rows of ==== with the indexes in ====. The same rule applies to matrices ====, ==== and index sets ====, ====. ==== is the vector with elements ==== for ====. The same rule applies to vectors ====, ====, ====, ==== and index sets ====, ====, ====. ==== is a ====-dimensional vector of zeros. The same rule applies to ==== and ====, where ==== and ==== are the cardinalities of sets ==== and ====, respectively. ==== is an ====-dimensional vector of ones. The same rule applies to ====.====, ====, ==== and ==== are similar to the results in ====. Their proofs are also similar. Hence, the proofs of ====, ====, ====, and ==== are omitted and available upon request. In the following, only the proof of ==== is presented.",Generalized ,https://www.sciencedirect.com/science/article/pii/S0167947319301665,28 July 2019,2019,Research Article,412.0
"Kwon Yongchan,Won Joong-Ho,Kim Beom Joon,Paik Myunghee Cho","Department of Statistics, Seoul National University, Seoul, 08826, South Korea,Department of Neurology and Cerebrovascular Center, Seoul National University Bundang Hospital, Bundang, 13620, South Korea","Received 8 October 2018, Revised 16 July 2019, Accepted 17 July 2019, Available online 28 July 2019, Version of Record 1 August 2019.",https://doi.org/10.1016/j.csda.2019.106816,Cited by (233),Most recent research of ,", ====, ====, ====). Improved decision support system bears significance in many applications, especially in healthcare.==== recently showed that a typical optimization of neural networks with dropout layers is equivalent to Bayesian variational inference with a specific variational distribution. ==== used the moment based predictive uncertainties by ==== for the dataset from the Kaggle diabetic retinopathy detection competition (====). Using the same Kaggle dataset, ==== applied the method by ====. ==== computed entropy based predictive uncertainties and illustrated how uncertainty measures can be used for improving prediction for a clinical brain tumor dataset.====In Bayesian modeling, predictive uncertainty can be decomposed into two different sources. Uncertainty can be expressed as aleatoric, capturing inherent randomness in the observations, and epistemic, accounting for model uncertainty (====). Recently, ==== independently proposed the same method as ====In medical imaging, decomposition to aleatoric and epistemic uncertainties can provide extra information as follows. In the process of classifying images into normal or abnormal, an expert can identify distinctive features in an image and weigh each feature to judge the likelihood that the image is abnormal. If this likelihood or ====Existing studies on the uncertainty quantification for classification via deep neural network have utilized extra parameters for variances without reflecting the functional relationship between a mean and a variance of multinomial random variables. Furthermore, few studies considered decomposition of aleatoric and epistemic uncertainties in classification taking into account the relationship between mean and variance. In this paper, we suggest a natural way to decomposing uncertainties in classification settings. We show that the proposed method considers the functional relationship and correlation structures among classes.====Our main contributions of this paper are:====Recall that the decomposition is as follows, ====The first equation follows from the definition of variance and it is enough to show the second equation. From the definition of ==== and by Fubini’s Theorem, we have ==== and similarly, ==== Thus, ==== For categorical variable ====, ==== and it is ==== because ==== is one-hot encoded.",Uncertainty quantification using Bayesian neural networks in classification: Application to biomedical image segmentation,https://www.sciencedirect.com/science/article/pii/S016794731930163X,28 July 2019,2019,Research Article,413.0
"Barthel Nicole,Czado Claudia,Okhrin Yarema","Department of Mathematics, Technische Universität München, Boltzmannstraße 3, 85748 Garching, Germany,Department of Statistics, Faculty of Business and Economics, Universität Augsburg, Universitätsstraße 2, 86159, Augsburg, Germany","Received 4 August 2018, Revised 16 May 2019, Accepted 9 July 2019, Available online 25 July 2019, Version of Record 1 August 2019.",https://doi.org/10.1016/j.csda.2019.106810,Cited by (0), based ==== support the excellent prediction ability of the proposed model approach.,"). This affects fields such as asset pricing, portfolio allocation and evaluation of risk.====High-frequency data allow to consistently estimate ex-post realized volatility and realized covariances using the sum of squared intra-day returns (====, ====). By making naturally latent variables, namely volatilities and covariances, observable and measurable, standard time-series approaches can be applied to model their realized counterparts. Building upon the aforementioned classical estimator first used in the context of high-frequency data by ====, many refinements were investigated to improve its overall quality and precision (====), to reduce market microstructure noise (====, ====) and to take into account jumps (====) and asynchronicity (====).====) and neglects e.g. dynamic volatility spillovers among the series of variances and covariances (====). Several multivariate approaches such as the Wishart Autoregressive (WAR) model (====) and its dynamic counterpart the Conditional Autoregressive Wishart (CAW) model (====) have been developed. ====) by modifying the Dynamic Conditional Correlation (DCC) model of ====. The basic idea of the latter model is to split up the estimation problem into the two simpler tasks of modeling the conditional volatilities and the correlation dynamics. ==== adopt this strategy using high-frequency data in the volatility part and daily data in the correlation part at the expense of less flexible correlation specifications. As an alternative, data transformation is one of the most frequently used approaches. ====). Both ARFIMA and HAR models can be extended by e.g. GARCH augmentations to account for non-Gaussianity and volatility clustering (====, ====).==== find that any partial correlation vine specifies algebraically independent (partial) correlations, i.e. the latter can take arbitrary values in ==== while still guaranteeing positive definiteness of the corresponding correlation matrix. This result advocates partial correlation vines to be a useful tool in several applications. ==== introduce a method to uniformly generate random correlation matrices from the space of positive definite correlation matrices. ==== base a parsimonious parameterization of correlation matrices on partial correlation vines in combination with factor analysis and ==== introduces a vine-GARCH approach as flexible multivariate GARCH-type model, which parametrizes the latent correlations appearing in the DCC model of ==== in terms of a partial correlation vine. Based on the specific nature of an R-vine tree structure, this estimation technique proceeds iteratively by evoking only bivariate GARCH models in each tree level and thus allows for dimension reduction as compared to computationally highly demanding classical multivariate GARCH models.==== can be captured. Combining in a third step the predicted realized variances and the predicted realized correlation matrix obtained after back-transformation of the underlying realized partial correlation vine guarantees a symmetric and positive definite realized covariance matrix forecast.====The paper is structured as follows. In Section ====, we introduce partial correlation vines combining the notion of partial correlations and an R-vine structure. The transformation of a correlation matrix to a partial correlation vine based on a given R-vine structure and vice versa is explained in detail. In Section ====, we introduce the general data setting and motivate the choice of Cholesky decomposition based models as our main benchmarks. In Section ====, we outline in detail the three main steps of the proposed partial correlation vine data transformation approach including R-vine structure selection in Section ==== and multivariate time-series modeling in Section ====, detailed investigation of the real data example will be continued. Section ==== shows the excellent forecasting performance of the partial correlation vine data transformation approach both with respect to statistical precision and mean–variance balance in portfolio optimization. This paper comes with extensive supplementary material.====See ====–====.",A partial correlation vine based approach for modeling and forecasting multivariate volatility time-series,https://www.sciencedirect.com/science/article/pii/S0167947319301550,25 July 2019,2019,Research Article,414.0
"Sun Yifan,Wang Qihua","Academy of Mathematics and Systems Sciences, Chinese Academy of Sciences, Beijing 100190, China,School of Statistics and Mathematics, Zhejiang Gongshang University, Zhejiang 310018, China","Received 20 February 2019, Revised 25 June 2019, Accepted 16 July 2019, Available online 24 July 2019, Version of Record 1 August 2019.",https://doi.org/10.1016/j.csda.2019.106814,Cited by (8),A quadratic regression model where the ,", ====, ==== and ==== for general introductions. There are abundant real examples with multiple underlying functional objects. To list a few, ==== studied regression models for curves of temperature and precipitation, and gene expression levels and histone variant levels.====A series of articles have made excellent contributions to investigate regression models with a functional response and functional predictors, most of which have concentrated on the standard function-on-function linear model ====Both the response ==== and the predictor ==== are functional, ====, Chapter 16 of ==== and ====. The most popular approach of the latter case is functional principal component analysis (FPCA). ==== expressed function parameter in eigenbasis expansion and studied the normal equation, see also ====. ==== extended this field to sparsely observed cases. Sophisticated theoretical results about FPCA in functional linear model are provided in ==== and ====. Recently ==== and ====.====However, structural linearity may be too restrictive in real applications. ====), ==== puts forward the function-on-function quadratic regression model ====
 ====, ==== and ==== on ====. When function ==== is zero a.e., model ==== degenerates to ====. The quadratic model ====Our first contribution is to construct estimates of unknown functional parameters in model ==== assumed that ====On the other hand, it is also of interest to test whether the quadratic term is significant since model ==== is more complex than simple linear model ====. There are relatively few works on test problems in functional models. For the case that the response ==== is scalar, ==== considered testing the linear effect of first finite basis projections, while ==== developed a scan test procedure to check a global linear effect. ==== extended several classical testing techniques to functional linear models. ==== studied structural test of general ==== and no effect tests can be further constructed, see ====. When ==== is also functional, a concise method for lack of effect test in model ==== is stated in Chapter 9 of ====. In this paper, a model checking method is suggested to check whether the function-on-function linear model is adequate based on the prior work of ====The rest of this paper is organized as follows. In Section ====. Simulation studies and real data analysis are conducted in Sections ====, ==== respectively. All technical proofs are postponed to ====.==== contains proofs of theoretical results stated in Section ====. Note that constant ==== may vary from lines and all of them are not related to ==== or ====.",Function-on-function quadratic regression models,https://www.sciencedirect.com/science/article/pii/S0167947319301616,24 July 2019,2019,Research Article,415.0
"Wang Cheng,Jiang Binyan","School of Mathematical Sciences, MOE-LSC, Shanghai Jiao Tong University, Shanghai 200240, China,Department of Applied Mathematics, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong","Received 2 December 2018, Revised 1 June 2019, Accepted 9 July 2019, Available online 23 July 2019, Version of Record 25 July 2019.",https://doi.org/10.1016/j.csda.2019.106812,Cited by (15),", many state-of-the-art methods do not scale well to solve problems with a very large ==== is very large.","). Despite recent advances, estimation of the precision matrix remains challenging when the dimension ==== is very large, owing to the fact that the number of parameters to be estimated is of order ====. For example, in the Prostate dataset we are studying in this paper, 6033 genetic activity measurements are recorded for 102 subjects. The precision matrix to be estimated is of dimension 6033 × 6033, resulting in more than 18 million parameters.====A well-known and popular method in precision matrix estimation is the graphical lasso (====, ====, ====). Without loss of generality, assume that ==== are i.i.d. observations from a ====. To estimate the precision matrix ====, the graphical lasso seeks the minimizer of the following ====-regularized negative log-likelihood: ==== is the element-wise ==== norm of ====, and ==== is the tuning parameter. Although ==== is constructed based on Gaussian likelihood, it is known that the graphical lasso also works for non-Gaussian data (====). Many algorithms have been developed to solve the graphical lasso. ==== proposed a coordinate descent procedure and ====. In order to obtain faster convergence for the iterations, second order methods and proximal gradient algorithms on the dual problem are also well developed; see for example ====, ====, and the references therein. However, eigen-decomposition or calculation of the determinant of a ==== matrix is inevitable in these algorithms, owing to the matrix determinant term in ====. Note that the computation complexity of eigen-decomposition or matrix determinant is of order ====. Thus, the computation time for these algorithms will scale up cubically in ====.====Recently, ==== and ==== proposed to estimate ====or equivalently, ====where ====, and ==== is the Kronecker product. Motivated by ==== and the LASSO (====), a natural way to estimate ==== is ====To obtain a symmetric estimator we can use both ====, ====, and estimate ==== by ====Denoting ====, ====is used in ==== and they proposed a column-wise estimation approach called SCIO. ==== is ====The loss function ====is equivalent to the D-trace loss proposed by ====, owing to the fact that ==== naturally force the solution to be symmetric.====In the original papers by ==== and ====, the authors have established consistency results for the estimators ====, ==== and have shown that their performance is comparable to the graphical lasso. As can be seen in the vectorized formulation ====, ====, the loss functions ==== and ==== are quadratic in ====. In this note, we propose efficient ADMM algorithms for the estimation of the precision matrix via these quadratic loss functions. In Section ====, we show that under the quadratic loss functions, explicit solutions can be obtained in each step of the ADMM algorithm. In particular, we derive explicit formulations for the inverses of ==== and ==== for any given ====, from which we are able to solve ====, ====, or equivalently ====, ====, with computation complexity of order ====. Such a rate is in some sense optimal, as the complexity for computing ==== is also of order ==== to demonstrate the computational efficiency and the estimation accuracy of our proposed algorithms. An R package “EQUAL” has been developed to implement our methods and is available at ====, together with all the simulation codes. All technical proofs are relegated to the ==== section.====Throughout the proofs, we will use two important results of the Kronecker product ====where ==== and ==== are matrices of such size that one can form the matrix products.",An efficient ADMM algorithm for high dimensional precision matrix estimation via penalized quadratic loss,https://www.sciencedirect.com/science/article/pii/S0167947319301574,23 July 2019,2019,Research Article,416.0
"Jentsch Carsten,Lee Eun Ryung,Mammen Enno","TU Dortmund, Fakultät Statistik, 44221 Dortmund, Germany,Department of Statistics, Sungkyunkwan University, 25-2, Sungkyunkwan-ro, Jongno-gu, Seoul 03063, South Korea,Institute of Applied Mathematics, Heidelberg University, Im Neuenheimer Feld 205, 69120 Heidelberg, Germany","Received 2 June 2019, Revised 4 July 2019, Accepted 8 July 2019, Available online 22 July 2019, Version of Record 5 August 2019.",https://doi.org/10.1016/j.csda.2019.106813,Cited by (6),"We consider Poisson reduced rank models where parameters depend on time. Our main motivation comes from studies in comparative politics where one wants to locate party positions in a certain political space. For this purpose, several empirical methods have been proposed using text as data sources. As the ==== of texts is quite complex, its analysis to extract information is generally a difficult task. Furthermore, political texts usually contain a large number of words such that a simultaneous analysis of word counts becomes challenging. In this paper, we consider ==== for each word count simultaneously and provide a statistical method suitable to analyze political text data. We consider a novel model which allows the political lexicon to change over time and develop an estimation procedure based on LASSO and fused LASSO penalization techniques to address high-dimensionality via significant dimension reduction. This model gives insights into the potentially changing use of words by left and right-wing parties over time. The procedure allows to identify automatically those words having a discriminating effect between party positions. To address the ","In this paper we consider models of matrices with Poisson distributed elements. We assume that after a log transform the mean of the matrix fulfills a reduced rank condition and is thus given by lower dimensional parameters. These Poisson reduced rank models were originally proposed in ====, ====, its extension (====, ====, ====) and the Wordfish method by ====. Political text analysis is an active field of research. Great availability and cheapness of texts of every description inspired scientists to consider text as a suitable data source and to work on different related aspects. Particularly, political sciences represent an active area, where methodological and empirical researches have been conducted for many years, see ====. In this context, different kinds of political texts as e.g. party manifestos or legislative speeches have been considered. Party manifestos represent an important source of political text data as they are usually produced regularly in certain intervals e.g. for parliamentary elections. Furthermore, they usually gather information on all kinds of political policies of the authoring party. Based on party manifesto data or other political texts, the research question of locating parties in a political space is an important task in comparative politics. Related questions include the choice of a proper political space, its interpretation or the identification of informative words having discriminating power between political positions to improve the general understanding of the political lexicon, see ====. In recent years, several empirical approaches have been proposed in the literature for the analysis of political texts. For log mean matrices of rank 1 the Poisson reduced rank model reduces to the Wordfish method of ==== who analyzed political texts on a one-dimensional left–right scale, see also ====, ====, ====, ====, ==== and ====.====The Poisson reduced rank model studied in ====, where applications in biomarker detection were discussed. Furthermore it is related to topic models (====. In ==== one can find a more detailed comparison with the Poisson reduced rank model.====In the general description of a Poisson reduced rank model one observes for individuals ==== Poisson random variables ====
 ==== with mean parameter ====, where ====with ====. Here, ====, ====, ==== and ==== are unknown parameters. Methods for data adaptive choices of ==== were discussed in ====. In our application to political text analysis this model is used for the counts ==== of word ====’s in political manifesto ====’s. More precisely, with double index ==== the number ==== denotes the count how often a word ==== appeared in the manifesto of party ==== at election year ====. The vector ==== is interpreted as the political position of party ==== at election ==== in a political space of dimension ====. ==== was modeled to be constant in the double index ==== and, consequently, also in time ====. Furthermore, dependences of ==== over different values of ==== and ==== are allowed in the theory of ====, but these dependences were not modeled. This paper fills two gaps in this respect. First, we allow the parameters ==== to depend on time ==== over time ====.====For a more detailed descriptions of these contributions we rewrite model ==== as follows. We suppose that for ====, ==== and ==== each ====, that is, ====In our main contribution, we generalize model ==== to allow the word weights ==== to depend on time ====, see Section ==== to vary in time is too high dimensional to get reasonable and stable statistical fits. We propose to use a penalized version of the Poisson likelihood criterion where we use L====-penalties for the absolute values of ==== and for their first order differences with respect to time. This is inspired by the LASSO of ==== and the fused LASSO of ====. These sparseness assumptions are natural in our context and this approach allows to achieve significant dimension reductions and to recover sparse and block structures of the parameters ====. Approaches using L====-penalties have also been used in other recent publications on statistical inference on text data, see e.g. ====, ==== and ====) modeled positions of political actors as VAR(1) process in a Bayesian analysis. In ==== time series parameters were estimated that quantify respect for human rights for different countries. This paper makes use of autoregressive prior distributions for latent parameters to take care of changing standards of accountability.====In a second contribution, we model the process ==== as an integer-valued autoregressive (INAR) process. This is done for each ==== and ==== for details.====In this paper we apply our methods to the same data set of party manifestos that has also been used in ====. We use so-called ====, where the text of manifestos is summarized by word counts ====, ====, ====, ====. Here, ==== denotes the gcount that a word ==== appears in the political manifesto of a party ==== in election year ====. A typical party manifesto data set will usually include a small number of parties ====, a small to moderate number of elections ==== and a rather very large number of words ====. For example, the data set we are using in this paper is based on German party manifestos of ==== parties over ==== elections. We apply simple pre-processing procedures to the manifesto data. Mainly, we use a stemming technique and we exclude stop words (e.g., ‘die’ (the), ‘ist’ (is) , ‘und’ (and) etc.) as well as words with low total frequency (====. For a detailed description, see Section 3.1 in ====. After the pre-processing, we have ==== different word stems and ==== party manifestos. That is, a resulting ==== TDM that consists of the word counts ==== (in approximately 225,000 cells). Among them, the largest ==== is 448. And 59.4%, 77.1%, 84.8%, 88.8%, and 91.2% of cells contain word counts less than or equal to 0, 1, 2, 3, 4, respectively. Manifesto lengths are quite heterogeneous and the total number of words used in each of the manifestos have a wide range between 2233 and 44192 (after pre-processing). The total number of words used for all the manifestos is about 422,000. We suppose that each ==== follows model ====. The parameters ====, ====, ====
 (====) denote the word fixed effect, a party-election year fixed effect and word weights capturing the importance of word ==== in discriminating party positions, respectively. In our analysis, the positions ==== applied their ==== algorithm to model ==== with the choice ====. They analyzed German party manifestos over five federal elections after German reunification from 1990 to 2005. They interpreted ==== as the political position of party ==== at time ==== in a left–right wing scale.====Our paper is organized as follows. The next section introduces LASSO methods for fitting models where in ==== the parameters ==== are allowed to depend on time ==== to implement a parametric bootstrap procedure to construct elliptical confidence sets for the model parameters. In Section ====, we illustrate the accuracy of bootstrap confidence sets for party positions estimates in a simulation study, where the data is generated according to data-based INAR models. The newly proposed models and methods are applied in Section ==== to our party manifesto data set. The paper concludes with final remarks in Section ====.",Time-dependent Poisson reduced rank models for political text data analysis,https://www.sciencedirect.com/science/article/pii/S0167947319301586,22 July 2019,2019,Research Article,417.0
"Karimi Belhal,Lavielle Marc,Moulines Eric","CMAP, Ecole Polytechnique, route de Saclay, 91120 Palaiseau, France,INRIA Saclay, 1 Rue Honoré d’Estienne d’Orves, 91120 Palaiseau, France","Received 19 November 2018, Revised 6 July 2019, Accepted 8 July 2019, Available online 19 July 2019, Version of Record 23 July 2019.",https://doi.org/10.1016/j.csda.2019.07.001,Cited by (2), of the global parameters.," and the references therein). Consider a study with ==== individuals from a same population. The vector of observations ==== associated to each individual ====. Then, inference on the individual parameter ====.====) is used to approximate the intractable likelihood at each iteration. For instance, this method is relevant when the model is SDE-based (see ====) that can introduce bias in the resulting parameters.====Note that generating random samples from ==== of the model by a maximum likelihood approach, i.e. by maximizing the observed incomplete data likelihood ====). Lastly, sampling from the conditional distributions ==== is also known to be useful for model building. Indeed, in ====, the authors argue that methods for model assessment and model validation, whether graphical or based on statistical tests, must use samples of the conditional distribution ==== to avoid bias.==== (RWM) algorithm (====, ====, ====The Metropolis-adjusted Langevin algorithm (MALA) uses evaluations of the gradient of the target density for proposing new states which are accepted or rejected using the Metropolis–Hastings algorithm (====, ====). The No-U-Turn Sampler (NUTS) is an extension to HMC that allows an automatic and optimal selection of some of the settings required by the algorithm, see ==== and ==== leads to a Gaussian approximation of the conditional distribution ====.====In the special case of continuous data, linearization of the model leads, by definition, to a Gaussian linear model for which the conditional distribution of the individual parameter ==== given the data ==== is a multidimensional Normal distribution that can be computed. Therefore, we design an independent sampler using this multivariate Gaussian distribution to sample from the target conditional distribution and embed this procedure in an exact inference algorithm, the SAEM, to accelerate the convergence of the vector of estimates of the global parameters ====.====The paper is organized as follows. Mixed effects models for continuous and noncontinuous data are presented in Section ====. The standard MH for NLME models is described in Section ====. The proposed method, called the nlme-IMH, is introduced in Section ==== as well as the f-SAEM, a combination of this new method with the SAEM algorithm to estimate the population parameters of the model. Numerical examples illustrate, in Section ====The following is the Supplementary material related to this article. ",f-SAEM: A fast stochastic approximation of the EM algorithm for nonlinear mixed effects models,https://www.sciencedirect.com/science/article/pii/S0167947319301483,19 July 2019,2019,Research Article,418.0
"Zhu Kailun,Kurowicka Dorota,Nane Gabriela F.","Delft University of Technology, Van Broekmanweg 6, Delft, The Netherlands","Received 21 January 2019, Revised 11 July 2019, Accepted 12 July 2019, Available online 18 July 2019, Version of Record 1 August 2019.",https://doi.org/10.1016/j.csda.2019.106811,Cited by (7),"The selection of vine structure to represent dependencies in a data set with a regular vine copula model is still an open question. Up to date, the most popular heuristic to choose the vine structure is to construct consecutive trees by capturing largest correlations in lower trees. However, this might not lead to the optimal vine structure. A new heuristic based on sampling orders implied by regular vines is investigated. The idea is to start with an initial vine structure, that can be chosen with any existing procedure and search for a regular vine copula representing the data better within vines having 2 common sampling orders with this structure. Several algorithms are proposed to support the new heuristic. Both in the simulation study and real data analysis, the potential of the new heuristic to find a structure fitting the data better than the initial vine copula model, is shown.",".====. Arguably the best known package is ==== (====), where different vine structures and copula families can be chosen to fit a data set in any dimension. One has to realize, however, that important assumptions and choices have been made for the functions in this package, which are the matter of convenience or necessity. For example, the vine copulas implemented in this package are so called simplified vines, in which conditional copulas do not depend directly on the conditioning variables. This assumption reduces the set of distributions that can be modeled with vine copulas but also cuts down the computation burden to estimating ==== marginal distributions. For the discussion and testing procedure of the simplifying assumption, we refer to ====, ==== and ====.====When data have been generated from a parametric distribution which satisfies the simplifying assumption, the estimation of a vine copula consists of: (i) the selection of a regular vine structure; (ii) the choice of (conditional) copula families and (iii) the estimation of copula parameters. The last two steps are satisfactorily covered by functions in the package ====, where almost forty parametric copula families with different properties have been implemented. The parametric copula families could also be replaced by non-parametric techniques which work well in two dimensions. However, the structure selection is still an open question.====, there are ==== regular vines on ==== elements. Some efforts to design a heuristic procedure to specify a vine structure have been reported in the literature. In ====, a method to generate vine structures starting from the top tree of the vine which is chosen to have smallest absolute partial correlation has been proposed. In ==== in the ==== package. To improve the performance of the heuristic in ====, different types of weights applied in maximum spanning tree algorithm have been proposed in ====, where not only strong dependencies, but also the performance of test for the simplifying assumption (====) is taken into account, in order to determine consecutive trees. In ====, two algorithms have been proposed to improve the structure selection presented by ====. The search for the vine structure is based on so called ====All the aforementioned algorithms for learning the structure of a vine rely on a tree-wise structure construction. A different approach has been proposed in ====. The authors introduce the concept of sampling orders implied by a regular vine and notice that vines having more sampling orders in common have more common elements in their density decomposition. Assume an initial vine structure is chosen for a given data set. The idea proposed in ====This paper is organized as follows. In Section ====, we will introduce the definition of regular vine and a new representation of information contained in regular vine, called vine binomial tree, will be presented as well. In Section ====, we briefly review and discuss sampling orders implied by a regular vine. Main contribution of this paper, where algorithms allowing generation of all regular vines having given number of common sampling orders with the initial structure is shown in Section ====. Then we propose our heuristic search method and test its performance via simulation study in Section ====. In Section ====. Algorithms are presented in ====.",Common sampling orders of regular vines with application to model selection,https://www.sciencedirect.com/science/article/pii/S0167947319301562,18 July 2019,2019,Research Article,419.0
"Chen Shuo,Bowman F. DuBois,Xing Yishi","Division of Biostatistics and Bioinformatics, School of Medicine, University of Maryland, Baltimore, MD, USA,Maryland Psychiatric Research Center, School of Medicine, University of Maryland, Baltimore, MD, USA,Department of Biostatistics, School of Public Health, University of Michigan, Ann Arbor, MI, USA,Department of Electrical and Computer Engineering, University of Maryland, College Park, MD, USA","Received 27 August 2018, Revised 18 June 2019, Accepted 18 June 2019, Available online 9 July 2019, Version of Record 15 July 2019.",https://doi.org/10.1016/j.csda.2019.06.007,Cited by (8)," level, extending more commonly applied ","Brain connectomic research has suggested that neuropsychiatric disorders are associated with altered interactions between distributed neuronal populations and brain regions (====; ====; ====; ====; ====; ====; ====; ====; ====; ====; ====). Such studies have identified connectome-phenotype relationships by leveraging these graph techniques, mainly using network graph descriptive metrics (====; ====; ====; ====; ====; ====; ====; ====; ====; ====).====The overarching goal of brain connectivity network/circuitry research is to enhance understanding of underlying pathophysiological mechanisms and clinically useful predictions concerning disease diagnosis and treatment selection (====; ====; ====).====; ====; ====; ====). On the other hand, the mass univariate analyses may keep localized information but are subject to the trade-off between controlling false positive findings and lack of statistical power. Direct application of the family-wise error control (FWER) and false positive discovery rate (FDR) could successfully prohibit spurious positive findings, yet they may be overly conservative and reduce the statistical power and lead to few or no significant findings. To mitigate such trade-offs, many studies pre-define networks of interest to lower the multiple testing burden. But, pre-defined networks are limited and may exclude true signals. Recently, more advanced statistical methods leverage multivariate models to link the edge connection strength and overall topological structure to improve model estimation (====; ====; ====; ====). However, these may not allow to automatic detection of differentially expressed subgraphs.====; ====In graph theory, a k-partite subgraph is a graph whose nodes could be partitioned into ==== distinct sets such that the nodes in the same set are not connected but nodes from different sets are connected. For brain connectivity analysis, the quantity of an edge is often continuous (rather than binary) that represents (i) the connection strength for one subject (e.g. the Pearson correlation coefficient) at an individual subject level or (ii) to what extent the connection is differentially expressed between different clinical groups (e.g. a test statistic). In this paper, we use distinct symbols to denote these two cases. When we try to identify the differentially expressed subnetworks, we use the latter case to detect latent topological structures. Specifically, we refer to a “k-partite phenomenon” (for illustration we let ====) when: (i) there are two sets of nodes and the edges connecting nodes within the same set show non-differential connectivity strengths between clinical groups; (ii) the between set connections demonstrate group-wise difference. Many studies have reported disrupted long-range connectivity patterns by neuropsychiatric disorders, for example, Parkinson’s disease (====; ====), whereas the inter-community connections are more vulnerable to the pathophysiology of diseases and more likely to show patient-healthy control differences(====; ====).==== has been a longstanding challenge. A few algorithms have been developed to recognize the latent k-partite subgraph (====; ====; ====). However, these methods are not applicable to detect potential brain connectivity networks biomarkers because (i) they rely on prior knowledge of ====; (ii) they do not allow k-partite ====The remainder of the paper is organized as follows. In Section ====. In Section ====, we apply these methods to our motivating resting-state fMRI Parkinson’s disorder (PD) study, and we detect a differentially expressed connectomic subnetwork with a k-partite topological structure and statistical significance. We conclude with a discussion in Section ====.====The following is the Supplementary material related to this article. ",Detecting and testing altered brain connectivity networks with k-partite network topology,https://www.sciencedirect.com/science/article/pii/S0167947319301410,9 July 2019,2019,Research Article,420.0
"Xie Jinhan,Hao Meiling,Liu Wenxin,Lin Yuanyuan","Key Lab of Statistical Modeling and Data Analysis of Yunnan Province, Department of Statistics, Yunnan University, Kunming 650091, PR China,School of Statistics, University of International Business and Economics, Beijing 100029, PR China,Department of Statistics, The Chinese University of Hong Kong, Shatin, N.T, Hong Kong, China","Received 4 December 2017, Revised 4 April 2019, Accepted 30 June 2019, Available online 8 July 2019, Version of Record 11 July 2019.",https://doi.org/10.1016/j.csda.2019.06.013,Cited by (5),", for example, ==== for diagnosing rare diseases. To address this issue, a fused screening procedure is proposed for dimension reduction with large-scale high dimensional imbalanced data under repeated case-control samplings. There are several advantages of the proposed method: it is model-free without any model specification for the underlying distribution; it is relatively inexpensive in computational cost by using the ",", ==== or ====, ====, ====, ====, ====, ====, among many others. Most of the research efforts there target at specific case studies and algorithms. Recently, for analyzing imbalanced data, ====), SCAD (====), group Lasso (====), adaptive Lasso (====, ====, ====, ====, etc. Recently, important findings on model-free feature screening were reported in the literature. ==== introduced a sure independence ranking and screening (SIRS) to identify significant predictors. ==== proposed to use Kendall’s tau correlation, rather than the Pearson’s correlation, as a robust ranking utility. ==== developed a sure screening procedure based on the distance correlation (DCS). A quantile-adaptive model-free variable screening (QA) was studied by ====. ====, ====, ====, ====, ====, ====, ====, ==== etc. These methods are elegant and examined to be effective for dimension reduction with prospective samples or i.i.d. samples of the underlying population. However, directly applying existing methods to ultrahigh dimensional imbalanced data without accounting for the imbalanced nature may result in inaccurate results.====, ====, ====, ====, ====, ====, ====, ====, ====, ==== and ====. Moreover, novel approaches to analyze length-biased data and general biased sampling data with semiparametric transformation and accelerated failure time models have been developed by ====, ====, ====, ====, ====, ====, ==== and ==== and ====, sampling designs that depend on the value of ==== are called response-selective or response-biased sampling. It is known that the joint distribution of the samples obtained by the response-selective sampling is typically not of the same distribution as the population distribution. But the response-selective sampling assumes that, for any ==== given ==== is the same as that of ==== given ====. Therefore, in case-control sampling, the conditional distribution of ==== given ==== is the population distribution of the covariates for all cases (controls), which is the same as that of the covariates of cases (controls) in the case-control sample.====In this paper, we propose a new variable screening procedure for ultrahigh dimensional imbalanced data. The proposed method is based on Kendall’s tau correlation under case-control sampling. The motivation of this work is that case-control sampling will not change the positive correlation between the ranks of the responses and predictors. Hence, the rank correlation can be used to rank the candidate variables with case-control sampling data. Moreover, to pursue a ranking index less sensitive to the case-control sampling design, we consider a fused ranking utility by repeating the case-control sampling for several times. Our proposed method enjoys the following several merits. First, it is a model-free approach and no need to specify an actual model for the original full data. Second, the ranking ==== is of very simple form and the computation is rather fast and straightforward. In contrast to a direct analysis of the full dataset which might cost vast computing resources, our proposed method saves plenty of computational costs with the help of multiple case-control samplings. Third, our method inherits the robust property of Kendall’s tau correlation and shall be robust to outliers in the predictors.====. We evaluate the performance of the proposed procedure through extensive simulation studies in Section ==== and a real data example in Section ====. A few closing remarks are given in Section ====. All technical details are given in ====.====More notations are needed. Let ====, ==== and ==== for each ====.",Fused variable screening for massive imbalanced data,https://www.sciencedirect.com/science/article/pii/S0167947319301471,8 July 2019,2019,Research Article,421.0
"Pensar Johan,Xu Yingying,Puranen Santeri,Pesonen Maiju,Kabashima Yoshiyuki,Corander Jukka","Department of Mathematics and Statistics, Faculty of Science, University of Helsinki, Helsinki, Finland,Department of Computer Science, Aalto University, Espoo, Finland,Department of Computer Science, University of Helsinki, Helsinki, Finland,Department of Mathematical and Computing Science, Tokyo Institute of Technology, Tokyo, Japan,Department of Biostatistics, University of Oslo, Oslo, Norway,Parasites and Microbes, Wellcome Sanger Institute, Cambridge, United Kingdom","Received 15 June 2018, Revised 10 May 2019, Accepted 27 June 2019, Available online 5 July 2019, Version of Record 10 July 2019.",https://doi.org/10.1016/j.csda.2019.06.012,Cited by (3),", is proposed as a tool to efficiently sample from sparse high-dimensional networks. The results of the study show that pairwise methods can be more accurate than pseudo-likelihood methods in settings often encountered in high-dimensional structure learning applications.","Learning the dependency pattern over a large collection of variables is an important problem encountered in various fields of science (====), statistical physics (====, ====), and computational biology (====, ====, ====, ====). In particular, as the size of the considered problems has increased, computational scalability of the algorithms has become more important. For example, the huge number of variables encountered in genome-wide sequencing studies in computational biology has steered the research towards methods based on simple pairwise tests (====, ====, ====, ====). Although pairwise methods do not come with any general asymptotic guarantees that would hold in a general setting, they have been shown to perform well on various real-world applications. In contrast, the pseudo-likelihood methods, which have been developed in the more theoretical fields of statistics (====) and statistical physics (====, ====), are more elaborate and enjoy nice theoretical properties, such as consistency.====.====We begin in Section ==== by introducing binary pairwise Markov networks. After that, we continue by presenting the Gibbs sampler and introducing the structure learning problem. In Section ====, we present the methods included in the study. We discuss the main idea and the underlying assumptions behind each method. In Section ====, we go through the experimental setup and present the key findings. We conclude this work with a discussion in Section ====.====The experiments were run in MATLAB using primarily existing and publicly available code packages. The methods were implemented and set up as follows:====To obtain a complete ranking for the precision–recall curves for each method, all edges were sorted according to their estimated interaction strength.",High-dimensional structure learning of binary pairwise Markov networks: A comparative numerical study,https://www.sciencedirect.com/science/article/pii/S016794731930146X,5 July 2019,2019,Research Article,422.0
"Zhang Jun,Lin Bingqing,Feng Zhenghui","College of Mathematics and Statistics, Institute of Statistical Sciences, Shenzhen University, 518060, Shenzhen, China,MOE Key Laboratory of Econometrics, Department of Statistics, School of Economics, and Wang Yanan Institute for Studies in Economics, Xiamen University, 361005, Xiamen, China","Received 6 October 2018, Revised 30 March 2019, Accepted 24 June 2019, Available online 4 July 2019, Version of Record 10 July 2019.",https://doi.org/10.1016/j.csda.2019.06.009,Cited by (15),"In this paper we consider partial linear regression models when all the variables are measured with multiplicative distortion measurement errors. To eliminate the effect caused by the distortion, we propose the conditional absolute mean calibration, which avoids to use the nonzero expectation conditions imposed on the variables. With these calibrated variables, a profile ==== rate. Simulation studies demonstrate the performance of our proposed procedure and a real example is analyzed as illustrate its practical usage.","where ==== is an unobservable response variable, ====” denotes the transpose operator on a vector or a matrix throughout this paper), ==== is an unknown ====, ==== is a univariate covariate, ==== is an unknown smooth function, ==== is an error term with finite variance, satisfying ==== and ====. The distorted variables ==== and ==== are the observed response variable and covariates vector. The confounding variable ==== is observable and independent of ====. The multiplicative distortion function ==== is a ====-diagonal matrix: diag====. Moreover, we assume that ====, ====, are unknown continuous distortion functions. It is noted that ====, ==== and ==== distort unobserved ====, ==== and ====This multiplicative fashion of measurement errors on response and covariates is commonly seen in biomedical and health-related studies. Especially, in some medical experiments, obtaining the true values of ==== and ==== may be very expensive or even impossible, instead, the surrogates of ==== and ==== and ==== therein. For the multiplicative fashion, ==== considered a multiplicative measurement error model where the observed value ==== is assumed to be ====, where ==== and ==== is known with ==== and ====. It is noted that model ====. In model ====, the confounding variable ==== is observable but distorting functions ====, ====’s and ==== are unknown.====The multiplicative distortion measurement errors model ====) and serum transferrin level (the observed ====) among haemodialysis patients, the researchers usually process the collected data by numerically normalizing the variables using the confounding variable ====, such as body mass index (BMI), body surface area, height or weight. The way of processing the collected dataset implies that there may exist a multiplicative fashion between the underlying primary variables ==== and the confounding variable, where ==== and ====. As claimed in ====, ====, ==== introduced a flexible multiplicative adjustment by adopting some unknown smooth distortion functions on the confounding variable in the model ====, ====, ====, ====, ====, ====, ====, ====, ==== and ====.====The condition which assumes underlying variables ==== and confounding variable ==== are independent seems strong at the first glance. But the distortion functions ====, ====’s and ==== are all unknown and very general with mild conditions. Thus, the general forms of distortion functions make the independence condition hold in practice. In other words, the assumption that the underlying variables ==== are independent of the confounding variable ==== is an assumption defining the proposed distortion measurement errors setting through defining these unobserved, underlying variables. In practice, the question to be answered is whether or not the independence condition helps to define the interpretable underlying variables of interest from their observable counter parts. Technically, if we drop the independence condition between the underlying variable ==== and the confounding variable ====, it is seen that ====. ====
 cannot be identified because ====, where ==== is a known function, and ==== is an unknown finite parameter. In this case, ==== becomes a semi-parametric model, and ====In this paper, we re-visit the partial linear models with multiplicative distortion measurement errors. The estimation and hypothesis test for the partial linear models were studied in ==== and ====. They used the direct-plug-in methods (====) and assumed the conditions ==== and ====. Moreover, they both assumed that the covariate ==== in the nonparametric part is observed without distortion. So the methods proposed in ==== and ==== fail to work in two cases: (1) ==== and ====; (2) the covariate ==== is unobserved and distorted. The case without imposing non-zero expectation conditions on the variables has been studied in ==== and ====, which proposed to estimate the distorting functions by the kernel smoothing methods with absolute value of distorted variables and the confounding variable. ==== and ==== which is required to estimate ==== and ====. The nonparametric part ==== is unobserved and distorted, we can use the estimation method proposed in ==== to obtain estimators of ==== and ====. But, the further statistical inference of parameters (such as the confidence intervals, hypothesis test for the parameters, variable selection and model checking) by adopting ====’s procedure to estimate ==== and ==== are still unclear. We will study these problems for model ==== in a general setting.====The rest of the paper is organized as follows. In Section ====, we construct the confidence intervals for parameters. In Section ====, a variable selection procedure is proposed. In Section ====, we develop a score-type statistic for checking the adequacy of partial linear regression models, and give theoretical properties of the test statistic. In Section ====, we conduct Monte Carlo simulation experiments to examine the performance of estimators and test statistics. An analysis of the Indian diabetes data is reported in Section ====. All technical proofs of the asymptotic results are given in “online-supplementary materials”.====The following is the Supplementary material related to this article. ",Conditional absolute mean calibration for partial linear multiplicative distortion measurement errors models,https://www.sciencedirect.com/science/article/pii/S0167947319301434,4 July 2019,2019,Research Article,423.0
"Igarashi Gaku,Kakizawa Yoshihide","Department of Policy and Planning Sciences, Faculty of Engineering, Information and Systems, University of Tsukuba, 1-1-1 Tennodai, Tsukuba, Ibaraki 305-8573, Japan,Faculty of Economics, Hokkaido University, Nishi 7, Kita 9, Kita-ku, Sapporo 060-0809, Japan","Received 9 August 2018, Revised 17 June 2019, Accepted 22 June 2019, Available online 4 July 2019, Version of Record 10 July 2019.",https://doi.org/10.1016/j.csda.2019.06.010,Cited by (12),"Multiplicative bias correction technique is revisited for asymmetric ==== (KDEs) when the data is nonnegative or bounded. It is crucial to classify the recently developed asymmetric KDEs into two types. The multiplicative bias correction applied to the non two-regime type is shown to effectively reduce the order of the bias, at the expense of a constant-factor inflation of the variance. However, it is revealed that, in common with other bias corrections, the multiplicative bias correction applied to the two-regime type fails in reducing the bias near the boundary, unless the density to be estimated satisfies the shoulder condition."," (see also ====, with ====, where ==== is a kernel and ==== is a bandwidth, have been well established when the support ==== of the underlying density is ====. See, e.g., ==== and ====. However, if ====, the Rosenblatt–Parzen KDE is, in general, inconsistent, due to the bias that is ==== near the boundary of ====. For this boundary bias problem, various remedies have been discussed; e.g., renormalization, reflection and generalized jackknifing (====), transformation (====) and advanced reflection (====).====Over the last two decades, when ====, there has been a growing interest of the development of the nonparametric density estimation using a certain asymmetric kernel. To the best of our knowledge, ==== first mentioned the possibility of using gamma or log-normal (LN) kernel for the nonnegative data, and ==== did pioneering studies on beta KDE using a beta kernel for the data from the unit interval ====. Note that Chen’s beta KDE is boundary bias free. Since then, there have been many attempts to suggest a suitable asymmetric kernel ====. Here, ==== is the location where the density estimation is made, and ==== is a smoothing parameter. Given an iid sample ==== from an unknown density ==== with support ====
 (====), we construct an average estimator ====, ==== (hereafter, referred to as an asymmetric KDE). The following points should be distinguished between the classical Rosenblatt–Parzen KDE and the recent asymmetric KDE. Whenever ====, the support of the asymmetric kernel ==== at the location ==== under consideration matches the support ==== of the underlying density, whereas any location-scale kernel ====, at the location ==== near the boundary of ====, necessarily has a mass outside the support ====) to the asymmetric KDE (==== or ====). See ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ==== and ====.====A common feature of the bias reduction techniques to the Rosenblatt–Parzen KDE is (i) the reduction of the order of the bias from ==== to ==== under the smoothness of ====, (ii) the constant-factor inflation of the variance (the order of the variance; ==== as in the fourth-order KDE. Even for some asymmetric KDEs, similar conclusions have been established by ====, ====, ====, ==== and ====, who applied the approaches of ====, ==== and ==== (hereafter, referred to as the SS-type, TS-type and JF-type). That is, typically, a “good” asymmetric KDE has the bias of order ====, the variance of order ==== in the interior of ====
 (==== near the boundary of ====), the MSE of order ==== in the interior of ====
 (==== near the boundary of ====) and the MISE of order ====; in contrast, applying the SS-type, TS-type and JF-type bias correction techniques enables us to reduce the bias from ==== to ====, at the expense of a constant-factor inflation of the variance (hence, the improvement of the M(I)SE rate). Here, it is well recognized after ==== that the different rates of the variance have negligible impact on the MISE. On the other hand, a possible application of ====’s bias reduction technique to the asymmetric KDE (hereafter, referred to as the JLN-type) was mentioned in ====. ==== asserted that the variance of the JLN-type bias-corrected beta KDE is asymptotically equivalent to that of the (bias-uncorrected) beta KDE, despite the variance inflation for the SS-type, TS-type and JF-type bias-corrected beta KDEs (====) or the gamma KDE (====), increases with the factor ====, for fixed ==== far away from the boundary of ====.====After revisiting the JLN-type bias-corrected asymmetric KDE in a more general way, we clarify that Hirukawa’s asymptotic variance misses two terms, which may also have implications for subsequent papers (====, ====, ====, ====, ====). To be exact, as ==== did for the classical Rosenblatt–Parzen KDE (====), we need to look at four terms from the law of total variance/covariance (see the top of Appendix A.2), in which only one term is negligible. That is why the aforementioned authors’ asymptotic variances would be “incorrectly” asserted in common. Our paper is, however, not intended just to make the correction.====The contribution of this paper is three-fold. First, it is pointed out that at least three specific asymmetric kernels (====), as well as the beta kernel (==== far away from the boundary of ====. The Gaussian approximation is a key to understanding why the same constant-factor ==== appears. Second, we derive explicit expressions for the bias, variance and MSE near the boundary of ====; note that the aforementioned literature described only the respective order, in a heuristic manner. Third, the asymmetric kernel under consideration is classified into two types. One is two-regime type and the other is non two-regime type (see ====, ====, ====, ====, ====, ====). It is demonstrated that, applying theJLN-type bias reduction technique to the non two-regime type asymmetric KDEs, (i) the order of the bias reduces from ==== to ==== under the smoothness of ====, (ii) the order of the variance remains unchanged and (iii) the MSE is of order ==== in the interior of ====
 (==== near the boundary of ====), whereas the (truncated) MISE is of order ====. However, it is found that, in general, the JLN-type bias reduction technique applied to the two-regime type asymmetric KDEs does not work in the sense that, unless the density ==== to be estimated satisfies a shoulder condition, the ====-bias does not vanish (hence, the MSE is still of order ====) near the boundary of ==== and the resulting (truncated) MISE is of order ====, slower than ====. Not surprisingly, these findings are the same as in other types (====, ====).====The rest of the paper is organized as follows. After introducing the construction of the asymmetric KDE for the nonnegative data, Section ==== gives basic results for the JLN-type bias correction and Section ==== is devoted to the special cases of three families of asymmetric KDEs. Section ==== discusses ====-variate density estimation for the data supported on ==== or ==== presents results from simulation studies. Section ==== concludes the paper, along with future work. The outlined proofs are given in ====.==== The dependency on the sample size ==== is suppressed (e.g., the smoothing parameter is denoted by ====, instead of ====), but, unless otherwise stated, the limits will be taken as ====. As usual, we use the notation ==== on ====. We also denote by ==== the ====th derivative of ==== (if it exists), and write ====. Further, the ====th moment around ====
 (====) of ==== is denoted by ====In this Appendix, we provide the proofs of the main results in Section ====.====For simplicity, we write ====, and ====Then, ====where, by virtue of the law of total variance/covariance, ====We define, for ==== and ====, ==== and ====. We use the stochastic expansion ====To complete the proofs below, we must deal with the integrals involving the unbounded function of the power of ====, as well as the asymptotic negligibility of the remainder term ====, for which Assumption ==== is crucial. In the spirit of ==== (see ====), the basic tools are the Rosenthal and Bennett inequalities of the absolute moment and tail probability of the sum of zero-mean independent random variables (and their conditional variants). Details are available from the authors.====To prove ====, ====, we need ====(ii).",Multiplicative bias correction for asymmetric kernel density estimators revisited,https://www.sciencedirect.com/science/article/pii/S0167947319301446,4 July 2019,2019,Research Article,424.0
"Wang Miaomiao,Liu Chunling,Xie Tianfa,Sun Zhihua","School of Mathematic Sciences, University of Chinese Academy of Sciences, Beijing, China,Department of Applied Mathematics, The Hong Kong Polytechnic University, Kowloon, Hong Kong,College of Applied Sciences, Beijing University of Technology, Beijing, China","Received 25 April 2018, Revised 28 March 2019, Accepted 8 June 2019, Available online 2 July 2019, Version of Record 9 July 2019.",https://doi.org/10.1016/j.csda.2019.06.003,Cited by (5),"In this work, the adequacy check of errors-in-variables varying-coefficient models is investigated when ==== of the test statistic under the null hypothesis, global and various local alternatives are established. Simulation studies and real ==== reveal that the proposed test performs satisfactorily."," and ====, among others.====, ====, ====, ====, ==== and ==== considered the goodness-of-fit test of parametric models in the context of the classical measurement error framework.====. Our novel approach is superior to the existing methods such as deconvolutional kernel techniques or minimum distance procedures because the accumulated residuals turn into an EP for model adequacy checking. Hence, this data-driven test method enjoys the advantage of the EP to avoid the nonparametric smoothing of the unknown distribution function of model errors.====) and the test based on the U-statistic (====, ====).====The rest of the paper is organized as follows. In Section ====, we propose our test statistic and establish the main results for an omnibus test. In Section ====.",Data-driven model checking for errors-in-variables varying-coefficient models with replicate measurements,https://www.sciencedirect.com/science/article/pii/S0167947319301379,2 July 2019,2019,Research Article,425.0
"Zhang Hong-Fan,Huang Lei,Liu Lian-Lian","Xiamen University, China,Southwest Jiaotong University, China,Guizhou University of Finance and Economics, China","Received 25 June 2018, Revised 3 June 2019, Accepted 5 June 2019, Available online 27 June 2019, Version of Record 9 July 2019.",https://doi.org/10.1016/j.csda.2019.06.002,Cited by (0),". As an application of this method, this paper proposes a conditional Wald type test for the parameter index. It will be shown by simulations that the conditional bootstrap based test is more powerful than the test based on the traditional plug-in covariance estimator. A real data analysis is also provided to demonstrate the effectiveness of the bootstrap method.","Given a response ==== and a ==== dimensional predictor vector ====, the single index model admits the following form ====where ==== is an unknown function, ==== is an unknown unit vector (called the parameter index) with constraint that ====, where ====. The error term ==== satisfies the equation ==== almost surely. Estimation methods for ==== have been extensively studied in the literature. For examples, ==== built the average derivative estimation (ADE) method. ==== proposed the sliced inverse regression (SIR) method and ==== created the sliced average variance estimation (SAVE) method. Later, the (weighted) semiparametric least squares (SLS) method was proposed by ==== and ====. In addition, ==== proposed the direct estimation (DE) method; ==== and ==== established the minimum average variance estimation (MAVE) method. Recently, ==== proposed the Hilbert–Schmidt Independence Criterion (HSIC) method. Among these methods, ADE, SIR, SAVE and MAVE are easy to be implemented by algorithms, while SIR, SAVE, DE and HSIC have capacities to handle the more general single index model in which the error term ==== is also included into the function ====, i.e., ====.====However, some of these methods may be prone to limitations. For example, SIR relies on the linearity condition that ====, and SAVE additionally requires the ====, has three main advantages: (1) it does not require strong conditions on the distribution of ====; (2) its computation can be easily iterated between two quadratic optimization steps, both of which have explicit solutions; (3) the estimated parameter index enjoys ====-consistency and optimal semiparametric efficiency with asymptotically normal errors. Nonetheless, making inferences on the parameter index estimated by MAVE remains to be a difficult problem, where a reliable estimator of the asymptotic covariance is crucial. The plug-in estimator (see, e.g., ====, ====, ====, ====, and ==== and ==== for partially linear models. In the work of testing regression models, bootstrap has also been widely used for helping mimic the null distribution of the test; for example, see ====, ====, ====, and ====; see also ==== for a comprehensive review of bootstrap in testing issues. Amongst, ==== used bootstrap to mimic the null distribution of a test for single index models with the link function ==== known, while ==== considered the case where ==== is unknown. However, application of bootstrap to asymptotic covariance estimation for single index models has not been properly investigated. This paper fills in this gap with an easy-to-implement algorithm and a rigorous theoretical justification.====The rest of this article is organized as follows. In Section ====, the proposed bootstrap method is applied to construct a conditional Wald type test, providing an alternative approach to testing the parameter index. To verify and demonstrate our proposals, simulated experiments and a real data analysis are both carried out in Section ====, followed by a conclusion in Section ====. Technical derivations are presented in ====.",On bootstrap consistency of MAVE for single index models,https://www.sciencedirect.com/science/article/pii/S0167947319301367,27 June 2019,2019,Research Article,426.0
"Zhang Jia,Chen Xin","School of Statistics, Southwestern University of Finance and Economics, Chengdu 611130, China,Department of Statistics & Data Science, Southern University of Science and Technology, Shenzhen 518055, China","Received 3 September 2018, Revised 15 June 2019, Accepted 15 June 2019, Available online 27 June 2019, Version of Record 3 July 2019.",https://doi.org/10.1016/j.csda.2019.06.004,Cited by (10),"Sufficient dimension reduction is an important branch of dimension reduction, which includes variable selection and projection methods. Most of the sufficient ==== are sensitive to outliers and heavy-tailed predictors, and require strict restrictions on the predictors and the response. In order to widen the applicability of sufficient dimension reduction, we propose BCov-SDR, a novel sufficient dimension reduction approach that is based on a recently developed dependence measure: ball covariance. Compared with other popular sufficient dimension reduction methods, our approach requires rather mild conditions on the predictors and the response, and is robust to outliers or heavy-tailed distributions. BCov-SDR does not require the specification of a forward regression model and allows for discrete or categorical predictors and multivariate response. The consistency of the BCov-SDR estimator of the central subspace is obtained without imposing any moment conditions on the predictors. Simulations and real data studies illustrate the applicability and versatility of our proposed method.","), sliced average variance estimation (====), inverse regression (====) and directional regression (====), require the linearity condition or the constant variance condition or both to hold, which are not easy to verify in practice. Although methods like minimum average variance estimation (====), sliced regression (==== and ==== and the Kullback–Leibler based approach proposed by ==== also require the predictors to be continuous. Other methods of sufficient dimension reduction, such as the likelihood based dimension reduction method proposed by ====, the method proposed by ==== and ==== for elliptically contoured inverse predictors and exponential family inverse predictors respectively, impose different assumptions on the distribution of ====, where ==== and ==== denote the predictors and the response respectively. ==== achieved sufficient dimension reduction via distance covariance, which does not need any conditions mentioned above and can work well when some predictors are discrete or categorical. However, their method requires the first order moments of ==== and ==== to be bounded. Hence, this method seems to be sensitive to outliers and heavy-tailed predictors.====Problems related to heavy tails arise frequently from a variety of applications, especially those in finance, economics, genomics and bio-imaging. In the field of variable selection, which is another line of dimension reduction, researchers have began to realize the importance of robust estimation. For example, ==== suggested a robust screening method based on the Kendall-==== proposed a generic sure independence screening procedure via ball correlation, which is robust to heavy-tailed predictors. Moreover, in principal component analysis, which is a vigorous unsupervised dimension reduction method, heavy-tailed phenomenon is drawing more and more attention. To name a few, ==== proposed a robust space method for principal component analysis based on a robust measure of variance. ==== proposed the elliptical component analysis, which is an alternative to principal component analysis, to tackle high dimensional elliptically distributed data. However, in sufficient dimension reduction, robustness has not received due attention, and few works have been done to address this problem. ====, ====). ==== suggested a robust extension of the minimum average variance estimation by means of modal regression to alleviate the influence of outliers. Nevertheless, these methods are only extensions of existed sufficient dimension reduction approaches, thus they also rely heavily on the conditions mentioned above, which are usually difficult to test and validate. Hence, the corresponding conclusions may be misleading if some of the conditions are violated.==== and the special distribution condition of ====), which seems similar to ours, the proposed method avoids the moment restrictions on the predictors and the response, thus it enjoys robustness to heavy tailed variables. All these advantages mentioned above ensure the practicability and versatility of our proposed method. Under mild conditions, we prove the validity of BCov-SDR as a sufficient dimension reduction method and construct the consistency of the BCov-SDR estimator of the central subspace. Simulation and real data analysis are conducted to exhibit the priority of our proposed method.====The rest of the paper is organized as follows. In Section ====, and real data analysis is conducted in Section ====. Section ==== concludes the paper, and all the proofs are deferred to the ====.==== ==== ====.",Robust sufficient dimension reduction via ball covariance,https://www.sciencedirect.com/science/article/pii/S0167947319301380,27 June 2019,2019,Research Article,440.0
"Groll Andreas,Hambuckers Julien,Kneib Thomas,Umlauf Nikolaus","Faculty of Statistics, Technische Universität Dortmund, 44221 Dortmund, Germany,Chairs of Statistics, Universität Göttingen, Humboldtallee 3, 37073 Göttingen, Germany,Finance Department, HEC Management School, University of Liège, Rue Louvrex, 14, 4000 Liège, Belgium,Department of Statistics, Faculty of Economics and Statistics, Universität Innsbruck, Universitätsstr. 15, 6020 Innsbruck, Austria","Received 3 September 2018, Revised 7 June 2019, Accepted 15 June 2019, Available online 26 June 2019, Version of Record 2 July 2019.",https://doi.org/10.1016/j.csda.2019.06.005,Cited by (27), effects and is based on L====-type penalties. The following three penalization options are provided: the conventional ,"). However, in high-dimensional data set-ups classical fitting procedures for the GAMLSS often become rather unstable and methods for variable selection are desirable. In addition, the more distributional parameters are related to covariates, the further the model’s complexity is increased.====The first ones who systematically addressed the issue of variable selection, i.e. the selection of a reasonably small subset of informative covariates to be included in a particular GAMLSS, were ==== and is based on classical gradient boosting, which they successfully adapted to the GAMLSS characteristics. Both variable selection and model choice are naturally available within their regularized regression framework. For an implementation into the statistical software ==== (====), see ====.====An alternative strategy for variable selection, which is mainly designed for linear covariate effects, uses L====-type penalties. A first attempt for such a penalization-based, regularized estimation in the high-dimensional GAMLSS framework is proposed in ==== follow ==== and ====-type penalties, but which extends the previous approaches by including penalization strategies that are specifically designed for nominal or ordinal categorical predictors. Using adequate penalties, not only the cases of the conventional LASSO for metric covariates, but also of both the group (====) and fused LASSO (====) for categorical predictors are covered. The implementation of the methods is incorporated into the unified modeling architecture for distributional regression models established in ====, which exploits the general structure of classical generalized additive models (GAMs) and encompasses many different response distributions, estimation techniques, model terms etc. The corresponding ====-package ==== (====The performances of these new methods are investigated in two extensive simulation studies and are compared to different other approaches. In the applications considered later in this work, we consider both Gaussian and generalized Pareto distributed responses. We focus on the fusion of factor levels of either nominal or ordinal factors. Different performance aspects are investigated, in particular, ==== of the fitted coefficients, but also the performance with regard to factor fusion and variable selection in the presence of noise variables.==== and in ====, where also a more detailed description of the data can be found. The second data set is a database of 10,217 extreme operational losses from the Italian bank UniCredit, covering a period of 10 years and 7 different event types. These data have recently been analyzed in ====.====The article is set out as follows. In the next section, we specify the underlying fully parametric regression model framework. We then introduce different L====-type penalties in Section ====, which are designed for different kinds of regularization. The algorithmic details related to the fitting procedures of the penalized models are presented in Section ====. Next, the performance of the different methods is investigated in simulation studies in Section ====. Then, we illustrate their applicability in the two aforementioned real data examples in Section ====. Finally, we summarize the main findings and conclude in Section ====.====The following is the Supplementary material related to this article. ","LASSO-type penalization in the framework of generalized additive models for location, scale and shape",https://www.sciencedirect.com/science/article/pii/S0167947319301392,26 June 2019,2019,Research Article,441.0
"Shen Pao-sheng,Chen Hsin-Jen,Pan Wen-Harn,Chen Chyong-Mei","Department of Statistics, Tunghai University, Taiwan, ROC,Institute of Public Health, School of Medicine, National Yang-Ming University, Taiwan, ROC,Institute of Biomedical Sciences, Academia Sinica, Taipei, Taiwan, ROC","Received 5 January 2019, Revised 13 May 2019, Accepted 15 June 2019, Available online 21 June 2019, Version of Record 2 July 2019.",https://doi.org/10.1016/j.csda.2019.06.006,Cited by (10),"Interval censoring and truncation arise often in cohort studies, longitudinal and sociological research. In this article, we formulate the effects of ==== on left-truncated and mixed case interval-censored (LTIC) data without or with a cure fraction through a general class of semiparametric transformation models. We propose the ","Cohort study is a widely applied study design to evaluate the temporal association between risk factors and disease occurrence. However, such design can lead to two methodological issues when estimating the associations. First, cohort study usually excludes the subjects who have developed the disease at baseline/the time of recruitment, resulting in sampling bias, i.e., left truncation. Second, interval censoring arises since individuals are often inspected intermittently and the disease of interest is only known to have occurred between two consecutive monitoring times. Data collected from such classical cohort study design are subject to left truncation and interval censoring, the so-called left truncated and mixed case interval-censored (LTIC) data. In addition to truncation and censoring, another feature of cohort study, as pointed by ==== and ====). In literature, there exist many studies for the PH model with interval-censored data (====, ====, ====, ====, ====). In practice, the PH and PO assumptions may both be violated. The semiparametric transformation models have been proposed as a broad class of regression models for survival times (====, ====). Under the semiparametric transformation models, ==== and ==== constructed rank-based estimators for interval-censored data. ====When truncation is present, ==== and ====. ==== considered the cMLE using the Newton–Raphson method. All the aforementioned methods are restricted to the PH model and cannot be extended in a straightforward manner to a general class of semiparametric transformation models with LTIC data. Furthermore, all the proposed approaches entail computational challenges since the dimension of parameters grows with sample size. Recently, ==== considered the nonparametric maximum likelihood estimation for the PH model with length-biased interval-censored data. They proposed a full likelihood approach by incorporating the information about length-biased data, i.e., the truncation time follows a uniform distribution, and developed a computationally simple and stable EM algorithm through two-layer data augmentation. However, their approach cannot be applied to situation where the distribution of left truncation times is unspecified.====Another complexity is the presence of a cure fraction. In clinical studies for genetic or chronic diseases, there may exist a fraction of individuals not experiencing the event of interest due to genes or medical advancements. Therefore, the cure rate/nonsusceptible rate should be taken into account. The mixture cure model is one of commonly used approaches to formulate the issue by modeling the cure rate and the latency survival function of non-cured subjects separately. As discussed by ====, the mixture model can be flexible in many situations because it has appealing interpretations for long-term effects on the cure fraction and short-term effects on the time to event. There is an extensive literature on mixture cure models for right-censored data (====, ====, ====, ====). The literature over the last decade gradually reflects considerable interest on the mixture cure models for interval-censored data (====, ====, ====, we investigate the finite-sample performance of our procedures through intensive simulation studies. In Section ====, using the empirical dataset from the Cardiovascular Disease Risk Factors Two-Township Study (CVDFACTS), we apply the proposed methods to analyze the differences in women’s ages at onset of abdominal obesity and hypo-====-lipoproteinemia between two towns in Taiwan. We conclude the article with discussions in Section ====.====Here, we establish the large sample properties for the cMLE under the mixture semiparametric transformation models with LTIC data. The same logic can be applied to the data without a cure fraction. The details of the proof are deferred in Web Appendix.====Let ==== and ==== denote the empirical measure from ==== independent observations and the true probability measure, respectively. The centered and scaled version of the empirical measure is defined by ==== for a certain function ====. Let ==== and ==== be the observed-data loglikelihood for a single subject, ==== where ====. Define ==== and class ====, where ==== is a compact set in ====, ====, and ====, where ==== denotes functions which have total variation in [0, ==== bounded by a given constant ====. We require the following conditions to establish the consistency of ==== and weak convergence of ====.====(A1) Let ==== and ==== denote the true values of ==== and ====, respectively. ==== lies in the interior of ==== and ==== is continuously differentiable with positive derivatives in ====, where ==== is the union of the support ====.====(A2) The covariates ==== and ==== are both bounded and covariance of each covariate vector is positive definite.====(A3) If ====
 ====almost surely for all ====, then ==== and ==== on ====, where ==== and ====.====(A4) The number of monitoring times, ====, is positive and ====. In addition, ==== for some positive constant ====. The conditional joint densities ==== given ==== and ====, denoted by ==== have continuous second-order partial derivatives with respect to ==== and ==== when ==== and are continuously differentiable with respect to ==== and ====.====(A5) The matrix ==== is nonsingular, where ==== is the score function for ==== defined in ====, ====, and ==== is the score function along the submodel ==== defined in ====, where ==== is the least favorable direction in ====.====The proof of ==== relies on the derivation of the least favorable submodel for ==== and empirical process theory. A key step is to show that ==== converges to ==== at the ==== rate. This result can be obtained by establishing ====.",Semiparametric regression analysis for left-truncated and interval-censored data without or with a cure fraction,https://www.sciencedirect.com/science/article/pii/S0167947319301409,21 June 2019,2019,Research Article,442.0
"Wang Wan-Lun,Castro Luis M.,Lachos Victor H.,Lin Tsung-I","Department of Statistics, Graduate Institute of Statistics and Actuarial Science, Feng Chia University, Taichung 40724, Taiwan,Department of Statistics, Pontificia Universidad Católica de Chile, Casilla 306, Correo 22, Santiago, Chile,Millennium Nucleus Center for the Discovery of Structures in Complex Data, Santiago, Chile,Centro de Riesgos y Seguros UC, Pontificia Universidad Católica de Chile, Santiago, Chile,Department of Statistics, University of Connecticut, Storrs, CT 06269, USA,Institute of Statistics, National Chung Hsing University, Taichung 402, Taiwan,Department of Public Health, China Medical University, Taichung 404, Taiwan","Received 17 May 2018, Revised 9 March 2019, Accepted 1 June 2019, Available online 19 June 2019, Version of Record 2 July 2019.",https://doi.org/10.1016/j.csda.2019.06.001,Cited by (16),"Mixtures of factor analyzers (MFA) provide a promising tool for modeling and clustering high-dimensional data that contain an overwhelmingly large number of attributes measured on individuals arisen from a ====. Due to the restriction of experimental apparatus, measurements can be limited to some lower and/or upper detection bounds and thus the data are possibly censored. In this paper, we extend the MFA to accommodate ====, and the new model is called the MFA with censoring (MFAC). A computationally feasible alternating ",", ==== proposed the likelihood-based approach to dealing with truncated and censored samples from normal populations. ==== employed the power transformation to convert the data with DLs to the counterpart following a normal distribution and offered the expectation maximization (EM) algorithm (====) to perform maximum likelihood (ML) inference. ====.====). ==== and ==== for a variety of applications with the implementation using the ==== package (====).====Based on a location-scale mixture of normal distributions, ==== introduced a partially adaptive estimator built on the EM algorithm for the Tobit model. ==== developed a Gaussian mixture model-based approach to handling multivariate data that contain partially censored vectors. ==== provided an EM algorithm to estimate parameters in mixtures of censored regression models. More recently, ====To perform the dimension reduction for high-dimensional data, factor analysis (FA; (====)) which describes variability among observed attributes (====) using a potentially lower number of latent variables (====) to describe variability and dependency structures of the multivariate data. ==== and ==== initially incorporated finite mixture models with FA to establish the mixtures of factor analyzers (MFA). The MFA approach has emerged as one of most powerful tools for clustering high-dimensional data and been extensively studied by ==== and ====, ====.==== package (====) designed as a benchmarked approach against the ==== of ====.====, ====, ====).====) algorithm, which is an extension of the ECM algorithm (====The rest of this paper is structured as follows. Section ==== presents the formulation the MFAC model. Section ==== describes the AECM algorithm for estimating model parameters and presents some practical tools related to the issues of clustering, recovery and prediction. Simulation studies are conducted in Section ==== to assess the performance of the MFAC model. Section ==== is devoted to the application of the proposed techniques to two real-life datasets. We conclude with a brief discussion in Section ====. The theoretical proof is sketched in====.====It is easy to verify that the conditional distribution of ==== given ==== is one-trial Bernoulli with success probability ====, defined as ====. Hence, the calculation of ==== is obtained with ==== evaluated at ====. Making use of the fact that ====, the desired formulations of ==== and ==== can be written as the combination of two auxiliary permutation matrices ==== and ====. The first two moments of ==== given ==== are then obtained by using the results of ==== and ==== discussed in ====.====Using the law of iterative expectations, we can get ====
 ==== and ",Model-based clustering of censored data via mixtures of factor analyzers,https://www.sciencedirect.com/science/article/pii/S0167947319301355,19 June 2019,2019,Research Article,443.0
"Tian Guo-Liang,Liu Yin,Tang Man-Lai,Li Tao","Department of Mathematics, Southern University of Science and Technology, Shenzhen City, Guangdong 518055, PR China,School of Statistics and Mathematics, Zhongnan University of Economics and Law, Wuhan City, Hubei 430073, PR China,Department of Mathematics and Statistics, School of Decision Sciences, The Hang Seng University of Hong Kong, Siu Lek Yuen, Shatin, N.T., Hong Kong, China","Received 17 December 2018, Revised 29 March 2019, Accepted 20 April 2019, Available online 31 May 2019, Version of Record 3 July 2019.",https://doi.org/10.1016/j.csda.2019.04.012,Cited by (1)," (MM) algorithm to calculate the ==== (MLEs) of parameters and the posterior modes for the analysis of general incomplete categorical data. Although the ==== (AR) algorithm aided with the proposed MM algorithm. The key idea is to construct a class of envelope densities indexed by a working parameter and to identify a specific envelope density which can overcome the four drawbacks associated with the traditional AR algorithm. The proposed mode-sharing based AR algorithm has three significant characteristics: (I) it can automatically establish a family of envelope densities ==== indexed by a working parameter ====, where each member in the family shares mode with the posterior density; (II) with the one-dimensional grid method searching over the finite interval ====, it can identify an optimal working parameter ====, which is more dispersive than the posterior density; (III) it can obtain the optimal envelope constant "," categories and ==== for ====, denoted by ====, where ====denotes the ",A novel MM algorithm and the mode-sharing method in Bayesian computation for the analysis of general incomplete categorical data,https://www.sciencedirect.com/science/article/pii/S0167947319300969,31 May 2019,2019,Research Article,444.0
"Rathke Fabian,Schnörr Christoph","Image & Pattern Analysis Group (IPA), Heidelberg University, Im Neuenheimer Feld 205, 69120 Heidelberg, Germany","Received 27 August 2018, Revised 3 February 2019, Accepted 8 April 2019, Available online 30 May 2019, Version of Record 14 June 2019.",https://doi.org/10.1016/j.csda.2019.04.005,Cited by (5)," are processed in two seconds, rather than in ==== hours required by the previous approach to terminate. For higher dimensions, density estimation becomes tractable as well: Processing 10000 samples in ==== requires 35 min. The software is publicly available as CRAN R package ====.","A large amount of computation time is spent on computing the gradient ==== of the objective ====. The gradient component with respect to a hyperplane normal ==== reads ====Gradient components for the intercepts ==== are similar. Since ==== for ====, a robust evaluation of terms ==== that prevents numerical overflow of the exp-function is given by ====Similarly, the smooth max-approximation ====
 ==== is numerically evaluated as ====Calculating ==== for all combinations of hyperplanes and grid points is the by far most expensive step in our approach. The problem is inherently sparse, however, since for most grid and data points only a few hyperplanes are relevant with most terms ==== negligibly small. We exploit this property in several ways.====Computing the exponential function on a CPU is relatively expensive (about 50 times more than addition/multiplication (====)). We set ====, whenever ====. A second strategy attempts to reduce the number of hyperplane evaluations ====. It utilizes two integration grids of varying density: A sparse grid to filter inactive hyperplanes and a dense grid to evaluate the integral of ==== and its gradient for all active hyperplanes. The sparse grid is divided into boxes ==== consisting of ==== adjacent grid points ====, e.g. 4 boxes in ==== (a). For each box ==== we perform the following steps:====For medium sized problems, this scheme reduces the number of evaluations of ==== by about 90%.====Using a numerical integration scheme based on a regular grid facilitates ====. We automatically distribute the numerical integration (and other expensive for-loops) across all available CPU cores, using the OpenMP API (====). In addition, we utilize Advanced Vector Extensions (AVX), a technique that ==== code by performing certain operations like addition or multiplication simultaneously for 8 floating point or 4 double values on a single CPU core. AVX is supported by all CPUs released since 2010. Both techniques, within-core and across-core parallelization led to speed ups by a factor of more than 10 on a standard four core machine. Due to the local character of most computations, transferring the code to the GPU promises even larger speed-ups.",Fast multivariate log-concave density estimation,https://www.sciencedirect.com/science/article/pii/S0167947319300891,30 May 2019,2019,Research Article,445.0
"Quessy Jean-François,Rivest Louis-Paul,Toupin Marie-Hélène","Département de mathématiques et d’informatique, Université du Québec à Trois-Rivières, Trois-Rivières, Canada, G8Z 4M3,Département de mathématiques et de statistique, Université Laval, Québec, Canada, G1V 0A6","Received 14 November 2018, Revised 9 April 2019, Accepted 9 April 2019, Available online 28 May 2019, Version of Record 4 June 2019.",https://doi.org/10.1016/j.csda.2019.04.008,Cited by (5),Nonparametric moment-based goodness-of-fit tests are developed for the family of chi-square ," and a shape parameter ====. The popular multivariate normal copula family corresponds to the special case when ====. Otherwise, the additional parameter ====Following (====), the construction of a chi-square copula starts with a random vector ==== that is multivariate normal with zero means, unit variances and correlation matrix ====. Then, for some ====, consider the random vector ====, where for each ====, ====. According to a celebrated theorem due to ====, there exists a unique function ====, called the copula of ====, such that for each ====, ====If ==== is the space of ====-dimensional correlation matrices, the set of dependence structures ==== is called the family of multivariate chi-square copulas. The latter has been introduced by ====)). The interested reader is referred to ==== for more details on the properties and usefulness of the chi-square copulas.====Now consider a random vector ==== whose marginal distributions ==== are assumed to be continuous and whose dependence structure is characterized by the chi-square copula ==== be the percentile function of ====, where ====. Then it follows that ==== and ====. Since for a fixed value of ====, the multivariate chi-square copulas have a pairwise structure in the sense that the information on the overall level of dependence in ==== is entirely contained in the pairs ====, ====, it makes sense to characterize the dependence via pairwise measures. To this end, define ==== Upon noting that ====, where ==== is the cdf of the standard univariate normal distribution, it follows that for each ====, ==== as ====. Hence, ==== corresponds to the van der Waerden coefficient of the pair ====. Another special case of interest is the centered chi-square copula arising when ====. In that context, ==== and ====.====This paper develops goodness-of-fit tests for the family of multivariate chi-square copulas with known shape parameter ====; these tests are build around ==== and ==== on linear rank statistics. The proposed goodness-of-fit procedures are computationally attractive when compared to the popular tests based on the empirical copula, especially when the dimension increases; see ==== for details on these tests.====The remaining of the paper is structured as follows. In Section ====, some basic properties of the population versions of the two association measures defined in Eqs. ====, ==== are given and explicit expressions are provided when the copula belongs to the chi-square family. In Section ====, empirical versions of the two measures are defined and their joint asymptotic behavior is established in the general case. Section ==== investigates the properties of these tests via simulations and an illustration on the five-dimensional Nutrient dataset is given in Section ====. All the proofs and complementary computations are relegated to two appendices.",Goodness-of-fit tests for the family of multivariate chi-square copulas,https://www.sciencedirect.com/science/article/pii/S0167947319300921,28 May 2019,2019,Research Article,446.0
"Yang Guangren,Liu Yiming,Pan Guangming","Department of Statistics, School of Economics, Jinan University, Guangzhou, 510632, China,School of Physical and Mathematical Sciences, Nanyang Technological University, 637371, Singapore","Received 19 September 2018, Revised 4 March 2019, Accepted 27 April 2019, Available online 23 May 2019, Version of Record 23 May 2019.",https://doi.org/10.1016/j.csda.2019.04.017,Cited by (1),The paper proposes a cross-validated ," is fixed. However, the dimension of data in an era of big data increases rapidly among many areas, including gene data, e.g., ====, economy data, e.g., ====, ==== and climate data, e.g., ====, etc. In those cases, the convention methods based on the low dimension setting are no longer applicable, especially when ==== is much larger than the sample size ====. There are two main approaches in finding a proper covariance matrix estimation in the high dimensional setting.====The first approach is the shrinkage method pioneered by ==== and the sample covariance matrix ====, ==== proposed a class of rotation-equivariant covariance estimator. This class of estimators was motivated by the idea in ====, i.e., pulling up the small eigenvalues of the sample covariance matrix and pulling down the large ones by an amount that is determined individually for each eigenvalue. ==== and ====, etc.====The second approach is to propose tailored estimators for the population covariance matrices with some special structures. Among others, ==== and ==== proposed the regularized estimation for either banding or tapering covariance matrices. ==== and ==== and ==== consider thresholding estimation with some general thresholding functions. ====Both types of methods have pros and cons. Theoretically, the thresholding method only keeps the large covariances and omits the small ones, so that the variation of the covariance estimates can be reduced. Thecorresponding consistency in terms of the spectral norm (====) and the entry-wise maximum norm (====) can be established. While, for the shrinkage method, it is applicable for more general settings. For example, the linear shrinkage method aims to find an optimal tradeoff between bias and variance. In this case, its consistency in terms of ==== defined in the section of the simulation studies), the thresholding estimators proposed by ==== and  ==== perform much better than the shrinkage estimators. However, without any special structures, as in ==== shows, both linear (====) and nonlinear  (====, ====) shrinkage estimators outperform the thresholding based estimators. However, which type of method is preferred when the underlying covariance matrices have some weak structure such as ====, where ==== has strong sparsity and ==== does not have any special structure? Is there an alternative method that can be applied in such a case?====The main contribution of this article is to propose a cross-validated linear shrinkage estimation for population covariance matrices and a new weighted estimator based on the thresholding and shrinkage methods such that it is applicable in a wider area. Furthermore, we propose a cross-validated linear shrinkage estimation. Our proposed linear shrinkage method performs better than some other shrinkage methods in many cases and enjoys fast computation time than nonlinear shrinkage method. In addition, we also propose an implementable algorithm in finding such an estimator by cross validation.====The rest of the article is organized as follows. Section ==== provides the methodology and the implementable algorithm. Section ==== introduces the theoretical properties of proposed estimators. Simulation results and real data analysis are illustrated in Section ====. We relegate all the proof details in the ====.====For easier presentation, set ====, ====, let ====, ====, ==== and ==== be the corresponding cardinalities. We also use ==== to denote the positive constants, which can change from line to line.====Before introducing the proof of the main theorems, we present some useful lemmas:====The proofs of Lemmas 1–2 follow from ====, and we omit them.",Weighted covariance matrix estimation,https://www.sciencedirect.com/science/article/pii/S0167947319301070,23 May 2019,2019,Research Article,447.0
"Skripnikov A.,Michailidis G.","Department of Statistics, University of Florida, 102 Griffin-Floyd Hall, P.O. Box 118545, Gainesville, FL 32611, USA","Received 24 October 2018, Revised 27 March 2019, Accepted 14 May 2019, Available online 22 May 2019, Version of Record 30 May 2019.",https://doi.org/10.1016/j.csda.2019.05.007,Cited by (11),"In a number of applications, one has access to high-dimensional ==== on several related subjects. A motivating application area comes from the neuroimaging field, such as brain fMRI time series data, obtained from various groups of subjects (cases/controls) with a specific neurological disorder. The problem of regularized joint estimation of multiple related Vector Autoregressive (VAR) models is discussed, leveraging a group lasso penalty in addition to a regular lasso one, so as to increase statistical efficiency of the estimates by borrowing strength across the models. A modeling framework is developed that it allows for both group-level and subject-specific effects for related subjects, using a group lasso penalty to estimate the former. An estimation procedure is introduced, whose performance is illustrated on synthetic data and compared to other state-of-the-art methods. Moreover, the proposed approach is employed for the analysis of resting state fMRI data. In particular, a group-level descriptive analysis is conducted for brain inter-regional temporal effects of Attention Deficit Hyperactive Disorder (ADHD) patients as opposed to controls, with the data available from the ADHD-200 Global Competition repository.","), brain fMRI data (====), macroeconomic time series forecasting and structural analysis (====, ==== lasso penalty (====, ====), with theoretical properties established in the former paper.====As previously mentioned, in many applications, on top of the typical high-dimensional setting, one also has to perform estimation of time series across a moderate to large number of related subjects. As a motivating example, the area of medical research brings about a lot of experimental settings with multiple subjects being monitored over time, e.g. a collection of fMRI time series data for a group of patients. Looking at patients with a particular disease/disorder (e.g. Alzheimer’s), it is expected that connectivity across brain regions exhibits common structural patterns. However, it has been well documented that, albeit sharing the same disorder, patients still exhibit individual patterns (====, ====), which leads to subject heterogeneity.====Standard analysis pipelines for fMRI time series data typically involve estimating a network for each subject separately, with subsequent accumulation of the estimates for further group-level analysis (====), autism (====), Parkinson’s disease (====), among others. While providing tools for group-level analysis, this approach does not incorporate the underlying assumption of similarity across subjects within the same group (e.g. cases or controls) into the estimation procedure.==== or fused lasso penalty in ==== and ====. In this work, we employ a group lasso penalty due to its ability to clearly identify a common structure across multiple subjects, while letting the magnitudes of effects vary. After having detected the common structure, a standard lasso procedure will be applied to obtain sparse estimates of subject-specific effects.====, ====). Nevertheless, we attempt to address some of those issues in our ADHD study in Section ====. For example, one of the six problems with the use of VAR models in application to fMRI data discussed in ==== concerns the varying signal strength across the brain regions from multiple study participants, even though they all share a common abstract processing structure. In our framework, this issue is addressed directly via a group lasso penalty that, while enforcing a common structure, allows for variability in magnitudes of common effects. It is also worth mentioning that a joint estimation approach via regularizing penalties had been used before for brain fMRI time series data (====, ====To introduce the joint modeling framework, consider a ====, ====, for ==== related subjects. VAR model with lag order ====, or ====, is given by ====
 ====where ==== is a ==== between the ==== variables for subject ====, ====, which allows us to break problem ==== into ==== simpler sub-problems that can be solved in parallel. In this work, we focus on the case of VAR model with lag order one (====), so as to emphasize studying the properties of the joint estimation procedure rather than the aspects of lag order selection. The joint estimation approach starts with the assumption of common and individual components for each VAR model: ==== of the ==== subjects during the first stage, followed by a sparse lasso optimization procedure to estimate the individual components ==== at stage two. The group lasso penalty effectively groups the respective elements of the transition matrices across all ==== subjects and either retains or excludes the whole group from the model, which guarantees a shared structure of the resulting common component estimates. Meanwhile, the residuals from the common component signal are used as data to estimate the individual structures, representing subject-specific effects, via standard lasso optimization.====The remainder of the paper is organized as follows: Section ==== describes the joint modeling framework and introduces the two-stage estimation procedure, Section ==== demonstrates the simulation study results of the joint estimation procedure for various settings and compares its performance with other state-of-the-art methods, Section ==== provides substantial empirical application of the introduced method to resting-state fMRI data for ADHD study, while Section ==== contains concluding remarks and discussions of future work.",Regularized joint estimation of related vector autoregressive models,https://www.sciencedirect.com/science/article/pii/S0167947319301185,22 May 2019,2019,Research Article,448.0
"Zhao Yan-Yong,Lin Jin-Guan","School of Statistics and Mathematics, Nanjing Audit University, Nanjing, 211815, People’s Republic of China","Received 24 September 2017, Revised 4 May 2019, Accepted 5 May 2019, Available online 21 May 2019, Version of Record 30 May 2019.",https://doi.org/10.1016/j.csda.2019.05.003,Cited by (5)," is proposed to estimate jump discontinuities based on the Nadaraya–Watson kernel smoothing and least-squares fitting, and ==== of resulting estimators are derived. Then, a jump size-based test statistic is developed for checking whether the estimated jump discontinuities are true. A computationally feasible ",", ====, ====). Another example is the relationship between the prototype electricity demand and other variables such as the income or production, the real price of electricity and the temperature. ==== be the univariate response variable, ====, and ==== be the corresponding covariates, then a varying coefficient model is of the following form ====where “====” denotes the transpose of a vector or matrix, ==== is a ==== is independent of ==== with mean zero and finite variance ====.====Many authors have extensively studied varying coefficient models. For instance, ==== studied an empirical likelihood inference for the model ==== contains a constant, say the first component ====, we can write ====. Further, if the coefficient vector associated with ==== is a vector of constants, say ====, then the varying coefficient model reduces to a partially linear model ====, see, e.g., ====. It is worth noting that the model ==== has been successfully applied to various fields, such as environics, medicine, econometrics, neuroimaging, etc.; see ====, ====, ====, ====, ==== and references therein.====For estimating coefficient curves in the model ====, one often makes the assumptions that the coefficient curves are smooth (i.e., at least continuous), leading to smooth estimates. ====, ====, ==== and references therein. For varying coefficient models, however, little work has been done except ====, in which an adaptive jump preserving method was proposed to estimate coefficient functions. This method requires a large amount of computation because it assumes that each design point is a potential jump discontinuity. Thus, it is necessary to develop a fast estimation procedure to deal with jump discontinuities in the varying coefficient model.====The main contribution of this paper is to study the model ====The remainder of the paper is organized as follows. In Section ====, we construct a jump size-based test statistic and propose a bootstrap algorithm to approximate critical values. Monte Carlo simulations, presented in Section ====, illustrate the performance of the proposed estimation and testing methods. In Section ====, we assess the proposed methodologies using stock data during the year 2000. We conclude the paper and give some extensions in Section ====.====In the Appendix, we outline the proofs of the main results.==== Let ==== denote the design density in the case of stochastic design, and put ==== for regularly spaced design. It may be proved by Taylor expansion and integral approximations to series that for either regularly spaced or stochastic design, ====uniformly in ====, ==== and ====.==== For the regularly spaced design, there is a single stochastic contribution, ====then, we have ====. With stochastic design, however, there is also a contribution from the design, through the term ====then, ====.==== ==== ==== If the errors are not normally distributed, we approximate to ==== using partial sums constructed in the Hungarian fashion. Define ====, and let ==== denote the variance of ====. There exists a standard Brownian motion ====, and positive constants ====, ==== and ====, depending only the error distribution, such that, with ====we have for ====, ====for all ====. Putting ====, ====, ==== (where ==== and ==== are constants depending only on ====, and ====), and ====, then, we have ====and ====We bound the right-hand side using somewhat different arguments in the cases of regularly spaced and stochastic design, respectively, and treat only the latter here. Therefore, first define ==== to be the event that ====Use Bernstein’s inequality, and an approximation to ==== on a lattice of pairs ====, of fineness ==== for fixed but arbitrarily large ====, to show that if ==== is given then ==== may be chosen so large that ==== for all sufficiently large ====. Let ==== denote the distribution function corresponding to density ====. If the complement of ==== holds, then ====where ==== stands for the maximum over integer pairs ==== such that ==== and ====, and ==== are the order statistic of a uniform random sample on the interval ====. Let ==== denote the event that ==== for some pair ==== such that ==== and ====, and some ====. It may be proved that ==== may be chosen so large that ==== for all sufficiently large ====. Combining the above results, we deduce that if ==== is given then ==== may be chosen so large that ====for all sufficiently large ====.====According to ====–====, we conclude that for regularly spaced and stochastic designs, given ==== there exists ==== such that for ====, ====for all sufficiently large ====.==== ==== ==== Assume that the design is stochastic. Defining ====the “other Hungarian construction” (====) may be employed to show that there exists a standard Brownian Bridge ==== such that ==== holds, for ==== and some ====.==== ==== ==== ==== ==== Fernique’s lemma (====) may be employed to prove that for each given ====, there exists ==== so large that for ==== and ====, ====From ====, it holds for ==== and ====, ==== is also valid for ==== and ====.==== We can write ====, where ==== if the design is regularly spaced, and ==== if the design is stochastic. Furthermore, according to ====, we have ====, where ==== and ====for all ====. In view of ==== and ====, for each ====, ==== and ====, there exists ==== such that for all sufficiently large ====, ====
 ====Together these results imply that if ==== denotes any sequence of positive numbers decreasing to zero more slowly than ====, if ====, and if the support of ==== is contained within ====, then ==== Part (a) of the theorem follows. ==== also implies that if ==== is as suggested in the definition of ====, then ====According to ====, if ==== and ==== is large, then with high probability the value of ==== that maximizes ==== over ==== and ==== is within ==== of ====. Next we show that it actually occurs within ==== of ====.==== Let ====, where we shall assume that ==== and ==== for positive constants ====. Note that ====, where ==== is bound in ====, and satisfies ==== as ====. Also, ====, where ==== for some ====, and ==== uniformly in ==== and ====. It follows that, uniformly in ==== and ====, ====Thus, ==== that maximizes ==== over ==== has the property that ====. Therefore, ====. Equivalently, ====Results ==== and ==== together imply that part (b) of the theorem.==== Let ==== denote the value of ==== that maximizes ==== on ====. We shall prove that under ====, ====This indicates that ====Parts (c) and (d) of the theorem follow from ==== and ====. For part (d), we need to show that with high probability, ==== in the definition of ==== lies in the interval ==== defined in Step 4, for small ==== and large ====. This is indeed true follows from ====–====.====For simplicity, we treat only the case of regularly spaced design. If ==== are independent random variables and ==== for all ==== and all ====, then if ====, ====For each ====, the quantity ==== equals a weighted sum of approximately ==== independent values of ====, the weights being, of course, derived from ====. From this result and ====, it follows that if ====, then for each ==== and some ==== not depending on ====, ====
 for all sufficiently large ====. Since ====, ====, are independent, then, writing ==== and ==== for the integer parts of ==== and ==== respectively, ==== For large ====, the right-hand side is dominated by ====, where ==== is any constant strictly less than ====. Therefore, if ==== is chosen so small that ====, and ====, then for all ====, as ====, ====A minor modification of the argument leasing to ==== shows that for any sufficiently small ====, and for ==== and all ====, ==== for each ====. The desired result ==== follows from ==== and ====.====The result ==== also holds under the present weaker conditions on ====. Under the condition ====, rather than existence of a finite moment generate function, the embedding result ==== for ==== is weakened to ====for ====. This leads the weaker version of ==== for ====
 ====For ====, ==== holds without change, and ==== is valid for ==== and 2, without alteration. When ====, we obtain that ====and combining ==== in the case ==== with ====, we have ====According to the above two formulae, and assuming that ====, where ==== is a positive number and ====, we see that for ==== and 2, and hence for ====, ==== may be chosen so small that the result ====obtain. The claimed formula ==== follows.==== Let ==== denote the set of all pairs ==== such that ==== and ====, and ==== denote the version of ==== defined by minimizing the sum of squares at ==== when ==== is redefined to equal the set of integers ==== such that ====. In view of ====, the proof of the theorem will be complete if we show that ====Next, we outline the proof.====It is straightforward to show that for each ====, ====Therefore, in establishing ==== it suffices to confine attention to the case where the series at ==== is minimized over integers ==== that satisfy ==== for a sequence ==== convergence to zero arbitrarily slowly, where ==== is the distribution function of the design density.====Given ====, let ==== be the set of indices ==== such that ====, let ====, and let ==== be the set of triples ==== arising in this way, with ====. We estimate the jump point in the data set ==== by minimizing over ==== the sum of squares, ====where ==== and ==== denote summation over indices ==== with ==== and ====, respectively, and where ==== and ==== with ====, ==== and ====. We estimate the jump discontinuity to occur at ==== if ==== for all ====.====Assume the jump discontinuity ==== lies between ==== and ====. With probability tending to 1, ==== uniformly in all triples ==== and all ====, so we may assume without loss of generality that this inequality holds. Let ==== and ==== summation over integers ==== satisfying ==== and ====, respectively, and put ====, ====, ====, ====, ==== and ====. We proof that in this notation, ====, where ==== and ====, with ====, ====, ====, ====, ====, and ====.====Given any large constant ====, and a statistic ==== such as ====, let ==== denote the supremum of ==== over all triples ====, all ==== and all integers ==== with ====. Let ==== denote the same supremum when the range of ====’s is restricted to ====, for some ====. Set ==== and ====, and let ==== denote the set of design points. It can be proved that for each ====, the conditional probabilities ====, ==== and ==== all converge to zero in probability. Hence, the unconditional probabilities converge to zero as well. Thus, defining ====, we have ====in probability. Furthermore, for each ====, ====According to the last two formulae, we see that for all ====, ====Therefore, ==== that the minimum of ==== over ====, ==== and integers ==== with ====, occurs with ====, satisfies ====Eq. ==== follows from ====.",Estimation and test of jump discontinuities in varying coefficient models with empirical applications,https://www.sciencedirect.com/science/article/pii/S0167947319301148,21 May 2019,2019,Research Article,449.0
"Vicuña M. Ignacia,Palma Wilfredo,Olea Ricardo","School of Management, Pontificia Universidad Católica de Chile, Chile,Department of Statistics, Pontificia Universidad Católica de Chile, Chile","Received 17 January 2018, Revised 4 May 2019, Accepted 8 May 2019, Available online 21 May 2019, Version of Record 31 May 2019.",https://doi.org/10.1016/j.csda.2019.05.005,Cited by (1),The minimum distance methodology can be applied to the estimation of locally stationary ,"Locally stationary processes have played an important role over the last two decades. Although stationary models are widely used, they have the disadvantage that their ====, along with their means and variances, must remain constant over time. However, these conditions are difficult to meet in practice. By comparison, the class of locally stationary models allows for the modeling of time series exhibiting non-stationarity. This approach is based on the assumption that the structure of the process varies smoothly over time so that it can be locally approximated by a stationary model.====, ==== and ====, among others. Later, ====, ==== proposed the methodologies called locally stationary, which have been widely discussed in the recent time series literature, see for example, ====, ====, ====, ====, ====, ====, ====, ==== and ====, ====, among others.====The methodology proposed by ====, ==== extended the estimator for fitting Gaussian long-memory locally stationary models.====In this paper, we introduce a new estimation methodology for fitting locally stationary moving average models. This is a local extension of the minimum distance estimator (MDE) discussed by ====, ====, ==== and ==== and ==== have proposed a class of minimum distance estimators, based on the ====. Additionally, we prove that the proposed estimator is asymptotically consistent and normally distributed.====The remainder of this paper is structured as follows. Section ==== defines a class of locally stationary processes while a minimum distance estimator for locally stationary process is presented in Section ====. An application of the proposed methodology to real data is shown in Section ====. Final remarks are presented in Section ==== while proofs of the Theorems are provided in the ====.====This appendix contains the proofs of ====, ==== stated in Section ====. Before presenting the proofs of these results, we introduce and prove three propositions. These involve the large sample properties of the functional operators defined next. Consider the functions ==== for ==== and define the functionals ====and ====where ==== is the local population and the local sample covariances of the process, ====, ==== are given in Section ====, where ==== and ====.====In what follows, for notation simplicity, we denote ====, that is, the expectation evaluated under the true parameter. By using the properties of ==== mentioned in Section ==== and considering Assumption A1, we have ====and ====where ====, for ====.",Minimum distance estimation of locally stationary moving average processes,https://www.sciencedirect.com/science/article/pii/S0167947319301161,21 May 2019,2019,Research Article,450.0
"Chang Bo,Joe Harry","Department of Statistics, University of British Columbia, Vancouver, BC V6T 1Z4, Canada","Received 23 July 2018, Revised 24 April 2019, Accepted 25 April 2019, Available online 20 May 2019, Version of Record 20 May 2019.",https://doi.org/10.1016/j.csda.2019.04.015,Cited by (40),"Vine copulas are a flexible tool for multivariate non-Gaussian distributions. For data from an observational study where the ==== and response variables are measured together, a proposed vine copula ==== uses regular vines and handles mixed continuous and discrete variables. This method can efficiently compute the ","In the context of an observational study, where the response variable ==== assuming a random sample ==== for ==== given ==== for making predictions. Observational studies are studies where researchers observe subjects and measure several variables together, and inferences of interest are relationships among the measured variables, including the conditional distribution of ==== given other variables when there is a variable ==== that one may want to predict from the other variables. In contrast, in experimental studies, the explanatory variables (treatment factors) are controlled for by researchers, and the effect of the non-random explanatory variables is then observed on the experimental units. The inferences of interest may be different for experimental studies.==== is multivariate Gaussian. Unlike multiple regression, the joint-distribution-based approach uses information on the distributions of the variables and does not specify a simple linear or polynomial equation for the conditional expectation.====When the explanatory variable is a scalar and continuous (====), the joint distribution of ==== show how different copula families can lead to quite different shapes in the conditional mean function ==== and say that linearity of conditional quantiles is a pitfall of quantile regression. There are applications of bivariate or low-dimensional copulas for regression in ==== and ====. However, none of the previous papers link the shape of conditional quantiles to tail properties of the copula family.==== ==== ==== ==== ==== ==== For question (A), the vine copula or pair-copula construction is a flexible tool in high-dimensional dependence modeling; see ====, ====, ====, ==== and ====.====The possibility of applying copulas for prediction and regression has been explored, but an algorithm is needed in general for (B) when some variables are continuous and others are discrete. ==== and ==== for quantile regression, but the vine structure is restricted to a boundary class of vines called the D-vine. A general regular-vine (R-vine) copula is adopted in ====, for the case where the response variable and explanatory variables are continuous. ====In this paper, we propose a method, called vine copula regression, that uses R-vines and handles mixed continuous and discrete variables. That is, the predictor and response variables can be either continuous or discrete. As a result, we have a unified approach for regression and (ordinal) classification. The proposed approach is interpretable, and various shapes of conditional quantiles of ==== as a function of ==== can be obtained depending on how pair-copulas are chosen on the edges of the vine. Another contribution of the paper is a theoretical analysis of the asymptotic conditional cumulative distribution function (CDF) and quantile function for vine copula regression. This analysis sheds light on the flexible shapes of ====The remainder of the paper is organized as follows. Section ==== gives an overview of vine copulas. Section ==== provides theoretical results on how the choices of bivariate copulas in a vine affect the asymptotic tail behaviors of the conditional CDF and quantile function. These results are more general than those given in ==== and provide insights into the possible tail behaviors for higher-dimensional copulas. Sections ====, ==== present a simulation study and applications of vine regression. Section ==== concludes the paper. The supplementary materials include the code and data for Sections ====, ====.====An R-vine can be represented by the edge sets at each level ====, or equivalently by a graph, such as ====. But those representations are not convenient; we need a more compact way to represent vine models. A vine array ==== for a regular vine ==== on ==== elements is a ==== upper triangular matrix. There is an ordering of the variable indexes along the diagonal, and row ====, column ==== shows the variable ==== is connected to the variable ==== in tree ====, conditioning on variables ====. That is, the first ==== rows of ==== and the diagonal elements encode the ====th tree ====, such that ====, for ====. For example, the vine array ==== represents the R-vine in ====. The edges of ==== include ====, ====, ====, ====. The edges of ==== include ====, ====, ====, ====. ====Note that a valid vine array represent a unique R-vine. However, a R-vine may have multiple vine array representations. For example, ==== and ==== encode exactly the same R-vine. In real applications, the variables are labeled arbitrarily. We can define a permutation of the variables so that the diagonal elements are ====. Therefore, 3.1 only applies to vine arrays with ordered diagonal elements.",Prediction based on conditional distributions of vine copulas,https://www.sciencedirect.com/science/article/pii/S0167947319301057,20 May 2019,2019,Research Article,451.0
"Choi Byeong Yeob,Lee Jae Won","Department of Epidemiology and Biostatistics, University of Texas Health Science Center, San Antonio, TX 78229, USA,Department of Statistics, Korea University, 5-1 Anam-Dong, Sungbuk-Gu, Seoul 136-701, South Korea","Received 2 April 2018, Revised 1 April 2019, Accepted 25 April 2019, Available online 18 May 2019, Version of Record 28 May 2019.",https://doi.org/10.1016/j.csda.2019.04.013,Cited by (0),"This paper discusses an instrumental variable estimation of the potential outcome distributions for compliers. The existing nonparametric estimators have a limitation in that they give non-proper cumulative distribution functions that violate the non-decreasing property. Using the ==== representation of the standard nonparametric estimators, a simple isotonic regression approach has been developed. A nonparametric ==== of a veteran status on future earnings.","). If both the IV and the treatment are binary, then a subpopulation in which the treatment choice is determined by the variation in the IV can be defined. This subpopulation is often called a group of compliers (====, ====).====It is important to realize that causal estimands considered by most empirical research on treatment effects are average differences of the potential outcomes for compliers (====, ====, ====). In addition to these mean contrasts, the process of comparing the entire distributions of the potential outcomes is useful for more complex analyses such as social welfare comparisons, which require the integration of the utility functions under the distributions of the outcomes. The study of ====, ====The paper is organized as follows. Section ==== reviews the existing IV method of ==== for estimating the CDFs for compliers, and describes why this IV method cannot give proper estimators for the CDFs. Section ==== derives the isotonic regression estimators using the identification results for compliers, which were developed to estimate the local average response function (====), and describes the methods for estimating treatment effects and confidence intervals. Next, Section ==== presents a simulation study to evaluate the application of the isotonic regression in the relevant IV models. Section ==== concludes with a discussion to summarize the results of the study.====In this appendix, we prove the uniform convergence of ====. That of ==== can be proven in the same way. Let ==== and ====, and ==== and ==== be the sample counterparts of ==== and ====. Then, ====. We express ==== as ==== The first term in ==== can be expressed as ==== By the strong law of large numbers, ==== with probability one, and by the Glivenko–Cantelli theorem (==== Theorem 19.1), ==== with probability one. Therefore, the first term in ==== converges to 0. In the similar way, the second term converges to 0, and hence ==== holds.",The isotonic regression approach for an instrumental variable estimation of the potential outcome distributions for compliers,https://www.sciencedirect.com/science/article/pii/S0167947319300970,18 May 2019,2019,Research Article,452.0
"Tian Yahui,Gel Yulia R.","Department of Mathematical Sciences, University of Texas in Dallas, USA","Received 27 December 2017, Revised 17 September 2018, Accepted 11 January 2019, Available online 16 May 2019, Version of Record 24 May 2019.",https://doi.org/10.1016/j.csda.2019.01.007,Cited by (6),A new nonparametric supervised algorithm is proposed for detecting multiple communities in complex networks using the Depth vs. Depth (DD(G)) classifier. The key idea behind the new ,", ====, ====, ====, ====, ====, ====, ====, ==== and references therein).====Although a notion of ==== is not defined uniquely and varies among domains of network applications (====, ====, ====, ====-means algorithm and model-based clustering, are sensitive to outliers, especially since a single graph can contain multiple different groups of outliers or anomalies (====).====To address the above challenges, we propose to introduce a concept of ====, ====, ====, ====, ==== and references therein). Given the proven power of depth function methodology with multivariate and functional data, it is highly appealing to extend these ideas to the complex setting of network data. Indeed, one such contribution is provided by ==== show utility of depth-based (dis)similarity measure based on ====-depth, for unsupervised network clustering. In the present treatment as well as in many applied studies, however, we deal with a data set consisting of a ==== with a small set of vertices with known membership, and the goals of detecting the network community structure and identifying its center, outliers, and other structural features.====Our key idea is to adapt the so-called data depth vs. depth classifier, or DD(G)-classifier, as a primary supervised community recovery method within a spectral clustering framework. The DD(G)-classifier is a completely nonparametric and data-driven procedure (====, ====). Recently, the depth vs. depth nonparametric classification has been systematically enhanced to a general multi-class case in functional data analysis (==== (i.e., criminal, terrorist and illicit) networks, there often exists some prior knowledge, and the primary interest is to cluster the remaining network data, given this prior set of labeled training data (====, ====, ====). Furthermore, the idea of the DD(G)-classifier can be intrinsically integrated as a part of semi-supervised network clustering, enhancing, for instance, choice of similarity measures and reducing the impact of anomalies. Finally, the notion of data depth which is a new concept in network studies, can be used in a much more general context of analysis, description and visualization of complex network structures.====The main contributions of our study are summarized as follows:====The paper is organized as follows. In Section ====, we introduce basic notations and review a concept of data depth. Section ==== proposes the new nonparametric community detection algorithm based on DD(G)-classifiers and provides an insight on its theoretical properties of the new method. Simulations studies are presented in Section ====. Section ==== illustrates application of the new DD(G)-classifier to detecting communities in terrorist networks, political blogs and bill cosponsorship in the Italian Parliament. The paper is concluded by discussion in Section ====.",Fusing data depth with complex networks: Community detection with prior information,https://www.sciencedirect.com/science/article/pii/S0167947319300088,16 May 2019,2019,Research Article,453.0
"Bindele Huybrechts F.,Nguelifack Brice M.","University of South Alabama, United States,United States Naval Academy, Annapolis, MD, United States","Received 1 June 2017, Revised 19 December 2018, Accepted 25 April 2019, Available online 15 May 2019, Version of Record 15 May 2019.",https://doi.org/10.1016/j.csda.2019.04.014,Cited by (0),"In regression modeling, it has become very common to deal with missing data, which render the statistical analysis more difficult. It is then of interest to develop robust methods toward "," (MAR), ==== (MCAR), and ==== (MNAR), which are well discussed in ====. Readers seeking for more details are referred to the aforementioned paper. While there has been a tremendous amount of work done involving the first two mechanisms (MAR and MCAR), there is still a lot to be done for the last mechanism (MNAR). Now, our focus will be given to the MNAR mechanism, also known as ====, ====, ====, ====, ====, and ====, ====, ====, ====, ====, and ====. A class of inverse probability of censoring weighted estimators under MNAR mechanism is proposed by ====. ====. Motivated by the work of ==== and ====, ==== and ====. ==== proposed an instrumental variable approach for model identification and estimation, and more recently, ==== proposed the identifiability of normal and normal mixture models with nonignorable missing data. Some other recent developments for estimation approaches under nonignorable missing data include those of ====, ====, ==== and ====.====To be more precise, consider a random sample of size ====, from which ==== is fully observed but ==== is subject to missingness, which is assumed MNAR. Also, suppose that ==== and ==== are related via the following general regression model ====where ==== is fully specified, and ==== is a vector of parameters with ====, some compact vector space, ====’s are independent and identically distributed (iid) ====, the model errors ==== are absolutely continuous, iid with positive variance. Our interest is placed in a robust and efficient estimation of the true parameter ==== when some responses in model ==== are MNAR. Set ====, if ==== is missing, and ====, if ==== is observed. As ==== is binary for ====. From the incomplete random sample ====, the MAR assumption is recovered when ====To this end, the paper is organized as follows: in Section ====, we recall the estimation of response probability using the semi-parametric logistic model proposed by ====. Asymptotic properties of such an estimator are established in Section ====. Extensive simulation studies and an illustrative real data example are given in Section ====. The paper is concluded with a short discussion. Proofs of main theoretical results are provided in the ====.====This Appendix provides proofs of main results in the paper. The following lemma, whose proof can be constructed along the lines of that given in ==== is key to proving ====. For sake of brevity, the proof will not be included here. Readers seeking for a detailed proof are referred to the aforementioned paper.",Generalized signed-rank estimation for regression models with non-ignorable missing responses,https://www.sciencedirect.com/science/article/pii/S0167947319300982,15 May 2019,2019,Research Article,454.0
"Cui Hengjian,Zhong Wei","Department of Statistics, School of Mathematical Sciences, Capital Normal University, Beijing, 100048, China,MOE Key Lab of Econometrics, Wang Yanan Institute for Studies in Economics, Department of Statistics in School of Economics and Fujian Key Lab of Statistics, Xiamen University, Xiamen, Fujian, 361005, China","Received 14 November 2017, Revised 4 May 2019, Accepted 4 May 2019, Available online 13 May 2019, Version of Record 27 May 2019.",https://doi.org/10.1016/j.csda.2019.05.004,Cited by (8)," and a continuous one ==== given each class of ==== and the unconditional distribution function of ====. The MV index is zero if and only if ==== and ==== are independent. The new MV test between ==== and ==== enjoys several appealing merits. First, an explicit form of the asymptotic null distribution is derived under the independence between ==== and ====. It provides an efficient way to compute critical values and ","where ==== denotes the empirical joint distribution function of ====, ==== and ==== denote the empirical marginal distributions of ==== and ====, respectively. This is also the well-known Cramér-von Mises criterion between the joint distribution function and the product of marginals. ==== and ==== defined a distance covariance (DC) between two random vectors ==== and ==== by ====where ==== denote the joint characteristic function, the marginal characteristic functions of ==== and ====, respectively, and ==== is a positive weight function. ==== if and only if ==== and ==== are independent. They further proposed a test of independence based on the statistic ====, where ==== is the estimator for ==== by using the corresponding empirical characteristic functions and ==== in which ==== is a random sample of ====. Under the existence of moments, it was proved that ====, where ====’s are independent standard normal random variables and the values of ==== depend on the distribution of ====. Recently, ==== developed a consistent multivariate test of association based on ranks of distances. ==== proposed another consistent test of independence based on a sign covariance related to Kendall’s tau.====In many scientific researches, it is of importance and interest to test whether one continuous random variable is statistically independent of one categorical one. For instance, the genetics researchers are interested in testing the independence between the cancer tumor types and the gene expression levels (continuous) and detecting the significant genes whose expression patterns can distinguish diverse tumor types. ====Let ==== be a categorical variable with ==== classes ====, and ==== given each ==== and the unconditional distribution function of ====. The MV index is equal to 0 if and only if two variables ==== and ==== are statistically independent. Thus, the MV index can be used to construct a test statistic for independence. The proposed MV test enjoys several appealing advantages. (1) Under the null hypothesis of independence between two variables, the asymptotic null distribution has an explicit form, ====, where ====, are independent ==== random variables with ==== degrees of freedom. It provides an efficient and fast way to compute the critical value and make a test decision quickly in practice. (2) The proposed MV test is distribution-free because there is no condition assumed on the distribution of two random variables to drive the asymptotic null distribution and the MV test statistic is invariant under one-to-one transformations of the continuous random variable. This merit is not shared by the distance covariance test (====) whose the asymptotic null distribution depends on the distribution of ====The rest of this paper is organized as follows. In Section ====, we introduce preliminaries of the mean variance index and its properties. Main results are included in Section ==== concludes the paper and discusses some extensions. Technical proofs are given in ====.====To prove ====, we first need to define ====where ====, for ====. The following lemma studies the difference between ==== and ==== under the null hypothesis of independence.==== further implies that the difference between ==== and ==== is the order of ==== in probability. That is, ====This lemma paves a road to derive the asymptotic null distribution of ==== in ====.",A distribution-free test of independence based on mean variance index,https://www.sciencedirect.com/science/article/pii/S016794731930115X,13 May 2019,2019,Research Article,455.0
"Chen Tong,Lumley Thomas","University of Auckland, New Zealand","Received 10 October 2018, Revised 2 May 2019, Accepted 2 May 2019, Available online 11 May 2019, Version of Record 21 May 2019.",https://doi.org/10.1016/j.csda.2019.05.002,Cited by (3)," of ====, where ==== is the matrix representing the quadratic form and ==== and moderate p-values ====. Motivated by genetic applications, moderate to large quadratic forms (====) and small to very small p-values ==== are studied. Existing methods are compared under these settings and a leading-eigenvalue approximation, which only takes the largest ==== to ==== on extracting eigenvalues and avoids speed problems with computing the sum of ==== and ==== in the extreme right tail, so it is usable for arbitrarily small p-values.",", where ==== is a multivariate normal random vector with mean vector ====, and ==== is a ==== symmetric and non-negative definite matrix. The question of interest is to estimate the upper tail probability of ====
 ====where ==== is a scalar.====The distribution of ==== variables, where the coefficients are the non-zero eigenvalues ==== of matrix ====. When ====, it is a linear combination of central ==== variables.====.====The null distribution of these tests is a weighted sum of central ==== variables, where the coefficients are the eigenvalues of ====. Many methods are proposed to evaluate the upper tail probability of the distribution of ====. We classified these existing methods into three categories: ‘exact’ methods (====, ====, ====) and a saddlepoint approximation (====).==== exploited the fact that the characteristic function of a sum is the product of characteristic functions, so the characteristic function for a weighted sum of ==== variables is straightforward to obtain. ==== showed that the tail probability can be written as an infinite series of central chi-squared distributions, by writing the linear combination as a mixture (====). ==== variables.====The Satterthwaite approximation approximates the distribution of ==== by ==== with ==== and ==== chosen to give the correct mean and variance. ==== proposed a four-moment approximation using a noncentral chi-squared distribution of the form ====, where ==== is an offset, ==== is a scaling parameter and ==== is the non-centrality parameter. ==== derived a form of saddlepoint approximation to the sum. The accuracy of these approximations has been previously studied (====, ====, ====), but only for small quadratic forms (====) and moderate p-values.====However, genetics studies often involve a large number of terms (====) and small p-values (====) raising concerns about both time complexity and accuracy. For time complexity, extracting all sets of eigenvalues scales as cube of sample size ==== and it would take more time to compute a tail probability when the number of terms ==== is large. For accuracy, moment methods are anti-conservative in the right tail of the distribution.====Recently, a companion paper (====) developed a leading-eigenvalue approximation to solve above problems. This method is mainly developed for large quadratic forms and ends up with less computational time without any important loss in accuracy. This is done by extracting the largest ====) and utilizing the cheap Satterthwaite approximation to approximate the rest ==== terms.====This work is motivated by genetic problems which often involve large quadratic forms with thousands or tens of thousands of terms, under which the existing methods would have a computational deficiency and may be less accurate. The main objective is to find an optimal way to perform convolutions for large quadratic forms. We provide empirical evidence for the existing methods and a leading-eigenvalue approximation under moderate and large quadratic forms. Evaluations and discussions of the existing methods under large quadratic forms are made in Section ====. In Section ==== is discussed in Section ====. Discussions are made in Section ====.====R codes for producing numerical examples can be found in Supplementary information and are available from ====.",Numerical evaluation of methods approximating the distribution of a large quadratic form in normal variables,https://www.sciencedirect.com/science/article/pii/S0167947319301136,11 May 2019,2019,Research Article,456.0
Wang Zhonglei,"MOE Key Laboratory of Econometrics, Wang Yanan Institute for Studies in Economics and School of Economics, Xiamen University, Xiamen, Fujian 361005, People’s Republic of China","Received 15 October 2017, Revised 3 March 2019, Accepted 2 May 2019, Available online 11 May 2019, Version of Record 21 May 2019.",https://doi.org/10.1016/j.csda.2019.05.001,Cited by (2)," under mild conditions. A practical method is introduced to detect the convergence of the proposed sampling algorithm. Two simulation studies are conducted to compare the proposed sampling algorithm and the corresponding Markov chain Monte Carlo methods without thinning, and results show that the estimation bias of the proposed sampling algorithm is approximately the same as the corresponding Markov chain Monte Carlo method, but the proposed sampling algorithm has a smaller Monte Carlo variance. The proposed sampling algorithm saves computer memory in the sense that the storage of a small portion of the Markov chain is required in each iteration.",", ====, ====, ====) are the most popular MCMC approaches.====MCMC is an active research area, and many methods have been proposed during the past thirty years. In order to handle the local-trap problem, where a target density has many local minima separated by regions with high densities (====), different solutions are provided. ====), and it is efficient since a cluster of spins is updated in each iteration. ==== generalized the Swendsen–Wang algorithm to other models. ==== discussed a slice sampling method, and it draws a sample from a region with large density values. ==== considered an adaptive direction sampling method to speed up the convergence of the MCMC. Based on the framework of the adaptive direction sampling method, ==== and ==== discussed MCMC algorithms when the target distribution is high-dimensional and the sample exhibits strong correlation. ==== proposed to use random walk to obtain a sample from a large graph. ====Reservoir sampling (====, ====) for drawing a sample from a data stream in a single pass, and it has been widely used in areas with data streams, such as financial applications, network monitoring and security (====In this paper, we propose a novel application of reservoir sampling to improve the standard MCMC methods in the sense that the generated sample is less correlated. The proposed sampling algorithm is a stochastic thinning procedure, and it can be embedded in most MCMC methods. The distribution of the sample generated by the proposed sampling algorithm converges in total variation to the target distribution in probability under mild conditions. It saves computer memory by storing a small portion of a Markov chain.====The rest of the paper is organized as follows. In Section ====, the concepts of MCMC methods and the Metropolis–Hastings algorithm are briefly reviewed. The sampling algorithm embedded in the Metropolis–Hastings algorithm is proposed in Section ====. A practical way to detect the convergence of the proposed sampling algorithm is introduced in Section ====. Two simulation studies are conducted to test the performance of the proposed sampling algorithm in Section ====. Discussion is given in Section ====.====Given ====, we can show that ==== with probability 1. More generally, we can show ==== for ====. The probability that an element is not removed achieves its maximum at the removal of the first element. Thus, we have ====Consider the function ==== for ====. Its first-order derivative with respect to ==== is ==== since ====. By ====, we have ====Consider the function ==== for ====. Its first-order derivative with respect to ==== is ==== where the inequality holds since ==== for ====. Since ====, by ====, ==== is a decreasing function. Thus, by ====, we have ====Thus, we have proved ====.====By the removal procedure, we can show ====for ====. Thus, we have ====By ====, for ==== and ====, there exists ==== such that ====holds for ====. By ====–====, we have proved ====.",Markov chain Monte Carlo sampling using a reservoir method,https://www.sciencedirect.com/science/article/pii/S0167947319301124,11 May 2019,2019,Research Article,457.0
"Li Haoqi,Lin Huazhen,Yip Paul S.F.,Li Yuan","School of Economics and Statistics, Guangzhou University, Guangzhou, 510006, China,Center of Statistical Research and School of Statistics, Southwestern University of Finance and Economics, Chengdu, 611130, China,The Hong Kong Jockey Club Centre for Suicide Research and Prevention, Department of Social Work, The University of Hong Kong, Hong Kong","Received 20 September 2018, Revised 21 April 2019, Accepted 27 April 2019, Available online 7 May 2019, Version of Record 17 May 2019.",https://doi.org/10.1016/j.csda.2019.04.016,Cited by (1),"A generalized partial linear regression model is proposed to estimate population size at a specific time from multiple lists of a time-varying and ====. The challenge is that we have millions of records and hundreds of parameters for a long period of time. This presents a challenge for data analysis, mainly due to the limitation of computer memory, computational convergence and infeasibility. In the paper, an analytical methodology is proposed for modeling a large data set with a large number of parameters. The basic idea is to apply the ==== to data observed at each time separately, and then combine these results via weighted averages so that the final estimator becomes the maximum likelihood estimator of the whole data set (full MLE). The ==== and inference of the proposed estimators is derived. Simulation studies show that the proposed procedure gives exactly the same performance as the full MLE, but the proposed method is computationally feasible while the full MLE is not, and has much lower computational cost than the full MLE if both methods work. The proposed method is applied to estimate the number of drug-abusers in Hong Kong over the period 1977–2014.","This work is intended to estimate the number of drug abusers in Hong Kong during the period 1977–2014. The Central Registry of Drug Abuse (CRDA) was established in the Narcotics Division of the Hong Kong Government to monitor drug abusers in Hong Kong. Reports of drug abusers on record are provided by different agencies and submitted to the CRDA on a standard sheet, including ID, year of contact, age on contact, sex, education, marital status, and work activity status. There are four major agencies: the Police Department, Correctional Services Department, Social Welfare Department, and hospitals. Certainly, there are individuals who are not “captured” by any of these agencies. The overlap among the agencies (lists) provides useful information on the number of “uncaptured” individuals (====, ====). Our interest is in estimating the number of drug abusers or, the number of uncaptured drug abusers for each half-year during the period 1977–2014.====, we need 96 parameters after all.====In the literature, many methods have been proposed for handling multiple-lists in a closed population: the Poisson log-linear model (====, ====, ====, ====), the multinomial model (====), and the sample coverage method (====). There are also methods for dealing with the open population problem (====, ====, ====, ====, ====, ====, ====). All the existing methods do not elaborate how to handle covariates and very large data. In addition they have not been extended to analyze dynamic multiple-list experiments. ==== proposed a semi-parametric method to estimate population sizes from multiple lists of a dynamic population. However, they made no adjustment for covariates such as sex, age, or activity status.====). For example, in the presence of continuous covariates, ==== and ====). ==== extended their method to include certain list interactions when there are many lists. ==== developed log-linear models that used penalized splines to express dependence among continuous covariates. ====, ==== and ====. However, these regression analyses are proposed only for small to medium data sets.====, ====) and meta analysis (====, ====, ====, ====), but there are few differences. First, the separated subsets for our method are time-dependent and different from each other, while both divide-and-conquer method and meta analysis require the subsets to have the same distribution. As a result, each block of data in our method involves the common parameters (====) and its own parameters (====), while the divide-and-conquer method and the meta analysis have the common parameters for all subsets.====The rest of the paper is organized as follows. Section ==== elaborates the model, the proposed COWA estimation and the sampling properties of the estimators. Based on the sampling properties, we also introduce a formula to estimate the variance of the COWA estimators in order to construct their confidence intervals for the parameters as well as population size. Section ==== presents simulation studies to investigate the performance of the COWA method. We apply the COWA method to Hong Kong drug abuse data and report the findings in Section ====. A discussion is presented in Section ====. For other researchers interested in our COWA method, we developed an efficient and user-friendly R package called COWA, which is available at ====.",Estimating population size of heterogeneous populations with large data sets and a large number of parameters,https://www.sciencedirect.com/science/article/pii/S0167947319301069,7 May 2019,2019,Research Article,458.0
"Liu Lili,Lin Lu","Zhongtai Securities Institute for Financial Studies, Shandong University, Jinan, China,School of Statistics, Qufu Normal University, Qufu, China","Received 16 September 2018, Revised 1 April 2019, Accepted 20 April 2019, Available online 27 April 2019, Version of Record 6 May 2019.",https://doi.org/10.1016/j.csda.2019.04.011,Cited by (7),-consistency and ,"). Another wide range of real-world applications is precision marketing. Heterogeneity of marketing strategies reflects the diversity of customers’ consumption behaviors and preferences. Precision marketing offers personalized customer service and is used to help enterprises increase their profits by identifying the different marketing subgroups (see ====There has been much work on subgroup analysis in the literature. ==== proposed using the logistic-normal mixture model approach to deal with the heterogeneity. However, this approach requires the specified number of mixture components and an underlying distribution assumption for the model. ==== partitioned the sample space of piecewise single-index models into several regions with the knowledge of a priori classification. In addition, some penalization methods have been well developed. For example, ==== proposed the fused lasso method which applies the pairwise ====, ====, ====). Our work concerns identifying subgroups of the observations, which is different from these penalty methods about studying on grouping effects of covariates.====We consider the following additive partially linear model with both homogeneous and heterogeneous componentsas ====where ==== is the homogeneous coefficient vector, ==== with ==== being heterogeneous intercepts and ====
 ==== being unknown smooth functions, ==== is the observation of ====, ==== is independent and identically distributed observation of ====. The random error ==== is independent of ==== with ==== and ====. It is assumed that ====
 ==== for identification purpose. Without loss of generality, we assume that all the covariates have mean zero. Our goal is to identify the subgroups of ==== for ====, such that ==== and the homogeneous parameter ====.==== approximated link functions in the single-index model by B-spline and obtained the consistent M-estimators of parametric and nonparametric components; ====, ====, ==== and among others.====A challenging problem is how to identify the subgroups when the number ==== of subgroups is unknown in advance. For linear model with unknown ====, ==== used the concave pairwise fusion penalty approach to identify heterogeneous subgroups. When ==== and ==== are large, however, this method is complicated and unstable since their implementation requires iteratively storing and manipulating the entire ====-dimensional parameters. The memory and computational cost can be extremely high. ====) with the weighted ==== penalty for the convex clustering. However, the ==== penalty can generate biases of the estimates, and the choice of the weights can affect the quality of the clustering solution, and there is no clear rule for choosing the valid weights. ==== proposed penalized regression-based clustering using the non-convex grouped truncated lasso penalty (gTLP). The non-convex gTLP performs much better than the Lasso and other ====-norms, since the parameter estimation bias is largely avoided. However, it will be seen later that this algorithm is relatively complicated than ours.==== unknown parameters simultaneously, making the estimation and classification procedures feasible. Moreover, our methods apply to the heterogeneous additive partially linear models when each nonparametric component is approximated by a linear combination of B-spline basis functions. Then we propose an ADMM algorithm for a modified ====The rest of the paper is organized as follows. In Section ==== introduces the theoretical properties of the proposed procedure. Section ==== we discuss the application of our model to car sales data. All detailed proofs are presented in ====.",Subgroup analysis for heterogeneous additive partially linear models and its application to car sales data,https://www.sciencedirect.com/science/article/pii/S0167947319300957,27 April 2019,2019,Research Article,459.0
"Sinha Shyamalendu,Hart Jeffrey D.","Department of Statistics, Texas A&M University, 3143 TAMU, College Station, TX 77843-3143, USA","Received 27 June 2018, Revised 5 April 2019, Accepted 8 April 2019, Available online 24 April 2019, Version of Record 4 May 2019.",https://doi.org/10.1016/j.csda.2019.04.006,Cited by (0)," methods, which are based on a normal–normal model. When fitting a mixture model, the algorithm is essentially clustering the unobserved mean and variance pairs into different groups, with each group having a different normal-inverse gamma distribution. The proposed estimator of each mean is the posterior mean of shrinkage estimates, each of which shrinks a sample mean towards a different component of the mixture distribution. The proposed estimator of variance has an analogous interpretation in terms of sample variances and components of the mixture distribution. If the diagonal covariance matrix is known, then the sample size can be as small as 1, and the pairs of known variances and unknown means across dimensions are treated as random observations coming from a flexible mixture of normal-inverse gamma distributions.",", ====, are observations generated from the following model: ====In multivariate notation, ====, ====, are ==== observations from a ====-variate normal distribution with mean vector ====.====The following assumptions are made:====The main goal is to estimate ====, ====, from ====, ====, ====. A secondary goal, which is necessary to efficiently achieve the main goal, is to estimate ====, the joint distribution of ====.====If ==== are not needed to estimate ====. Without loss of generality, we may assume ====, in which case model ==== reduces to ====In this case, we observe the pairs ====, ====, and the main goal is to estimate the unknown parameters ====, ====.====In the classical one-dimensional framework, i.e. ====, the sample mean, ====, and ==== showed that the vector of sample means is inadmissible when ====. The seminal work of ==== and ==== are all the same (the homoscedastic case) and known. A nice introduction to this class of estimators can be found in the book of ====. ====. This is because the shrinkage factor remains constant under the transformation, as opposed to what intuition entails, namely that more shrinkage should be applied to the components with larger ====. They assumed a hierarchical normal model in which ====, and estimated the variance ==== from the marginal density of ====. As noted by ====, such a hierarchical model is a “Bayesian statement of belief that the ==== are of comparable magnitude”, a belief which is not always realistic.==== showed that some common choices of improper prior on hyperparameters lead to inadmissible estimators, and encouraged the use of a proper prior on hyperparameters. ==== proposed a nonparametric empirical Bayes solution for estimating the mean.====In contrast, the literature on the heteroscedastic case is scant. ==== assumed that ==== is known and estimated the mean vector, ====, by minimizing Stein’s unbiased risk estimate (====) of ==== and ==== estimators of ==== do not provide the same solution as in the homoscedastic case and proved a few results about the consistency of the ==== estimates. By not limiting the prior on the normal density, they explored a semiparametric option which we will discuss in detail later. ==== further extended the work of ==== in the heteroscedastic case when ==== is unknown by modifying the loss function and assuming a gamma prior on the precision parameters, the inverse of the variance parameters.====Theorem 5.7 of ==== should be shrunk more. ==== proposed a minimax estimator when the covariance matrix ==== is known under arbitrary quadratic loss, where the shrinking direction is open to specification and the shrinking factor is determined. This minimax estimator is similar to the estimator arising from the assumption that ==== are independent with ====, ====. ==== developed an empirical Bayes method to estimate a sparse normal mean. ==== developed an empirical Bayes estimator assuming that ==== are part of the random observations. They binned the pairs ==== on the basis of ==== and applied a spherically symmetric estimator separately in each group. Even though we also assume that ==== come from a joint distribution, ==== and ====.====See ====, ====, ==== and ====.",Estimating the mean and variance of a high-dimensional normal distribution using a mixture prior,https://www.sciencedirect.com/science/article/pii/S0167947319300908,24 April 2019,2019,Research Article,460.0
"Mazo Gildas,Averyanov Yaroslav","MaIAGE, INRA, Université Paris-Saclay, 78350, Jouy-en-Josas, France,MODAL, Inria Lille Nord Europe, Lille, France","Received 14 May 2018, Revised 12 April 2019, Accepted 15 April 2019, Available online 22 April 2019, Version of Record 30 April 2019.",https://doi.org/10.1016/j.csda.2019.04.010,Cited by (2),A novel algorithm for performing inference and/or clustering in semiparametric copula-based mixture models is presented. The standard ==== is replaced by a weighted version that permits to take into account the constraints put on the underlying marginal densities. Lower ==== error rates and better estimates are obtained on simulations. The ,", ====) is not able to deal with heterogeneous clusters either.====Recently more flexible models have been considered. On the one hand, there are copula-based methods (====, ====), Knowledge Discovery and Database Management (====). Copulas allow for concatenating discrete and continuous data, too (====). In this paper, we only consider continuous data.====, ====).====In this paper, following the work in ==== and ours. In the former, the distributions in the clusters were not allowed to vary in scale. In the latter, change in scale is possible. This additional degree of freedom induces a structural constraint on the component marginal densities of the mixture. The constraint is not satisfied by the kernel density estimator used in the algorithm in ====The rest of this paper is as follows. We present the models in Section ====. The first part reviews the paradigms under which one can build mixture models (Gaussian, copula-based, nonparametric and semiparametric) and the second part presents the model of interest in this paper. We give the learning algorithms in Section ====. Section ==== contains the definition and the consistency result for the weighted kernel density estimator. This section is written in a generic framework and therefore can be read independently. Sections ====, ==== contain the simulation experiments and the real data analysis, respectively. A summary closes the paper.====From ====, p. 155, we know that ====where ====, ==== is the distribution function of the exponential distribution, ==== is the distribution function of the Gumbel distribution and ====, ====. Let ====, ====, be the distribution function of the Laplace distribution on the positive real line. Let ====, ==== and ====. By identification of the binomial coefficients in the binomial theorem, we have ====meaning that ==== and ==== are the appropriate constants. If ====, the same formula applies because ====.  □====In order to obtain the desired formulas ==== and ==== it is convenient to introduce ====so that we have the decompositions by blocks: ====Let ==== be the projection matrix onto the linear space spanned by the rows of ====. With this notation, we have ====Decomposing ==== and applying formula ==== then yields ==== with ====, this last equality being equivalent to ====.====We now introduce an intermediate lemma in order to facilitate the study of remainder terms which shall appear in the proof of consistency.====We write ====and the proof will be complete if (i) ==== and (ii) ==== can be bounded above by a quantity which would not depend on ==== and would vanish asymptotically.====We first show (i). We have ====
 The first term on the right hand side is a ==== and does not depend on ====. Now ==== where ==== is a compact notation for ====. By assumption, ==== converges in distribution. By symmetry, so does ====. Hence, by the continuous mapping theorem, the maximum of ==== and ==== converges in distribution. Thus ==== The bound does not depend on ==== and vanishes asymptotically in probability by assumption on the sequences ==== and ====. This is enough to conclude that (i) holds with probability tending to one.====We finally show (ii). It is convenient to introduce ==== the proof of which is deferred to the end of this section.====In view of ====, one has ====(we used the fact that ==== for the denominator). By assumption and by symmetry, both ==== and ==== are ==== and by assumption on ====, ====Hence the numerator in ==== is ====. The denominator equals ==== if ==== and equals ==== if ====. Either way, the denominator tends to a constant in probability and ==== This upper bound does not depend on ==== and vanishes asymptotically in probability. This proves (ii). It only remains to prove ====.",Constraining kernel estimators in semiparametric copula mixture models,https://www.sciencedirect.com/science/article/pii/S0167947319300945,22 April 2019,2019,Research Article,461.0
"Lin Chang-Yun,Yang Po","Department of Applied Mathematics and Institute of Statistics, National Chung Hsing University, Taichung, 40227, Taiwan,Department of Statistics, University of Manitoba, Winnipeg, MB R3T 2N2, Canada","Received 24 September 2018, Revised 5 March 2019, Accepted 9 March 2019, Available online 22 April 2019, Version of Record 6 May 2019.",https://doi.org/10.1016/j.csda.2019.03.007,Cited by (4),"Multistratum designs have gained much attention recently. Most criteria, such as the ====-====-optimal designs and one-stage generalized Bayesian ====-optimal designs, we show that the GBDD-optimal designs have higher efficiency on fitting the true models. The extensions of the GBDD criterion for more complicated cases, such as more than two stages of experiments and more than one class of potential terms, are also developed.","The ====, page 135). However, it is known that the ==== criterion is too model dependent. If the given model is misspecified, then the ====-optimal design may not be the best choice for fitting the true model. To overcome this problem, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====). Some experiments are even more complicated and the designs used for the experiments have more than two strata, such as the split–split-plot designs (see ====, ====), blocked split-plot designs (see ====, ====, ====), strip-plot designs (see ====, ====, ====, ====, ====), and staggered-level designs (see ====, ====, ====, ====, ====). The BD criterion proposed by ==== requires the experiments being completely randomized and hence cannot be used for selecting designs which have multistratum structures. To further extend the BD criterion, ==== (GBD) criterion, which can be used not only to choose single-stratum designs for completely randomized experiments but also to select multistratum designs for more completed experiments.====To select designs using the GBD or BD criterion, experimenters need to specify a set of potential terms. An appropriate set of potential terms should include as many important effects as possible. However, when the true model is highly uncertain, many potential terms given by the experimenters may not be actually active. If this is the case, then the designs selected by the two criteria may not be efficient for fitting the true model. To deal with the problem of highly uncertain models, some authors have suggested conducting an experiment in two stages (see ====, ====). The idea is that a smaller design is first selected and used to collect data in the first-stage experiment; then the information about the true model is extracted from the data and used to select a design for the second-stage experiment. In the literature, the methods for constructing two-stage designs were developed for the completely randomized experiments. Since many experiments have more complex structures, it is important to study how to construct multistratum designs for two-stage experiments to increase design efficiency when the true model is highly uncertain.====In this paper, we establish a framework under multistratum structures for two-stage experiments. By applying the GBD criterion, we propose a construction method to select two-stage multistratum designs. In Section ====, the multistratum structure and the GBD criterion are introduced. Section ==== describes the method we propose for constructing two-stage multistratum designs. An example for selecting two-stage split-plot designs is given in Section ====. In Section ====, we evaluate the efficiencies of the two-stage designs and compare them with those of the one-stage GBD-optimal designs and ====-optimal designs. In Section ====, we apply our method to construct more complicated multistratum designs, such as the strip-plot designs and the staggered-level designs. The extensions of our method for more complicated cases are discussed in Section ====. Section ==== is the concluding remarks.",Data-driven multistratum designs with the generalized Bayesian ,https://www.sciencedirect.com/science/article/pii/S016794731930074X,22 April 2019,2019,Research Article,462.0
"Johnson Brad C.,Kirkland Steve","Department of Statistics, University of Manitoba, Winnipeg, Manitoba, Canada,Department of Mathematics, University of Manitoba, Winnipeg, Manitoba, Canada","Received 12 April 2018, Revised 12 April 2019, Accepted 13 April 2019, Available online 22 April 2019, Version of Record 4 May 2019.",https://doi.org/10.1016/j.csda.2019.04.009,Cited by (4)," for the Markov chain has a cut-point, an alternate strategy for computing the random walk centrality is outlined that may be of use when the centrality values are of interest for only some of the states. In order to illustrate the effectiveness of the results, estimates of the random walk centrality arising from random walks for several directed and undirected graphs are discussed.","Let ====. Define ==== by ====as the first passage time from state ==== to state ====. For each ====, we denote the mean first passage time from state ==== to state ==== by ====. Letting ==== be the stationary distribution vector for the Markov chain, it is well-known that ==== for each ====.====Fix a state ====. The accessibility index for state ====, denoted by ====, is given by ====and measures the expected time to reach state ==== from a randomly chosen initial state (that is, randomly chosen according to the stationary distribution). Thus the ==== correspond to states that are easy to reach, while high values of ==== correspond to states that are not so easy to reach. Indeed, in ====, introduce the so-called random walk centrality for vertex ====, which we denote here by ====. In ====, it is shown that in Noh and Reiger’s setting, ==== for each vertex in the graph. Thus the accessibility index can be seen as an extension of the notion of random walk centrality to irreducible Markov chains.====How might one compute the ====s? On the face of it, the computation might be costly and time-consuming, as both the mean first passage times and the entries in the stationary distribution would need to be found first. However, an observation in ==== suggests a statistical approach to estimating the accessibility indices. In order to outline that approach, we need a little more notation. Define ==== as the time of the first visit to state ==== and, for ====, ====so that ==== is the time of the ====th return to state ====. Finally, for ==== and ====, define ====That is, ==== is the ====th inter-arrival time between visits to state ====. Given ====, the ====. Theorem 1.1 of ==== shows that the accessibility index ==== may be written as ====
 Eq. ==== informs our approach to estimating the ====s: by taking a realization of the Markov chain, we can produce estimates of both ==== and ==== in order to estimate ====. We observe that this strategy may offer an advantage in the situation that the state space is large but one is only interested in the accessibility indices for a small number of states, or where explicit computation of the stationary distribution and/or the mean first passage times is prohibitively expensive or numerically unstable.====Given a sequence ====, a ratio type estimate of ==== is given by ====For the corresponding random walk centrality measure ====, the plug in estimate is simply ====For a large integer ====, let ==== be a realization of a random walk of length ==== on ==== starting at an arbitrary state ====. Then, for each ====, we also obtain a vector of (====) realizations of ==== (say ====). In ==== (====), this is easily accomplished by the command ====Provided that the number of visits ==== to state ==== in ==== is reasonably large, a (consistent) estimate of ==== is given by ====Note that, since ==== as ==== for all ====. Furthermore, since ==== exists for all ==== and ====, we have, ==== almost surely and it follows by (for example) Slutsky’s Theorem, that ==== is consistent for ==== (and that ==== is consistent for ====).====The estimator ==== suffers from two sources of bias. The first source is from the fact that ==== Chapters 12–14, for example). The second source of bias is due to the fact that, by using random walks, we are essentially sampling from a truncated distribution of ==== because inter-arrival times longer than the walk length are impossible. Longer random walks should mitigate this bias. In practice, we perform a moderate number, say ====, of independent random walks, each of length ====, with random starting points.====In this paper we focus on strongly connected directed and undirected graphs, with the object of estimating the accessibility indices (equivalently, the random walk centralities) for the vertices using the technique described above. Section ==== presents some numerical results that illustrate our technique. In Section ==== we prove a theoretical result for graphs with a cut-point that allows one to compute mean first passage times, stationary distribution entries and accessibility indices by working with the corresponding quantities for Markov chains on a smaller state space. Again this result may offer some advantage in the setting where one is only interested in these quantities for a subset of states.",Estimating random walk centrality in networks,https://www.sciencedirect.com/science/article/pii/S0167947319300933,22 April 2019,2019,Research Article,463.0
"Liang Wei,Dai Hongsheng,He Shuyuan","Xiamen University, China,University of Essex, UK,Capital Normal University, China","Received 26 October 2018, Revised 7 April 2019, Accepted 7 April 2019, Available online 17 April 2019, Version of Record 29 April 2019.",https://doi.org/10.1016/j.csda.2019.04.007,Cited by (11)," are widely used in different settings to construct the confidence regions for parameters which satisfy the moment constraints. However, the empirical likelihood ratio confidence regions may have poor accuracy, especially for small sample sizes and multi-dimensional situations. A novel Mean Empirical Likelihood (MEL) method is proposed. A new pseudo dataset using the means of observation values is constructed to define the empirical likelihood ratio and it is proved that this MEL ratio satisfies Wilks’ theorem. Simulations with different examples are given to assess its finite sample performance, which shows that the confidence regions constructed by Mean Empirical Likelihood are much more accurate than that of the other Empirical Likelihood methods.","Empirical likelihood (EL) method proposed by ==== is a very powerful tool in non-parametric and semi-parametric statistics (====, ====, ====, ====, with an unknown distribution function ====, are observed. The research interest is the estimation problem for a ====-dimensional parameter ====. The true parameter value ==== for some ====-dimensional function ====. The original Empirical Likelihood (OEL) is defined as ====Assume ====. ==== proved that ====Therefore, the ==== confidence region can be constructed as ====, where ==== is such that ====.====Although EL method has found its application in many statistical areas, its finite sample properties may not work well because of low precision of ====, ==== and ==== showed that empirical likelihood ratio confidence regions could have poor accuracy, especially in small sample and multi-dimensional situations. Many methods have been proposed to improve the performance of the EL approach in the literature. For parameters defined by standard estimating equations, the Bartlett correction Empirical Likelihood (BEL) (====) achieves the second order accuracy, which is substantially more accurate than the original EL approach. An alternative method is to add a pseudo-observation to the sample. This leads to the adjusted Empirical Likelihood (AEL) (====) and it also achieves the second order accuracy. Recently, ==== developed an extended Empirical Likelihood method (EEL), attaining the second order accuracy as well. However, all the above-mentioned methods require the calculation of the Bartlett correction constant, which has no analytical formula since it depends on the moments of ====. In practice, using a ====-consistent estimator for Bartlett correction constant is feasible, but it may be difficult to calculate the estimator in certain practical scenarios (====). Apart from the practical estimation challenge for Bartlett correction constant, ==== proved that exponentially tilted likelihood is actually not Bartlett correctable and all existing methods only have the first order accuracy. Therefore, all the above practical and theoretical challenges motivate us to search new EL approaches.====In this paper a new method, named Mean Empirical Likelihood (MEL), is presented. It constructs an empirical likelihood function based on a set of pseudo data and it is easy to compute (not requiring the calculation of the Bartlett correction constant). The large sample properties of MEL are presented. This new MEL is particularly more important for exponentially tilted likelihood, where Bartlett correction is not available. Simulation studies find that the confidence intervals constructed by MEL are much more accurate than those found by the other Empirical Likelihood methods with second order accuracy, such as BEL and AEL. In particular, MEL outperforms all other methods for heavy-tail or highly-skewed distributions and for exponentially tilted likelihood.====This paper is constructed as follows. Section ==== presents the MEL methodologies in different settings: standard estimating equation framework, two-sample mean comparison problem, exponentially tilted likelihood and generalized empirical likelihood framework, with all theoretical proofs provided in Appendix. Simulation studies are presented in Section ==== and they demonstrate that MEL outperforms all other existing methods. Section ==== provides a real data analysis and the paper concludes with a discussion in Section ====.====Denote ==== and ====. We shall introduce the following lemma, which is a key for the proof of ====.====Now we can prove ====.",Mean Empirical Likelihood,https://www.sciencedirect.com/science/article/pii/S016794731930091X,17 April 2019,2019,Research Article,464.0
"Lu Jun,Zhu Xuehu,Lin Lu","School of Statistics and Mathematics, Zhejiang Gongshang University, Hangzhou, China,Zhongtai Securities Institute for Financial Studies, Shandong University, Ji’nan, China,School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China,Department of Mathematics, Hong Kong Baptist University, Hong Kong,Department of Statistics, Qufu Normal University, Qufu, China","Received 22 May 2018, Revised 1 March 2019, Accepted 9 March 2019, Available online 16 April 2019, Version of Record 6 May 2019.",https://doi.org/10.1016/j.csda.2019.03.006,Cited by (3),"In this paper, we propose a novel method to consistently estimate, at the root-","Note that almost all the existing methods rely on the model unbiasedness to derive consistent estimation of the parameters of interest, whereas there are few proposals in the literature to discuss about the issue of model bias. Thus, studying how to consistently estimate the parameters or more generally the regression function in such models is obviously in demand. A direct, but key idea is to correct model bias. In this paper, we provide a bias-correction estimation method for partial linear single index models (PLSIMs) which covers several important models as special cases such as the linear, single index, and partial linear models. Suppose that the full model with all variables ==== is in the form: ====where ==== is the response, ==== and ==== are respectively two sets of the predictors, ==== is an unknown smooth function, ==== is the error term such that ====. In the present paper, we consider the case where ==== and ==== have no overlapping variables. Without loss of generality, we assume that the ==== norm of ==== has been widely studied in statistical analysis for its nice model interpretation and dimension-reduced structure. The relevant references include ====, ====, ==== and ====. A relevant reference about model checking is ====.====As is stated above, if only a part of “important predictors” is included into the regression function, the model ==== might be biased. Specifically, let ==== and ==== be the sets of “important predictors” of ==== and ====, respectively, where ==== and ==== are two index sets. Without loss of generality, suppose ==== and ====. Also, we define ==== and ==== respectively as the sets of the rest predictors of ==== in ==== and ==== in ====, where ==== is the complement of ==== in ==== and ==== is the complement of ==== in ====. Similarly, the corresponding coefficients are respectively written as ==== and ====, and ==== and ====. Consequently, the full model ==== can be reformulated as ====where ====. It is often the case that many of the predictors are correlated with each other. Therefore, if we want to consistently estimate the regression coefficients or the nonparametric smooth function, the following two conditions are required: First, the model itself should be sparse; second, ==== and ==== and ====. Otherwise, the model ==== is biased in the sense that ====We now give the outline of the newly proposed bias-correction procedure. First of all, note that the model ==== contains a nonparametric smooth function ====The rest of this paper is organized as follows. Section ==== introduces a transformation of the PLSIM into an equivalent pro forma linear model. Section ==== presents the main steps of constructing the artificial variable. In Section ====, a thresholding ridge ratio criterion is developed to estimate the dimension of the artificial variable. Section ====The following is the Supplementary material related to this article. ",Estimation for biased partial linear single index models,https://www.sciencedirect.com/science/article/pii/S0167947319300738,16 April 2019,2019,Research Article,465.0
"LeSage James P.,Chih Yao-Yu,Vance Colin","Texas State University, Department of Finance & Economics, 601 University Drive, San Marcos, TX 78666, USA,RWI Leibniz Institute for Economic Research and Jacobs University Bremen, Hohenzollernstr. 1-3, D-45128 Essen, Germany","Received 22 September 2018, Revised 1 April 2019, Accepted 4 April 2019, Available online 12 April 2019, Version of Record 25 April 2019.",https://doi.org/10.1016/j.csda.2019.04.003,Cited by (8),"Focus is on efficient estimation of a dynamic space–time panel data model that incorporates spatial dependence, ====, as well as space–time covariance and can be implemented where there are a large number of spatial units and time periods. Quasi-maximum likelihood (QML) estimation in cases involving large samples poses computational challenges because optimizing the (log) likelihood requires: (1) evaluating the log-determinant of a large matrix that appears in the likelihood, (2) imposing stability restrictions on parameters reflecting space–time dynamics, and (3) simulations to produce an empirical distribution of the partial derivatives used to interpret model estimates that require numerous inversions of large matrices.","Dynamic panel data models that accommodate spatial dependence have attracted attention in the spatial econometrics panel data literature. A variety of models that control for various types of correlation across locations have been explored. ==== consider a dynamic model that models spatial dependence in the error terms, whereas ==== introduce a dynamic panel specification that treats spatial dependence in the dependent variable vector. ==== of cross-sectional observations as well as a large number (====) of time periods. The approach taken draws on previous work by ====, ====, ==== who introduce a space–time filter view of these models. Since we are focusing on cases involving large sample sizes, the data generating process can be treated as conditional on the initial period cross-sectional observations, as in ====. For models that treat spatial dependence in the disturbances rather than the dependent variable, treatment of the initial period observations in small ==== situations has been found to be important (====). ====, ====, ==== provide Monte Carlo results showing that when the initial cross-sectional observations are endogenous, but incorrectly treated as exogenous, estimates and inferences can be biased, a finding similar to that of ==== also shows that when the number of time periods becomes large, treatment of the initial period observations becomes irrelevant, which is the case addressed here.====The model is a space–time dynamic extension of the panel data model in ====, where the ==== vector ==== are region-specific fixed effects and ==== a time-period specific fixed effect, with ==== being an ==== vector of ones. ==== The ==== vector ==== in ====, is defined as ====, and the ==== vector ====, where it is assumed that ==== are independent, identically distributed (====) across ==== and ====, with zero mean and constant scalar variance ====. The ==== matrix ==== observations ==== are assumed observable, since with large ==== samples this should not matter. The ==== matrix of spatial weights ==== is assumed to be non-stochastic, and row-normalized to have row-sums of one.==== discuss quasi-maximum likelihood (QML) estimation of the model in ==== after eliminating both region-specific and time-specific fixed effects. They propose a transformation procedure to eliminate ==== from the model in ==== involves minimizing the negative of the log-likelihood function, subject to stability restrictions on the spatiotemporal parameters ====. For notational simplicity in the sequel, it is assumed that fixed effects have been eliminated from the model relationship.====In cases involving large ==== and ====, computational challenges arise because optimizing the (log) likelihood requires: (1) evaluating the log-determinant of an ==== matrix that appears in the likelihood, (2) imposing stability restrictions on parameters reflecting space–time dynamics, and (3) simulations involving numerous inversions of large matrices needed to produce an empirical distribution for partial derivatives used to interpret the model.====, ====).====We provide an illustration of the method using a sample of ==== daily gas station prices for more than ====), resulting in ==== greater than six million. In the application, fixed effects for the brand configuration of each station and its nearest neighboring station are introduced. Using price markups (over cost) as the dependent variable, we explore brand competition/cooperation between six different station brands.====Section ==== sets forth space–time filter expressions for the model and discusses computational issues that arise, along with proposed solutions. Section ==== summarizes results from a Monte Carlo study that examined the accuracy and speed of MCMC estimation. Section ==== applies the estimation method to the price markup model of German gas stations, where ==== exceeds six million.====The following is the Supplementary material related to this article. ",Markov Chain Monte Carlo estimation of spatial dynamic panel models for large samples,https://www.sciencedirect.com/science/article/pii/S0167947319300878,12 April 2019,2019,Research Article,466.0
"Zhang Yuexia,Qin Guoyou,Zhu Zhongyi,Xu Wanghong","Department of Statistics, Fudan University, Shanghai 200433, China,Department of Biostatistics, School of Public Health and Key Laboratory of Public Health Safety, Fudan University, Shanghai 200032, China,Collaborative Innovation Center of Social Risks Governance in Health, Fudan University, Shanghai 200032, China,Department of Epidemiology, School of Public Health and Key Laboratory of Public Health Safety, Fudan University, Shanghai 200032, China","Received 14 June 2018, Revised 3 April 2019, Accepted 5 April 2019, Available online 11 April 2019, Version of Record 24 April 2019.",https://doi.org/10.1016/j.csda.2019.04.002,Cited by (3),"A new robust estimating equation approach for analysis of ==== is developed. To achieve robustness against outliers, a novel approach which corrects the bias induced by outliers through centralizing the ====. Extensive simulation studies show that the proposed method is robust, has a high efficiency, and is not limited to some specific error distributions. In the end, the proposed method is applied to the longitudinal study of prevalent patients with type 2 diabetes and confirms the effectiveness of dietary fibre intake in reducing glycolated hemoglobin A1c level.",", ====, ====, ====, ==== and ====) and ====, ====, ====). Therefore, it is important to consider robust methods, especially for longitudinal data analysis. Because in longitudinal studies, an outlier in a subject-level measurement can generate multiple outliers in the sample (====). Our research is motivated by a longitudinal study of prevalent patients with type 2 diabetes conducted in Shanghai, China (====, ====, ====). This study was designed to determine the effectiveness of dietary fibre intake in reducing glycolated hemoglobin A1c (HbA1c) level. A total of ==== patients are excluded from the present study. To preliminarily explore potential outliers in the data, we adopt the method proposed by ====, which is plotted by using the function ==== in ==== (version 3.5.1) (====). It is apparent that there are some potential regression outliers with large robust standardized residuals, among which there are several bad leverage points with high robust distances, such as data points 60, 288 and 330. Hence, it is necessary to consider robust analysis of this data set.====For longitudinal data analysis, it is more important to incorporate the within-subject correlation into the robust methods for improvement of estimation efficiency, because the robust methods usually lose some efficiencies for achievement of robustness. Robust generalized estimating equation methods (====, ====, ====, ====, ====, ====), but it still enjoys the good properties of the classical GEE approach. Extensive simulation studies show that the proposed method exhibits both robustness and high efficiency. Besides, the results of the proposed method are not sensitive to the local change of bandwidth, which is important for the local linear smoother.====The rest of this article is organized as follows. In Section ====, the mean model and outlier generation models are specified. Besides, the new robust estimating equation method is proposed. In Section ====, extensive simulation studies are carried out to assess the performance of the proposed method. In Section ====, the proposed method is applied to real data analysis. Concluding remarks can be found in Section ====The following is the Supplementary material related to this article. ",A novel robust approach for analysis of longitudinal data,https://www.sciencedirect.com/science/article/pii/S0167947319300866,11 April 2019,2019,Research Article,467.0
"Vexler Albert,Zou Li,Hutson Alan D.","Department of Biostatistics, State University of New York at Buffalo, Buffalo, USA,Department of Statistics and Biostatistics, California State University, East Bay, CA, USA,Department of Biostatistics and Bioinformatics, Roswell Park Cancer Institute, Buffalo, NY, USA","Received 3 January 2018, Revised 2 April 2019, Accepted 3 April 2019, Available online 11 April 2019, Version of Record 24 April 2019.",https://doi.org/10.1016/j.csda.2019.04.001,Cited by (3),"The practice of employing empirical likelihood (EL) components in place of ==== likelihood functions in the construction of Bayesian-type procedures has been well-addressed in the modern statistical literature. The EL prior, a Jeffreys-type prior, which asymptotically maximizes the Shannon mutual information between data and the parameters of interest, is rigorously derived. The focus of the proposed approach is on an integrated Kullback–Leibler distance between the EL-based posterior and prior density functions. The EL prior density is the density function for which the corresponding posterior form is asymptotically negligibly different from the EL. The proposed result can be used to develop a methodology for reducing the ==== of the proposed technique in practical aspects.",". In this fundamental work, Jeffreys employed the Kullback–Leibler (K–L) measure to quantify a distance between the corresponding posterior and prior density functions. An excellent review of the justifications for Jeffreys prior is presented in ====, ====, ====). ====, pp. 261–262).====The above mentioned analysis related to the selection of priors corresponds to the parametric setting when the form of the likelihood is completely specified. ====, ====, ====).====, ====). Towards this end, we can find within the modern applied and theoretical statistical literature a line of research around Bayesian empirical likelihood (BEL) techniques based on the empirical likelihood (EL) concept (====, ====, ====, ====, ====, ====). ====). ==== proposed and examined the BEL credible set estimation as an analogue to the traditional and efficient parametric Bayesian approach. Recently, ==== for details).====, ====). MELEs can be associated with solutions of general estimating equations (====) as well as with estimates obtained by employing the generalized method of moments (====). In addition, the MELE framework can be extended to certain ====-estimators (====). Thus, we provide the theoretical justification regarding the use of the derived EL prior in order to penalize the EL in an effort to improve the small sample properties of MELEs. This will be applied to reducing the EL bias in the framework of general estimating equations and ====-estimation schemes.====This paper is organized as follows: In Section ====, we demonstrate the process for using EL prior densities for the purpose of eliminating the first-order bias terms from the asymptotic expectation of MELEs. In Section ====, an extensive Monte Carlo (MC) study is conducted to examine the proposed method. The applicability of the proposed method is illustrated through a real world example of myocardial infarction in Section ====. In Section ====, we provide concluding remarks. Proofs of the theoretical results presented in this paper are outlined in ====.====The corresponding ==== proof scheme is based on results shown in ====.====In order to show that the remainder term ==== in ==== vanishes to zero, we present the lemmas below. These lemmas incorporate the Lagrange multiplier ==== related to the Lagrangian equation ====in order to maximize the EL, ====, given the constraints ==== and ====. It can be easily shown that ====, ====, ====, and ==== is the solution of ====; for details, see ====.====By virtue of ====–====, we conclude that the remainder term asymptotically vanishes with the following lemma.====Thus we show that the remainder term ==== in ==== vanishes asymptotically to zero. Then by virtue of ====, we have ====. This completes the first stage of the proof scheme of ====.====In the second stage of the proof scheme of ====, we analyze the term ==== at ====. In a similar manner to the evaluation of ==== shown above, we prove that ====where ==== with ====. Intuitively this result follows from the fact that ====, where ==== and ==== is the solution of ====
 (====). Following the algorithm of proofs shown in ====, we denote a positive sequence ==== with the property that ==== where ==== and focus on the following expression ====where the remainder term ====.====By virtue of that ====, we have the following two inequalities ==== where we use ==== with ====. We will show that the upper and lower bounds in ==== converge to a same value as ====. Towards this end, we derive a bound for ==== in the next lemma. This bound will assist in evaluating the remainder term ====.====By virtue of ==== and ====, using ==== we have ==== Taking into account ==== and the following lemma result, we complete the second stage of the proof of ====.==== proof scheme is based on results shown in ==== and ====.====Thus lemmas ==== and ==== complete the proof of ====.====By virtue of the results that ==== and ==== (for details see the proof of ==== in the SM, ====), we have the following lemma.====The above expansion ==== and ==== imply that ====The expression of ==== can be easily found by taking the derivative of the constraint equation ==== with respect to ====. Then one can obtain the following results regarding ====: ====, ====, where ==== (for details see the proof of ==== in the SM, ====).====Thus, it is clear that by ==== and ==== we complete the proof of ====.",The empirical likelihood prior applied to bias reduction of general estimating equations,https://www.sciencedirect.com/science/article/pii/S0167947319300854,11 April 2019,2019,Research Article,468.0
"Cheng Jing,Chan Ngai Hang","School of Statistics, Huaqiao University, Xiamen, Fujian, China,School of Statistics, Southwest University of Finance and Economics, Sichuan, China,Department of Statistics, The Chinese University of Hong Kong, Shatin, NT, Hong Kong","Received 8 August 2017, Revised 11 February 2019, Accepted 15 March 2019, Available online 8 April 2019, Version of Record 25 April 2019.",https://doi.org/10.1016/j.csda.2019.03.010,Cited by (0),"This paper studies the ==== and another nonlinear state space model for illustration, where the results show better estimation performance."," and ==== is preferred due to its advantage in handling incomplete data problems, which is applied to analyze the NLSS models in this paper.====To implement the MCEM algorithm for the NLSS models, particle smoothers are used to approximate the expectation in E-step. One famous work is the forward filtering backward simulation (FFBSi) algorithm by ====. But particle smoothers suffer degeneracy problems, hence requiring extremely large particles to obtain precise parameter inference; see the experimental evidences in ====. This paper will further provide the empirical evidence that the inference with moderate particle size is unstable by the FFBSi algorithm due to the existence of the particle degeneration in Section ==== and ====. Moreover, the PIMH algorithm greatly reduces the influence of particle degeneration on parameter estimation, which will be explained further in Section ====. Therefore, the PIMH algorithm is adopted in this paper to conduct inference for NLSS models more efficiently.====In the implementation of the MCEM algorithm, how to choose sample size at each iteration constitutes an important issue, since it strikes a balance between the estimation accuracy and computational cost. In ====The remaining of this paper is organized as follows: Section ==== proposes an automatic rule to determine sample size at each iteration in the MCEM framework by the PIMH algorithm, which offers more efficient inference. Applications to stochastic volatility (SV) model and other nonlinear state space model are considered in Section ====. Section ==== concludes.====In this appendix, the assumptions of convergence theorem for the MCEM algorithm proposed in ==== are listed. The same method of ==== is used to prove the convergence of the PIMH algorithm within the MCEM framework in the SV model.",Efficient inference for nonlinear state space models: An automatic sample size selection rule,https://www.sciencedirect.com/science/article/pii/S0167947319300775,8 April 2019,2019,Research Article,469.0
"Ng Kenyon,Turlach Berwin A.,Murray Kevin","Department of Mathematics and Statistics (M019), The University of Western Australia, 35 Stirling Highway, Crawley WA 6009, Australia,Centre for Applied Statistics (M019), The University of Western Australia, 35 Stirling Highway, Crawley WA 6009, Australia,School of Population and Global Health (M431), The University of Western Australia, 35 Stirling Highway, Crawley WA 6009, Australia","Received 14 March 2018, Revised 31 January 2019, Accepted 18 March 2019, Available online 26 March 2019, Version of Record 5 April 2019.",https://doi.org/10.1016/j.csda.2019.03.011,Cited by (0), is freely available on GitHub.,", ====), or shape constrained models can be used to estimate dose–response curves (====), cumulative distribution functions (====) and various response curves in economics that are convex by some underlying theory (====), such constraints can be achieved through an appropriate model parameterisation (====, ====, ====, ====, ====, ====, ====, ====, ====). However, shape-constrained models other than these are less common, partly due to the difficulty of deriving appropriate closed-form expressions required for enforcing the constraints. This motivates us to develop a flexible method that is capable of imposing shape constraints on a wide variety of regression models, even in the absence of closed-form expressions of the constraints.====) algorithm. However, CEPSO requires a pilot estimate to operate, which is usually the unconstrained global minimum. Since the unconstrained global minimum is often difficult to obtain, especially when there are multiple local minima in the loss function, there are some circumstances when CEPSO may be unsuitable. Our method is based on Sequential Monte Carlo–Simulated Annealing (SMC–SA, ====), and will overcome the need for a pilot estimate.====The remainder of the paper is outlined as follows: In Section ====. In Section ====, the performance of our method will be assessed by fitting regression models on both simulated and real-world datasets. We conclude and discuss our algorithm in Section ====.====A B-spline model with degree ==== and a set of knots ==== can be written as ====where ==== only evaluates in ====. Suppose that knots are equidistant and separated by a distance ====, the basis functions can be derived from the following recursive formula (====) ==== The corresponding first order derivative of ==== is given by ====and consequently, ==== can be deduced ",A flexible sequential Monte Carlo algorithm for parametric constrained regression,https://www.sciencedirect.com/science/article/pii/S0167947319300787,26 March 2019,2019,Research Article,470.0
"Guo Xu,Song Lianlian,Fang Yun","School of Statistics, Beijing Normal University, Beijing, China,College of Economics and Management, Nanjing University of Aeronautics and Astronautics, Nanjing, China,Department of Mathematics, Shanghai Normal University, Shanghai, China,Department of Mathematics, Hong Kong Baptist University, Hong Kong, China","Received 19 January 2018, Revised 14 March 2019, Accepted 16 March 2019, Available online 26 March 2019, Version of Record 4 April 2019.",https://doi.org/10.1016/j.csda.2019.03.009,Cited by (5),"Model checking for the general linear regression model with nonignorable missing response is studied. Based on an exponential tilting model, two estimators are proposed for the unknown parameter in the regression model. Then, two empirical-process-based tests are constructed. The ","Due to its easy interpretation and well developed theories, the linear regression model is widely used to describe the relationship between scalar response ==== of dimension ====. Consider the following general linear regression model with the form ====where ==== is a known smooth function with dimension ====, and ==== is an unknown parameter to be estimated. Furthermore, ==== is the error term satisfying ==== and ====. The superscript ==== in ==== denotes the transpose. When ====, model ==== becomes the classical linear model. Compared with the classical linear model, model ==== is more flexible and applicable because interaction and high-order terms of the covariates can be included.====To prevent incorrect conclusions and improve interpretations, any statistical analysis conducted using model ==== should be accompanied by a check of whether the hypothetical model holds. The literature contains a number of proposals for doing so when all the response measurements are available (====, ==== for a review). However, missing responses are often encountered in practice. For instance, the response ==== may be very expensive to measure, and, because of financial limitations, the response values are available for only some of the subjects; the sampled individuals may have refused to supply the desired information to some survey questions, or the investigators may have failed to gather the correct information. Simply excluding the units with missing responses and conducting statistical analysis based on only the completely observed samples can often lead to biased and inefficient parameter estimates when the data are not missing completely at random (====, ====). Consequently, we cannot directly extend the existing model checking procedures for complete data to deal with model checking with missing responses.====It is now realized that to develop a more accurate and useful methodology in the presence of missing response values, we often need to make some assumptions regarding the missing mechanism. Here we simply present the related concepts; for more discussions, see ====, ====. Let ==== be a missing indicator of whether ==== is observed (====) or not (==== the response ==== is independent of the missing indicator ====, then the response is missing at random (MAR) or ignorable. If the missing depends on the value of the underlying unobserved response, then we call it nonignorable or not missing at random (NMAR). The NMAR case is common in social surveys, especially when the questions are sensitive. Consider a survey of personal income, to which people in low socioeconomic groups are more likely to refuse to provide the desired information.====The literature contains some proposals for model checking when the response value ==== is MAR. ==== extended the method due to ====, with the constructed test statistic being based on the ==== with MAR response, ==== proposed a test that is based on the minimum integrated squared distances between nonparametric and parametric fits, which can be viewed as an extension of the minimum-distance test proposed by ====. See also ====.====, ====, ====). For nonignorable missing response, the missing mechanism cannot be verified from the observed variables alone. It is then desirable to make the weakest possible model assumptions about the missing mechanism. To this aim, ==== proposed a semiparametric exponential tilting model for the missing mechanism. Based on the semiparametric missing model, they imputed the missing responses by nonparametric kernel regression imputation, whereupon the sample average of the observed and imputed responses is used to estimate the mean function. Based on regression imputation and augmented inverse probability weighting, ==== to construct the confidence interval of the mean function. ==== discussed the confidence interval for the parameters in a linear regression model with nonignorable missing response. ==== investigated the inference on parameters in estimating equations by adopting the empirical likelihood. ==== developed jackknife empirical likelihood inference for nonignorable missing response. ==== proposed rank-based estimators for a general parametric regression model. Other recently developed estimation approaches for nonignorable missing data include those of ====, ====, and ====. For an up-to-date review of nonignorable missing data, see ====.====In this paper, we aim to perform a model checking procedure for model ==== with nonignorable missing response and completely observed covariates. In other words, we want to test the following hypothesis: ====for some ==== and known ==== with nonignorable missing response.====We begin by discussing how to estimate the unknown parameter ==== under the null hypothesis, and we suggest two estimators for ====The rest of this paper is organized as follows. In Section ====, we construct the test statistics. In Section ====, we derive their asymptotic properties under the null hypothesis and local alternative hypotheses when the tilting parameter is known or estimated. In Section ====, we report simulation results and analyze real data to illustrate the proposed tests. In Section ====, we make some concluding remarks. The conditions are described in the ====. The proofs of the theoretical results are given in the supplementary material.====The following conditions are required for the theorems in Section ====. Note that the detailed proofs of the theoretical results are given in the supplementary material.",Model checking for general linear regression with nonignorable missing response,https://www.sciencedirect.com/science/article/pii/S0167947319300763,26 March 2019,2019,Research Article,471.0
"Liu Jicai,Xu Peirong,Lian Heng","School of Statistics and Mathematics, Shanghai Lixin University of Accounting and Finance, Shanghai, China,College of Mathematics and Sciences, Shanghai Normal University, Shanghai, China,Department of Mathematics, City University of Hong Kong, Kowloon Tong, Hong Kong","Received 5 March 2018, Revised 4 February 2019, Accepted 9 March 2019, Available online 21 March 2019, Version of Record 28 March 2019.",https://doi.org/10.1016/j.csda.2019.03.008,Cited by (3),"In this paper, we focus on the estimation of the index coefficients in single-index models and develop a new procedure based on ==== divergence. Since the proposed procedure can capture automatically the conditional mean dependence of the response variable on the ==== studies. We further illustrate the proposed method through empirical analyses of a real dataset.","In this paper, we consider the following single-index regression model ====where ==== is a scalar response variable, ==== is a ==== is the unknown univariate link function and ==== is the unknown index coefficients. The error term ==== is allowed to be heteroscedastic with ==== belongs to the parameter space ====, where ====, assumed to be nonsingular. Of interest for this model is to estimate ====, whereas the unspecified function ====The single-index model ==== based on nonparametric smoothing methods, for example, the average derivative approach (ADE) (====, ====, ====, ====), the minimum average variance estimation method (MAVE) (====, ====) and the estimating equations approach (====, ====, for instance, the ordinary least squares method (OLS) (====) and the principal Hessian directions method (PHD) (====, ====). However, these methods often require certain conditions on ====, such as linearity condition or constant covariance condition, which are not always satisfied in practice.====Recently, ==== proposed the distance covariance (DCOV) criterion for a general single-index model ====, where the random error ==== is independent of ====. DCOV was developed by ====, ====, which can measure the dependence between two random vectors. DCOV has been explored for feature screening in ultrahigh-dimensional data analysis, see, e.g., ====. As demonstrated in ==== is non-normal.====In ====, the general model is posed as reducing the dimension of ====. However, in many applications, our primary interest is the conditional mean ==== rather than entire conditional distribution of ====. In this case, ====’s method cannot effectively capture the information about the conditional mean ====. Thus, it is desirable to focus on the conditional mean model ==== and develop a model-free method in practice.====In this paper, we propose a new method to estimate ==== in model ==== is a natural extension of DCOV and can be used to quantify the conditional mean dependence of a scalar response variable given a vector of covariates. ==== showed that MDD equals zero if and only if the response is conditionally mean independent of the covariates. Furthermore, MDD only involves certain norm and expectation calculations so that it can be easily estimated from the observed data. ==== used MDD for feature screening in ultrahigh-dimensional data to screen out covariates that do not contribute to the conditional mean of the response variable. These remarkable properties motivate us to use MDD to estimate ====. Like ====’s method, our MDD-based procedure also enjoys the desirable model-free property. Our numerical results indicate that in all settings the proposed procedure is comparable to best performer among the existing methods, such as MAVE, DCOV, OLS and PHD, and can be superior in many settings, including when the link function is non-smooth.====The rest of the paper is organized as follows. In Section ====, the MDD-based procedure is introduced, and the theoretical properties of the proposed estimators are also presented. In Section ====, we conduct extensive simulation studies to examine the performances of our approach, and compare it with several existing methods. In Section ====, we apply our methodology to the Hitters salary dataset. We complete the paper with a brief discussion in Section ====. All the technical proofs are deferred to ====.====In order to prove ====, the following lemma is needed.",Estimation for single-index models via martingale difference divergence,https://www.sciencedirect.com/science/article/pii/S0167947319300751,21 March 2019,2019,Research Article,472.0
"Li Lingzhu,Chiu Sung Nok","Department of Mathematics, Hong Kong Baptist University, Hong Kong,School of Statistics, Beijing Normal University, Beijing, China","Received 17 November 2018, Revised 7 March 2019, Accepted 11 March 2019, Available online 21 March 2019, Version of Record 10 April 2019.",https://doi.org/10.1016/j.csda.2019.03.003,Cited by (4),"For regression models, most of the existing model checking tests can be categorized into the broad class of local smoothing tests and of global smoothing tests. Compared with global smoothing tests, local smoothing tests can only detect local alternatives distinct from the null hypothesis at a much slower rate when the dimension of predictor vector is high, but can be more sensitive to oscillating alternatives. A projection-based test is suggested in multivariate scenarios to bridge between the local and global smoothing-based methodologies such that a local smoothing test can be transferred to a global smoothing test and still, to a certain extent, inherits some feature of local smoothing tests. The test construction rests on a kernel estimation-based method and the resulting test becomes a distance-based test with a closed form. Although it is eventually similar to an Integrated Conditional Moment test in spirit, it results in a test with a weight function that helps to collect more information from the samples than Integrated Conditional Moment test. Simulation results show that the proposed test has better performance than some typical competitors in this area when dimension goes higher. A real data example is analyzed to show its usefulness.","Suppose the model we are interested in has the following form: ====where ==== is a known function, ==== is some unknown parameter and ====. The predictor ==== is a random vector in ==== and ==== is the response variable in ==== and ====, and a wrongly specified model would result in unreliable references. Therefore, it is meaningful to check the adequacy of the assumed model structure. In contrast to the parametric model in ====, when we have little information on the underlying model we may consider a general nonparametric alternative model: ====where ==== is an unknown function that does not belong to the family ====. Therefore the null hypothesis for this model checking problem is ====where ====There are many proposals available in the literature. But how to efficiently deal with high-dimensional data is always a concern. A promising methodology is to transform the problem under high-dimension to the ones at all projection directions. To be precise, test statistics can be based on, say, univariate projected predictors ==== for all ==== in a subset of ====. This is the idea of projection pursuit regression which was proposed by ====. ==== provided a comprehensive reference of this methodology.  ==== used the method by Fourier transformation which gave the weight functions to be ==== for all ====. The integration over ==== with respect to a measure can then formulate a final test statistic. This test is of a dimension reduction nature as every weight function uses univariate projected predictor ====. Therefore we may also include this method as a projection-based test in a broad sense.  ====, ====, ====, ====, ==== and ==== are the relevant references in this field. To get a clear idea of our motivation, we give a brief review on the previous work.  ==== sought a direction that maximizes the kernel-based test statistic with a penalty term to construct a final test.  ====’s (====) test was also based on a residual marked empirical process, but its index set contained only one projection direction.  ==== used a predictor-marked residual process to construct a test.  ==== developed a smooth integral conditional moment test constructed through  ==== that used nonparametric kernel estimation of some conditional moment. For a more comprehensive review, see ====.====The aforementioned projection-based tests can be categorized into two very different broad classes — nonparametric estimation-based and empirical process-based. They then can be classified as local smoothing and global smoothing tests. This is because nonparametric estimation-based methods rely on local smoothing techniques and empirical process-based tests are the averages of functions of weighted sum of residuals over an index set, which is a global smoothing step. As can be seen,  ====, ==== and ==== belong to the former class, while ==== and ==== are in the latter class. More examples in the class of global smoothing tests include  ==== and ====.====As is well known, if we do not use projected predictors but the original ====-dimensional predictor ==== increases. They can only detect the local alternatives distinct from the null at the rate of order ====, where ==== is the bandwidth going to zero as ====. The projection-based tests work well without additional assumption on model structure, in alleviating the negative impact from the dimensionality. The test in ==== detects the local alternatives distinct from the null at the rate of ====. For parametric single-index models ==== invented an adaptive-to-model methodology that can substantially improve the performance of local smoothing tests to also well detect the local alternatives converging to the null at the rate of ====. It is worth noticing that it is still a local smoothing test as ====, this rate must be slower than ====. In other words, both are still local smoothing tests. In contrast, global smoothing tests can always detect local alternatives distinct from the null at the fastest possible rate that is ====.  ==== proved that some global smoothing tests such as  ==== and ====In this paper, we propose a projection-based test. Like any projection-based test such as  ==== and ====, we project the predictor onto one-dimensional subspaces such that at any direction, the test only involves univariate predictor. However, the key feature of the proposed test distinguishing from these existing projection-based tests is that the proposed test bridges between local and global smoothing methodologies. The hybrid of projection and local smoothing makes this reality. It is worth mentioning that ==== and ==== can be viewed as a specific integrated conditional moment test in ==== and ====. But, we also notice that, the two transformed tests still require ====-dimensional kernel estimation because if the projection approach is not applied, high-dimensional estimation is indispensable. In contrast, our test can have a simple closed form and becomes a global smoothing test although it is developed from a local smoothing test. Also, the resulting weight function in the test statistic is the reciprocal of a function of the distance between two samples ==== and ====. If ==== is close to ====, the corresponding weight in our test tends to be larger than it is when ==== is far from ==== and the corresponding residuals would have significant bias under the alternatives. In this sense, we may consider that the proposed test also inherits, to a certain extent, a feature that local smoothing tests usually have. Further, we will discuss the same and different aspects in ==== at the end of Section ==== between our test and the integrated conditional moment test in ==== and ====, the proposed test avoids the inefficient estimation in high-dimensional scenarios. Besides, our test has a more stable performance when the dimension ==== increases as it eventually becomes a distance-based test that, as confirmed in the literature, has the advantage on handling high-dimensional data. In our numerical studies, we found that it is sufficient to reveal how the dimensionality affects the tests when the dimension is only up to ====. That is because, as we can see, the proposed test already has the superiority to its competitors when the dimension is ====. The competitors are obviously affected negatively by the dimensionality and higher dimension goes against them. We also show in the limited numerical studies its superiority to the existing local smoothing, global smoothing and projection-based test.====The rest of this paper is organized as follows. In Section ====, the test statistic construction is described. Section ==== has some conclusions. Technical proofs are postponed to the ====.====The following assumptions are regularity conditions for the consistency and asymptotic normality of ====.====The following lemma shows the asymptotic properties of ====.",Model checking for regressions: An approach bridging between local smoothing and global smoothing methods,https://www.sciencedirect.com/science/article/pii/S0167947319300702,21 March 2019,2019,Research Article,473.0
"Wagner Heiko,Kneip Alois","Universität Bonn, Adenauerallee 24-26 53113, Bonn, Germany","Received 11 March 2018, Revised 9 March 2019, Accepted 9 March 2019, Available online 20 March 2019, Version of Record 9 April 2019.",https://doi.org/10.1016/j.csda.2019.03.004,Cited by (5),"Registration aims to decompose amplitude and phase variation of samples of curves. Phase variation is captured by ==== which monotonically transform the domains. Resulting registered curves should then only exhibit amplitude variation. Most existing methods assume that all sample functions exhibit a typical sequence of shape features like peaks or valleys, and registration focuses on aligning these features. A more general perspective is adopted which goes beyond feature alignment. A registration method is introduced where warping functions are defined in such a way that the resulting registered curves span a low dimensional ==== are discussed in detail, and connections to established registration procedures are analyzed. The method is applied to real and simulated data.","The data that we consider is a sample of i.i.d. smooth random functions ==== defined over a closed interval on the real line. Registration literature focuses on the situation where all functions share a common set of shape features, such as peaks and valleys. The curves displayed in the top panel of ==== provide an example. The sizes of the features vary, and we refer to this as ====. The locations of the features also vary from curve to curve, which indicates the existence of ====, ====).====, called ====, which eliminate phase variation such that the ==== functions ==== of the form ==== represent amplitude variation. Since monotone transformations do not destroy shape features the registered functions will possess the same sequences of peaks and valleys as the original functions ====.====Well-known problems with these techniques, as for example pinching of curves (see ====), have lead to the development of more sophisticated techniques based on alternative distance measures ====. The methods proposed by ====, ====, ====, ====, or ==== all possess the property that they are able to reproduce the correct warping functions if ==== for some ====.====All these methods share a common point of view. The success of a registration method is assessed in terms of how well it is able to align visible features. Templates are often determined iteratively from the sample and their construction aims to establish a “structural mean” which possess all common shape features at mean locations and with mean amplitude.====More recent work tends to apply registration procedures in the context of more complex problems of statistical data exploration and inference. In functional data analysis, the most frequently applied procedures are based on identifying ==== use a norm based method for aligning functions, and then apply FPCA separately for registered curves and warping functions. ====, ==== and ==== present multi-resolution approaches to registration. Assuming discretized observations, they rely on ==== basis expansions for amplitude and phase variation and use algorithms designed for fitting mixed-effect models.====For clustering functions, ==== propose a procedure which is based on several templates instead of only a single “structural mean”. The “====-mean” approach assumes that each observed curve belongs to one of ==== specific clusters. The method then tries to determine the mean (template) of each cluster iteratively and uses scale and shift to align the curves within the clusters.====We consider registration from a more general perspective. Registration may be used as a tool for statistical analysis whenever the random functions ==== possess “bounded shape variation”, i.e. there exists a fixed value ====. Our approach is based on an observation already made by ==== that for random functions with bounded shape variation there exist a finite ==== and warping functions ==== such that with probability 1 ====for some basis functions ==== and individually different coefficients ====.====We are going beyond ==== by studying decomposition ==== from a theory-guided, conceptional point of view and by deriving some basic inference results for situations where the true functions have to be reconstructed from discrete, noisy observations. Appropriate values of ==== depend on the structure of ====, and possible non-uniqueness of solutions to ==== are resolved by selecting the registration procedure with the least complex warping functions. Furthermore, we present a new algorithm which estimates the components of ==== for all possible values of ==== and seems to work well for many applications.====Assuming that functional shapes are of bounded complexity does not seem to be restrictive in important applications for instance consider biomedicine, engineering, chemometrics, etc., and often the presence of phase variation is already imposed from a substantial point of view (different reactions times, etc.). Our approach then generalizes the usual concept of registration by feature alignment and extends the rather limited range of applicability of traditional alignment techniques. If ====, then an optimal registration based on ==== will usually not align shape features, since for the registered curves ====. Furthermore, subspace registration also opens a way to treat applications like the genetic data of Section ====, where together with some function ==== we also observe other functions being close to ====. Obviously, ==== and ==== belong to the same linear subspace, but it does not make sense to require alignment of shape features of ==== and ====.==== represents functional data satisfying ==== with ====. Registration aiming at peak alignment is done with the R-package “fdasrvf”, where, following ====, the Fisher–Rao metric is used to determining warping functions by minimizing ==== with respect to a suitably defined template function ====. It is shown in Section ==== that peak alignment in tendency corresponds to adjusting a one dimensional model, and in this example results of the FR-metric registration are very similar to those obtained by applying our algorithm with ==== (also see the comparison of the two methods in Section A.3 of the online appendix). At the same time, the figure shows that an optimal selection of warping functions depends on ==== dimensional registration. As can be seen from an FPCA decomposition of ====, for the one dimensional approximation the space of the registered functions is more complex and cannot be described by two components anymore.====Decomposition ==== may also serve as a basis for registering functional data such as displayed in ====. These curves are qualitatively different from the type of data usually considered in a registration context since they do not possess a clearly visible “common shape”. On the other hand, the number of local extrema of these functions varies between 1 and 3, and these random functions are of bounded shape variation. A closer look at the unregistered curves shows that there exist structurally similar curves which quite obviously exhibit some phase variation. However, fitting a ==== dimensional model leads to unreasonable results with extreme warping functions. Indeed the true minimal dimension is ====, and the figure shows that the ==== dimensional registration rests upon structurally simple warping functions.====The following is the Supplementary material related to this article. ",Nonparametric registration to low-dimensional function spaces,https://www.sciencedirect.com/science/article/pii/S0167947319300714,20 March 2019,2019,Research Article,474.0
Xie Chuanlong,"Beijing Normal University, Beijing, China,Hong Kong Baptist University, Hong Kong, China,Jinan University, Guangzhou, China","Received 26 January 2018, Revised 13 December 2018, Accepted 31 January 2019, Available online 18 March 2019, Version of Record 5 April 2019.",https://doi.org/10.1016/j.csda.2019.01.018,Cited by (21),This research provides a projection-based test to check ,"Regression models are widely used to describe the relationship between response variable ==== and ====-dimensional predictor ====. Generally, the response ==== and the predictor ==== are assumed to be observable. But in some cases, obtaining the true values of ==== and ==== are expensive or impossible, while the surrogates ==== and ====, ==== and ==== are comprehensive references. Motivated by a real dataset, ====.====Consider a general variable-adjusted regression model where the response ====, the predictor ==== and their surrogates ====, ==== are related to each other by the following relations: ====with ====, ====. Here ====, ==== and ==== are unknown distorting functions. The efforts are mainly devoted to estimation. A natural idea is to estimate the true values of ==== and ==== by adjusting the observed surrogates ====, ==== and then to further estimate ==== with ==== and ====. The reference includes ====, ====, (====), ====, ==== and so on. However there is less attention on goodness-of-fit test for this model based on variable adjustment. ==== proposed a residual marked empirical process-based test that is a ==== developed a local smoothing test for variable-adjusted models, which is very simple and easy to implement. The cost is that it can only detect local alternatives distinct from the null hypothesis at the rate of ====, which is slower than that of the test in ==== and greatly affected by the dimensionality ====. This means when the dimensionality of the predictor is large or the sample size is moderate, this test may not work well.====In the case of no distortion error, the projection-based methods are broadly discussed to develop tests of dimension reduction type. These methods could be tracked back to ====, motivated from projection pursuit (see ====). The later developments include ====, ====, ====, ====, and ====. A relevant reference is ====.  ==== proposed an adaptive-to-model test, which significantly improves the performances of local smoothing tests. Motivated by this work, we will take the advantages of the model adaptation strategy and combine the special construction for variable-adjusted models to develop a projection-pursuit test. To utilize dimension reduction structure in the hypothetical model, the problem of interest is to check whether the model is single-index and the hypotheses are formulated as ==== where ==== is a given function, ==== and ==== are the parameters of ====-dimension and ====-dimension respectively. From the viewpoint of sufficient dimension reduction (SDR, ====), a general alternative regression model can be written as ==== where ==== is an unknown function, different from ==== and ==== is a ==== matrix(or vector) with unknown number ==== of columns. Note that ==== and usually ==== is much smaller than ====. If ====, the alternative model has a dimension reduction structure. When ====The case ==== corresponds to the null hypothesis ==== and the alternative holds when ====. When ==== is a fixed constant, the alternative is a global alternative under ==== and when ====, ==== specifies a sequence of local alternatives. The test in ==== can only detect the local alternatives converge to the null model at the rate of ==== such that ==== is bounded above or goes to zero. Thus, ==== is the fastest rate to ensure that their test can detect the local alternatives. We will show that the proposed test can detect the local alternatives with the rate ==== and be consistent for any ====. On the other hand, according to the arguments in ====, an estimate ====, which converges to ==== under ==== and to ==== when ==== is true, is the key to make the proposed test adaptive to the underlying model. In this paper, we use the sufficient dimension reduction technique proposed by ==== to obtain ====, and systematically investigate its asymptotic properties under the local alternatives. We give more details in the following.====The paper is organized as follows. Section ==== describes the test problem for variable-adjusted model and proposes an adaptive-to-model test procedure. In Section ====, we present the large sample properties of the proposed test. Sections ====, ==== report the simulation results and real data application to illustrate our method. The assumptions and proofs are postponed to ====.",A goodness-of-fit test for variable-adjusted models,https://www.sciencedirect.com/science/article/pii/S0167947319300337,18 March 2019,2019,Research Article,475.0
"Mariñas-Collado Irene,Bowman Adrian,Macaulay Vincent","Dpto. de Estadística, Facultad de Ciencias, Universidad de Salamanca, Plaza de los Caidos s/n 37008, Salamanca, Spain,School of Mathematics and Statistics, University of Glasgow, University Avenue, Glasgow, G12 8QQ, UK","Received 31 August 2018, Revised 11 March 2019, Accepted 11 March 2019, Available online 18 March 2019, Version of Record 10 April 2019.",https://doi.org/10.1016/j.csda.2019.03.002,Cited by (1),"Statistical methods which enable shape information on organisms to be used to construct a phylogenetic tree and to learn how shape evolves are developed. In particular, this allows the evolution of facial curves to be used in studying relationships between and within different ethnic groups and their ancestors. The main challenge is to exploit the details of surface shape, while maintaining computational feasibility. A ==== approach is adopted."," (====) but the major advances from a statistical, computational and algorithmic point of view have mostly been made in the past 50 years. ==== summarised well the major advances that had been achieved in the course of the previous four decades.====). Methods of curve estimation are described by ==== and ====. ==== shows a human face with a set of anatomical curves superimposed. The nasal curves in red are used in an application in Section ====.==== presented a flexible statistical model for such data by combining assumptions from phylogenetics with GPs. Their approach generalises the Brownian motion and Ornstein–Uhlenbeck models of continuous-time evolution from quantitative genetics (====). This paper extends their model to data in the form of points on curves embedded in ====-dimensions, where the covariance of the different coordinates needs to be modelled, in addition to the spatial and phylogenetic covariances.====Inside the branching structure of a phylogenetic tree, one can focus on the evolution of one single curve along one branch, without taking into account the branching patterns and the ancestors. This is equivalent to modelling the curve evolving over time as a linear continuous variable, which can be regarded as a degenerate scenario of the phylogenetic GP model and, therefore, is the first model introduced (Section ====). The model is later extended, using the phylogenetic covariance function to allow for branching points in the evolution in Section ====. The challenges encountered when implementing the model are discussed in Section ====. Finally, in Section ====, we present a case study to compare nose shape between and within three broad ethnic groups: Sub-Saharan Africans, White British and Chinese.",A phylogenetic Gaussian process model for the evolution of curves embedded in ,https://www.sciencedirect.com/science/article/pii/S0167947319300696,18 March 2019,2019,Research Article,476.0
"Bergé Laurent R.,Bouveyron Charles,Corneli Marco,Latouche Pierre","Laboratoire MAP5, UMR CNRS 8145, Université Paris Descartes, Paris, France,Laboratoire J.A. Dieudonné, UMR CNRS 7351, Université Côte d’Azur, Nice, France,Epione, INRIA Sophia-Antipolis, Valbonne, France,Laboratoire SAMM, EA 4543, Université Paris 1 Panthéon-Sorbonne, Paris, France,Université du Luxembourg, 162a, Avenue de la Faïencerie, L-1511, Luxembourg","Received 10 April 2018, Revised 8 March 2019, Accepted 9 March 2019, Available online 15 March 2019, Version of Record 27 March 2019.",https://doi.org/10.1016/j.csda.2019.03.005,Cited by (5),Textual interaction data involving two ,") of both customers and goods. The task of simultaneously clustering the rows and the columns of such an array (here defined by the interactions between individuals and objects) is known as co-clustering. When the interactions are ==== (e.g. a review), the text can provide significant information to perform a more realistic clustering. For instance, a group of users could review the same goods but with different arguments. Unfortunately, a large amount of the existing co-clustering methods do ==== account for the textual information. The aim of this paper is to tackle this issue by providing a new model-based method to co-cluster both individuals and objects while accounting for the textual content of their interactions.====For all pairs ==== such that ====, the update step for ==== (for all ====) is given by ====where the constant term includes everything not depending on ==== and ==== denotes the digamma function. The functional form of a multinomial distribution can be recognized in the above equation. Hence ====where ",The latent topic block model for the co-clustering of textual interaction data,https://www.sciencedirect.com/science/article/pii/S0167947319300726,15 March 2019,2019,Research Article,477.0
"Liu Baisen,Wang Liangliang,Nie Yunlong,Cao Jiguo","School of Statistics, Dongbei University of Finance and Economics, Dalian 116025, China,Department of Statistics and Actuarial Science, Simon Fraser University, Burnaby, BC V5A1S6, Canada","Received 21 March 2018, Revised 1 December 2018, Accepted 4 March 2019, Available online 13 March 2019, Version of Record 20 March 2019.",https://doi.org/10.1016/j.csda.2019.03.001,Cited by (3), hierarchical framework. The proposed method is demonstrated by estimating a ==== mixed-effects ODE model. The finite sample performance of the proposed method is evaluated using some simulation studies.,", ====, ====Several methods have been developed for estimating ODE parameters from the noisy data. For instance, ==== and ==== developed a generalized profiling approach to estimate the ODE parameters. ==== proposed a robust method for estimating ODE parameters when the data have outliers. ==== suggested a class of fast, easy-to-use, genuinely one-step procedures for estimating unknown parameters in dynamical system models. ==== developed a gradient matching approach for estimating ODE parameters. ====). ==== and ==== proposed a penalized spline method to estimate ANOVA models based on integro-differential equations. ==== considered the two-step estimation under the Bayesian framework. ====Longitudinal dynamical systems, also called mixed-effects ODE models, have been studied by ====, ====, ====, ==== and ====. For instance,  ==== used the maximum likelihood approach directly to estimate unknown parameters in mixed-effects ODE models. ==== proposed a fast two-stage estimating procedure for mixed-effects dynamical systems and applied it to study longitudinal HIV virus data. ==== proposed a semiparametric method to estimate a mixed-effects ODE model for the HIV combination therapy study. A common fundamental assumption of these methods is that the observations for the dynamical process follow a normal distribution, but this assumption may lack robustness and lead to biased inference when outliers exist.====As an illustration, we consider the PK/PD experiment (see ==== displays the histogram and normal Q–Q plot of the obtained residuals by applying the conventional method which assumes the observations and random effects follow normal distributions. ==== shows that the underlying distribution of serum concentration may not follow the normal distribution. Hence, assuming normal distributions may be too restrictive to accurately model the serum concentration of the IDV in ODE mixed-effects models. Moreover, by performing a Shapiro–Wilk test of normality for the obtained residuals, the ====-value is approximately ====, which confirms that the normal distribution assumption is quite doubtful in this PK/PD data set.====To make robust inference on the ODE parameters, one possible approach is to implement a ==== (MLE) method. However, due to the complexity of dynamic systems, the solutions of ODEs generally have no explicit expressions, which makes it difficult to maximize the likelihood function. In contrast, the Bayesian methods are widely welcomed due to the convenient and efficient implementations.====This article has four main contributions. (i) We propose a mixed-effects ODE model, which considers the within-subject and between-subject variations simultaneously and makes ==== by borrowing information from all subjects. (ii) Our method uses a class of heavy-tailed distributions for random effects and observations for the dynamical process, which is robust against the outlying subjects and the outlying observations within individual subjects. (iii) Our method can detect the subjects which are outliers or have outlying observations by estimating latent variables in the model. (iv) We develop a highly efficient MCMC sampling scheme which allows to estimate complex dynamic models using the hierarchical structure of the proposed approach.====The remainder of this article is organized as follows. Section ==== briefly reviews the scale mixture of multivariate normal distributions. Section ==== demonstrates our proposed method in comparison with conventional methods by analyzing a real pharmacokinetics application. Section ==== evaluates the finite sample performance of our proposed method using some simulation studies. We end this article with conclusions and some discussions in Section ====.====We use the Markov chain Monte Carlo (MCMC) methods which consist of the Metropolis–Hastings algorithm and the Gibbs sampling method to sample the parameters ====, ====, ====, ====, ====, ====, ====, ====, ====, and ====. In this appendix, the symbol ==== denotes ==== for the vector ==== and the matrix ====. When ====, a symbol ==== is used instead. Define ====. The full conditional distributions for ====, ====, ====, ====, ====, ====, ====, ====, ==== and ==== are displayed as follows (where ==== denotes all variables except the one to be sampled):====(a) Full conditional distributions of ==== for ====. ====(b) Full conditional distributions of ==== and ====. ==== Then the full conditional posterior distribution of ==== is a multivariate normal distribution with mean vector ==== and covariance matrix ====. The full conditional posterior distribution of ==== is an Inverse Wishart distribution with the scale matrix ==== and degrees of freedom ====.====(c) Full conditional distributions of ==== and ====. ==== Assuming that ====, then the full conditional posterior distribution of ==== is still a Gamma distribution with shape parameter ==== and rate parameter ====. Similarly, the full conditional posterior distribution of ==== is a Gamma distribution with shape parameter ==== and rate parameter ====.====(d) Full conditional distributions of ==== and ====. ==== Assuming that ==== and a truncated exponential prior ==== is assigned on ====, then the full conditional posterior distribution of ==== is proportional to ====. This is not a standard distribution; however, we can apply the Metropolis–Hastings algorithm to sample it. In the same way, under the assumption of ==== and the prior ====, the full conditional posterior distribution of ==== is given by ====which is also sampled by the Metropolis–Hastings algorithm.====(e) Full conditional distributions of ==== and ====. ==== Assuming that a truncated exponential prior ==== for ==== and a Uniform prior distribution ==== for ====, then the full conditional posterior distribution of ==== is a truncated Gamma distribution ====. Similarly, under the assumption of ==== and a Uniform prior distribution ==== for ====, the full conditional posterior distribution of ==== is a truncated Gamma distribution ====.====(f) Sample ====. ====Assuming that ==== has a Gamma prior ====, then the full conditional posterior distribution of ==== is a Gamma distribution with shape parameter ==== and rate parameter ==== where ====.====Generally, in the above Gibbs sampler algorithm, the full conditional distribution in (a) has no closed form. We apply the Metropolis–Hastings method to sample ====. The details are as follows: in the ====th iteration, a candidate, ====, is generated from a proposal distribution, ====, like a multivariate normal distribution, ====, where ==== is a pre-specified scalar to control the acceptance rate. Then, the acceptance probability is calculated by ====. However, this acceptance probability depends on the ODE solution ==== which generally has no explicit expression and has to be obtained numerically. Conditioning on ====, ==== is estimated by minimizing Equation ==== numerically.",Bayesian inference of mixed-effects ordinary differential equations models using heavy-tailed distributions,https://www.sciencedirect.com/science/article/pii/S016794731930060X,13 March 2019,2019,Research Article,478.0
"Müller Dominik,Czado Claudia","Department of Mathematics, Technische Universität München, Boltzmannstraße 3, 85748 Garching, Germany","Received 30 March 2018, Revised 10 February 2019, Accepted 13 February 2019, Available online 2 March 2019, Version of Record 15 March 2019.",https://doi.org/10.1016/j.csda.2019.02.007,Cited by (13),"To model ====, Gaussian methods are widely used since they remain tractable and yield parsimonious models by imposing strong assumptions on the data. Vine ==== are more flexible by combining arbitrary marginal distributions and (conditional) ","In many areas of scientific research but also in business applications, high dimensional problems arise. For example, if a financial institution owns stocks ==== and wants to calculate a ====, see e. g. ==== dimensions is required. For large companies, one can easily imagine this number to increase into thousands for a single asset class and much higher when several asset classes are considered. Another active field of research in which dimensions grow rapidly is computational biology. For example in ====, genes are measured simultaneously to make inference about dependence within the biological system or with respect to some disease, see e. g. ====. In this case, the number of genes measured describes the dimension ==== and in recent applications can increase to several thousands. Similar to this, hundreds of ==== g. ====, where ====. This example is also easily imaginable to be extended to thousands of different metabolites. Finally, in ====, high dimensional spatial data is analysed.====One approach to cope with high dimensions is factor analysis for dimension reduction, which dates back to ==== are also heavily used, and non-Gaussian factor models as introduced in ==== or ==== gain widespread attention and are employed for their parsimony with respect to parameters, especially in finance applications.====Another very frequently used model class to analyse high dimensional datasets are ==== dimensions. Neglecting the mean of the distribution, the problem remains to estimate a ====. A favourable property of the multivariate Gaussian distribution is that for ==== and a zero entry of ====, i. e. ==== and ==== given the rest ====. By drawing a graph with nodes ==== and omitting an undirected edge ==== whenever ====, we obtain a ==== for the conditional independence in this distribution. There has been a considerable effort on how to estimate (sparse) inverse covariance matrices and thus, (sparse) Gaussian Graphical Models. It started with ==== of ==== to the current state of the art algorithm, the ==== (====). The huge advantage of these methods is their computational tractability also in ultra high dimensions, i. ==== of ==== where the marginal distributions must not necessarily be Gaussian. For the dependence part however, especially for financial data, the multivariate Gaussian is a too strong simplification. It does not allow for heavy tails as the Student’s-t distribution or asymmetric dependence, thus more sophisticated models are required.====The main idea of the so called ====, see ====, is to couple ==== marginal distributions to obtain a joint ====-dimensional distribution, called ====. The huge benefit is now that all involved distributions can be chosen arbitrarily and entirely independent from each other. However, this construction is not unique but is described by a graphical model, a ==== (====, ====). Each possible R-vine constitutes a different construction of a ====-dimensional distribution, if we impose the so called ====, which we will do for computational reasons, see Section ==== come into place. Secondly, an R-vine model needs ==== pair copulas. Clearly, this is growing too fast in hundreds of dimensions, let alone thousands. Thus, this also demands sophisticated approaches in high dimensions to keep models parsimonious. The large number of applications with vine copulas in recent years shown by ==== can be easily imagined to expand to hundreds or thousands of dimensions, which makes an extension of vine copulas in ultra high dimensions desirable.====Our contribution is as follows and can be summarized in a three-step procedure.====Thus, we show how well established Gaussian methods in high dimensions can be fundamental for clustering R-vines, see (i). More precisely, we break a ====-dimensional dependence model in multiple dependence sub-models with significantly smaller dimensions. These sub-models are now tractable again, see (ii). Within these sub-models, we use a refined algorithm to improve the accuracy of the standard search algorithm for vine copulas by ====. Afterwards, the sub-models are recombined to obtain one joint parsimonious model in ultra high dimensions, see (iii). Thus, this approach is also modular such that the steps (i), (ii) and (iii) can be modified independently from each other. This enables the framework to be adapted to different clustering approaches than the Graphical Lasso in step (i) for future research.====We show that this is working well in moderate dimensions and outperforming previously suggested methods in several hundreds of dimensions in computation time and goodness of fit. Going to ultra high dimensions, i. e. several thousands, it is to our knowledge the only feasible way to estimate vine copula models, and is actually doable in a comparably short time. Finally, we will also demonstrate that non-Gaussian models give clearly a competitive edge on Gaussian models using real world financial data.====The paper is structured as follows. We will briefly introduce vine copulas in Section ==== and discuss current model selection methods. We recapitulate Gaussian Graphical Models in Section ==== where we focus on the Graphical Lasso. Section ==== contains our ==== approach where we split the R-vine selection into sub-problems according to a path of solutions of the Graphical Lasso and solve each separately with increased accuracy. We sketch an algorithmic implementation and continue to numerical examples in Section ====. There, we show a simulation study in ==== dimensions to demonstrate the feasibility of our approach in moderate dimensions. Afterwards, we increase the dimension to over ==== to demonstrate the high efficiency of our approach with respect to time consumption, at the same time outperforming standard methods in terms of penalized goodness of fit measures. We finally include an example in more than ==== dimensions which demonstrates that a pure Gaussian fitting is too restrictive for real world datasets, and can be improved significantly by ultra high dimensional vine copulas.====The following is the Supplementary material related to this article. ",Dependence modelling in ultra high dimensions with vine copulas and the Graphical Lasso,https://www.sciencedirect.com/science/article/pii/S0167947319300568,2 March 2019,2019,Research Article,479.0
"He Shengmei,Ma Shuangge,Xu Wangli","Center for Applied Statistics and School of Statistics, Renmin University of China, Beijing 100875, China,School of Financial Mathematics & Statistics, Guangdong University of Finance, Guangzhou 510521, China,School of Public Health, Yale University, New Haven, CT, USA","Received 17 September 2018, Revised 2 February 2019, Accepted 4 February 2019, Available online 1 March 2019, Version of Record 11 March 2019.",https://doi.org/10.1016/j.csda.2019.02.003,Cited by (4),"Cui et al. (2015) proposed a mean–variance feature-screening method based on the index ====. By modifying ==== with a weight function, a new index ==== is introduced to measure the dependence between ==== and ","With the rapid development of scientific, especially data acquisition techniques, high-dimensional and ultrahigh-dimensional data are more frequently emerging in various scientific fields, such as molecular biology, clinical genomics, brain image research, and economics. For example, the human carcinoma (CAR) data in ==== contain 12,533 gene expression measurements. High-dimensional-data variable selection problems can be studied with, among others, Lasso (====), SCAD (====), and the Dantzig selector (====). However, when the dimension is ultrahigh, it is very challenging to do variable selection using the methods mentioned above.====To overcome the variable selection problem in ultrahigh dimension, ==== proposed a nonparametric independence screening (NIS) by using the B-spline to estimate the marginal correlation utility. ==== applied the Kendall’s ==== based on the distance correlation introduced by ====, but DC-SIS may not perform well when the variables are heavy-tailed because it assumes the finite moment condition. ==== introduced a modified Blum–Kiefer–Rosenblatt correlation (MBKR) to measure the relationship between ==== and ====There is also extensive literature investigating the ultrahigh-dimensional classification problem. ==== proposed a variable screening based on the two-sample ====-statistic. ==== as an extension of the KF and can be applied to multi-classification. ==== proposed an MV-SIS screening method for ultrahigh-dimensional discriminant analysis based on the difference between conditional and unconditional distribution functions.==== use the index ==== to measure the dependence between the continuous variable ====, where ====, ==== is the unconditional distribution function of ====, and ==== given ====. ==== if and only if ==== and ==== are statistically independent, which implies that ====). ==== is an average of the squared difference ==== which may not reflect the discrepancy well in a tail. Consequently, it is necessary to develop more robust feature-screening procedures for heavy-tailed distributions. One way is to add a nonnegative weight function ==== to make a clearer distinction between ==== and ====. That is, we are willing to sacrifice slight efficiency in which the difference between ==== and ==== is near the median of ==== and to enlarge that near the tail of ====. A natural choice is ====, which is larger for ==== near ==== and ==== and smaller near ==== (====). In this paper, we introduce an index ==== to measure the dependence between ==== and ==== by modifying ==== with the weight function ====, and propose an Anderson–Darling sure independence screening (AD-SIS) feature-screening procedure to screen features in ultrahigh-dimensional discriminant analysis.====The remainder of this paper is organized as follows. Section ==== introduces the ====. Concluding remarks are provided in Section ====. Proofs are proposed in the ====.",A modified mean-variance feature-screening procedure for ultrahigh-dimensional discriminant analysis,https://www.sciencedirect.com/science/article/pii/S0167947319300386,1 March 2019,2019,Research Article,480.0
"Bischofberger Stephan M.,Hiabu Munir,Mammen Enno,Nielsen Jens Perch","Cass Business School, City, University of London, 106 Bunhill Row, London, EC1Y 8TZ, United Kingdom,School of Mathematics and Statistics, University of Sydney, Camperdown NSW 2006, Australia,Institute for Applied Mathematics, Heidelberg University, Im Neuenheimer Feld 205, 69120 Heidelberg, Germany","Received 18 September 2018, Revised 21 February 2019, Accepted 21 February 2019, Available online 1 March 2019, Version of Record 9 March 2019.",https://doi.org/10.1016/j.csda.2019.02.009,Cited by (4),"In-sample forecasting is a recent continuous modification of well-known forecasting methods based on aggregated data. These aggregated methods are known as age-cohort methods in demography, economics, epidemiology and sociology and as chain ladder in non-life insurance. Data is organized in a two-way table with age and cohort as indices, but without measures of exposure. It has recently been established that such structured forecasting methods based on aggregated data can be interpreted as structured histogram estimators. Continuous in-sample forecasting transfers these classical ","In a period where mathematical statistical fitting of big data via ==== type of algorithms gets a lot of attention in computational driven advances of prediction, it is worth to remember that some of the most important problems in mathematical statistics are forecasting problems. While a major field of econometrics, mathematical statistics, finance and other fields have researched time series approaches to forecasting, in practice age-cohort methods have often been used as a simpler and more stable alternative to time series. In this paper, we study in-sample forecasting. In-sample forecasting is a recently suggested continuous modification of age-cohort methods that takes advantage of modern smoothing technology. In particular, we will present a detailed simulation study comparing several estimators proposed for in-sample forecasting.====In age-cohort models, a cohort is a group of individuals or objects with shared characteristics. Analysis of cohorts is considered in many academic fields, with cohorts representing a common date: date of birth (longevity), admission date to a hospital or prison (longitudinal studies, epidemiology), start date of unemployment (economics), underwriting date of an insurance policy (actuarial science), etc. When modeling different cohorts it is implicitly assumed that individuals in the same cohort have similarities due to a shared environment that differentiates them from other cohorts. In an age-period-cohort model one additionally considers age, i.e., the time from the initial date until onset of an event, and period, i.e., the calendar date of the event. The outcome of interest, ====, for cohort ==== and age ==== is modeled log-linearly: ====where ==== is the effect of cohort ====, ==== corresponds to age ==== and ==== to period ====. The parameters ==== are assumed fixed but unknown and have to be estimated from the data. The dependence on the period, ====, is implicit via ====. Model ==== is omnipresent in a wide array of fields often arising from repeated cross-sectional studies. Recent contributions among many others include aging (====), blood pressure (====), health inequalities (====), social capital (====), household savings (====) and obesity epidemic (====).====Nested within the age-period-cohort model is the simpler age-cohort model which arises for ====, meaning that there is no period effect: ====The age-cohort model in comparison to the age-period-cohort model has two major advantages. Firstly, the parameters are identifiable up to a constant. In contrast, in model ====, a solution ====) can be shifted by an arbitrary linear trend without altering the outcome. This makes interpretation and extrapolation of the parameter estimates difficult. Secondly, forecasting, i.e., estimation for ====, is possible “in-sample”, i.e., without time series extrapolation: Assume that cohorts are observed for ====, and that age is observed for ====. Period is given by ====. Once the parameter values ====, ====, ==== are fitted, forecasts for the effect ==== for observed cohorts, i.e., ====, are given up to ==== units ahead via ====. If one further assumes that ==== is an upper bound of age, then complete forecasts are indeed available for all observed cohorts without the need of extrapolation. Clearly, mathematical ease alone cannot justify the choice of a model. But in many cases period-effects seem indeed not significant. Hence, often the age-cohort model ==== ensures both a better model fit and mathematical tractability.====: For each past claim, one considers the date (cohort ====) when the accident had happened and the delay (age ====) there was until the claim was reported to the insurer. Hence, cohort and age satisfy ====; given a certain year-wise aggregation. This information is then used to estimate the number of future claims ====, ====, for accidents in the past, ====. Under model ====, the parameters ==== and ==== for each cohort ==== and age ==== can be estimated from past data. Assuming a maximum delay (usually 7 to 10 years in practice, depending on the business line), the estimates of the parameters can be used to forecast the number of future claims with ====. More details of this age-cohort-reserving example are given in the recent contribution ==== and are also included in the highly-cited overview paper of actuarial reserving (====).====Other examples where no significant period effect has been found include among many others cancer studies (====, ====), returns due to education (====), unemployment numbers (====), mesothelioma mortality (====, ====).==== is devoted to a small simulation study showing how a non-smoothed estimator breaks down when the sample sizes are too small; hence making forecasts unreliable. A series of recent papers introduced several continuous versions of ====, ==== and extensions thereof in what are coined there as in-sample forecasters (====, ====, ====, ====, ====, ====).====This paper is devoted to the continuous analogue of the simple age-cohort model, Eq. ====: ====for ==== and where ==== is a two-dimensional density function as considered in ====, ====, ==== and ====. If ==== in ==== denotes occurrence, then ==== arises from ==== by replacing the discrete arguments ==== by continuous arguments ====. Note that age ==== and cohort ==== and age ==== on ==== are independent from each other in model ====. Analogue to model ====, ==== represents the effect of age and ==== that of cohort. Instead of estimating the effects ==== and ==== for all ====, we now estimate the marginal distributions ==== and ==== from the data and thus get an estimate for the joint distribution under the assumption of independence. The estimated joint distribution then provides information, without extrapolation, about the future, i.e., density values for ====. The estimation problem of model ==== is different to classical statistical literature because observations are not available on the full set ====, with interest often exactly in the unobserved area, ====.====In-sample forecasters generate a unified approach to the class of age-cohort and age-period-cohort models and therefore provide opportunity for a general improvement across disciplines. Generally, consider a distribution on a set ==== where data generated from that distribution is only available for observation on a strict subset ====. Our particular interest is in the density on ====. An in-sample forecaster is a structured model with the property that the distribution on ==== is known from the distribution on ====. In most of the applications we are aware of, ==== represents the past and ==== represents the future — hence the term forecasting. One necessary assumption for this methodology to work is that the parameters of the distribution can be estimated from the observations in ====. For example unspecified nonparametric one-dimensional functions are sufficient to describe the distribution on ====. More generally, the distribution on ==== is a function of some components and the distribution on ==== is another function of the very same components. It is therefore necessary to work inside the world of structured models. Summarizing, the guiding principle of in-sample forecasting is that a forecaster can be constructed from in-sample estimators without further extrapolation. This often seems more intuitive, simpler and more stable than time series forecasting that requires first estimation and then extrapolation. Variations of in-sample forecasters have therefore been developed by practitioners who wish to have a hands-on understanding of all entering components and their relative importance for the forecast. Practitioners often deviate from standard statistical estimation when prior knowledge provide them with extra information. It is of course extremely important that the practitioners understand all entering components to be able to perform such manual corrections in a reliable way. Therefore in-sample forecasting is a powerful methodology in many practical forecasting settings.====Another common two-dimensional application, besides reserving in non-life insurance, appears in medical studies, specifically in the research of the mortality of a disease. Typically, patients enter the study when the disease is diagnosed and they are observed until current calendar time or until some event happens. That event could be death, see for example ==== forecasting future asbestos related deaths in the UK via a structure as above. ==== do not coin their methodology in-sample forecasting and they use a discrete non-smooth estimating technique that is common in age-cohort, age-period, period-cohort or age-period-cohort studies. But the structure is the same as the in-sample forecasting methodology considered in this paper and the likelihood based approach of ====In the above mentioned example about future asbestos related deaths, we have data about past deaths in ==== and future deaths will happen in ====. The event under observation is death; future deaths are of course unknown at the day the data collection ends. Only the number of deaths that have already occurred is known. The purpose of the forecasting exercise might be to forecast the number and timing of future deaths in the considered cohort. In this scenario, we have truncated data represented by ==== where ==== is the date an individual has entered the study and ==== is time until death. Truncation occurs because ==== must be before the day of data collection. The region ====, where ==== is after the day of data collection, contains future events only. The typical in-sample forecasting assumes data to be structured in such a way that the distribution of interest depends on one-dimensional components only and that these one-dimensional components can be estimated from the data in ====.====The aim of this paper is to summarize those methods that solve ====, extend them with multiplicative bias corrected versions and compare them both theoretically and in a simulation study. This should give practitioners and applied researchers guidance when estimation of a continuous age-cohort-model is considered. This study should also be seen as first cornerstone in the understanding of more complex models including continuous analogues of ==== and extensions thereof. We chose to concentrate on the simple model ====, which are not the continuous analogue to ====. One example would be the model ====, modeling an additional operational time term ==== (====). Also such other generalizations need a good fundament of the understanding of the simple age-period model before they can be fully developed.====. In Sections ====, ====, ==== we introduce the different estimators and their multiplicative bias corrected versions are defined in Section ====. Different problems in finite sample simulation studies and their results are described in Section ====To derive the asymptotic behavior of the smoothed histogram estimator we illustrate the assumptions and proofs for the covariate ====. For simplicity of illustration we leave out subscripts ==== and dependence on the bandwidth ==== where the interpretation is clear.====First we define aggregated observations ==== that approximate the real but unknown continuous observations ==== and describe the assumptions we need. We identify a counting process through the aggregated observations and derive its intensity and hazard rate. With this new notation we can state the assumptions for our results.",A comparison of in-sample forecasting methods,https://www.sciencedirect.com/science/article/pii/S0167947319300581,1 March 2019,2019,Research Article,481.0
"Matsuda Takeru,Komaki Fumiyasu","Department of Mathematical Informatics, Graduate School of Information Science and Technology, The University of Tokyo, 7-3-1, Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan,RIKEN Center for Brain Science, 2-1, Hirosawa, Wako, Saitama, 351-0198, Japan","Received 17 April 2018, Revised 10 February 2019, Accepted 13 February 2019, Available online 1 March 2019, Version of Record 14 March 2019.",https://doi.org/10.1016/j.csda.2019.02.006,Cited by (3),We develop an ,"In various applications, we encounter problems of estimating the unobserved entries of a matrix from the observed entries. For example, in the famous Netflix problem, we have a matrix of movie ratings by users and aim to predict the preference for movies of each user for recommendation. This problem is called the matrix completion problem and many studies have investigated its theoretical properties (====, ====) and developed efficient algorithms (====, ====, ====, ====).====In the matrix completion problems, the low-rank property of the underlying matrix plays a central role. For example, in the Netflix problem, the rank is interpreted as the number of ====In practice, the data matrix often contains observation noise and we aim to recover the true underlying matrix. If the data matrix is fully observed with the Gaussian observation noise, then the matrix completion problem reduces to the estimation of the mean matrix parameter of a matrix-variate normal distribution. For this problem, ==== pointed out that this estimator shrinks the singular values of the observed matrix for each. Therefore, this estimator performs well when the true value of the mean matrix parameter has low rank. Based on this idea, ====In this study, we develop an empirical Bayes (EB) algorithm for matrix completion. The EB algorithm is a natural extension of the Efron–Morris estimator. Since the EB algorithm is essentially the Expectation–Maximization algorithm applied to a simple model, it does not require heuristic parameter tuning other than tolerance. Numerical experiments demonstrate the effectiveness of the EB algorithm compared with existing algorithms. Specifically, the EB algorithm works well when the rank of the underlying matrix is relatively large or the proportion of observed entries is small. In other words, the EB algorithm is suitable for situations where the amount of information available from observed entries is small. Application to real data also shows the practical utility of the EB algorithm.====This paper is organized as follows. Section ==== reviews the previous results on the empirical Bayes estimation of matrix means. Section ==== provides details of the EB algorithm. Section ==== presents the results of the numerical experiments while Section ==== applies the EB algorithm to real data. Section ==== gives concluding remarks.====Suppose that ==== and ====. Here, we derive the posterior distribution of ==== given a subvector of ====.====First, consider the case where we observe the first ==== entries of ====. By putting ====, the observed subvector of ==== is given by ====. Then, the posterior distribution of ==== given ==== is obtained as ==== where ====
 ====Here, we used the Sherman–Morrison–Woodbury formula (====) ====Therefore, the posterior distribution of ==== given ==== is the normal distribution ====.====The above result is straightforwardly extended to the general case where we observe ==== entries of ==== with indices ====. Namely, the observed subvector of ==== is given by ====. Let ==== and ==== be defined by ====
 ====
 ====
 ====Then, the posterior distribution of ==== given ==== is obtained as ====where ====
 ====Therefore, the posterior distribution of ==== given ==== is the normal distribution ====.",Empirical Bayes matrix completion,https://www.sciencedirect.com/science/article/pii/S0167947319300556,1 March 2019,2019,Research Article,482.0
"Li Jinqing,Ma Jun","Department of Statistics and Actuarial Studies, School of Insurance and Economics, University of International Business and Economics, China,Department of Statistics, Macquarie University, Australia","Received 23 July 2018, Revised 21 February 2019, Accepted 23 February 2019, Available online 28 February 2019, Version of Record 11 March 2019.",https://doi.org/10.1016/j.csda.2019.02.010,Cited by (10),"Existing likelihood methods for the additive hazards model with interval censored survival data are limited and often ignore the non-negative constraints on hazards. This paper proposes a maximum penalized likelihood method to fit additive hazards models with interval censoring. Our method firstly models the baseline hazard using a finite number of non-negative basis functions, and then ==== and baseline hazard are estimated simultaneously by maximizing a penalized log-likelihood function, where a penalty function is introduced to regularize the baseline hazard estimate. In the estimation procedure, non-negative constraints are imposed on both the baseline hazard and the hazard of each subject. A primal–dual interior-point algorithm is applied to solve the ==== problem. ==== are obtained and a simulation study is conducted for assessment of the proposed method.","The additive hazards (AH) model (e.g., ====, and ====) provides an important alternative to the conventional proportional hazards (PH) model of ====. It is well known that the PH model may not be appropriate for some survival data when the proportional hazards assumption is violated. In this case, the AH model provides an attractive alternative. The semi-parametric AH model, given by Eq. ====This paper considers estimation of the semi-parametric AH model with partly interval-censored data (e.g. ====). Therefore, event times as well as left, right and interval censoring times can all be involved when fitting the model. We implement a primal–dual interior point algorithm (e.g., ====The existing methods for fitting the AH model suffer from the following deficiencies. (i) Most approaches do not treat appropriately the non-negative constraints on the baseline and the individual hazards. For example, ====, ====, ==== and ==== totally ignore such constraints, and ==== consider indirectly constraining only the baseline hazard. ==== is the only reference we can find that includes both baseline and individual hazards, but only on current status data which is a special case of the partly interval-censored data considered by this paper. (ii) None of the existing methods can directly provide smooth estimates to the baseline hazard. In practice, a changing pattern in the baseline hazard is more easily detected with a smooth estimate of the baseline hazard.==== in the simulation.====The rest of this paper is arranged as follows. Section ==== formulates the constrained MPL estimation problem. A primal–dual interior-point algorithm is presented in Section ====. In Section ==== explains optimal smoothing parameter selection using a marginal likelihood. Section ====.====The following is the Supplementary material related to this article. ",Maximum penalized likelihood estimation of additive hazards models with partly interval censoring,https://www.sciencedirect.com/science/article/pii/S0167947319300593,28 February 2019,2019,Research Article,483.0
"Fuentes-García Ruth,Mena Ramsés H.,Walker Stephen G.","Facultad de Ciencias, Universidad Nacional Autónoma de México, Mexico,IIMAS, Universidad Nacional Autónoma de México, Mexico,Department of Mathematics, University of Texas at Austin, Austin, USA","Received 25 April 2018, Revised 15 February 2019, Accepted 16 February 2019, Available online 23 February 2019, Version of Record 6 March 2019.",https://doi.org/10.1016/j.csda.2019.02.008,Cited by (13),"Motivated by the Hopfield’s network, a conditional maximization routine is used in order to compute the posterior mode of a random allocation model. The proposed approach applies to a general framework covering ==== and nonparametric ==== mixture models, product partition models, and change point models, among others. The resulting algorithm is simple to code and very fast, thus providing a highly competitive alternative to ","Given a set of observations, ====, ====), density estimation, (e.g. ====, ====), change point detection, (e.g. ====, ====), and product partition models, (e.g. ====).====The problem can be recast by means of the so-called latent allocation variables, ====, where ==== indicates ==== is allocated to cluster ====, where ==== can include being ====. If one assumes a probabilistic model for ==== which depends on ====, say ====, then a prior ==== on all possible allocations induces a posterior model given by ====To provide a concrete example, let us consider the data model where the ==== are independent and identically distributed (i.i.d.) from the mixture model ====where the weights ==== sum to one almost surely (a.s.), the parameters ==== are i.i.d. from some fixed distribution with density ====, and are independent of ====. Here, ==== is a density function for each ====. Hence, the posterior on allocations reduces to ====where the first term accounts for the prior ==== and the second term for the intra-cluster likelihood ====. Note that this approach includes both finite and infinite mixture models. Also note that for any partition on the ==== there are many ====s yielding such a partition for which ==== are all identical. In particular, there will be a configuration of ==== which has no gaps; in the sense that if the number of partitions is ==== then the components of ==== are precisely taken from the integers ====.====Letting ==== and considering only those ==== for which ====, the variable ==== induces a partition ====, i.e. a ==== valued variable, where ==== denotes the set of all partitions of ====, where, e.g. ====. Another approach to construct a ==== without directly using a mixture model, is through the so-called product partition model (PPM), given by ====where ==== is known as a ==== function, (see ====, ====, ====).====Conditioning on a particular allocation ====, the intra-cluster likelihood is computed as ====Though in principle, such approaches resolve posterior clustering inference of a given data set, several complications arise. The foremost of these is the lack of an ordering and the possible values ====Direct sampling strategies for sampling ==== from ==== have been investigated by, among others, ====, who proposed an algorithm to sample from ==== and ==== attempted to reduce the dimension of ==== by approximating ====Starting with ====, one can then easily force ==== to be one of the previous ====, i.e. no new group, or ====, implying a new group.====Another direction has been to find an optimal ==== according to some specified loss function. That is, one minimizes ====for some choice of ====. ==== and ==== propose loss functions which penalize miss-assigned groups. A discussion on types of loss functions and routines to find the optimal ==== are given in ====.====Here we argue that a coherent Bayesian point estimate of ==== is the maximum a-posteriori (MAP) estimate. Nevertheless, there are points of view criticizing this particular single point estimate; see ====. As a Bayesian there is a posterior belief distribution on ==== space, each with a belief probability ====. Given the only information is ====, to us at least, it appears incoherent to select a ==== as opposed to the MAP estimate ==== for which ====For whatever ==== is, the experimenter believes it is less likely to be the true classification compared to ====. At this juncture there seems little motive for selecting ==== as a single point estimate.==== yet ====. This, to us, appears incoherent. It is worth noting that we would not make the same claim in a continuous setting, as in that case the mode of a posterior density does not represent a probability of a particular outcome.====However, it remains a non-trivial exercise to achieve the goal of finding ====. The main objective of this paper is to provide an algorithm for finding the MAP estimate, and which does it fast, so that in particular it can be run with multiple starting points (recommended for all optimization algorithms), thus mitigating the problem of local modes.====The inspiration for the algorithm comes from neuroscience in the form of Hopfield’s network. It is the speed at which the Hopfield’s network algorithm reaches a mode which provided the backdrop for the initial experimentation of using sequences of conditional maximizations. While such routines are known in the ==== literature, they are far from popular. Hopefully, the present paper will spark a new life into these type of algorithms.====The layout of the paper is as follows. In Section ==== we look at choices of prior for the allocation variables in the context of a mixture model; whereas in Section ==== we consider various intra-clustering models for the data for which allocation variables are relevant. Section ==== contains numerical illustrations involving both synthetic and real data.====The following is the Supplementary material related to this article. ====
 ",Modal posterior clustering motivated by Hopfield’s network,https://www.sciencedirect.com/science/article/pii/S016794731930057X,23 February 2019,2019,Research Article,484.0
"Griffin Maryclare,Hoff Peter D.","Center for Applied Mathematics, Cornell University, Ithaca, NY, USA,Department of Statistical Science, Duke University, Durham, NC, USA","Received 28 February 2018, Revised 3 February 2019, Accepted 8 February 2019, Available online 22 February 2019, Version of Record 12 March 2019.",https://doi.org/10.1016/j.csda.2019.02.005,Cited by (0),"Consider the problem of estimating the entries of an unknown mean matrix or tensor given a single noisy realization. In the ====, this problem can be addressed by decomposing the mean matrix into a component that is additive in the rows and columns, i.e. the additive ANOVA decomposition of the mean matrix, plus a matrix of elementwise effects, and assuming that the elementwise effects may be sparse. Accordingly, the mean matrix can be estimated by solving a penalized ","Researchers are often interested in estimating the entries of an unknown ==== mean matrix ==== given a single noisy realization, ====, where the entries of ====. Consider a noisy matrix ====This is challenging because no replicates are observed. Each unknown ==== corresponds to a single observation ==== has high variability. Accordingly, simplifying assumptions that reduce the dimensionality of ==== are often made. Many such assumptions relate to a two-way ANOVA decomposition of ====: ====where ==== is an unknown grand mean, ==== is an ==== vector of unknown row effects, ==== is a ==== vector of unknown column effects, ==== is a matrix of elementwise “interaction” effects and ==== and ==== are ==== and ==== vectors of ones, respectively. In the absence of replicates, implicitly assuming ==== is common. This reduces the number of freely varying unknown parameters, from ==== to ====, but is also unlikely to be appropriate in practice.====Alternatively, one might assume that elements of ==== can be written as a function of a small number ==== of multiplicative components, i.e. ==== where ==== and ==== and ==== and ==== row and column factors. This corresponds to a low-rank matrix ==== and an additive-plus-low-rank mean matrix ====. Additive-plus-low-rank models have a long history and continue to be very popular (====, ====, ====, ====, ====, ====). However, in the settings we consider it is reasonable to expect that ==== may fail, e.g. if ==== were an ==== were large while all ====, ==== were equal to zero. In this case, a low-rank estimate of ==== would not suffice because a full rank ==== estimate would be needed.====If ==== is approximately additive in the sense that large deviations from additivity are rare, then ==== is sparse and estimation of ==== may be improved by penalizing elements of ====: ====The ==== and ====. Elements of ====Although ==== is a standard lasso regression problem that can be solved easily given values of ==== and ====, specifying values of ==== and ==== is uniquely challenging in this setting. The methods suggested by ====, ==== are correlated. The same is true of the unbiased risk estimate minimization procedure suggested by ====. Although columns of the design matrix will become less correlated as ==== and ====, the correlations may not be negligible in practice especially if ==== or ==== is relatively small. ==== also suggested cross validation, which could be performed after rewriting Eq. ==== to depend on a single parameter ====. However, cross-validation is also poorly suited to this setting. Consider leave-one-out cross validation to select a value of ====, and suppose we hold out ==== and solve ==== for any fixed value of ==== using the elements of ==== excluding ====. We obtain estimates of ====, ====, ====, and all elements of ==== ==== ==== contains any information about ====. For this reason, we cannot compute an out-of-sample prediction for ==== without making additional assumptions that relate ====, ====, ==== and all of the elements of ==== except ==== to ==== and selecting ==== by cross validation without additional assumptions is not possible.==== can alternatively be interpreted as outliers. ==== interpret elements of ==== in this way and consider the more general problem with an arbitrary full rank design matrix ====. They approach specification of ==== and ==== by introducing a conservative extension of the methods suggested by ==== for orthogonal design matrices, setting ==== different values ==== where ====. Because ==== can be very challenging to estimate, they suggest setting ==== in a data-adaptive way using a modified BIC. Although the methods proposed by ==== have the advantage of applying to general regression problems with arbitrary design matrices ====, they have computational disadvantages in high dimensions because they require computing an initial robust estimate of and iteratively re-estimating ====.====We take another approach and view the ==== penalty on ==== as a Laplace prior distribution, in which case ==== and ==== penalty and the Laplace prior has long been acknowledged (====). It offers not only a framework for specifying ==== and ====, but also decision theoretic justifications for using estimates of ==== and ==== obtained by solving ==== using estimated ==== and ==== because the posterior mode is known to minimize a specific data-adaptive loss function (====, ====). The challenge is in the estimation of ==== and ====, because computing maximum marginal likelihood estimates may be prohibitively computationally demanding and intractable in practice (====, ==== and ==== that are easy to compute, consistent and independent of assumptions made regarding ==== and ====. As our approach to estimating ==== and ==== uses the Laplace prior interpretation of the ==== penalty, we refer to estimation of ==== via optimization of Eq. ==== using these nuisance parameter estimators as LANOVA penalization and we refer to the estimate ==== as the LANOVA estimate.====The paper proceeds as follows: In Section ====, we introduce moment-based estimators for ==== and ====, show that they are consistent as ==== the number of rows ==== columns of ====, we discuss estimation of ==== via Eq. ==== given estimates of ==== and ==== and introduce a test of whether or not elements of ==== are heavy-tailed, which allows us to avoid LANOVA penalization in settings where it is especially inappropriate. We also investigate the performance of LANOVA estimates of ==== relative to strictly additive estimates, strictly non-additive estimates, additive-plus-low-rank estimates, IPOD estimates from ====, and examine robustness to misspecification of the distribution of elements of ====. In Section ====, we extend LANOVA penalization to include penalization of lower-order mean parameters ==== and ==== and also to apply to the case where ==== and ==== are ====-way tensors. In Section ====, we apply LANOVA penalization to a matrix of gene expression measurements, a three-way tensor of fMRI data and a three-way tensor of wheat infection data. In Section ==== we discuss extensions, specifically multilinear regression models and opportunities that arise in the presence of replicates.====The following is the Supplementary material related to this article. ",Lasso ANOVA decompositions for matrix and tensor data,https://www.sciencedirect.com/science/article/pii/S0167947319300520,22 February 2019,2019,Research Article,485.0
"Grömping Ulrike,Fontana Roberto","Beuth University of Applied Sciences, Berlin, Germany,Politecnico di Torino, Torino, Italy","Received 12 January 2018, Revised 13 November 2018, Accepted 31 January 2019, Available online 21 February 2019, Version of Record 6 March 2019.",https://doi.org/10.1016/j.csda.2019.01.020,Cited by (3)," package, using one of two commercial optimizers.","). Mixed level experiments, i.e. experiments for which not all factors have the same number of levels, are common in applications, especially if some factors are qualitative in nature. If a particular mixed level experiment is required, availability of a suitable array can be an issue. It is common to create a factorial design from a subset of the columns of a published array, and ==== discussed ways to improve this type of usage by optimizing the choice of columns.====Of course, optimization of column selection from an existing array cannot be better than the creation of a tailor-made optimized array for the task at hand. For experiments with some qualitative factors, one will often not have a particular model in mind, but will aim for the estimation of main effects, perhaps also of two-factor interactions, assuming that lower order effects are more important than higher order effects. For such a context, ==== introduced an algorithm for the creation of an orthogonal array in a given number of runs that fulfills the quality criterion “generalized minimum aberration” (GMA, see Section ====); his formulation relied on the complex coding (====). For ==== factors, Fontana’s algorithm used a sequence of ==== matrix, where ==== denotes the number of runs needed for a full factorial experiment in the factors to be investigated, i.e. is usually very large. The intriguing feature about the approach is its principal ability to ==== create an ==== array for a specific experimental situation with mixed numbers of levels that is at the same time ====. Unfortunately, the details of the algorithm’s implementation were such that its application in practice would only work for very small data situations; we set out to improve that situation.====We modify and improve Fontana’s proposal in the following ways: Based on ====, because of using the complex coding). For the objective of the first actual minimization, we derive a lower bound, which, if attained, allows to finish this optimization step fast (see Sections ====, ====Many specialized algorithms in the design community, e.g. the enumeration algorithms by ==== for symmetric orthogonal arrays or by ==== for general orthogonal arrays, are intended as a tool for design researchers, for enumerating all existing designs or for providing an authoritative list of designs that practitioners can turn to. Other algorithms, e.g. the ==== algorithm, can be used in design creation of a (mixed level) factorial design for a specific experiment. Our algorithm is of that latter nature. It is more general than the ==== algorithm, in that it incorporates (part of) the GMA quality criterion (see Section ====) into design creation; furthermore, the ==== algorithm is restricted to the construction of arrays that can be obtained from design keys (using pseudofactors), whereas our algorithm can construct arrays from the larger class of ==== orthogonal arrays. On the other hand, the ==== package (====, ====, ==== package DoE.MIParray; ====, ==== package planor).====This paper derives the algorithm and provides examples that evidence its usefulness. All calculations have been done using the afore-mentioned ==== package DoE.MIParray, which offers functions based on two different commercial solvers for mixed integer optimization problems (==== and ====, as documented in ==== and ====). Both vendors provide free academic licenses and ==== packages that support access from within ==== on the algorithm’s implementation in the two solvers; the purpose of presenting the specifics of inputs to the commercial solvers is to enable readers to create their own implementations in other professional solvers, e.g. CPLEX. Apart from giving a high-level overview, we do not discuss any specifics of how mixed integer optimization is implemented within the solvers we use; readers are referred to the literature, e.g. the very accessible modeling cookbook provided by ==== and references therein.====Section ==== introduces the necessary fundamentals and some basic results. Section ==== provides a motivating example that illustrates the type of design that benefits most from our algorithm, Section ==== describes the algorithm, and Section ==== applies it to the test cases of ==== (====) and to further interesting mixed level requests for orthogonal arrays (====) or supersaturated strength 1 arrays (====). The discussion highlights the merits of our proposal and points out needs for further research.====The following is the Supplementary material related to this article. ====
 ",An algorithm for generating good mixed level factorial designs,https://www.sciencedirect.com/science/article/pii/S0167947319300350,21 February 2019,2019,Research Article,486.0
"Thaden Hauke,Klein Nadja,Kneib Thomas","Georg-August-Universität Göttingen, Germany,Humboldt University of Berlin, Germany","Received 18 February 2018, Revised 3 December 2018, Accepted 9 December 2018, Available online 21 February 2019, Version of Record 6 March 2019.",https://doi.org/10.1016/j.csda.2018.12.004,Cited by (0),"Modeling complex relationships and interactions between variables is an ongoing statistical challenge. In particular, the ==== modeling of multiple response variables is of great interest in methodological and applied research. Within this context the incorporation of semiparametric predictors into ==== recursive simultaneous equation models is considered. Extending the existing framework by imposing effect priors that account for potential correlation of the effects across equations allows for borrowing strength across equations as with multivariate conditional autoregressive priors used for the analysis of multivariate spatial data. A ",", ====, ====, ====, ====), the development of models for multivariate responses in the context of distributional regression (====, ====) or extensions of simultaneous equation models (for example ====). These approaches have been applied successfully in the presence of endogeneity, confounding, sample selection (====, ====, ====, ====).====In this article, we contribute to the development of certain of these areas by joining the framework of simultaneous equation models (SEM, ====) with the flexibility of semiparametric effect types. More precisely, we summarize how different effects can be conceived using a unifying basis function approach (as shown in e.g. Chapter 9 of ==== and ====In contrast to ====). We transfer MCAR priors not only to the context of bivariate SEM but also to general effect types.==== along the sub-dataset applied in ====.====The rest of the paper is structured as follows: In Section ====. The simulations in Section ==== evaluate the performance of the proposed method while Section ==== illustrates the applicability in two complex real datasets coming from different applied areas (health and ecology) and dealing with relevant research questions for our days. Finally, Section ==== summarizes our findings and discusses possible extensions for future research.====The following is the Supplementary material related to this article. These contain additional figures for the application. ",Multivariate effect priors in bivariate semiparametric recursive Gaussian models,https://www.sciencedirect.com/science/article/pii/S0167947318302834,21 February 2019,2019,Research Article,487.0
"Azzimonti Laura,Zaffalon Marco","IDSIA - SUPSI/USI, Manno, Switzerland","Received 30 May 2018, Revised 23 November 2018, Accepted 8 February 2019, Available online 19 February 2019, Version of Record 6 March 2019.",https://doi.org/10.1016/j.csda.2019.02.004,Cited by (12), from related domains.,". Almost always, BNs model the relations between discrete variables. Indeed, for discrete data many powerful ==== algorithms have been developed (==== Chap. 6–8). In the BN terminology, ==== a Bayesian network is accomplished in two main steps: ====, which consists in the identification of the most probable structure, defined by means of a Direct Acyclic Graph (DAG), and ==== (CPT) of each variable (a node of the DAG), containing the conditional probability distributions of the variable given each possible configuration of its parents. A single conditional distribution is also referred to as a ==== of the CPT.====While there have been many works on ==== in recent years (====, ====, ====, ====), ==== is usually addressed adopting the established approach (==== Sec. 17.5.4) that the estimates could be improved by estimating the different columns jointly rather than independently, since different conditional distributions belonging to the same CPT are expected to be similar to each other. Yet, parameter estimation becomes in this way much harder, as it is no longer possible to express the posterior distribution of the parameters in closed form.====, based on finite version of the Hierarchical Dirichlet Process (====) with respect to simple Dirichlet distributions. On the other hand, the mixing parameter is a random vector ====, whose posterior distribution is informative about the distribution towards which the estimates should be smoothed. Such distribution is determined by the values of ==== that are most probable ====. Moreover, the hierarchical model jointly estimates the different conditional distributions belonging to the same CPT, thus ==== from each other (==== Sec. 6.3.3.2). As a result, the hierarchical approach yields a major improvement in parameter estimation, compared to the traditional approach.====), dynamic Bayesian networks (====) and specific Bayesian network classifiers (====). Variational Inference (VI) (====, ====) is instead a technique which efficiently approximates the posterior distribution.====We derive an original VI algorithm for the proposed hierarchical model, which computes the posterior distributions with a negligible decrease of accuracy with respect to MCMC, while reducing the computational time by two orders of magnitude. We also show that, being specifically tailored for our model, it compares favourably to recently proposed algorithms for automatic variational inference (====).====We then extensively assess the impact of the novel parameter estimates based on hierarchical modelling when performing classification with Bayesian networks. We consider a large number of data sets, and we show a consistent improvement of ==== when the hierarchical model is adopted for parameter estimation. Remarkably, the improvement is found even if we favour the traditional Multinomial–Dirichlet by tuning its equivalent sample size, while we run the hierarchical model without any tuning. The advantage over the traditional estimation approach is huge when dealing with very small sample sizes, while it decreases as the sample size increases, as predicted by the theoretical analysis.====We then consider the problem of learning parameters from related data sets; this task is called ==== in the statistical literature (==== Chap. 5.6) and ==== (====, ====, ====). Variational inference is fundamental in this application, as it performs parameters estimations borrowing strength across 100 users in minutes rather than in days, as it would be required by MCMC.====The paper is organised as follows. Section ==== introduces both the traditional and the hierarchical models for parameter estimation in Bayesian networks; moreover, it derives analytical properties and exact inference for the proposed hierarchical model. Section ==== describes a variational inference algorithm for approximating the posterior distributions under the hierarchical model and reports some simulation studies for assessing the accuracy of the proposed method. Section ==== describes the parameter estimation setting in Bayesian networks and reports some simulation studies for comparing the performance of hierarchical estimators with respect to traditional estimators in a classification setting. Section ==== shows how to apply the hierarchical model to the problem of parameter learning in Bayesian networks from data coming from related data sets and shows the application to a real case study. Section ==== outlines future research directions. The appendix details exact inference for the proposed hierarchical model and reports all the proofs.====The computation starts from the innermost integral, which corresponds, e.g., to the last index of ====. We always define the quantity ====. At each step of the algorithm the integrand function ==== is a polynomial function of ====, whose coefficients depend on the quantity ====. The integrand function ==== is computed by multiplying the integral computed in the previous step ==== by all the binomials ==== in ==== or ====. In the first step ====. The primitive ==== associated to ==== is computed by means of symbolic calculus with ====. ==== is still a polynomial function in ==== with degree increased by one with respect to the integrand. The integral is then computed by evaluating the primitive function in ==== and in 0, i.e. ====. ==== is a polynomial function in ====, while ====, since there are no constant terms in the primitive function. The quantity ==== is now written as ====, with ====, thus by means of symbolic calculus we set ====. The polynomial function ==== represents the inner integral as a function of the new integration variable ====, with coefficients depending on ====. All these steps are repeated through all the layers of the multiple integral. When the outer integral has been reached, the final result is obtained as ====.",Hierarchical estimation of parameters in Bayesian networks,https://www.sciencedirect.com/science/article/pii/S0167947319300519,19 February 2019,2019,Research Article,488.0
"Ippel L.,Kaptein M.C.,Vermunt J.K.","Institute of Data Science, Maastricht University, Maastricht, The Netherlands,Tilburg University, Tilburg, The Netherlands","Received 19 January 2018, Revised 19 January 2019, Accepted 21 January 2019, Available online 18 February 2019, Version of Record 23 February 2019.",https://doi.org/10.1016/j.csda.2019.01.010,Cited by (0),"It has become increasingly easy to collect data from individuals over long periods of time. Examples include smart-phone applications used to track movements with GPS, web-log data tracking individuals’ browsing behavior, and longitudinal (cohort) studies where many individuals are monitored over an extensive period of time. All these datasets cover a large number of individuals and collect data on the same individuals repeatedly, causing a nested structure in the data. Moreover, the data collection is never ‘finished’ as new data keep streaming in. It is well known that predictions that use the data of the individual whose individual-level effect is predicted in combination with the data of all the other individuals, are better in terms of squared error than those that just use the individual mean. However, when data are both nested and streaming, and the outcome variable is binary, computing these individual-level predictions can be computationally challenging. Five computationally-efficient estimation methods which do not revise “old” data but do account for the nested ","Researchers often encounter ==== data where the outcome variable of interest is ====. For example, ==== compared the smoking behavior (smoking versus none smoking) of students that are grouped within different schools. ==== studied the effect of schools on students’ voting behavior (vote versus no vote), and ==== monitored children over a long period of time (repeated measurements nested within children) to investigate the effect of air pollution on the presence (or absence) respiratory symptoms. Furthermore, ====To settle for an unambiguous terminology throughout, we adopt the terms of the latter e-commerce example. Here, a researcher could be interested in the individual-level effect ==== to index the grouping factor which, in this particular case of multiple observations nested or grouped within the individual, denotes the individual customer whose click-through rate is being estimated. However, the methods discussed in this paper do not restrict themselves to the nesting of observations that are nested within individuals but could also be used for groupings such as individuals within schools or schools within districts. We do, however, restrict ourselves to situations where the individual’s probability to click is stationary, which implies that observations are, conditional on the individual, independent. Our interest lies in estimating the individual-level effect ====, accurately and computationally efficiently.====In a now classical paper, ==== showed that predicting the individual-level effects of one individual (i.e., ====, with the estimated sample mean over all data points, ====, results in better out-of-sample predictions (see, for instance, ====). Following this result, ==== in (====) introduced the idea of a ====, a way to weigh the estimated mean of an individual and the mean over data points to obtain a prediction of ====. The resulting weighted combination can be denoted as follows: ====where ==== is the so-called ====. Because we focus on binary outcomes, the ==== in our case denotes the proportion of (for instance) clicks. In the remainder of this paper, we refer to ==== as the sample mean for individual ====, and ==== as the sample mean for all individuals combined.====The aim of this paper is to develop and evaluate different shrinkage factors which can be used to efficiently estimate the individual-level effect in a situation where new data present themselves over time. We refer to this situation as a ====. In a data stream, the data collection is never “finished”, for instance in click-behavior data on a website. In the case of real-time prediction, where up-to-date predictions of the individual-level effects are required at each moment during the stream, methods that can ==== rather than ==== the individual-level effects, greatly improve the speed of the estimation process (====).====In general, various methods are available to deal with data streams. For instance, one could subsample from the data stream (i.e., at random include some of the data points in the analysis while excluding others), and analyze the subsample in order to obtain predictions (====). While this method solves the problem of a growing dataset, it inherently limits the information and risks not being able to include data of specific individuals who are of future interest. Another method that deals well with a data stream is a sliding-window approach. Effectively, the sliding window is also a subsample of the data, existing of only the most recent data points. The advantages of this method are that memory burden is fixed and, in cases in which the data-generating process is not stationary over time, the most recent observations most heavily influence the resulting predictions. However, choosing the size of the window often requires domain knowledge: too small might not catch any meaningful events, too large a window might computationally be too expensive (see, ==== for an introduction on many more data-stream techniques, including sliding windows).====In this paper, we focus on another method to deal with data streams: ====. This approach incrementally updates parameter estimates, without revisiting previous data (among others, ====, ====, ==== or ====), we restrict our attention solely to random-intercept models with binary outcomes.====A possible solution to efficiently obtaining estimates in a situation where the data come streaming in, is to estimate the individual-level effects in real time using ==== estimated shrinkage factors. Online estimation (or online learning) implies that a parameter (e.g., a mean, or regression coefficient) is updated using a single (or small batch of) data point and some sufficient statistics (e.g., a summation of the previous data points, ====, ====). An illustrative example is the computation of the sample mean ====. Estimating a sample mean in a data stream using online learning can be done as follows: ====where ==== indexes the data points, or equivalently, ====where ==== is the total number of observations and ‘====’ is an assignment operator, meaning that the left-hand side is updated using the expression on the right-hand side.====Note that the ==== estimation procedure stores all the observations and for each new estimate revisits the older data points. Updating the sample mean offline in a data stream thus takes increasingly more time because more and more data need to be processed. On the contrary, the ==== estimation procedure only stores ==== and ==== in memory, and, when a new data point enters, these are updated according to Eq. ====. This results in a time-constant update. Attractively, using online estimation methods, there is no need to revisit previous data points, which can therefore be discarded from memory (====). However, not every offline estimation procedure can be used exactly for online estimation (see, e.g., ====, ====The paper is organized as follows. Section ==== describes five existing shrinkage factors and develops the online implementation of each of the shrinkage factors. Section ==== presents a simulation study where we compare the online and offline implementations of the shrinkage factors in terms of the accuracy of the estimated individual-level effects. Here, we explicitly explore different data-generating mechanisms. In Section ====, we apply the developed online shrinkage factors to analyze a real dataset. The dataset contains data coming from a large panel study. Because dropouts in panel data is a serious threat, we focus on predicting the probability of non-response per repeatedly observed individual. These predictions could facilitate the choice of which respondents to invite for the next wave, or personalize the response request to achieve higher response rates. Finally, in Section ====, we discuss the limitations of the shrinkage factors and their possible extensions to a broader setting.====The following is the Supplementary material related to this article. ",Online estimation of individual-level effects using streaming shrinkage factors,https://www.sciencedirect.com/science/article/pii/S0167947319300246,18 February 2019,2019,Research Article,489.0
"Bui Anh Tuan,Apley Daniel W.","Department of Industrial Engineering & Management Sciences, Northwestern University, Evanston, IL 60208-3119, USA","Received 7 August 2018, Revised 20 January 2019, Accepted 30 January 2019, Available online 16 February 2019, Version of Record 26 February 2019.",https://doi.org/10.1016/j.csda.2019.01.019,Cited by (8), R package.,"), lumber surfaces (====), stone countertops (====), ceramic capacitor surfaces (====, ====The stochastic nature across a set of image samples can vary as a result of manufacturing or other condition changes. To illustrate, ==== shows various image samples for the simulated 2-D stochastic process considered in Section ==== are not 16 statistically equivalent realizations of the same stochastic process; rather, they are realizations of different stochastic processes and hence have stochastic nature that varies across the set of image samples. The differences across the set of image samples could represent surface roughness behavior that varies across a set of manufactured metal parts or material microstructure characteristics that vary across a set of microstructure samples, due to unstable processing conditions that vary over time. Throughout, we use the term “variation” to refer to such systematic differences in the stochastic nature of the surfaces across a set of image samples.====This work also has broader applicability. In the context related to this work, which is understanding variation, our approach is applicable to a wide range of materials. This includes random heterogeneous materials, which are ubiquitous in science, engineering, and nature (====, ====), cancer tissue image clustering (====), and outlying mammalian cell image detection (====).====The remainder of this paper is organized as follows. In Section ====, we derive and investigate the two new pairwise dissimilarity measures between image samples, which is a critical element of the approach. Section ==== describes how a form of manifold learning that takes pairwise dissimilarities as the input can be used for understanding the variation patterns existing in the image samples. Sections ====, ==== demonstrate the approach and compare the different dissimilarity measures with a simulation example and a real textile example, respectively. Section ==== provides some further discussions on visualization for understanding variation, choice of parameters when computing the image sample dissimilarities, and choice of dissimilarity measures and manifold learning algorithms. We have documented and released the computer codes used in these examples in the ==== R package (====). Section ==== concludes the paper.",An exploratory analysis approach for understanding variation in stochastic textured surfaces,https://www.sciencedirect.com/science/article/pii/S0167947319300349,16 February 2019,2019,Research Article,490.0
"Ditzhaus Marc,Pauly Markus","Institute of Statistics, Ulm University, Germany","Received 17 August 2018, Revised 1 February 2019, Accepted 3 February 2019, Available online 11 February 2019, Version of Record 5 March 2019.",https://doi.org/10.1016/j.csda.2019.02.001,Cited by (11),-package ==== and its application is demonstrated in an empirical example.,"A typical question in a two-armed clinical trial is whether a treatment is superior over a control. For time to event data, this task is usually coped with the one-sided version of the logrank test (====, ====). As it is only asymptotically optimal to detect alternatives in which the hazards are proportional, several substitutes have been proposed. In addition to weighted logrank tests (====, ====, ====, ====, ====, ====), different modifications have been established in order to derive certain power enhancements at least in the two-sided case, see, e.g., ====, ====, ====, ====, ====, ==== and the references cited therein. From a methodological point of view, the projection-type procedure of ====, against a pre-specified number of alternatives of interest. However, the method has not yet found its way into statistical practice for several reasons:====While it is nowadays accepted that permutation tests for complex heterogeneous models, here given by possibly different censoring distributions, need a certain studentization (====, ====, ====, ====, ====), the permutation technique of ==== permutations, it a priori requires the calculation of the critical value within the test statistic based on ==== Monte Carlo iterations. This regrettably results in time-consuming ==== calculations for critical values instead of the usual ==== iterations, see ==== in Section ==== for more details on the computational load.====, ====, ====, ====, ==== method. For an easy application, we implemented the novel method within the ====-package ====The paper is organized as follows: The basic set-up is explained in Section ====. We then recall the method of ==== in Section ====. An application of the resulting procedure to an empirical example can be found in Section ==== and the paper closes with a discussion and outlook in Section ====. We note that all proofs are deferred to the supplementary material, where additional simulation results and a demonstration of the R-package can be found as well.====The following is the Supplementary material related to this article. ",Wild bootstrap logrank tests with broader power functions for testing superiority,https://www.sciencedirect.com/science/article/pii/S0167947319300362,11 February 2019,2019,Research Article,491.0
"Castillo-Páez Sergio,Fernández-Casal Rubén,García-Soidán Pilar","Dep. de Ciencias Exactas, Universidad de las Fuerzas Armadas ESPE, Ecuador,Dep. de Matemáticas, Universidad de A Coruña, Spain,Dep. de Estadística e I.O., Universidad de Vigo, Spain","Received 22 May 2018, Revised 30 January 2019, Accepted 31 January 2019, Available online 11 February 2019, Version of Record 21 February 2019.",https://doi.org/10.1016/j.csda.2019.01.017,Cited by (6),"The aim is to provide a nonparametric ==== for spatial data, which can be either stationary or depart from the ","), were designed for more general dependence situations. A review of these methods can be found in ==== and in ====.==== mechanism to the multidimensional case proposed in ====. Other approaches were specifically designed for spatial data, as the one introduced in ====, which is a usual strategy in geostatistics. The semiparametric bootstrap (====, ====), and then obtaining independent samples from the uncorrelated data.====Under stationarity, both the ==== and the ==== methods may be adequate for approximating the distribution of certain estimators, such as the sample mean. However, difficulties arise when the interest is focused on estimating second-order characteristics of the process. Thus, for instance, the ==== approach fails to reproduce the variability of the covariogram parameter estimators, whereas the ==== procedure leads to accurate results only if an appropriate model is selected for the covariogram (====).====The ==== Section 3.4.3). Furthermore, this resampling approach relies on the wrong assumption that the variability of the residuals equals that of the (unobserved) theoretical errors. Thus, our main objective in the current study is to propose a nonparametric bootstrap (====. The resulting ==== method has been implemented in the ==== language (====) by using the tools available in the ==== package (====).====The ==== approach requires achieving an appropriate partition of the observation region, unlike the ==== and the ==== methods. For a stationary process, a drawback of the ==== method. On the other hand, under the presence of a non-constant trend, the main advantage of the ==== mechanism over the ==== one is to incorporate a procedure to correct the bias effect that the residuals induce on the semivariogram estimation. A similar tool for bias adjustment could be applied to the parametric semivariogram in the ==== approach, as pointed out in ====To compare the above-mentioned bootstrap techniques, simulation studies were carried out under different scenarios (with stationary and non-stationary Gaussian data). Specifically, we checked their performance to approximate properties of two well-known semivariogram estimators, since our aim was to determine whether the resampling methods reproduced the dependence structure of the random process in an adequate way.====This paper is organized as follows. Section ==== introduces some preliminaries concerning the spatial setting and the estimation of the semivariogram. The resampling techniques considered for spatial data are described in Section ====. Section ==== and the main conclusions are summarized in Section ====.",A nonparametric bootstrap method for spatial data,https://www.sciencedirect.com/science/article/pii/S0167947319300325,11 February 2019,2019,Research Article,492.0
Gregory Alastair,"Lloyd’s Register Foundation’s Programme for Data-Centric Engineering, Alan Turing Institute, United Kingdom,Department of Mathematics, Imperial College London, United Kingdom","Received 21 May 2018, Revised 28 December 2018, Accepted 26 January 2019, Available online 5 February 2019, Version of Record 19 February 2019.",https://doi.org/10.1016/j.csda.2019.01.015,Cited by (2),Empirical ==== to the empirical copula function of a ==== stream of data. A succinct space-memory efficient summary of values seen in the stream up to a certain time is maintained and can be queried at any point to return an approximation to the empirical bivariate copula function with guaranteed error bounds. An example then illustrates how these summaries can be used as a tool to compute approximations to higher dimensional copula decompositions containing bivariate copulas. The computational benefits and approximation error of the algorithm are theoretically and numerically assessed.,"Streaming data is found in many applications where data is acquired continuously. This characteristic, in addition to any space-memory constraints of the user, makessuch data a challenge for analyses. As data is acquired the analyser of the data must utilise it before the next piece of data is acquired and the entire stream cannot be stored. Therefore, given a particular statistical quantity of the data, a ====). A host of studies (====, ====, ====, ==== (e.g. ====, ====). Therefore, approximations to such a copula can be found by using the ====This work is related to other studies that also consider the construction of summaries for multidimensional data. These summaries have been used to query multidimensional ranks and ranges (====, ====, ====). Querying multidimensional ranges, such as a rectangle of points on the plane, is analogous to finding empirical copulas, only considering the actual data points on the plane rather than the marginal quantiles. This is where our motivation differs to that of ==== and ====. On this note, another closely related piece of literature to the scope of this paper is that of ==== which considers the online computation of pair-wise nonparametric correlations. However, this does not provide any theoretical error bounds on the summarised statistical approximations.====Due to the vast range of industries that use copulas to model dependent data, this application of copula models to streaming data is an important contribution to the data science community. The paper is structured as follows. A background on empirical copulas is given in Section ====. In Section ====, an algorithm to construct the summary used to obtain approximations of empirical copula functions is presented. This is followed by a theoretical and numerical assessment of the approximation from the algorithm in Sections ====, ==== respectively. Section ==== gives an example of how higher dimensional copulas framed as sets of bivariate copulas can be approximated using the ====-approximate copulas presented in this paper. A discussion then concludes the paper.====This appendix section will cover in detail the operations used in the construction and querying of the copula summary proposed in this paper. The operations in ====, ====, ==== are slightly modified versions of the ones presented in ==== and ====.",A streaming algorithm for bivariate empirical copulas,https://www.sciencedirect.com/science/article/pii/S0167947319300295,5 February 2019,2019,Research Article,493.0
"Baey Charlotte,Cournède Paul-Henry,Kuhn Estelle","Laboratoire Paul Painlevé, Université de Lille, 59655 Villeneuve d’Asq, France,Laboratoire MICS, CentraleSupélec, 92190 Gif sur Yvette, France,MaIAGE, INRA, Université Paris-Saclay, Domaine de Vilvert, 78352 Jouy-en-Josas, France","Received 27 July 2018, Revised 18 January 2019, Accepted 26 January 2019, Available online 5 February 2019, Version of Record 21 February 2019.",https://doi.org/10.1016/j.csda.2019.01.014,Cited by (17)," are widely used to describe heterogeneity in a population. A crucial issue when adjusting such a model to data consists in identifying fixed and random effects. Testing the ==== in some particular cases. Extending the existing results, a likelihood ratio test procedure is studied, to test that the variances of any subset of the random effects are equal to zero in nonlinear mixed effects model. More precisely, the asymptotic distribution of the test statistics is shown to be a chi-bar-square distribution, that is to say a mixture of chi-square distributions, and the corresponding weights are identified. In particular, it is highlighted that the limiting distribution depends strongly on the presence of correlations between the random effects. The finite sample size properties of the test procedure are illustrated through simulation studies and the test procedure is applied to two real datasets of dental growth and of coucal growth.",", ====, ====). In mixed effects models, parameters are of two types: on one side, fixed effects that are common to all the individuals of the population; on the other side, random effects that vary from one individual to the other. The last ones are also called individual parameters.==== p14).==== the parameter space, by ==== the subset of ==== corresponding to the null hypothesis and by ==== the subset corresponding to the alternative hypothesis, with ====. ====, assuming that ==== is open, treated the case where the true value of the parameter lies on the boundary of ==== and ====, which is assumed to be a proper set of ====, i.e. strictly contained in ==== generalized these results by considering the case where the true value lies in a subset of ==== studied the asymptotic distribution of a larger class of tests when the true value is on the boundary of ==== and an interior point of ====. He established that the asymptotic distribution is a mixture of chi-square distributions. Simultaneously, ==== obtained similar results in the case where the true value is on the boundary of ====. They established a general convergence result and they derived the expression of the asymptotic distribution only in specific cases, assuming in particular that the parameter space ==== is equal to the product of a finite number of either closed, half-open or open intervals of ====. They considered several specific cases for applications. They identified in particular the limiting distribution of the LRT for testing that the variance of one single random effect is equal to zero as a mixture ====, where ==== is the Dirac distribution at zero and ==== is the chi-square distribution with one degree of freedom (==== case 5). They also identified the limiting distribution of the LRT for testing simultaneously that the variance of one single random effect is equal to zero and that its mean is equal to a constant as a mixture ====
 (====, ==== for linear mixed models with one single random effect, and ==== extended these results to linear mixed models with more than one random effect.====Other approaches were also inquired. Several years later, ==== proposed a procedure based on the score test for testing several variance components in linear mixed models, and ==== studied a bootstrap test based on the score test for testing several variance components in a generalized linear mixed model, while ====, ==== and ==== considered permutation tests for testing several variance components in the context of linear and generalized mixed effects models. ==== or ====. They also exhibited the common limiting distribution in some specific cases. However, to the best of our knowledge, there exist no results identifying the limiting distribution of the LRT for general tests on variance components in mixed effects models.====In this paper, we consider the LRT in nonlinear mixed effects models to test that the variances of any subset of the random effects are equal to zero and identify its asymptotic distribution as a mixture of chi-square distributions. In Section ====, we present the framework of nonlinear mixed effects models. Section ==== is devoted to the description of the proposed test and its theoretical properties. Practical implementation guidelines are presented in Section ====. Experimental results illustrate the performances of the procedure through simulation studies and real datasets analysis in Section ====. The paper ends with some discussion in Section ====. The technical proofs are given in the ====.",Asymptotic distribution of likelihood ratio test statistics for variance components in nonlinear mixed effects models,https://www.sciencedirect.com/science/article/pii/S0167947319300283,5 February 2019,2019,Research Article,494.0
"Choi Ji-Eun,Shin Dong Wan","Department of Statistics, Ewha University, Republic of Korea","Received 16 September 2017, Revised 8 August 2018, Accepted 15 October 2018, Available online 5 February 2019, Version of Record 19 February 2019.",https://doi.org/10.1016/j.csda.2018.10.016,Cited by (2),"Based on the test of Wied et al. (2012), we construct a ==== CUSUM test for correlation change. The bootstrap test uses the bootstrap critical value obtained from the distribution of the moving block ====. The asymptotic null distribution of the bootstrap test is shown to be the same as that of the original test. Consistency of the bootstrap test is proved under an alternative hypothesis of a correlation change. A ==== shows that the proposed bootstrap test has a good size performance while the existing tests have serious size distortion for conditionally heteroscedastic samples and for serially correlated samples. The better size of the bootstrap test than the existing tests is achieved at the cost of some power loss in some cases."," and ==== presented that the correlation is instrumental in determining the risk as a main factor of the optimal portfolio selection. The correlation can be taken as constant or as time-varying with persistent dynamics in various ways. However, since the correlation tends to increase in times of financial crisis, the constancy of the correlation is not satisfied over time, see ====. Therefore, interest in testing constancy of the correlation is gradually increasing.====Several studies are conducted for testing constancy of correlation. ==== and ==== proposed a CUSUM type correlation constancy test in order to detect changes in correlation rather than changes in covariance. ==== demonstrated that their test based on the correlation change is generally more powerful than the test based on covariance change for the alternatives of correlation changes. ==== developed a method to estimate both the time and the number of multiple change points by applying the usual binary segmentation procedure to the CUSUM test of ====. ==== for single structural change tests, ==== for stationary vector autoregressive models against continuous change, ==== for unit root tests with nonstationary volatility and ==== is widely used, see ====, ====, ====, and many others. We apply the moving block bootstrap to the correlation break test of ==== and develop a bootstrap test.====. The critical values will be shown to be valid in that the CUSUM test and bootstrapped CUSUM test have the same limiting null distribution. Consistency of the bootstrap test will also be proved under an alternative hypothesis of a correlation break.====A Monte Carlo experiment reveals that the proposed bootstrap test has good size in conditionally heteroscedastic samples and in serially correlated samples, which resolves the over-size problem of the existing tests under conditional heteroscedasticity and/or under serial correlation. On the other hand, the bootstrap test has some worse power performance than the existing tests in a case of double breaks but not in some cases of a single break.====The proposed bootstrap test can improve the break time detection procedure of ==== which first detects a presence of break by the CUSUM test of ==== and next detects the time of break if any. In case of conditional heteroscedasticity and serial correlation, since the test of ==== is over-sized, the procedure of ==== would over-detect breaks and break times. This over-detection problem may be resolved if test of ==== in the first step of ==== is replaced by our bootstrap test.====The remaining of the paper is organized as follows. Section ==== describes the CUSUM test for correlation break and its asymptotic null distribution. Section ==== develops the bootstrap CUSUM test and establishes its asymptotic validity. Section ==== gives a conclusion.",Moving block bootstrapping for a CUSUM test for correlation change,https://www.sciencedirect.com/science/article/pii/S0167947319300301,5 February 2019,2019,Research Article,495.0
"Chaudhuri Arin,Hu Wenhao","Internet of Things, SAS Institute Inc., 500 SAS Campus Dr, Cary, NC 27513, USA","Received 25 October 2018, Revised 19 December 2018, Accepted 28 January 2019, Available online 5 February 2019, Version of Record 15 February 2019.",https://doi.org/10.1016/j.csda.2019.01.016,Cited by (37),", and Kendall’s ==== distance between the joint characteristic function and the product of marginal distributions; it is 0 if and only if two random vectors ==== and ==== cost, where ==== algorithms is developed. The proposed algorithm essentially consists of two sorting steps, so it is easy to implement. Empirical results show that the proposed algorithm is significantly faster than state-of-the-art methods. The algorithm’s speed will enable researchers to explore complicated dependence structures in large datasets.","Detecting dependencies between two random vectors ==== and ====, and Kendall’s ==== are used in almost all quantitative areas; example areas are bioinformatics (====, ====) and time-series (==== and ==== whose dependence cannot be detected by classical dependence measures. To overcome these limitations, ==== and ==== proposed distance covariance as a weighted ==== distance between the joint characteristic function and the product of marginal characteristic distributions. The distance covariance is ==== if and only if two random vectors ==== and ==== are independent. A closely related measure is the Hilbert–Schmidt independence measure (HSIC), which has been extensively studied in machine learning literature (====, ====, ====). Distance covariance is shown to be equivalent to HSIC in ====.====Despite the power of sample distance covariance to detect a dependence structure, its use for large sample sizes is inhibited by the high cost of the computation that is required. Computation of sample distance covariance and HSIC typically requires ==== memory for storing them. This is undesirable and greatly limits the use of distance correlation for large datasets. In the era of big data, it is not rare to see data that consist of millions of observations. For such data, an ==== and memory cost ====. Our proposed method essentially consists of just two sorting steps, which makes it easy to implement.====A closely related ==== algorithm for sample distance covariance was proposed in ====. Our algorithm differs from ==== in the following ways: First, ==== implicitly assumes that there are no ties in the data (see Algorithm 1 and proof in ==== and ==== requires 40 s. Because our implementation consists only of MATLAB code whereas the key step in the (====) routine is implemented in C, even greater speed increases are possible by rewriting the critical parts of our implementation in C.====The remainder of this paper is organized as follows. In Section ====, we briefly introduce the definition of distance covariance and its sample estimate. In Section ====, we describe the proposed ==== algorithm for sample distance covariance. In Section ====, experiment results are presented. Finally, conclusions and remarks are made in Section ====.",A fast algorithm for computing distance correlation,https://www.sciencedirect.com/science/article/pii/S0167947319300313,5 February 2019,2019,Research Article,496.0
"Derumigny Alexis,Fermanian Jean-David","CREST-ENSAE, 5, avenue Henry Le Chatelier, 91764 Palaiseau Cedex, France","Received 26 June 2018, Revised 25 January 2019, Accepted 26 January 2019, Available online 5 February 2019, Version of Record 19 February 2019.",https://doi.org/10.1016/j.csda.2019.01.013,Cited by (7),. The goal is to predict whether the pair is concordant (value of 1) or discordant (value of ====) conditionally on some ," and ====, for instance, and, as a reminder, some basic definitions in ====, ====, ====, ==== have popularized this approach, with a focus on conditional Kendall’s tau and Spearman’s rho. Note that conditional dependence measures have been invoked in different frameworks, often without any explicit link with conditional copulas: truncated data (e.g. ====), multivariate dynamic models (====, ==== among others), vine structures (====), etc.====Now, let us introduce our key dependence measure: for each ==== given some covariates ==== may be defined as ====where ==== and ==== are two independent versions of ====. To simplify, we will assume that the law of ==== given ====. This implies ====A conditional Kendall’s tau belongs to the interval ==== and reflects a positive (====) or negative (====) dependence between ==== and ====, given ====. Unlike correlations, this measure has the advantage of being always defined, even if some ====, ====Some estimators of conditional Kendall’s tau have already been proposed in the literature, either as a by-product of the estimation of conditional copulas – see ==== and ==== – or directly, as in ====, ====. Nonetheless, to the best of our knowledge, nobody has yet noticed the relationship between conditional Kendall’s tau and classification methods.====Let us explain this simple idea. Denote ==== and ====Actually, the prediction of concordance/discordance among pairs of observations ==== given ====), but applied here to pairs of observations.====Indeed, for every ====, ====, define ==== as ====A classification technique will allocate a given couple ==== into one of the two categories ==== (or “concordant versus discordant”, equivalently), with a certain probability, given the value of the common covariate ====.====Section ====. In Section ====, these techniques are applied to European stock market data. We evaluate to what extent the dependence between pairs of European stock indices may evolve with respect to different covariates. All proofs have been postponed into appendices.====Here, we recall the main concepts around copulas and conditional copulas. First, a ====-dimensional copula is a cdf on ==== whose margins are uniform distributions. Sklar’s theorem states that, for any ====-dimensional distributions ====, whose marginal cdfs’ are denoted as ====, there exists a copula ==== s.t. ====for every ====. If the law of ==== is continuous, the latter ==== is unique, and it is called ==== associated to ====. Inversely, for a given copula and some univariate cdfs’ ====, ====, Eq. ==== defines a ====-dimensional cdf ====.====The latter concept of copula is similarly related to any random vector ==== whose cdf is ====, and there is no ambiguity by using the same term. Copulas are invariant w.r.t. strictly increasing transforms of the margins ====, ====. They provide very practical tools for modeling complex and/or highly dimensional distributions in a flexible way, by splitting the task into two parts: the specification of the marginal distributions on one side, and the specification of the copula on the other side. Therefore, a copula can be seen as a function that describes the dependence between the components of ====, independently of the marginal distributions. Several popular dependence measures are functionals of the underlying copula only: Kendall’s tau, Spearman’s rho, Blomqvist coefficient, etc. The classical textbooks by ==== or ==== provide numerous and detailed results.====Numerous parametric families of copulas have been proposed in the literature: Gaussian, Student, Archimedean, Marshall–Olkin, extreme-value, etc. Several inference methods have been adapted to evaluate an underlying copula, possible without estimating the marginal cdfs’ (Canonical Maximum Likelihood). See ==== for details. Nonparametric methods have been developed too, since the seminal papers of ====, ==== about empirical copula processes.====Second, conditional copulas have been formally introduced by ====, ====. They are rather straightforward extensions of the latter concepts, when dealing with conditional distributions. Formally, for a given sigma-algebra ====, let ==== (resp. ====) be the conditional distribution of ==== (resp. ====, ====) given ====. The “conditional version of” Sklar’s theorem now states that there exists a random copula ==== s.t. ====for every ====. If the law of ==== is continuous, the latter ==== is unique, and it is called ==== associated to ====, given ====. Inversely, given ====, a conditional copula ==== and some univariate cdfs’ ====, ====, Eq. ==== defines a ====-dimensional conditional cdf ====. See ==== for extensions of the latter concepts.",A classification point-of-view about conditional Kendall’s tau,https://www.sciencedirect.com/science/article/pii/S0167947319300271,5 February 2019,2019,Research Article,497.0
"Zamanzade Elham,Parvardeh Afshin,Asadi Majid","Department of Statistics, University of Isfahan, Isfahan, 81744, Iran,School of Mathematics, Institute of Research in Fundamental Sciences (IPM), P.O Box 19395-5746, Tehran, Iran","Received 5 July 2017, Revised 10 January 2019, Accepted 12 January 2019, Available online 2 February 2019, Version of Record 16 February 2019.",https://doi.org/10.1016/j.csda.2019.01.005,Cited by (8)," ==== is no larger than its counterpart in the SRS design, regardless of the quality of ranking. Different methods of constructing a confidence interval for MRL in the RSS and SRS designs are then discussed. It is observed that while both the RSS and SRS-based confidence intervals do not control the nominal confidence level equally well, the RSS-based confidence intervals have generally shorter lengths than those in the SRS scheme. Finally, a potential application in the context of medical studies is presented for illustration purpose."," and a finite mean ====, then the MRL of ==== at age ==== is defined as ====for ==== such that ====. Note that ==== we have ====, for all ====. For the concept and some related theoretical results on MRL, we refer the reader to ====, ==== and ====. For applications of MRL in reliability engineering we refer, among others, to ====, ====, ==== and ====. For applications in survival analysis, one can refer to ====, ==== and ==== and references therein.====The problem of estimating the MRL, based on simple random sampling (SRS) design, has been well studied in the literature. For example, ==== considered the non-parametric estimation of MRL ==== on a finite interval. ==== generalized Yang’s (1978) results for a fixed ==== on an infinite interval. ==== introduced some estimators for MRL based on the estimation of the density function. ====The purpose of this paper is to estimate the MRL for the case where the data are collected in ranked set sampling (RSS). In recent decades, the RSS design draws much attention in the literature. RSS was originally proposed by ====, one draws a simple random sample with size ==== from the population of interest and ranks them from smallest to largest without actual measurement. One then selects the sample unit with judgement rank 1 for actual measurement. Again, he/she draws a simple random sample of size ==== and ranks them in increasing magnitude and obtains the sample unit with judgement rank 2 for actual quantification. This process is continued until a unit with judgement rank ==== from the ====th sample is obtained and measured. This process is repeated ==== times (cycles) if it is required to obtain a ranked set sample of size ====. In this paper, the term ==== is used to emphasize that ranking of sample units in RSS is done using any method which does not require obtaining precise values of sample units (e.g. visual inspection, concomitant variable), and therefore it is prone to error (imperfect ranking). The ranking process is called perfect if there is no error in the ranking of sample units.====Although the first motivation of RSS was in an agricultural problem, it has been applied in various fields since then, including forestry (====), environmental studies (====) and medicine (====; ====; ====). In recent years, substantial works have been reported in the literature based on RSS discussing many standard statistical problems. These include, among others, estimation of the cumulative distribution function (====; ====; ====; ====; ====), estimation of the density function (====), variance estimation (====; ====; ====; ====; ====), the two sample problems (====; ====), reliability estimation (====; ====), perfect ranking tests (====; ====; ====; ====; ====; ====; ====). For more information on RSS and its applications, we refer the reader to ==== and references therein.====The rest of the paper is organized as follows. In Section ====, some preliminary results on estimating MRL in the SRS and RSS designs are presented. Section ==== is devoted to the study of the finite sample performance of the proposed estimator of ==== includes an example from medical studies to illustrate the proposed method. Some concluding remarks are given in Section ====. The lengthy proofs of theoretical achievements are presented in the ====.",Estimation of mean residual life based on ranked set sampling,https://www.sciencedirect.com/science/article/pii/S0167947319300064,2 February 2019,2019,Research Article,498.0
"Chen Yichen,Datta Somnath","Department of Biostatistics, University of Florida, 2004 Mowry Road, 5th Floor CTRB, Gainesville, FL 32611, USA","Received 24 April 2018, Revised 22 January 2019, Accepted 23 January 2019, Available online 1 February 2019, Version of Record 14 February 2019.",https://doi.org/10.1016/j.csda.2019.01.012,Cited by (7),-statistics that can be used for comparing distribution of outcomes in two groups are considered. Adjustments to the classical ====-statistics under different censoring mechanisms. It is also demonstrated that large sample inferences for the adjusted ,") and Jonckheere–Terpstra (====), will be briefly discussed in this paper. The Kruskal–Wallis statistic can be used to test the null hypothesis that outcome distributions in ==== groups are equal against the alternative that at least one outcome distribution differs from the others. The alternative hypothesis for the Jonckheere–Terpstra test is that the outcome distribution in group ==== is stochastically greater than that in group ==== for all ====, i.e., outcome distributions in the ==== groups are increasingly ordered. One common feature of two-sample Wilcoxon, Kruskal–Wallis, and Jonckheere–Terpstra statistics is that all of their centering vectors can be considered as expectation functionals and ====-statistics have normal distribution provided the second moment of the kernel is finite.====If we consider comparing the distribution of a time-to-event outcome in two or more groups, we may consider using these ====-statistic type tests. Unfortunately, the estimators one gets from the ====-statistics are no longer unbiased for the corresponding population parameter due to right-censoring. A right-censored version of ====-statistics, was proposed by ====-statistics with a kernel of arbitrary order and finite second moment was established by the authors under some regularity assumptions.====-statistics have to be adjusted to ensure that the observed difference across the groups is not contributed by distributions of confounding covariates in different groups.====-statistics that account for confounding covariates were recently proposed by ====. Their approach is to re-weight each summand of a ====-statistic by propensity- or stratification-score based weights (referred as score-based weights in this paper). This extension of ====-statistics was motivated by standardizing the empirical cumulative distribution function (CDF) in each group, so that the distribution of confounding covariates is balanced in each group after the adjustment.====In this paper, we consider applying ====-statistics to analyze observational studies with a time-to-event outcome. Thus, we need to adjust both confounding covariates and right-censoring issues related to the classical ====-statistics. Confounding covariates are assumed to be fixed and completely observed for each individual. However we allow additional covariates, fixed or time varying, to affect the censoring mechanism. The main contribution of this paper is to thoroughly explore the performance of these adjustments in different censoring mechanisms of varying degrees of complexity in simulation studies. In turn, the results of simulation studies lead to useful recommendations for analyzing such data sets in practice.====The rest of the article is organized as follows. In Section ====, we propose confounding and censoring adjusted (CCA) ====-statistics for comparing two groups, and generalized Kruskal–Wallis statistic and Jonckheere–Terpstra statistic for comparing more than two groups. In Section ====, we illustrate the performance of the newly proposed ====.",Adjustments of multi-sample ,https://www.sciencedirect.com/science/article/pii/S016794731930026X,1 February 2019,2019,Research Article,499.0
"Li Haocheng,Shu Di,He Wenqing,Yi Grace Y.","Department of Mathematics and Statistics, University of Calgary, Canada,Department of Statistics and Actuarial Science, University of Waterloo, Canada,Department of Statistical and Actuarial Sciences, Western University, Canada","Received 2 February 2018, Revised 22 January 2019, Accepted 23 January 2019, Available online 31 January 2019, Version of Record 15 February 2019.",https://doi.org/10.1016/j.csda.2019.01.011,Cited by (0)," with multilevel structures are commonly collected when following up subjects in clusters over a period of time. Missing values and variable selection issues are common for such data. Biased results may be produced if incompleteness of data is ignored in the analysis. On the other hand, incorporating a large number of irrelevant ==== into inferential procedures may lead to difficulty in computation and interpretation. A unified penalized composite likelihood framework is developed to handle data with ==== and variable selection issues. It is flexible to handle the situation where responses and covariates are missing not simultaneously under the assumption of missing not at random. The method is justified both rigorously with theoretical results and numerically with simulation studies. The method is also applied to the Waterloo Smoking Prevention Project data.","). The project uses a questionnaire to investigate teenagers’ smoking behavior in ==== Canadian middle schools. In each school, three students are enrolled to consecutively answer the questionnaire at Grade 6, Grade 7 and Grade 8. The survey collects students’ smoking behavior information as well as their personality and social background measurements. A research interest focuses on the assessment of the influences of smoking control intervention and other related factors on smoking frequencies. However, the dataset involves missing observations in both outcomes and covariates, which complicates our analysis. To be specific, only ==== of the schools have all of their three students to provide complete observations and only ====As illustrated in ====, there are three students nested in a school. The students are followed from Grade 6 to Grade 8, but none of them provides complete information during this survey. According to ==== can be selected into the research model, but not all of them are predictive to the outcome.====The difficulties in this research lie on data modeling and parameter estimation. As we will show in Section ====, with the increasing number of students nested in schools and missing observations, existing maximum likelihood approaches (e.g. ====, ====, ====, ====, ====).====Our solution to this problem is motivated by the specific structure in missing data process. ==== displays the missing data rate for WSPP across schools and students in Grade 6, 7 and 8, respectively. Although the outcome and four of the covariates are subject to missingness in Grade 7 and 8, we find that the first visit, Grade 6, is missing-free for all variables. This missing data structure is caused by the requirement that all students should have a complete record at baseline, while the students are allowed to provide incomplete records in the following-up visits.====As we will display in Section ====, the proposed method does not involve the model specification and estimation for missing data process, and thus, it avoids the non-identifiability and model misspecification problems in MNAR settings. In addition, this approach will not include the integrals for incomplete observations in response and covariates, and the dimension of integrals for random effects in handling multilevel data structure is suppressed up to two. Therefore, it is computationally convenient. To further eliminate redundant covariates, we discuss a penalized composite likelihood framework to simultaneously estimate model parameters as well as select predictive covariates.====The remainder of this paper is organized as follows. In Section ====, we introduce notations and models. In Section ====. An application to the WSPP data is presented in Section ====. Concluding remarks are given in Section ====.====The following is the Supplementary material related to this article. ",Variable selection via the composite likelihood method for multilevel longitudinal data with missing responses and covariates,https://www.sciencedirect.com/science/article/pii/S0167947319300258,31 January 2019,2019,Research Article,500.0
"Wang Xia,Shojaie Ali,Zou Jian","Department of Mathematical Sciences, University of Cincinnati, Cincinnati, OH 45221, USA,Department of Biostatistics, University of Washington, Seattle, WA 98195, USA,Department of Mathematical Sciences, Worcester Polytechnic Institute, Worcester, MA 01609, USA","Received 19 October 2017, Revised 17 January 2019, Accepted 18 January 2019, Available online 29 January 2019, Version of Record 4 April 2019.",https://doi.org/10.1016/j.csda.2019.01.009,Cited by (8),An optimal and flexible multiple ==== is constructed for dependent data based on ==== and clusterwise analysis. Its performance is compared with existing approaches using both simulated and real data examples.,") using existing agnostic procedures, such as the step-down procedure of ====. Recent research, however, has showed that ignoring correlation often degrades statistical accuracy and can lead to high variability of testing results and hence irreproducibility of scientific findings (====, ====, ====, ====, ====, ====, ====, ==== and ====.====. The proposed models have broad applications in many other settings, including finance, marketing and neurosciences.====’s (====) approach for clusterwise inference. In practice, the fundamental units of interest for testing may be a cluster of units instead of individual units. For example, the policy makers may need to get inference regarding disease breakouts in regular spaced periods, such as monthly, quarterly, or yearly, to identify time periods that the public might be exposed to higher risks. Then testing on these clustered units is more appropriate than testing on the individual units separately (====, ====). The Bayesian framework proposed here allows straightforward calculation of error criteria on cluster specific false positive and negative rates as in ====.==== developed a full Bayesian procedure based on the hidden Markov tree model for Gene Ontology data. Recent development in ==== further proposed computationally efficient algorithms for their model. ====. Their approach has been further extended to deal with heterogeneous chromosome groups (====), two-dimensional graphical correlation studies (====, ====), and three-dimensional neuroimaging data (====).====. The proposed Bayesian algorithm is particularly advantageous when the non-null distribution is a mixture of normal distributions, instead of a single normal distribution: our simulation experiments confirm the previous findings that the results from the EM algorithm, such as the one used in ====, are sensitive to starting values, while the results from the Bayesian method are relatively robust to initialization.==== to alleviate the above problem.====Our second contribution is the development of a flexible and robust Bayesian nonparametric framework that alleviates the need to pre-specify the number of mixture components in the non-null distribution. ==== considered the setting where the number of mixture components in the non-null distribution is either known, or is decided using a model comparison criterion, such as BIC. More recently, ==== and ==== for its applications in estimating the non-null distribution, ==== for a latent random partition model for variable or model selection. The marriage between the Dirichlet process prior and the hidden Markov model has created a rich collection of models in many areas, see ==== for the infinite HMM using the hierarchical Dirichlet process (HDP), ==== for the sticky HDP-HMM, and ==== for Bayesian nonparametric hidden semi-Markov models with non-Markov dependence structure.====Our work extends the testing procedures in ==== by the Bayesian framework with two well established modeling components, HMM for modeling dependence and Dirichlet process priors for density estimation. It shows that similar or better performance can be achieved. More importantly, this extension reconstructs ==== and ====.====The rest of the paper is organized as follows. In Section ====, we describe our Bayesian hidden Markov model methods for multiple hypotheses testing. In Section ====, we introduce validity and optimality concepts in multiple testing and their extension under the Bayesian approach. We then describe the hidden Markov model (HMM) and its application to hypothesis testing in Section ====. The proposed Bayesian hierarchical models are presented in Section ====. We discuss three cases of the non-null distribution: a single distribution, a mixture distribution with a known number of components, and a mixture distribution with an unknown number of components. In Section ====, we establish the posterior consistency of the Bayesian HMM with a known number of components. Extensive simulation studies are presented in Section ====, where the proposed methods are compared with existing multiple hypotheses testing procedures. Both pointwise and clusterwise inference are discussed. Applications of the proposed methods to two real data applications are presented in Section ====. Section ==== concludes the paper with discussions and ongoing research. Additional details on the proposed Bayesian computation and posterior consistency are relegated to the Online Supplementary Materials.====The following is the Supplementary material related to this article. ====
 ====
 ",Bayesian hidden Markov models for dependent large-scale multiple testing,https://www.sciencedirect.com/science/article/pii/S0167947319300234,29 January 2019,2019,Research Article,501.0
"Mahmoud Hamdy F.F.,Kim Inyoung","Department of Statistics, Mathematics, and Insurance, Assiut University, Egypt,Department of Statistics, Virginia Polytechnic Institute and State University, Blacksburg, VA, USA","Received 19 June 2017, Revised 8 December 2018, Accepted 20 January 2019, Available online 28 January 2019, Version of Record 4 April 2019.",https://doi.org/10.1016/j.csda.2019.01.008,Cited by (4),", and nonparametric function. This model is not only to estimate this nonparametric relationship but also to incorporate spatial effects and other weather variables. It is useful when the spatial areas are located close to each other because the nonparametric function may not be separated from spatially correlated random effects. Based on the simulation study, the semiparametric integrated-spatial mixed effects single index model provides more accurate estimates of spatial correlation and prediction. The advantage of the semiparametric integrated-spatial mixed effects single index model is further demonstrated using mortality data of six cities in South Korea from January 2000 to December 2007.",The following is the Supplementary material related to this article. ,Semiparametric spatial mixed effects single index models,https://www.sciencedirect.com/science/article/pii/S0167947319300222,28 January 2019,2019,Research Article,502.0
"Yu Dengdeng,Zhang Li,Mizera Ivan,Jiang Bei,Kong Linglong","Department of Mathematical and Statistical Sciences, University of Alberta, Edmonton, Alberta, Canada, T6G2G1","Received 30 March 2018, Revised 5 August 2018, Accepted 6 December 2018, Available online 24 January 2019, Version of Record 4 April 2019.",https://doi.org/10.1016/j.csda.2018.12.002,Cited by (19),To study ==== (ADMM) which was recently employed by many researchers in solving penalized quantile regression problems. The ,", ====, ====where the coefficient ==== is a function, and ==== is a random error. To estimate the functional coefficient ====, ====), functional principal component basis (====, ====, ====, ====). Recently in imaging analysis, ====, ==== and ====The functional linear model ==== can be extended to a partial functional linear model with multiple functional covariates ====where covariates ==== are scalars and ==== are the coefficients. The functional coefficients ==== can be estimated by using regularization techniques. In particular, penalized principal component basis has been an especially popular choice (====, ====). Recently, ==== successfully applied such technique to model ==== in the setting of ultrahigh-dimensional scalar predictors.====, has been well developed and recognized in functional linear regression. Many of them are focusing on the functional linear quantile regression model: ====where ==== is the ====-th conditional quantile of response ==== given a functional covariate ==== for a fixed quantile level ====). To estimate the functional coefficient ====, functional basis can as well be used to approximate it; for instance, general basis like B-spline basis (====, ====), functional principle component basis (====, ====, ====) and partial quantile basis (====).====In this article, we extend model ==== to a partial functional linear quantile regression model with multiple functional covariates ====where ==== is the ====-th conditional quantile of ==== given scalar covariates ==== and multiple functions ====. To our best knowledge, only a few works have studied this model; for example, ==== used partial quantile basis while (====) used penalized principal component basis. Inspired by the success of wavelet basis with regularization in functional linear model (====, ====, ====), we use it to approximate the functional coefficients ==== in model ====The penalization we impose is sparse group lasso (====, ====There are five major contributions of this paper. First, our conditional quantile framework provides a more suitable modeling of reality especially when the response is heavy tailed (====). It is also a compelling choice of dealing with heteroscedasticity issues and can provide a more complete picture of the response (====The rest of paper is organized as follows. In Section ====, we review some necessary background on wavelets and provide the penalized quantile objective function with sparse group lasso penalty. The asymptotic properties such as the convergence rate and predictor error bound are established in Section ====. In Section ====, the quantile penalization problem is reformulated into a second-order cone program (SOCP) and solved by an interior point method by using a powerful R package: Rmosek. We also propose a novel algorithm using alternating direction method of multipliers (ADMM). Finite sample simulations and a real data from ADHD-200 fMRI data are investigated in Section ==== to illustrate the superiority of our proposed method.====See ====, ====, ====, ====, ====, ====.",Sparse wavelet estimation in quantile regression with multiple functional predictors,https://www.sciencedirect.com/science/article/pii/S0167947318302810,24 January 2019,2019,Research Article,503.0
"Golightly Andrew,Bradley Emma,Lowe Tom,Gillespie Colin S.","Newcastle University, Newcastle upon Tyne, UK,Hiscox Ltd., London, UK","Received 20 February 2018, Revised 9 January 2019, Accepted 12 January 2019, Available online 21 January 2019, Version of Record 4 April 2019.",https://doi.org/10.1016/j.csda.2019.01.006,Cited by (10)," of intermediate times between observations. Pseudo-marginal Metropolis–Hastings schemes are increasingly used, since for a given discretisation level, the ==== can be unbiasedly estimated using a particle filter. When observations are particularly informative, an auxiliary particle filter can be implemented, by propagating particles conditional on the next observation. Recent work in state-space settings has shown how the pseudo-marginal approach can be made much more efficient by correlating the underlying pseudo-random numbers used to form the estimate of likelihood at the current and proposed values of the unknown parameters. This approach is extended to the time-discretised SKM framework by correlating the innovations that drive the auxiliary particle filter. The resulting approach is found to offer substantial gains in efficiency over a standard implementation.",", ====, ====), population ecology (====, ====, ====, ====, ====, ====). A concise introduction to SKMs can be found in ====.====Whilst exact simulation of the MJP is straightforward (using for example the direct method of ====, ====, ====), population Monte Carlo (====) and particle MCMC (====, ====, ====, ====, ====). Our overall aim is the development of fully Bayesian inference schemes that are both computationally and statistically efficient.====When working with the time-discretised process, inference is still far from straightforward. Ensuring a desired level of accuracy requires the introduction of a pre-specified number of intermediate time-points between observations. Since the latent process at these time-points cannot be integrated out analytically, the observed data likelihood under the approximate model remains intractable. We therefore develop a particle MCMC scheme for performing fully Bayesian inference for either the Poisson leap or CLE and improve computational efficiency over a vanilla implementation in two ways. First, an auxiliary particle filter (====) is used to (unbiasedly) estimate the observed data likelihood. As shown by ====, this is crucial in avoiding highly variable likelihood estimates in scenarios where intrinsic stochasticity outweighs the error in the observation process. Essentially, particles are propagated conditional on future observations by using a suitable bridge construct. When using the Poisson leap, we propose to use the conditioned reaction hazard of ====. For the CLE, we use the modified diffusion bridge (MDB) of ==== (or the appropriate extension to incomplete observation described in ====). Finally, we make use of the recently proposed correlated pseudo-marginal algorithm (====, ====), which introduces positive correlation between successive likelihood estimates in order to reduce the variance of the acceptance ratio.====Our approach is to introduce correlation between the bridges generated by the auxiliary particle filter at iteration ==== and those generated at iteration ====. When using the CLE, this can be achieved by correlating the Gaussian innovations that drive the MDB. When using the Poisson leap, the numbers of reaction events conditional on the next observation can be used. A similar approach is described in ==== consider a Lotka–Volterra reaction network and calculate the observed data likelihood by averaging ==== ‘blocks’ of unbiased estimates (which can be computed in parallel). Correlation is introduced by only updating the likelihood in a randomly chosen block. This is the so-called blockwise pseudo-marginal method. Our contribution is a unified framework for applying a correlated pseudo-marginal algorithm to a general class of time-discretised stochastic kinetic models, that additionally allows a flexible observation regime. In particular, we consider incomplete observation of the model components as well as Gaussian measurement error. We apply the resulting methodology to four examples arising in systems biology and epidemiology, using both real and synthetic data.====The remainder of this paper is organised as follows. Section ==== gives a brief introduction to SKMs with particular attention to the derivation of the Poisson leap and CLE approximations. The inference algorithm is described in detail in Section ==== and applied to several examples in Section ====. Conclusions are drawn in Section ====.====We give a brief description of the modified innovation scheme (MIS) and refer the reader to ==== and the references therein for further details.====Consider the joint posterior of ==== and the latent process ==== under the CLE given by ====where ==== and ==== can be found in ====, ====. A Gibbs sampler can be used to generate draws from ==== by alternately sampling from the full conditionals====It is straightforward to sample ==== using Metropolis within Gibbs coupled with a suitable blocking approach. For example, the latent process can be updated over each interval ====, ==== with the modified diffusion bridge construct in ==== used as the proposal mechanism. The use of overlapping blocks in this way ensures that latent process is updated at the observation times (as well as at all other intermediate times). The full conditional ==== can be sampled via Metropolis within Gibbs however for small values of ====, dependence between the parameters and latent process can render this approach impractical. This well known problem is discussed at length in ====. The issue is circumvented by the MIS via a reparameterisation. The basic idea is to draw parameter values conditional on a process whose quadratic variation does not determine ====. For example, for a time interval ====, conditioning on the innovations that drive the modified diffusion bridge construct leads to the continuous-time innovation process ==== where ==== and ====. A justification for conditioning on realisations of this process in a Gibbs sampler can be found in ====. In practice, we work with a discretisation of ====, that is, the modified diffusion bridge construct. For the induced invertible mapping ==== (where we have suppressed dependence of ==== on ==== and the values of the latent process at the observation times), the full conditional density required in step 2 is easily shown to be ====where ==== is given by ==== and ====is the Jacobian determinant of ====. Naturally, the full conditional in ==== will typically be intractable, requiring the use of Metropolis-within-Gibbs updates. We propose to update ==== using random walk Metropolis with Gaussian innovations.",Correlated pseudo-marginal schemes for time-discretised stochastic kinetic models,https://www.sciencedirect.com/science/article/pii/S0167947319300076,21 January 2019,2019,Research Article,504.0
"Chakraborty Sounak,Lozano Aurelie C.","209F Middlebush Hall, Columbia, MO, 65211, United States,Thomas J. Watson Research Center, Yorktown Heights, NY USA","Received 27 December 2017, Revised 3 January 2019, Accepted 5 January 2019, Available online 14 January 2019, Version of Record 4 April 2019.",https://doi.org/10.1016/j.csda.2019.01.003,Cited by (10)," (MAP) estimate which simultaneously allows for variable selection, coefficient estimation and predictive group identification. The connections between the GL-prior (graph Laplacian) and the existing regularized ",", ====, ====, ====, ====, ====, ====, ====, ====, ====, ==== and ====. These approaches utilize the “spike and slab” prior formulation, where the coefficients are modeled independently using mixture distributions with point masses at ====. Bayesian shrinkage priors, which are connected to the variable selection approaches via penalization such as the Lasso penalty (====) have also been recently proposed by ====, ====, ====, ====, ====, ==== and ====. Under the shrinkage prior formulations, the penalties in the penalized approaches are formulated with special choices of prior distributions, and the ====, ====, ====, ====, ====). All these methods assume either independence ====, ====). As reported in ====, ==== and ====. However, in the paper (====) we had to heavily rely to some thresh holding methods to do the variable selection. Therefore it (====) was only designed to work well for small dimension cases. In contrast to that our current article is targeted for high dimensional large datasets with many covariates.==== (EM) algorithm, using an augmented Lagrangian approach (see ====The rest of this article is organized as follows. Section ==== describes the computational algorithms. Section ==== provides the results from empirical evaluation on simulated data, and Section ==== discusses the analysis of the Arabidopsis thaliana microarray data. We conclude in Section ==== with a discussion. Details of proofs and computations are relegated to the Appendices.==== Proof of ====.==== Proof of ====.==== Proof of ==== Proof of ====.==== Proof of ====.",A graph Laplacian prior for Bayesian variable selection and grouping,https://www.sciencedirect.com/science/article/pii/S0167947319300040,14 January 2019,2019,Research Article,505.0
"Xiu Xianchao,Liu Wanquan,Li Ling,Kong Lingchen","State Key Lab for Turbulence and Complex Systems, Department of Mechanics and Engineering Science, College of Engineering, Peking University, China,Department of Applied Mathematics, Beijing Jiaotong University, Beijing, PR China,Department of Computing, Curtin University, Perth, WA, Australia","Received 5 June 2017, Revised 19 November 2018, Accepted 4 January 2019, Available online 11 January 2019, Version of Record 4 April 2019.",https://doi.org/10.1016/j.csda.2019.01.002,Cited by (8),It is well-known that the fused ," and the observation ====, i.e., ====where ==== is an error vector which is assumed to follow the normal distribution ==== with mean ==== and standard deviation ====.====However, when the number of variables is much larger than the number of observations, i.e., ====, the classical statistical methods cannot work efficiently. To perform variable selection and model fitting, ====where ==== is defined as the sum of absolute values of all entries, and ==== is a tuning parameter.====One drawback of ==== is the fact that it ignores ordering relationship among the coefficients. For this purpose, ==== considered the fused least absolute shrinkage and selection operator (FLASSO) model: ====where ==== and ====, image denoising in ====, and so on. When ====, problem ==== degenerates into problem ====. We would also like to point out that when ====, problem ==== degenerates into the total variation problem in ====, ====. The total variation is widely used in image processing because it is able to improve the smoothness of boundary and trajectory that are usually the most important for image recovery, see ==== for example.====Recently, the nonconvex penalty has been drawn more and more attention in sparse learning problems, because it is more efficient to extract the essential features of solutions than the ====-norm. At the same time, the nonconvex penalty has the nearly unbiased property, and could overcome the shortcomings of the ====-norm as discussed in ====. An interesting question naturally comes out to us: can we extend the FLASSO model ==== in the nonconvex framework? Therefore, in this paper, we will consider the following model: ====where ==== is the nonconvex penalty functions with parameters ==== and ====, see Section ====. As this model is motivated by the FLASSO with nonconvex penalty functions, we call it the nonconvex fused regression in this paper.====Our contributions in this paper can be summarized as the following three parts:====The remaining of this paper is organized as follows. Some related work and preliminaries will be reviewed in Sections ====, ====. In Section ====, extensive experiments will be conducted to substantiate the superiority of the proposed model over the other existing one. This paper will be concluded in Section ====.",Alternating direction method of multipliers for nonconvex fused regression problems,https://www.sciencedirect.com/science/article/pii/S0167947319300039,11 January 2019,2019,Research Article,506.0
"Lee Keunbaik,Joo Yongsung","Department of Statistics, Sungkyunkwan University, Seoul, 03063, South Korea,Department of Statistics, Dongguk University-Seoul, Seoul, 04620, South Korea","Received 11 October 2017, Revised 21 December 2018, Accepted 2 January 2019, Available online 11 January 2019, Version of Record 4 April 2019.",https://doi.org/10.1016/j.csda.2019.01.001,Cited by (0),"In this paper, we propose two marginalized models for longitudinal count data. The first marginalized model has a Markovian structure to account for the ==== show that the proposed models perform well in the sense of reducing the bias of marginal mean parameters compared to the misspecification of the dependence model in these models. The models are used to draw inferences from a previously analyzed trial on ====.","). The study was a randomized, double-blind, parallel group multi-center study designed to compare placebo to a new anti-epileptic drug (AED), in combination with one or two other AEDs. Longitudinal outcomes were obtained weekly from 89 subjects who experienced epileptic seizures. The maximum number of replications was 27 and it was a long-series of longitudinal count data. The main objective of this study was to identify an improved treatment for reducing the number of epileptic seizures. Since epileptic seizures were repeatedly measured in the same subject, the repeated outcomes were correlated (====’s (==== proposed a first-order Markov transition model with a random intercept for longitudinal Poisson data. The Markov structure and random intercept were used in order to consider the possible influence of previous counts and to capture baseline heterogeneity across subjects, respectively. ==== reviewed generalized linear mixed models with normal random effects and demonstrated that such models accommodate overdispersion. ====. Additionally, similar to ====, the random effects in our proposed models account for the overdispersion of the count data.====, ====). Each marginalized model has two submodels: marginal mean and dependence models. In the marginal mean submodel, a regression mean structure exists in order to explain the population-averaged effects of covariates on a response. In the dependence submodel, correlation among the outcomes is explained via random effects (====, ====, ====, ====, ====) or a Markov structure (====, ====, ==== combined random effects and Markov dependence in order to analyze long series of longitudinal binary data. ==== extended the work of ====, ====, ====, ====, ====, ====).====In this paper, we analyze longitudinal count data using marginalized models. ====). Therefore, we propose Poisson marginalized models with correct serial correlation structures: Poisson marginalized transition models (PMTMs) and Poisson marginalized transition random effects models (PMTREMs). The PMTMs have a Markovian structure for short longitudinal Poisson data. The PMTREMs have both the Markovian structure and random effects to accommodate long-range longitudinal count data. Additionally, the random effects in the PMTREMs describe the overdispersion of longitudinal count data. Therefore, our proposed model simultaneously encompasses overdispersion and long-range dependence correlation between repeated outcomes.====The rest of this paper is arranged as follows. In Section ====, we propose marginalized transition models for longitudinal Poisson data. We also extend the models in order to accommodate longitudinal overdispersed count data using random effects. In Section ====, we use these models to draw inferences from the epilepsy study. Finally, a summary is included in Section ====.",Marginalized models for longitudinal count data,https://www.sciencedirect.com/science/article/pii/S0167947319300027,11 January 2019,2019,Research Article,507.0
"Huang Lei,Jiang Hui,Wang Huixia","School of Mathematics, Southwest Jiaotong University, China,School of Mathematics and Statistics, Huazhong University of Science and Technology, China","Received 22 November 2017, Revised 12 December 2018, Accepted 26 December 2018, Available online 8 January 2019, Version of Record 14 February 2019.",https://doi.org/10.1016/j.csda.2018.12.012,Cited by (6),"Partial-linear single-index models have been widely studied and applied, but their current applications to ==== still need some strong and inappropriate assumptions. A novel method which relaxes those assumptions is proposed. It extends the applicability of partial-linear single-index models to time series modeling, taking both lag variables and autocorrelated errors into consideration. An estimation procedure based on Whittle likelihood is proposed and some asymptotical properties of the corresponding estimators are derived. In addition, some simulation studies are conducted to elaborate that the proposed model is necessary in certain situations. The proposed models are also shown to be useful and reasonable in real data analysis, indicating the feasibility and practicability of the proposed estimation method.","As a compromise between classical linear models and nonparametric regression models, partial linear models have attracted a lot of attention due to their flexibility. Such works include ====, ==== and ==== result in reliable estimators. To fix this problem, the partial-linear single-index models as below have been proposed (====, ====, ====), ====where they assume ====, ==== means almost surely hereafter. Model ==== could achieve the goal of dimension reduction , due to its assumption that the influence of covariates ==== is compressed into a single index ==== with nonparametric link function ==== .====Some popular regression models could also be described by model ====. When the dimension of ==== is one, model ==== turns out to be a partial-linear model; when covariates ==== in the linear part are absent, model ==== becomes the famous single index model. There is a large amount of literature on single index models, see for examples, ====, ====, ====, ====, ==== and ==== are continuous, whereas model ==== relaxes this assumption by adding ====, i.e. allowing some discrete(or continuous) variables to have linear effects on ====. Besides, existing studies of model ==== and the aforementioned works on single index models do not relax assumption ====This paper aims to propose a novel model based on ====, ====, ====, ==== and ====. Note that all these models could be generalized as ====where ==== is the ==== observation, ==== is the innovation. ==== represent the lag variables, and ==== is the maximum order of lag. ==== to be serially correlated, otherwise the mean of residuals conditional on explanatory variables can hardly satisfy ====where ==== represents all the information provided by explanatory variables at time ====. On one hand, if the conditional mean-zero assumption ==== failed, any estimators of ==== approximately, models as ==== often need a large number of auto-regressors ==== (large ====. For a simple example ARMA(1,1), although AR(====) model with large ==== can fit an ARMA(1,1) process approximately, it is widely known that ARMA(1,1) is more efficient than AR(p) when the time series is indeed generated from ARMA(1,1) process.====Therefore, when analyzing time series data using nonlinear models as ====, we should take into account the case that errors ==== are autocorrelated. Otherwise, large number of lag variables have to be considered to make condition ====, ====, ==== and ====. Their models form like ====where ==== are explanatory variables, and ==== are autocorrelated errors. However, in order to satisfy the conditional mean-zero assumption of ====, models as ==== in all those aforementioned works never include lag variable ==== at all, which limits their applications to time series modeling.====Recently, some nonlinear time series models with autocorrelated errors have been investigated by ====, ==== or improvement in forecast. With a different focus in this article, we will study the partial-linear single-index model ==== with lag variables as predictors and autocorrelated errors simultaneously. Additionally, in the estimation procedure, we will adopt spline approach to estimate the unknown index function. Spline methods for single index models are popular in literature, ==== used P-spline method to estimate the coefficients by assuming the index function falling in a finite-dimensional spline space; ==== employed B-spline to estimate the coefficients by a two stage method. However, their works have all imposed the aforementioned assumption ====, limiting their applications to time series modeling as discussed earlier.====The paper is organized as follows. Section ====. Section ==== introduces some practical techniques and conducts some statistical simulation results, showing the rationality and feasibility of our proposed model and method. An example of real data analysis will be shown in Section ====. Finally, Section ==== concludes this paper with some discussions.",A novel partial-linear single-index model for time series data,https://www.sciencedirect.com/science/article/pii/S0167947318302913,8 January 2019,2019,Research Article,508.0
"Cevallos-Valdiviezo Holger,Van Aelst Stefan","ESPOL Polytechnic University, Escuela Superior Politécnica del Litoral, ESPOL, (Facultad de Ciencias Naturales y Matemáticas, FCNM), Campus Gustavo Galindo Km. 30.5 Vía Perimetral, P.O. Box 09-01-5863, Guayaquil, Ecuador,Ghent University, Department of Applied Mathematics, Computer Science and Statistics, Krijgslaan 281 S9, 9000 Gent, Belgium,KU Leuven, Department of Mathematics, Celestijnenlaan 200B, 3001 Leuven, Belgium","Received 7 June 2018, Revised 28 December 2018, Accepted 28 December 2018, Available online 6 January 2019, Version of Record 14 February 2019.",https://doi.org/10.1016/j.csda.2018.12.013,Cited by (6),"Dimension reduction is often an important step in the analysis of high-dimensional data. PCA is a popular technique to find the best low-dimensional approximation of high-dimensional data. However, classical PCA is very sensitive to atypical data. Robust methods to estimate the low-dimensional subspace that best approximates the regular data have been proposed. However, for high-dimensional data these algorithms become computationally expensive. Alternative algorithms for the robust subspace estimators are proposed that are better suited to compute the solution for high-dimensional problems. The main ingredients of the new algorithms are twofold. First, the principal directions of the subspace are estimated directly by iterating the first order solutions corresponding to the estimators. Second, to reduce the computation time even further five robust deterministic values are proposed to initialize the algorithms instead of using random starting values. It is shown that the new algorithms yield robust solutions and the computation time is largely reduced, especially for high-dimensional data.",", ====, ====, ====, ==== introduced spherical PCA which uses the covariance matrix of the data projected onto the unit sphere and is fast to compute.==== and ====, ====.====Instead of looking for one direction at a time as in PP, one can robustly estimate a lower-dimensional subspace directly (see e.g. ====, ====). ROBPCA (====) seeks for a lower-dimensional subspace via a multiple-step procedure. Briefly, the first step aims to identify a subset of ====subset and the optimal dimension of the subspace is determined. Next, the data points are projected onto the lower-dimensional subspace. Finally, the reweighted MCD estimator is computed on the projected data to obtain the estimates for the principal directions of the subspace.====Principal Component Pursuit method (PCP) aims to decompose the data matrix into a low-rank component and a gross outlier component (====). See e.g. ==== and ====, ====). Therefore, ==== introduced a robust orthogonal complement (ROCPCA) approach to deal with orthogonal complement outliers. The authors model the projected data onto the orthogonal complement subspace with a mean term, a sparse outlier matrix and a noise term. The sparse outlier matrix identifies orthogonal outliers. More recently, ==== combined ideas from PCP and ROCPCA to construct a reinforced robust PCP (RRPCP) method that considers outliers in both the observation space and in the orthogonal complement subspace. RRPCP decomposes the data matrix into a low-rank component, a sparse outlier matrix to represent outliers in the observation space, a noise term, a mean term and another sparse outlier matrix to represent outliers in the orthogonal complement space. The two latter components are represented in the orthogonal complement space and then transformed back to the observation space. However, according to their model formulation ROCPCA and RRPCP have to estimate the directions of the orthogonal complement space, which becomes computationally intensive for high-dimensional data.==== proposed to robustly estimate the best lower-dimensional approximation by minimizing either an M-scale or a least trimmed squares (LTS) scale of the Euclidean distances corresponding to the observations. He also characterized the solutions by the orthogonal complement directions and showed that these directions correspond to the eigenvectors associated with the smallest eigenvalues of a weighted covariance matrix. Based on this characterization ====, ==== and ==== for the M-scale estimator, and ====, ==== and ==== for the LTS-scale estimator. Moreover, ==== also provides a thorough theoretical study of the properties of the estimator based on the LTS scale.====However, in case of a low-dimensional approximation for high-dimensional data Maronna’s orthogonal complement algorithm requires to decompose a high dimensional covariance matrix and needs a large number of its eigenvectors to characterize the solution. This makes computing the subspace estimators time consuming or even infeasible in high dimensions. Therefore, we propose an algorithm for the robust subspace estimators of ==== that directly calculates principal directions of the low-dimensional subspace.====The main ingredients of our new algorithm are twofold. First, we use the first order conditions corresponding to the estimator to update the principal directions of the subspace iteratively. This approach only requires low-dimensional vector and matrix operations rather than manipulating high-dimensional covariance matrices. Second, instead of using random starting values, similarly to ==== we propose five robust deterministic values to initialize the algorithm. These starting values yield robust fits that are usually close to the sought after robust solution, so that convergence occurs quickly.====The remainder of the manuscript is organized as follows. Section ==== reviews the robust subspace estimators based on an M-scale or the LTS-scale. Our definition is equivalent to the definition in ==== but characterizes the solution by the principal directions of the subspace which better serves our needs for the development of the new algorithm in Section ====. In Section ==== we introduce the robust deterministic values to initialize the algorithm. In Section ====, while in Section ==== we extend the simulation study in Section ==== to higher-dimensions. Section ==== contains a real data illustration and Section ==== presents our final conclusions.",Fast computation of robust subspace estimators,https://www.sciencedirect.com/science/article/pii/S0167947319300015,6 January 2019,2019,Research Article,509.0
"Agostinelli Claudio,Valdora Marina,Yohai Victor J.","Department of Mathematics, University of Trento, Trento, Italy,Departamento de Matematicas and Instituto de Cálculo, Facultad de Ciencias Exactas y Naturales, University of Buenos Aires, Argentina,Departamento de Matematicas and Institituto de Cálculo, Facultad de Ciencias Exactas y Naturales, University of Buenos Aires, CONICET, Argentina","Received 24 February 2018, Revised 18 December 2018, Accepted 18 December 2018, Available online 4 January 2019, Version of Record 14 February 2019.",https://doi.org/10.1016/j.csda.2018.12.010,Cited by (2),". The proposed method is applied to M-estimators based on transformations. In addition, an iteratively reweighted ==== algorithm is proposed for the computation of the final estimates. The new methods are studied by means of Monte Carlo experiments.",", ====, ====, ====, ==== and ====). Stabilizing the variance allows the correct scaling of the loss function used in the definition of M-estimators.====Consider a GLM in which ==== is the response and ==== is a ====where ==== is an unknown vector of parameters and ==== is a known link function. We further assume that ====where ==== with a density of the form ====for given functions ==== and ====.====Let ==== be a variance stabilizing transformation and ==== be a function such that:====(1) ==== is a non-decreasing function of ====,====(2) ====,====(3) there exists ==== such that ==== is strictly increasing in ==== and ==== is constant in ====.====MT-estimators are defined as ====where ====and ==== is the function defined by ====where ==== denotes the expectation of ==== when ====. It is assumed that ==== is univocally defined, therefore ==== implies the Fisher consistency of ====. The solution to ==== can be found by iterative methods which typically solve the corresponding system of estimating equations ====where ==== is the derivative with respect to ==== of ====. In ====. The difficulty in the case of redescending M-estimators is that the objective function ==== may have several local minima. As a consequence, the iterative procedure may converge to a solution to Eq. ====. To avoid this, one must begin the iterative algorithm at an initial estimator which is a very good approximation of the global minimum of ====, i.e. the solution of ====. If ====). Based on the algorithm described in ==== for linear models, this method consists in computing a finite set ==== of candidate solutions of ==== and then replaces the minimization over ==== by a minimization over ====. The set ==== is obtained by randomly drawing subsamples of size ==== and then computing the maximum likelihood (ML) estimator based on the subsample. If the original sample contains a proportion ==== and the probability of having at least one subsample free of outliers is ====, where ==== is the number of subsamples drawn. If we want this probability to be greater than a given ====, we must draw a number of subsamples such that ====that is to say, ====This makes the algorithm infeasible for large ====. ==== studied this problem in the case of linear models, introducing an alternative method to compute the set of candidate solutions ====. Their proposal succeeds in obtaining a set ==== which contains very good approximations of the actual solution and, on the other hand, requires the computation of a small number of subsamples, namely ====. This makes the algorithm much faster and feasible even for very large values of ====.====We modify the method introduced by ==== in order to apply it to generalized linear models. We study its application to MT-estimators by means of an extensive Monte-Carlo study, which shows that the method is very fast and robust for large values of ====.====As a particular case of the MT-estimator we define the Least Squares estimator based on Transformations (LST), which corresponds to ====, in the following way ==== is not bounded, they are, in general, non robust.====The paper is organized as follows. In Section ==== we describe the proposed procedure in detail. In Section ==== we describe the simulation study used to compare the proposed procedure with existing methods and give a summary of its result. In Section ==== we apply the proposed method to a real data example and compare the results to those obtained by other methods. In Section ==== we summarize our conclusions. In ==== we provide an iteratively reweighted least squares algorithm to find the solution to the optimization problems ====, ====In this Appendix we describe the iteratively reweighted least squares algorithms that were used to compute the LST and the MT estimators. Suppose that we have an initial estimator ==== and call ====. Then, using a Taylor expansion of order one we can approximate ==== by ====Then, an approximate value to the LST estimator can be found as the value ==== that minimizes ====Therefore, ==== is the LS estimator for a linear model with responses ==== and regressor vectors ==== and consequently ====where ==== is the ==== matrix whose ====th row is ====, ====, ==== is the diagonal matrix with diagonal elements ==== and ====.====An iterative procedure to compute the LST estimator can be obtained by ====Iterations will continue until ====, where ==== is the error tolerance.====Suppose that ==== converges to ====; then this value should satisfy the LST estimating equation. In fact, taking limit in both sides of ==== we get ====which is equivalent to ====Then, ==== satisfies the estimating equation of the LST estimator.====To start the algorithm, it will be convenient to write Eq. ==== in the following, slightly different, way ====Observe that, according to ====, to compute ==== we only need to give ====. Then, since for Poisson regression and log link, ====, it seems reasonable to take ==== The value 0.1 is added to avoid numerical problem when ====. To compute the LST estimators in our procedure, only one iteration is performed. The reason is that, for these auxiliary estimators, the accuracy is not as important as the speed at which they can be computed. Our experiments show that there is no noticeable loss in the precision of the final estimate by doing this but, on the other hand, the computation times decrease significantly.====We describe now an analogous iterative algorithm for computing the MT estimator. Suppose that we have an initial robust estimator ====. We compute a new value using two approximations. As in the case of the LST estimator, replacing, in ====, ==== by ==== we consider the approximate loss function ====Differentiating with respect to ==== we obtain the estimating equation ====where ====. Note that this equation can be written as ====where ====Since ==== should be close to ====, the second approximation is to replace, in ====, ==== by ====. Then ==== is defined as the solution to the approximate estimating equation: ====and is given by ====where ==== is the ==== diagonal matrix with diagonal elements ====.====Then, the iterative procedure to compute the MT estimator is given by ====Suppose that ==== then taking limits in both sides of ====, we get ====and this is equivalent to ====where ====. Then ==== satisfies the estimating equation of the MT estimator.",Initial robust estimation in generalized linear models,https://www.sciencedirect.com/science/article/pii/S0167947318302895,4 January 2019,2019,Research Article,510.0
"Yoshida Takuma,Naito Kanta","Graduate School of Science and Engineering, Kagoshima University, Kagoshima 890-8580, Japan,Graduate School of Science and Engineering, Shimane University, Matsue 690-8504, Japan","Received 18 July 2018, Revised 9 November 2018, Accepted 22 December 2018, Available online 28 December 2018, Version of Record 14 February 2019.",https://doi.org/10.1016/j.csda.2018.12.011,Cited by (0),This paper studies a curve estimation based on empirical ,", ====, ====) and projection pursuit regression (PPR; ====, ====). These methods were clarified in ====.====, ====, ==== and ==== and ==== developed the stagewise estimators and their non-asymptotic error bounds in density estimation. Our approach is posited as the regression version of ==== and ====.====This paper is organized as follows. Section ==== introduces the risk function and provides an example. In Section ==== shows the non-asymptotic error bound of the proposed stagewise estimator. In Section ====. Section ==== concludes the paper. The ==== presents the proofs of the theorems and lemmas used in Section ====. The ==== provides the information of supplementary file.",Regression with stagewise minimization on risk function,https://www.sciencedirect.com/science/article/pii/S0167947318302901,28 December 2018,2018,Research Article,511.0
"Rodrigues T.,Dortet-Bernadet J.-L.,Fan Y.","School of Mathematics and Statistics, University of New South Wales, Sydney 2052, Australia,Departamento de Estatistica, Universidade de Brasilia, 70910-900, Brasília, Brazil,Institut de Recherche Mathématique Avancée, UMR 7501 CNRS, Université de Strasbourg, Strasbourg, France","Received 11 December 2017, Revised 14 December 2018, Accepted 17 December 2018, Available online 28 December 2018, Version of Record 14 February 2019.",https://doi.org/10.1016/j.csda.2018.12.009,Cited by (5), when compared to existing alternative approaches. The method is illustrated with three real applications.,", ====, ====, ====). For modelling the ====-th conditional quantile curve of a response variable ==== given the value ==== of some covariate ==== as ====, ====, consider ====where the error variable ==== satisfies ====, and the function ==== changes with ==== and describes the relationship between ==== and ====. In this article, we are interested in the case where the curve ==== is modelled with spline functions of a given degree, ====, so that ====where ==== and where ====, represent the locations of ==== knot points (see ====). Typically, the degree ====where ==== is an asymmetric loss function given by ==== if ==== and ==== otherwise. In the spline fitting context, a penalty term is added to ==== in order to restrict the class of functions ==== to be sufficiently smooth. However, the simplicity of linear programming is jeopardised when the classical quadratic penalty, ====, is added to this objective function (====). ==== advocate the use of a linear norm under the total variation roughness penalty, ====, nevertheless one becomes harshly delimited by the space of piecewise linear fits. On the other hand, under the scope of linear programming, extension to quadratic splines is still possible when using the ====-norm, ====, see ==== ==== or ====. Smoothing cubic splines can be easily incorporated considering a prior density for ==== proportional to ==== (see ====).====However, one common issue facing these approaches is that quantiles at different levels have to be fitted singly for each ==== and final estimates may cross, i.e. estimates may not respect the monotonicity of the quantile function. In the context of nonparametric regression, the flexibility granted to the quantile curves makes crossing more severe. Postprocessing procedures (e.g. ====, ====) which correct for crossing still suffer from a poor borrowing of information, and can still lead to wildly variable curves across quantile levels ====. ====), ====, ====, (====). Indeed simultaneous quantile fitting is a challenging problem that has gained a lot of attention in recent times. A solution to the constrained minimisation problem was proposed by ====, however estimation is again limited to piecewise linear fits. One can also achieve noncrossing by restricting the class of models to the location-scale family (====), or by minimising the objective function sequentially while imposing ordering through the parameters (====), but there are naturally limitations when rigid assumptions are imposed. More recently, ====Pyramid quantile regression was first presented in ==== as an alternative Bayesian procedure for simultaneous quantile fitting. This approach compared favourably with competing methods in empirical studies. It relies on the use of several so-called quantile pyramid priors introduced by ====The rest of the article is organised as follows. Section ==== presents the optimal convex set enclosing the data cloud with the given number of vertices. Section ==== recalls some basics on quantile pyramids introduced by ==== and briefly describe their use in ==== in a regression set-up. In Section ==== we introduce the penalised quantile splines model and describe the modelling set up. Section ==== presents simulation studies and comparisons with the best alternative approaches. Several real datasets are analysed in Section ====, and concluding remarks are discussed in the final section.",Simultaneous fitting of Bayesian penalised quantile splines,https://www.sciencedirect.com/science/article/pii/S0167947318302883,28 December 2018,2018,Research Article,512.0
"Singh Rakhi,Mukhopadhyay Siuli","IITB-Monash Research Academy, Mumbai 400 076, India,Department of Mathematics, Indian Institute of Technology Bombay, Mumbai 400 076, India","Received 17 May 2018, Revised 30 November 2018, Accepted 16 December 2018, Available online 26 December 2018, Version of Record 14 February 2019.",https://doi.org/10.1016/j.csda.2018.12.008,Cited by (3),"Exact D-optimal ==== designs for time series experiments are discussed in this article. This work is motivated by an RNA sequencing experiment and two disease surveillance studies, where the response is count type and has a correlated structure over ==== are studied. An estimating approach based on only the first two moments of the responses is used for parameter estimation. The D-optimality criterion based on minimization of the log determinant of the variance–covariance matrix of the parameter estimates is used for choosing the exact designs. To address the dependency of the design selection criterion on the unknown parameter values, prior distributions are assumed on the parameters. From the numerical results, it is noted that for linear predictors with only trend component, the optimal design is very close to the equispaced design for high correlation values. However, when the linear predictor has both the trend and seasonal components, the two designs are similar for smaller correlations.","In this article, we propose robust exact optimal designs for time series experiments with count type responses. This work is motivated specifically by an RNA sequencing experiment for studying the gene profile of the organism ====), or the number of airline accidents measured over some period of time for a certain airline, and its relationship to the economic health of the airline (====), or the study of annual number of bank failures recorded in a certain period (====) or number of live births over a specified age interval of the mother (====). In such studies, the data are usually correlated over time, concentrated at a few small discrete values, left skewed, with variance increasing with the mean (====, ====, ====, ==== and ====. In contrast, parameter-driven models make use of an unobserved latent process to account for the correlation. Conditioning on the latent process, the responses are assumed to be independent of the past history of the response process. Parameter-driven models were first considered by ====. Later, these models were also investigated ====, by ====, ====, ====, and  ====.====In this article, we use the parameter-driven model similar to that of ====The main reasons for choosing the parameter-driven approach over the observation-driven models are, (a) we are able to determine the marginal distribution of the count responses and fit a log-linear model to the marginal mean, while this is not possible in the observation-driven setup as we have only the ==== of the responses given the past information, (b) also the correlation and the possible over dispersion present in the count responses are accounted for when selecting the designs by our approach. While in the observation-driven approach the responses conditioned on the past are assumed to be independent, thus the design criterion based on the variance–covariance matrix of the regression coefficients is unable to take into account the correlated structure of the responses.====, ====, ====, ====, ====, ====, ==== and ====. For selecting a design we use the D-optimality criterion. As in GLM designs for count responses (====, ====, ====, ====, ====, ====, ====, ==== and ====.====In the last few years, there has been an increasing interest in determining exact optimal designs for experiments with correlated measurements. Some of the recent papers on this topic are namely, ====, ====, ====, ====, ==== and ====). Similarly, continuous disease surveillance is a labor intensive and expensive endeavor (====). Thus, while designing such experiments, we look at a relatively small number of observations over time.",Exact Bayesian designs for count time series,https://www.sciencedirect.com/science/article/pii/S0167947318302871,26 December 2018,2018,Research Article,513.0
"Morris Katherine,Punzo Antonio,McNicholas Paul D.,Browne Ryan P.","Department of Mathematics & Statistics, University of Guelph, Ontario, Canada,Department of Economics and Business, University of Catania, Italy,Department of Mathematics & Statistics, McMaster University, Ontario, Canada,Department of Statistics and Actuarial Science, University of Waterloo, Ontario, Canada","Received 21 September 2017, Revised 8 November 2018, Accepted 5 December 2018, Available online 24 December 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.csda.2018.12.001,Cited by (20),", adding a flexibility to our approach that is absent from other approaches such as trimming. Moreover, each observation is given an ","-dimensional random vector ====where ==== is the mixing proportion, so that ====, and ==== is the pdf for the ====th component, ====.==== and ====). If a component pdf in the mixture is associated with a cluster, as we assume in this paper (see ==== Section 9.1 for a wider discussion on this point), then normal mixture components imply elliptically symmetric (or elliptically contoured) clusters, which is rather restrictive. Other examples of mixtures implying elliptically symmetric clusters are those with multivariate ==== (====, ====, ====), multivariate power exponential (====, ====), and multivariate leptokurtic-normal (====) components. One way to overcome such a restrictiveness is to argue that asymmetric clusters can be approximated quite well by a mixture of several elliptically symmetric densities such as the normal (====, ====, ====). While this can be very helpful for density estimation purposes, it might be misleading when dealing with clustering applications because one group may be represented by more than one component just because it has, in fact, an asymmetric density (see ==== for an interesting example). One possible approach to dealing with asymmetric groups consists of considering transformations so as to make the components as elliptical and symmetric as possible, and then fitting mixtures of elliptically symmetric (usually normal) distributions (====, ====, ====), multivariate shifted asymmetric Laplace (SAL) distributions (====), and other approaches (e.g., ====, ====). The mixture of SAL distributions approach has the advantage of simplifying the computational effort required by the EM algorithm to fit the model.====However, real data, in addition to being characterized by underlying asymmetric clusters, are often “contaminated” by outliers or otherwise “bad” points. The use of “bad” in this sense is by analogy with ====, and refers to points that have a deleterious effect on parameter estimation, including the mixing proportions, as discussed, for example, in ====. Thus an important practical application is the development of methods capable of detecting bad points and performing robust parameter estimation when they are present. Using the nomenclature of ====, and choosing a convenient mixture model as a reference for the good data, there are two main approaches to cope with the latter robustness issue (see also ====). In the “additional component” approach, protection against outliers is obtained by adding a further convenient component distribution to the reference mixture to capture outliers. In the “component-wise” approach, on which this paper focuses, the component distributions of the reference mixture are separately protected against outliers by embedding them in more general heavy-tailed distributions. Examples in this direction are: mixtures of multivariate skew-==== distributions (see, e.g., ====, ====, ====, ====, ====, and ====, ====), mixtures of multivariate ====-distributions with the Box–Cox transformation (====, ====, ====), mixtures of multivariate skew-slash distributions (====), mixtures of multivariate generalized hyperbolic distributions (====), scale mixtures of multivariate skew-normal distributions (====, ====), mixtures of variance-gamma distributions (====), mixtures of hidden truncation hyperbolic distributions (====), and mixtures of joint generalized hyperbolic distributions (====To overcome this drawback, in Section ==== we propose to “contaminate” the components (clusters) of the multivariate SAL mixture to accommodate outliers and to allow for their automatic detection (see Section ==== to define mixtures of multivariate contaminated normal distributions. For other models involving the contaminated normal distribution see ====, ==== and ====, while for the contamination of univariate skewed distributions see ==== and ====. According to our contamination scheme, the unimodality of the cluster distribution is preserved (====, ====, ====) and a single cluster is most naturally characterized by a unimodal pdf (see ==== for further discussion). Indeed, as highlighted in ==== and ==== and ====, referring to bimodality rather than to mixtures of two distributions.====After a contaminated SAL (CSAL) mixture is fitted to the available data, each observation can be first assigned to one of the clusters, by means of maximum ====. Moreover, bad points are automatically down-weighted in the estimation of the parameters of the nested SAL mixture. Thus, we have a model for simultaneous robust clustering, in the presence of asymmetric clusters, and detection of bad points. Note that the mixture of multivariate skew-contaminated normal (SCN) distributions, introduced by ====, is capable of coping with skewed clusters under the occurrence of outliers that, eventually, can be also automatically detected (even if the authors do not refer to this possibility). However, although each SCN distribution is defined as a mixture of two SN distributions, the good and bad SN components of this mixture have different modes (see ==== and ==== for details). In this regard note that, although the mode of the SN distribution is unique (==== and ====), there is no analytic expression for it (==== p. 140). Hence, the SCN mixture cannot be considered as fitting within our contamination scheme. For maximum likelihood parameter estimation of the CSAL mixture proposed herein, an expectation–conditional maximization (ECM) algorithm (====) is developed (Section ====. Section ==== investigates the performance of our mixture, in comparison with mixtures of some well-established multivariate elliptically countered and skewed distributions, on artificial and real data. Section ==== provides the conclusion and suggestions for future work.====The following is the Supplementary material related to this article. ",Asymmetric clusters and outliers: Mixtures of multivariate contaminated shifted asymmetric Laplace distributions,https://www.sciencedirect.com/science/article/pii/S0167947318302809,April 2019,2019,Research Article,514.0
Geraci Marco,"Arnold School of Public Health, Department of Epidemiology and Biostatistics, University of South Carolina, Columbia SC, USA","Received 14 January 2018, Revised 12 December 2018, Accepted 12 December 2018, Available online 21 December 2018, Version of Record 4 April 2019.",https://doi.org/10.1016/j.csda.2018.12.005,Cited by (17)," problem. While the estimation algorithm is iterative, the objective function to be optimized has a simple analytic form. The proposed methods are assessed through a simulation study and two applications, one in ==== and one related to growth curve modelling in agriculture.","). There is an increasingly wider acknowledgement of the importance of investigating sources of heterogeneity to quantify more accurately costs, benefits, and effectiveness of interventions or medical treatments, whether it be an after-school physical activity program, a health care reform, or a thrombolytic therapy (see, for example, ====, ====, ====, ====, ====, ====, ====). QR is particularly suitable for this purpose as it yields inferences that are valid regardless of the true underlying distribution. Also, quantiles enjoy a number of properties (====), including equivariance to monotone transformations and robustness to outliers.==== on methods for cross-sectional observations, there have been a number of proposals on how to accommodate for the dependency induced by cluster designs (e.g., longitudinal). As outlined by ==== and then extensively reviewed by ====, approaches to linear QR with clustered data can be classified into distribution-free and likelihood-based approaches. The former include fixed effects (====, ====, ====, ====) and weighted (====, ====) approaches. The latter are mainly based on the asymmetric Laplace (AL) density (====, ====, ====, ====, ====). A different classification can be made into approaches that include cluster-specific effects (e.g., ====, ====) and those that ignore or remove them (====, ====, ====).====) and growth curve modelling (====, ====). Contributions to statistical methods for nonlinear mean regression when data are clustered can be found in the literature of mixed-effects modelling (====, ====, ====) as well as generalized estimating equations (====, ====, ====, ====). In contrast, the statistical literature on parametric nonlinear QR functions with clustered data is somewhat sparse. To our knowledge, there seem to be only a handful of published articles. ====) with pre-specified weights. ====, taking her cue from ====’s (====. Finally, ==== established the consistency of the ====-norm nonlinear quantile estimator under weak dependency.====Here, we propose an extension of ====’s (====). In contrast, we will provide an analytic form of the objective function to be optimized. What is more important, in the proposal by ====). On the other hand, while ====’s (====) approach is frequentist, it does not allow for inference at the cluster level. Moreover, our model has the ‘quantile’ interpretation conditionally on the random effects, whereas ====’s (====) model gives a (weighted) estimate of the marginal (with respect to the clusters) quantiles.====, ====) are discussed further on.====In Section ====, we briefly introduce the standard linear QR model and outline the LQMM approach. In Section ====, we introduce the nonlinear quantile mixed-effects model, or nonlinear quantile mixed model (NLQMM) for short, and related inference. In Section ====, we carry out a simulation study to assess the statistical and computational performance of the proposed methods. Since there are no alternative models that can be placed in a direct comparison with ours for the reasons given above, we consider nonlinear QR for cross-sectional data to investigate potential gains in efficiency when intra-cluster correlation is accounted for. In Section ====, we consider an application of NLQMM to pharmacokinetics and growth curves modelling. We conclude with some remarks in Section ====.====The estimation algorithm for NLQMM is based on a set of decreasing values of ====. This optimization approach has the appealing advantage of reducing the original nonsmooth problem to an approximated ==== problem. The pseudo-code is given below. ",Modelling and estimation of nonlinear quantile regression with clustered data,https://www.sciencedirect.com/science/article/pii/S0167947318302846,21 December 2018,2018,Research Article,515.0
"Song Junmo,Baek Changryong","Department of Statistics, Kyungpook National University, 80 Daehakro, Bukgu, Daegu, 41566, Republic of Korea,Department of Statistics, Sungkyunkwan University, 25-2, Sungkyunkwan-ro, Jongno-gu, Seoul, 110-745, Republic of Korea","Received 11 April 2018, Revised 13 December 2018, Accepted 15 December 2018, Available online 21 December 2018, Version of Record 14 February 2019.",https://doi.org/10.1016/j.csda.2018.12.007,Cited by (6),"This paper considers the detection of structural changes in realized volatility based on HAR–GARCH models. For this, we propose a quasi-likelihood based score test for parameter changes in HAR–GARCH models. We derive the limiting null distribution of the score test by first introducing the quasi-maximum likelihood estimator to the HAR–GARCH model and establishing its ",", ====, ==== and references +therein.==== suggested reduced form models based on a popular long memory process, namely a fractionally integrated autoregressive moving average (FARIMA) model. Alternatively, ====However, many researchers often question long memory models for volatility modeling. The major argument is that the long memory property can be spurious by neglecting structural or parameter changes. Particularly in finance, a financial crisis or high-risk bubbles can significantly contribute to structural changes in the underlying dynamics of financial market and lead to strong serial dependence. In this regard, many studies, such as those by ====, ==== and ==== dealt with the long memory phenomenon in terms of structural or parameter changes. In addition, ==== and ====, among others, studied test procedures to distinguish structural breaks and long memory. For an overview on long memory and structural changes, we refer to ====.====Meanwhile, several authors considered both approaches together; that is, a long memory model allowing for parameter changes. ====, ====In this study, inspired by the work of ==== proposed Fisher-score change process to detect change points in distribution functions. ==== and ==== recently extended the test, calling it the Z-process method, to general statistical models. In previous studies, such as by ====.====The paper proceeds as follows. In Section ====. Section ==== illustrates a real data application to the S&P 500 RV from 2005 to 2016. Section ==== concludes. Section ==== provides the technical proofs.",Detecting structural breaks in realized volatility,https://www.sciencedirect.com/science/article/pii/S016794731830286X,21 December 2018,2018,Research Article,516.0
"De Canditiis Daniela,De Feis Italia","Istituto per le Applicazioni del Calcolo “M. Picone”, Rome, Italy,Istituto per le Applicazioni del Calcolo “M. Picone”, Naples, Italy","Received 9 April 2018, Revised 23 November 2018, Accepted 29 November 2018, Available online 10 December 2018, Version of Record 14 February 2019.",https://doi.org/10.1016/j.csda.2018.11.003,Cited by (2)," is performed by means of the grouped lasso penalty. Furthermore, a result of asymptotic ","This paper deals with the problem of simultaneously recovering ==== different signals independently or simultaneously recorded under the hypothesis that these signals share common characteristics. Indeed, when drawing ====, ====, ==== and ====; but common characteristics are also expected in the medical field to model special EEG data, where one waits the simultaneous signals derived from the electrodes located in the subject’s scalp at specific areas, see ====, ==== and ====. See ====, ====, ====), in the signal and image processing community as the multi-channel recovering problem (====), which is a modern and fast computational tool for analyzing a very general class of signals. In statistical high-dimensional data analysis it is established that the grouped-Lasso technique (====The remainder of the paper is organized as follows. Section ==== describes the data model we are considering with the working hypothesis. Section ==== presents and discusses the inference procedure within the paradigm of group-lasso procedures, enlightening the connections with other existing procedures. Section ==== provides convergence results, while Section ==== shows numerical experiments.====Before proving ====, let us present some preliminary results.====For each ====, define the random variables ====where ==== is the ====th column of matrix ==== and ==== the ====th column of matrix ====.====For each ====, define the random variables ====being a matrix of dimension ==== with ==== the ====th column of matrix ====.",Simultaneous nonparametric regression in RADWT dictionaries,https://www.sciencedirect.com/science/article/pii/S0167947318302792,10 December 2018,2018,Research Article,517.0
"Yue Lili,Li Gaorong,Lian Heng,Wan Xiang","College of Applied Sciences, Beijing University of Technology, Beijing 100124, PR China,Beijing Institute for Scientific and Engineering Computing, Beijing University of Technology, Beijing 100124, PR China,Department of Mathematics, City University of Hong Kong, Hong Kong,Shenzhen Research Institute of Big Data, Shenzhen 518172, PR China","Received 26 April 2018, Revised 23 October 2018, Accepted 22 November 2018, Available online 4 December 2018, Version of Record 14 February 2019.",https://doi.org/10.1016/j.csda.2018.11.002,Cited by (9),None,"As pointed out by ====, and ====,  ==== and ==== under what is presently called as Neyman or Neyman–Rubin (Rubin causal) model. For each individual in the study, the model only assumes that there are a pair of potential outcomes under the possibilities of receiving the treatment or not. More details can be found in ====, and ====, ====, ====, ====, among others).====, and ====).====In this paper, we show that the regression adjustment to randomized experiments works under the high-dimensional collinearity data. We frame our analysis in terms of the Rubin causal model and follow the notations introduced in ====, ====, and ====. Let us index the units in the population of size ==== by ====, denote the random treatment indicator by ====, with ==== for a treated individual and ==== otherwise. For each individual in the study, there are a pair of potential outcomes ==== and ====, but there is only one observed potential outcome for each individual ====, that is ====It can also be written as ==== for the treatment on individual ====
 cannot be observed in theory. Instead of trying to infer causal effects of individual-level, the researchers assume that the intention is to estimate ATE defined by ==== over the whole population, which relies on a superpopulation perspective. In most randomized experiments, the sample is not considered as taken at random from the population (superpopulation) of interest. Hence, we focus on the average treatment effect in the finite sample, rather than on the population at large. In this paper, we focus on the estimation for SATE of ==== and ==== as follows: ====where ==== is the average response if all subjects received treatment, ==== is the average response for the control group, and ==== and ==== are fixed. ==== showed that a simple mean difference between the treatment and control groups can be used to estimate SATE defined in ====, and showed that the estimator is statistically unbiased. However, this estimator does not use the information of covariates when there exist covariates in Rubin causal model. When the number of covariates is smaller than the sample size, ====. Subsequently, ==== considered the interaction effect of the treatment indicator and covariates, and proposed an interaction regression adjustment estimator. When the dimension of covariates is larger than the sample size, ====, ==== suggested that one can use a standard high-dimensional estimation method, such as Lasso (====), to obtain estimators of treatment effects. ==== proposed a risk-consistent regression adjustment approach for population-average treatment effect based on the Lasso penalty. ==== proposed a Lasso-based adjusted estimator for SATE in ====, ====), weighted fusion (====), among others.====, which provides much better performances in applications. By our simulation studies, we find that the proposed method can reduce the variances with shorter confidence interval lengths compared to both the unadjusted and the Lasso-based adjusted estimation methods. A real data analysis illustrates a similar effect.====The rest of this article is organized as follows. In Section ==== to assess the effectiveness of our proposed method. The proposed estimation approach is then applied to a real data example in Section ====. Proof of the theorems is given in Section ====, and some concluding remarks are presented in Section ====. Some lemmas for the proof of main theorems are provided in the ====.==== For the sake of description, we first introduce some notations as follows. For any column vector ====, we denote ====, ====, ====, and ====. For a matrix ====, ====, ==== and ====, respectively. For a vector ==== and a subset ====, ==== for ====, ==== is the complement of ====, and ==== is the cardinality of the set ====. The notation “====This section provides some lemmas that are needed for the proofs of ====, ====, ====, ====. Since similar analysis can be adopted to both treatment and control groups, we will focus some theories on the treatment group ====.",Regression adjustment for treatment effect with multicollinearity in high dimensions,https://www.sciencedirect.com/science/article/pii/S0167947318302780,4 December 2018,2018,Research Article,518.0
"Qiu Zhiping,Peng Limin,Manatunga Amita,Guo Ying","Department of Biostatistics and Bioinformatics, Emory University, Atlanta, USA,Research Center of Applied Statistics and Big Data, School of Statistics, Huaqiao University, Quanzhou, China","Received 19 March 2018, Revised 8 November 2018, Accepted 14 November 2018, Available online 4 December 2018, Version of Record 14 February 2019.",https://doi.org/10.1016/j.csda.2018.11.001,Cited by (2),"The problem of determining cut-points of a continuous scale according to an established ==== is often encountered in practice for the purposes such as making diagnosis or treatment recommendation, determining study eligibility, or facilitating interpretations. A general analytic framework was recently proposed for assessing optimal cut-points defined based on some pre-specified criteria. However, the implementation of the existing nonparametric estimators under this framework and the associated inferences can be computationally intensive when more than a few cut-points need to be determined. To address this important issue, a smoothing-based modification of the current method is proposed and is found to substantially improve the computational speed as well as the asymptotic convergence rate. Moreover, a plug-in type variance estimation procedure is developed to further facilitate the computation. Extensive simulation studies confirm the theoretical results and demonstrate the computational benefits of the proposed method. The practical utility of the new approach is illustrated by an application to a mental health study.","), the 20-item self-rating Zung Depression scale (====) was administered to aid in the assessment of depression severity in diabetes subjects. Compared with clinician-administered instruments, Zung has the advantage of being less time consuming and more cost effective; however there is no well-established cut-points of Zung to reflect the degree of severity of depression (e.g. no/mild depression, moderate depression and severe depression). Having such cut-points for Zung will facilitate identifying depression subjects for large populations.====); using cut-points that yield disease rates consistent with a known population disease prevalence, or the highest proportions of correct classification based on a gold standard (====, ====). Another popular approach is to utilize the receiver operating characteristic (ROC) curve (====) in conjunction with various accuracy measures, such as Youden index (====, ====, ====, ====, ====, ====, ====) and the point closest-to-(0, 1) corner method (====), among others. Dong et al. ====, ====, ====), or by maximizing a distance measure between the differentiated two groups (====, ====, ====, ====, ====).====More recently, ==== proposed a general nonparametric analytic framework for defining and assessing optimal cut-points based on a pre-specified criterion, which can be an association or agreement measure, or an accuracy measure attached to ROC (e.g. Youden index). In particular, ==== derived rigorous nonstandard theory and developed appropriate inferential procedures for their proposed cut-point identification.====Though ====’s framework is broad and theoretically sound, the implementation of their method may involve heavy computation, which can limit its practical applications. The issue is that ==== proposed to evaluate the empirical objective function at all possible sets of cut-points, the number of which is roughly of order ====, where ==== denotes the sample size, and ==== corresponds to the number of cut-points that need to be determined. This means the computation loan increases significantly when ==== becomes large. Additionally, the currently available inference procedure is built upon resampling, which further increases the computational demand.====. In literature, the smooth approximation techniques have been used in various contexts (====, ====, ====, ====, ==== for example). In this work, the basic idea is to replace the non-smooth objective function of ==== times faster when ==== with appropriately selected bandwidth and local distribution function, while the original estimator proposed in ==== only achieves the cubic-==== convergence rate. This result reflects a rather significant efficiency gain resulted from using the new nonparametric approach. In addition, we derive a plug-in type variance estimator for the estimated cut-points. This is shown to further expedite the computation.====The rest of paper is organized as follows. In Section ====, we first review the framework of ==== for defining and estimating optimal cut-points and then present the proposed nonparametric estimator of the optimal cut-points. In Section ====, we present our asymptotic studies and develop inference procedures, including covariance estimation as well as the selection of the local distribution function and bandwidth parameter involved in the proposed estimation procedure. We conduct extensive numerical studies, including Monte-Carlo simulations and an application to the Diabetes and Depression study, which are presented in Section ====. Concluding comments are given in Section ====.====We first introduce necessary notation and present the specific form of ====. Given two matrices ==== and ====, write the Kronecker product of ==== and ==== as ====, which is equal to ====and has dimension, ====. When the matrices ==== and ==== have the same number of columns (i.e. ====), the Khatri–Rao product (====) is defined as ====, which is the ====-by-==== columnwise Kronecker product ==== and ====. For ====, define ====where for ====, ====Let ====
 denote the ==== identity matrix and a ==== matrix ====For ====, let ====and for any ====, define ====and ==== where ==== denotes the ====th element of ====, ==== and ==== are, respectively, the first-order derivative and second-order derivative of the conditional distribution function of ==== given ====, ====.====Next we present the proofs of ====, ====.",A smooth nonparametric approach to determining cut-points of a continuous scale,https://www.sciencedirect.com/science/article/pii/S0167947318302779,4 December 2018,2018,Research Article,519.0
"Shi Guiling,Lim Chae Young,Maiti Tapabrata","Department of Statistics and Probability, Michigan State University, East Lansing, MI, USA,Department of Statistics, Seoul National University, Seoul, Republic of Korea","Received 20 June 2017, Revised 14 August 2018, Accepted 10 October 2018, Available online 9 November 2018, Version of Record 16 February 2019.",https://doi.org/10.1016/j.csda.2018.10.007,Cited by (8), and to ====. The selection based on a nonlocal prior eliminates unnecessary variables and recommends a simple model. The method is validated by simulation study and illustrated by a real data example.,"), Bayes information criterion (====), Mallows ====) based on ==== and ==== norm penalties. Smoothly clipped absolute deviation (SCAD) (====) uses nonconcave penalty, which leads oracle property. Adaptive LASSO (====) using adaptive weights for penalizing different coefficients in ==== norm shows consistency under some conditions. Dantzig selector (====) also shows efficient convergence. Previous literature on variable selection in logistic regression used criterion based methods such as AIC or BIC. However, with some approximation technique, penalized methods can also be applied in generalized linear regression (====, ====.====) using Laplacian shrinkage, Bayesian model average technique (====), spike and slab prior (====) with a mixture prior assumed for each variable, stochastic search variable selection (====, ====, ====).====. ==== considered the logistic regression in which prior under some conditions leads to consistent convergence toward the true model. Then, the theory was extended for generalized linear models in ====. ====. Most of these Bayesian methods put a large mass on the density of a null parameter value to reach the result of shrinkage and do the variable selection which is referred as model selection based on local prior densities by ====.====In this paper we are interested in nonlocal prior (====). Nonlocal prior density is a density function that is identically zero whenever a model parameter is equal to its null value, typically zero in model selection settings. Most current Bayesian model selection procedures employ local prior density, which is positive at null parameter values so that posterior density for the model parameter is not necessarily zero. On the other hand, nonlocal prior density would be zero if any component of parameters is zero and this property could bring a parsimonious model selection.====Bayesian model selection procedures for linear regression model by imposing a nonlocal prior density is proposed by ====. We specifically find the convergence rate for a logistic regression model with nonlocal prior density. The numerical results are promising.====The rest of the paper is organized as follows: The Section ==== discusses the proposed methodology. The Section ==== discusses the algorithm for implementation and Section ==== provides the numerical results along with a real data example. The proofs are given in the ====.==== provides general conditions for the prior to give a convergence rate of the probability regarding the Hellinger distance between the posterior model and the true model. We check the conditions are satisfied in our setting. In particular, it is enough to show conditions (O) and (N) of ==== are satisfied. Condition (O) limits the tail densities of prior and Condition (N) defines the prior density on an approximation neighborhood.====In the following description of Condition (O), ==== is the same as the candidate dimension ====. To be consistent with the notation with ====, we use ==== to denote ====.",Bayesian model selection for generalized linear models using non-local priors,https://www.sciencedirect.com/science/article/pii/S0167947318302524,May 2019,2019,Research Article,520.0
"Lennon Hannah,Yuan Jingsong","School of Mathematics, University of Manchester, Oxford Road, Manchester, M13 9PL, UK","Received 2 November 2017, Revised 24 October 2018, Accepted 30 October 2018, Available online 7 November 2018, Version of Record 16 February 2019.",https://doi.org/10.1016/j.csda.2018.10.015,Cited by (9),Dependence modelling of integer-valued ,"). One particular approach is to use the binomial thinning operator, beginning with the work of ==== and ==== for the INAR(1) model. The INARMA(====, ====) model has similarities to the ARMA model and its estimation has been considered by ==== using MCMC.====Let ==== be an integer-valued stationary time series that is observed at ====. Such a time series ==== can be obtained by simply taking the integer parts of a continuous-valued stationary time series ====, or by more elaborate digitisation as follows. Let ==== be a discrete distribution function and ==== be the distribution function of ====. It is well-known that ==== and ==== has distribution given by ==== to ==== is a standard method of simulating from ==== (====).====It is much easier to specify a model for the latent process ==== because it is continuous-valued. The distribution of the discrete-valued series ==== then follows. The Gaussian ARMA====). Therefore it provides the basis of a fairly general model that can be written as follows: ====
 where ==== is an unknown distribution function of the discrete type, and ====. It is assumed that the innovation ==== for some ==== such that ====. The ARMA coefficients ==== and ==== only takes values in ==== and ==== are assumed to be known.====This model has a system equation ==== for the latent ARMA process ==== and an observation equation ==== for ==== in terms of ====. For models that utilise a latent process in different ways, we refer to ==== and the references therein.====It is easy to see that the transformation from ==== to ==== is not invertible: ====for any integer ====. In other words, each probable value of ==== corresponds to an interval for the value of ==== follows from that of ==== which we assume.====The likelihood of the observations ==== is ==== where ==== (====). This is difficult to compute when ==== is moderately large due to the ==== number of ====-dimensional normal distribution function evaluations involved.==== is not observed. The EM (Expectation Maximisation) algorithm is designed for such a situation (====). It has proved very popular and useful, see ==== for a summary of applications among 1700 publications.====The complete data ==== has the Gaussian ARMA likelihood and established algorithms can be used for its computation. Let the complete data log-likelihood be ==== and the incomplete data log-likelihood ==== where ====. The EM algorithm works on an estimate of the former, yet it can be shown that the latter log-likelihood improves through the iterations, i.e., ====Furthermore, if ==== converges then it converges to a local stationary point of ==== (====).====The EM algorithm alternates between two stages, the E-step (Expectation) and the M-step (Maximisation), where the parameter values are updated repeatedly until a convergence criterion is met. More specifically, the E-step at iteration ==== evaluates the objective function ====as an estimate of ==== using ==== to obtain an updated set of parameter values ====.====For the digitised Gaussian ARMA model, the second moments of the latent process ==== given the observed count data ==== is used in the M-step. We implement a Geweke–Hajivassilou–Keane (GHK) simulator (====, ====, ====) to generate samples recursively.====This paper is organised as follows. We derive the E-step of the EM algorithm for model ====–==== in Section ====. A Monte Carlo E-step is described in ==== with further details of simulation in ====. A brief mention of the M-step is in ====. We consider standard errors of the parameter estimates in ====. This is followed by practical considerations in ==== and a summary of the estimation method in ====. Examples using real and simulated data are provided in Section ====. We conclude in Section ==== with a discussion of the strengths and limitations of this MCEM approach to maximum likelihood estimation of the dependence parameters.",Estimation of a digitised Gaussian ARMA model by Monte Carlo Expectation Maximisation,https://www.sciencedirect.com/science/article/pii/S0167947318302767,May 2019,2019,Research Article,521.0
"Wang Kangning,Li Shaomin,Sun Xiaofei,Lin Lu","School of Statistics, Shandong Technology and Business University, Yantai, China,Guanghua School of Management, Peking University, Beijing, China,Institute for Financial Studies, Shandong University, Jinan, China","Received 23 October 2017, Revised 6 October 2018, Accepted 12 October 2018, Available online 7 November 2018, Version of Record 16 February 2019.",https://doi.org/10.1016/j.csda.2018.10.010,Cited by (8),"Modal regression is a good alternative of the mean regression, because of its merits of both robustness and high inference efficiency. This paper is concerned with modal regression based ",", ====, ====, ==== and ==== be the ====th observation of the ====th subject, where ==== is response variable, ==== is a ==== is a ====-vector of covariates, and assume that index variable ==== without loss of generality. We consider the semivarying coefficient models for this kind of data, which is given by ====where ==== are smooth but unknown functions.====A major aspect of longitudinal data is the within-subject correlation, and ignoring the correlation may cause a loss of efficiency. This motivated ====, ====, ====, ====, ==== and so on.====How to construct confidence regions for parameters is an important issue. A convenient choice is to use the asymptotic normal distribution. However, with this method, a plug-in estimator of the limiting variance is needed. The empirical likelihood (EL, ==== and ====, ====, ==== and ==== all considered the EL for the semivarying coefficient models. Furthermore, many EL based methods for the longitudinal data have been proposed, an incomplete list of the recent results include ====, ====, ====, ====, ====, ====, ====, ====, ====, ==== and ====.==== pointed out that the EL confidence regions may be greatly lengthened in the direction of the outliers. In longitudinal data, one outlier in the subject level may generate a set of outliers due to repeated measurements. Hence, robustness is very important in longitudinal studies.====Recently, there is a huge literature devoted to constructing robust GEE and EL, e.g., ====, ====, ====, ====, ====, ==== and ====Although the Huber’s score function is robust, it has limitation in terms of efficiency. To address this issue, ==== and ==== investigated a new modal regression estimation procedure. Specially, for the linear regression model ====, modal regression estimate the parameters by maximizing ====where ====, ==== is a kernel density function and ==== is a bandwidth, determining the degree of robustness and efficiency. Obviously, maximizing the objective function ==== is equivalent to solve the following estimating equations ====where ==== is the first derivative of ====. In contrast to other estimation methods, modal regression treats ==== as a loss function, ==== and ==== showed that, since modal regression can estimate the “most likely” conditional values, it can provide more robust and efficient estimation than other existing methods by choosing an appropriate bandwidth ====. Similar conclusions have been further confirmed in ====, ==== and ====.==== is proposed through constructing robust modal regression auxiliary random vectors. (iii) Our new modal regression based GEE and EL all can incorporate the working correlation matrix automatically to interpret the correlations within the subjects.====), adaptive Lasso (====), SCAD (==== developed a new variable selection procedure called the smooth-threshold estimating equations. Recently, ====, ==== and ====As the second goal of this paper, a new smooth-threshold modal regression based GEE variable selection procedure for the longitudinal data semivarying coefficient models is proposed, it can select the nonparametric and parametric parts simultaneously. Theoretically, the variable selection procedure works beautifully, including consistency in variable selection and oracle property in estimation. By inheriting the properties of the proposed modal regression based GEE, the new variable selection procedure has good robustness and efficiency, and can incorporate the correlation structure of longitudinal data.====The outline of this paper is as follows. Section ==== introduces the modal regression based GEE. Section ==== gives the modal regression based EL. The smooth-threshold modal regression based GEE variable selection procedure is introduced in Section ====. Concluding remarks are given in Section ====. All the proofs are provided in the Appendix.====This lemma is also used in ====. Let ====, so by ====, ==== holds.====By Taylor expansion and similar arguments in the proof of Lemma ==== of Qin etal (2012), ==== can be obtained.","Modal regression statistical inference for longitudinal data semivarying coefficient models: Generalized estimating equations, empirical likelihood and variable selection",https://www.sciencedirect.com/science/article/pii/S016794731830255X,May 2019,2019,Research Article,522.0
"Yu Xue,Zhao Yichuan","Department of Mathematics and Statistics, Georgia State University, Atlanta, GA 30303, United States","Received 15 October 2017, Revised 21 October 2018, Accepted 22 October 2018, Available online 7 November 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.csda.2018.10.012,Cited by (3),"The semi-parametric transformation models under length-biased sampling are considered. The well-known ==== and proportional odds model are special cases of the semi-parametric transformation models. Empirical likelihood and adjusted empirical likelihood inferences for semi-parametric transformation models with length-biased sampling are proposed, and the empirical log-likelihood ratio test statistic is shown to converge to a standard chi-squared distribution. In addition, ====."," as ====where ==== is a failure time, ==== is an unknown ====The empirical likelihood (EL) approach was introduced by ====, ====, ==== based on the original idea proposed by ====) still holds for the empirical log-likelihood ratio statistic. The limiting distribution for the EL ratio is a chi-squired limiting distribution. This approach offers the advantages of eliminating the need to specify a distribution of the data, and often yields more efficient estimates of the parameters than many common estimators.==== used the empirical likelihood method to construct a confidence interval based on the rank estimators of the regression parameter in the censored accelerated failure time model. ==== applied an empirical likelihood ratio method and derived the limiting distribution of the EL ratio for the regression parameters in the semi-parametric transformation model via ====-statistics. ==== showed that the limiting distribution of the EL ratio for the linear transformation model is a weighted sum of standard chi-squared distributions. Subsequently, ==== appropriately modified the constructions based on ====, thereby deducing that the limiting distribution of the EL ratio follows a standard chi-squared distribution. More recently, in the light of Owen’s work, ==== developed the adjusted empirical likelihood method for general estimating equations, which has been extended in various research papers, such as ====, ====, ====.====. Length-biased data have been studied extensively more recently. ==== made inferences for semi-parametric transformation models with length-biased sampling based on the ranks of observed failure times, while ==== obtained the estimators of regression parameters from counting process-based unbiased estimating equations. The crucial step of the latter method was to construct martingale estimating equations. Although ====, and derive an empirical log-likelihood ratio statistic, which has a standard chi-squared limiting distribution.====The remainder of this paper is organized as follows. In Section ====, empirical likelihood and adjusted empirical likelihood (AEL) inference procedures are introduced. In Section ====, simulation studies are carried out to demonstrate the performance of the proposed EL and AEL methods. Furthermore, a real data analysis is shown in Section ====. We discuss the findings, along with the further work in Section ====. Proofs are provided in the ====.====In this section, we provide the proofs of ====, ====, ====, ====.====In order to ensure the central limit theorem for counting process martingales, certain regularity conditions, which can be found in ====, need to hold. On the other hand, the same conditions as in ==== are stated.====(D.1) ==== is positive and ==== is bounded and continuous on ====, where ==== is a finite constant.====(D.2) For some constant ====, ====.====(D.3) ==== has continuous and positive derivatives on ====.====(D.4) ==== and ==== are assumed to be finite and non-degenerate matrices.====(D.5) ====.",Empirical likelihood inference for semi-parametric transformation models with length-biased sampling,https://www.sciencedirect.com/science/article/pii/S0167947318302573,April 2019,2019,Research Article,523.0
"Liu Li,Xiang Liming","School of Mathematics and Statistics, Wuhan University, Hubei 430072, PR China,School of Physical and Mathematical Sciences, Nanyang Technological University, Singapore, 637371, Singapore","Received 15 June 2018, Revised 7 October 2018, Accepted 18 October 2018, Available online 5 November 2018, Version of Record 14 February 2019.",https://doi.org/10.1016/j.csda.2018.10.011,Cited by (1),". To overcome the problem of missing data, we propose two novel methods relying on auxiliary variables: a penalized ",", p43). As such, there have been a variety of improved approaches to account for missing data problems, especially for missing covariates. ====. ====With the assumption of normally distributed random effects, ==== considered the GLMMs with continuous auxiliary covariates using a kernel smoother. Since random effects are unobservable, it is difficult to verify a parametric distribution assumption in practice. In addition, ==== proposed a conditional quadratic inference function approach to estimate GLMMs for longitudinal data with unspecified random-effects distribution. Furthermore, as shown in ====, the random effects may depend on covariates, and ignoring the relationship between random effects and covariates would lead to inconsistent estimators. This encourages us to explore more effective estimation methods for inference of GLMMs with missingness and no distribution assumptions about random effects while allowing dependence between random effects and covariates.====, ====, ====, ==== and ====.==== and ====, where a finite number of longitudinal observations for each subject and a finite number of clusters were required, respectively. We need to tackle the problems allowing the cluster size or the number of clusters to be divergent, hence the derivation of the subsequent theoretical properties is much more challenging than those in ==== and ====.====The rest of the paper is organized as follows. We first introduce the notation and model setting up in Section ====. In Section ====. Section ==== presents simulation results, and Section ==== applies the proposed methods to a forest health data set. In Section ====, we conclude the paper with a discussion.",Missing covariate data in generalized linear mixed models with distribution-free random effects,https://www.sciencedirect.com/science/article/pii/S0167947318302561,5 November 2018,2018,Research Article,524.0
"Barbaro Ryan P.,Sen Ananda","Department of Biostatistics, University of Michigan, Ann Arbor, USA,Division of Pediatric Critical Care, University of Michigan, Ann Arbor, USA,Child Health Evaluation and Research Unit, University of Michigan, Ann Arbor, USA,Department of Family Medicine, University of Michigan, Ann Arbor, USA","Received 19 March 2018, Revised 28 October 2018, Accepted 28 October 2018, Available online 5 November 2018, Version of Record 16 February 2019.",https://doi.org/10.1016/j.csda.2018.10.014,Cited by (1),"In ====, separation occurs when a ","A default prior in principle falls between nearly flat/improper priors, e.g. Jeffrey’s prior (====). Such a prior mildly shrinks toward some null value non- or weakly identified parameters and leaves alone those well supported by the likelihood (====, ====, ====). However, it does not borrow strength via shared hyperpriors as do informative shrinkage priors.====, ====, ====, ====, ====, ====, ====, ====, ====). The scale-family of ====-priors, or reference informative priors, is one early example of a default prior for regression coefficients (====). The degree of shrinkage – and therefore the extent to which that family of priors may be viewed as ‘default’ – depends on the choice of scale parameter ====, which is shared by all regression coefficients. If fixed at some diffuse or prespecified value, this would satisfy our definition of a default prior (====). If instead ==== is adaptively tuned via a hyperprior, as in ====, this would not, as information is shared across parameters.==== propose quantitative measures of separation: ==== is the size of the smallest subset of observations that, if removed from the data, would completely separate the complementary subset. Separation is equivalent to ====; the resulting lack of numerical convergence allows for relatively easy detection. In contrast, near-separation, i.e. a small but positive ====, yields finite MLEs but manifests symptoms of separation including instability and efficiency loss. For this reason, mild regularization from default priors can be just as useful when there are a few dozen predictors as when there are hundreds or more, particularly when the number of observations is of a similar order, i.e. ====.====The point of departure in this work is a focus on the intercept parameter. In contrast to regression coefficients, there is generally not an intuitive null direction toward which shrinkage should be, and borrowing strength for the intercept is not possible (e.g. Section 3.4, ====). The usual recommendation is that a prior should be flat or effectively so (====, ====, ====). We demonstrate that straightforward efficiency gains are possible by assuming that exceptionally large values of the intercept are either implausible or unverifiable in the data. Thus, down-weighting these regions frequently improves efficiency.====This paper makes several contributions. First, we use complete separation to establish a rationale that mild shrinkage of the intercept can improve estimation of the regression coefficients. Following ====, we consider a stronger type of separation that we call ‘pivotal separation’. Second, we propose to adapt the exponential-power scale-family of distributions (====, ====, ====) for default use as a prior on the intercept in binary data regression models and develop an algorithm to determine a suitable scale for this prior. Finally, our work highlights the correspondence between choice of prior on the intercept and that of the regression coefficients.====In Section ====, ==== describe our choices of the different class priors for the intercept and the regression coefficients, respectively. Findings from a comprehensive simulation study are documented in Section ====, which provides a comparative appraisal of the Bayesian estimators under varied scenarios including sparsity and ====. The demonstration of our methodology on ten datasets in Section ==== illustrates the heterogeneity in the degree of separation in real data and, more importantly, highlights the stabilizing properties of our proposed priors in estimation of the regression coefficients. Section ==== interprets our results and discusses some counterarguments against using priors on the intercept parameter.====The following is the Supplementary material related to this article. ",Default priors for the intercept parameter in logistic regressions,https://www.sciencedirect.com/science/article/pii/S0167947318302755,May 2019,2019,Research Article,525.0
"Jhong Jae-Hwan,Koo Ja-Yong","Department of Statistics, University of Korea, Seoul 02841, Republic of Korea","Received 2 November 2017, Revised 9 July 2018, Accepted 6 October 2018, Available online 23 October 2018, Version of Record 16 February 2019.",https://doi.org/10.1016/j.csda.2018.10.005,Cited by (2),We consider the problem of simultaneously estimating a finite number of ==== rate under ====.,"). This can be considered an alternative to mean regression. Since a full range of different quantile functions can be fitted to data, quantile regression functions provide a panoramic view of the stochastic relationship between a response variable and predictors. From a nonparametric point of view, regression splines offer an attractive tool to handle quantile regression functions. ====, ====, and ====, among others, have investigated the applicability of nonparametric quantile regression. ==== proposed the B-spline quantile estimator using total variation penalty assuming that the initial knots are evenly spaced over a domain.====In certain circumstances, one may be interested in estimating a finite number of conditional quantile functions simultaneously. ====, ====, and ====.==== investigate coordinate descent algorithms with their convergence theory. ==== and ==== propose the coordinate descent algorithm for the various types of penalized regression, such as lasso (====) and fused lasso (====) penalty. ==== develop an iterative coordinate descent algorithm for high-dimensional quantile regression with nonconvex penalty. ==== consider an algorithm for the elastic-net (====) penalized Huber loss regression and quantile regression in high-dimensional settings. ==== use the coordinate descent algorithm in penalized mean regression with B-spline and the total variation penalty.====, ====, ====, we also consider non-crossing quantile function estimators having additional constraints at the knots of constant or linear spline functions. While ====The rest of the paper is organized as follows. Section ==== develops algorithms for estimators. Section ==== tests the algorithms on simulated and real data, while Section ==== provides theoretical results. The ==== gives all details of the technical proof. An ==== software package ==== is available at ====.====Choose ==== and ====. Since B-splines are nonnegative, we obtain ====where ====The univariate penalty function ==== has various forms according to the order ==== of splines and index ==== of the B-spline coefficient. Using the entries of ==== related to ====, the univariate penalty function can be represented as ====where ==== and ==== for ====.====Let ====
 ==== implies that there always exists a minimizer of ==== over a compact interval ====. If the set of minimizers of ==== is a sub-interval ==== of ====, then we define the minimizer of ==== by ====. Then, the minimizer must be an element of ====.",Simultaneous estimation of quantile regression functions using B-splines and total variation penalty,https://www.sciencedirect.com/science/article/pii/S0167947318302500,May 2019,2019,Research Article,526.0
"Tang Niansheng,Xia Linli,Yan Xiaodong","Key Lab of Statistical Modeling and Data Analysis of Yunnan Province, Yunnan University, Kunming 650091, China","Received 25 February 2018, Revised 4 August 2018, Accepted 6 October 2018, Available online 17 October 2018, Version of Record 16 February 2019.",https://doi.org/10.1016/j.csda.2018.10.003,Cited by (5),"This paper proposes a new feature screening procedure in ultrahigh-dimensional partially linear models with missing responses at random for ==== based on the profile marginal kernel-assisted estimating equations imputation technique. The proposed feature screening procedure has three key merits. First, it is computationally efficient, and can be used to screen significant ====. Simulation studies are conducted to investigate the finite sample performance of the proposed screening procedure. An example is used to illustrate the proposed procedure."," for local influence analysis of PLMs based on the penalizing gaussian likelihood; ==== for the estimation and variable selection; ==== for PLMs with missing responses at random; ==== for efficient semiparametric estimator for heteroscedastic PLMs; and ====), the smoothly clipped absolute deviations penalty (SCAD, ====). To efficiently shrink the dimension of covariates in the first step, many feature screening procedures have been proposed over the past years. For example, for independent and identically distributed datasets, ==== presented a robust rank correlation screening procedure, which is applicable to transformation regression models and single-index models. ==== developed a model-free feature screening procedure based on the distance correlation (DC). ==== established a score-test-based screening framework. ==== presented a nonparametric independence screening (NIS) procedure for ultrahigh dimensional longitudinal data. ==== proposed a GEE-based screening procedure for ultrahigh dimensional time course genomic data. ==== presented a Pearson correlation sure independence screening procedure based on the partial residual for PLMs.====). To this end, there is considerable literature on variable selection for low-dimensional regression models with missing data. For example, ====, for missing data problems within an expectation and maximization framework. ==== extended the quasi-likelihood under the independence model criterion and the missing longitudinal information criterion to generalized estimating equations with multiply imputed longitudinal data. ==== presented a novel model selection criterion, called the penalized validation criterion, in the presence of nonignorable nonresponse with unspecified propensity score. In particular, feature screening for ultrahigh dimensional data with missing data has also received a little attention in recent years. For example, ==== proposed a missing indicator imputation screening procedure for ultrahigh dimensional data in the presence of missing data, which may avoid the well-known “curse of dimensionality”. However, to our knowledge, feature screening for ultrahigh-dimensional PLMs with missing response at random for longitudinal data has not been studied yet.====The rest of this paper is organized as follows. Section ==== proposes a feature screening approach based on the profile kernel-assisted estimating equations imputation scheme, and studies its theoretical properties under some regularity conditions. Simulation studies are conducted to investigate the finite sample performance of the proposed methodologies Section ====. An example is used to illustrated the proposed methodologies in Section ====. Some concluding remarks are given in Section ====. Technical details are given in the ====.====Before presenting the proof of ====, we first introduce some useful lemmas.====Note that ====where ==== and ====. Under the uniformly bounded condition ====, (C1), (C2), (C3) and (C5), we can select a small enough ==== so that the Taylor’s expansion can be applied to ==== for any given ====: ====. Following the argument of Theorem 2 of ====, we have ====, which indicates that ==== for sufficiently large ====.====According to Lemma S2 of ====, ==== is uniformly bounded by ==== for some constant ====, and we have ==== provided that ====. Thus, we have ====. Hence, for a sufficiently small ==== and a large ====, Eq. ==== satisfies ====Let ====. It follows from the Taylor’s expansion that inequality ==== can be simplified as ====Similarly, we can obtain ====According to condition (C4) and Lemma S3 of ====, there are some positive constants ==== and ==== such that for any ====, we have ====. Hence, by taking ====, we obtain ====.====Combining the above results yields ==== where ====, which shows that ==== holds. Other inequalities can be similarly proved.====By the same technique, the second term of ==== is simplified as ====Combining ====, ====, ====, we have ====Therefore, we have ==== uniformly in ==== according to the condition (C6), and we also have ====.====Following the argument of ====, there are some positive constants ==== and ==== so that for any ====, we have ====, which indicates ==== if taking ====. Combining the above equations yields ====, where ====The above argument can be used to obtain the upper bounds of ====, and ====. According to inequality properties and Lemma S4 of ====, we have ====Now we show that for any ====, there is some ==== so that ==== holds. To this end, let ====, ====, ====, ====, ====, ====, ====, and ====. Denote ====. Then, we have ====Now we show ==== for some ====. For any ====, we have ====Similar to the derivation of ====, it is easily shown that ====.====Following the above argument, we can obtain the upper bounds of ====, ==== and ====. According to inequality properties and Lemma S4 of ====, we have ====Combining Inequality ====, ====, ==== and using Lemma S4 of ==== yields ====where ====.====Now, we find an upper bound of ====, it is straightforward to verify that ====Let ====. To obtain an upper bound of ====, we first show ==== for some ====. To this end, we denote ====, ====, and ====, and ====,. Then, we have ====Now we show ====. For any ====, we have ====where the first part is obtained by Hoeffding’s inequality, and the second part ====for ==== and ==== by condition (C4) and Lemma S3 of ====. Thus ====Because ====, we take ====, where ====. Thus, for a sufficiently large ====, we have ====, and ====. Hence, we have ====Similarly, we can prove ====, ==== and ====.====Combining the above equations yields to ====where ====.====In the similar fashion, we can prove ====Together with Inequality ====, ====, ====, we have ====where ====. Combining Inequality ====, ====, ====, we can have ====Now we show ====. Under condition (C6) and (C7), ==== holds, hence for large ====, there exists some ==== such that ====. Then, we have ====The last inequality is the direct result from Inequality ====, and it goes to ==== as ====, for ====, where ====. Then, using the Fatou’s Lemma and combining the above equations leads to ====which shows that ====Thus, we prove the ranking consistency property.====For any ====, we can obtain ====where ====It is easily shown from Lemma S5 of ==== and ==== that there exists a positive constant ==== such that ====. Similar to ====, we have ====, where ====. Combining the above equations yields ====. Similar to the argument of ====, we can obtain that there exists a positive constant ==== such that ====.====Using the same argument as ====, we can obtain the upper bounds of ==== and ====. By Lemma S5 of ====, we have ====Similarly, we can obtain ====Combining the above inequalities, we show that there exists some ==== such that ====Similar to the proof of ====, combining the above inequality and Inequality ====, we have ====where ==== is a positive constant, and ====. The last equation is because the first term dominates the second when ==== and ====. Hence, ====Thus, we finish the proof of the first part of ====.====By the definition of ==== and ==== for ====, we have ====Thus, we prove the sure screening property.",Feature screening in ultrahigh-dimensional partially linear models with missing responses at random,https://www.sciencedirect.com/science/article/pii/S0167947318302482,May 2019,2019,Research Article,527.0
"Karavarsamis N.,Huggins R.M.","School of Mathematics and Statistics, The University of Melbourne, Victoria 3010, Australia,Department of Mathematics and Statistics, La Trobe University, Victoria 3086, Australia","Received 8 March 2018, Revised 25 September 2018, Accepted 26 September 2018, Available online 16 October 2018, Version of Record 16 February 2019.",https://doi.org/10.1016/j.csda.2018.09.009,Cited by (6),None,"Occupancymodels were introduced in ====Occupancy models are currently fitted to data using the full likelihood where the parameters associated with occupancy and detection are simultaneously estimated. The likelihood may be maximised numerically using the ==== package ==== (====) for example. We have observed that the full likelihood can be numerically unstable. This is distinct from boundary solutions that occur in occupancy models, as noted in ====, ====, ====, ==== where it is seen that maximum likelihood can give extreme estimates of occupancy parameters when the two-stage approach does not.====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====). These may be fitted using the ==== and ==== functions in ==== package. In our two-stage approach we address potential instability by considering detection and occupancy separately. This allows us to compute the estimates over two lower dimension parameter spaces. Moreover, the more complex modelling of the effect of time dependent covariates on the detection probabilities is relatively straightforward in the two-stage approach.====To help stabilise the numerical optimisationalgorithm the package ==== (see p. 2 of the R vignette ==== ) recommends that covariates be standardised. However, as observed in the documentation for the ==== package, standardising may cause problems with standard R functions, such as ====. The ==== package may solve some issues but not in all instances of data dependent parameters. Moreover, there is no guarantee that users will standardise their data. In addition, the choice of algorithm for the numerical maximisation may be changed in ====, however this may be sensitive to the algorithm used.====Recently ==== showed that for the homogeneous occupancy model a simple transformation yielded orthogonal parameters resulting in a two-stage estimation procedure that simplified the computation of the estimates. We see in Section ==== function in the ==== package in R (====, ====, ====Our notation is given and the full likelihood is examined in Section ====. In Section ==== we describe the two-stage approach. In the first stage in Section ==== we use conditional likelihood to estimate the detection probabilities, with time independent detection probabilities discussed in Section ==== and time dependent detection probabilities considered in Section ====. In the second stage in Section ==== we introduce a partial likelihood approach to estimate the occupancy probabilities using the detection probabilities estimated in the first stage. In Section ==== we give an IWLS algorithm to compute the occupancy estimates. A simulation study is conducted in Section ==== and the methods are applied to several data sets in Section ====. Some discussion is given in Section ====. Some technical derivations and the implementation of ==== in this setting are given in the appendices.====The R package ==== (====) is a powerful and flexible package that fits models to vector responses. As such, at first glance it can be overwhelming. However, its handling of time dependent covariates makes it preferable to writing one’s own functions. Here we give a description of how it can be used to fit some common models to detections using conditional likelihood in the first stage of our approach.",Two-stage approaches to the analysis of occupancy data II. The heterogeneous model and conditional likelihood,https://www.sciencedirect.com/science/article/pii/S0167947318302469,May 2019,2019,Research Article,528.0
Rauf Ahmad M.,"Department of Statistics, Uppsala University, Sweden","Received 15 November 2017, Revised 10 October 2018, Accepted 11 October 2018, Available online 15 October 2018, Version of Record 5 November 2018.",https://doi.org/10.1016/j.csda.2018.10.008,Cited by (7)," are studied under fairly general conditions. The accuracy of the modified estimator and the test is shown through simulations under a variety of parameter settings. In comparisons against several existing methods, both the proposed estimator and the test exhibit similar performance to the distance correlation. Several real data applications are also provided.","The most distinguishing feature of multivariate theory, compared with its univariate counterpart, is the presence of a large number of correlations emerging from multiple observations measured on each unit. This, in turn, makes the study of these correlations an essential and one of the most interesting aspects of multivariate theory. Often in practice, vectors of multivariate observations are divided into subgroups and the interest focuses on the cross-correlations among subvectors. ==== is just one example.====).====RV coefficient (====) is one such measure and stands discernably different from its competitors due to several attractive features. Composed of symmetric trace-operators (see Eq. ====, ====).====RV coefficient has been a subject of interest since its introduction. ====, ====, ====, ==== and ====; see also ==== for a brief review.====The construction of RV coefficient finds its similarity and close connection to the modern distance correlation framework (====, ====). ==== provide a comprehensive update in this context, including comparison with other similar measures. ====, ==== and ==== demonstrate its applications for brain imaging data whereas ==== applies a generalized version of RV coefficient to genomic data. A kernel based extension is discussed in ==== with applications on micriobiome data.====The evaluation of the RV coefficient and its possible use in constructing tests of zero correlation, particularly for high-dimensional data, has been quite limited so far. ====, although only for ==== case; see also ====.====In this paper, we propose a modification to the RV coefficient for high-dimensional data and use it to construct a test of zero correlation. We motivate the idea in the next section after discussing the RV coefficient in detail and several related measures from the literature. In Section ====. Several real data applications are given in Section ====. Section ==== summarizes the main points. Some technical results are collected in the appendix.",A significance test of the RV coefficient in high dimensions,https://www.sciencedirect.com/science/article/pii/S0167947318302536,March 2019,2019,Research Article,529.0
"Fang Fang,Chen Yuanyuan","Key Laboratory of Advanced Theory and Application in Statistics and Data Science (East China Normal University), Ministry of Education, 500 Dongchuan Road, Shanghai 200241, China,School of Statistics, East China Normal University, 500 Dongchuan Road, Shanghai 200241, China","Received 13 February 2018, Revised 1 October 2018, Accepted 6 October 2018, Available online 15 October 2018, Version of Record 16 February 2019.",https://doi.org/10.1016/j.csda.2018.10.004,Cited by (23),irectly ====aximizes the ====olmogorov-,"For each customer, let ==== if the customer will not default and ==== otherwise. A “credit score” ==== and is considered to be positively related to the conditional non-default probability ====, ====, ====, ====, ====, ====, ==== and the references therein for more details.====, ====). Specifically, KS is defined as the Kolmogorov–Smirnov statistic for the difference between the score distribution of the bad customers and the score distribution of the good customers, i.e., ====Note that we do not take absolute value for the difference between two conditional cumulative distribution functions in ==== as in the usual definition of Kolmogorov–Smirnov statistic. Actually, in credit scoring, the intention is that higher score ==== corresponds to lower risk, in which case we would expect ====. If we use ==== as a “cut-off score”, i.e., customers with credit scores no more than ==== will be rejected, ==== is the percentage of bad customers that are correctly rejected. On the other hand, ==== is the percentage of good customers that are mistakenly rejected. Therefore, KS can be considered as the maximum oracle benefit we can get based on the scoring model.====In practice, we develop a credit scoring model based on a data set ==== and then evaluate the model by an estimator of ====: ====where ====, ====, ==== stands for the indicator function, and ==== is the estimated credit score for the ==== proposed a parameter estimation method by minimizing KS statistic. But the purpose is not for credit scoring. A natural question is: can we directly maximize KS for credit score estimation since it is commonly used for performance evaluation?====In this paper, we develop a new approach for credit scoring, denoted as DMKS, by ====irectly ====aximizing the ====olmogorov-====mirnov statistic. We assume that ====where ==== is an unknown increasing function between 0 and 1, ==== is a ==== for parameter identification, where ==== with ====. Our main work consists of the following three parts. First, we show that when ====, the population level KS achieves its maximum. Then we propose to estimate ==== by maximizing the sample version of KS. The proposed estimator is shown to be consistent to ====. Second, the objective function is neither smooth nor continuous. Therefore, the most typically used Newton–Raphson type algorithm cannot be directly used, which makes the computational aspects challenging. We use an iterative marginal optimization (IMO) algorithm to overcome the difficulty. The IMO algorithm was first proposed by ==== to maximize the area under the ROC curve (AUC), which is another important evaluation criterion for credit scoring. Third, after ====The rest of the paper is organized as follows. Section ==== describes our methodology including the IMO algorithm and the smoothing PAVA algorithm in detail and shows some theoretical results. Empirical results of simulation studies and two real business examples are presented in Section ==== and Section ====, respectively. Some concluding remarks are given in Section ====. All the proofs are in the ====.",A new approach for credit scoring by directly maximizing the Kolmogorov–Smirnov statistic,https://www.sciencedirect.com/science/article/pii/S0167947318302494,May 2019,2019,Research Article,530.0
Zhang Shibin,"Department of Mathematics, Shanghai Maritime University, 1550 Haigang Avenue In New Harbor City, Shanghai 201306, China","Received 15 April 2018, Revised 30 July 2018, Accepted 3 October 2018, Available online 12 October 2018, Version of Record 16 February 2019.",https://doi.org/10.1016/j.csda.2018.10.001,Cited by (9),"Recently, quantile-based spectral analysis has drawn much attention due to that it can capture serial dependence more than covariance-related. One of typical quantile-based spectra is the ==== (HMC) step. The parameter grouping scheme reduces the encoding workload, and the HMC reduces the computation complexity. Both of them allow the method to be applicable to estimate a large number of CSDKs simultaneously.","). To capture serial features that cannot be accommodated in the classical spectral analysis, many researchers started to try quantile-based spectral analysis tools in recent decades. By using quantile-based spectrum, ====, further studied by ====. Meanwhile, the results from ====.====The method introduced by ==== (defined in ====) are indexed by couples ==== of quantile levels, where ==== and ==== (====), with ==== being the one-dimensional marginal cumulative distribution function of a strictly stationary univariate process ====. To detect more features of the joint distributions of pairs ====, the collection of many CSDKs with different couples ====, should be estimated. To ensure that the estimate of ==== is positive definite, all the RLPs should be smoothed in the same way. However, it is believed in most of situations that different components in the matrix have different degrees of smoothness, and hence different smoothing parameters are required.====Since ==== produced an automatically smoothed spectral matrix estimator so that many researchers extended their approach to estimate the time-varying multivariate spectra such as (====) and ====. ====Theoretically, the proposed methodologies of ==== and ==== provide some means for estimating the multivariate spectra of ====-dimensional time series with general ====. However, they are impractical when ==== is greater, e.g., ====. Since the number of Cholesky components is ==== becomes lower with increasing ====. For involving a high-dimensional semi-definite programming problem in minimizing the penalized Whittle negative loglikelihood, the computation in the PWL approach of ==== becomes excessively costly with the dimension of data increasing. Actually, we have not found any simulation study or data analysis that manipulates the time series with ==== more than three in the literature. However, to search for more detailed features of the joint distributions of pairs ====, it is of great necessity to estimate the CSDK matrix ==== with a large ====.====In this paper, by Theorem 3.3 of ==== (or Proposition 3.4 of ==== to be a summation-form, by which the coefficients of spline basis and smoothing parameters are grouped independently. We use an MCMC technique to fit smoothing splines to all components of ====The rest of this article is organized as follows. Section ==== presents the model and prior specification for parameters. Section ==== presents our proposed HMC-within-Gibbs sampling scheme. Section ==== indicates the performance of the proposed method by several simulation examples. Section ==== contains our concluding remarks.====In what follows, the current and proposed values of ==== are denoted by the superscripts ==== and ====, respectively. Suppose the chain for ==== is currently at ====. As an illustration, we only state the details of the steps for sampling ====. Given ====, ====, ====, ==== and ====, ====, the Hamiltonian Monte Carlo (HMC) is designed to sample ==== jointly from ====.====In addition to the posterior density, the HMC also requires the gradient of log-posterior density. We first give the analytical expression of the gradients, and then state the detailed HMC step.",Bayesian copula spectral analysis for stationary time series,https://www.sciencedirect.com/science/article/pii/S0167947318302457,May 2019,2019,Research Article,531.0
"Li Shuwei,Hu Tao,Zhao Xingqiu,Sun Jianguo","School of Economics and Statistics, Guangzhou University, Guangzhou, China,School of Mathematical Sciences, Capital Normal University, Beijing, China,Department of Applied Mathematics, The Hong Kong Polytechnic University, Hong Kong,Department of Statistics, University of Missouri, Columbia, MO, USA","Received 9 May 2018, Revised 21 August 2018, Accepted 29 September 2018, Available online 11 October 2018, Version of Record 16 February 2019.",https://doi.org/10.1016/j.csda.2018.09.008,Cited by (6),"This paper discusses ==== of the resulting estimators are established and in particular, the estimators of ==== are shown to be semiparametrically efficient. The numerical results obtained from a simulation study indicate that the proposed approach works well for practical situations. An application to a set of data on children’s mortality is also provided.","Failure time data occur commonly in many scientific fields including epidemiological surveys, clinical trials, demographic studies and tumorigenicity experiments (====, ====To address the existence of a cured subgroup, one method is to employ the two-component mixture cure model, which models the non-cured and cured subpopulations separately (====, ====, ====, ====, ====, ====, ====, ====, ====). Corresponding to this, the non-mixture proportional hazards or promotion time cure model was studied in the literature based on the adaptation of the proportional hazards model to allow for a cured subgroup (====). To be more specific, let ==== denote the failure time of interest and ==== given ==== can be written as ====where ==== is an unknown cumulative distribution function and ====, represents the proportion of cured individuals. The non-mixture cure model above has easier interpretation for practitioners, but inference on such model is computationally intensive, especially under interval-censored data.====, ====, ====, ====, ====, ====, ====). For example, ====, ==== and ==== discussed regression analysis of such data under the proportional hazards, proportional odds and transformation models, respectively, and ==== gave a comprehensive review of some existing inference methods on the topic. In particular, ==== and ==== considered the inference about model ==== discussed the same problem but for the transformation cure rate models based on current status data, a special case of interval-censored data (====).==== to analyze current status data with the spline-based proportional hazards and proportional odds models. ==== extended the idea of ==== to fit the proportional hazards model to interval-censored data. ====The remainder of the article is organized as follows. In Section ====, we introduce some notation and assumptions, and then present the resulting likelihood function. Section ====. In Section ====, we present some numerical results obtained from a simulation study conducted to assess the finite sample properties of the proposed methodology. Section ==== applies the method to a set of children’s mortality data arising from the 2003 Nigeria Demographic and Health Survey. Some discussion and concluding remarks are given in Section ====.====In this appendix, we will sketch the proofs for ====, ====, ==== and for this, we will mainly use some results about empirical processes given in ====. In the following, for a function ==== and a random variable ==== with the distribution ====, define ==== and ====. Let ==== represent a generic constant that may vary from place to place.",A class of semiparametric transformation cure models for interval-censored failure time data,https://www.sciencedirect.com/science/article/pii/S0167947318302445,May 2019,2019,Research Article,532.0
"Sheng Ying,Wang Qihua","Academy of Mathematics and Systems Sciences, Chinese Academy of Sciences, Beijing 100190, China,School of Statistics and Mathematics, Zhejiang Gongshang University, Zhejiang 310018, China","Received 7 December 2017, Revised 7 September 2018, Accepted 8 September 2018, Available online 9 October 2018, Version of Record 16 February 2019.",https://doi.org/10.1016/j.csda.2018.09.002,Cited by (0),"Two new methods are proposed to solve the problem of constructing ====, selecting important variables for classification and determining corresponding discriminative variables for each pair of classes simultaneously in the high-dimensional setting. Different from existing methods, which are based on the separate estimation of the ==== and mean vectors, the proposed methods construct classifiers by estimating products of the precision matrix and mean vectors or all discriminant directions directly with appropriate penalties. This leads to the use of the distance criterion instead of the log-likelihood used in the existing literature. The proposed methods can not only consistently select important variables for classification but also consistently determine corresponding discriminative variables for each pair of classes. For the "," proposed a direct sparse estimation approach to construct an efficient binary classifier and proved nice theoretical properties. ==== proposed multiclass sparse discriminant analysis (MSDA) to select important variables and construct a linear discriminant rule. In addition to the above, there are many other relevant works, including ====, ====, ====, ====, ====, ====For the multiclass classification problem, it is also important to determine corresponding discriminative variables for each pair of classes in addition to selecting important variables for classification. A motivating example is given by ====. In the SRBCT dataset studied by ====, there are four tumor subtypes: Ewing’s sarcoma, rhabdomyosarcoma, neuroblastoma and Burkitt’s lymphoma. Identifying corresponding discriminative variables for each pair of classes is conducive to understanding the roles of particular genes in separating different subtypes of cancers. Thus ==== assumed independence among all variables and constructed a linear discriminant rule by maximizing the log-likelihood with the adaptive pairwise fusion penalty. However, no asymptotic theoretical properties were established for his method. Besides, the assumption that all variables are independent is too strict and easily violated in the high-dimensional setting. As an improvement, ====, ====, ====. Additionally, we can also concentrate on estimating all discriminant directions directly. For our purposes, we introduce penalties by combining an adaptive version of the group bridge penalty proposed by ==== with the adaptive pairwise fusion penalty ====, ====, ====. It should be noted that even though the linear programming discriminant rule (LPD) proposed by ==== and ====. At last, simulations and the real data analysis well demonstrate advantages of our methods compared with other related methods.====The rest of this paper is organized as follows. In Section ====, we propose two new methods to construct multiclass classifiers, select important variables for classification and determine corresponding discriminative variables for each pair of classes simultaneously. We state theoretical properties for the proposed methods in Section ====. Simulations and the real data analysis are conducted in Section ==== and Section ==== respectively. A discussion is given in Section ==== and the brief proof of asymptotic theoretical properties are given in the ====.====In what follows, we use ==== to denote a generic positive constant for convenience. We present the brief proofs of theorems here. The detailed proofs are given in the supplementary material.",Simultaneous variable selection and class fusion with penalized distance criterion based classifiers,https://www.sciencedirect.com/science/article/pii/S0167947318302305,May 2019,2019,Research Article,533.0
"Yue Mu,Li Jialiang,Cheng Ming-Yen","University of Electronic Science and Technology of China, National University of Singapore, Singapore,National University of Singapore, Duke University-NUS Graduate Medical School, Singapore Eye Research Institute, Singapore,Hong Kong Baptist University, Hong Kong","Received 5 January 2018, Revised 12 September 2018, Accepted 5 October 2018, Available online 9 October 2018, Version of Record 5 November 2018.",https://doi.org/10.1016/j.csda.2018.10.002,Cited by (7), is further provided to demonstrate the proposed methodology.," is relatively small but the number of features ====), SCAD (====), MCP (====) and others (====, ====, ====We consider boosting as an effective alternative tool for high-dimensional modeling in this article. Boosting was proposed as a family of powerful machine-learning techniques that iteratively forms base learners to minimize the loss function. The original boosting algorithms were proposed by  ==== and  ==== while both of them did not take full advantage of the base learners. Then ====Boosting (====Boosting (====) with the penalized loss function and HingeBoost (==== for the pAUCBoost;  ==== for the Twin Boosting; ==== for the Twin HingeBoost; ==== for the ==== for GSBoosting. In addition to these extensions, much progress has been made in understanding the advantages of boosting such as relatively smaller computational cost, lower over-fitting risk and simpler adjustment to incorporate extra constraints. Furthermore, properties of the boosting have been extensively studied. With fixed predictor dimension, the statistical convergence and consistency for boosting-type algorithms has been established by ====, ==== and ==== under classical data setting and further proved by ==== and ==== under high-dimensional data setting. In this paper we will examine a novel sparse boosting approach for varying-coefficient models with high-dimensionality.====), finance (====) and epidemiology (====). For example, in a study to examine the Boston housing price, ==== adopted varying-coefficient model to identify a set of gene expression signatures highly predictive in the lung cancer where the effects are functions of age. ====Longitudinal data consist of repeated measures on the same subject over time. The assumption that all observations are independent is violated for such data. One important issue in longitudinal analysis is how to take into account of the correlation within subjects and make efficient inference. This issue has been thoroughly investigated by many authors (====, ==== and ====. There are some recent development on high-dimensional longitudinal data using varying-coefficient models (====, ====, ====, ====). All previous authors considered the penalty methods.====In this paper, a novel two-step sparse boosting approach is proposed to carry out the variable selection and the model-based prediction. In particular, we consider to fit high-dimensional varying-coefficient models with longitudinal data. In the first step of two-step sparse boosting, the sparse boosting technique is used to yield an estimate of the correlation structure and in the second step, the within-subject correlation structure is taken into account and variable selection and estimation are conducted by sparse boosting again. Although (sparse) boosting algorithms have been widely studied in literature, to the best of our knowledge, there is no existing work about the two-step (sparse) boosting. Extending known boosting methods for such a complicated task is methodologically challenging and needs significant progress. The detailed development in this work makes a substantial contribution for such data analysis. Simulation results confirm the effectiveness of the two-step sparse boosting method.====The main contents of this paper are arranged as follows. In Section ====, the varying-coefficient model for longitudinal data is formulated and a two-step sparse boosting algorithm is proposed. In Section ====, simulation studies are conducted to demonstrate the validity of this two-stage method. In Section ====.====Given ====, where ==== is a ====-dimensional covariate vector and ==== a one-dimensional response variable. ==== are usually assumed to be independent sample. Let ==== denote the loss function, our purpose is to estimate the unknown function ====. Estimate ==== can be done by boosting which descends the empirical risk ==== via steepest gradient descent in function space, where the function space is provided by base learners. We summarize a generic boosting algorithm (====, ====) as follows:==== Initialization. Let ==== and let ====.==== Iteration. Increase ==== by 1. Compute negative gradient of the loss function ==== and evaluate at ====, i.e. ==== for ====. Fit negative gradient vector ==== by the base learner, i.e. ====.==== Update. ====, where ==== is the pre-defined step-size parameter.==== Iteration. Repeat step (b)–(c) until ==== for some stopping criterion ====.====For ==== Boosting, the squared loss function is a special case of generic boosting algorithm with ==== and ==== in iteration ====.",Two-step sparse boosting for high-dimensional longitudinal data with varying coefficients,https://www.sciencedirect.com/science/article/pii/S0167947318302470,March 2019,2019,Research Article,534.0
Godolphin J.D.,"Department of Mathematics, University of Surrey, Guildford, Surrey. GU2 7XH, UK","Received 28 February 2018, Revised 15 August 2018, Accepted 17 September 2018, Available online 25 September 2018, Version of Record 16 February 2019.",https://doi.org/10.1016/j.csda.2018.09.006,Cited by (2),"For ==== two-level factors, designs comprising full replicates with runs in blocks of size two are investigated. The minimum number of replicates for estimation of all main effects and two-factor interactions is established and a construction method is developed based on replicate generators. Complete design classes are given in the minimum number of replicates for ====. Designs in full replicates are used as root designs to obtain designs in fractional ==== replicates, again to estimate main effects and two-factor interactions, and designs are recommended for ====. Guidance is given on design construction when only a subset of the interactions is of interest."," gives a clear description of the use of designs with blocks of size two in such experiments.====Incorporating blocks of size two in a design is costly with respect to resources. For a ==== experiment comprising replicates arranged in this way, ==== establish that ==== replicates are needed to estimate all factorial effects. ==== focus on estimation of main effects and two-factor interactions and construct designs for ====. ==== and ==== seek designs to estimate all main effects and two-factor interactions for general ====. ==== replicates and ==== replicates, where ==== denotes ====. ==== gives a construction in ==== replicates, where ==== denotes ====, and confirms by computer searches that designs cannot be obtained in fewer replicates for ====. Related work in ==== has a small number of full replicate designs.====There is relatively little work on designs in fractional replicates in blocks of size two. Key work includes ====, ====, ==== and ====. In these papers it is assumed that interactions involving three or more factors are negligible. The same assumptions are made for fractional designs in this work. ==== and ==== give designs in half replicates. ==== provides constructions in ==== and ==== fractional replicates. ==== estimate effects of interest by combining information from half factorial replicates for ====.==== and ==== blocked ==== replicates, with blocks of size two, which enable estimation of all main effects and two-factor interactions. A commonality between the process and the ==== construction is the use of replicate generators. The novelty of this work is in the representation of designs in ==== replicates by ==== matrix. The DCCP operates by seeking replicate generator matrices with specific properties regarding the columns. If these properties are achieved then the ==== rows of the matrix correspond to ==== replicates such that each main effect and each two factor interaction is estimable from at least one replicate. The approach has several advantages: for given ==== and ==== the number of potential designs is much smaller than that checked by the computer search of ====; the method of design generation is systematic and incorporates recognition of isomorphic designs to avoid double counting, features not shared by other methods. These properties enable complete sets of designs in ==== replicates to be identified for given ====. As a further benefit of the approach, the structure of the underlying ==== matrix gives insight into the range of design properties, for given ==== pair.====Unless ==== is small, the number of runs required for several replicates is likely to exceed the resources available. The need to limit the number of runs is taken into account of in two ways, which can be combined. First, restricting estimation of the interactions to a subset of the two-factor interactions may allow the use of fewer replicates. Second, designs in ==== blocked ==== replicates from which all effects of interest are estimable can be constructed. Both approaches can keep the number of runs to a manageable number.====Fundamental concepts are introduced in Section ====. The DCCP is developed in Section ==== and it is established that the number of replicates used in the ==== in Section ==== using Matlab R2017a. Complete design classes do not appear to have been produced previously for ====. Guidance is given on construction of designs in fewer replicates to estimate all main effects but only selected interactions in Section ====. In Section ==== and Section ==== designs in blocked ==== replicates are used to form designs in blocked ==== replicates and a systematic approach exploits the availability of complete design classes to yield up to four fractional designs for given ====, with designs involving successively larger ====, i.e. smaller fractions, and larger ====. Designs are recommended for ====The following is the Supplementary material related to this article. ",Two-level factorial and fractional factorial replicates in blocks of size two,https://www.sciencedirect.com/science/article/pii/S0167947318302342,May 2019,2019,Research Article,535.0
"Lee Hangsuck,Ahn Jae Youn,Ko Bangwon","Department of Actuarial Science/Mathematics, Sungkyunkwan University, Seoul, Republic of Korea,Department of Statistics, Ewha Womans University, 11-1 Daehyun-Dong, Seodaemun-Gu, Seoul 120-750, Republic of Korea,Department of Statistics and Actuarial Science, Soongsil University, Seoul, Republic of Korea","Received 25 September 2017, Revised 9 August 2018, Accepted 12 September 2018, Available online 25 September 2018, Version of Record 16 February 2019.",https://doi.org/10.1016/j.csda.2018.09.004,Cited by (1),"In this paper, we intend to develop a consistent methodology for constructing multiple decrement tables under generalized fractional age assumptions. Assuming that decrements have a common distribution at fractional ages, we derive conversion formulas to split or merge given multiple decrement tables in order to obtain a new multiple decrement table of interest. The assumptions that we consider are quite general, with a wide range of fractional age assumptions including the uniform distribution of decrements or the constant forces of decrement. Our proposed approaches allow us to directly obtain multiple decrement tables without the need for the associated single rates of decrement. They will also enable us to avoid potential inconsistency under the uniform distribution assumptions or unnaturalness arising from the constant forces assumption. In addition, as they navigate through a larger window, they will deepen our understanding of the classical results under the uniform distribution assumptions. Although our methodology is based on a "," and ====, or a survival analysis textbook, such as ====However, in the real world, such a situation would not be so simple. These days, in the actuarial context, competition drives insurance companies toward introducing new contracts with more complicated and enhanced benefits that are contingent upon various causes of decrement. In order to evaluate such contracts, it may be necessary to combine several multiple decrement tables to obtain a new multiple decrement table of interest. However, as will be explained in further detail throughout the paper, the classical approach based on the uniform distribution assumptions may result in some inconsistency between the assumptions during the process of splitting and merging the decrement tables. To briefly get an idea of inconsistency, readers may want to see ====, where the acronyms are explained later. In addition, the approach based on the constant forces of decrement assumption looks somewhat unnatural because the forces of decrement are expected to increase rather than remain constant within each year. In order to resolve such inconsistency or unnaturalness, this paper intends to develop a consistent methodology for constructing multiple decrement tables under a more generalized framework. Our proposed approaches will not require more distributional information about decrements than the classical approaches, either.====Our methodology is developed based on the assumption of a common distribution of decrements (CDD) proposed by ==== such that ====
 holds for integer age ====, ==== and all causes (====). Although the CDD assumption is motivated by the fractional independence (FI) assumption in ====, it differs from the FI assumption since the function ==== reflects the age dependence, and the main focus lies in the multiple decrement model. At first glance, the CDD assumption might appear to be very restrictive, but it is in fact quite general. As shown in ====, the CDD assumption includes the uniform distribution and the constant force assumptions as special cases. Moreover, it is the most general form of assumption under which the classical conversion formula (Eq. (10.5.12) of ====) between multiple decrement probabilities and the associated single rates of decrement can hold. See Figure 1 and Section 3 of ====. This means that, when applying our approaches under the CDD assumptions, knowing the specific form of the function ==== is not required, since the conversion formulas do not depend upon it.====Although practitioners are often faced with constructing multiple decrement tables, only a limited number of studies have been carried out on this topic, and even these have focused on generalizing the fractional age assumptions of a single life table rather than those of a multiple decrement table. For instance, ==== introduced the FI assumption to generalize the actuarial present value calculation under the assumption of a uniform distribution of deaths (UDD). ====, ==== discussed some of the limitations of the classical fractional age assumptions and introduced a family of distributions at fractional ages under a single life model. Similar issues have also been addressed in ==== as well as in ====. Within the multiple decrement context, ==== showed that Riemann–Stieltjes integration is useful in calculating the multiple decrement probabilities when the force of decrement is infinite. Recently, ==== proposed the CDD assumption in their search for the most general assumption that retains the same conversion formula under the MUDD or the assumption of constant forces of decrement.====In this paper, building on ====. Thus, our methods can be appropriately used in those situations.====The rest of the paper is constructed as follows. In Section ====, we define our symbols and state our assumptions. In Sections ====, ====, we develop our approaches under the two versions of the CDD assumption. In Section ====, we provide an error analysis through numerical examples, and in Section ====, we discuss some miscellaneous issues we faced when applying our approaches. Finally, Section ==== concludes the paper with a brief discussion on the classical approach.",Construction of multiple decrement tables under generalized fractional age assumptions,https://www.sciencedirect.com/science/article/pii/S0167947318302329,May 2019,2019,Research Article,536.0
"He Kevin,Kang Jian,Hong Hyokyoung G.,Zhu Ji,Li Yanming,Lin Huazhen,Xu Han,Li Yi","Department of Biostatistics, School of Public Health, University of Michigan, United States,Department of Statistics and Probability, Michigan State University, United States,Department of Statistics, University of Michigan, United States,School of Statistics, Southwestern University of Finance and Economics, China","Received 9 December 2017, Revised 18 August 2018, Accepted 6 September 2018, Available online 22 September 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.csda.2018.09.001,Cited by (7),"Modern bio-technologies have produced a vast amount of high-throughput data with the number of predictors far greater than the sample size. In order to identify more novel biomarkers and understand biological mechanisms, it is vital to detect signals weakly associated with outcomes among ultrahigh-dimensional predictors. However, existing screening methods, which typically ignore correlation information, are likely to miss weak signals. By incorporating the inter-feature dependence, a covariance-insured screening approach is proposed to identify predictors that are jointly informative but marginally weakly associated with outcomes. The validity of the method is examined via extensive simulations and a real data study for selecting potential genetic factors related to the onset of multiple myeloma.","Rapid biological advances have generated a vast amount of ultrahigh-dimensional genetic data. Extracting information from these data have become a major driving force for the development of modern ==== in the last decade.==== proposed sure independence screening (SIS) for selecting variables from ultrahigh-dimensional data. The essence of this approach is to select variables with strong marginal correlations with the response. Much research has been inspired thereafter. ==== and general single-index models ====.====Though varied in many contexts, these methods are based on the marginal associations of individual predictors with the outcome; i.e. they assume that the true association between the individual predictors and outcomes can be inferred from their marginal associations. The condition, however, is often violated in practice. As marginal screening methods ignore inter-feature correlations, they tend to select irrelevant variables that are highly correlated with important variables (false positives) and fail to select relevant variables that are marginally unimportant but jointly informative (false negatives).====Because of these limitations, there has been a surge of interest in conducting multivariate screenings that account for inter-feature dependence: ==== developed a partial correlation based algorithm (PC-simple); ==== proposed a sequential approach (Tilting) that measures the contribution of each variable after controlling for the other correlated variables; ====Conceptually, multivariate screenings have been appealing. However, the computational burden increases substantially with the number of ====. Although simplifications, such as PC-simple and Tilting, have been applied to improve the computational efficiency in ultrahigh-dimensional cases, they may not adequately assess the true contribution of each covariate.====For adequately assessing the association of each covariate with the response, while maintaining computational feasibility, this paper presents a covariance-insured screening (CIS). Leveraging the inter-feature dependence, the proposed approach is able to identify marginally unimportant but jointly informative features that are likely to be missed by conventional screening procedures. In our methodological development, we have relaxed marginal correlation conditions that have often been assumed in the literature. Without such restrictive assumptions, we can still produce consistent results for variable selection in ultrahigh-dimensional situations. Moreover, the proposed method is computationally efficient and suitable for the analysis of ultrahigh-dimensional data.====The remaining article is organized as follows. In Section ====, we provide some requisite preliminaries and describe our proposed method in Section ====. We then compare it with existing methods in Section ====. In Section ====, we study the theoretical properties and propose a procedure for selecting tuning parameters. Finite-sample properties are examined in Section ====. We apply the proposed method to analyze multiple myeloma data in Section ====. We conclude with a discussion in Section ====. All technical proofs have been deferred to ====.==== We move on to study the third term in ==== and show that, under Condition (A7) and (A9), the third term is negligible compared to the first term. To proceed, we first reproduce a result from Lemma 14.9 of ==== for the sake of readability.====The following lemma provides the ground for the proof of Step 1.3.",Covariance-insured screening,https://www.sciencedirect.com/science/article/pii/S0167947318302202,April 2019,2019,Research Article,537.0
"Zhu Rui,Ghosal Subhashis","North Carolina State University, Raleigh, USA","Received 22 March 2018, Revised 7 September 2018, Accepted 8 September 2018, Available online 21 September 2018, Version of Record 16 February 2019.",https://doi.org/10.1016/j.csda.2018.09.003,Cited by (4),"The Receiver Operating Characteristic (ROC) surface is a ==== of the ROC curve and is widely used for assessment of the accuracy of diagnostic tests on three categories. Verification bias occurs when not all subjects have their labels observed. This is a common problem in disease diagnosis since the gold standard test to get labels, i.e., the true disease status, can be invasive and expensive. The same situation happens in the evaluation of semi-supervised learning, where the ==== are incorporated. A ","The Receiver Operating Characteristic (ROC) curve analysis has been widely used as an effective tool in measuring the accuracy of diagnostic tests in the two-class classification problem. It shows a tradeoff between sensitivity and specificity by varying the cut-off point through all possible values of the diagnostic marker. Recently some surfaces related to the ROC curve have been proposed for practical use ====, ====. Let ==== and ==== be continuous measurements from three different classes, ==== are measurements from Class 0, ==== are measurements from Class 1, and ==== are measurements from Class 2. Suppose that the ordering of interest for these three classes is ====. A decision rule that classifies subjects can be defined by using two ordered threshold points ====, i.e. choose Class 0 when a measurement is less than ====, choose Class 1 when it is between ==== and ====, and choose Class 2 otherwise. This will result in three True Class Fractions (TCFs): ====Varying ==== and ==== will give us a set of TCFs. To construct the ROC surface, we plot ==== in a three-dimensional coordinate system. The functional form of the ROC surface can be obtained by expressing ==== as a function of ==== given by ==== ====.==== ====. The VUS is given by ====see ====.====The ROC surface has been used in diagnostic medicine when the disease has two phases, for example, the early phase and late phase of a progressive disease. The symptoms in the early phase may be mild and ignorable, while the late phase tends to have more severe symptoms. Clearly, there is an inherent ordering between healthy, early phase diseased and late phase diseased. One example is Alzheimer’s disease, which can be graded low, intermediate and high according to the progress of the disease ====. Different medical treatments should be applied to different phases. The treatment for the late phase of the disease can be expensive and invasive to patients, even requiring surgeries, while the treatment for the early phase can be conservative. This necessitates the identification of the two phases of the disease and thus leads to the consideration of three classes. We assume, without loss of generality, that a higher test value indicates a higher level of disease.====Many methods have been proposed for estimating the ROC surface. The naive method is to plug-in the empirical estimates of the distribution functions. ====.====, ====, ====, where ==== denotes the normal distribution with mean ==== and variance ====. ==== proposed using the Box–Cox transform. ==== introduced two semi-parametric estimators of the ROC surface by extending the methods of ==== and ==== for estimating the ROC curve.====In this paper, we propose a new semiparametric method for estimating the ROC surface. This is a generalization of a Bayesian method for estimating the ROC curve using a rank-based likelihood (BRL) introduced by ====. We assume that, under some kind of unknown strictly monotone increasing transformation, the measurements follow three different normal distributions. Since ranks are invariant under a strictly monotone increasing transformation, exploiting the rank-likelihood eliminates the need for estimating the unknown transformation, and enable us to construct a Bayesian estimator of ROC surface.====We shall also consider the possibility of verification bias in the data which widely occurs in practice but is rarely considered in the existing literature. That is to say, the true labels of the subjects are partially missing. In the biomedical setting, the label of a subject refers to the true disease status, which is verified only through a very accurate existing diagnostic test called a gold standard test. However, a gold standard test is usually very expensive and can even be invasive, so the common practice is to apply it only on high-risk subjects identified through a screening test. Because patients who are marked as low-risk are more likely to have their true disease status missing, simply ignoring this ==== and estimating the ROC surface using only subjects with verified disease status may lead to biased results, and will cause a loss of efficacy as well.====. In the evaluation of these models, we also want to incorporate the unlabeled samples for efficacy.====So to deal with the missing labels, the commonly used missing at random (MAR) assumption ====The existing literature is very limited for this problem. ==== considered this problem for continuous diagnostic tests. They proposed several bias-corrected estimators of TCFs and thus constructed several bias-corrected ROC surfaces. These methods are extensions of Full Imputation (FI), Mean Score Imputation (MSI), Inverse Probability Weighted (IPW), Semi-Parametric Efficient (SPE) estimators for the ROC curves in ====Through some modifications, our method for estimating the ROC surface can be extended to the setting under verification bias. This can also be regarded as a generalization of the ROC curve estimation under verification bias proposed by ==== dependent on the covariates, and should lead to more efficient labeling. However, we do not pursue this proposed method in this paper.====The paper is organized as follows. The methodology is described in Sections ====, ====. Extensive simulation studies are conducted in Section ====. The proof of the posterior consistency result is given in the appendix.====The following lemma gives the probability distribution of the disease status when unobserved, conditional on the latent measurement ==== introduced in the trinormal model.====The following lemma will be needed in the proof of the consistency theorem.",Bayesian Semiparametric ROC surface estimation under verification bias,https://www.sciencedirect.com/science/article/pii/S0167947318302317,May 2019,2019,Research Article,538.0
"Chen Ying,Niu Linlin,Chen Ray-Bing,He Qiang","Department of Statistics and Applied Probability, Risk Management Institute, National University of Singapore, Singapore,Wang Yanan Institute for Studies in Economics, Xiamen University, China,Department of Statistics, National Cheng Kung University, Taiwan,Department of Statistics and Applied Probability, National University of Singapore, Singapore","Received 19 March 2018, Revised 27 July 2018, Accepted 31 August 2018, Available online 21 September 2018, Version of Record 16 February 2019.",https://doi.org/10.1016/j.csda.2018.08.027,Cited by (0),"We propose a Sparse-Group ==== of the loading matrix estimator, demonstrate its finite sample performance with simulation studies, and illustrate its application using the daily US Overnight Index Swap rates from Oct 2011 to Mar 2015 with 15 maturities ranging from 1 week to 30 years. With higher efficiency of extracting factors, the forecasting performance of the SG-ICA is remarkably better than the popular ==== DNS model in an era of quantitative easing with short-term interest rate being close to zero."," and ====, on the contrary, not only extracts factors through a linear projection but also provides statistical independence among the factors, based on which further investigations can be easily carried out in the univariate space.====The independent components can be estimated using various approaches, including maximizingnon-Gaussianity ====, ====, ====, minimizing mutual information ====, ====, ====, maximizing likelihood ====, ====, ====, minimizing distance covariance ====. Moreover, the ICA model has been extended to non-linear ICA ====, kernel ICA ====, nonparametric ICA ====, ==== and penalized ICA ====.====.====For a standard linear regression analysis with observed real-valued scalar response and a large number of covariates, many penalty choices are available for variable selection, including the Lasso ====, Ridge ====, the smoothly clipped absolute deviation penalty ==== and the adaptive lasso ====. ==== proved that the ====, ====, ====, ====. ==== that incorporates two-layer regularization, eliminating both insignificant covariates and insignificant coefficients of regression simultaneously, see also ====, and ====. A primary challenge in the regularized ICA model is that we do not observe the covariates and ICs are latent.==== proposed a sparse ICA on the signals, see also ==== by combining a sparse method and kernel ICA and ==== for implementation on image separation. ==== built sparsity on the mixing matrix that is also assumed to be generic. ==== adopted the adaptive Lasso penalty for ICA.====In our study we assume sparsity on the loading matrix, i.e. the inverse of the mixing matrix and develop a Sparse-Group Independent Component Analysis (SG-ICA) method, aiming to extract statistically independent factors under sparse group assumption. Compared to the existing work e.g. ==== and ====The reminder of the paper is structured as follows. Section ==== illustrates the usefulness and robustness of the SG-ICA model in practically oriented simulation studies. In Section ====, we implement the SG-ICA to the US overnight index swap rates and show its application in out-of-sample prediction. Section ==== provides concluding remarks. All of the theoretical proofs are contained in the ====.",Sparse-Group Independent Component Analysis with application to yield curves prediction,https://www.sciencedirect.com/science/article/pii/S0167947318302184,May 2019,2019,Research Article,539.0
"Lee Myeonggyun,Jung Inkyung","Division of Biostatistics, Department of Population Health and Environmental Medicine, New York University School of Medicine, 180 Madison Ave, New York, NY10016, United States,Division of Biostatistics, Department of Biomedical Systems Informatics, Yonsei University College of Medicine, 50-1 Yonsei-ro, Seodaemun-gu, Seoul 03722, Republic of Korea","Received 30 November 2017, Revised 13 July 2018, Accepted 13 September 2018, Available online 21 September 2018, Version of Record 16 February 2019.",https://doi.org/10.1016/j.csda.2018.09.005,Cited by (4),Spatial scan statistics are widely used as a technique to detect geographical disease clusters for different types of data. It has been pointed out that the Poisson-based spatial scan statistic tends to detect rather larger clusters by absorbing insignificant neighbors with non-elevated risks. We suspect that the spatial scan statistic for ,"Spatial scan statistics are widely used to detect spatial disease clustering in different types of data, such as Bernoulli ====, Poisson ====, ordinal ====, ====, multinomial ====, survival ====, and continuous ====, ==== data. For geographical surveillance, these spatial scan statistics have been applied in various fields, e.g., to find spatial clusters for birth defects ====, to detect high-risk areas for leprosy in Bangladesh ====, and to identify geographical patterns with high or low rates for cancer disease ====, ==== ==== as well as geographical clustering in sugar-sweetened beverage consumption for among Boston youth ====.==== and irregular ====, ====, ====The most commonly used model is the Poisson-based spatial scan statistic for geographic disease surveillance using incidence or mortality data. Although the method has been widely used, one drawback has been pointed out that the Poisson-based spatial scan statistic tends to detect an unrealistically larger cluster than the expected true cluster by absorbing adjacent regions with irrelevant risk ====, ====. Furthermore, in the case of irregularly shaped clusters, the spatial scan statistic using the circular scanning window, a general approach to find spatial clusters, has difficulty in correctly detecting the true clusters; thus, the over-detection phenomenon can occur ====. To resolve this undesirable tendency, ==== proposed a modified Poisson-based spatial scan statistic using a restricted likelihood ratio for scanning only regions with elevated risk. His modified method worked well for preventing such undesirable phenomena in detecting clusters compared with the original spatial scan statistic.====In this paper, we focus on spatial cluster detection for ordinal outcome data. Ordinal scaled data, such as cancer stage or grade, are often obtained in nature, especially in the medical field. At this time, we are interested in geographical cluster detection of high rates of more severe categories (e.g. later stage or high grade in cancer disease). Two spatial scan statistics have been proposed based on likelihood ratio ordering ==== and stochastic ordering ====, by ====, and by ====. A recent study by ==== proposed a criterion for optimizing the maximum reported cluster size for ordinal-based spatial scan statistic. Their simulation study results showed that the ordinal-based spatial scan statistic with circular windows tends to detect a larger cluster than the true cluster especially when the true cluster is non-circular.====The purpose of this study is to propose modified spatial scan statistic using a restricted likelihood ratio to circumvent the over-detection problem in ordinal outcome data as ==== took individual region’s risk into account in the Poisson-based spatial scan statistic. While the idea of screening each region using the restricted likelihood ratio is similar to ====, an appropriate criterion should be applied to the ordinal model and the proposed method should be carefully evaluated. Our proposed method can prevent absorbing insignificant neighbors by scanning only significant regions through the restricted likelihood ratio. In Section ====. In Section ====, we evaluate the performance of the proposed approaches for a comparison with the original methods through a simulation study. We illustrate application of the proposed methods to real data in Section ==== and present a conclusion of our study in Section ====.====The following is the Supplementary material related to this article. ",Modified spatial scan statistics using a restricted likelihood ratio for ordinal outcome data,https://www.sciencedirect.com/science/article/pii/S0167947318302330,May 2019,2019,Research Article,540.0
"Kirschstein Thomas,Liebscher Steffen,Pandolfo Giuseppe,Porzio Giovanni C.,Ragozini Giancarlo","School of Economics and Business, Martin Luther University Halle-Wittenberg, Gr. Steinstrasse 73, D-06108 Halle, Germany,University of Cassino and Southern Lazio, Italy,University of Naples Federico II, Italy","Received 12 November 2017, Revised 29 August 2018, Accepted 31 August 2018, Available online 20 September 2018, Version of Record 16 February 2019.",https://doi.org/10.1016/j.csda.2018.08.028,Cited by (3)," of a given size while the sMST seeks for a well-separated subset. Finally, the robust estimators are compared with respect to the max-bias and to the bias under shift outlier scenarios by means of an extensive simulation study. The results indicate that –in contrast to linear data– the ==== shows high robustness in terms of the finite-sample max-bias. However, robust estimators are clearly superior to the maximum likelihood estimator in shift outlier contamination schemes.","Directional data, i.e. unit vectors on the surface of the ====-dimensional unit sphere ==== for ====, occur when observations are recorded as directions. They arise in many scientific fields such as geology, meteorology and biology — just to name a few. Most applications are for ==== (circular data) or ==== (spherical data), but applications in higher dimensions can also be found, e.g. in gene-expression analysis (see ====), text mining (see ==== and ====, ====.====While a vast body of literature exists on robust estimation of location for data in the multivariate space ====-type estimators. ==== give an overview of estimators falling into the former category which contains the spherical median (due to ==== as the most prominent representatives. Estimators falling into the latter category have been proposed by ==== (limited to data on the circle) and ==== (suitable for higher dimensional data). ==== also showed that median-type estimators fit into the ====-estimator framework if the ====-function is properly chosen. Quite recently ==== introduced some new ====-type estimators able to robustly estimate both the location and concentration parameters at the same time.====Given that the bias is bounded for a bounded parameter space, classic robustness concepts like influence function or breakdown point are not directly applicable for directional data. Therefore, ====. ==== assessed the SB-robustness, consistency, and asymptotics of general ====-estimators for directional location and dispersion. In ==== and ==== the contamination rate was kept fixed and the concentration parameters of the samples were varied, ==== studied the inverted setting (various contamination rates, fixed concentration parameter).====In this work, we compare robust directional location estimators in terms of maximum bias, a way of measuring robustness which is well-known in the literature, but so far has hardly been used for directional data (see e.g. the final discussion in ====). We pursue a finite-sample approach and conduct an empirical evaluation by means of an extensive simulation study.====In addition to the worst case perspective taken by the max-bias concept, we also compare directional location estimators by investigating the bias occurring in a shift outlier model. As subsample-based estimators have been shown to be very efficient for such a contamination scheme in linear space, we additionally introduce two estimators belonging to this class. More specifically, the spherical Minimum Covariance Determinant estimator (sMCD) and the spherical ==== estimator (sMST) are developed in analogy with the corresponding linear location estimators.====. In Section ==== robust directional location estimators are briefly reviewed and two new estimators are presented. Section ==== discusses the max-bias from a sample-based perspective. In Section ====In order to find the subset with minimum spherical variance (either ==== or ====), the following proposition proves that the sMCD procedure converges to the (locally) densest subset of a given size. I.e., for each starting solution the sMCD procedure converges to a local optimum.====Before the Proposition is stated more notation is introduced. Let ==== be a set of points ====. Given their sample mean vector ====the mean direction ==== is defined as: ====The Euclidean norm of the mean vector is called mean resultant length, denoted as ====. That is, ====It plays a role in measuring the dispersion of spherical data sets. More specifically the sample spherical variance depends on ==== and can be alternatively defined as: ====or ",On finite-sample robustness of directional location estimators,https://www.sciencedirect.com/science/article/pii/S0167947318302196,May 2019,2019,Research Article,541.0
"Chowdhury Shrabanti,Tiwari Ram C.,Ghosh Samiran","Department of Family Medicine & Public Health Sciences, USA,Center of Molecular Medicine and Genetics, Wayne State University, USA,Division of Biostatistics, CDRH, FDA, USA","Received 13 November 2017, Revised 22 August 2018, Accepted 22 August 2018, Available online 15 September 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.csda.2018.08.018,Cited by (6),"Three-arm non-inferiority (NI) trial including the experimental treatment, an active reference treatment, and a placebo where the outcome of interest is binary are considered. While the risk difference (RD) is the most common and well explored functional form for testing efficacy (or effectiveness), however, recent FDA guideline suggested measures such as relative risk (RR), odds ratio (OR), number needed to treat (NNT) among others, on the basis of which NI can be claimed for binary outcome. Albeit, developing test based on these different functions of binary outcome are challenging. This is because the construction and interpretation of NI margin for such functions are non-trivial extensions of RD based approach. A ==== test based on traditional fraction margin approach for RR, OR and NNT are proposed first. Furthermore a conditional testing approach is developed by incorporating assay sensitivity (AS) condition directly into NI testing. A detailed discussion of sample size/power calculation are also put forward which could be readily used while designing such trials in practice. A clinical trial data is reanalyzed to demonstrate the presented approach.",", ====, ==== and references (e.g. ====, ====, ====, ====). NI trials may or may not include a placebo arm due to ethical reasons. Two-arm placebo-free NI trials make two important assumptions regarding Assay Sensitivity ====, ==== and Constancy and depend heavily on external validations ====, ==== and several other limiting factors as specified in ====. To alleviate some of these issues and if ethically acceptable and practically feasible, it is recommended by ==== to include a placebo arm in the current trial, resulting in a three-arm “gold-standard” design that has greater confidence concerning AS and lesser concern related to external validity.==== first proposed the fraction margin approach, where NI margin is adaptively formulated as the pre-specified negative fraction of the unknown effect size of the reference treatment over placebo in the current three-arm trial. ==== extended this approach for the binary outcome for risk difference (RD). While RD is the simplest functional form for binary outcomes, as mentioned in the recent FDA guidance (====, Page 24) there are other functionals (e.g., risk ratio (RR), odds ratio (OR), number needed to treat (NNT), risk reduction, etc.) which could also be used to test the treatment effect ==== and claim NI. Under the NI setup, there exists published work for odds ratio using Frequentist’s approach for a two-arm trial, see for example ==== and ====, but no work exists for three-arm trials. Also, to the best of our knowledge for RR and NNT ====The rest of the article is organized as follows. In Section ====, we give the NI hypothesis and the details of NI margin. We show the non-uniqueness of the NI margin for different functionals. In Section ====, we discuss the existing methods and propose a conditional Frequentist’s method for testing NI. In Section ====, we apply our proposed methods on a published clinical trial dataset. We conclude the article with discussions in Section ====. All proofs are provided in supplementary file for brevity purpose.====The following is the Supplementary material related to this article. ","Non-inferiority testing for risk ratio, odds ratio and number needed to treat in three-arm trial",https://www.sciencedirect.com/science/article/pii/S0167947318302019,April 2019,2019,Research Article,542.0
"García Treviño E.S.,Alarcón Aquino V.,Barria J.A.","Department of Electrical and Electronic Engineering, Imperial College London, London, SW7 2AZ, United Kingdom,Universidad de las Américas Puebla, Puebla, Mexico","Received 14 August 2017, Revised 23 August 2018, Accepted 24 August 2018, Available online 14 September 2018, Version of Record 3 October 2018.",https://doi.org/10.1016/j.csda.2018.08.021,Cited by (2),"The estimation of ==== is one of the fundamental problems in scientific research. It has been shown that Wavelet Density Estimators, which are a well-documented nonparametric approach, outperform other nonparametric estimators in problems involving densities with discontinuities and local features. However, the use of this type of estimators is not widely extended in the scientific community mainly because of their heavy ==== and their difficult algorithmic implementation. A novel multidimensional Wavelet Density Estimator approach based on new multidimensional scaling functions with analytic closed-form expressions is proposed. The key advantages of the proposed estimator are its simpler multidimensional algorithmic implementation and its significant reduction in computational complexity. Algorithmic formulations for four different data analysis scenarios are presented: (1) batch processing of input data, (2) online estimation for ====, (3) online estimation for non-stationary contexts and (4) batch estimation of high-dimensional data. The assessment results show that the proposed approach reduces the computational time of the estimation process while maintaining competitive estimation errors.",".==== and ==== and the thresholding method that exploits the non-stationary variance structure of wavelet coefficients proposed by ====.====Among recent work in the context of wavelet-based density estimation, relevant approaches are the estimator developed by ==== within the context of stratified sized-biased data and the Maximum Likelihood Wavelet Density Estimator (MLWDE) proposed by ==== and extended in ====. The second relevant work in this context is the sliding-window approach reported in ====. Note that even though more complex algorithms have been proposed to obtain bona fide densities, such as the estimation of the square root of a density ====, ==== and the use of non negative wavelets ====, the complexity of such algorithms makes the resulting estimators even computationally more expensive.====In this paper, we propose a novel WDE based on radial ====-splines functions that we call Radial Wavelet Frame Density Estimator (RWFDE), that overcomes the three issues above highlighted. On the one hand, this novel WDE approach is based on radial ====-splines scaling functions which are functions with analytic closed-form expressions that reduce the computational burden of this type of estimators. On the other hand, since the proposed radial ====The main motivation behind the work presented in this paper was to reduce the computational complexity of existing WDEs while preserving most of their well-recognized capabilities (i.e., good ====, flexibility in convergence, local manipulation of the estimated density). A key feature of WDEs, which makes them crucial in the age of Big Data, is the fact that the number of basis functions employed does not depend on the number of data points to be processed and, in that sense, they are a density estimation approach useful in domains where the application of methods such as KDE becomes prohibited. Although there is a great body of research dedicated to investigate WDEs, none of the existing approaches has considered a WDE with a wavelet frame before.====There are two main contributions of this paper. The first one is the formulation of the proposed estimator itself. Note that even though WDEs have been thoroughly studied in the literature, all available works have focused on the use of orthogonal scaling and wavelet functions. In this sense, no one has considered neither wavelet frames nor radial ====-spline scaling functions for the estimation of the underlying probability density of multidimensional data. The second contribution is the formulation of radial B-spline scaling functions, which are multidimensional extensions of ====-splines that make easier the multiresolution analysis of multidimensional functions. Results presented in this paper show the potential of the proposed estimator which, for the experiments assessed, reported similar and sometimes even better performances than WDE with a significative reduction in computational time.====The rest of this paper is organized as follows. In Section ====, we review the theoretical concepts and frameworks relevant to the RWFDE approach. In Sections ====, ====, we introduce the proposed multidimensional radial ====-splines scaling functions and the proposed RWFDE approach, respectively. In Section ====, we describe the experiments performed to evaluate the proposed methods. Finally, in Section ====, we present some final remarks about this work.====See ====, ====.",The radial wavelet frame density estimator,https://www.sciencedirect.com/science/article/pii/S0167947318302044,February 2019,2019,Research Article,543.0
"Zhang Chun-Xia,Xu Shuang,Zhang Jiang-She","School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, Shaanxi 710049, China","Received 22 January 2018, Revised 21 August 2018, Accepted 28 August 2018, Available online 13 September 2018, Version of Record 16 February 2019.",https://doi.org/10.1016/j.csda.2018.08.025,Cited by (25)," is equipped with a binary latent variable indicating whether it is important. The Bernoulli-type prior is adopted for the latent indicator variable. As for the specification of the hyperparameter in the Bernoulli prior, we provide two schemes to determine its optimal value so that the novel model can achieve "," with ==== and ====where ==== denotes ====, say, ====, via maximizing ====. Although there is no closed-form solution of ====. In high-dimensional situations, especially when ====, the estimation accuracy of ==== will be unsatisfactory because of the limited information provided by training data. Fortunately, the true model is often ==== and the prediction accuracy of the model can be significantly enhanced. In some applications, on the other hand, researchers may be more interested in identifying important variables so that the relationship between covariates and our interested outcome can be more easily revealed. Therefore, it is particularly important to perform variable selection efficiently and accurately.====, smoothly clipped absolute deviation penalty (SCAD) ==== as well as ====-norm penalty ====. These techniques can be naturally applied to handle variable selection problems in logistic regression by means of penalized IRLS algorithms. A representative but incomplete list of references includes ====, ====, ==== ==== and ====. It is worthwhile that shrinkage methods demand careful selection of their associated tuning parameters. In high-dimensional situations, it is often challenging to choose an appropriate tuning parameter so that truly important variables can be exactly detected.==== in ====, ==== or local variational Bayesian methods ====. Thereafter, the Gaussian conjugate prior becomes available. Recently, ==== is a popular way to identify important variables ====, ====, ====, ====, ====. As a matter of fact, the indicator model equips each covariate ==== with a binary latent variable ====, where ====. The covariate ==== is important if ==== being less than 0.5 are deemed as unimportant. To infer from the posterior distribution of unknown parameters such as ==== and ====, ====. Moreover, ====, ==== is large ====.==== proposed an iterated conditional modes/medians (ICMM) algorithm to select informative variables from massive candidates in linear regression models. At present, the ICMM algorithm has been extended to the situations of generalized linear regression models ====. It assumes that each regression coefficient ==== is governed by a spike and slab mixture prior, that is, ====where ==== is zero if ====, otherwise it is drawn from ====-norm penalty. The parameters ==== and ==== is similar to ICMM, with the exception that ==== by means of importance sampling. On account of ==== being a latent variable, the expectation–maximization (EM) algorithm is very suitable for indicator models. ==== developed an EM-based variable selection (EMVS) algorithm for identifying important variables in linear regression models. At present, the EMVS algorithm has been generalized to logistic regression models ====, ====. With respect to each indicator variable ====, it assigns independent and identically distributed (i.i.d.) Bernoulli priors to them, i.e., ====. In addition, a beta hyper-prior distribution is employed for ====. Despite EMVS alleviates the high computational cost of MCMC sampling to a certain degree, sometimes it may be slow due to intrinsic features of EM algorithm. Besides this, the EM algorithm may often be trapped into a local minimum. ==== suggested to solve this by applying EM algorithms several times in which the estimate obtained in last iteration is taken as the initial value in next iteration. Recently, some other variants of EM algorithms for variable selection in linear regression have been designed to avoid the local minimum ====, ====. More importantly, different from linear regression, the crux of logistic regression lies in that there is no closed-form solution in the M-step and the Newton–Raphson algorithm can be utilized. Since some iterative method needs to be used to find the corresponding solution, it is often time-consuming to apply EMVS in logistic regression ====, ====.====As an alternative of variational Bayes, the expectation propagation (EP) algorithm is also an efficient tool to approximate posterior distributions with relatively low computational burden. In this aspect, ==== developed a novel EP algorithm to implement approximate inference in linear regression models with spike and slab priors. Except for the assumed different models, their purpose is to achieve high prediction accuracy instead of accurate identification of important variables. Furthermore, ==== discussed the application of variational Bayesian method in sparse logistic regression by assuming hierarchical priors for unknown parameters. ====Motivated by the variational Bayesian method put forward by ==== to perform variable selection in linear regression models, we propose in this paper a Bayesian indicator model for logistic regression to identify important variables. In the novel model, all ==== and ==== are assumed to be independent. The indicator ==== is governed by the Bernoulli distribution ==== and each coefficient ==== is assigned with a separate shrinkage prior. As shown in later experiments (Section ====. To select an optimal value for ====The remainder of the paper is organized as follows. Section ==== describes all the details about our proposed variational Bayesian variable selection model for logistic regression. Particularly, the formulation of the model, how to make variational inference of unknown (hyper)parameters, the initialization of (hyper)parameters as well as the method to predict new data are presented in the corresponding subsections of Section ====. In Sections ====, ====, experiments are conducted with synthetic and some publicly available data to examine and compare the performance of the proposed method with several other popular techniques, respectively. Section ==== discusses the difference between our method and some other closely related counterparts. Finally, Section ==== includes the conclusions of the paper. In appendix, we present the detailed process to make inference of unknown (hyper)parameters with a variational Bayesian technique.====In this section, we provide more details about the variational inference of the (hyper)parameters which are used in their updating formula ==== and the derivation of ELBO for our model ==== in ====.",A novel variational Bayesian method for variable selection in logistic regression models,https://www.sciencedirect.com/science/article/pii/S0167947318302081,May 2019,2019,Research Article,544.0
"Hu Lixia,Huang Tao,You Jinhong","School of Statistics and Mathematics, Shanghai Lixin University of Accounting and Finance, Shanghai, China,School of Statistics and Management, Shanghai University of Finance and Economics, Shanghai, China","Received 3 April 2018, Revised 25 August 2018, Accepted 25 August 2018, Available online 13 September 2018, Version of Record 3 October 2018.",https://doi.org/10.1016/j.csda.2018.08.023,Cited by (7),"In the analysis of locally stationary process, a time-varying ====. An environmental dataset illustrating the proposed method is also considered.",", ====, ==== and  ====; and nonparametric regression methods see ====, ====, ====, ====.====, ====, ====, ====, ==== and ==== adopted tvAM and proposed a smoothed backfitting estimate ==== nonparametric function. Furthermore, the proposed estimators are oracle efficient in the sense that it has the same asymptotic property as oracle estimator as if other component functions were known in advance. The asymptotic normality of two-step estimators are considered as well.==== and ====. Extensive simulation studies show that the proposed testing procedure is powerful.====The organization of this paper is as follows. Section ====. Section ==== deals with the practical problem on the smoothing parameter selection and the testing of time-varying property of regression function. Two simulation examples presented in Section ==== investigate the finite-sample performance of the proposed methods. We also illustrate our method via an environmental dataset. The brief concluding remarks are given in Section ====. The main proofs are relegated in the Appendix ====, and some preliminary lemmas and propositions are shown in the Supplementary Materials.====In this paper, we adopt the following formal definition of locally stationary process, which is first formulated by ====.====
               ",Two-step estimation of time-varying additive model for locally stationary time series,https://www.sciencedirect.com/science/article/pii/S0167947318302068,February 2019,2019,Research Article,545.0
"Bogomolov Marina,Davidov Ori","Faculty of Industrial Engineering and Management, Technion - Israel Institute of Technology, Haifa, Israel,Department of Statistics, University of Haifa, Haifa, Israel","Received 27 September 2017, Revised 14 August 2018, Accepted 21 August 2018, Available online 10 September 2018, Version of Record 16 February 2019.",https://doi.org/10.1016/j.csda.2018.08.019,Cited by (0),In a variety of applications researchers are interested in comparing two or more naturally ordered experimental conditions after adjusting for ,", ====). Another example, which we will later analyze in more detail, is the study by ==== which examined the associations between the levels of various plasma lipids and exposure to Perfluoroalkyl substances (PFASs) in a cohort of pregnant Norwegian women. PFASs are widespread and persistent environmental pollutants used in the manufacturing of many industrial and consumer products. Exposure levels to PFASs are determined by a blood test. Typically, exposure to each PFAS is considered to be an ordered categorical variable with four levels corresponding to the four quartiles of the respective empirical distribution. There is considerable evidence in the scientific literature (cf. ====. Related papers are by ==== who develop estimation and testing procedures for ordered means in ANCOVA models in the presence of interactions, ====, ==== who discuss the ordering of distribution in the presence of covariates, and ====In all of the papers above it is assumed that a function of the mean response is linearly related to the covariates. However, in many applications, including ====, ==== and ==== and the references therein. We note that there is almost no literature on constrained inference for such models. ==== present a method for estimating treatment effects under linear equality constraints, and ====Initially we assume that our observations follow a PLM of the form ====where ==== is the outcome for ====th observation in the ====th group. Here ==== i.e., we have ==== groups, and ==== where ==== is the size of the ====th group, and ==== is the total sample size. Each outcome is associated with covariates ==== and ==== may be used to analyze data collected using various experimental set-ups. For example the design may be ====, i.e., the covariates are fixed, while the assignment to groups is random, or alternatively, the covariates are random, but the assignment to groups is fixed. For simplicity, in the body of the paper we assume a fully fixed design. A detailed account of the required assumptions in all four cases is provided in the Supplementary Information.====The vectors ==== and ==== parameterize the linear components and ==== is an unknown function from ==== to ==== Finally ==== are independent and identically distributed random errors with mean zero and a finite variance ==== Model ==== is an extension of classical ANCOVA to the setting of PLMs ====, ====, ====, ====, ====. Actually, a special case of model ====, in which ==== and ==== has been considered by ==== and ====. It is also worth noting that model ==== can be viewed as a special case of the more general model considered in ====, see Section ====.====As in parametric ANCOVA, model ==== is not identifiable. In particular note that for each ==== and ==== the sum ====, e.g., set ==== Our main interest is testing for an ordering of the components of ==== In other words we are interested in the differences ====
 ====, rather than the actual values of the components ====. Thus an appropriate simple constraint is to fix ==== we use the standard assumption that the matrix of covariates is of full rank. Thus in the following we define ==== for ==== and ==== which have the same covariates.====with at least one strict inequality under ====. The ordering of the means under the alternative in ==== is known as the simple order. ==== discuss methods for testing ==== when ====, i.e., when model ==== reduces to a standard ANCOVA. Here, we generalize their methods to the case where there is a nonparametric component in the model. In fact, our methodology can be used for testing any set of hypotheses which can be formulated as ====versus ==== where ==== is a linear space and ==== are independent and identically distributed then ==== implies that ==== for all ====, i.e., a conditional stochastic ordering. See ==== for a related problem.====The paper is organized as follows. Unconstrained estimation is discussed in Section ====. In Section ==== we develop methods for order restricted estimation and testing. We illustrate the performance of the proposed method using simulations in Section ==== and apply it to a real data example from the study of ==== in Section ====. In Section ==== we provide a brief discussion and some concluding remarks. All proofs are collected in Supplementary Information. In addition the Supplementary Information lists the assumptions required for the validity of the theoretical results, extends the methodology to multivariate outcomes and contains additional details and full results of our simulation study.====The following is the Supplementary material related to this article. ",Order restricted univariate and multivariate inference with adjustment for covariates in partially linear models,https://www.sciencedirect.com/science/article/pii/S0167947318302020,May 2019,2019,Research Article,546.0
"Abpeykar Shadi,Ghatee Mehdi,Zare Hadi","Department of Computer Science, Amirkabir University of Technology, Tehran, Iran,Faculty of New Sciences and Technologies, University of Tehran, Tehran, Iran","Received 7 September 2017, Revised 15 August 2018, Accepted 19 August 2018, Available online 7 September 2018, Version of Record 5 November 2018.",https://doi.org/10.1016/j.csda.2018.08.015,Cited by (31)," on high-dimensional datasets it can be shown that the proposed solution has outperformed the state of the art classifiers. Furthermore, the computational complexity and the convergence of this method are theoretically proven and the robustness analysis under noisy conditions is conducted.",". In addition, these challenges were expanded to 4-V’s and 5-V’s by adding Veracity and Value ====. Recently, multitudes of works considered yjfls2506the challenges of big data in each of the aforementioned aspects ====, ====, ====, and ====, which helps to deal with high-dimensional features ====. The filters are independent from the classifiers and the learning stage. They are based on statistical and information theoretic criteria to select more relevant and less redundant features ====. Some researchers proposed some hybrid techniques to exploit the properties of wrapper and filter approaches simultaneously ====. Generally, the filter techniques have more speed but less accuracy than the wrapper ones. Since both of training accuracy and redundancy should be improved, a hybrid method is proposed in this paper. Furthermore, in some of the traditional methods, the irrelevant features are removed but redundant features are not considered, which failed in handling features structures ====. However, handling the feature structure and making feature clusters can improve the classification performance ====, ====, ====, ====, ====, ====, ====. Therefore, this paper focuses on feature clustering rather than feature selection.====, ====, ==== have been utilized. However, the latter strategy covers some developments of learning algorithms to accelerate the classification time and to achieve more accurate results. ====, ==== and ====, ====.==== and ====The rest of this paper is organized as the following. Section ==== introduces some fundamental concepts. Section ==== describes the components of our proposed classifier. The performance analysis is presented in Section ====. Section ==== is devoted to the efficiency analysis on some benchmark datasets. Finally, Section ==== concludes the paper with discussions.",Ensemble decision forest of RBF networks via hybrid feature clustering approach for high-dimensional data classification,https://www.sciencedirect.com/science/article/pii/S0167947318301981,March 2019,2019,Research Article,547.0
"Cui Xia,Zhao Weihua,Lian Heng,Liang Hua","School of Economics and Statistics, Guangzhou University, Guangzhou, China,School of Sciences, Nantong University, Nantong, China,Department of Mathematics, The City University of Hong Kong, Kowloon Tong, Hong Kong,Department of Statistics, George Washington University, Washington, DC, USA","Received 1 January 2018, Revised 29 June 2018, Accepted 20 August 2018, Available online 7 September 2018, Version of Record 17 September 2018.",https://doi.org/10.1016/j.csda.2018.08.017,Cited by (0)," with dynamic (time-varying) component functions. We allow some of the component functions to be non-dynamic, and show, as expected but technically nontrivially, that estimators of the non-dynamic functions have a faster convergence rate. A penalization-based method, called dynamic structure pursuit, is proposed to automatically identify these non-dynamic functions. Finally, in the sparse setting, a four-stage estimation procedure is proposed which first identifies the nonzero component functions and then applies the identification strategy of the non-dynamic functions. Theoretical and numerical results are provided to illustrate the performance of the estimators."," the response, ==== the predictors, one assumes ====where ==== is the intercept parameter, ==== are unknown component functions, and ==== represents the mean zero noise.====In the early literature, kernel-based backfitting and local scoring procedures have been proposed by ==== to iteratively estimate the nonparametric components by solving a large system of equations ====. ==== applied the marginal integration approach ====, ==== and ==== suggested penalized regression splines, which share most of the practical benefits of smoothing spline methods combined with ease of use and reduction of the computational cost of backfitting generalized additive models (GAMs). But no theoretical justifications are available for these procedures.====To overcome these limitations, ==== proposed to estimate the nonparametric components by polynomial splines ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====. After the spline basis is chosen, the coefficients can be estimated by an efficient one-step procedure of maximizing the quasi-likelihood function. Thus the gain of the proposed in terms of computational reduction is remarkable in contrast to alternative estimation methods. In addition, the proposed procedure can easily formulate a penalized functional to implement variable selection. See ==== for more details.====, ====, ====, ====.====Here we consider functional/longitudinal data model, allowing both responses and predictors to be functional. ==== proposed the following dynamic time-varying model: ====, this reduces to the conventional additive model. The model achieves dimension reduction while being able to capture potential dynamic relations of functional/longitudinal predictors and responses. ====The main contributions of our work are four-fold, extending the approach of ==== in various ways. First, we consider quantile estimation of ====.====The rest of the article is organized as follows. In Section ====, we consider splines-based quantile regression for partially dynamic additive models, assuming the identities of the non-dynamic component functions are known. In the case that the identities of the non-dynamic component functions are unknown, it can be regarded as the infeasible oracle model. For the latter case, in Section ====, under the paradigm of sparse modelling, we use a four-stage procedure to separate the zero components, nonzero dynamic components and the nonzero non-dynamic components. Section ==== contains simulation studies and a real data analysis. We conclude in Section ==== with discussions. Finally, The technical proofs are relegated to the ====.====In the proofs ==== denotes a generic positive constant which may assume different values even on the same line. The double sum ==== below always means ====. For any matrix ====, ==== denotes its Frobenius norm, and ====. We also write ==== as ====.====
               ====
            ====
               ====
            ====
               ====
            ====
               ====
            ====
               ====
            ====Now we consider faster rates for the univariate component functions. Let ==== contain basis functions for the intercept and the dynamic component functions, and ==== those for the non-dynamic component functions. Let ====, ====, ====, ====. Define the empirical projection matrix ==== where ====, with rows of ==== denoted by ====.====We write ====where ==== is the subvector of ==== corresponding to the non-dynamic component functions, ==== is the subvector of ==== corresponding to the intercept function and the dynamic component functions, and ==== is the approximation error for the conditional quantile. We decompose ==== where ==== is the approximation of the non-dynamic component functions and ==== is the approximation for the other functions.====Let ==== with rows ====. Define also ====. We further write ====with ====. The proof of the following lemma is similar to that of ====.====
               ====
            ====
               ====
            ====
               ====
            ====
               ====
            ====
               ====
            ====
               ====
            ====
               ====
            ====The proofs of ====, ==== are very similar, and we consider ==== first, given its high-dimensional (and thus slightly more general) nature.====
               ====
            ====
               ====
            ",Pursuit of dynamic structure in quantile additive models with longitudinal data,https://www.sciencedirect.com/science/article/pii/S0167947318302007,February 2019,2019,Research Article,548.0
"Zhang Yaohua,Zou Jian,Ravishanker Nalini,Thavaneswaran Aerambamoorthy","Department of Statistics, University of Connecticut, Storrs, CT, United States,Department of Mathematical Sciences, Worcester Polytechnic Institute, Worcester, MA, United States,Department of Statistics, University of Manitoba, Winnipeg, Canada","Received 11 September 2017, Revised 16 August 2018, Accepted 23 August 2018, Available online 6 September 2018, Version of Record 5 November 2018.",https://doi.org/10.1016/j.csda.2018.08.020,Cited by (5), to increase the accuracy of the final estimates. The analyses provide very good fits and predictions that can assist in trading decisions. The approach can be easily extended to other models for financial durations as well as to a large class of linear and nonlinear time series models.," of asset dynamics, especially using flexible methodology with minimal assumptions. This poses a set of new challenges for researchers and practitioners.====, and exhibit varying patterns of intra-day and intra-week behavior. An event may be defined as a single transaction, as a price change exceeding a certain amount, or as some other characteristic of interest, and durations are defined as the times between consecutive events. ==== argued that clusters of high activity versus low activity, indicated by short durations versus long durations between events, can reveal useful information about the market microstructure, and may be predictable. A trade duration is one of the most popular measures in financial econometrics for quantifying trading speed and intensity, and is closely related to the liquidity of the underlying asset. Successful statistical modeling and inference of the duration process will give insight into the market’s buyer and seller trading activity patterns. Let ==== be the time of occurrence for the ====th event. The ==== and ====: ==== Several classes of statistical models for durations have been proposed in the literature, including the popular Autoregressive Conditional Durations (ACD) models ==== and Log ACD models ====, among others ====, ====, ====, ====. However, all these methods require some distributional assumptions on the error terms in the model. This motivates us to seek a more flexible and distribution-free approach to tackle the duration modeling problem.==== who showed that maximum likelihood and minimum chi-squared methods are asymptotically equivalent by comparing the first order conditions of the two estimation procedures, i.e., analyzing properties of estimators by focusing on the corresponding EFs rather than on the objective functions or estimators themselves. ==== and ==== provided an excellent survey on the historical development of this topic, while ==== described misspecified EFs. For time series, the approach is discussed in ====, ====, ====, among others. Although ====In recent high dimensional and functional data applications, penalized likelihood methods such as LASSO ==== and the adaptive LASSO ====.====In this article, we describe a novel penalized EF approach for the analysis of intra-day financial durations. The structure of the paper follows. In Section ====, we describe properties of durations obtained from the TAQ database. Section ==== presents a brief review of statistical durations models. In Section ====. A simulation study in Section ==== discusses the relative accuracies of the three approaches. In Section ====, we discuss results from using the PEF approach to fit Log ACD(====) models to long sequences of intra-day adjusted financial durations data from four stocks. Section ==== provides a discussion and summary.",Modeling financial durations using penalized estimating functions,https://www.sciencedirect.com/science/article/pii/S0167947318302032,March 2019,2019,Research Article,549.0
"Samejima Kim,Morettin Pedro A.,Sato João Ricardo","Institute of Mathematics and Statistics, Federal University of Bahia, Brazil,Institute of Mathematics and Statistics, University of São Paulo, Brazil,Center of Mathematics, Computation and Cognition, Federal University of ABC, Brazil","Received 14 November 2017, Revised 28 August 2018, Accepted 29 August 2018, Available online 6 September 2018, Version of Record 21 September 2018.",https://doi.org/10.1016/j.csda.2018.08.026,Cited by (0)," for ==== locally ====, named directed wavelet covariance, is introduced and discussed. Theoretically, when compared to Fourier-based quantities, wavelet-based estimators are more appropriate to non-stationary processes and processes with local patterns, outliers and rapid regime changes. Results of directed coherence (DC), wavelet coherence (WTC) and directed wavelet covariance (DWC) with simulated data are also presented. All three quantities could identify the simulated covariances structures. Finally, an illustration of the proposed directed wavelet covariance in a task-based EEG experiment is given.","), partial directed coherence (PDC, ==== and ====, especially in experiments involving external stimuli.====In this article, we introduce a decomposition of the wavelet covariance ====In the following, we describe briefly two techniques commonly used to detect the ==== between time series.====
               ====
            ====
               ====
            ====
               ====
            ",Directed wavelet covariance,https://www.sciencedirect.com/science/article/pii/S0167947318302093,February 2019,2019,Research Article,550.0
"Wei Yuhong,Tang Yang,McNicholas Paul D.","Department of Mathematics & Statistics, McMaster University, Hamilton, Ontario, Canada","Received 24 March 2017, Revised 31 July 2018, Accepted 19 August 2018, Available online 5 September 2018, Version of Record 17 September 2018.",https://doi.org/10.1016/j.csda.2018.08.016,Cited by (20),"Robust clustering from incomplete data is an important topic because, in many practical situations, real datasets are heavy-tailed, asymmetric, and/or have arbitrary patterns of missing observations. Flexible methods and algorithms for model-based clustering are presented via mixture of the generalized hyperbolic distributions and its limiting case, the mixture of multivariate skew-t distributions. An analytically feasible "," points out, the association between mixture models and clustering goes back at least as far as ====, ====, ====, ====, ====, ====). The multivariate ====-distributions the most natural extension (e.g., ====, ====, ====, ====, ====). In many practical situations, however, real world datasets exhibit clusters that are not just heavy tailed but also asymmetric; furthermore, clusters can also be asymmetric yet not heavy tailed. Over the few past years, much attention has been paid to non-Gaussian approaches to model-based clustering and classification, including work on multivariate skew-t distributions (e.g., ====, ====, ====, ====, ====, ====, ====, generalized hyperbolic distributions ====, ====, ====, and hidden truncation hyperbolic distributions ====. A comprehensive review of model-based clustering work, up to and including some recent work on non-Gaussian mixtures, is given by ====.==== and ====The maximum likelihood approach to clustering incomplete data has been well studied and is often used, particularly for Gaussian mixture models (e.g., ====, ====, ====). ==== to fit a mixture of multivariate ==== to efficient supervised learning via the parameter expanded (PX-EM) algorithm ==== further develops a family of multivariate-==== mixture models with 14 eigen-decomposed scale matrices in the presence of missing data through a computationally flexible EM algorithm by incorporating two auxiliary indicator matrices. ==== use a formulation of the mixture of skew-t distributions for model-based clustering with missing data.====We consider fitting mixtures of generalized hyperbolic distributions (MGHD) and mixtures of multivariate skew-t distributions (MST) with missing information. In each case, an EM algorithm is used for model selection. The chosen formulation of the (multivariate) generalized hyperbolic distribution (GHD) is that used by ==== and has formulations of several well-known distributions as special cases such as the multivariate skew-t, normal inverse Gaussian, variance-gamma, Laplace, and Gaussian distributions (see ====). In addition to considering missing data, we develop families of MGHD and MST mixture models, each with 14 parsimonious eigen-decomposed scale matrices corresponding to the famous Gaussian parsimonious clustering models of (GPCMs; ====, ====); see ==== (====).==== consider an eigen-decomposition of the component scale matrices (which is equivalent to the component covariance matrices for Gaussian mixtures), i.e., ====where ====, ==== is the matrix of eigenvectors of ====, and ==== is a diagonal matrix, such that ====, containing the normalized eigenvalues of ==== in decreasing order. Note that the columns of ==== are ordered to correspond to the elements of ====. As ==== point out, the constituent elements of the decomposition in ==== can be viewed in the context of the geometry of the component, where ==== represents the volume in ====-space, ==== the shape, and ==== the orientation. By imposing constraints on the elements of the decomposed covariance structure in ====, ==== introduce a family of GPCMs (====).",Mixtures of generalized hyperbolic distributions and mixtures of skew-t distributions for model-based clustering with incomplete data,https://www.sciencedirect.com/science/article/pii/S0167947318301993,February 2019,2019,Research Article,551.0
"Bolin David,Wallin Jonas,Lindgren Finn","Chalmers University of Technology and the University of Gothenburg, Gothenburg, Sweden,Lund University, Tycho Brahes väg 1, 220 07 Lund, Sweden,University of Edinburgh, Edinburgh, UK","Received 9 November 2017, Revised 5 August 2018, Accepted 10 August 2018, Available online 5 September 2018, Version of Record 22 September 2018.",https://doi.org/10.1016/j.csda.2018.08.007,Cited by (8), and standard MRF models using simulated data and in application to upscaling of soil ====.,", geology in soil permeability models ====. These covariates cause discontinuities in the data that easily can be accounted for ====, ====, where ==== is the number of classes, ====, and ==== with a GMM distribution can be written as ====. Here ====, and ==== where ====. Spatial dependency can be introduced by modeling the collection of the random variables ====, ====, ====), which we refer to as an MRF mixture model.==== for each class by a spatially dependent Gaussian random field (see e.g. ====, ====). This allows us to use the model for classification, but also for noise reduction and spatial interpolation in cases where the data consist of noisy partial observations of fields with discontinuities. We refer to models of this type, which are introduced in more detail in Section ====, as latent Gaussian random field mixture (LGFM) models.====The proposed model could be viewed as a non-stationary Gaussian random field, with a specific prior on spatially varying parameters. There is an extensive literature on non-stationary Gaussian fields, see for example ====, ====, ==== and ====. A non-stationary Gaussian field that resembles the LGFM model is that of ====. However, all these methods either lack the sharp and flexible discontinuities, or the computational efficiency, of the LGFM model.====Since spatial problems often have massive amounts of data, a computationally efficient estimation method is needed in order to fit the LGFM model to data. Further, likelihood estimation for discrete MRFs is problematic due intractable normalizing constants. Two common methods for dealing with this issue are gradient-based minimization and pseudo-likelihood methods ====, ====. Recently, gradient-based methods have also been developed for large-scale Gaussian random field models ====, ====, and is introduced further in Section ====. Finally, Section ==== contains a discussion of possible extensions and further work. The code used to obtain the results in the article is available at ====.====In this appendix, we provide details for how to compute the gradients needed for the parameter estimation procedure.",Latent Gaussian random field mixture models,https://www.sciencedirect.com/science/article/pii/S0167947318301907,February 2019,2019,Research Article,552.0
"Heyard Rachel,Held Leonhard","Department of Biostatistics at the Institute of Epidemiology, Biostatistics and Prevention, University of Zurich, Switzerland","Received 27 July 2017, Revised 23 August 2018, Accepted 24 August 2018, Available online 1 September 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.csda.2018.08.022,Cited by (2),-prior has been developed for the linear model where the ," in order to select a single best model out of a set of different candidates ====, where ====. If there are ==== potential variables, the selection is made among ==== models and ==== which compares two models by taking the ratio of their marginal likelihoods. The marginal likelihood ====depends on the prior distribution ====. Eliciting those prior distributions is a tedious task. Objective Bayesian methods unburden the statistician from choosing the parameter priors for all the models if no subjective prior information is available. Those methods have been well developed in the Gaussian linear model where Zellner’s ====-prior ====. This ====, where ====. The multiplicative factor ====, such as the hyper-====, hyper-==== ==== or Zellner–Siow (ZS) prior ====.====The posterior model probabilities also depend on the prior model probabilities ====. Different prior settings on the model space have been proposed. However, in practice there is often sensitivity of the MPM with respect to such prior choices on the model space and on the regression coefficients. We therefore propose the QPM, which is less sensitive to these prior settings.====This paper is structured as follows: Section ==== first describes how objective Bayesian variable selection methodology has been extended to generalised linear models. Then, Section ==== defines the median probability model, reviews different objective prior settings on the model and parameter priors in Section ====, and illustrates in a case study described in Section ==== as the quantile probability model. Section ==== presents a novel, computationally fast approach to efficiently compute the Monte Carlo standard error of the deviance information criterion (DIC), which is used to determine the QPM. Returning to our case study, we show in Section ==== that the QPM is independent of prior choices. Alternative information criteria, such as the Watanabe–Akaike information criterion (WAIC) or the leave-one-out cross-validation information criterion (LOO IC) are considered in Section ==== and lead, in our application, to the very same QPM. In Section ==== we discuss a potential quantile probability model average and close with some discussion in Section ====.====In order to study the sensitivity of the median probability model (MPM) towards prior settings we performed a simulation study on logistic regression. We closely follow the setup of the logistic regression simulation study in ==== using ==== potential predictors. For each of the ==== simulated training dataset, we draw the columns of the design matrix ==== with ==== rows from a standard normal distribution with pairwise correlation ====, with ====. As ====, we consider four different levels of sparsity in the true model which are summarised in ====. A Beta(====) distribution is chosen on the prior inclusion probabilities with first, ==== resulting in a multiplicity-corrected model prior and second, with ==== giving each variable a prior inclusion probability (IP) of ==== instead of 0.5.====In each of the 250 simulated dataset, the QPM as well as the MPM were retrieved using a sample size of 10,000 for the DIC computation. The model complexity, meaning the number of variables selected, is stored.====Then, the weighted sum of squared errors SSE==== is computed, where ==== is the shrunken estimated coefficient for variables ====, with ==== referring to the unshrunken intercept, ==== and ==== being the expit response function for logistic regression introduced in Section ====. We used a weighted version of the SSE to account for the difference in magnitude of the coefficients in ====, with weights ==== equal to the derivative of ==== at ====. Ten times the average weighted SSE(====) of the selected QPM and MPM for each of the 250 realisations of the sparse and medium scenarios is reported in the first two columns of ====. To ensure readability of the findings we only show the results for the sparse and medium versions of the true model. The estimation error is lower for the MPM if the true model is sparse and ====. For the remaining scenarios, QPM usually performs slightly better.====The QPM, as well as the MPM, are designed to be efficient model selection methods. ==== discuss the fact that an efficient selection approach cannot be consistent to find the truth. ==== shows that both methods struggle to select the true variables. From this figure, it is already possible to conclude that the MPM performs generally better than the QPM in finding the null as well as the full model. The four last columns of ==== show the average number of correctly treated variables (excluded or included) out of 15 variables by the QPM and the MPM selection methods for different prior choices in the 250 simulated datasets for the sparse and medium true model scenarios. The MPM correctly includes/excludes more variables than the QPM if the true model is sparse whereas QPM does better if the truth is medium.====Then, to compare the performance of the MPM with the QPM’s performance, we simulate an independent test dataset with ==== for each training dataset and compute the area under the ROC curve (AUC, measures discrimination), the calibration slope (CS) (====, measures calibration), the logarithmic score (LS) as well as the Brier score (BS) (measure both discrimination and calibration) ====. See ==== for a practical review on the methods to validate probabilistic predictions. Both, CS and AUC should be as close as possible to 1, which means perfect calibration, respectively discrimination. The LS and the BS are negatively oriented, meaning that the smaller they are the better the model’s performance. ==== shows these scores for the sparse and medium scenarios. If the Monte-Carlo error of the difference in scores between QPM and MPM is sufficiently small, the best score is marked in bold. Generally, the MPM performs better if the true model is sparse whereas the QPM shows better calibration and discrimination if the true model is medium.====The most important lesson learned from this simulation study is the sensitivity of the MPM with respect to the prior settings. The QPM tends to include a little bit more variables in the model, but this selection does not get influenced by the priors much. However using for example LEB for the estimation of ==== and a multiplicity-corrected prior on the model space, leads to a much more complex MPM then using a ZS adapted prior on ==== and a prior inclusion probability of 0.25 (light green and red circles in ====). Further, if the true model is sparse, MPM scores better with regards to discrimination and calibration whereas QPM does best if the truth is a more complex model.",The quantile probability model,https://www.sciencedirect.com/science/article/pii/S0167947318302056,April 2019,2019,Research Article,553.0
"Flores-Agreda Daniel,Cantoni Eva","Research Center for Statistics and Geneva School of Economics and Management, Université de Genève, Bd du Pont d’Arve 40 CH-1211 Geneva 4, Switzerland,Operations Department, Faculty of Business and Economics, Université de Lausanne, Bâtiment Anthropole, CH-1015 Lausanne, Switzerland","Received 17 November 2017, Revised 9 August 2018, Accepted 10 August 2018, Available online 27 August 2018, Version of Record 15 September 2018.",https://doi.org/10.1016/j.csda.2018.08.006,Cited by (10)," representation and (iii) other bootstrap schemes, on the grounds of relative bias, relative efficiency and the coverage ratios of resulting prediction intervals. The second simulation study serves the purpose of illustrating the properties of our proposal in a Non-Gaussian GLMM setting, namely a Mixed Logit Model, where the alternatives are scarce."," of the Random Effects, a denomination used to state its difference from the ==== of the Model Parameters.====The problem of Prediction of Random Effects has been widely explored in the literature of ==== (LMM), a setting for which theoretical results have led to the determination of the ==== (BLUP) in full knowledge of the model parameters, and its ==== version (EBLUP) when the parameters are estimated. Naturally, this approach has analogues in the Non-Gaussian framework in the form of the ==== (BP) and EBP, often approximated by ==== and ====.====Similarly to the estimation problem, where point estimates are provided alongside their ==== for inferential purposes, it is useful to retrieve a measure of uncertainty of the point predictions e.g. to classify observational units according to “significant” differences in their predicted response or to construct prediction intervals for new observations drawn from a given unit. In LMM, this translates into the computation of the ====, ====, ==== and ====. In a more general framework, it is customary to report estimates of the ====, ==== and ==== or the more widespread ==== (PB) method, used to produce estimates of MSEP, see for instance ==== or to build Prediction Intervals, as seen in ====, ====, ==== and ====.====To the best of our knowledge, there are very few proposals that attempt to tackle this problem by means of a non-parametric bootstrap procedure. Moreover, these methods often rely on the resampling of some sort of residuals and predictions of the random effects making their implementation in the Gaussian LMM framework straightforward and intuitive, yet harder to export to the ==== i.e. non-Gaussian setting. Hence, we propose to confront the MSEP estimation by means of a non-parametric Bootstrap method resulting from the adaptation of the ==== (RWLB) ====, a scheme having the main advantage of being applicable in the entire class of GLMM. This proposal is compared to adaptations of other schemes such as the so-called ==== (REB), see e.g. ====, ====, and ====, and the more widespread Parametric Bootstrap alternatives.====The article is structured as follows: In Section ====, we set up the notation of the GLMM, characterize the special case of LMM (Section ====) and summarize the problem of prediction of random effects (Section ====). Section ====, contains an overview of two methods for the evaluation and estimation of the uncertainty in prediction namely the approach via the MSEP (Section ====) and the Empirical Bayes approaches relying on CV (Section ====). We briefly review some resampling schemes for LMM in Section ====, highlight or propose adaptations to the estimation of uncertainty in the Non-Gaussian context and formulate our proposals based on the RWLB scheme. Finally, Section ==== contains two simulation studies as a basis of comparison of the different methods, one carried on a LMM (Section ====) and second one in a Mixed Logit context (Section ====).====In what follows, we display tables for the Simulation Study in the Mixed Logit setting when the effect of interest is the ==== ====. These tables contain only the results for the most competitive methods, namely the RWLBE and PBE in all of their versions.",Bootstrap estimation of uncertainty in prediction for generalized linear mixed models,https://www.sciencedirect.com/science/article/pii/S0167947318301890,February 2019,2019,Research Article,554.0
"Liebl Dominik,Rameseder Stefan","Department of Statistics, University of Bonn, Germany,Department of Econometrics, University of Regensburg, Germany","Received 14 November 2017, Revised 30 July 2018, Accepted 12 August 2018, Available online 27 August 2018, Version of Record 5 November 2018.",https://doi.org/10.1016/j.csda.2018.08.011,Cited by (22), ," is observable for all ==== (see, for instance, the textbooks ====, ====, ====, and ====). However, this regular situation does not apply to many functional data sets of practical relevance where the functions ==== are only partially observable, i.e., where ==== is only observable for ==== with ==== describing the ====-specific observable subset of the total domain.====The latter situation is often referred to as fragmented, truncated, incomplete, or partially observed functional data and its practical relevance has triggered a series of research works dealing with different aspects of this problem. ==== propose a “shift-and-connect” procedure to reconstruct and classify fragmentary functional data and ==== and ==== use functional linear regression models to predict the missing parts. ==== and ==== propose a functional regression model for incomplete curves. ==== models and forecasts partially observed price functions and ==== Ch. 1). All of the above cited works lead to inconsistent results if the MCAR assumption is violated. To the best of our knowledge, we are the first to consider specific violations of the MCAR assumption in the context of partially observed functional data.====The classical missing data literature can be divided into the following four – not mutually exclusive – classes (see ====.====, ====, ====, and ====, among others). Our estimation procedure requires the availability of derivatives and, therefore, cannot directly deal with the case of sparse functional data.====. For each ==== we only observe the initial part for all ====; however, the final part with ==== correlates with the overall level of the price curves ====. Larger values of ==== are associated with price curves having an overall high price level and vice versa (see ====). The general exposition of our methodology is geared towards the data situation in this real data application, which provides a simple and instructive walk-along setup. However, we generalize this introducing setup also for broader violations of the MCAR assumption and broader missing data designs.====The rest of the paper is structured as follows. Section ====, we propose a practical approach for testing the considered violations of the MCAR assumption. Section ==== contains our simulation study and the application is found in Section ====. Proofs and further derivations can be found in ====.",Partially observed functional data: The case of systematically missing parts,https://www.sciencedirect.com/science/article/pii/S0167947318301944,March 2019,2019,Research Article,555.0
"Hsu Hsiang-Ling,Chang Yuan-chin Ivan,Chen Ray-Bing","National University of Kaohsiung, Taiwan,Academia Sinica, Taiwan,National Cheng Kung University, Taiwan","Received 22 December 2017, Revised 13 August 2018, Accepted 14 August 2018, Available online 27 August 2018, Version of Record 10 September 2018.",https://doi.org/10.1016/j.csda.2018.08.013,Cited by (3),We study a logistic model-based active learning procedure for ," studied the method for building a detection model using bank account data. This is a good example because in this situation, the label of interest (money laundering account) is limited in a regular bank account data set. It would require a huge amount of time and resources to verify whether an account is suspicious or non-suspicious, even though the major parts of the transactions in a bank account should be normal. Efficiently determining the potential risks within a bank account in addition to effectively and efficiently using the unlabeled subjects to improve the classification rule is the key issue, and the concept of active learning can be applied to this situation.====, ====, ====. There are many classification performance indexes, and it is clear that this subject selection process may depend on the targeted index ====, ====, ====. For example, ==== studied active learning procedures that maximize the area under the ROC curve (AUC), ==== were interested in the ranking of the data, and ====, in which we select a batch of new samples at each iteration and then update our binary classification model via a variable selection approach. Both subject selection and variable selection are featured in this novel active learning procedure.====We organize the rest of this paper as follows. Section ==== presents the details of the subject selection and variable selection steps, and then we describe our active learning algorithm with an integrated subject and variable selection steps. Section ==== presents numerical results, where in addition to the simulation studies, we apply our algorithm to a well-known wave data set used in ==== and a MAGIC gamma telescope data set. We present a brief discussion and conclusion in Section ====.",Greedy active learning algorithm for logistic regression models,https://www.sciencedirect.com/science/article/pii/S0167947318301968,January 2019,2019,Research Article,556.0
"Zang Yong,Cao Sha,Ng Hon Keung Tony,Zhang Chi","Department of Biostatistics, Indiana University, Indianapolis, IN, USA,Center of Computational Biology and Bioinformatics, Indiana University, Indianapolis, IN, USA,Department of Statistics and Actuarial Science, University of Hong Kong, Hong Kong, China,Department of Statistical Science, Southern Methodist University, Dallas, TX, USA,Department of Medical and Molecular Genetics, Indiana University, Indianapolis, IN, USA","Received 25 May 2017, Revised 22 May 2018, Accepted 20 August 2018, Available online 27 August 2018, Version of Record 7 September 2018.",https://doi.org/10.1016/j.csda.2018.08.014,Cited by (1),-value formula for MAX3 have also been derived. Comprehensive simulation studies and a real data application on the genome-wide association study (GWAS) have been conducted using these analytical tools and the results demonstrate desirable operating characteristics of the proposed robust tests.,", ====. Alternatively, under the assumption of G–E independence and rare disease, the G–E interaction can be evaluated by simply assessing the G–E association on the cases only. Such case-only design can yield a higher power than a case-control design when these assumptions hold ====, ====.====In both case-control and case-only designs, if the genetic model of inheritance can be specified a priori, then a score test can be performed to detect the G–Einteraction. The genetic model determines the orders of individuals’ risk of having the disease based on the number of risk alleles in the genotype. Generally speaking, for a diallelic marker, three genetic models, namely the recessive (REC), multiplicative (MUL) and dominant (DOM) are commonly used ====, ====. For each genetic model, an optimal set of scores should be used to maximize the power of the test. In particular, the value 0, 1/2 and 1 are the optimal scores to code the genotype conferring one risk allele when the genetic model is REC, MUL and DOM, respectively ====. Hence, if the genetic model is correctly specified, the corresponding optimal scores can maximize the power of the score test. However, for many complex diseases, the underlying genetic models are unknown and using an inappropriate genetic model can substantially undermine the power of the tests ====. Therefore, robust tests against genetic model mis-specification are in urgent demand.====Despite intensive studies on robust tests for detecting the maingenetic effect ====, ====, ====, ====, ====, ====-value of the robust tests together with an user-friendly software are also released in this paper to facilitate the use of the proposed methods in practice. Simulation study demonstrates that the proposed robust tests could control type I error rate under the null hypothesis and yet yield satisfactory power under the alternative hypothesis, even when the genetic model is mis-specified. The proposed method is also applied to a real genome-wide association study (GWAS) dataset for illustrative purpose.====The rest of this paper is organized as follows. We develop the robust tests for the case-control design and case-only design in Sections ====, ====. In Sections ====, ==== we extend the proposed tests to handle non-monotonic genetic model and categorical environment factor with possible environmental level mis-classification. In Section ====, we carry out comprehensive simulation studies to investigate the operating characteristics of the proposed tests. In Section ====, we apply the robust tests to analyze a genome-wide association study (GWAS) of bladder cancer ====. We provide a brief discussion and concluding remarks in Section ====.====Denote ==== as the nuisance parameters, we can write the observed information matrix based on model ==== in the form of a block matrix as ====where ====with ====and ====, ====, ==== and ==== are the MLEs based on model ==== given ====. By inverting the information matrix, we have ====
            ====Finally, based on the inverted information matrix, the variance estimate for ==== is ====
            ",Robust tests for gene–environment interaction in case-control and case-only designs,https://www.sciencedirect.com/science/article/pii/S016794731830197X,January 2019,2019,Research Article,557.0
"Yu Dalei,Ding Chang,He Na,Wang Ruiwu,Zhou Xiaohua,Shi Lei","School of Statistics and Mathematics, Yunnan University of Finance and Economics, Kunming 650221, China,Industry and Commerce Administration of Yunnan Province, Kunming 650228, China,Center for Ecological and Environmental Sciences, Northwestern Polytechnical University, Xi’an 710072, China,Department of Biostatistics, The University of Washington, Seattle, WA 98195, USA","Received 14 July 2017, Revised 22 May 2018, Accepted 11 August 2018, Available online 27 August 2018, Version of Record 8 September 2018.",https://doi.org/10.1016/j.csda.2018.08.010,Cited by (4),"Meta-analysis provides a quantitative method for combining results from independent studies with the same treatment. However, existing estimation methods are sensitive to the presence of outliers in the datasets. In this paper we study the robust estimation for the parameters in meta-regression, including the between-study variance and ====. Huber’s rho function and Tukey’s biweight function are adopted to derive the formulae of robust maximum likelihood (====) estimators. The corresponding algorithms are developed. The ====
 and restricted ==== when outliers exist in the dataset. The proposed methods are applied in three case studies and the results further support the eligibility of our methods in practical situations.","Meta-analysis provides a full and comprehensive summary of related studies which have addressed a similar question ====. In medical research, results from a single research project rarely provide exhaustive scientific evidence ====, ====. When the distribution of the effect size is specified, the maximum likelihood (====) estimator ====, ==== and the restricted maximum likelihood (====) estimator ====, ====, ==== are usually adopted to conduct the parameter estimation in random effects meta-regression. Other parameter estimation methods are also studied extensively in literatures, see for example ====, ====, ==== and the references therein for details.====The current study is motivated by the fact that many meta-analyses will include at least a few studies yielding observed effects that appear to be outlying or extreme ====. In Section ====, we provide a motivating example where the data at hand are deemed to be contaminated by outlying trials. The presence of outlying studies could substantially alter the conclusions in a meta-analysis ====. Focusing on the detection and identification of outliers, ==== extended standard diagnostic procedures developed for linear regression analysis to the meta-analytic fixed- and random/mixed effects models. Moreover, ==== developed a forward search algorithm for identifying outlying and influential studies in meta-analysis models. In addition to the detection of outlying studies, it is also important to accommodate the effects of these studies appropriately. In general, it is untenable to exclude studies from a meta-analysis on the basis of their results ====. Besides, the identification and understanding of the reason for unusual study results can in itself lead to further understanding of the subject area ====.====An alternative approach is to depart from simply deleting outlying studies but accommodate the outlying studies via robust estimation. ==== proposed a model with long-tailed distribution for random effects, where the problematical outlying result is adjusted with a reduced weight. Instead of using the normally distributed random effects, ==== proposed a new meta-regression model based on finite mixture of a normal distributions, where a finite mixture of outliers and non-outliers is modeled. ==== model provides a number of advantages over traditional methods of outlier detection and robustness, and is shown to be useful in practical meta-analysis. All the aforementioned studies fall into the class of “random effects contamination models”, where the heavy-tailed random effects distribution is involved to achieve robustness (R package for implementing these methods are available at ====). However, the random effects contamination models are deemed to be more complicated than the usually used meta-regression model. Therefore more computationally intensive methods are employed to conduct parameter estimation. For example,  ==== adopted Markov Chain Monte Carlo methods to implement parameter estimation, and R package “metaplus” employs expectation–maximization algorithm and adaptive Gaussian quadrature to overcome the numerical difficulties induced by heavy-tailed random effects distribution. More importantly, these more sophisticated models pose theoretical difficulties to the study of finite sample properties and thus it is in general very difficult (if not impossible) to study the improved confidence interval in finite sample settings (like Section ==== in current paper). ==== established a unifying framework for meta-analysis and applied it to study robust estimation. The above two studies are mainly focusing on the estimation of overall effect, where no moderators (study-level variables) are involved. ==== and ==== are all focusing on the robust estimations of standard error which are robust to misspecification of the correlation structure of the effect size or poor estimation of ==== and are not designed for handling outliers.====In a closely related but different field, the robust estimation in linear mixed-effects model (====) has been studied extensively and a rich literature on approaches of robust ====s has emerged in the past two decades. For example, ==== extended the bounded influence estimation into the domain of ==== developed and implemented a ==== using the t-distribution and incorporated the outlier robustness into general design ====s. Book-length treatments of ====s with the flexible generalized skew-elliptical random effects can be found in ====. Recently, ==== introduced an R package, robustlmm, to robustly fit ====s. The package unifies a number of existing robust estimation methods in ====s (e.g. the mixed-effects model with t-distribution, the Design Adaptive Scale estimation by ====, and so on) and provides a very useful robust alternative for modeling the data with complicated correlation structure. However, the robust estimation methods in ====s cannot be used directly in meta-regression. The reason is that ==== ==== ==== Unlike in ====s are unknown and need to be estimated from the observable data. Additionally, existing studies about robust estimation in ====s are mainly focusing on the asymptotic properties, but in many practical applications, the meta-analysis sample size is relatively small ====s directly to handle the outlying studies in meta-analysis/regression. It is necessary to study the robust estimation in meta-regression, ====.====In current study, we aim to develop a new class of robust methods in the context of meta-regression models. There are two major contributions.====
               ====To the best of our knowledge, these topics have never been studied in the literature. The rest of the paper is arranged as follows. In Section ====, we use a data example to further motivate our current study. In Section ====, formulations of robust ==== estimators are provided, the corresponding large sample based confidence interval and second-order-corrected confidence interval are investigated. In Section ====, simulation studies are conducted to assess the finite sample performance of the proposed methods. In Section ====, the proposed robust estimation methods are used to analyze three sets of real data, which are deemed to be contaminated by some outliers. Section ====. Additional theoretical results, tables, MATLAB/R code and datasets used in the case studies are provided in Supplementary Material.",Robust estimation and confidence interval in meta-regression models,https://www.sciencedirect.com/science/article/pii/S0167947318301932,January 2019,2019,Research Article,558.0
"Banerjee Sayantan,Akbani Rehan,Baladandayuthapani Veerabhadran","Operations Management & Quantitative Techniques Area, Indian Institute of Management Indore, Indore, India,Department of Bioinformatics & Computational Biology, UT MD Anderson Cancer Center, Houston, TX, USA,Department of Biostatistics, UT MD Anderson Cancer Center, Houston, TX, USA","Received 14 November 2017, Revised 31 July 2018, Accepted 10 August 2018, Available online 23 August 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.csda.2018.08.009,Cited by (7), for ," and a comprehensive list of references therein).====From a modeling standpoint, graphical models ====, ====, ====, ====, ====, ====, ====, ====, ====, ==== and ====. Penalization based regularization methods have been developed utilizing the graphical model framework, which include using a lasso type penalty on the elements of the precision matrix ====, ====, ====, ====, ====, ====, ====, or a SCAD type penalty ====. In high dimensional situations, penalized regression based approaches have been developed too, including the neighborhood-selection approach ====.====, ====, ==== and ====. While these methods have attractive computational properties, the main drawbacks are that they assume known graph structures and, due to the heuristic/algorithmic nature of the formulation, do not allow for the explicit incorporation of uncertainty in estimation and inference for both graph structure learning and clustering. For ==== and Laplacian eigenmaps ====In this article, we propose a model based approach to the problem by considering a ====, where the ==== (unique) clusters. The primary inferential aim is to retrieve the clusters along with their (connected) components from a random sample of size ==== from the underlying distribution, where the dimension ==== may grow with ==== and ====). More importantly, we formalize the arguments for Laplacian embedding as suitable projections for graph clustering by providing theoretical support. From a computational standpoint, we adopt fast computational methods for both graphical structure learning and clustering of variables. For clustering in particular, we resort to fast and scalable algorithms based on asymptotic representations of Dirichlet process mixture models.====We evaluate the operating characteristics of our methods using both simulated and ====. In simulations, we compare our graph-based clustering via graph structure learning method with both standard (“off-the-shelf”) graph-based clustering methods as well as methods that do not incorporate the structural information obtained from a graphical model formulation so as to evaluate the effectiveness of using a graph-based clustering model. Using various metrics of clustering efficiency (such as normalized mutual information scores and between-cluster edge densities), we demonstrate the superior performance of our methods compared to the performance of the competing methods. We apply our methods to a novel pan-cancer proteomic data set, through which we evaluate protein networks and clusters across 11 different cancer types. Our analyses reveal several biologically-driven clusters that are conserved across multiple cancers as well as differential clusters that are cancer-specific.====The paper is organized as follows. In Section ====, we lay out the inferential problem and present preliminaries on graphical models and related concepts, including graph Laplacian and Laplacian embedding. In Section ====, we present our graph clustering method. We discuss the theoretical results pertaining to the consistency of the graph Laplacian in Section ====. We present the results of our simulation study in Section ====, ====, ====, ====, ====, ====.====We present the proof of the consistency of the graph Laplacian. The proof uses standard matrix ordering inequalities, which we present in the following lemma.",Spectral clustering via sparse graph structure learning with application to proteomic signaling networks in cancer,https://www.sciencedirect.com/science/article/pii/S0167947318301920,April 2019,2019,Research Article,559.0
"Hinoveanu Laurentiu C.,Leisen Fabrizio,Villa Cristiano","School of Mathematics, Statistics and Actuarial Science, University of Kent, United Kingdom","Received 18 July 2017, Revised 27 July 2018, Accepted 10 August 2018, Available online 23 August 2018, Version of Record 8 September 2018.",https://doi.org/10.1016/j.csda.2018.08.008,Cited by (3),"A loss-based approach to change point analysis is proposed. In particular, the problem is looked from two perspectives. The first focuses on the definition of a prior when the number of change points is known a priori. The second contribution aims to estimate the number of change points by using a loss-based approach recently introduced in the literature. The latter considers change point estimation as a model selection exercise. The performance of the proposed approach is shown on simulated data and ====.","There are several practical scenarios where it is inappropriate to assume that the distribution of the observations does not change. For example, financial data sets can exhibit alternate behaviours due to crisis periods. In this case it is sensible to assume changes in the underlying distribution. The change in the distribution can be either in the value of one or more of the parameters or, more in general, on the family of the distribution. In the latter case, for example, one may deem appropriate to consider a normal density for the stagnation periods, while a Student ==== focused on the change in the means of normally distributed variables. ==== looked into the single change point problem when different knowledge of the parameters of the underlying distributions is available: all known, some of them known or none of them known. ==== focuses on the binomial and normal distributions. In ==== have showed that the Dirichlet process prior could have a strong effect on the inference and may lead to wrong conclusions in the case of a single change point. ==== have approached the single change point problem in the context of a Poisson likelihood under both proper and improper priors for the model parameters. ==== build on the work of ==== by considering a two level hierarchical model. Both papers illustrate the respective approaches by studying the well-known British coal-mining disaster data set. In the context of multiple change points detection, ==== have provided a fully Bayesian treatment for the product partitions model of ====. Their application focused on stock exchange data. ==== in the change point literature to handle multiple change points. ==== extend the product partition model of ==== by adding a graphical structure which could capture the dependencies between multivariate observations. ====, who proposed a method which treats the change points as latent variables. ==== have proposed an extension to the hidden Markov model of ==== by using a Dirichlet process prior on each row of the regime matrix. Their model is semiparametric, as the number of states is not specified in advance, but it grows according to the data size. ====, ====, ==== and ====.==== and ====. The former paper discusses the single change point problem in a model selection setting, whilst the latter paper, which is an extension of the former, tackles the multivariate change point problem in the context of linear regression models. Our work aims to contribute to the methodology for change point analysis under the assumption that the information about the number of change points and their location is minimal. First, we discuss the definition of an objective prior for change point location, both for single and multiple changes, assuming the number of changes is known a priori. Then, we define a prior on the number of change points via a model selection approach. Here, we assume that the change point coincides with one of the observations. As such, given ==== data points, the change point location is discrete. To the best of our knowledge, the sole general objective approach to define prior distributions on discrete spaces is the one introduced by ====.====To illustrate the idea, consider a probability distribution ====, where ==== is a discrete parameter. Then, the prior ==== is obtained by objectively measuring what is lost if the value ==== is removed from the parameter space, and it is the true value. According to ====, where ==== is the parameter characterising the nearest model to ====, represents the utility of keeping ====. The objective prior is then obtained by linking the aforementioned utility via the self-information loss: ====where the Kullback–Leibler divergence ==== from the sampling distribution with density ==== to the one with density ==== is defined as: ====Throughout the paper, the objective prior defined in Eq. ==== will be referenced as the ==== prior. This approach is used to define an objective prior distribution when the number of change points is known a priori. To obtain a prior distribution for the number of change points, we adopt a model selection approach based on the results in ====, where a method to define a prior on the space of models is proposed. To illustrate, let us consider ====where ==== is the sampling density characterised by ==== and ==== represents the prior on the model parameter.====Assuming the prior on the model parameter, ====, is proper, the model prior probability ==== is proportional to the expected minimum Kullback–Leibler divergence from ====, where the expectation is considered with respect to ====. That is: ====
            ====The model prior probabilities defined in Eq. ====where ==== and model ====, defined as ====with ====.====This paper is structured as follows: in Section ==== we establish the way we set objective priors on both single and multiple change point locations. Section ==== shows how we define the model prior probabilities for the number of change point locations. Illustrations of the model selection exercise are provided in Sections ====, ====, where we work with simulated and real data, respectively. Additionally, in Section ==== we perform a comparison of the proposed method to another Bayesian approach discussed in the literature. Section ==== is dedicated to final remarks.====Here, we show how model prior probabilities can be derived for the relatively simple case of selecting among scenarios with no change points (====), one change point (====) or two change points (====). First, by applying the result in ====, we derive the Kullback–Leibler divergences between any two models. That is:====
               ====The next step is to derive the minimum Kullback–Leibler divergence computed at each model:====
               ====Therefore, the model prior probabilities can be computed through Eq. ====, so that:====
               ",Bayesian loss-based approach to change point analysis,https://www.sciencedirect.com/science/article/pii/S0167947318301919,January 2019,2019,Research Article,560.0
"Dickhaus Thorsten,Sirotko-Sibirskaya Natalia","Institute for Statistics, University of Bremen, Germany","Received 5 September 2017, Revised 22 February 2018, Accepted 14 August 2018, Available online 22 August 2018, Version of Record 5 September 2018.",https://doi.org/10.1016/j.csda.2018.08.012,Cited by (1), methodology in ==== models (DFMs) is extended to the multiple testing context based on a ,", ==== and ====.====Relevant features of a DFM can be estimated both parametrically and non-parametrically in the time as well as in the frequency domain. Classical time-domain estimation procedures employ maximum-likelihood-based methods such as the expectation maximization (EM) algorithm, see, e.g., ====, or non-parametric methods based on extracting principal components; see, for instance, ====. Recently, the frequency domain analog of the EM-based method has been proposed by ==== and principal components-based procedures have been extended to the frequency domain by ====, ==== and ====. Whereas methods based on the EM algorithm and principal components are traditionally used to estimate larger-scale DFMs, smaller-scale DFMs can be efficiently estimated via direct optimization of the likelihood function. In the present paper we consider small-scale DFMs, thus, we employ the method of direct optimization of the likelihood and provide a detailed description of its step-by-step implementation.====With the introduction of DFMs an important question has been raised as of deciding on the presence of dynamics in the common as well as in the idiosyncratic components. Correct model specification is especially crucial when the cross-sectional dimension is small as, e.g., neglecting the dynamics in the idiosyncratic factors may lead to erroneous model selection and to a subsequent misinterpretation of the model; cf. ==== and ====. Testing procedures for model specification in the DFM context include likelihood-ratio (LR) tests, see ====, ==== and ====. ==== offer an alternative LM testing approach to check the factors for autocorrelation. However, their method is initially developed for a single common factor case and has to be extended to the multiple common factor context first.====Thus, we address two important open problems of ====
               ====
            ====
               ====
            ====We will exemplify the proposed methodology by means of these two problems. The paper is organized as follows. Section ==== summarizes the statistical methodology underlying our work. For technical details, we refer to ====, ====. Furthermore, the two approximation methods for the null distribution of such vectors (chi-square and bootstrap) are discussed. Section ==== describes the estimation of DFM parameters, Section ==== presents numerical results from simulation studies, and Section ==== is devoted to the analysis of real data. We conclude with a discussion in Section ====.",Simultaneous statistical inference in dynamic factor models: Chi-square approximation and model-based bootstrap,https://www.sciencedirect.com/science/article/pii/S0167947318301956,January 2019,2019,Research Article,561.0
"Xia Ye-Mao,Tang Nian-Sheng","Department of Applied Mathematics, Nanjing Forestry University, Nanjing, 210037, China,Department of Mathematics and Statistics, Yunnan University, Kunming, 650031, China","Received 1 October 2017, Revised 9 July 2018, Accepted 6 August 2018, Available online 18 August 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.csda.2018.08.004,Cited by (8),Latent variable hidden Markov models (LVHMMs) are important statistical methods in exploring the possible heterogeneity of data and explaining the pattern of subjects moving from one group to another over time. Classic subject- and/or time-homogeneous assumptions on ,", ====, ==== and EQS ====, LVMs have been successfully applied to various scientific fields including behavioral sciences, social psychology, medicine and economics.====, ==== and establish finite LVM mixture analysis. For example, see among others, ====, ====, ==== in different contexts. However, single mixture model is limited in characterizing dynamic characteristics since the heterogeneity of observations often takes on nonhomogeneous behavior over time. A more feasible and flexible method is to introduce latent variables within hidden Markov model framework and to establish latent variable hidden Markov model (LVHMM, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ==== suggested using multivariate ==== distribution as an alternative to normal assumption to address the over-disperse problem; ==== proposed a semiparametric approach to modeling univariate longitudinal count data. Their observed process involves a finite mixture model mixed with non-homogeneous Poisson kernel; ==== extended this idea to deal with the non-homogeneous behavior of transition model. For the LVHMM, ==== in learning the trajectories for Robot programming. These methods, though successful in downweighting the influence of the distributional deviations or outliers in time series, may not be effective in describing clustering behavior for multiple multivariate observed sequences. It cannot explain the difference in transition patterns of states as well as emission distributions between any distinct groups/clusters. In particular, they are not naturally extended to exploit the interrelationships of latent factors underlying the multiple measurements.====As an alternative to modeling heterogeneity, ====, ====The article is organized as follows. Section ==== introduces the proposed model for multivariate longitudinal data. Section ==== outlines the MCMC algorithm. Posterior inferences on unknown parameters and model assessment are also presented in this section. A simulation study to assess the performance of the proposed model is given in Section ====. In Section ====, we apply the proposed method to the cocaine use longitudinal study. And Section ==== concludes the paper.====The following is the Supplementary material related to this article. ",Bayesian analysis for mixture of latent variable hidden Markov models with multivariate longitudinal data,https://www.sciencedirect.com/science/article/pii/S0167947318301877,April 2019,2019,Research Article,562.0
"Lin L.,Fong D.K.H.","Department of Statistics, The Pennsylvania State University, University Park, USA,Smeal College of Business, The Pennsylvania State University, University Park, USA","Received 18 December 2017, Revised 14 July 2018, Accepted 15 July 2018, Available online 18 August 2018, Version of Record 30 August 2018.",https://doi.org/10.1016/j.csda.2018.07.007,Cited by (10)," information into the dimension reduction analysis through the use of a variable selection strategy. An efficient computational algorithm to implement the procedure is also developed. A series of simulation experiments and a real data analysis are conducted, and the proposed model is shown to outperform several ==== based on some measures commonly used in the literature.",", ====, ====; ====). Furthermore, in most of the cases, there are also side information (external variables) on each subject such as gender, ethnicity, age, Body Mass Index, etc. that a researcher would like to include in the analysis to obtain the MDS solution. These external variables typically have different measurement scales from the primary data and many of them may not be discriminative factors to differentiate subjects. As an example, subjects cannot be diagnosed solely based on their gender. However, these variables can be helpful to better stratify the subjects when performing dimension reduction based on the primary data (e.g., ====; ====; ====; ====; ====; ====).====MDS has found many applications for scientific studies. For example, in psychological research, MDS has been used to construct personality profiles where similarities between individuals were analyzed to uncover how (groups of) individuals differ with respect to their profiles ====, ====, ====, ====, ====, ====. In addition to visualization, MDS can also help in uncovering the underlying dimensionality associated with the dissimilarities. For example, MDS can help in marketing research to answer the most common question: what product attributes and features most contribute to a product’s position in the market share ====.==== for useful textbook accounts.====The earliest practical MDS method, which is called Classical Multidimensional Scaling (CMDS), is proposed by ====. CMDS assumes the distances to be Euclidean, and utilizes the principal components of the double-centered distance matrix to obtain the projected low dimensional space. A more popular method, proposed by ====).====In this paper, we develop a more general Bayesian MDS procedure, namely Bayesian MDS with Variable Selection (BMDS-VS). First, the assumption of constant Gaussian measurement error variance in the observed dissimilarity is removed. Hence, we allow different error variances for different pairwise dissimilarities which can accommodate different degrees of uncertainty associated with the observed dissimilarity. Second, our model allows the incorporation of additional covariate information with a variable selection option to select the most important covariates that help to better explore low dimensional spaces. We believe our method provides a realistic modeling approach for real data analysis. Lastly, most existing MDS methods do not provide an explicit function that maps data from the original space to the latent map, hence it is not possible to directly make prediction on new data set. BMDS-VS can potentially learn a mapping to the latent space by leveraging on existing side information. This enhancement could lead to a more accurate prediction. We apply BMDS-VS to two simulation studies and one real data analysis, and demonstrate the ability of BMDS-VS to yield better solutions compared with some ====.====The rest of this paper is organized as follows. In Section ====, we introduce notations and provide an overview of existing methods that are most relevant to our proposed model. In Section ====, we describe the proposed Bayesian MDS model and the corresponding estimation procedures: Section ==== defines the model and the prior, Section ==== outlines an MCMC algorithm for the model update, and describes posterior summary procedures. Section ==== describes the dimension selection criteria for determining the optimal number of dimensions. In Section ====.====The MCMC analysis for BMDS-VS has technical components as detailed here. In each conditional distribution, the conditioning ==== represent the data and all other parameters and/or indicators.",Bayesian multidimensional scaling procedure with variable selection,https://www.sciencedirect.com/science/article/pii/S0167947318301737,January 2019,2019,Research Article,563.0
"Manghi Roberto F.,Cysneiros Francisco José A.,Paula Gilberto A.","Instituto de Matemática e Estatística, Universidade de São Paulo, São Paulo, Brazil,Departamento de Estatística, Universidade Federal de Pernambuco, Brazil","Received 23 March 2018, Revised 7 August 2018, Accepted 9 August 2018, Available online 18 August 2018, Version of Record 6 September 2018.",https://doi.org/10.1016/j.csda.2018.08.005,Cited by (7),Statistical procedures are proposed in generalized additive partial linear models (GAPLM) for analyzing ,", ====, ====. Recent reviews on these subjects may be found, respectively, in the books by ==== and ====.====The aim of this paper is to propose statistical procedures in GAPLM for analyzing correlated data. The paper is organized as follows. In Section ====, a motivating example is presented, whereas the GAPLM are introduced in Section ====. A small simulation study for assessing the empirical distribution of the parameter estimators as well as of three proposed residuals are summarized in Section ====. The motivating example is analyzed in Section ==== under GAPLM, in which the diagnostic procedures developed through the paper are applied for selecting an appropriate model. The last section deals with some conclusions.",Generalized additive partial linear models for analyzing correlated data,https://www.sciencedirect.com/science/article/pii/S0167947318301889,January 2019,2019,Research Article,564.0
"Li Kan,Luo Sheng","Merck Research Lab, Merck & Co, 351 North Sumneytown Pike, North Wales, PA 19454, USA,Department of Biostatistics and Bioinformatics, Duke University Medical Center, 2400 Pratt St, 7040 North Pavilion, Durham, NC 27705, USA","Received 30 January 2018, Revised 30 June 2018, Accepted 26 July 2018, Available online 16 August 2018, Version of Record 30 August 2018.",https://doi.org/10.1016/j.csda.2018.07.015,Cited by (7)," is increasingly common across medical studies of neurodegenerative diseases and is exemplified by the motivating Alzheimer’s Disease Neuroimaging Initiative (ADNI) study, in which serial brain imaging, clinical and neuropsychological assessments are collected to measure the progression of Alzheimer’s disease (AD). The proposed functional joint model consists of a longitudinal function-on-scalar submodel, a regular longitudinal submodel, and a survival submodel which allows time-dependent functional and scalar ====. A ==== is adopted for parameter estimation and a dynamic prediction framework is introduced for predicting the subjects’ future health outcomes and risk of AD conversion. The proposed model is evaluated by a simulation study and is applied to the motivating ADNI study.",". Since mild cognitive impairment (MCI) is often considered as a transitional stage to AD, MCI patients are usually enrolled as the target population for early prognosis and evaluating interventions ====. Existing research has identified a number of biomarkers in predicting an individual’s likelihood of converting to AD, as well as differences in biomarker values among MCI and AD individuals ====, ====, ====. Although most of the current studies measure regional atrophy using a single volume-based value, some researchers ====, ====. However, the proposed FJM only accommodates baseline imaging marker as a time-invariant function predictor. Since the imaging markers (e.g., hippocampus) from MRI, along with other neurocognitive markers, are often collected repeatedly in the studies of AD, it is of scientific interest to investigate the combined predictive performance of these repeatedly measured functional and scalar outcomes.====Several methods for the analysis of repeatedly measured functional outcome exist in the literature. One category of the methods is based on functional principal component analysis (FPCA), as well as its extension for multilevel FPCA by ====, longitudinal FPCA by ==== and by ==== and ====, ====. Multivariate joint models have been well studied by considering multivariate continuous, binary, ordinal, or a mixture of different outcome types. ====Compared with the existing literature, we make two major contributions to both multivariate joint modeling and ====: (1) We propose a multivariate joint model considering both longitudinal functional and scalar outcomes. To the best of our knowledge, this paper is the first to model the repeatedly measured functional outcomes, scalar outcomes, survival process simultaneously while accounting for the associations among the processes. (2) We propose a dynamic prediction framework that provides accurate personalized predictions of disease risk and progression. We investigate the potential capability of the longitudinal functional outcome in improving the prediction of AD progression. Previous studies involving functional data mainly focused on model inference rather than prediction of risk and longitudinal outcome trajectories. These important predictive tools can provide valuable information to monitor each patient’s disease progression and to make early decisions about targeted prevention and treatment selection.====The remainderof the article is organized as follows. In Section ====, we apply the proposed method to the motivating ADNI study. In Section ====, we conduct a simulation study to assess the performance of the method. Concluding remarks and discussion are presented in Section ====.====The following is the Supplementary material related to this article. ====
            ",Bayesian functional joint models for multivariate longitudinal and time-to-event data,https://www.sciencedirect.com/science/article/pii/S0167947318301816,January 2019,2019,Research Article,565.0
"Park Yeonjoo,Simpson Douglas G.","Department of Statistics, University of Illinois at Urbana-Champaign, 725 S Wright St., Champaign, IL 61820, USA","Received 14 November 2017, Revised 12 May 2018, Accepted 3 August 2018, Available online 11 August 2018, Version of Record 5 November 2018.",https://doi.org/10.1016/j.csda.2018.08.001,Cited by (6),"A robust probabilistic classifier for functional data is developed to predict class membership based on functional input measurements and to provide a reliable ==== for class membership. The method combines a ==== assumptions on the within-curve covariance. Simulation studies evaluate the proposed method and competitors in terms of sensitivity to heavy tailed functional distributions and outlying curves. Classification performance is evaluated by both error rate and logloss, the latter of which imposes heavier penalties on highly confident errors than on less confident errors. Run-time experiments on the R implementation indicate that the proposed method scales well computationally. Illustrative applications include data from quantitative ultrasound analysis and phoneme recognition.","A common goal in medical image analysis is to use the information in the image from a new subject to classify type or stage of a region of interest based on prior training data. For example, a classification tool might be utilized during surgery to evaluate areas with abnormal tissues or to access tumor margins using biomedical imaging techniques (e.g., ====A variety of techniques have been developed for classifying functional data. Existing methods include density-based classifiers ====, ====, ====, ====, ====, regression-based methods ====, ====, ====, ====, ====, k-nearest neighborhood classifiers ====, ====, ====, ====. Although there has been extensive development, many existing classifiers require data to be observed over the same interval, which cannot be applied to our motivating example without further processing or imputation.====. These Gaussian or second order methods might be vulnerable to outlying curves and heavy tailed response distributions.====Robust versions of FPCA ====, ==== are available for regular functional data but not for irregular functional data. ==== and ====The rest of this article is organized as follows. The classification procedure is developed in Section ==== including details such as model selection and computational procedures. Section ==== presents comparative simulation studies under different scenarios. Section ====. Computational implementation of the method in R is provided in the supplementary material.====The following is the Supplementary material related to this article. ",Robust probabilistic classification applicable to irregularly sampled functional data,https://www.sciencedirect.com/science/article/pii/S0167947318301841,March 2019,2019,Research Article,566.0
"Wang Bo,Xu Aiping","Department of Mathematics, University of Leicester, Leicester LE1 7RH, UK,Sigma (Maths and Stats Support), Coventry University, Coventry CV1 5DD, UK","Received 25 October 2017, Revised 11 April 2018, Accepted 19 July 2018, Available online 26 July 2018, Version of Record 5 November 2018.",https://doi.org/10.1016/j.csda.2018.07.009,Cited by (18)," of Gaussian process regression, and can naturally accommodate both scalar and functional variables as the predictors, as well as easy to obtain and express uncertainty in predictions. The numerical experiments show that the proposed methods significantly outperform the competing models, and their usefulness is also demonstrated by the application to two real datasets.",".====Among others one of the most important problems in FDA is functional regression which describes the relationship between the predictor and the response variable, where at least one of them is of functional nature. The first functional regression model was formulated by ====, and has ever since been extensively studied and further developed by ==== and many other researchers.==== study in detail the functional linear models and consider various cases such as scalar response with functional predictors, functional response with scalar predictors and functional response with functional predictors. Later on a large number of further extensions and developments have been proposed, for instance the generalised functional linear model ====, ====, ====, the functional quadratic regression model ====, the penalised function-on-function regression ====, and references therein. On the other hand, ====, ==== and ==== and ==== for details. ==== first apply GPR methods to functional data, which is further developed by ====, ==== and ====. However in the above works the functional response variable depends on the functional predictors at the current time only, therefore their models are types of the concurrent functional models ====, ====. In this paper we propose Gaussian process methods for functional regression where the response variable depends on the entire trajectories of the functional predictors. The proposed methods enjoy the intrinsic desirable properties of GPR, and can naturally incorporate both scalar and functional variables of high dimension as the predictors, as well as easy to obtain the predictive variance. Since bandwidth selection, which usually needs cross validation, is not required in our models, the proposed methods are much faster in computation than some of the existing kernel methods considered in the numerical examples at the same time of significantly improved prediction accuracy. And due to the nature of GPR, the dimension of the predictors has little impact on the computational time, so the models can easily deal with high dimensional predictors. Our numerical examples show that the proposed methods significantly outperform the existing methods in comparison.====The rest of the paper is organised as follows. Section ==== briefly reviews the GPR methods and then introduces the Gaussian process nonparametric regression methods for functional data with scalar response and functional response. The proposed methods are evaluated by the simulation studies in Section ====, and are applied to two real datasets in Section ====. Finally, Section ==== concludes the paper with some discussions.",Gaussian process methods for nonparametric functional regression with mixed predictors,https://www.sciencedirect.com/science/article/pii/S0167947318301750,March 2019,2019,Research Article,567.0
"Febrero-Bande Manuel,Galeano Pedro,González-Manteiga Wenceslao","Department of Statistics, Mathematical Analysis and Optimization, Universidade de Santiago de Compostela, Spain,Department of Statistics and UC3M-BS Institute of Financial Big Data, Universidad Carlos III de Madrid, Spain","Received 29 November 2017, Revised 11 July 2018, Accepted 12 July 2018, Available online 23 July 2018, Version of Record 5 November 2018.",https://doi.org/10.1016/j.csda.2018.07.006,Cited by (11),"Two different methods for estimation, imputation and prediction for the functional linear model with scalar response when some of the responses are missing at random (MAR) are developed. The simplified method consists in estimating the model parameters using only the pairs of predictors and responses observed completely. In addition the imputed method consists in estimating the model parameters using both the pairs of predictors and responses observed completely and the pairs of predictors and responses imputed with the parameters estimated with the simplified method. The two methodologies are compared in an extensive simulation study and the analysis of two real data examples. The comparison provides evidence that the imputed method might have better performance than the simplified method if the numbers of functional principal components used in the former strategy are selected appropriately."," and ==== and ====, among others.====In particular, regression models in which the response and/or the predictors have a functional nature are very popular among researchers and practitioners on FDA. Some surveys on the topic include ==== and ====, who focused on estimation methods when there is a functional predictor and a scalar response, and ====, ====, ====, ====, ====, ====, ====, ====, ====, and ====, among others.====This paper considers the situation in which some of the real responses of a data set generated from the functional linear model with scalar response are missing at random. This situation has been only considered in ====, who focused on the problem of imputing the missing responses of the data set. Additionally, when the regression function operator is of an unknown form, ==== focused on estimating the unconditional mean value of the response when some of the responses are missing at random, ==== proposes a method to estimate the unknown regression function operator, while ====The rest of this paper is structured as follows. Section ==== presents the functional linear model with scalar response and the estimation methodology based on the functional principal components approach when all the predictors and responses in the data set are observed completely. Section ==== introduces the simplified and imputed methodologies for estimating the model parameters of the functional linear model with scalar response when there are responses that are missing at random, for imputing the missing responses and for predicting new responses. Finally, Section ==== compares the behavior of both methods with an extensive simulation study and illustrates with two real data examples that the conclusions from the simulation study might be relevant for practical applications.====The following is the Supplementary material related to this article. ====
 ====
 ====
 ====
 ====
 ","Estimation, imputation and prediction for the functional linear model with scalar response with responses missing at random",https://www.sciencedirect.com/science/article/pii/S0167947318301725,March 2019,2019,Research Article,568.0
"French Joshua,Stoev Stilian,Hall Lauren","University of Colorado, United States,Colorado State University, United States,University of Michigan, United States","Received 6 March 2018, Revised 10 July 2018, Accepted 11 July 2018, Available online 20 July 2018, Version of Record 5 November 2018.",https://doi.org/10.1016/j.csda.2018.07.004,Cited by (19)," of yet unobserved rare events that are not seen in historical records. Data from the North American Regional Climate Change Assessment Program, which has produced computer model predictions of current and future temperatures across much of North America, are used. The approach allows for the computation of probabilities for heat waves of any pre-specified temporal duration, spatial extent, and overall magnitude. It can be applied to the computation of probabilities of other extreme weather events, including cold spells and droughts.","There is no universally agreed definition of a heat wave. Heat wave definitions depend on the objective of a study; relevant research is reviewed in the Supplemental Material. However, all definitions combine temporal duration and some threshold levels. If a study extends beyond a specific location or a relatively small region, the spatial aspect of the data is also incorporated. For smaller regions, definitions based on fixed thresholds or historical ==== are useful, especially from the public health point of view. However, such definitions may not be relevant to environmental studies, as plant and animal species adapted to specific regions respond to temperature changes far above (or below) what is typical for the environment they inhabit rather than to those crossing fixed thresholds. In this respect, measures based on locally-defined percentiles are more appropriate. However, a limitation of such an approach is that upper percentiles reflect historical records, so it is not clear how to use them to compute probabilities of heat waves whose amplitude, duration, and spatial extent are not seen in historical records. Since we aim at developing a tool set for computing probabilities of possibly yet unobserved rare events, we propose a different definition. It quantifies departures from values typical for a specific region over a specific period of time, with the magnitude of the departure being potentially extremely high. As with the other studies, the definition we propose is motivated by the objective we want to achieve and the data we use. No claim of its universal superiority is made.====As noted above, our objective is to compute the probability of an extreme heat wave of a prespecified duration, amplitude, and spatial extent. Many previous studies (see the Supplemental Material) use specific probabilistic models, typically involving some hierarchy with point process and continuous distributions components. Since we focus on extreme events, we do not need to specify a detailed probabilistic mechanism that heat waves occurrences should follow. We instead use their limit ==== behavior, which is justified by the heat wave definition we propose. This relatively simple approach is suitable for our purpose. As always, some precision can be lost due to the application of limit results, but biases resulting from a misspecified probabilistic model can be reduced. Additional advantages of the proposed methodology are that it is fast, simple, and can produce easily interpretable and practically useful results. It is hoped that it will complement useful research that has recently been done.====The paper is organized as follows. We introduce the NARCCAP Data in Section ====. Section ==== focuses on the definition of a heat wave and shows how it allows us to identify regions at risk of extreme heat waves. We derive a general loss function that can be used to quantify heat waves, along with methodology based on extreme value theory that allows us to compute the probability of a heat wave. The NARCCAP data are analyzed in Section ====. Section ==== presents the results of several simulation studies designed to assess the performance of our approach. The contribution of the paper is summarized in Section ====.====The following is the Supplementary material related to this article. ",Quantifying the risk of heat waves using extreme value theory and spatio-temporal functional data,https://www.sciencedirect.com/science/article/pii/S0167947318301701,March 2019,2019,Research Article,569.0
"Sang Peijun,Wang Liangliang,Cao Jiguo","Department of Statistics and Actuarial Science, Simon Fraser University, BC, Canada V5A 1S6","Received 2 November 2017, Revised 12 June 2018, Accepted 10 July 2018, Available online 18 July 2018, Version of Record 5 November 2018.",https://doi.org/10.1016/j.csda.2018.07.003,Cited by (7)," for random functions with irregular observations. Furthermore, simulation studies demonstrate that the new approach is considerably more efficient in computation than the bootstrap method. The new approach is illustrated with three applications. The first application investigates the dynamical correlation of air pollutants. The second application studies the dynamical correlation of EEG signals in different regions of the brain in response to some stimuli. The third application estimates the dynamical correlation of gene expressions during the activation of T-cells.",", ====, ====, ====, ====, ====. In contrast, multivariate functional data have not been studied popularly.==== considered the dynamical correlation of two random functions, which can be regarded as an extension of the correlation coefficient in multivariate data. ==== developed singular valued decomposition for pairs of functional data.==== based on the dynamical correlation. ==== applied the dynamical correlation in a psychological study. The dynamical correlation was employed in ==== to study the mechanism when CD8==== also considered such an estimator in applications under the bootstrap-based inference framework, even though this estimator was not thoroughly investigated in the paper. Furthermore, we find in simulation studies that this bootstrap procedure is unstable and cannot provide a reliable confidence interval for dynamical correlation when the functional data are irregularly spaced.====Motivated by these observations, we propose a weighted average estimator for the dynamical correlation which can adaptively adjust weights on each subject. The weights are chosen via the weighted empirical likelihood. This new estimator can be regarded as an alternative to that mentioned in applications of ====. This improvement will be demonstrated in our simulation studies presented in Section ====The paper is organized as follows. The definition of dynamical correlation is reviewed in Section ====. Then a new method of constructing confidence intervals for dynamical correlation based on the weighted empirical likelihood is proposed. For comparison, we introduce the bootstrap method for constructing confidence intervals for dynamical correlation proposed by ==== later. Some theoretical properties of the proposed inference tool are given in Section ====. The proofs are deferred to ====. Section ==== compares the performances of the weighted empirical likelihood-based method and the bootstrap method in associated confidence intervals for dynamical correlation via simulation studies. The proposed method is illustrated with three applications in Section ====. Section ====.====In this section, we lay out regularity conditions which can guarantee that the difference between the smoothed random function ==== and the true underlying random function ==== is negligible. Suppose the observations satisfy ====, ====, ==== and ====. We assume that the random errors ==== are identically and independently distributed with ==== and ====. Let ==== denote the bandwidth used in the local linear smoother in the pre-smoothing step. All limits below are taken given the number of subjects, ==== diverges.==== Observational time points ==== follow a density ====, which is equi-continuous and bounded away from 0. Furthermore, the support of ==== is ====, which is compact.==== The random errors satisfy ==== for some ====.==== The bandwidth ==== and the number of observations satisfy ==== and ====
 ==== for any ==== with ====.==== The random functions ==== are twice differentiable. Furthermore, the second derivatives are equi-continuous satisfying ====, where the supremum is taken over ====, ==== and ====.==== The number of observations for each random function satisfies ==== The bandwidth ==== satisfies ",Weighted empirical likelihood inference for dynamical correlations,https://www.sciencedirect.com/science/article/pii/S0167947318301695,March 2019,2019,Research Article,570.0
"Fu Eric,Heckman Nancy","Department of Statistics, University of British Columbia, 2207 Main Mall, Vancouver, British Columbia, Canada","Received 9 February 2018, Revised 28 May 2018, Accepted 25 June 2018, Available online 10 July 2018, Version of Record 5 November 2018.",https://doi.org/10.1016/j.csda.2018.06.010,Cited by (8), of the model often renders the inference computationally challenging. An alternative method for model-based curve registration is proposed which is computationally more stable and efficient than existing approaches in the literature. The proposed method is applied to the analysis of elephant seal dive profiles. The result shows that more intuitive groupings can be obtained by clustering on phase variations via the predicted warping functions.,"Functional data often exhibit variation not only in amplitude but also in horizontal scaling, or phase. For example, in the classical Berkeley Growth Study, which considers growth as a function of age ====, both the magnitude of growth spurts and their age of occurrence vary across subjects.==== for individual curves, ====, and sampled points, ====, as ====with ==== a smooth function and ==== a noise term. The function ==== is typically modeled flexibly, in terms of spline functions, either via minimizing a penalized likelihood ====, ====, ====. Indeed, under general conditions, one can show that penalized likelihood estimates are the same as linear mixed effects model estimates ====. ==== took a different approach by simply assuming that the functions, ====Often, however, data do not follow model ====, but rather require inclusion of phase variation via a time transformation, ====The warping function ==== is continuous, strictly increasing and maps the sampling time, ====, to the system time, ====, in such a way that, in system time, the ==== curves are in synchrony. Ignoring phase variation and analyzing unaligned curves, that is, using model ==== instead of model ====, can lead to incorrect conclusions. For instance, estimating a population mean curve by the point-wise average of non-aligned individual curves may lead to dampened or even completely masked peaks and valleys.====A common approach to analyzing data as in ==== is to first smooth the ==== curves individually and then align the curves to remove phase variations. The aligned curves are then analyzed as if they are generated from ====. The process of curve smoothing and alignment is often called curve registration. Some well-known registration approaches are landmark registration ====, ====, ==== and methods based on optimization of some fitting criteria ====, ====, ====, ====. Essentially, these procedures aim to align the peaks and valleys of the curves or their derivatives.====This sequential approach to fitting model ====, the pre-smoothing step can be problematic for noisy data. In this case an individual curve’s data might be overfitted, resulting in a “bumpy” curve with fictitious features being used in the subsequent alignment. Another problem with the sequential approach is with inference on population level parameters, since the sequential approach does not take into account the uncertainty of the alignment.====In this paper, we consider an alternative to the sequential approach and work directly with the model in ====, estimating the amplitude and phase variation simultaneously via a new expectation–maximization (EM) algorithm. Our proposed model is similar to those of ====, ==== and ====. In Section ====, and the warping function, ====. The estimated warping functions are then used to cluster the depth trajectories into groups of similar shape. We find that clustering based on the warping function is superior to clustering based on the original data, yielding more interpretable clusters.====The proof of the theorem requires some topological arguments because of requirements of measurability of mappings between function spaces and measurability of subsets of function spaces. We provide details here. We make full use of the following argument. Suppose ====, ====, are metric spaces with corresponding Borel sigma-algebras ==== and ==== is a function from ==== to ====. Let ==== be a probability space. If ==== and ==== are random variables from ==== to ==== with ==== for all ====, then ==== for all ==== provided that ==== for all ====, that is, provided that ==== is measurable. In our proofs, we show that the function ==== is measurable by showing that ==== is continuous with respect to the metrics on ==== and ====. In some cases, the function ==== will only be defined on ====, a subset of ====. In this case, we show that ==== is in ==== and ==== is continuous on ====.====We define the following function spaces and distances, which induce topologies.====For any cross-products of function spaces, we use the usual cross-product metric/topology. The following lemmas are presented without proof.",Model-based curve registration via stochastic approximation EM algorithm,https://www.sciencedirect.com/science/article/pii/S0167947318301543,March 2019,2019,Research Article,571.0
"Godolphin P.J.,Godolphin E.J.","Nottingham Clinical Trials Unit, University of Nottingham, Nottingham, UK,Stroke Trials Unit, Division of Clinical Neuroscience, University of Nottingham, UK,Department of Mathematics, University of Surrey, Surrey, UK,Department of Mathematics, Royal Holloway University of London, London, UK","Received 6 December 2017, Revised 27 June 2018, Accepted 30 June 2018, Available online 6 July 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.csda.2018.06.020,Cited by (2),"In scientific experiments where human behaviour or animal response is intrinsically involved, such as clinical trials, there is a strong possibility of recording missing values. Missing data in a clinical trial has the potential to impact severely on study quality and precision of estimates. In studies which use a cross-over design, even a small number of missing values can lead to the eventual design being disconnected. In this case, some or all of the treatment contrasts under test cannot be estimated and the experiment is compromised since little can be achieved from it. Experiments comparing two treatments that use a cross-over design with more than two experimental periods are considered. Methods to limit the impact of missing data on study results are explored. It is shown that the breakdown number and, if it exists, perpetual connectivity of the planned design are useful ==== which guard against the possibility of a disconnected eventual design. A procedure is proposed which assesses planned designs for robustness against missing values and the method is illustrated by assessing several designs that have been previously considered on cross-over studies.","Cross-over designs are a popular choice for clinical trialists, due in part to the additional efficiency gains they provide ====, ====, ====. In a cross-over trial, fewer participants are needed than the equivalent parallel group trial, and, from a clinical viewpoint, the experimental treatments are tested within each subject which eliminates many of the confounding factors that might occur in studies with a different design. In particular, ====, point out that regulatory agencies, such as the U.S. Food and Drug Administration (FDA), look favourably on studies which implement a cross-over design for bioequivalence and bioavailability pharmacology trials. For bioequivalence studies, the cross-over design is the study design recommended by both the FDA and the European Medicines Agency ====, ====.====Designs that have two treatments and two periods were frequently utilized by researchers, but it has been shown that these designs lack the structure to test for carry-over and also produce biased direct treatment effects under the presence of carry-over ====, ====. Potential solutions to these problems have also been considered, but these designs are not normally recommended in practice ====, ====. Therefore, higher-order designs that involve two treatments administered over more than two periods are preferable and are becoming more widely used. Such a design repeats any of the two experimental interventions a specific amount of times in a number of sequences. A four period design with four unique sequences is proposed by the FDA as the most suitable design to use for bioequivalence studies with two treatments if carry-over is expected ====. There has been a thorough examination to determine the best two-treatment higher-order design in terms of both statistical and cost efficiency ====, ====, ====, ====, ====, ====, ====.====However, little attention has been paid in the literature to how robust various cross-over designs are to data that becomes unavailable during the course of the experiment. In clinical trials, missing data is not uncommon and many studies experience subject drop-out of up to 30% (==== page 39). Participants can drop-out through administrative issues such as change of location, unhappiness with trial processes or difficulty with attending study visits. It is also not unusual for a number of participants to withdraw consent part-way through the trial. As well as this, subjects may leave the study prematurely for reasons related to treatment, or even be excluded due to protocol deviations, for example poor adherence to the intervention or use of concomitant medication. This issue can be especially prevalent in cross-over studies, as participants experience more treatments and longer follow-up time when compared to the equivalent parallel-group study. Exposure to different interventions could result in a multitude of side effects and/or adverse drug reactions which could lead to subject drop-out at various points in the study. In higher order cross-over studies, this issue is heightened further as the number of experimental and associated washout periods are increased which can lead to trials with lengthy follow-up. Similar difficulties with drop-out during the term of the experiment can also arise when animal subjects are involved in pharmaceutical studies; see for example, ====.====Missing data in any experiment will result in a loss of precision of parameter contrasts in effects of interest and, in some cases, can lead to a design which is disconnected; see for example ====. In studies which use a cross-over design, a specific pattern of drop-out behaviour can result in a disconnected design in which some and occasionally all contrasts in treatment direct, treatment carry-over and period effects will not be estimable. Such a situation has the potential to compromise the experiment severely, and could result in substantial loss of information about the aims of the study as well as incurring unwanted excess monetary and time costs from a repeated experiment. Throughout this paper, the design that is selected for the experiment is called the ==== and the design that remains after any drop-out is referred to as the ====. It is expected that the analysis of the experiment will be based on the eventual design. A useful measure when planning an experiment to reduce or even prevent the possibility of a disconnected eventual design is the concept of the minimum number of observations that a planned design is required to lose for the corresponding eventual design to be disconnected; this is referred to in what follows as the ==== of the planned design. Thus, planned designs with a high breakdown number are advantageous on grounds of robustness to missing data.====Contributions to the analysis of cross-over designs when one or more subjects fail to complete all periods of treatment are given by ====, ====, ==== and ====. This class of designs necessarily excludes from consideration the two-treatment higher-order designs.====In this paper many of the planned two-treatment higher-order cross-over designs are assessed for robustness to missing values due to subject drop-out. By tradition, the two treatments are labelled ==== and ",Robust assessment of two-treatment higher-order cross-over designs against missing values,https://www.sciencedirect.com/science/article/pii/S016794731830166X,April 2019,2019,Research Article,572.0
Sambucini Valeria,"Dipartimento di Scienze Statistiche, Sapienza Università di Roma, Italy","Received 11 November 2017, Revised 25 June 2018, Accepted 27 June 2018, Available online 4 July 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.csda.2018.06.015,Cited by (8)," predictive strategy for interim monitoring in phase II trials focused on ==== binary outcomes is proposed. At any interim analysis, the stopping rules are based on the evaluation of the ==== that the trial will show a conclusive result at the planned end of the study, given the observed data. The proposed procedure is applied using hypothetical scenarios that represent different situations which may occur at the interim stage. A real data application is also illustrated with the use of both non-informative and informative prior distributions. Finally, simulation studies to evaluate the operating characteristics of the design have been performed.","In phase II of clinical trials, the primary objective is to evaluate the effectiveness of a new treatment, in order to assess whether it warrants further investigation in the subsequent phase III. Therefore, in the clinical development process of a novel therapy, phase II studies play a key role in identifying promising therapies and screening out those who are not sufficiently effective. Traditionally, phase II trials are designed as non-comparative studies based on a binary endpoint that measures the clinical response to the treatment, by indicating whether patients achieve complete or partial response during a defined follow-up period. Two-stage or multi-stage designs, that allow early termination of the trial for futility, are typically implemented. The aim is to reduce study participants’ exposure to an inefficacious treatment, in addition to saving time and resources. The most popular and used two-stage designs are the ==== and the ==== designs of ====, ====, ====, ====, ====, ====, ====).====). The aim is to let the trial stop early, if there is a high posterior predictive probability that a definitive conclusion will be reached at the end of the study, based on the strength of the currently observed data. ==== used this predictive approach to develop a Bayesian phase II design that guaranties good frequentist operating characteristics. Subsequently, the idea of continuously monitoring the trial using the posterior predictive probabilities was successively exploited by ====, ====, ====, ====, ====, ====, ====, ====, ====).====In this paper, we propose a Bayesian interim procedure, based on posterior predictive probabilities, for monitoring efficacy and safety in single-arm phase II trials. The idea of evaluating the predictive probability that the trial will show a conclusive result at the end of the study, given the accumulated data at the current moment, is here adapted to the case of two binary endpoints. In order to have a real simultaneous control on response and toxicity, we do not consider marginal models that lead to a global decision obtained through the conjunction of two independent decisions, one concerning response and one concerning toxicity. We exploit the Dirichlet-Multinomial model and define a treatment as promising on the basis of conditions about parameters that account for the association between the two endpoints. In particular, we specify target requirements on the marginal probability of experiencing toxicity and on the ==== of simultaneously experiencing efficacy and safety.====The outline of the paper is as follows. In Section ====, we formalize the problem and describe the proposed interim methodology. A detailed illustration of the procedure is also provided and a real data example is illustrated. Some numerical results are given in Section ====, using different hypothetical situations that may occur at the interim stage, while in Section ==== we present the results of simulation studies aimed at evaluating the operating characteristics of the proposed methodology. Final conclusions are reported in Section ====.",Bayesian predictive monitoring with bivariate binary outcomes in phase II clinical trials,https://www.sciencedirect.com/science/article/pii/S0167947318301610,April 2019,2019,Research Article,573.0
"Marbac Matthieu,Vandewalle Vincent","CREST and Ensai, France,Univ. Lille, EA2694 Santé publique: épidémiologie et qualité des soins, F-59000 Lille, France,Inria, France","Received 19 January 2018, Revised 26 June 2018, Accepted 27 June 2018, Available online 3 July 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.csda.2018.06.013,Cited by (10),"In the framework of model-based clustering, a model allowing several latent class variables is proposed. This model assumes that the distribution of the observed data can be factorized into several independent blocks of variables. Each block is assumed to follow a latent class model ("," assume that the considered variables explain a single partition among the observations. However, the available data could convey more that one partition of the data. For instance, one can imagine that different blocks of variables describing a customer (variables about work, variables about hobbies, variables about family, etc.) can give different clustering/partitioning of the data set at hand. In absence of prior knowledge on how to group the variables into blocks, a challenging question for the statistician is to find these blocks of variables based on the data.====The problem of finding several partitions in the data, based on different groups of continuous variables, has been addressed by ==== in a model-based clustering framework ==== have proposed an extension of their previous works which relaxes the independence assumption between sub-vectors. This extension considers three types of variables, the classifying variables, the redundant variables with respect to the classifying variables, and the variables which are not classifying at all. This can be seen as extension of the models proposed by ==== and ====The problem of finding several partitions in the data has also been considered by ====, in what they called facet determination. Their model is similar to ==== but it also allows tree dependency between latent variables, the resulting model is called pouch latent tree models (PLTMs). The best model is then selected using the BIC criterion by using a greedy search based on search operators such as node introduction or node deletion for instance. This model allows a rich interpretation of the data, however the huge number of possibilities due to the tree structure search make it even more difficult to use than previous models when the number of variables is large.====In order to deal with large numbers of variables, the main idea is to use a more constrained model to be able to easily perform model selection. We assume that the distribution of the observed data can be factorized into several independent blocks of variables, each one following its own mixture distribution. The considered mixture distribution in a block is a latent class model (==== each variable of a block is supposed to be independent of the others given the cluster variable associated to this block). This model is an extension of the approaches proposed by ==== and ==== in the framework of variable selection in clustering, where only two blocks are considered, ====. The simplicity of the model allows to estimate the repartition of the variables into blocks and the mixture parameters simultaneously like in ==== and ====. We present a procedure for performing model selection (choice of the number of blocks, the number of clusters inside each block and the repartition of variables into block) with the BIC ==== have proposed the MICL criterion derived from the ICL criterion ==== and ====, thus avoiding to run EM algorithms for each repartition of variables into blocks. Note that the proposed model allows to deal with mixed-data as in ====Let usnotice that the proposed framework has similarities with the biclustering framework, and in particular the block clustering models proposed by ====. Block clustering consists in clustering the rows and the columns simultaneously while our approach makes blocks of variables, ==== clustering of columns, and for each block of variables makes a clustering of the individuals, ====The outline of the paper is the following. In Section ====, we present the multiple partitions mixture model. In Section ====, we present the EM algorithm used for the estimation of the parameters by maximum likelihood when the blocks are known. In Section ====, we present how the model search can be performed using the BIC criterion. In Section ====, we present how the model search can be performed using the MICL criterion. In Section ====, we show the interest of the proposed model on simulated and real data.",A tractable multi-partitions clustering,https://www.sciencedirect.com/science/article/pii/S0167947318301592,April 2019,2019,Research Article,574.0
"Shen Keren,Yao Jianfeng,Li Wai Keung","Department of Statistics and Actuarial Science, The University of Hong Kong, Pokfulam, Hong Kong Special Administrative Region","Received 25 August 2017, Revised 21 March 2018, Accepted 4 June 2018, Available online 2 July 2018, Version of Record 5 November 2018.",https://doi.org/10.1016/j.csda.2018.06.004,Cited by (1)," demonstrate that the spectrum of the PA-RCov is ====, that is, a few ==== (spikes) stay away from the others which form a rather continuous distribution with a density function (bulk). Therefore, any inference on the ICVs must take into account this spiked structure. As a methodological contribution, a spiked model is proposed for the ICVs where spikes can be inferred from those of the available PA-RCov matrices. The consistency of the inference procedure is established. In addition, the methodology is applied to the real data from the US and Hong Kong markets. It is found that the model clearly outperforms the existing one in predicting the existence of spikes and in mimicking the empirical PA-RCov matrices."," (ICV) of asset returns using the ==== ====, ====, ====. More precisely, suppose there are ==== stocks whose log-price processes are denoted by ====. A commonly used model for ====where ==== is a ====-dimensional drift process, ==== is a ==== co-volatility matrix, and ==== is a ====In practice, the intraday prices are contaminated by the market microstructure noise. As a matter of fact, we observe, instead of the underlying process ====, a noisy version: ====where ==== denotes the observation, and ==== denotes the noise, which is assumed to be i.i.d. with mean ==== and covariance matrix ====, and is independent of ====.====To filter out the effect of the noise, there are many approaches proposed in the literature, for example, the ==== estimator by ====, the ==== estimator by ====, and the ==== by ====. See also ====, ====, ====, ====, ====, and ====. In low dimensional case when the number of the stocks ==== is small compared to the sample size ==== becomes large, they are no longer good estimators even in the simplest case when the co-volatility process remains unchanged. In particular, the spectrum of the estimators will deviate from that of the ICV. See ====.====A few papers have appeared recently in the literature to deal with this challenging problem of estimation of high-dimensional ICV matrices. ==== use a similar approach to construct the realized covariance matrix and incorporate low-frequency dynamics in their modeling. However, few work exists to deal with non-sparse ICV matrices while the microstructure noise is present. In a recent paper, ==== propose a new pre-averaging estimator for the spectrum of such ==== is required to belong to Class ==== such that ==== for some constant matrix ==== and ==== is a càdlàg function from ==== to ====. Then, the ICV can be written as ====In other words, ==== serves as a time-varying scaling of the base ICV matrix ====. Notice that here the dynamic of the volatility process ==== only. However, a fully general model ==== using ==== functions would be impossible to identify. Furthermore, we may later extend such setting to a greater freedom as follows. Stocks are naturally grouped to, say ==== sectors (====). With ==== stocks in ====th sector (====), one can apply a similar strategy blockwisely. Nevertheless, in this paper, we use the setting in ====. Choose a window length ==== and group the intervals ====, ==== into ==== pairs of non-overlapping windows. For any process ====, let ====Then, the following PA-RCov matrix is considered: ====where for any vector ====, ====Recall that the ==== of size ==== is the probability distribution ====where ==== are the eigenvalues of ====, and ==== is the Dirac mass. A key ingredient here is that the ESDs of ==== and the ICV are related in the high-dimensional case. Precisely, when ==== and ====, the ESDs of the ICV and ==== converge almost surely to probability distributions ==== and ====, denoted by ====, satisfies the Mar====enko-Pastur equation associated with ====. As a result, the spectrum of the ICV can be inferred from that of ==== using this equation. For more information about the Stieltjes transform and the Mar====enko-Pastur equation, please refer to ====.====From the Mar====enko-Pastur equation, it is known that the eigenvalues of the PA-RCov have a finite support, made of a finite number of intervals. However, ==== and ====, separated from the core spectrum (====, ====, ====, ====, ==== and ==== made with its 92 eigenvalues is plotted in ====.====The spiked model is first introduced by ==== where all eigenvalues of the population covariance matrix are unit except a fixed small number of them. ==== and ==== consider a ==== with a much extended structure of spiked eigenvalues. Other related works include ==== and ==== where the observed returns are contaminated with finite variation and microstructure noise parts. In this paper, we establish the asymptotic relationship between the spiked eigenvalues of the PA-RCov and those of the ICV, by an equation similar to that in the generalized spiked model introduced by ====. As a result, the spikes of the ICV can be inferred from those of the PA-RCov. In the simulation study, it is found that the theoretical result holds well at the finite sample in various kinds of situations, where there are one or more spikes, and where the spikes have different magnitudes. In addition, we apply our model to real data in the US market and the Hong Kong market. We find that our model consistently outperforms the model without spike in predicting the existence of spikes and in mimicking the empirical PA-RCov matrices.====In summary, the main contributions of this paper are as follows. To our best knowledge, this is the first paper introducing a spiked model for the ICV. In addition, we apply the model to analyze real data, thus extending the ICV estimator of ====. The main empirical findings include:====Finally, we show in detail how the integration of spikes improves the ICV estimation.====The rest of the paper is organized as follows. Section ==== introduces our new spiked model and establishes the main theorem on the consistency of the estimated spikes. The finite sample performance of the theorem is checked through extensive simulation studies in Section ====. We apply our model to the real data in Section ==== where we compare our model with that of ====. Section ==== concludes.====The following is the Supplementary material related to this article. ",On a spiked model for large volatility matrix estimation from noisy high-frequency data,https://www.sciencedirect.com/science/article/pii/S0167947318301427,March 2019,2019,Research Article,575.0
"Martínez-Hernández Israel,González-Farías Graciela","Statistics Program, King Abdullah University of Science and Technology, Thuwal 23955-6900, Saudi Arabia,Centro de Investigación en Matemáticas, A.C. Jalisco S/N, Col. Valenciana 36240, Guanajuato, Mexico","Received 14 November 2017, Revised 11 April 2018, Accepted 4 June 2018, Available online 14 June 2018, Version of Record 5 November 2018.",https://doi.org/10.1016/j.csda.2018.06.003,Cited by (14), concentrations in California.,", ====, ====, ====, ====, ====). However, little research exists on the case of functional predictor with a functional response.====: FAR====. In practice, the autoregressive order ==== takes small values such as ==== or ==== (====). Most research has assumed ====, and many analyses have been developed with FAR==== (====, ====, ====). We, too, assume that the FAR==== model.====To define our proposed estimator, we first introduce the FAR==== model. Let ====, with inner product ====, and ==== to ====. Let ==== with finite moments of order ====. Then, the FAR(====) process is defined as a sequence of functional random variables ==== in ==== such that ====where ==== is an operator in ====, ====, and ==== is functional white noise (====). Without loss of generality, we assume that ====. If ==== is defined as an element of ==== by ====; the corresponding empirical version is defined as ==== where ====Under this framework, integral operators form an important class. Let ====. The integral operator ==== with kernel ==== is defined as ==== A crucial issue is to estimate ==== or ====. If model ====and if ==== is invertible, then ==== However, ==== does not exist (====). There is a significant amount of research that focuses on avoiding this problem in order to obtain the estimator of ==== under this approach. The method used most often is the principal component analysis (PCA) approach, where only the first ==== is approximated by ====. ==== proposed projecting data into a space spanned by finite eigenfunctions of ==== to get a bounded ====. ==== used the same idea to predict a FAR==== can be computed instead of ==== (====).====Another approach for estimating ==== is through ====, who proposed a penalized LS (PeLS) method. Similarly to the regression model case in ====, ==== can be obtained by minimizing errors in the ====-norm. As before, we have an ill-posed problem that requires regularization to obtain the estimator. ==== discussed several approaches, including the assumption of finite basis function and regularization. ==== computed the estimator using a weighted LS approach. ==== extended penalized functional regression considering P-splines for the regularization, and ==== outliers, ==== outliers, and ==== outliers (see ==== be a functional time series. Then ==== is an observation with isolated outlier at time ==== if ====, where ==== represents the outlier and ==== is the contamination size. Notice that the effect is only in the observed time and does not affect the subsequent observations. In this paper, we suppose that the observed functional time series has isolated outliers.==== defined the ====, which is based on the modified band depth (MBD) (====) or other curve rankings. The functional boxplot detects shift outliers efficiently, even in the case of temporal dependence (====). Other proposals for the detection of functional outliers can be found in ====, ==== and ====. ==== proposed a concept of total variation depth to detect shape and magnitude outliers, and combined this concept with the functional boxplot. ====. We also use the corresponding depth to down-weight the outliers and obtain a robust estimator of the kernel surface, ====.====To illustrate the effect of contamination with isolated outliers on a dataset, we simulate ====, where ==== is such that ====. We consider a modified set of five fixed observations by adding an outlier with Model ==== defined in Section ====. Then, we estimate ==== for each simulation using the PeLS estimator, ==== (defined in Section ====), and compute the Integrated Squared Error (ISE) (defined in ====) as a function of the perturbation level ====. ==== depicts how the error increases as the perturbation level increases.====The aim of this paper is to propose a robust estimator of the kernel ==== under the presence of replacement outliers. Our method considers directional outlyingness as a centrality measure to define weights that are used in a PeLS estimation. Potential outliers in the data correspond to smaller weights. To identify the potential outliers, we use the two-stage functional boxplot, since this approach is robust to various types of outliers and takes into account the shape and the magnitude of the functional data.====The remainder of our paper is organized as follows. In Section ====, we define the weights based on the functional depth and the Depth-based Least Squares (DLS) estimator for the kernel ====, and theoretical properties of the proposed estimator are studied. In Section ====, we derive the theoretical influence function of the DLS estimator and we present a visualization tool for the empirical influential function. In Section ====, we compare the performance of our DLS estimator with those of the PeLS, PCA, and robust PCA estimators under different simulation settings. The performance is measured by the integrated squared error (ISE) and the empirical influence function (EIF). In Section ====, we analyze a dataset of hourly measurements of ambient ==== in California. Finally, in Section ==== we present some discussions and a direction for future work.",Robust depth-based estimation of the functional autoregressive model,https://www.sciencedirect.com/science/article/pii/S0167947318301415,March 2019,2019,Research Article,576.0
"Ma Haiqiang,Li Ting,Zhu Hongtu,Zhu Zhongyi","School of Statistics, Jiangxi University of Finance and Economics, Nanchang 330013, China,Research Center of Applied Statistics, Jiangxi University of Finance and Economics, Nanchang 330013, China,Department of Statistics, Fudan University, Shanghai 200433, China,Department of Biostatistics, University of North Carolina at Chapel Hill, NC, USA","Received 11 May 2017, Revised 22 May 2018, Accepted 4 June 2018, Available online 13 June 2018, Version of Record 18 September 2018.",https://doi.org/10.1016/j.csda.2018.06.005,Cited by (34)," regression for functional partially linear model in ultra-high dimensions is proposed and studied. By focusing on the conditional quantiles, where conditioning is on both multiple random processes and high-dimensional scalar ",".====When the observed data are in the form of random curves rather than scalars or vectors, there are some literatures on functional linear quantile regression models. ==== developed a method for conditional quantile regression analysis when predictors take values in an infinite-dimensional functional space. ==== studied smoothing splines estimators for functional linear quantile regression models. ==== estimated the quantile slope function, and established the global convergence rate of the quantile estimator of unknown slope function. ====Frequently, one has access to a “mixed” dataset which involves both multiple functional predictors and a large number of scalar covariates.For example, in our real data analysis, we are interested in establishing the association between the mini-mental state examination (MMSE) scores and the brain volume imaging data of 20 regions of interest (ROI) as well as 1071 single nucleotide polymorphisms (SNPs) at different quantiles. The multiple ROIs and high dimensional SNPs pose challenges to the existing methods. Although ====.==== and ====.==== is not applicable. Meanwhile, since functional data are often observed intermittently with measurement errors, one can pre-smooth the discrete observations for every individual, and then regard the smoothed trajectory as a fully observed functional predictor. The smoothing errors caused by this smoothed step and its influence carried over to the functional principal component estimates should be taken into account in theoretical studies.====, and has been already used by ==== and ====, additional challenges arise from the effects of errors involving in smoothing the discrete observations, the unobserved FPCA scores, and the two non-convex penalties.====The rest of this paper is organized as follows. In Section ====, the ultrahigh-dimensional functional partially linear quantile model is proposed formally. Estimation and prediction with the proposed model are described in detail. The oracle properties of the proposed estimators and the convergence rate of the prediction of the condition quantile function are discussed in Section ====. Some simulation studies and real data analysis are conducted in Sections ====, ====. Concluding remarks can be found in Section ====. The technical proofs and additional numerical results are collected in Supplementary Materials.====The following is the Supplementary material related to this article. ====
            ",Quantile regression for functional partially linear model in ultra-high dimensions,https://www.sciencedirect.com/science/article/pii/S0167947318301439,January 2019,2019,Research Article,577.0
"Wong Raymond K.W.,Zhang Xiaoke","Department of Statistics, Texas A&M University, College Station, TX 77843, USA,Department of Statistics, George Washington University, Washington, DC 20052, USA","Received 20 July 2017, Revised 7 May 2018, Accepted 18 May 2018, Available online 28 May 2018, Version of Record 5 November 2018.",https://doi.org/10.1016/j.csda.2018.05.013,Cited by (9)," ==== is developed. Despite their nonparametric nature, the covariance estimators are automatically positive semi-definite, which is an essential property of covariance functions, via a one-step procedure. An unconventional representer theorem is established to provide a finite dimensional representation for this class of covariance estimators based on data, although the solutions are searched over infinite dimensional functional spaces. To further achieve a low-rank representation, another ",", ====, ==== and ====. Typically functional data are collected from ==== curves ==== that are regarded as independent copies of a real-valued ==== defined on a compact domain ==== with mean function ====. In reality, due to discrete recording and the presence of noise, the data are often represented by ====, where ==== is the number of observations from the ====th curve ====, and ==== is the noisy observation from ====, i.e., ====. Here ==== are independent errors with zero mean and finite variance ====. For simplicity and without loss of generality we assume ==== for all ====, and ==== is nonrandom but may vary over ====.====Among various population quantities, the covariance function ==== is fundamental in FDA. Generally ==== has two major roles. It is not only an important quantity that characterizes the temporal dependency ====, ====, ====. As for the estimation of ====, ====, B-splines ====, ====, penalized splines ====, ====, and smoothing splines ====, ====, ====, ====, ====).====Positive semi-definiteness is an essential characteristic of covariance functions. Therefore, a valid covariance estimator is usually desired to be positive semi-definite, especially when this feature is indispensable in subsequent analyses, such as correlation estimation (see Section ====) and functional linear regression ====. Meanwhile, having a low rank is another appealing feature of a covariance estimator. First, a low rank can encourage dimension reduction, facilitate simple interpretations (e.g., of FPCA), and alleviate computational and storage burdens. Moreover, a low rank is often technically needed in trajectory prediction, functional linear regression and some other FDA methods (e.g., ====, ====, ====, ====, ====).====A majority of existing FDA methods cannot directly produce a covariance estimator that is positive semi-definite or of low rank. In the literature there are roughly two types of indirect approaches that always involve multi-step procedures, apart from tuning parameter selection. The first type begins with a constraint-free covariance function estimator, then followed by a reconstruction step, e.g., via FPCA and truncation. See ==== and ====, ====, and then a positive semi-definite covariance function estimator of low rank can be reconstructed in terms of eigen-decomposition. These methods, however, are unfavorable since they may not only complicate the theoretical analysis of the final estimator, but also make computation unstable due to the non-smooth truncation. Therefore, in this article, we aim to develop a coherent “one-step” procedure that can automatically produce both a positive semi-definite and a low-rank covariance function estimator.====), and can easily enable low-rank representations, e.g., when the trace-norm penalty is used. Given any penalty, the corresponding covariance estimator is obtained by a single step, which can reduce the computational and theoretical complexities of the aforementioned indirect approaches.====We establish a crucial representer theorem that provides a finite dimensional representation for this class of covariance estimators based on data, although the solutions of the corresponding optimizations are searched over infinite dimensional functional spaces. Compared with its classical counterparts (e.g., ====, ====), this representer theorem is unconventional and technically innovative due to the semi-positivity constraint and a wide range of regularizations.A byproduct of the theorem is an explicit expression of the ====. The numerical performance of the resulting estimator is shown in a simulation study to be the best among popular alternatives with respect to rank reduction, estimation accuracy, and computational stability. Its applicability is convincingly illustrated in the analysis of a traffic dataset. Note that, asymptotically, the use of trace-norm regularization does not rule out the case when ==== is of high or infinite rank. Irrespective of the true rank, our estimator is consistent with the optimal convergence rate, up to some order of ====, as implied by the theoretical results below.====Despite the lack of a closed form due to the semi-positivity constraint and possibly non-differentiable penalties, we develop the empirical ==== is additionally periodic, we can improve the result significantly and achieve the optimal one-dimensional nonparametric rate, up to some order of ==== and the ==== rate achieved by ==== for sparse functional data. In contrast to these two pioneer works, our objective function is not necessarily differentiable, which thus requires different technical treatments. Our theoretical results are established in terms of empirical processes techniques. The success of the proofs depends on the upper bound of the entropy for tensor product Sobolev–Hilbert spaces, which is the first appearance in the FDA literature to our best knowledge.====To summarize, the main contribution of this article is three-fold. First, we propose a new and broad class of RKHS covariance estimators via a variety of spectral regularizations of an operator. The resulting estimator is automatically positive semi-definite through a one-step procedure. Additionally, low-rank estimation is encouraged when a proper penalty is chosen, e.g., the trace-norm penalty. Second, we establish an unconventional representer theorem that provides a finite dimensional representation for the covariance estimator. This theorem makes the estimation procedure practically computable and facilitates our algorithmic development. Lastly, we develop the asymptotic results for a broad class of covariance function estimators in tensor product Sobolev–Hilbert spaces, which hold for both fixed and random designs, and incorporate a variety of spectral regularizations. For periodic functional spaces, in particular, the estimators can be shown to achieve a one-dimensional rate for a two-dimensional target, based on a new entropy upper bound.====The rest of the article is organized as follows. The proposed methodology is presented in Section ==== and computational issues are discussed in Section ====. The empirical performance of the trace-norm-regularized estimator is evaluated by a simulation study in Section ==== and a real data application is given in Section ====. Section ==== provides theoretical results and the article is concluded with Section ====” based on this article is available at ====.====The following is the Supplementary material related to this article. ",Nonparametric operator-regularized covariance function estimation for functional data,https://www.sciencedirect.com/science/article/pii/S0167947318301221,March 2019,2019,Research Article,578.0
"Wichitchan Supawadee,Yao Weixin,Yang Guangren","Department of Statistics, University of California, Riverside, CA 92521, United States,Department of Statistics, School of Economics, Jinan University, Guangzhou, 510632, China","Received 30 December 2017, Revised 7 May 2018, Accepted 7 May 2018, Available online 26 May 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.csda.2018.05.005,Cited by (6), (LRT) does not have the usual asymptotic ====. It is demonstrated that a simple application of GOF test statistics to finite mixture models can provide comparable or even superior hypothesis test performance compared to the existing cutting edge EM test method through extensive simulation studies. The effectiveness of GOF test to choose the number components is also demonstrated based on limited empirical studies and a real data application.,"Let ==== be a random sample from an ====component mixture model with the density in the form ====where ==== and ==== is the ====th component density with component parameter ==== and component proportion ====. The likelihood function of ==== is in the form ====For a general introduction to mixture models, see ====, ====, ====, ====, ====, and ====.==== be the parameter space for ====. A likelihood ratio test for testing ==== versus ==== is in the form ====where ==== and ==== are two specified subsets of ==== such that ==== for the mixture likelihood ====. The traditional asymptotic ==== distribution of likelihood ratio test (LRT) does not hold for the mixture likelihood. It is well known that the likelihood function of a finite normal mixture model ==== has unbounded likelihood ====, ====, ====. In addition, the mixture model is not identifiable under the null hypothesis. ==== and ==== prove that the LRT of ==== is generally unbounded for finite normal mixture models. In addition, the mixture model is not identifiable under the null hypothesis. Many studies consider the limit distribution of the LRT under some restrictions. For example, under the assumption of a compact parameter space, ==== investigate the large sample behavior of the LRT statistic for testing homogeneity under an unknown but equal component variance of normal mixtures; ==== derive the asymptotic distributions of LRT statistics for testing homogeneity in mixture regression models; ==== obtain the asymptotic distribution of the LRT statistics under some conditions in mixture models with or without a structural parameter. However, the asymptotic distribution of the LRT for general finite normal mixture models remains an open question.==== developed an elegant expectation–maximization (EM) test for testing the homogeneity in normal mixture models. ==== developed an EM test for assessing the number of components of finite mixture models with single parameter component distributions. ==== developed a likelihood-based EM test for testing the null hypothesis of an arbitrary number of components (====) in finite normal mixture models. This testing procedure is restricted to situations where all component mean parameters are different. Additionally, ==== proposed the likelihood-based testing procedure of the number of components in normal mixture regression models with heteroscedastic components. ====The rest of the article is organized as follows. In Section ====, we introduce a simple class of test procedures for hypothesis testing on finite mixture models based on goodness of fit test statistics. In Section ====, we use a simulation study and a real data application to demonstrate the effectiveness of the suggested test procedures. Section ==== contains a summary and discussion of the results we obtained.",Hypothesis testing for finite mixture models,https://www.sciencedirect.com/science/article/pii/S0167947318301142,April 2019,2019,Research Article,579.0
"Zhang Jin-Ting,Cheng Ming-Yen,Wu Hau-Tieng,Zhou Bu","Department of Statistics and Applied Probability, National University of Singapore, Singapore,Department of Mathematics, Hong Kong Baptist University, Hong Kong,Department of Mathematics and Department of Statistical Science, Duke University, United States,School of Statistics and Mathematics, Zhejiang Gongshang University, China","Received 6 July 2017, Revised 26 February 2018, Accepted 6 May 2018, Available online 16 May 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.csda.2018.05.004,Cited by (33)," consistent in detecting local alternatives. Simulation studies show that the proposed test outperforms several existing tests in terms of both size control and power when the correlation between observations at any two different points is high or moderate, and it is comparable with the competitors otherwise. Application to an ischemic heart dataset suggests that resting electrocardiogram signals may contain enough information for ischemic heart screening at outpatient clinics, without the help of stress tests required by the current standard procedure.",", ====, ====. We focus on the one-way ANOVA problem with functional responses ====, ====, ====, which is a fundamental problem in the inference but has received much less attention in functional data analysis compared to other problems such as regression, clustering, and classification ====, ====, ====, ====.====.====Let ====, ====, denote ==== groups of random functions defined over a given finite interval ====, where ==== is the number of observed functions in the ====th group. Let ==== with mean function ====. Throughout this article, we refer the within-function correlation to the correlation between observations on the function at any two different points i.e. ====. Assuming that ====the ==== (or the ====) for the functional data is to test the equality of the ==== mean functions; that is, ====against the alternative that at least two of the mean functions are not equal. In this problem, the ==== mean functions are often decomposed as ==== where ==== is the ==== and ==== are the ====.====There are several existing methods for testing the one-way functional ANOVA problem ====. For example, the ====-norm based test and the ====-type test for linear models with functional responses ====, ====, ====, ====, ====, ====-test was proposed by ====, naturally extending the classical ====-test to the current context; see also ====. Its test statistic is defined as ====where and throughout ==== denotes the total sample size, ====denote the pointwise ==== and ==== variations, respectively, and ====are the ==== and the ====, respectively. Then, the null hypothesis ==== is rejected as long as the pointwise null hypothesis ==== is rejected at some point ====.====There are a few advantages of using the above pointwise ====-test. When the functional data ====, ==== follows the same F-distribution for any given ==== at all of the points in ==== using the same critical value. On the other hand, the pointwise ====-test has some limitations too. In particular, for a given significance level, it is not guaranteed that alternative hypothesis is overall significant even when the pointwise ====-test is significant for every point in ==== at the same significance level. To overcome this difficulty, ==== proposed and studied a so-called ====, abbreviated as GPF test, whose test statistic is taken as the integral of the pointwise ====-test statistic over ====: ====Via some simulation studies, ==== showed that the GPF test is in general comparable with the ====-norm based and the ====-type tests, in terms of both size control and power. Further, via extensive simulation studies, ==== presented an exhaustive comparison of a number of existing tests and concluded that the ==== test is one of the best global tests.====Alternatively, we can globalize the pointwise ====: ==== (p. 234) briefly mentioned the use of the squared-root of ==== as the test statistic and a permutation-based critical value. By an application to a real dataset using a bootstrap-based critical value of the ==== test statistic, ==== realized that it may have higher power than the GPF test, particularly when the within-function correlation is high or moderate which is often the case for functional data. In addition, when applied to an ischemic heart ECG dataset as presented in Section ====, the ====-test.====The contributions of this paper are as follows, which are not offered by ==== (p. 234). First, we derive the asymptotic null distribution of the ====-test based on the NPB has the correct asymptotic level. Thus we can rely on the significance of the ====-test when it is applied to the ischemic heart data and ignore the fact that the GPF test is not significant for the same dataset; see Section ==== for the details. This result has important implications in ischemic heart screening in outpatient clinics. In other words, we can conclude that ECG signals may contain relevant features that are sufficient for the purpose of distinguishing between ischemic heart and normal groups, without the help of stress tests. Therefore this work calls for the need of some larger scale clinical studies to confirm the finding. Furthermore, we show that the ====-test is root-==== consistent, i.e., its power tends to ==== under local alternatives that depart from the null hypothesis at the ==== order.====Thirdly, we conclude from some simulation studies that the ====-test is preferred to the ====-type test ====, the ====-norm based test ====, and the GPF test ==== in the following senses. In terms of size control, generally the ====-test is comparable with the F-type test and it outperforms the other two competitors. In terms of power, the ====-test is substantially better than the other three tests when the within-function correlation is moderate or high; otherwise it is only slightly worse. Intuitively speaking, when the within-function correlation is moderate or high, the three competitors tend to average down the information provided by the functional data and hence they have lower powers. And, when the within-function correlation is low, they tend to summarize more uncorrelated information than the ====-test as the latter takes into account only the information available at the maximum of the process ====. Since the within-function correlation in functional data is usually moderate or high, the ====-test is therefore preferred to the three competitors in general. This also explains why we have the aforementioned contradictory results given by the ==== and GPF tests in the ischemic heart example, as the ECG signals at different points are highly correlated.====Lastly, we mention that it is straightforward to extend the ====-test to other linear regression models for functional data, including higher-way functional ANOVA (====, ch. 5) and functional linear models (====-test is negligible in terms of both size control and local power.====The methodology and the main theoretical results on the level accuracy and local power of the ====-test are presented in Section ====. Results of extensive simulation studies and the real data example on ischemic heart screening are given in Sections ====, ====, respectively. Proofs of the theoretical results are deferred to Section ====. The supplementary material contains a theoretical study on effect of discretization, and additional simulation studies to consider asymmetric error distributions, non-smooth functional data, complicated mean differences, and unequal group covariance functions, and to study the effect of discretization resolution.====The following is the Supplementary material related to this article. ",A new test for functional one-way ANOVA with applications to ischemic heart screening,https://www.sciencedirect.com/science/article/pii/S0167947318301130,April 2019,2019,Research Article,580.0
Dai Wenlin,"Statistics Program, King Abdullah University of Science and Technology, Thuwal 23955-6900, Saudi Arabia","Received 22 August 2017, Revised 28 February 2018, Accepted 18 March 2018, Available online 7 April 2018, Version of Record 5 November 2018.",https://doi.org/10.1016/j.csda.2018.03.017,Cited by (53),The direction of ,"Functional data are frequently observed in various fields of science, including but not limited to meteorology, biology, medicine, and engineering. Examples of functional data are temperature records from weather stations, curves capturing infant growth, expression levels of genes recorded over time, and hand-writing data, to name but a few. Over the past two decades, much work has been done that analyzes functional data, among which ==== focused on related inferential methods for functional data. Typically, each observation is a real function, either univariate or multivariate, defined on an interval, ====, in ====.====, to classify functional data ====, and to detect outlying curves ====, ====, medical science ====, and neural science ====.====), ====, random Tukey depth and integrated dual depth ====, band depth and modified band depth (BD and MBD; ====), spatial depth ====, ====, ==== depth ====, ====, and total variation depth ==== for univariate functional data; weighted modified band depth (WMBD; ====), simplicial band depth and modified simplicial band depth (SBD and MSBD; ====), and multivariate functional skew-adjusted projection depth (MFSPD; ====) for multivariate functional data. ==== proposed a formal definition of statistical depth for functional data from a topological point of view. ==== and ====, ==== investigated the consistency of non-integrated and integrated depths for functional data, respectively.====. Many outlier detection methods for functional data have been developed based on functional depth. ==== proposed several visualization tools and made a comparison of some outlier detection methods for functional data. ==== proposed a now-popular visualization and outlier detection tool named ====; this tool is based on MBD but the band depth can be replaced by other functional depths. ==== proposed an ==== that detects shape outliers by connecting two functional depths. ====, ====, ====.====Most existing functional depths, for instance, ID, MBD, MHD, MSBD, MFHD, and MFSPD, fall into the category of integrated functional depth: weighted average of point-wise depth. These point-wise depths are always non-negative scalars in the interval ====. Among these curves, 41 gray lines take fixed values across ====: ====, ====, and ====, ====, respectively, and a red curve is ====; see ==== (a). When we project these curves along the ==== direction, we get ====. What makes the red curve an outlier is the change of the direction of outlyingness, which unfortunately has never been considered in existing definitions of functional depths from the literature.====The remainder of this paper is organized as follows. In Section ====, we propose the framework of directional outlyingness for multivariate functional data. In Section ====, we derive an outlyingness decomposition from functional directional outlyingness and visually compare it with classical functional depths. Also, we study important properties and consistency of functional directional outlyingness. In Section ====, we construct an outlier detection procedure for functional data based on functional directional outlyingness. In Section ====. We end the paper with a discussion in Section ====. Proofs of the theoretical results are provided in ====.",Directional outlyingness for multivariate functional data,https://www.sciencedirect.com/science/article/pii/S016794731830077X,March 2019,2019,Research Article,581.0
"Ahonen Ilmari,Nevalainen Jaakko,Larocque Denis","Department of Mathematics and Statistics, University of Turku, Finland,Institute of Biomedicine, University of Turku, Finland,Health Sciences/Faculty of Social Sciences, University of Tampere, Finland,Department of Decision Sciences, HEC Montreal, Canada","Received 18 November 2016, Revised 9 January 2018, Accepted 19 January 2018, Available online 31 January 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.csda.2018.01.012,Cited by (3)," compared to more traditional one-class models. However, existing ",", ====, ====, ==== and a ====. The data are assumed to originate from a population consisting of multiple latent, unobserved classes. Depending on the class, a separate regression model between the outcome and the covariate applies. Assuming a Gaussian linear model for each component, the density function of ==== is ====where ==== is the number of classes, ==== is the density function of the normal distribution with mean ==== and variance ====, ====, ==== is the standard deviation of the error term in class ==== and ====. Models of this type are applied in a variety of fields including ecology, genetics, economics and marketing where such unobserved latent classes are often appropriate. The parameters of this model are typically estimated using the expectation maximization (EM)-algorithm ====, ====. Penalized versions of the FMR have also been proposed ====, ====, ====, ====.==== to better model possible nonlinear dependencies and interactions between the covariates. However, instead of trying to accurately capture the underlying mean functions, we aim to estimate the full mixture density ====, which can then be used for prediction purposes. Thus, our emphasis is on identifying ==== as a whole, not on the more conventional aim of recovering the underlying mean functions and the class labels. Semiparametric approaches for FMR with similar aims have been recently suggested by ==== and ====, whereas ==== discuss the problem from a functional data-analysis perspective. ==== assume that the outcome follows a mixture of Gaussians whose class means and variances are unknown smooth functions of the covariates. The functions are estimated by local smoothing accompanied with a modified EM-algorithm. Similar assumptions are made by ==== reduces the usefulness of these extensions. Thus, their applicability in high dimensional datasets is questionable.====If the finite mixture regression model is thought of as a supervised learning method, its unsupervised counterpart would be model based clustering. Similar to FMR, these methods assume that the data originate from a mixture of ==== are assumed to have the same mean ====. This corresponds to applying FMR on a multidimensional outcome with only the intercept term in the model. Recent reviews on model based clustering from a variable selection perspective are given by ==== and ====.====. The idea of MOE is to have a collection of different learners, each specializing to separate parts of the covariate space, to form the overall regression model. For example, in a simple case of a continuous outcome ====, one could consider a linear model ==== when ==== and a quadratic fit ==== when ====, a certain smoothing technique is used to ensure a soft transition. MOE and FMR can be seen as opposites in this matter: A MOE model invokes a strict (although soft) partition of the covariate space while FMR assumes global and constant class proportions. An extension of MOE, called hierarchical mixtures of experts (HME), allows the class probabilities ==== to depend on the covariates. Although there are similarities between the HME approach and the problem posed in this paper, HME models cannot be easily applied to high-dimensional data without compromising flexibility ====, ==== requires the definition of the correct structure for the linear predictor.====The paper is organized as follows. Section ==== considers methods for measuring the predictive power in the FMR setting and the construction of prediction intervals. In Section ====, the method is applied to a worker wage prediction dataset. Finally, the paper is concluded with a short discussion in Section ====.",Prediction with a flexible finite mixture-of-regressions,https://www.sciencedirect.com/science/article/pii/S0167947318300136,April 2019,2019,Research Article,582.0
"Cheng Ming-Yen,Guindani Michele,Lee Jae Won,Li Yi,Liu Catherine Chunling","Hong Kong Baptist University, Hong Kong,University of California, Irvine, USA,Korea University, Seoul, Republic of Korea,University of Michigan, Ann Arbor, USA,The Hong Kong Polytechnic University, Hong Kong","Available online 15 January 2019, Version of Record 7 February 2019.",https://doi.org/10.1016/j.csda.2019.01.004,Cited by (1),None,None,Special Issue on Biostatistics,https://www.sciencedirect.com/science/article/pii/S0167947319300052,April 2019,2019,Research Article,592.0
"Einbeck Jochen,Hinde John,Ingrassia Salvatore,Lin Tsung-I.,McNicholas Paul D.","Durham University, UK,National University of Ireland Galway, Ireland,University of Catania, Italy,National Chung Hsing University, Taiwan,McMaster University, Canada","Available online 20 December 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.csda.2018.12.006,Cited by (0),None,None,Editorial for the 4th Special Issue on advances in mixture models,https://www.sciencedirect.com/science/article/pii/S0167947318302858,April 2019,2019,Research Article,593.0
"Ferraty Frederic,Wang Jane-Ling,Wu Yichao","Toulouse Jean Jaures University, France,Colorado State University, USA,University of California Davis, USA,University of Illinois Chicago, USA","Available online 15 October 2018, Version of Record 5 November 2018.",https://doi.org/10.1016/j.csda.2018.10.009,Cited by (4),None,None,Editorial for the special issue on High-dimensional and functional data analysis,https://www.sciencedirect.com/science/article/pii/S0167947318302548,March 2019,2019,Research Article,595.0
"Lee Jae C.,Kontoghiorghes Erricos John,Colubi Ana,Park Byeong","Korea University, Republic of Korea,Cyprus University of Technology and Birkbeck, University of London, UK,University of Oviedo, Spain,Seoul National University, Republic of Korea","Available online 12 October 2018, Version of Record 22 October 2018.",https://doi.org/10.1016/j.csda.2018.10.006,Cited by (0),None,None,"Interview of Professor Stanley Azen, Founder of Computational Statistics and Data Analysis (CSDA)",https://www.sciencedirect.com/science/article/pii/S0167947318302512,March 2019,2019,Research Article,596.0
Afifi Abdelmonem A.,"UCLA Fielding School of Public Health, Biostatistics, Mail Code 177220, Los Angeles, CA 90095-1772, United States","Received 28 January 2018, Revised 28 July 2018, Accepted 26 August 2018, Available online 11 October 2018, Version of Record 22 October 2018.",https://doi.org/10.1016/j.csda.2018.08.024,Cited by (0),None,None,Stan Azen: A Master of Numbers and Notes,https://www.sciencedirect.com/science/article/pii/S016794731830207X,March 2019,2019,Research Article,597.0
