name,institution,publish_date,doi,cite,abstract,introduction,Title,Url,Time,Year,Type,Unnamed: 0
"Colombo Stefano,Filippini Luigi,Pignataro Aldo","Università Cattolica del Sacro Cuore, Largo A. Gemelli 1, Milano I-20123, Italy,Italian Regulatory Authority for Energy, Networks and Environment (ARERA), Piazza Cavour 5, Milano I-20121, Italy","Received 11 July 2022, Revised 22 May 2023, Accepted 27 May 2023, Available online 2 June 2023.",https://doi.org/10.1016/j.infoecopol.2023.101032,Cited by (0),"We study collusion sustainability in an infinitely repeated game in which firms might price discriminate, by offering personalized prices for the share of consumers they have information about. We do not impose any restrictions to the distribution of consumers and the product characteristic space. In such a general framework we show that when firms share their personal information about consumers, collusion is more difficult to sustain. We also show that, for intermediate levels of the discount factor, an antitrust policy aiming to discourage joint profit maximization and to maximize the ==== should allow information sharing between firms. Instead, a ban on information sharing is optimal only if firms have imperfect information about their own consumers.","The growth of the digital economy and the development of big data analytics, such as data mining and machine learning techniques, have enabled firms to refine their decision-making processes, to define their strategic choices more consciously and to share more easily the information they acquire. Nowadays, firms are used to adopt tracking tools to identify individual consumers with greater accuracy than even before, with the final aim of offering personalized and tailored prices (Shaffer and Zhang, 2002).==== However, as discussed by Gu et al. (2019), most of the data a firm can collect are exclusive and confer a competitive advantage with respect to the rival, by leaving the possibility to price discriminate. Hence, such information remains private unless consumers’ data are shared with the rival, bought from a third party (e.g., a digital platform) or made public through rules and regulations that increase market transparency.====Although privacy concerns have driven Public Authorities to limit the transfer of personal information (see e.g., the GDPR in Europe and the CPRA in California), little is known about the effects of sharing consumers’ data in a dynamic framework, in which firms might collude. Does a legal ban on sharing information facilitate collusion sustainability? By contrast, what is the effect of making the market transparent?====In this paper, we contribute to the current debate in the antitrust literature about the impact of sharing consumers’ personal information on collusion sustainability through a simple but general theoretical model. We develop an infinitely repeated game in which firms might offer personalized prices for the share of consumers they have information about. Based on Lederer and Hurter (1986), we study collusion sustainability in a regime with shared or unshared information, and we show that information sharing, by increasing the profits in the case of a deviation from the collusive agreement, makes collusion less sustainable. This highlights a novel tension between transparency and collusion: if price discrimination is viable, sharing information never facilitates the sustainability of a tacit agreement.==== Furthermore, we show that an antitrust policy aiming to maximize the consumer surplus should allow the firms to share their personal information about consumers in the case of an intermediate discount factor; when the discount factor is high or low, banning or not information sharing is irrelevant for the consumer surplus.====We also provide an extension of the model in which firms' decision to share or not information is endogenized and such a possible exchange of information allows firms to improve the degree of accuracy of their consumers’ data. In practice, firms jointly commit to one of the alternative information regimes before taking part of the pricing game. We show that the results of the baseline model hold in this setting and, interestingly, when the reservation price is not too large, collusion is more difficult to sustain as the information gathering process is more accurate and orientated toward the closest consumers. This means that, in that case, collusion can be facilitated if firms have a more balanced information between close and far consumers in the market rather than a wider knowledge only about their own consumers. By contrast, knowing better the closest consumers reduces the difference in terms of collusion sustainability between unshared and shared information regimes. Moreover, we point out some differences with respect to the baseline model regarding the optimal antitrust policy. When firms have imperfect information about their own consumers, punishment and collusive profits vary depending on whether information is shared or unshared. Specifically, they diverge as the information becomes less accurate. This leads us to consider not only the information regime that makes collusion more difficult to sustain, but also the optimal policy intervention that maximizes consumer surplus for any discount factor. As for the baseline model, when the discount factor is intermediate, sharing information is optimal as it prevents firms from colluding. However, if the policy intervention cannot affect the sustainability of collusion (because it is, or it is not, enforceable in both information regimes), banning the exchange of information is optimal as it reduces the consumer surplus extraction.====This paper is related to several strands of literature. First, it is related to those papers discussing the implications of information sharing on collusion sustainability. As pointed out by Stigler (1961), there are in theory several profits maximizing price structures. Therefore, colluding firms might find it difficult to coordinate, and information sharing might help firms finding a focal point for coordination. On the other hand, as emphasized by Genesove and Mullin (2001), exchange of information might reduce the monitoring problem: in particular, when it is not easy to distinguish whether changes in the own demand depend on market volatility or on deviation strategies by other firms participating to the agreement, information sharing might help properly detecting deviation and then making collusion more sustainable (Green and Porter, 1984). From a different perspective, Compte (1998) and Athey and Bagwell (2001) prove that sharing information about costs help firms reaching the most profitable collusive equilibrium within the set of the sustainable collusive agreements. Differently from these papers, our paper discusses an exchange of information regarding the preferences of the consumers. Each firm has private information about the preferences of a set of consumers: in the case of information sharing this information becomes available to every firm. We show that in this case collusion is less sustainable, since this kind of information exchange increases the profits in the case of a deviation while it does not affect the profits during the collusive and the punishment stages.====Our paper is closely related also to the growing literature studying the effect of big data and algorithms on collusion sustainability (Calvano et al., 2020a,b; Klein, 2021; Martin and Rasch, 2022). These advanced techniques allow to maintain supra-competitive prices and to make collusion easier to sustain and harder to be detected, so raising several concerns about their use in a dynamic framework (see e.g., OECD, 2017).==== Therefore, identifying those factors, such as the increase in market transparency, that might facilitate the emergence and sustainability of a cooperative agreement is crucial in order to enforce an efficient anti-cartel policy. This has generated a large discussion between scholars and policymakers about the (potentially anti-competitive) effects of more precise information. For instance, Liu and Serfes (2007), Miklós‐Thal and Tucker (2019), and Peiseler et al. (2022) analyse the impact of an increase in the information accuracy, in the sense of a more precise knowledge (Liu and Serfes, 2007, and Miklós‐Thal and Tucker, 2019) or lower probability of erroneous signals (Peiseler et al., 2022) about consumers’ preferences, on the sustainability of a tacit cartel. While Liu and Serfes (2007) and Miklós‐Thal and Tucker (2019) find that more precise knowledge has a negative impact on collusion sustainability, Peiseler et al. (2022) show that greater signal precision has an inverse U-shaped impact.==== Colombo and Pignataro (2022), instead, investigate the effects of a wider completeness of the information at firms’ disposal, and find that more complete information has in general a U-shape impact on collusion sustainability. Interestingly, Sugaya and Wolitzky (2018) share with this paper the idea that a better demand prediction can hinder the sustainability of a tacit cartel by increasing the deviation profits. They focus on a specific collusive strategy – i.e., the home-market principle according to which firms act as local monopolies – and they show that, under some conditions, increasing the market transparency, in terms of firms’ ability to monitor their competitors, can make collusion less likely to be sustained.====In contrast to these models, we do not impose any restrictions to the distribution of consumers and the product characteristic space, by providing a general setting that sheds light also on the (negative) effect of sharing information about consumers’ preferences and making the market transparent on collusion sustainability.====Finally, our paper also adds to the theoretical IO literature dealing with price discrimination and collusion. The policy debate about banning price discrimination to hinder collusion in a dynamic framework or making it lawful to foster competition in a static game is still open (see e.g., CMA, 2018). Liu and Serfes (2007) and Helfrich and Herweg (2016) argue that a ban on price discrimination has pro-collusive effects. In contrast, Gossl and Rasch (2020) show that the impact of banning price discrimination on collusion sustainability hinges on whether authorities order the use of linear or fixed fees only, whereas, by focusing on history-based price discrimination, Colombo and Pignataro (2022) show that a ban on price discrimination has pro-competitive effects when the degree of information accuracy is sufficiently high. In our paper, we do not consider a complete ban on price discrimination (which remains possible on those consumers each firm has information about), but rather a forbid of information sharing, which amounts preventing each firm to price discriminate by means of personalized prices on consumers belonging to the rival's market. We show that such a partial ban on price discrimination has pro-collusive effects.====The rest of the paper proceeds as follows. In Section 2 we introduce the model. In Section 3 we derive the main result under a general framework. In Section 4 we provide a robustness check by showing the result in a harsher punishment code and an application in the Hotelling (1929) framework. In Section 6, we extend the model to account for imperfect degrees of collusion and information. Section 7 presents some concluding remarks.","Information sharing, personalized pricing, and collusion",https://www.sciencedirect.com/science/article/pii/S0167624523000173,Available online 2 June 2023,2023,Research Article,0.0
"Posso Alberto,Zhang Quanda","Centre for International Development, RMIT University, Melbourne, VIC 3004, Australia,Institute of Innovation, Science and Sustainability, Federation University Australia, Mount Helen, VIC 3350, Australia","Received 14 June 2022, Revised 2 March 2023, Accepted 27 March 2023, Available online 28 March 2023, Version of Record 10 April 2023.",https://doi.org/10.1016/j.infoecopol.2023.101030,Cited by (0),"The economics literature views R&D as an important conduit for growth because it generates new ideas that can be translated into technological innovations. Some of this R&D occurs in universities, making academic freedom an important part of this process. This literature ignores the potential role that academic research in the social sciences plays toward achieving non-commercial societal outcomes. We bridge this gap by proposing that academia generates social R&D. We posit that greater degrees of academic freedom allow for social R&D to flourish and be transformed into policies that improve societal conditions. We test our hypothesis by studying the relationship between academic freedom and ==== using panel data of 132 countries over the 1967–2018 period. We measure academic freedom using an index developed by the V-Dem Institute. Our econometric analysis suggests that an increase in the index is associated with a decrease in inequality. We employ instrumental variable and interactive fixed effects techniques to try to lend support to the causal relationship between academic freedom and inequality. We argue that this negative relationship can be explained by academia, predominantly the social sciences, exerting pressure on governments to enact policies that redistribute wealth. We find evidence in support of this mechanism using data from other sources.","The economics literature places academic freedom within endogenous growth theory (Romer, 1990). Essentially, if economic growth is a function of human capital and R&D, then a vibrant and free academic community, free to undertake R&D, is an important conduit for growth. Academic freedom, therefore, is deemed important because it generates new ideas that can be translated into technological innovations, leading to new products and economic growth (Jaffe, 1989; Aghion et al., 2008; Berggren and Bjørnskov, 2021). In this setting, academics are thought of as engineers, physicists or computer scientists developing new products, such as Wifi, with significant real-world commercial applications.====While these areas of innovation are important, the existing literature ignores the potential role that research in the social sciences can play toward achieving non-commercial societal outcomes. This paper bridges this gap by proposing that academia also generates a type of social R&D, which focuses on improving societal conditions. Innovations in economics, political science and sociology can translate into new policies that aim to address social problems.==== We posit that greater degrees of academic freedom allow for social R&D to flourish and be transformed into policy. Societies with a greater degree of scholarly freedom will arguably be more likely to explore social issues critically, assess situations using both quantitative and qualitative data, and discuss their findings openly with a view to influence public debates. We present a conceptual model that discusses the channels through which academic freedom may influence social policy. Here university-government collaboration leads to the creation of new knowledge, which is reinforced and shared through formal and informal cross-institutional linkages. We then test our hypothesis by studying the relationship between inequality and academic freedom across countries. We argue that inequality is a relevant dependant variable in this setting given the preoccupancy that this issue has received in both classical and modern research (Christiansen and Jensen, 2019).====Inequality, for example, has been analysed in association with trade, economic growth, and education, while a large field in political economy looks at institutional factors and labor market characteristics (Roser and Cuaresma, 2016). Researchers have also studied different forms of inequality, including gender, ethnic-based, intergenerational, and geographical (Arrow, 1998; Erikson and Goldthorpe, 2002; Ponthieux and Meurs, 2015; Veenstra, 2011). Inequality has fuelled debates in social science and addressing it has influenced policy levers such as tariffs, taxes, subsidies and employment policies (Chaudhuri et al., 2018; Diamond and Saez, 2011). More generally, discussions on inequality and how to address it are central to the classical theories of Marx, Weber, Rousseau, Kuznets, Stolper and Samuelson, and several others (Christiansen and Jensen, 2019).====Our econometric analysis uses cross-country panel data consisting of 1497 observations from 132 countries over the period 1967 to 2018. Academic freedom is proxied with the index developed by the V-Dem Institute (Kinzelbach et al., 2021; Spannagel et al., 2020). The Academic Freedom Index (AFI) provides an aggregated measure that captures the ==== realization of academic freedom ranging from 0 (low) to 100 (high). It is made up of the following five indicators: (i) freedom to research and teach, (ii) freedom of academic exchange and dissemination, (iii) institutional autonomy, (iv) campus integrity, and (v) freedom of academic and cultural expression. The index is developed by asking approximately 2000 country experts to rate each of the indicators using a 5-point Likert scale. We match the AFI to inequality (Gini coefficient) data from the World Bank's World Development Indicators. This data set also provides us with a list of controls, normally included in regressions as determinants of inequality. In robustness exercises, we also employ an alternative data set that provides us with a longer time series (1902–2018). We use four alternative inequality indicators based on the share of income or wealth held by the richest 10 and 1 per cent, respectively. The wealth data is available for 8 countries, while the income data is available for 147 economies. The longer time coverage of these data means that we are only able to use a limited number of controls due to unavailability.====Our baseline analysis relies on standard country-level fixed effects regressions. In robustness exercises, we address potential endogeneity bias using instrumental variable (IV) as well as interactive fixed effects (IFE) techniques. Endogeneity is potentially problematic because greater inequality could lead to less demand for tertiary studies, naturally curtailing the academic sector. We use data on publications of scientific and technical journal articles per country by year as instruments for academic freedom. We show evidence to suggest that our instrument choice is adequate, albeit potentially weak. Our baseline regressions suggest that an increase in the AFI by one standard deviation is associated with a statistically significant decrease in the Gini coefficient by 0.10 index points. Our IV, IFE estimates and robustness exercises give consistent results.====We argue that the negative relationship between academic freedom and inequality can be explained by academia, predominantly the social sciences, exerting pressure on governments to enact policies that redistribute wealth. We test this mechanism by examining the relationship between academic freedom and institutional environment using data from the Worldwide Governance Indicators and the Country Policy and Institutional Assessment (CPIA). Our analysis shows a positive and statistically significant relationship between academic freedom and control of corruption, rule of law, regulatory quality, voice and accountability, gender equality, and policies for social protection. Those findings are broadly supportive of our hypothesis.====We contribute to a small literature on the economic consequences of academic freedom. As argued above, the existing literature can be placed within the broader endogenous growth literature (Romer, 1990) that treats academia as a conduit between innovation and total factor productivity. For example, Aghion et al. (2008) study the respective advantages and disadvantages of academic and private-sector research and identifies, theoretically, the process by which an idea transitions from academia to the private sector. These authors’ definition of academic freedom differs from ours in that it focuses on freedom to pursuit ideas, even if those may have little potential commercial value. The value of those ideas to an economy is simply put that often the potential commercial worth of a new idea cannot be ascertained until an idea is well developed. Academic freedom, in this case, from coercion from the private sector, ensures that academia can produce innovations without clear commercial relevance at inception.====Berggren and Bjørnskov (2021) provide an empirical assessment of the relationship between academic freedom, labor productivity, and total factor productivity growth using panel-data of 127 countries over the 1960–2015 period. They find that productivity is unrelated to academic freedom, unless the latter is interacted with the quality of judicial institutions. They show that marginal effect of academic freedom on productivity is positive and increasing when the quality of the judicial system is sufficiently high. Their findings suggest that institutional quality is vital for academic innovations to be employed by the private sector.====We contribute to this literature by looking at a previously unexplored association between academic freedom and social equity, namely inequality. Our approach is also different to previous studies in that our premise does not focus on the value of academic research to firm-level productivity and technological innovation, but on the influence that academia may have on social policy. As such, our approach recognises that universities produce innovative material in subjects such as economics, sociology, geography, political science, and anthropology. Our focus is, therefore, not on R&D resulting in technological innovation, but rather on social R&D resulting in policy innovation.====As such, we also contribute to a broader literature on academic freedom in social science.==== This literature can be categorised into two interrelated components: (i) normative and (ii) positive. The latter component can be further divided into two additional categories: (i) the determinants and (ii) consequences of academic freedom. The broad philosophical and normative literature focuses on the benefits of academic freedom to researchers, students, and society. This literature addresses issues such as the history of academic freedom in Europe and elsewhere (Thorens, 2006; Mama, 2006) as well as its broad societal implications, in particular its role in reinforcing free speech and democracy (Bok, 2021).==== While this study focuses on a positive, not normative, analysis of academic freedom, its normative implications are clear – academic freedom is an important conduit for social progress, leading to policy innovation and societal betterment.====The positive literature can be further divided into two strands. The first focuses on outcomes of academic freedom. Above we discussed the literature on economic growth and academic freedom as well as our contribution to this literature. Other studies in this field focus on less well-defined societal implications. For example, Eicher et al. (2018) suggest that academic freedom improves social infrastructure, which is broadly defined as the institutions and government policies that determine the economic environment. We contribute to this literature by more specifically highlighting a relationship between academic freedom and a societal outcome of policy relevance and academic interest – inequality. The second strand of the positive literature focuses on the determinants of academic freedom. Not surprisingly, the existing literature finds that political institutions positively affect academic freedom. Berggren and Bjørnskov (2022) find that democracy improves academic freedom. More specifically, they argue that bicameral legislatures and judicial accountability promote academic freedom. Overall, their findings suggest that limiting the power of the state strengthens and gives a voice to academia.====The rest of the paper is structured as follows. Section 2 presents a basic conceptual framework. Section 3 discusses the empirical strategy and data. Section 4 summarises our results and the final section concludes.",Social R&D: Does academic freedom contribute to improved societal outcomes?,https://www.sciencedirect.com/science/article/pii/S016762452300015X,28 March 2023,2023,Research Article,1.0
Kim Jeong-Yoo,"Department of Economics, Kyung Hee University, 1 Hoegidong, Dongdaemunku, Seoul, 137-701, Republic of Korea","Received 20 April 2022, Revised 5 January 2023, Accepted 11 January 2023, Available online 13 January 2023, Version of Record 27 January 2023.",https://doi.org/10.1016/j.infoecopol.2023.101019,Cited by (1),"Gautier and Somogyi (2020) showed that the monopolistic Internet service provider (ISP) can extract more surplus from consumers by giving priority to the weaker content to restore symmetry between content providers (CPs). In this study, we reexamine the issue and argue that their result depends critically on the shape of the delay cost function. We first show that under a linear delay cost, if the delay cost of contents from each CP increases with its own traffic amount, the opposite is true, that is, the ISP prefers to give priority to a strong CP, whereas it prefers to give priority to a weak CP if the delay cost of contents from an unprioritized CP decreases with its traffic amount. We confirm our insight in two specific models; the M/M/1 queuing model and the bandwidth subdivision model. We also discuss some implications of the ISP’s prioritization choice for social welfare.","The concept of net neutrality had a long history. In the era of the Roman Empire, all monopoly hostels, ports, and even surgeons in a community had an obligation to provide their services at a reasonable price. This is known as the principle of common carriage (common carrier) which is regarded as an essential element of net neutrality. This principle was adopted in the U.S. Communications Act of 1934 and inherited in the U.S. Telecommunications Act of 1996, although whether Internet service providers (ISPs) should be classified as Title I carrier with no common carrier obligation or Title II carrier with a common carrier obligation has been controversial. Formal net neutrality rules were established by Open Internet Order 2010==== and propelled by Open Internet Order 2015 when the U.S. Federal Communications Commission (FCC) classified ISPs as Title II carrier (common carriers).====However, in 2017, the FCC repealed the net neutrality rules that barred ISPs from blocking or slowing Internet content or from offering paid “fast lanes.” It is still controversial whether ISPs should be forced to be neutral, that is, treat all traffic the same or should be allowed to prioritize certain services over others. Nevertheless, it seems clear that certain services such as YouTube or Netflix require more bandwidth than others, that is, it seems sensible to prioritize such traffic because they would be almost unusable if streaming were subject to irregular delays.====Under the current regime, without net neutrality, it is important to decide for which priority should be granted. In a recent study, Gautier and Somogyi (2020) compared the outcomes with and without net neutrality when there is asymmetry between content providers (CPs). They showed that an ISP can extract more surplus from consumers by privileging the relatively weaker content to restore symmetry between CPs.==== In particular, with prioritization,==== they showed that it is preferable for an ISP to give priority to weak CPs because the resulting price is higher than the price when it gives priority to strong CPs. They used a specific function form to model the effect of delay on the utility of end users.====In this study, we argue that, in general, the priority of ISP grants crucially depends on the shape of the delay cost function. We show that if the delay cost of contents from each CP increases with its own traffic amount, the opposite is true, that is, the ISP prefers to give priority to the strong CP, whereas it prefers to give priority to the weak CP if the delay cost of contents from the unprioritized CP decreases with its traffic amount as in Gautier and Somogyi (2020).====The intuition behind this is as follows. If the ISP grants priority to the weak CP, there are two effects on the marginal end user who uses contents from a strong CP, namely, a positive transportation cost effect and a negative delay cost effect, because the traveling distance of the marginal end user is shorter but the delay cost is higher because of the absence of priority. However, the disadvantage in the delay cost (of content from the unprioritized CP) outweighs the advantage in the transportation cost for any degree of asymmetry if the delay cost of contents from the unprioritized strong CP (with higher demands) increases with the traffic amount. Consequently, the ISP will find it better to grant priority to the strong content. On the other hand, if the delay cost decreases with the traffic amount, the converse is true; the disadvantage in the delay cost is outweighed by the advantage in the transportation cost, meaning that it is better for the ISP to grant priority to weak content. We then examine our insight in two specific models; the M/M/1 queuing model of Choi and Kim (2010) and Cheng et al. (2011), and the bandwidth subdivision model of Economides and Hermalin (2012). It turns out that the M/M/1 model corresponds to the latter case, while the bandwidth subdivision model corresponds to the former. we We obtain the contrasting results that the ISP prefers to grant priority to the weak CP for any level of asymmetry in the queuing model, whereas it prefers to grant priority to the strong CP for any level of asymmetry in the bandwidth subdivision model.====We also obtain some policy implication by comparing social welfare when priority is granted to the weak CP and to the strong CP. We first show under a linear delay cost that if the delay cost of contents from each CP increases with its own traffic amount, it is socially optimal as well as privately optimal to grant priority to the strong CP, although the welfare comparison is ambiguous if the delay cost of contents from the unprioritized CP decreases with its traffic amount. We then show that in the queuing model, there is a conflict between the private incentive and the social incentive in terms of which CP is given priority. An ISP may grant priority to the weak CP in equilibrium, even though this is actually welfare inferior to the outcome when it grants priority to the strong CP. On the other hand, in the bandwidth subdivision model, it is privately and socially better for the ISP to prioritize the strong CP. However, in both models, it is socially worse than the outcome under net neutrality, regardless of who is given priority. This implies that regulation of net neutrality is needed in terms of static social welfare.====The remainder of this paper is organized as follows. In Section 2, we provide an analysis of the model with a linear delay cost to examine the ISP’s incentive to prioritize. In Section 3, we consider two specific models, the ==== queuing model and the subbandwidth model to address the issue. Section 4 discusses social welfare. Finally, concluding remarks and caveats are presented in Section 5.",Prioritization between asymmetric content providers,https://www.sciencedirect.com/science/article/pii/S0167624523000045,13 January 2023,2023,Research Article,2.0
Choi Eleanor Jawon,"College of Economics and Finance, Hanyang University, 222 Wangsimni-ro, Seongdong-gu, Seoul 04763, South Korea,Institute of Labor Economics (IZA), Schaumburg-Lippe-Strasse 5-9, 53113 Bonn, Germany","Received 9 August 2021, Revised 28 October 2022, Accepted 5 January 2023, Available online 9 January 2023, Version of Record 21 February 2023.",https://doi.org/10.1016/j.infoecopol.2023.101017,Cited by (0),"This study examines the effect of internet job search (IJS) on job-finding rates among unemployed job seekers during the rapid expansion of the internet from the mid-1990s to the early 2010s. To address endogenous selection into IJS, I use an ==== (IV) strategy exploiting the rise of IJS within occupations over time, which varied across occupations depending on pre-internet exposure to computers at work. The analysis sample includes unemployed workers from the December 1998, August 2000, September 2001, October 2003, and July 2011 Current Population Survey (CPS) Computer and Internet Use Supplements and the September 1992 Basic Monthly CPS, longitudinally matched with their employment outcomes from the subsequent monthly CPS files. The IV estimates indicate that IJS increased the 15-month job-finding rate by 12.9 percentage points (25.1% relative to the mean). Results from placebo exercises and various specification checks support a causal interpretation of the estimated effects. Additionally, the effectiveness of IJS remained stable over time throughout the analysis period.","Using the internet for job search and recruitment saw a rapid expansion after the advent of the internet in the mid-1990s. In the US, large commercial job sites, such as Monster.com, CareerBuilder.com, and Hotjobs.com, started in 1994–1996. America’s Job Bank was founded by the US government in 1995. Craigslist.org, a well-known non-profit advertisement site that includes job posts, was launched in 1995. During the late 1990s and early 2000s, a number of niche job boards and social networking sites, such as LinkedIn, were introduced. The proportion of unemployed workers searching for jobs online increased sixfold from 13% in 1998 to 79% in 2015 (see Fig. 1). Nowadays, the internet provides essential resources and tools for job seekers, and online job search has become even more prevalent with the rise of social media and mobile devices (Cumming, Bahn, Zickuhr, 2022, Pew Research Center, 2015, Zhao, 2019). On the employer side, already 94% of the Global 500 companies were recruiting online by 2003, up from only 29% in 1998 (Nakamura et al., 2009).====This study examines the effect of internet job search (IJS) on the employment prospects of unemployed job seekers during the rapid expansion of the internet from the mid-1990s to the early 2010s. While studies demonstrate that the diffusion of the internet substantially improved search efficiency in the product (Brown and Goolsbee, 2002) and marriage (Bellou, 2015) markets, research on internet advantages in job search processes is limited and mixed. Among the few studies focusing on IJS in the US labor market, Kuhn and Skuterud (2004) and Kuhn and Mansour (2014) find that until the early 2000s, unemployment durations among online job seekers were not shorter than their observably similar counterparts who only used offline search methods. Among young workers in the late 2000s, however, Kuhn and Mansour (2014) show that IJS was associated with shorter unemployment spells. I extend the seminal studies of Kuhn and Skuterud (2004) and Kuhn and Mansour (2014) by addressing endogenous selection into IJS and identifying the effectiveness of IJS.====The analysis sample includes unemployed workers aged 16 or above from the December 1998, August 2000, September 2001, October 2003, and July 2011 Current Population Survey (CPS) Computer and Internet Use Supplements and the September 1992 Basic Monthly CPS. Utilizing the CPS survey structure, I longitudinally link individuals from these six baseline surveys with their employment outcomes up to 15 months after, which are observed in subsequent waves of the monthly CPS. Given that the internet was unavailable for general use until 1994, respondents from the September 1992 CPS serve as the pre-intervention sample, in which no one engaged in IJS activities.====As individuals freely decide whether to use the internet for job search, IJS status would be confounded with job seekers’ observable and unobservable characteristics. To address this selection problem, I use an instrumental variables (IV) strategy exploiting the rise of IJS within occupations over time, which varied across occupations depending on pre-internet exposure to computers at work. The instrument is constructed by multiplying a time trend with the occupation-specific computer use rate at work before the emergence of the internet, which is computed using data from the October 1993 CPS School Enrollment Supplement. This empirical strategy is effectively a difference-in-differences IV design in that I exploit variation within occupations over time before and after the intervention in 1994. The instrument strongly predicts differential growth in IJS adoption across occupations.====The key identifying assumption of this IV strategy is that pre-internet computer use intensity cannot predict within-occupation ==== in job-finding rates without IJS expansion. This assumption would be violated if changes in labor market conditions were systematically correlated with pre-internet computer use intensity across occupations. To address this concern, I perform multiple internal validity checks. First, I conduct a placebo exercise illustrating that the instrument has no predictive power for employment outcomes before internet expansion. Second, I show that the IV estimates are robust to a range of specification checks, especially to the inclusion of control variables representing concurrent labor demand and supply conditions. Lastly, I show that the instrument is unrelated to the probability of switching an occupation, which may exclusively affect the job-finding rate. Results from these exercises suggest that the instrument is unconfounded with preexisting or contemporaneous occupation-specific trends in employment outcomes and thus unlikely to violate the exclusion restriction.====I find that online job seekers are substantially more likely to be employed at various follow-up points than those only searching offline. The IV estimates show that IJS raises the probability of finding a job within the 15-month follow-up period by 12.9 percentage points (25.1% relative to the average employment probability of 0.514). As results from the aforementioned placebo exercise and various specification checks support a causal interpretation of the estimated effects, the IV estimates provide evidence that IJS is effective in facilitating (re-)employment opportunities for the unemployed and reduced search frictions in the labor market. However, the benefits of IJS do not appear in the ordinary least squares (OLS) regression controlling for observable characteristics only. Thus, the difference between the IV and OLS estimates suggests that online job seekers are negatively selected on unobservables. Moreover, I document that IJS effectiveness and selection patterns remained stable over time throughout the analysis period.====This paper contributes to the under-studied literature on online job search and its effectiveness. Kuhn and Skuterud (2004) and Kuhn and Mansour (2014) highlight that online job seekers are positively selected on observables and appear to be negatively selected on unobservables. Only a few studies circumvent this endogeneity problem. Among them, two studies focusing on the expansion of a specific search engine find mixed results. Bagues and Labini (2009) study AlmaLaurea, a job search engine founded in 1994 by a consortium of Italian universities to facilitate the school-to-work transition of college graduates. Their analysis shows that AlmaLaurea lowered the unemployment rate among graduates of AlmaLaurea member universities. On the other hand, Kroft and Pope (2014) find that the expansion of Craigslist between 2005 and 2007 had no measurable impact on local area unemployment rates in the US. More recently, Bhuller et al. (2021), Denzer et al. (2021), and Gürtzgen et al., 2021 exploit geographic and temporal variation in broadband roll-outs in Germany and Norway. They show that better access to the internet has enhanced both IJS usage and employment prospects among the unemployed. This paper is differentiated from the recent prior works in three aspects. First, the endogeneity of IJS behaviors is addressed at the individual level. Second, I provide more direct evidence of the effectiveness of IJS in the context of the US labor market. Lastly, I examine whether IJS effectiveness and selection patterns have changed over time.====The remainder of this paper is organized as follows. Section 2 describes the construction of the analysis sample and key variables from the CPS data. Section 3 explains the IV estimation strategy used to address the selection problem in the IJS status and measure the effect of IJS on job-finding rates. Section 4 presents the results of IV estimation and internal validity checks. Section 5 discusses the mechanisms and implications of the findings. Section 6 concludes.",Does the internet help the unemployed find jobs?,https://www.sciencedirect.com/science/article/pii/S0167624523000021,9 January 2023,2023,Research Article,3.0
"Katz Raúl,Jung Juan","Columbia Institute for Tele-Information, Columbia University, United States,ICADE, Facultad de Ciencias Económicas y Empresariales, Universidad Pontificia Comillas, Spain","Received 10 January 2022, Revised 29 August 2022, Accepted 4 January 2023, Available online 7 January 2023, Version of Record 17 January 2023.",https://doi.org/10.1016/j.infoecopol.2023.101016,Cited by (0),"In this article we study the impact of taxation in the performance of the telecommunications sector. To do so, we develop a model that considers the taxes and fees imposed directly or indirectly along the telecommunications value chain. Overall, we find strong evidence of a negative impact on investment from an increase in regulatory fees, profit taxes, and ","As supported by extensive research, digitization has been identified as a key driver of productivity and economic growth. This evidence has usually provided support to the launch of national digital agendas to spur the development and adoption of digital technologies by consumers, enterprises, and governments. Undoubtedly, the core of the digital revolution lies in the mass adoption of telecommunications networks, and in particular, of high-speed internet connectivity. Considering that still a third of the world population does not use internet,==== a top priority for policymakers is to develop suitable frameworks that facilitate closing the digital divide and accelerate the economic impact of digitization.====In addition to competition policy and regulation, the development of digital infrastructure is influenced by fiscal policy, particularly because of its potential effect in stimulating (or discouraging) investment and adoption levels. As it is the case with every economic sector, telecommunication operators face the imposition of general taxes such as income taxes, while ICT services purchased by consumers are usually subject to Value Added Tax (VAT). In addition, however, other specific levies and contributions are imposed on this sector: license fees, spectrum payments (one-off or recurrent), excise taxes, and universal service contributions are some examples of these additional obligations.====The application of taxes, charges and fees in the telecommunications industry is not homogeneous worldwide. Matheson and Petit (2017) argue that governments have “conflicting objectives” regarding the tax treatment of the telecommunications industry, because, on the one hand, they know about the positive externalities generated by this sector on the economy, while on the other hand, they perceive telecommunications operators as being a good source of revenues, due to the sector's formal status and its large turnover. Therefore, it is not surprising that there are multiple taxation approaches applied to this industry. They reflect not only a country's general fiscal framework but also trade-offs around public policy objectives for the sector. Generally, and depending on the tax burden applicable to both operators and consumers, we can identify different fiscal models: on the one hand, countries that have decided to impose a reduced taxation burden to stimulate adoption and investment; and on the other hand, countries that rely on the telecommunications industry to maximize government revenues through taxes.====In developing fiscal policies, governments need to consider the trade-offs between revenue generation and the potential negative impact on the sector's development and performance. As the evidence regarding the positive socioeconomic impact of digital industries continues to grow, the argument to reduce potential distortions emerging from over-taxation of the sector is gaining ground (ITU, 2022). Furthermore, high fiscal pressures over the telecommunication industry may compromise the sector's long-term sustainability, operating in a context of revenue decline==== and increasing investment required to deploy next generation networks.====In this context, the objective of this paper is to provide a comprehensive framework to study, both theoretically and empirically, the incidence of taxation in the telecommunications industry. While there are several academic articles that have studied the economic effects of taxation broadly, to the best of our knowledge no published paper has yet addressed the issue exhaustively for the telecommunications industry. Considering that the telecommunications sector is so critical for economic development (Hardy, 1980; Karner and Onyeji, 2007; Jensen, 2007; Katz et al., 2008; Fornefeld et al., 2008; Koutroumpis, 2009; Katz et al., 2012; Rohman and Bohlin, 2012; Mack and Faggian, 2013; Arvin and Pradhan, 2014; Katz et al., 2020), while, at the same time, subject to several impositions beyond regular goods, the main objective of this paper is to fill this gap in the literature. This is relevant because, as highlighted by Matheson and Petit (2017), many of the fiscal instruments currently applied to telecommunications operators generate market distortions, negatively affecting efficiency, affordability, and growth. Along these lines, the contribution of this paper is threefold. First, we provide a comprehensive review all possible taxes, fees and obligations affecting the telecommunications sector. Next, we estimate the impact of those impositions across the whole value chain of the telecommunications sector -from investment decisions to final adoption by end consumers. Finally, we simulate a fiscal scenario of tax reform to provide policymakers with useful inputs on potential policies.====The remainder of this article is structured as follows. Section 2 reviews the research literature on taxation and telecommunications. Section 3 presents, from a theoretical viewpoint, all the different taxes and fees identified as potentially affecting this industry. Section 4 specifies the empirical model estimating the tax impact on each stage of the value chain of the telecommunications sector. Section 5 presents the dataset and main descriptive statistics. Section 6 reports the results from the econometric model estimates. Section 7 presents a simulation scenario of potential fiscal reform, by applying the parameters estimated in the econometric models to the estimation of sector performance. Section 8 summarizes the conclusions of the analysis and draws policy implications.",The impact of taxation in the telecommunications industry,https://www.sciencedirect.com/science/article/pii/S016762452300001X,7 January 2023,2023,Research Article,4.0
García-Uribe Sandra,"Banco de España. Alcalá, 48, Madrid, 28014, Spain","Received 10 December 2021, Revised 4 October 2022, Accepted 16 October 2022, Available online 23 October 2022, Version of Record 1 November 2022.",https://doi.org/10.1016/j.infoecopol.2022.101006,Cited by (0),"This paper studies front-page choices made by editors of major US newspapers. I document that newspaper front pages are biased to certain combinations of news after controlling for the newspaper bias and the overall market coverage of such news. I also provide a reader-maximization model for front-page decisions that I use to interpret the empirical biases as preferences of the newspaper population of target readers. Through the lens of my model, my estimates recover maps of complementarities among pairs of topics for each newspaper and I find that these contribute to the probability that news on a topic appears on the front page.","Mass media play a fundamental role in the provision of information to society; indeed, there is evidence that news affects the behavior of political agents (Eisensee, Strömberg, 2007, Durante, Zhuravskaya, 2018), collective action (Hendel et al., 2017), and financial decisions (Fang, Peress, 2009, Garcia, 2013), among others. Although lead news stories receive major attention every day, there is limited evidence of what forces govern them. Days after Lehman Brothers collapsed, on the day of the US administration’s first bail-out proposal, economic news was the most relevant topic in the media, followed by political news and, as distant third, by other topics. However, the ==== decided to devote half of its front page to legal news, whereas the ==== had full coverage of the economy. In this paper, I show that complementarities between certain news items are an important determinant in the choice of top news stories of major news outlets.====This paper studies the choice of front-page news made by editors of major printed newspapers in the USA. Front-page news not only plays a special role in determining public awareness of events, but is also a clear-cut, observable outcome on a daily basis that is amenable to systematic scrutiny. The literature has so far centered on measures of news slant along the one-dimensional left-right political spectrum (Groseclose, Milyo, 2005, Gentzkow, Shapiro, 2010). The basic motivation for such focus has been that information about politics affects political attitudes, and, eventually, voting behavior and political outcomes. However, the notion of bias may be applied more generally to the choice of lead news or bundle of news that a media outlet decides to emphasize among the relevant news on a particular date. Understanding such reporting patterns is also important because of their potential effects not only on political attitudes, but more broadly on lifestyles, including values, world views, or health.====The first contribution of this paper is to provide an empirical framework for measuring news slant across bundles of news among major US daily newspapers. My measure of slant of a newspaper’s two main news items is a double difference average quantity. The first difference measures the inclination of the newspaper to a set of news items given the importance of such news on a day, and the second measures the inclination to the combination of different news items. Therefore, this magnitude is a measure of slant toward a combination of news items net of the inclination of the newspaper to each piece of news. Moreover, the notion of slant is relative to the overall importance of the news in a day, that is, it accounts for deviations of front page choices relative to a common benchmark across newspapers.====A second contribution of the paper is to provide a readership-maximization model under which my empirical measure of multidimensional slant captures the preferences of readers for particular sets of news items, given the importance of those items on a given day. Gentzkow and Shapiro (2010) found that newspaper slant is mostly explained by firms responding to consumer preferences. If this is true for political slant, a model of reader maximization seems natural for interpreting choice data over broader news topics.====The third contribution of the paper is empirical. Although front pages are observable, there is no obvious way to estimate multidimensional measures of slant. To identify my measures of slant, I exploit variation in news relevance across different topics and days. To construct measures of news relevance, I use lead news choices of a number of mass media. The News Coverage Index Data (NCID) of the Pew Research Center for Journalism provides suitable data for this purpose which the literature has not exploited yet. It contains lead news choices from multiple US mass media outlets for the period 2007 to 2012.====As an output of the estimation, I obtain maps of complementarities among pairs of news topics for each of the five major US newspapers. These maps show the extent and the directions in which each newspaper deviates from the aggregate flow of news in the US media. Moreover, under the utility-based interpretation, they also speak to the preferences over pairs of topics of the target population of readers associated with each newspaper. The existence of such complementarities implies that the probability of some piece of news on a particular topic making it to the front page depends not only on the importance of the news on a particular day and the newspaper’s bias to the news, but also on the cross-effect of the satisfaction associated with that news’s appearing alongside other news. There is scarce evidence on multidimensional slant: Larcinese et al. (2011) studies the supply incentives for some economic indicators coverage, Eisensee and Strömberg (2007) provides empirical evidence that newsworthiness substitution in the media is an explanation to disaster relief across different episodes in the US and Durante and Zhuravskaya (2018) find empirical support to the hypothesis regarding the strategic timing of unpopular political measures made by politicians in expectation of other newsworthy events. More recently, Qin et al. (2018) starts the analysis from an abstract point of view about newspaper content to build a unidimensional measure of bias from which to study the effect of competition. To my knowledge, this is the first paper to document the presence multidimensional slant in the media.====This paper also contributes to the literature that empirically studies the trade-offs between the market relevance of news and preferences on characteristics of news under the same decision framework. Earlier works have considered either one or the other factor for newspaper decisions. I can distinguish between preference for topics and preference for the relevance of topics within the newspaper by comparing days with different numbers of news items for each topic in the media and the repeated choice of the newspaper. Evidence on these forces is also a test for the market behavior of the mass media.====I find that the ====’s complementarities to economic news imply that half of the probability that it publishes economic news on the front page is due to the satisfaction of combining it with other topics. Something similar happens with political news: almost half of the probability of its reaching the front page is due to positive cross-effects. Alternatively, the probability that legal news makes it to the ====’s front page is half of what it could be due to the dissatisfaction of publishing it alongside other topics. The probability of legal news is 3.5 percent, -2.3 percentage points of which are explained by negative cross-effects with other topics, especially with economic news. The empirical results also indicate that the relative importance of complementarity drops on days in which a particular topic is dominant, although complementarities still play an important role on those days.====The estimates of slant for each newspaper and combination of news are robust to a different classification of news, alternative measures of news relevance and correlation of news relevance across pairs of topics. Moreover, I perform multiple testing procedures to document the statistical significance of complementarities in news reporting for major US newspapers.====The structure of the paper is as follows. I introduce the measure of multidimensional slant in Section 2. In Section 3, I describe the dataset and the construction of measures of news relevance. Section 4 deals with the empirical methodology. In Section 5 I define the decision problem for the newspaper choice of front-page news and discuss a set of assumptions under which the theoretical model matches the empirical specification; thus, I offer a utility-based interpretation for the evidence on multidimensional slant in the data. Section 6 contains the full set of empirical results and Section 7 concludes.",Multidimensional media slant: Complementarities in news reporting by US newspapers,https://www.sciencedirect.com/science/article/pii/S0167624522000452,23 October 2022,2022,Research Article,7.0
"Wen Huwei,Wen Changyong,Lee Chien-Chiang","Research Center of the Central China for Economic and Social Development, Nanchang University, Nanchang 330031, China,School of Economics and Management, Nanchang University, Nanchang 330031, China","Received 22 April 2022, Revised 4 October 2022, Accepted 21 October 2022, Available online 23 October 2022, Version of Record 23 November 2022.",https://doi.org/10.1016/j.infoecopol.2022.101007,Cited by (9),"Clean production and digitalization are the main directions of global manufacturing development under the new industrial revolution. Based on the application of digital technologies in enterprises caused by the smart city construction in China, this study adopts the difference-in-difference method to investigate the impact of digitalization and environmental regulation on total factor productivity. Using panel data on Chinese manufacturing listed enterprises from 2008 to 2019, this study finds that digitalization and environmental regulation can significantly improve the total factor productivity. It also shows that digitalization improves the total factor productivity of manufacturing enterprises by reducing transaction costs, facilitating servitization, and stimulating innovation investment. In addition, environmental regulation can force manufacturing enterprises to transform and improve their total factor productivity. However, the interaction between environmental regulation and digitalization has a significant negative impact on total factor productivity, implying that the paths of clean transformation and digitalization transformation is not coordinated. Our findings have implications for enacting effective policies to help manufacturing enterprises achieve digital transformation and clean production.","The industrial revolution is the main force that changed the technological paradigm of manufacturing industry. On the basis of the three industrial revolutions, the manufacturing industry is entering the era of the fourth industrial revolution, which is brought about by the rise of the digital revolution in recent decades (Lee et al., 2022; Li et al., 2021). As is different from last three ones, the Fourth Industrial Revolution explores to promote efficiency throughout the whole value chain by relying on digital technologies. Specifically, these digital technologies are widely used in all aspects of manufacturing production and business activities, including production, circulation, marketing, after-sales service, and research and development. Resources and environment are paid more and more attention in modern economic growth theory. A series of researches on global economic growth potential by ""Rome Club"" in 1970s aroused widespread concern about sustainable development. This triggered the first wave of global debate on whether sustainable development could be achieved. Henceforth, environmental sustainability has also become a key issue in industrial development, and the development of the global manufacturing industry faces stronger environmental regulations than before (Lee and Lee, 2022; Zhu and Lee, 2022; Wen et al., 2023). The international goal of sustainable development requires clean production techniques in manufacturing. Hence, clean production and digitalization are the main directions of global manufacturing development under the new wave of scientific and technological revolution (DeStefano et al., 2018; Lee et al., 2021; Wen et al., 2021). This study aims to investigate that how clean production and digitalization affect manufacturing development and total factor productivity of manufacturing enterprises.====As the largest manufacturing country, China plays a leading role in the digitalization and cleaner production transformation of manufacturing industry. The manufacturing industry in China has always been characterized by high inputs, low quality, and low efficiency, which cause problems of excess capacity and high pollution (Wen and Zhao, 2021; Ye et al., 2022; Zheng et al., 2022). Carbon emissions in Chinese manufacturing sector have reached their highest level in nearly two decades, reaching 6.5 billion tons, up from 1.9 billion tons in 2001, according to the World Bank database. To promote the cleaner transformation of the manufacturing industry, the Chinese government has implemented strict environmental policies and restrained the polluting behavior of manufacturing enterprises (Wang and Shen, 2016; Lv et al., 2021). Environmental regulation would increase the operating cost of enterprises and thus exerts a direct impact on enterprise performance. Although Porter's hypothesis argues that environmental regulation forces manufacturers to invest in innovation activities and improve their competitiveness (Porter and Van der Linde, 1995;Liu and Xie, 2020; Wang and Lee, 2022), the establishment of the Porter effect involves certain boundary conditions. Hence, how does environmental regulation affect total factor productivity? Does Porter's hypothesis hold true in the new era?====The new round of industrial revolution provides an opportunity for the transformation of manufacturing enterprises. The emerging digital technologies, such as the ====, Internet of Things and edge computing, are adopted to promote the digital transformation of the manufacturing industry (Nambisan, 2017; Rindfleisch et al., 2017). With the in-depth application of digital technologies in manufacturing industry, digitalization provides many opportunities for industrial development, and the trend of digital transformation for manufacturing sector is inevitable. However, the research on the nexus of digitalization, environmental regulation and total factor productivity remains limited (Li et al., 2020; Dai et al., 2021). Research in this area can be traced back to the Solow paradox, which claims that information and communication technologies (ICT) could not increase the productivity of American enterprises (Zhang et al., 2021). Many studies have also explored whether Solow's paradox holds true in various scenarios (Stanley et al., 2018; Wannakrairoj and Velu, 2021). Similar to Porter's hypothesis, it is worth discussing whether Solow's paradox exists in the digital economy era. In addition, the nexus of digitalization, environmental regulation and productivity must be clarified for the manufacturing industry undergoing digital transformation and cleaner transformation.====For the purpose of discussing questions above, this study employs the panel data of China's manufacturing A-share listed enterprises from 2008 to 2019 to discuss the path and influence of digitalization on the total factor productivity. The moderating effect of environmental regulation on total factor productivity are also introduced. Specifically, we treat the pilot policy of smart city construction in China as the proxy variable of digitalization and use the difference-in-difference (DID) model to investigate the impact of digitalization on enterprise total factor productivity and its influencing mechanisms. Besides, this study also verifies the role of environmental regulation in the nexus of digitalization on total factor productivity by heterogeneity analysis and moderating effect analysis. This study finds that digitalization can improve total factor productivity (TFP), while environmental regulation can weaken the productivity effect of digitalization.====Our study contributes to the existing literature in several aspects. First, we regard a pilot policy as the shock for manufacturing enterprises to adopt digital technologies and provide reliable evidence on the causality between digitalization and total factor productivity. By evaluating the pilot policy of digitalization on enterprise total factor productivity and its transmission path, endogenous interference can be avoided, and the reliability of the results can be improved. Second, we take Chinese listed manufacturing enterprises as an example to provide empirical evidence for testing Porter's hypothesis and to expand the research scope of this hypothesis. Last, we investigate the moderating effect of environmental regulation on the nexus of digitalization and total factor productivity and verify the harmony between digitalization and environmental regulation. This study provides an empirical basis for developing countries to formulate cleaner production and intelligent manufacturing policies.====This paper proceeds as follows. The second section discusses the relevant literature and the hypotheses. Section 3 introduces the background, methods and data. Section 4 shows the results on the impact of digitalization on enterprise total factor productivity. Section 5 provides the results for the role of environmental regulation. The last section presents the research conclusions.",Impact of digitalization and environmental regulation on total factor productivity,https://www.sciencedirect.com/science/article/pii/S0167624522000464,23 October 2022,2022,Research Article,8.0
Saruta Fuyuki,"Graduate School of Commerce, Doshisha University, Karasuma-higashi-iru, Imadegawa-dori, Kamigyo-ku, Kyoto, 602-8580, Japan","Received 22 October 2021, Revised 7 September 2022, Accepted 30 September 2022, Available online 4 October 2022, Version of Record 17 October 2022.",https://doi.org/10.1016/j.infoecopol.2022.101005,Cited by (0),"This study presents investigation of the effects of vertical integration between an internet service provider (ISP) and a content provider (CP) on the ISP’s zero-rating choice and social welfare. We develop a simple model in which a monopolistic ISP delivers content from two CPs to a representative consumer. The ISP can offer zero-rating contracts to one or two CPs, thereby allowing the consumer to use zero-rated content without consuming monthly data usage. We investigate how integration between the ISP and a CP affects the ISP’s zero-rating choice and social welfare. Our findings are the following. First, the vertically integrated ISP might zero-rate the unaffiliated CP exclusively when the CPs’ profitability is not so low. Second, the integration increases the total surplus and the independent CP’s profit. Our results indicate that vertical integration is welfare enhancing and beneficial to the independent CP.","Zero-rating is a commercial tactic that is frequently observed in mobile internet service markets.==== With internet plans for mobile phones, internet service providers (ISPs) often set monthly caps on subscribers’ data consumption, which represent upper bounds of total data use. Zero-rating plans==== designate the data of particular CPs as free content. Plan subscribers can use zero-rated contents as much as they want without additional payment. ISPs offer mobile internet plans with zero-rating to collect zero-rating fees from CPs or to promote their own content.====The practice of zero-rating is controversial. Some debate arises on the subject of whether it violates the principle of “net neutrality.” The term “net neutrality” is generally used to mean that ISPs must not favor certain content or charge CPs for delivering content. This principle is based on the idea that an open internet is necessary for technological innovation and economic growth. Moreover, zero-rating might hinder fair competition among CPs. Particularly when ISPs zero-rate their content or their affiliated CPs’ content, the plans become subject to investigation and regulation.====Proponents of zero-rating, mostly ISPs, argue that (sponsored) zero-rating plans benefit consumers because they can enjoy services for free. Additionally, they argue that there is no violation of fair competition because all CPs can choose to become zero-rated CPs. It might be the case in which ISPs hold an unspoken belief that not only ISPs but also CPs should pay for consumers’ data use because of the rapid growth in internet usage. Opponents of zero-rating criticize it because consumers are unable to choose the content included in zero-rating plans. They must pay a higher price if they want to enjoy content that is not zero-rated. They also point out that, in the long run, the price of plans can increase and that zero-rating discourages the development of new content.====Zero-rating has been prohibited in some countries such as Canada, Brazil, and India because it is regarded as harming net neutrality. Nevertheless, the debate over its effects remains active in some countries. For example, in the US, despite the Open Internet Order intended to protect net neutrality, the Federal Communications Commission (FCC) reported in 2017 that it would end its investigation into zero-rating, effectively allowing zero-rating plans.==== In addition, the FCC has repealed the Open Internet Order, and jointly with the Federal Trade Commission, has adopted a stance of allowing ISPs more freedom.==== In response, several consumer protection groups and US states that support net neutrality are actively fighting zero-rating. Particularly in California, a stringent net neutrality law was passed in 2018.==== This law forced AT&T to discontinue its plan that zero-rates its video streaming service in 2021.==== In the European Union in 2021, the Court of Justice of the EU ruled that zero-rating plans offered by Telecom Deutschland and Vodafone violated EU law (Court of Justice of the European Union, 2021).====Although much debate has persisted on the issue of whether zero-rating violates net neutrality, no consensus has been reached on its actual effects on competition among CPs and social welfare. To fill this gap, we construct a simple monopolistic ISP model in which the ISP chooses the optimal zero-rating plan and then sets the data cap and subscription fee for consumers. Moreover, we examine whether the results vary depending on whether the ISP is integrated with a CP. We demonstrate that vertical integration between the ISP and a CP might alter the ISP’s zero-rating choice and consequently increase the total surplus.====To elaborate on this point, this study examines a situation in which a monopolistic ISP delivers content from two CPs to a representative consumer. Then we analyze the following two-stage game. First, the ISP makes zero-rating offers with zero-rating fees to one or both CPs if necessary. The CP or CPs which receive the offer decide whether to accept or decline it. Then the zero-rated CPs pay the zero-rating fee to the ISP. Second, the ISP sets the subscription fee and the data cap for the consumer, who chooses whether to join the plan. A consumer who subscribes to the plan determines the consumption amount for each type of content.====We derive the following results. First, the independent ISP implements open zero-rating (i.e., zero-rating with all CPs) or makes no zero-rating offers. Zero-rating increases the ISP’s operating cost and the revenue from the consumer through subscription fees. Moreover, the ISP receives zero-rating fees from CPs. When the increase in cost outweighs the increase in revenue, the ISP implements open zero-rating; otherwise, it chooses net neutrality. This result might represent a situation in which consumers are permitted to use data freely and their data use is too large for ISPs to bear alone, thereby leading the ISPs to offer zero-rating plans so that CPs contribute to paying for the consumers’ data use.====Second, if the ISP is integrated with a CP, then the integrated ISP might exclusively zero-rate the unaffiliated CP. The ISP chooses exclusive zero-rating with the unaffiliated CP or no zero-rating when the CP’s advertising revenue per unit of consumption is lower than the ISP’s marginal cost. The integrated ISP internalizes the revenue of the affiliated CP. Therefore, when the ISP chooses the exclusive zero-rating with the unaffiliated CP, it can collect a zero-rating fee from the CP while adjusting the data caps to ensure that the usage of the integrated CP is at an optimal level. Conversely, when the ISP chooses net neutrality, it can control the consumption of contents of both types to the second-best level. When the CPs’ advertising revenue is lower than the ISP’s marginal cost, then the integrated ISP finds that these plans are more profitable than open zero-rating or exclusive zero-rating with its affiliated CP.====Third, the vertical integration increases the total surplus and the unaffiliated CP’s profit. The integrated ISP internalizes the affiliated CP’s revenue. Then, the ISP need not implement excessive zero-rating to collect zero-rating fees, or to choose net neutrality with a small data cap to limit its operating costs. Without integration, the ISP chooses too low or too high traffic plans. On the other hand, the integrated ISP implements plans that engender mid-level consumption, which is welfare-enhancing. This profits the unaffiliated CP because it can take advantage of the increased data cap under net neutrality, which the integrated ISP increases to gain more revenue from its affiliated CP.====The remainder of the paper is the following. Section 1.1 gives a survey of the related literature. Section 2 explains details of our model. Section 3 analyzes the independent ISP case. Section 4 presents an investigation of the vertically integrated ISP case. Section 5 describes effects of the vertical integration on social welfare. Section 6 presents concluding remarks.",Effects of vertical integration on internet service providers’ zero-rating choice,https://www.sciencedirect.com/science/article/pii/S0167624522000440,4 October 2022,2022,Research Article,9.0
Umezawa Masashi,"Department of Business Economics, Tokyo University of Science, 1-11-2 Fujimi, Chiyoda-ku, Tokyo 102-0071, Japan","Received 22 January 2021, Revised 16 August 2022, Accepted 30 September 2022, Available online 3 October 2022, Version of Record 18 October 2022.",https://doi.org/10.1016/j.infoecopol.2022.101004,Cited by (1),"This paper analyzes behavior-based price discrimination (BBPD) in an asymmetric ==== with switching costs and including both vertical and horizontal differentiation. We demonstrate that there are two configurations of market share in equilibrium. In the first configuration, where both firms poach their rival’s consumers, the equilibria arise when switching costs are low and the firms are relatively symmetric. In the second configuration, where only the firm with more supporting services poaches the rival’s consumers, the resulting equilibria reverse. We reveal the impact of switching costs on firm profits under BBPD as well as under uniform pricing and show that with either high switching costs or sufficiently large firm asymmetries, BBPD may benefit both firms. Moreover, we find that in the second market configuration, social welfare can be higher with BBPD than with uniform pricing under high switching costs and large firm asymmetries. We also reach the same conclusion regarding social welfare when consumers are myopic in the first market configuration.","The continuing advance of information technology allows firms to easily distinguish between their customers with information about their past behavior. Using consumer purchase histories, firms can then offer different prices to existing loyal and new customers. This form of price discrimination is known as behavior-based price discrimination (BBPD) and is widely employed in many industries, including air travel, banking, hotels, Internet providers, supermarkets, telecommunications, and web retailers. Consequently, BBPD has received considerable attention in the literature. However, as discussed later, existing studies have mostly focused on horizontally differentiated markets where the consumer value of a brand is determined only by the distance from the consumer to the brand following the traditional Hotelling model. In the real world, however, when consumers purchase a product, they not only have preferences for its horizontal features but may also gain satisfaction from a variety of brand-specific complementary products or services (so-called supporting services), representing its vertical features. For example, in the US market for satellite TV service providers, DirecTV provides a range of channels that offer various programs such as news, movies, dramas, and sports, while the DISH Network advertises on its website that it has more channels than DirecTV.====The availability of programs or music tracks also applies to prime video services such as Netflix and Hulu, and music streaming services such as Apple Music, YouTube (previously Google Play) Music, and Spotify. That is, consumers care about the number of streaming videos or music tracks (i.e., supporting services) when they choose their service provider. For instance, even loyal Apple users may choose a music streaming service other than Apple Music if more music tracks are offered by the other streaming service. In these markets, suppliers do not necessarily offer the same number of supporting services. Consumers can then obtain great value from a variety of supporting services. Thus, firms are vertically as well as horizontally differentiated.====Moreover, in markets like these with repeated purchases, consumers face switching costs when they change from one supplier to another. This may be from learning or transaction costs. In a market where consumers incur switching costs, firms may be particularly interested in price discrimination as a means of customer acquisition, with firms often setting lower prices to poach rivals’ customers. If consumers find that the poaching price is reasonable even after considering switching costs, they will purchase the competing brand. Nonetheless, when firms engage in BBPD, the effect on their profits can differ according to the level of switching costs. Thus, when examining BBPD as a firm strategy, it is necessary to explicitly incorporate switching costs into the model.====The aim of this paper is to extend the analysis of BBPD to a situation with switching costs in an asymmetric duopoly. Of particular note, we incorporate both horizontal and vertical differentiation in our model. The paper then examines the conditions under which equilibrium exists, the impacts that firm asymmetries and switching costs have on firm behavior under BBPD, and the effects BBPD has on firm profits and social welfare (SW) given switching costs. A two-period duopoly model in a market with repeated purchases is considered. We use this to explore the existence of the subgame perfect equilibrium. Each firm locates itself at the end point on the unit line and provides a product with supporting services, where the number of provided services is not usually equal among firms. Based on the purchase history in the first period, firms can then set different prices for their own and their rival’s customers in the second period.====Consumers are assumed to be farsighted and uniformly located on the line. When they buy a firm’s product, its utility depends on the number of provided supporting services and the distance from the brand to the consumer (brand preference). Moreover, the model incorporates the fact that consumers have different sensitivities to the number of supporting services, depending on their location. For example, when the number of music tracks on Apple Music increases, loyal Apple users that typically patronize Apple products may be better off than loyal Google users. The opposite is true for loyal Google users. The marginal utilities of the two different types of users will then differ. That is, when a brand increases the number of supporting services by one unit, consumers closer to the brand gain more satisfaction. In addition, we introduce switching costs into the model, such that when consumers change from a brand that they purchased in the first period to another brand in the second period, they incur exogenous switching costs.====The analysis first reveals that there are two configurations of market share in equilibrium. In the first configuration (case (II) in the main text), the market shares of each firm in the first period are even or near equal, and both firms poach customers from their rival in the second period. The prices and profits under BBPD are compared with those under uniform pricing (UP) to examine the impact of BBPD. In the equilibrium, both firms tend to set high prices in the first period and low prices in the second. Moreover, we show that this equilibrium arises only when switching costs are low and the firms are relatively symmetric. Conversely, under high switching costs and large firm asymmetries, the firm with fewer supporting services (hereinafter referred to as the small firm) finds it difficult to poach its rival’s customers in the second period, which causes the firm to deviate in the first period to an alternative market configuration such that the small firm can make more profit without poaching.====In the second type of configuration (case (I) in the main text), the market share of the firm with more supporting services (hereinafter referred to as the large firm) is much smaller in the first period, and only the large firm can poach customers from its rival in the second period. Unlike the first configuration, in the equilibrium the small firm then sets a lower price under BBPD than under UP in the first period, and higher prices in the second period. The analysis shows that this equilibrium arises only when switching costs are high and firm asymmetries are relatively large. This is because, under low switching costs and low firm asymmetries, the small firm prefers the first configuration with even market shares to the second one with uneven market shares and can profitably deviate to the first market configuration. Technically, there is then a third configuration of market share in which only the small firm poaches customers from its rival firm in the second period. However, there is no equilibrium with this market configuration because the large firm does not prefer this configuration given the low profit margins. Indeed, the large firm can exploit its strong position and can always be better off deviating to the second market configuration.====We then investigate the effects of BBPD on welfare by comparing it with UP, and this offers some useful policy implications. At the same time, the analysis reveals the impact of switching costs on firm strategies under UP as well as BBPD. Under UP, we show that there are many equilibria in the pure strategies, of which we focus on two types. One is the equilibria where each firm chooses the highest first-period price among the equilibria, called UH. Another is the equilibria where each firm chooses the lowest first-period price among the equilibria, called UL. That is, the market can be either more or less competitive in the first period than a market without switching costs. Each firm’s profit is then larger under UH than UL, with the UH and UL equilibria serving as benchmarks. The results of the welfare analysis can be summarized as follows.====First, BBPD tends to decrease each firm’s profit relative to UP under most parameter spaces but depends on the degree of the firm asymmetries. We find in the first market configuration that when the two firms are quite asymmetric, there is an exception in which the small firm’s profit is higher under BBPD if the switching costs are exceptionally low. These large firm asymmetries and sufficiently low switching costs result in relaxed competition in the first period and the impact of this on the first-period profit benefits the small firm. In the second market configuration, we also find that when firm asymmetry is sufficiently large, both firms may be more profitable under BBPD than under UP.====Second, we find that when switching costs are high, BBPD can benefit the large firm as well as the small firm in the first market configuration. This arises in the comparison of BBPD and UL. High switching costs usually make it difficult for firms to poach their rival’s customers, so firms may suffer from setting a lower price in poaching. Nevertheless, firms can then be better off using BBPD. This is attributed to the negative effect of switching costs on firm profits under UL. That is, under UL, firms compete more aggressively for market share in the first period, as switching costs cause consumers to continue buying from the same firm in the second period. By contrast, under BBPD, firms can expect a second-period profit from poaching. Thus, they do not attempt to aggressively acquire consumers in the first period by setting low prices, which can provide the firms with more first-period profit, even in the presence of switching costs. Moreover, under UP, firms offer one price each period. Conversely, under BBPD, firms offer different prices to rival firms’ customers and their own customers in the second period. Such customized pricing may work well for firms when switching costs are high. This is because it allows firms to set higher prices for their own customers in the second period, as switching costs are higher, although this is not possible with UP. Moreover, the literature on BBPD has found that BBPD harms firms in symmetric market models without switching costs. This paper, however, also demonstrates by introducing switching costs that even if the two firms are symmetric, both have the opportunity of earning more profit under BBPD than under UP when switching costs are high.====Third, we confirm that SW is lower under BBPD than under UP in most cases for both market configurations, while consumer surplus (CS) tends to be higher under BBPD. Moreover, we reveal that in the second market configuration, SW can be higher with BBPD than UP under high switching costs and large firm asymmetries. CS is then also higher under BBPD. This arises because only the large firm poaches consumers and the high switching costs prevent excessive competition in the second period. That is, firm profits do not decrease significantly, while consumers can benefit from the moderate increase in competition due to BBPD.====Finally, we consider the case where consumers are myopic and investigate the effect of consumers’ myopia on poaching competition. Myopic consumers are more sensitive to first-period prices, which may lead to intensified first-period competition under BBPD. Thus, we confirm that in the first market configuration, the likelihood that firms earn more under BBPD than UP decreases relative to when given farsighted consumers. Oppositely, in the second market configuration, consumers’ myopia increases the likelihood because the myopia of consumers yields lessened first-period competition. Moreover, we find that SW can be higher with BBPD than UP for the first market configuration in addition to the second under high switching costs and large firm asymmetries, and unlike the result given farsighted consumers. This provides a useful policy implication for regulatory authorities.====Early studies on BBPD include Chen (1997), Villas-Boas (1999), Villas-Boas (2004), Fudenberg and Tirole (2000), Shaffer and Zhang (2000), and Taylor (2003). Subsequently, many other studies have analyzed BBPD in a variety of contexts.==== As the literature on competitive price discrimination reveals, the ability to price discriminate often leads to intensified competition (see, e.g., Thisse and Vives, 1988 and Corts, 1998). Much of the literature has also found that firms typically suffer lower profits from intense competition under BBPD. In particular, this paper contributes to the literature on BBPD with asymmetric firms. Chen (2008) analyzes a dynamic model with an arbitrary finite time horizon such that two vertically asymmetric firms compete and the weaker firm may choose to exit the market. Chen (2008) shows that the effects of dynamic price discrimination on competition and welfare dramatically differ between symmetric and asymmetric markets. The present paper also delivers results unlike those in symmetric markets. In this regard, Carroni (2016) is likely the closest to the present analysis. Carroni (2016) analyzes a BBPD model with a strong firm and a weak firm. In that model, consumers are willing to pay a price premium for a product offered by the strong firm, which is determined independently of the brand preferences for each firm. By contrast, in this paper, the two firms are asymmetric in terms of the number of supporting services. Consumers with different brand preferences will then have different sensitivities to the number of each good’s supporting services. Therefore, the present analysis assumes that the utility of buying a product with some supporting services is not independent of the brand preferences for the firm. In addition, Carroni (2016) considers consumers with different levels of farsightedness and provides extensive welfare implications. Carroni (2016) shows that if consumers are sufficiently myopic under sufficient asymmetries of firms, price discrimination softens competition compared with UP, and demonstrates when BBPD benefits each firm. Instead of considering the level of the consumers’ farsightedness, we introduce switching costs into an asymmetric market and focus on their effect on the equilibria and welfare. In addition, the present paper reveals the impact of consumers’ myopia on welfare in markets with switching costs.====The following studies of BBPD with asymmetric firms also relate to our analysis, except that our model introduces switching costs.==== Jing (2017) and Rhee and Thomadsen (2017) investigate BBPD in vertically differentiated markets consisting of a low-quality firm and a high-quality firm. They focus on the role of quality-adjusted cost differences between the firms. Rhee and Thomadsen (2017) particularly consider all configurations of market share and find a pure strategy equilibrium for that configuration in which both firms poach customers from the rival firm in the second period. This configuration is identical to the first configuration we provide in our analysis. Both of these studies thus show that BBPD may benefit the low-quality firm as well as the high-quality firm, according to the quality-adjusted cost.====The present paper also relates to Shaffer and Zhang (2002). They explore a BBPD model with asymmetric firms, where competing firms differ in size and consumers display heterogeneous brand loyalty. Therefore, like our model, they consider a model of both horizontal and vertical differentiation. Their model, however, examines personalized pricing depending on the level of brand loyalty, which is a different form of BBPD, and finds that the firm with a higher-quality product can gain from personalized pricing. By contrast, our model demonstrates that both firms may benefit from BBPD without any personalized promotion.====The remainder of the paper is organized as follows. Section 2 details the BBPD model. Section 3 examines the prices and firm profits in equilibrium and discusses the results. Section 4 presents the welfare analysis. Section 5 investigates the effect of consumers’ myopia. Finally, Section 6 provides a conclusion.",Behavior-based price discrimination in a horizontally and vertically differentiated duopoly with switching costs,https://www.sciencedirect.com/science/article/pii/S0167624522000439,3 October 2022,2022,Research Article,10.0
"Congiu Raffaele,Sabatino Lorien,Sapi Geza","Politecnico di Torino, Department of Management, Corso Duca degli Abruzzi, 24, 10129 Turin, Italy,Düsseldorf Institute for Competition Economics (DICE), Heinrich Heine University of Düsseldorf","Received 3 February 2022, Revised 20 July 2022, Accepted 26 September 2022, Available online 29 September 2022, Version of Record 10 October 2022.",https://doi.org/10.1016/j.infoecopol.2022.101003,Cited by (1),"We use traffic data from around 5,000 web domains in Europe and United States to investigate the effect of the European Unions General Data Protection Regulation (GDPR) on website visits and user engagement. We document an overall traffic reduction of approximately 15% in the long-run and find a measurable reduction in engagement with websites. Traffic from both paid and unpaid channels dropped significantly. We observe an inverted U-shaped relationship between website size and change in visits due to privacy regulation: the smallest and largest websites lost visitors, while medium-sized ones were less affected. Enforcement matters as well: The effects were amplified considerably in the long-run, following the first significant fine issued eight months after the entry into force of the GDPR. Exploring potential mechanisms, both a reduction in advertising effectiveness and a higher user awareness of privacy issues can explain our results.","The rise of the internet has enabled new products and services, revolutionizing the ways we work and interact. In the United States, as in other mature economies, the growth of the internet economy exceeds that of other sectors by orders of magnitude Interactive Advertising Bureau (2021). The shift of private and commercial activities to the internet goes hand in hand with increased attention to online security, privacy, and the protection of data.==== In recent years, policymakers around the world reacted to increased data protection concerns by putting in place a wave of regulations to protect online and offline privacy, thereby limiting the ways companies can collect and use data.==== Typically, the goal of such privacy regulations is to empower consumers by giving them additional control over how firms gather and use their personal information.====One of the most far-reaching regulatory interventions to boost privacy is without doubt the General Data Protection Regulation (GDPR) of the European Union. The GDPR entered into force in May 2018 intending to protect personal privacy across all domains, both online and offline. The regulation mandates data protection ====, and prescribes several principles and tools for managing data. It requires data controllers to inform individuals about the collection and use of data. Firms collecting personal data must obtain the users’ informed opt-in consent to such practices. The regulation assigns liability to firms handling data and – for the first time – imposes significant fines on privacy breaches up to 4% of global turnover.====While the GDPR applies both online and offline to any entity handling personal information, no other economic domain is expected to be as heavily affected as the internet ecosystem. Today’s online businesses rely heavily on the collection and use of visitors’ data. Websites attract visits by offering a broad spectrum of content ranging from news, music, entertainment, and commerce, among others. More often than not, they monetize by tracking their visitors and providing targeted advertisements to individuals or homogeneous groups of users. Digital stars such as Google and Facebook grew to titans of the internet by harvesting user data from millions of websites, connecting users’ browsing paths, aggregating these signals into user profiles, and offering advertising services to third parties to target consumers based on their specific interests and characteristics.====Since its introduction, the GDPR has attracted considerable interest from researchers, policy-makers, and industry players across the globe.==== So far, less attention has been devoted to understanding the consequences of the GDPR on the websites’ ability to attract internet users, as well as on how those users engage with websites’ content. Our research aims to fill in this gap. We investigate the effect of the GDPR on online traffic and user engagement in the EU.====We use data on traffic for around 5,000 websites in Europe and the United States, broken down into detailed traffic acquisition channels. We exploit the fact that the GDPR applies to users in the EU, leaving the non-EU audience unaffected. We apply a difference-in-differences (DiD) analysis that exploits the geographic origin of web traffic. In particular, our treatment assignment identifies traffic originated from EU countries, leaving US traffic in the control group. This implies that multinational websites having visitors from both the EU and abroad are treated only for the portion of traffic coming from EU countries. We test the validity of our DiD design through a standard parallel trends test, which shows no variation between treatment and control before GDPR. To account for potential contaminations between treatment and control, we also refine our control group in two different ways. First, we exclude multinational websites to exclude the possibility of potential spillovers for those websites receiving traffic both from EU and US. Second, we exploit websites’ country of origin to account for potential heterogeneities in compliance between EU and US-based domains. Results are robust to different model specifications, thus increasing our confidence in the DiD baseline model.====We find that the GDPR leads to an average reduction in web traffic of approximately 10%. This effect unfolds fully with a delay, several months after the hard date of the GDPR’s entry into force, and following the issuance of the first large fine. In fact, we observe an initial 4% reduction which rises to 15% in the long-run. We can break down the overall traffic reduction into various traffic acquisition channels. Paid channels, that is those generating a remuneration to third parties, or deriving from an internal marketing campaign, are the ones mostly affected by the GDPR. For instance, traffic from email and advertising banners collapsed by 29% and 38% in the long-run, respectively. However, also unpaid channels experienced a reduction in visits, thus suggesting a change in users’ online behavior on top of the reduction in online advertisement efficacy Goldfarb and Tucker (2011) from privacy regulations. On the intensive margin, we observe a significant deterioration of user engagement with websites following the GDPR, both in the short and in the long term. We estimate a significant reduction in average visit duration and the number of web pages visited, as well as a measurable increase in website bounce rate – i.e., the share of website visitors that leave almost immediately after arriving on a website. Finally, although the GDPR was much anticipated to damage predominantly small businesses Kottasová (2018); BBC (2018), we instead find an inverted U-shaped relationship between website size (measured in visits) and the traffic reduction induced by the privacy regulation. Small websites lost traffic, but so did the largest ones, while medium-sized websites remained unaffected and may have even grown.====This paper makes several contributions to the fast-growing literature on the impact of GDPR in the digital market. First, we investigate how the GDPR affected web traffic and user engagement, thus measuring its effects both in the extensive and intensive margin of internet consumption. Second, we are able to explore both short- and long-run effects of the GDPR. This is important, as most of the effects unfold several months after the enactment of the regulation. Third, contrary to other related studies Peukert et al. (2020); Goldberg et al. (2019), our empirical strategy is based on the geographical source of website traffic, rather than on the country domain.==== In such a framework, the treatment assignment targets precisely the EU audience that falls under the scope of the GDPR. Finally, we assess empirically the differentiated effects of privacy regulation based on website size Campbell et al. (2015); Dimakopoulos and Sudaric (2018); Sabatino and Sapi (2022). To the best of our knowledge, we are the first in providing evidence of an inverted U-shaped relationship between website size and privacy regulation.====We document and measure some anticipated and less expected effects of the GDPR. While our results indicate an overall loss of traffic via most channels and less interaction with websites, this in itself does not imply that the GDPR is a harmful regulation. On the contrary, the decline of direct website traffic – the largest source of website visits – may be the result of users’ conscious choice, following the increased awareness of privacy issues raised around the introduction of the GDPR.====The remainder of the paper is organized as follows. Section 2 presents an overview of the related literature and our contribution. Section 3 describes the main provisions of the GDPR. Section 4 introduces the data used in our empirical analysis. Section 5 discusses our empirical strategy. Section 6 reports our main findings. Section 7 provides robustness checks. Section 8 focuses on the potential mechanisms behind our results. Section 9 concludes.",The Impact of Privacy Regulation on Web Traffic: Evidence From the GDPR.,https://www.sciencedirect.com/science/article/pii/S0167624522000427,29 September 2022,2022,Research Article,11.0
"Szabó Andrea,Pham Vinh","Department of Economics, University of Houston, 3581 Cullen Boulevard, Houston, TX 77204, USA,FPT University Hanoi, Hanoi, Vietnam","Received 9 September 2021, Revised 6 May 2022, Accepted 4 August 2022, Available online 13 August 2022, Version of Record 8 September 2022.",https://doi.org/10.1016/j.infoecopol.2022.100993,Cited by (0),"Proponents of Net Neutrality rules argue that these regulations prevent internet service providers (ISP) from slowing down content that competes with some of their own services (vertical foreclosure). To study these incentives, we measure consumers’ willingness to pay for speed on the video on-demand market. We use a survey experiment to estimate a differentiated-product demand system for choosing how to view specific content. We establish a necessary condition for ISPs to have an incentive for vertical foreclosure: consumers respond to reduced speeds by substituting to a service offered by the ISP. We also show that by eliminating vertical foreclosure, Net Neutrality could provide incentives for ISPs to compete on prices.","In February 2015, the Federal Communications Commission (FCC) adopted the Open Internet Order, commonly known as “Net Neutrality.” These rules prohibited internet service providers (ISP) from discriminating among customers based on how they use the internet, specifically preventing ISPs from intentionally blocking or slowing down specific content. In a sharp policy reversal that received much attention, the FCC voted to repeal this regulation in June 2018. In September 2018, California passed its own Net Neutrality law, and was immediately sued by the Department of Justice. As of early 2019, more than 30 states had introduced bills to create their own Net Neutrality protections.====One of the arguments made by proponents of Net Neutrality is that, without the rules, ISPs have an incentive to block or slow down content that competes with some of their own services (vertical foreclosure). Opponents of Net Neutrality claim that these incentives, to the extent that they exist, would simply result in ISPs charging content providers for speed, and this would lead to efficient price discrimination. To support the policy repeal, the FCC argued that removing the Net Neutrality rules will reduce internet congestion and help future technology investment, which in turn will help consumers.====A first step towards measuring ISPs incentive for vertical foreclosure (or for charging content providers for speed) is to measure the magnitude of consumers’ response to changes in speed. Consumers having a nontrivial willingness to pay (WTP) for speed is a necessary condition for ISPs to have an incentive to foreclose, and may be suggestive of the magnitude of fees ISPs could charge content providers. This paper provides an estimate of such willingness to pay in the context of the transactional video on-demand (TVOD) market,==== and uses these estimates to simulate consumer response to some of the changes in speed and prices that may happen as a result of Net Neutrality rules.====TVOD is an important market, with an estimated revenue of $1.15 billion in 2017.==== Between 2017–2022, the revenue on this market has doubled, and this trend is expected to continue due to the continuing effects of the Covid-19 pandemic on consumer habits and industry practices (such as the release of feature films on TVOD simultaneously with, or instead of, movie theaters).==== TVOD is vertically integrated with the ISP market, and it is a major revenue source for several ISPs such as Comcast.====On this market, ISPs offering cable TVOD have faced increasing competition from online TVOD services. To offset this increased competition, an ISP who also provides cable TVOD could reduce the download speed of online providers that offer similar movies or TV shows. However, the ISP only has an incentive to do this if consumers are sensitive enough to download speeds that they would substitute to cable TVOD instead. In this paper, we measure this substitution by estimating the demand for different platforms, such as online TVOD or the ISP’s cable TVOD platform, for viewing ====, such as a TV show or a movie. Holding the content fixed allows us to isolate consumers’ trade-off between price and speed.====Estimating the differentiated-products demand system for various platforms has nontrivial data requirements. To collect a dataset with the necessary information, we use a conjoint survey experiment (Ben-Akiva et al., 2019). In the survey, consumers are presented with different options for viewing specific media content (in one version of the experiment, a TV show, in another version, a movie). Consumers can view the content on cable TVOD, online TVOD, or buy a physical DVD (or decide not to view the content). Each option is described by different combinations of price and “wait time” (in the case of downloaded content, this is the buffer time before the content begins to play;==== in the case of a DVD, it is the time it takes to buy the physical product). Based on the observed choices, we estimate consumer preferences using a random utility discrete choice model.====Our estimates imply that the median consumer’s WTP for 1 min less buffer time is 3.6 cents for the TV show and 3.1 cents for the movie. With a typical internet connection, we estimate that the average consumer would be willing to pay 10 percent more for completely eliminating online TVOD buffer time for the 140 min long high-definition movie used in our experiment. In this setting, consumers appear to attach high value to download time when choosing how to view specific content. We also find that demand for the various viewing platforms is price elastic, particularly for online TVOD and cable.====We use the estimated model to simulate some of the ways Net Neutrality rules could impact the demand for viewing platforms. Relative to a baseline with positive buffer time for online TVOD, we simulate the impact of eliminating the buffer. This might correspond to a situation where the ISP is prohibited from creating extra buffer time for online TVOD compared to cable on-demand. In this counterfactual experiment, we find that eliminating the buffer time for online TVOD increases this platform’s market share by 1.1 (4.5) percentage points for the TV show (movie). Cable loses the most from this change, with a decline in its market share of 0.4 (2.2) percentage points for the TV show (movie). This finding highlights the incentive that an ISP who also offers cable TVOD may have for limiting the speed of competing online TVOD providers in the absence of Net Neutrality.====We also study a scenario where the ISP/cable provider adjusts its price to match the (lower) online TVOD price in order to limit the adverse impact of the regulation on its market share. This experiment quantifies one way in which Net Neutrality could lead to ==== competition by giving cable providers an incentive to lower prices on their TVOD content. When the elimination of buffer time is followed by cable lowering its price, this wipes out the gain of online TVOD from the reduction in buffer time in the case of the TV show. For the movie, the gains from the buffer time reduction for online TVOD are large enough that its market share increases even after cable’s price reduction. Here both online TVOD and cable gain a market share of around 2.7 percentage points, while the DVD market share declines. These findings suggest that if Net Neutrality eliminates competition in download speed, the nature of competition in the remaining attribute, price, is likely to be a crucial determinant of the impact of the regulation on the market shares of different content providers.====The literature on Net Neutrality, which is primarily theoretical, is discussed in the next section. In terms of methodology, the paper closest to ours is Leung (2013), who studies the impact of government response to software piracy. While that paper addresses a different topic, it also features estimates of consumer response to increased download speeds for a specific product (a pirated copy of Microsoft Office). Other than this study, we are not aware of WTP estimates for download speeds of fixed content in the existing literature.====Our paper is also related to a growing literature estimating consumer demand for internet service (Nevo, Turner, Williams, 2016, Grzybowski, Hasbi, Liang, 2018, Liu, Prince, Wallsten, 2018, Malone, Nevo, Williams, 2019, Tudon, 2021). This literature uses different sources of variation to estimate consumers’ WTP for internet speed in order to study ISP incentives related to issues such as congestion and bundling. While these papers focus on different ISP incentives, WTP for speed is a key parameter to those incentives as well. An important challenge is that consumer choices between internet plans are affected by what a consumer uses the internet for, i.e., the content that (s)he wants to access. Our experimental approach allows us to estimate WTP for internet speed holding the content constant, thus isolating consumer responses from other potential incentives that would affect the interpretation of the results.====Three key limitations of our study should be kept in mind. First, online TVOD is only part of the large market for online entertainment. Another important part is subscription-based video on-demand (SVOD), which involves several considerations that our experiment was not designed to study. Second, we focus exclusively on the demand side, and cannot directly address questions that would also require modeling provider behavior (including the full welfare effects of Net Neutrality rules). Third, although our sample is representative of the US population in several relevant dimensions, it excludes older customers (see Section 6.4 for details).====In the remainder of the paper, Section 2 gives some background, Section 3 presents our experiment and the data, Section 4 describes the demand model and the estimation method, Section 5 contains the estimation results, section 6 presents the counterfactual experiments, and Section 7 concludes.",Net neutrality and consumer demand in the video on-demand market,https://www.sciencedirect.com/science/article/pii/S0167624522000324,13 August 2022,2022,Research Article,12.0
Wolfstetter Elmar G.,"Humboldt-University at Berlin, Germany","Received 8 December 2021, Revised 1 July 2022, Accepted 6 August 2022, Available online 8 August 2022, Version of Record 13 August 2022.",https://doi.org/10.1016/j.infoecopol.2022.100994,Cited by (0),"Millions of citizens and firms lack access to high speed internet, even though governments pledged to spend huge sums of money to subsidize internet networks. In this paper we review some systematic flaws of present policies and outline a promising alternative. We propose that governments should treat the broadband infrastructure as a public responsibility and set up public-private partnerships that deploy, fund, and temporarily operate the broadband in exchange for collecting service fees and, if necessary, subsidies. Least-present-value-of-revenue auctions can be used to award all concessions, not only those that are expected to require subsidies, and concessions should revert to public ownership and be re-auctioned if the promised present value of revenue has been reached through collection of service fees. This procurement method is easy to implement, efficient, and immune to strategic manipulations and renegotiations.","A comprehensive network of high-speed broadband internet connections is an essential part of a nation’s infrastructure, comparable to the network of roads, railways, and airports, and letting private firms use public space to construct a network of broadband internet cables and operate internet services is similar to letting private firms construct and operate toll-roads and bridges. Inspired by this similarity, we propose a surprisingly simple yet efficient approach to deploy, fund, and operate a universally accessible broadband infrastructure that is closely aligned with innovative methods to procure other infrastructure such as toll-roads and bridges.====In this approach the government treats the broadband infrastructure as a public responsibility. It sets up public-private partnerships (PPPs) that fund and temporarily operate the broadband in exchange for being allowed to charge service fees and, if necessary, collect subsidies. A simple auction mechanism is used to allocate concessions, to determine possible subsidies, and to determine whether and if so when the concession shall revert to public ownership for re-auction. Unlike in common procurement methods the proposed auction mechanism can be employed to ==== projects, not only those in “white areas” that need to be subsidized.====The proposed auction method is surprisingly simple, efficient, and immune to strategic manipulations. Unlike the common approach it does not expose the concessionary to risk, thus eliminates incentives for low balling in the auction and renegotiations after the auction, eliminates the need for enforcing regulated service fees, and even determines the compensation a concessionary can claim if the public authority is determined to replace the existing technology.====Ubiquitous access to a high-speed broadband infrastructure contributes significantly to economic growth and balanced development with strong linkages across urban and rural areas. It thus exerts significant positive externalities (see, for example, Bresnahan, Trajtenberg, 1995, Czernich, Falck, Kretschmer, Woessmann, 2011).==== In order to foster these benefits, many countries around the globe have introduced ambitious broadband targets that promise access to high speed internet connections for all households and firms.====For example, the European Commission pledged to give all Europeans internet access at speeds above 100 Mbps as early as 2025, regardless of their location (see European Commission, 2016). This target cannot be realized without substantial subsidies. Therefore, ==== member countries have allocated huge amounts of funding to assure that rural areas, where rolling out high speed broadband connections is not feasible without subsidies, do not fall behind. Similarly, in the U.S. the ==== initiated several universal service programs to subsidize the broadband infrastructure in rural areas by means of universal service auctions that are geared to minimize subsidies. Countries like Germany have not made use of comprehensive well-structured auctions and the promised high-speed broadband provision progressed at a slow pace.====Plan of the paper: ==== states the basic assumptions of our analysis and ====  summarizes the common dual approach. ====  presents the proposed alternative approach, its practical implementation, and explains why it is superior to the common approach. ====  characterizes the ideal welfare optimal procurement contract and shows that it can be implemented by the proposed simple auction under plausible conditions. ====  discusses some specific design issues that are geared to generate competition in the auction. The paper closes in ====  with a discussion.",Universal high-speed broadband provision: A simple auction approach,https://www.sciencedirect.com/science/article/pii/S0167624522000336,8 August 2022,2022,Research Article,13.0
"Dejean Sylvain,Lumeau Marianne,Peltier Stéphanie","La Rochelle University, CEREGE, France,University of Angers, GRANEM and Labex ICCA, France,La Rochelle University, CRHIA and Sorbonne Nouvelle University, IRCAV,, France","Received 22 July 2021, Revised 19 June 2022, Accepted 27 July 2022, Available online 28 July 2022, Version of Record 6 August 2022.",https://doi.org/10.1016/j.infoecopol.2022.100992,Cited by (0),"The development of online social media has raised concerns about how individuals are over-exposed to partisan news. However, social media are only a part of the daily media diet of an average consumer (====; ====). The aim of this paper is therefore to examine partisan news exposure with respect to the entire media diet. We develop a partisan selective exposure index that indicates the over-representation of partisan political opinions in individual daily news consumption. Our analysis of data from a survey of 4,000 people in France shows that on average, partisan exposure is low when social media are excluded. Our estimations also indicate that social media consumption increases selective exposure to partisan content. While the youngest and the less educated appear to be less exposed to partisan content in the absence of social media, our results suggest that this is no longer the case when social media are included in the news diet. Finally, we show that the more people declare extreme (right- or left-wing) political views, the more they are over-exposed to like-minded content.","Recent years have seen growing concern about the interplay between online media and democracy. Critics accuse social media, in particular, of exacerbating the polarization of people's political attitudes. The mechanism underlying this effect is referred to as the ==== phenomenon (Pariser, 2012) or the ==== (Sunstein, 2001; 2007; 2017). While consuming news online, internet users may reduce their exposure to conflicting opinions in two ways. First, they may naturally seek out information that is in line with their opinions and beliefs, in a high-choice environment; and second, algorithms and online media have learned to manage preferences to consolidate their audience, and propose more partisan content. With respect to social media, a growing body of empirical studies reports conflicting results on the existence of such a phenomenon (Halberstam and Knight, 2016; Goel et al., 2010; Barberá, 2015).====In practice, social media only represents a part of overall news consumption, and its importance should not be overestimated (Allcott and Gentzkow, 2017; Guess et al., 2018). In a recent study, Allen et al. (2020) showed that it only represents 14% of Americans’ daily media diet. Television, radio or newspapers are other platforms where individuals have been consuming news for decades. According to the latest Standard Eurobarometer (European Commission, 2020), television remains the most-consumed news platform in Europe: 90% of Europeans watch television on a television set at least once a week, followed by radio (74%), social media (64%) and the written press (55%). On each of these platforms, consumers can choose from a wide range of media outlets with different political slants. This makes the information ecosystem of an individual, and selective exposure to different points of view, very complex. In this context, the aim of this paper is to study how the media diet exposes individuals to like-minded content.====As far as we know, the study by Gentzkow and Shapiro (2011) is the first attempt to investigate how cross-platform news consumption impacts the way people are selectively exposed to different opinions. Based on the literature on racial segregation, the authors define an isolation index and apply it to American news consumption. They report that individuals consume more like-minded news online than offline, even if the absolute level is low. Replications of the isolation index indicate an increase in the tendency to consume online partisan content in recent years (Levy, 2021; Peterson et al., 2021) along with a more pronounced increase for social media (Halberstam and Knight, 2016; Levy, 2021).====In line with these works, the first contribution of this paper is to propose a measure of ideological exposure at the individual level that considers the global news diet and the individual political position: we call this the partisan selective exposure index. This index indicates, for an individual, the share of daily news consumption that reflects her or his ideological opinions. To measure the degree to which an outlet contains like-minded news, we compare the political position of the individual and the political position of the audience of the consumed outlet (Gentzkow and Shapiro, 2011). However, as each social media user is exposed to a personalised set of news, we cannot approximate the slant of a social media outlet by the ideology of its audience. To overcome this limitation, we propose three scenarios that range from a situation where the media is as partisan as the most partisan platform, to a situation where it is as partisan as the least partisan platform. To the best of our knowledge, our index is the first measure that can reflect the consumption of partisan news outlets in the overall media diet of an individual. It also offers the advantage of accounting for both aggregate measures at the media or platform levels, and individual heterogeneity (such as age, gender, level of education or political position). Even if partisan selective exposure is, on average, low at the population or platform levels, particular groups could be over-exposed to partisan content. Identifying these groups and their characteristics is only possible with an individual index and a detailed analysis of their different individual characteristics.====The second contribution of this paper is to apply our index to a sample of French news consumers. We draw upon an original survey conducted among 4000 French individuals who were asked about their news consumption in the previous week on all platforms (television, radio, newspapers, pure players, news aggregators and social media) and their political stance. Then, we estimate the global selective exposure of our sample and discuss the contribution of each platform. At the individual level, we analyze differences according to their socioeconomic characteristics and political position. In a context of increasing mistrust of online news sources (following the Cambridge Analytica scandal, the emergence of fake news, etc.) we also estimate the role of social media in global exposure to partisan content.====Our results show that, without social media, the average news consumer is only selectively exposed to 3.5% more like-minded information compared to someone who is randomly exposed to a representative bundle of news outlets. This result is consistent with the literature documenting a relatively moderate (online) media diet among Americans (Flaxman et al., 2016; Gentzkow and Shapiro, 2011; Guess, 2021). Pure players, newspapers and listening to the radio contribute most to the increase in the index. Our finding is robust to a validation exercise. Furthermore, we estimate a possible increase in selective exposure to partisan information when online social media are introduced. We also show heterogeneity according to individual characteristics such as age and education. Finally, we found that the more people declared being at the extreme of the political spectrum, the more they are over-exposed to like-minded content. Our estimations indicate that this effect could be reinforced by consumption on social media.====Our paper is organised as follows. In the next section, we review the literature on selective exposure, then describe our dataset, before presenting our methodology and the index of like-minded selective exposure. Finally, we present our results, discuss their implications and conclude.",Partisan selective exposure in news consumption,https://www.sciencedirect.com/science/article/pii/S0167624522000312,28 July 2022,2022,Research Article,14.0
"Cuntz Alexander,Bergquist Kyle","World Intellectual Property Organization (WIPO) 34, chemin des Colombettes, Geneva CH-1211, Switzerland","Received 28 January 2021, Revised 24 June 2022, Accepted 8 July 2022, Available online 22 July 2022, Version of Record 7 August 2022.",https://doi.org/10.1016/j.infoecopol.2022.100989,Cited by (0),"Platforms often compete over non-price strategies such as the exclusive distribution of products. But these strategies are not always welfare-enhancing. Using rich data on audiovisuals distributed on platforms in Brazil, we find that non-exclusive distribution and availability of titles across platforms is more effective in deterring online piracy than in the single-homing case. Moreover, for the subset of domestic movies, it induces higher average investment in the quality of new titles upstream. We discuss options of copyright and competition policies in the light of these findings.",None,Exclusive content and platform competition in Latin America,https://www.sciencedirect.com/science/article/pii/S0167624522000282,22 July 2022,2022,Research Article,15.0
"Bertschek Irene,Kesler Reinhold","ZEW Mannheim, Digital Economy, P.O. Box 103443, D–68034 Mannheim, and University of Giessen, Germany,University of Zurich, Department of Business Administration, Plattenstrasse 14, Zurich CH–8032, Switzerland, and ZEW Mannheim, Germany","Received 27 April 2021, Revised 30 June 2022, Accepted 18 July 2022, Available online 20 July 2022, Version of Record 30 July 2022.",https://doi.org/10.1016/j.infoecopol.2022.100991,Cited by (2),"Social media open up new possibilities for firms to exploit information from various external sources. Does this information help firms to become more innovative? Combining firm-level survey data with information from firms’ Facebook pages, we study the role that firms’ and users’ activities on Facebook play in the innovation process. We find that firms’ adoption of a Facebook page as well as feedback from users are positively and significantly related to product innovations. Our results withstand a large set of robustness checks, including estimations that take potential endogeneity of firms’ Facebook use as well as unobserved heterogeneity into account. Analyzing the content of firm posts and user comments reveals that Facebook adoption is only correlated with product innovations if firms and users actively participate in a discussion, especially when engagement is above-average and comes from both sides.","In today’s information-rich environment, a firm’s competitive advantage is increasingly determined by the leverage of external knowledge (Tambe et al., 2012). Social media, such as online social networks and microblogging services, open up new possibilities to gather this information. As the largest of these platforms, Facebook has close to three billion monthly active users as of the end of December 2021==== and is also of great importance with respect to the time spent online by the average user.==== Attracted by the opportunity to access a large user base, firms increasingly adopt a social media presence with Facebook being the favourite platform comprising more than 200 million business pages in 2022.==== While the main objective of social media is marketing, surveys among companies show that it also serves other purposes such as receiving customer feedback in order to improve products and services (Bertschek, Erdsiek, German Federal Statistical Office). Accordingly, it provides faster and cheaper access to knowledge, thereby facilitating product development and innovation due to users’ input. As a result, external information from social media can be utilised by firms across all innovation stages ranging from idea generation contests and user feedback through comments or polls to entire co-creation campaigns (Roberts and Piller, 2016). Examples include Gillette launching the very first product for assisted shaving based on feedback inferred from social media,==== Tesla improving the company’s app based on CEO Elon Musk reading a complaint from a customer on Twitter,==== and Airbnb CEO Brian Chesky asking on Twitter what the company should launch in 2017.==== Beyond this anecdotal evidence of sourcing information from social media users, there is, to the best of our knowledge, no large-scale empirical evidence on whether or not firms’ external focus through a social media presence on a platform like Facebook significantly enables corporate innovation.====In this paper, we examine the role that social media, specifically Facebook, plays for firms’ innovation activity measured by the realization of newly developed or significantly improved products or services (‘product innovation’). We use a unique and rich data set of 2932 German manufacturing and service firms collected in 2015 and supplemented by information from firms’ Facebook pages available from 2010 until the end of 2013. Combining survey data with web-crawled data allows us to not only take into account a huge set of firm characteristics relevant for innovation output but also to conduct a content analysis of both firm and user activity on Facebook. Moreover, in contrast to studies focussing on large listed companies, our data set includes a large share of small and medium-sized enterprises which fairly accurately reflects the structure of the German economy. As a platform to engage with users with low barriers to entry, social media might be particularly relevant for such companies.====We find that both the adoption of Facebook by a firm and user activity on a firm’s Facebook page is positively and significantly correlated with the firm’s probability to introduce a product innovation.====A large set of robustness checks supports these results. These checks comprise considering Facebook activity from earlier periods, employing different innovation and social media measures, controlling for further sourcing channels as well as digital capabilities, matching firms with and without Facebook, and taking the persistence of firms’ innovation behaviour into account. Additionally, we employ a panel analysis to control for time-constant unobserved heterogeneity and look at Facebook adopters over time. Moreover, an instrumental variable approach taking into account the potential endogeneity of firms’ Facebook use underpins the credibility of our results with the instruments also allowing to model a selection of Facebook adoption that leaves user activity to be a significant determinant of product innovation.====Analyzing the content of posts and comments on a firm’s Facebook page in multiple ways reveals that only through the engagement by the firm and by the users a firm’s Facebook page is positively and significantly related to product innovation output. The results are especially pronounced for firms with above-average engagement and when there is engagement from both sides. Engagement in this case means asking questions or using relevant keywords as well as conducting discussions with a positive or negative sentiment of user comments.====Thus, a firm’s Facebook presence and the information from users are relevant for introducing product innovations. However, simply adopting a Facebook page and posting generic content does not necessarily mean that firms are benefitting from the knowledge of the user base. Firms should rather use this social media channel strategically by actively encouraging users to leave valuable feedback that can be then translated into improved products and services or into developing new ones. By creating an interactive environment that allows engagement and discussions by the users, firms may benefit particularly from the content of users.",Let the user speak: Is feedback on Facebook a source of firms’ innovation?,https://www.sciencedirect.com/science/article/pii/S0167624522000300,20 July 2022,2022,Research Article,16.0
"Abrardi Laura,Cambini Carlo","Politecnico di Torino, Department of Management, Corso Duca degli Abruzzi 24, Torino 10129, Italy","Received 8 November 2021, Revised 29 April 2022, Accepted 26 June 2022, Available online 29 June 2022, Version of Record 2 July 2022.",https://doi.org/10.1016/j.infoecopol.2022.100988,Cited by (0),"In this paper, we study the optimal design of incentives to induce a digital platform to limit the extraction of data from users, whose privacy loss is further aggravated by their naive use of the platform. We show that caps on the amount of data collected can induce the optimal data-saving effort by the platform. If the platform’s effort is not observable, a menu of data caps should be provided and it entails a higher (lower) loss of privacy for less (more) naive users, relative to the first best. We also show that compensating users for their data can efficiently incentivize effort, but might increase the privacy loss of more naive users.","The massive and unprecedented scale of data collected by big digital platforms is creating serious concerns to policymakers for the large loss in terms of privacy as well as for its effects on competition, consumers, and society (Cremer, De Montjoye, Schweitzer, 2019, ACCC, 2019, C., 2020). The policymakers’ burden is aggravated by the fact that users often naively approach the privacy problem, and are nudged by questionable interface designs towards privacy intrusive options.==== How to induce platforms to willingly choose privacy by design and frontally attack the issue of people’s behavioral biases? In this paper we study the role of ex ante interventions towards a monopolistic digital platform that collects data from consumers’ internet usage, and the incentives to adopt measures that reduce the privacy loss of users.====So far, the problem of excessive data collection by digital platforms has mainly been addressed by the privacy legislation, through the definition of the limits for the processing of personal information.==== However, such measures require a substantial effort of enforcement to verify compliance, due to the large information asymmetries and rapid pace of innovation characterizing digital ecosystems (DeStreel et al., 2020). Regulating people’s data is challenging also because few consumers are fully informed of, or fully understand, the scale and scope of data collected. While consumers claim to care about how their data will be used, in practice they often show little concern about it in their daily behavior (the so-called ====, Acquisti et al., 2015). Because of such inconsistency, consumers often relinquish considerable control over their uploaded content to digital platforms, thereby strengthening the platforms’ potential of exploitation of the collected data (Solove, 2013).====In the face of the struggles of privacy law, a new idea is gaining consensus about the need for a more direct, regulatory-like approach to the problem of data and its use by internet giants (Stiglitz, 2019).==== In a report for the European Commission, Cremer et al. (2019) make the case for the creation of a new Authority for data regulation, to be enforced at a supranational level by an ad-hoc body (DeStreel et al., 2020). In January 2020, the UK government announced that companies sharing user-generated content will soon be subject to an independent regulator, and in June of the same year the European Commission launched a consultation to seek views on Digital Services Act package, arguing that Europe needs a modernized regulatory framework for digital services.==== Other countries are going along the same path: for example, the Australian Antitrust Authority recently argued that the regulatory framework governing digital platforms needs to be addressed (ACCC, 2019). If the problem of regulating digital services is at the forefront of public authorities, it is still unclear how the problem should be approached, missing a theoretical framework that could guide the policymakers’ action.====In our model, the usage of the service of a digital platform –for example, a search engine or a social network– generates personal data about users, that can be monetized by the platform. The data extracted causes a privacy loss to users, which they naively underestimate, thus overdisclosing data. The platform could reduce the users’ privacy loss by exerting a non-observable effort in data minimization. For example, the effort could entail developing a protocol that reduces the collection of data during the provision of the service. The socially optimal provision of data trades-off the cost of effort and the cost of the privacy loss. However, the platform does not have an incentive to exert effort in the data-saving technology. We study two potential policy interventions: a cap on the data extracted, that might be applied to restore the platform incentives to enhance its effort, and redistribution of the platform’s revenues through compensations paid to users for their data. The cap on data is the maximum amount of data that the platform can extract, for example the information on the user’s name, location, contacts, etc. The optimal policy entails a cap on the quantity of data that the platform can collect. For example, the social planner could mandate the anonymization of some data before it is stored at the service provider, or limit the access to data such as the contact list or the geolocation information via a smartphone.====Owing to the fact that the platform’s effort to protect users’ privacy might be difficult to observe by a regulator, the platform could gain a rent by blaming the users’ naivety for its large extraction of data, thus concealing its low effort. We show that the optimal second best intervention requires to offer to the platform a menu of contracts, characterized by a more stringent data cap (relative to the first best) when users have severe naivety, and a larger cap when users have mild naivety.====We also show that the possibility to compensate users for their data, jointly with data caps, improves welfare but worsens the privacy loss of more naive users relative to the case in which only data caps are imposed. In fact, the value extracted by the platform from data can be redistributed to users via the transfers. Given that users are reimbursed for the privacy loss when transfers are available, the effort to develop a data-saving architecture is lower and users suffer a larger privacy loss relative to the case where no compensations are feasible.====Our goal is to link the literature on data acquisition and its impact on privacy with the literature pertaining to the inconsistency of preferences underlined by behavioral sciences (Kahneman, Tversky, 1979, Ainslie, 1992, Laibson, 1997). Our contribution to the theoretical literature is thus twofold. First, we model the effort exerted by the platform to protect users’ privacy. In our model, the platform plays an active role in internalizing the network externalities via the prices, and needs incentives to reduce the privacy loss of users. Second, we introduce the element of the users’ naivety, which hampers the effectiveness of initiatives such as consent policies and enhances the potential of data exploitation. Our approach is close in spirit to the literature on regulation of monopolistic firms under information asymmetries (e.g., Baron and Myerson, 1982), with one crucial difference. In our setup, the regulator needs to take into account not only the firm’s lack of incentive to exert effort, but also the inefficiency generated by consumer’s behavioral responses. Our results highlight that regulating a monopolist with behavioral users introduces an additional dimension to the usual efficiency vs. rent extraction trade-off, in terms of ability to mitigate users’ (mis)behavior.====The next section of the paper presents our relation to the pertinent literature. Section 3 describes our model. Section 4 studies the optimal design of incentives when only caps on data can be imposed, while Section 5 analyzes optimal regulation when users can also be compensated for their data. In Section 6 we extend the model to the case in which the platform’s effort increases its revenues. Finally, Section 7 concludes. All proofs and technical details are relegated into an Appendix.",Carpe Data: Protecting online privacy with naive users,https://www.sciencedirect.com/science/article/pii/S0167624522000270,29 June 2022,2022,Research Article,17.0
"Amorim Guilherme,Lima Rafael Costa,Sampaio Breno","Department of Economics, University of Illinois at Urbana-Champaign, 214 David Kinley Hall, 1407 West Gregory Drive, Urbana, IL 61801, USA,Department of Economics, Universidade Federal de Pernambuco, Brazil","Received 25 March 2021, Revised 17 March 2022, Accepted 8 June 2022, Available online 23 June 2022, Version of Record 2 July 2022.",https://doi.org/10.1016/j.infoecopol.2022.100982,Cited by (0),"This paper investigates the influence of broadband internet availability in the occurrence of events of civil unrest. Using collected data on 2011’s Occupy Movement in the U.S., we find that each new Internet Service Provider (which is associated to an increase in broadband penetration) accounts for an increase between 1 and 3 p.p. in the probability of observing protests in a given location. Results are consistent when analyzing county-level data for the contiguous U.S., for each different U.S. region separately (Northeast, Midwest, South and West), and when analyzing city-level data for California.","In this paper we use data on protesting activities related with the Occupy Movement in the United States to investigate whether the availability of broadband Internet induces political mobilization in the form of public demonstrations. We find robust evidence that Broadband Internet is positively related to the emergence of protests.====Throughout history, events of civil unrest have played an important role in shaping the social and political structures of societies. Particularly, this past decade has been a stage for various acts of protest around the world, as well as for a perceptible rise in the number of such events each and every year.==== As a result, the role of protests as means for political participation is receiving growing attention from the media and the academic literature.====Social interactions have also changed dramatically in the last two decades as a result of the communication revolution. From dial-up internet to mobile access in smartphones and social networks, these innovations have transformed the way human beings produce, acquire, and share information with one another. It is estimated that Internet usage and mobile phone subscriptions have increased worldwide at a rate between 2.78% and 7.4% per year, respectively, in the course of this last decade.==== Evidence of the consequences of these transformations are discussed in Benyon and Dunkerley (2014), which points to the global exportation of western culture, and on Hussain and Howard (2012), which describes the new breaths towards democratization in countries from the Arab world. It is clear that Information and Communication Technologies (ICTs, hereinafter) are now part of how our society evolves.====This scenario has brought attention to the possible influence of the Internet and of social media in the course of recent popular movements. Many argue that the advent of the Internet was a key factor that allowed these movements to spontaneously emerge. However, there is still insufficient evidence on the relation between access to the Internet and the emergence of protests,==== particularly for developed nations where communication infrastructure is already well established. Additionally, political rights in such nations are usually constitutionally guaranteed, whereas in other countries the reasons for protesting can be manifold. All these features make the connection between ICTs and protests less obvious in our context.==== The main objective of this study, therefore, is to provide empirical evidence of how the development of ICTs have positively influenced the incidence of events of civil unrest, by studying the impact of broadband Internet availability on the spread of the Occupy Movement in the United States.====The Occupy Movement refers to the series of protests that spread throughout the U.S. territory in late 2011, eventually becoming one of the most relevant episodes of mass protesting in recent years. The movement was sparked by an embryonic event known as Occupy Wall Street (the first “Occupy” demonstration that happened on September 2011 in Zuccotti Park, New York City). It was also inspired by other recent events of social unrest, such as the Arab Spring and the Spanish “Indignados.” Its core purpose was to raise awareness about the control that financial systems and big corporations exert in modern society. It claimed that the disproportionate benefits granted to a minor share of the population would undermine the proper functioning of a democratic society. Evidence of demonstrations held as part of the movement can be traced to hundreds of U.S. cities. The movement even had international presence with demonstrations being held in places outside the U.S. Its main motto, “we are the 99%” – a reference to the income and wealth distribution inequality between the wealthiest 1% and the rest of the population – was widely disseminated by the worldwide media.====The choice of the Occupy movement as a case study is justified by a number of particular features that help isolate the raw relationship between broadband availability and protesting. First, the U.S. is the oldest and still one of the most solid democracies in the world today. The First Amendment of the United States Constitution specifically allows citizens to engage in peaceful demonstrations and guarantees their freedom of assembly, which means that U.S. citizens face relatively low barriers to organize and participate in such demonstrations to redress their grievances. In fact, most of the Occupy’s protests were peaceful sign-holding marches, rallies and pickets. Real acts of trespassing were more notably observed in the bigger cities, where protesters “occupied” public squares by organizing sit-ins and/or camping communities. Only in a few of these situations protesters encountered severe repression from the police. Such an environment signals low opportunity costs of engaging in protests, thereby lowering the psychological threshold that prevents disgruntled individuals from taking it to the streets.====Second, despite the striking proportions the movement achieved, outdoor demonstrations did not last long. The bulk of collected data of Occupy-related protests is mainly concentrated in the months of October and November 2011 with very few episodes registered outside this time frame. On one hand, this turns out to be convenient to our analysis since possible sources of bias such as changes in climate, in macroeconomic conditions and in other possible unobservable variables are unlikely to be of great impact to our estimates. On the other hand, by dealing with data of cross-sectional nature, our possibilities for investigating long-term trends between our variables and for designing robustness checks such as placebo regressions are somewhat restricted. We attempt to overcome this limitation with alternative data for political participation in the U.S. (i.e. presidential elections turnout).====A third feature concerns the observed motives of the Occupy protests. Despite the declared general demand to end financial corporations’ influence over politics, as disseminated by most of the mainstream media covering the protests, there was not a unifying claim that could characterize the movement as a whole. As pointed by Castells (2015): “The movement demanded everything and nothing at the same time. In fact, given the widespread character of the movement, each occupation had its local and regional specificity: everybody brought in her own grievances and defined her own targets.” The author later indicates that the list of most frequently mentioned demands debated in various of the Occupy-related events and campsites was of extraordinary diversity, ranging from positions against economic austerity, government corruption and other financial and political subjects to concerns about health care, student loans, global warming, sexism, and animal rights. This supports the idea that mass movements of social unrest, in their most contemporary forms, are a consequence of the Internet’s potential for disseminating information and facilitating collective action, thus bringing together unsatisfied individuals from a much more diverse pool of complaints.====To identify the sought causal relationship between broadband Internet and the Occupy protests, we use an instrumental variable approach in our main empirical specification, with topographic elevation as an exogenous determinant to the provision of broadband Internet. Identification using physical geographic instruments, such as weather and topography, is common in many empirical studies due to their generally random and predetermined nature, as in Miguel et al. (2004), Hussain and Howard (2010) and Yanagizawa-Drott (2014), for example. Most specifically, Jaber (2013) has introduced the use of terrain elevation as an instrument to broadband Internet in the U.S.==== The relationship comes from the costs of building and maintaining cable infrastructure – which, by 2011, was the most used technology for signal distribution in the U.S. We argue, based on evidence from his research, that low-lying areas are more prone to floods and exhibit higher summer temperatures, and that such climate conditions played a role in today’s deployment of broadband infrastructure. To provide support to the exclusion restriction hypothesis, we test some variations in our main specification using several sets of covariates.====We perform analogous exercises for each of the different U.S. regions separately (Northeast, Midwest, South and West) using county-level data, and for California using city-level data. We also explore alternative identification methods, such as the estimator developed by Lewbel (2012) that exploits heteroskedasticity for identification and does not rely on exclusion restrictions. In all cases, estimates obtained were very similar, thus providing us robust evidence that the effect of broadband Internet on the occurrence of protests is positive and statistically significant.====To shed some light on the mechanisms behind the studied relationship, we propose a model to describe an individual’s decision to participate in a protest. We follow the approach from Passarelli and Tabellini (2017), and introduce broadband availability as a parameter that affects an individual’s access to information and their perception of group size. We identify two channels through which internet availability affects the decision to protest. The first one is an information effect. The idea is that dissatisfaction with unfair policies increases with greater access to information, made possible through higher broadband internet availability. The second is a coordination effect, which follows from an interdependence in the decision to protest. The larger the individual’s group is, the more inclined he/she is to engage in protests. Broadband internet thus makes it easier for individuals to realize the size of the groups they are part of. As a result, both effects point to a positive relationship between broadband internet availability and the incidence of protests.====Our paper contributes to two strands in the economics literature. First, it joins recent studies investigating the causal effect of broadband Internet on politics, particularly voter behavior and election outcomes in the U.S. Two similar and contemporaneous studies that stand out in this topic are Larcinese and Miner (2012) and Jaber (2013). Both make use of county-level data aligned to different IV identification strategies to measure the impact of broadband Internet availability on campaign funding, voter turnout, and party vote share in recent presidential elections. In a similar fashion, some authors have replicated this same approach to several countries other than the U.S., such as Germany (Falck, Gold, Heblich, 2014, Czernich, 2012), Italy (Campante et al., 2018), and Malaysia (Miner, 2015).====Second, it goes alongside a growing body of empirical and theoretical research on the determinants of events of social unrest. In the theoretical side, we highlight Little (2016) and Passarelli and Tabellini (2017). Little (2016) develops a model where communication technology generates both information about the level of dissatisfaction with a political regime and logistical information about potential protests. Passarelli and Tabellini (2017) formulates a general theory of how political unrest is motivated by emotions and, ultimately, influences public policy. In the empirical side, important contributions are exemplified by Machado et al. (2011), Esteban et al. (2012) and Voth (2012). In addition, the works of Enikolopov et al. (2020) and Manacorda and Tesei (2020) are most closely related with our paper. The former uses data on social network penetration to explore its impact on protest participation in Russia, and the latter uses data on protests and mobile phone coverage in the African continent to study how communication technology fosters political mobilization. We provide evidence of the aforementioned link using a series of protests that spread throughout the U.S., the largest economy in the world.====In a broader sense, our paper also contributes to the diverse literature on the interplay between politics and media. Many of these studies investigate how the diffusion of printed media (particularly newspapers) influences the relationship between government and civil society. Besley and Burgess (2002), for example, show that state governments in India are more responsive to economic downturns where newspaper circulation is higher. For the U.S., Snyder Jr and Strömberg (2010) show that press coverage on political issues increases government accountability, and Gentzkow et al. (2011) show that the spread of newspapers increase electoral turnout. Other empirical studies suggest that newspapers are related to more diverse consequences such as reduction in corruption (Reinikka and Svensson, 2005) and increase in economic development (Cagé and Rueda, 2016). Broadcast media is also a rich theme of investigation. Gentzkow (2006) identifies that the growth of the television market in the U.S. had a negative impact in elections turnout. DellaVigna and Kaplan (2007) finds that the introduction of a major news outlet influenced U.S. elections outcomes. Finally, Yanagizawa-Drott (2014) shows that signal access to a specific radio station in Rwanda was a major inciter of political violence in the country.====The remainder of this paper is organized as follows. Section 2 characterizes the Occupy Movement in the U.S. with greater detail. Section 3 develops our empirical framework and Section 4 describes the data. Section 5 presents the main results while alternative results and robustness tests are in Section 6. Section 7 concludes the paper.",Broadband internet and protests: Evidence from the Occupy movement,https://www.sciencedirect.com/science/article/pii/S016762452200021X,23 June 2022,2022,Research Article,18.0
Okubo Toshihiro,"Faculty of Economics, Keio University, 2-15-45 Mita Minato Tokyo, 108-8345, Japan","Received 19 October 2021, Revised 11 May 2022, Accepted 17 June 2022, Available online 20 June 2022, Version of Record 6 July 2022.",https://doi.org/10.1016/j.infoecopol.2022.100987,Cited by (7),"In the spread of coronavirus disease (COVID-19), people have been requested to work from home with information and communication technology (ICT) tools, i.e. telework. This paper investigates which factors (infection of COVID-19, individual characteristics, task characteristics, and working environments) are associated with telework use in Japan. Using the unique panel survey on telework, our estimation finds that although telework use remains low in Japan, educated, high ICT-skilled, younger, and female workers who engage in less teamwork and less routine tasks tend to use telework. Working environments such as the richness of IT communication tools, digitalized offices, and flexible-hour working systems are all positively correlated with telework use.",") as well as some occupations (e.g., manual laborers and medical service workers) (Dingel and Neiman, 2021). Telework tends to reduce workers’ performance due to less face-to-face communication (====). For a variety of reasons, some countries such as Japan have observed a low percentage of telework utilization.====A further reason for lower telework use is the government's infection controls. Unlike many other developed countries, Japan has seen a lower percentage of infections in the total population (on a cumulative basis, 6.1% in Japan, 24.6% in the US, 29.1% in Germany, 32.6% in the UK, and 43.8% in France, as of April 28, 2022).==== Japan has taken a unique approach to infection controls. In particular, lockdown was request-based and did not involve any legal restrictions, sanctions, and punishments, which originates from the Constitution of Japan.==== The Prime Minister, the government's COVID-19 subcommittee, and local governments just asked for the cooperation of all people. Such a request-based lockdown might not greatly boost telework use. Therefore, it seems that many factors result in a low percentage of telework use in Japan.====This paper studies telework in Japan with regard to the COVID-19 pandemic using the unique panel survey on telework. In particular, the following questions are addressed in this paper: to what extent is telework used in Japan following the spread of COVID-19, whether occupation-based suitability of telework can be related to actual telework use, and how basic individual traits (e.g., gender, age, education, and income), working environments (e.g., flexible working hours and ICT tools at the workplace), and task characteristics (e.g., nonroutine tasks) are associated with telework use.====As a result of the investigation, we draw several conclusions. First, telework use in Japan remains low in some occupations. Some occupations are suitable for telework (e.g., information processing, business consultants, and ==== insurance workers), whereas some are not (e.g., food services, hotel accommodation, doctors, and nurses). Second, according to our estimation results, female, educated, and younger workers with higher income tend to use telework. The occupational index for teleworkability is positively associated with telework use. Third, persons with higher ICT skills and carrying out fewer routine tasks tend to use telework. Workers under the flextime employment system and with a wide variety of available ICT tools are positively correlated to telework use.====The remainder of this paper is structured as follows. ==== reviews the literature and ==== describes the background. ==== shows data and some evidences. ==== and ==== provide some estimation results. Finally, ==== concludes the paper.====Japan Household Panel Survey (JHPS) by Keio University ====.====The survey on the Impact of COVID on Individual's Job and Life by JILPT ====.====The Japanese Panel Study of Employment Dynamics (JPSED) by the Recruit Works Institute ====.====Survey on Individual's Life and Behaviours by Cabinet Office of Japan ====.====Survey on Telework by Japan Trade Union Confederation (Rengo) ====.====Line Research ====.====PERSOL Research and Consulting ====.",Telework in the spread of COVID-19,https://www.sciencedirect.com/science/article/pii/S0167624522000269,20 June 2022,2022,Research Article,19.0
"Khanna Rupika,Sharma Chandan","Indian Institute of Management Rohtak, India,Indian Institute of Management Lucknow, India","Received 10 November 2021, Revised 19 April 2022, Accepted 17 June 2022, Available online 20 June 2022, Version of Record 14 July 2022.",https://doi.org/10.1016/j.infoecopol.2022.100986,Cited by (0),"India has risen to prominence as a global supplier of information technology (IT). However, the extent to which IT benefits Indian manufacturing is less well known. We assess the effects of IT capital on firm's growth from 1998 to 2016. We make two significant contributions: (i) we make comparisons between different periods, high-IT intensive and low-IT intensive sectors and 14 manufacturing industries at a disaggregated-level, (ii) to overcome the problems of simultaneity in production function estimation, we employ a recently developed semi-parametric technique. Our findings indicate that (i) IT as a production input contributes significantly to firm's output, yet the estimated effects vary significantly by time-period and ====, (ii) the impact of IT is maximum between 2000 and 2009, and then begins to weaken between 2010 and 2016, (iii) IT elasticities in high- and low-technology sectors appear to be converging, particularly between 2010 and 2016.","Economists have long identified investments in general-purpose technologies, such as information technology (IT), as a vehicle of economic progress and technical change (Brynjolfsson and McAfee, 2014; Helpman 1998; Nelson, 1959; Schumpeter, 1942). Accordingly, the world has been spending around $500 billion annually in building IT infrastructure (Gartner, 2015). In 2018, IT expenditure in emerging economies, such as India, reached approximately $87 billion, registering an annual growth of 9 percent (Gartner IT Budget Report, 2017).==== The large sums devoted to IT every year raise the quintessential policy question regarding the contribution of IT towards a firm's growth. In particular, the effect of IT on a firm's output has been studied extensively at the firm, industry, and country level. Recent reviews of the literature by Polák (2017), Cardona et al. (2013), and Draca et al. (2007) provide a comprehensive list of studies applying different methodologies. To date, while many studies appear on the role of IT in advanced economies, evidence on developing economies is largely weak and ambiguous. This is mainly due to the lack of high-quality macro- and micro-level IT-related datasets in these countries (Niebel, 2018).====Overcoming data limitations, a few scholars have assessed this relationship in developing economies and summarized the promising contribution of IT in boosting industrial output and productivity growth, alleviating poverty, and reducing operating costs (Arvanitis and Loukis, 2009; Ho and Mallick, 2010; Niebel, 2018; Stanley et al., 2018; World Bank, 2012). However, a few other studies, such as Dewan and Kraemer (2000) and Hajli et al. (2015), fail to find significant economic growth effects of IT in developing economies. Further, a common feature of the above studies is that most of them are at the macro-level. It has long been argued that aggregated analyses often fail to address specific policy questions meaningfully. Besides, aggregated data are prone to large measurement errors imposed by aggregation schemes (Straub, 2011).====Nonetheless, some studies have analysed the effects of IT at the broad sector-level, for instance, Singh and Sharma (2020); Erumban and Das (2016; 2020), Hall et al., (2013); Sharma and Singh (2012); Wilson (2009); Sánchez et al., (2006) and Stiroh (2002). Brynjolfsson and Hitt (1995) study the output contribution of IT for the broad manufacturing sector. However, contemporary growth literature suggests that technological change occurs mainly at the industry level (De Vries et al., 2012). Thus, taking into account industry-level heterogeneity at a more detailed level is extremely important in understanding the productivity impact of technologically-advanced investments such as IT. However, to the best of our knowledge, none of the above firm-level studies throws light on the distinct patterns of IT effects across manufacturing industries. A few exceptions to this are studies whose focus remains limited to a specific manufacturing industry; for example, Chandran et al., (2020) assess the growth effects of IT in Malaysian food manufacturing firms.====A priori, there may be some rationale behind arguing that the effects of IT differ widely across industries. Industries with lower levels of absorptive capacities, viz., an appropriate level of human capital, or other complementary factors, including R&D, organizational design, etc., may experience lower gains than other industries (Verspagen, 1991). For instance, global data reveals wide disparities in the R&D spending share of industries, such as electronics (22 percent), automobile (16 percent), chemicals (4 percent), telecommunication (percent), etc. (Statista dataset, 2018).==== Thus, the effects of IT are bound to vary too. Industry-level disparities of technological capacity have been all the more prominent in developing economies. For instance, industry experts have cautioned against a growing ‘digital divide’ within the Indian manufacturing sector. As a handful of players prepare to embrace Industry 4.0, a majority of firms in key manufacturing industries, such as textiles, food and beverages, industrial chemicals, etc. continue to rely on Industry 2.0 processes, such as manual inputs, inadequate IT integration in manufacturing processes, etc. (Iyer, 2018). Some economists suggest that these technology gaps may serve as an ‘additional’ source of productivity growth by creating opportunities for low-cost technology assimilation by the laggard industries (Hultberg et al., 2004; Soete, 1985; Verspagen, 1991). Substantial payoffs from IT investments can enable laggard industries’ catch up by ‘leapfrogging’ traditional ways of increasing productivity, such as fixed capital accumulation, in-house R&D, human capability accumulation, etc. (Jiménez-Rodríguez, 2012; Wu et al., 2004), etc.====Against this backdrop, we address whether there are significant output gains from investments in IT to Indian manufacturing firms. Additionally, we assess whether inter-industry patterns in the output elasticities of IT justify the “leapfrogging” hypothesis of industrial growth and whether the effect of IT varies across manufacturing industries. We assess ‘leapfrogging’ patterns by evaluating the temporal and intra-sectoral patterns of IT-led growth in leading and laggard industries in terms of their IT intensity. We follow this with an evaluation of the IT effect in various manufacturing industries separately. In achieving these objectives, the present paper uses a micro-level dataset of 3582 firms for the period 1998–2016 from Indian manufacturing.====The article offers two main contributions. Firstly, the study addresses a somewhat unfairly overlooked question in the growth literature, i.e., heterogeneous IT effects within the manufacturing sector. We make comparisons between two time periods: 1998–2009 and 2010–2016, as well as between high-IT intensive and low-IT intensive sectors and 14 manufacturing industries at a disaggregated level. We relax the assumption of uniform production technology across heterogeneous sub-groups to achieve this. Relaxing the assumption also helps us improve the reliability of our results against the previous studies that have, more often than not, conducted aggregated analyses for the manufacturing sector.==== Our more recent data on an emerging economy enables assessment of the IT effects for a period not covered before (e.g., Erumban and Das, 2016; 2020; Sharma and Singh, 2012).====Secondly, unlike traditional methodological frameworks such as fixed effects, generalized method of moments (GMM), and instrumental variables, which are not well-founded in the theory of producer behaviour (Beveren, 2012), our estimation methodology accommodates some of the most recent advancements in the modelling of producer behaviour in the production function context (Rovigatti and Mollisi, 2018). Precisely, we adopt a control function approach, where we overcome the problem of simultaneity in a semi-parametric setting. We make suitable extensions to the Wooldridge (2009) framework towards this end. One key extension entails dividing capital input into two types: IT capital and non-IT (ordinary) capital, which helps us segregate the effects of IT investments undertaken by these firms.====The rest of the paper is organized as follows. Section 2 presents a survey of the recent IT-growth literature. Section 3 derives the estimation model and discusses related empirical issues. The data used for the present exercise is summarized in Section 4, followed by results and their interpretation in Section 5. Section 6 concludes the paper and presents policy implications and directions for future research.",Impact of information technology on firm performance: New evidence from Indian manufacturing,https://www.sciencedirect.com/science/article/pii/S0167624522000257,20 June 2022,2022,Research Article,20.0
"Bisceglia Michele,Padilla Jorge,Piccolo Salvatore,Shekhar Shiva","Toulouse School of Economics, France,University of Bergamo, Italy,Compass Lexecon, United States,CSEF, Italy,University of Passau, Germany","Received 9 September 2021, Revised 14 April 2022, Accepted 8 June 2022, Available online 16 June 2022, Version of Record 30 June 2022.",https://doi.org/10.1016/j.infoecopol.2022.100981,Cited by (1),We study the competitive effects of a vertical ,"In this paper, we revisit the classical arguments related to control changes arising from a vertical merger in the context of digital markets. The adoption of hybrid business models by platforms such as Amazon, Apple, Google and Facebook and their aggressive acquisition strategies have renewed the policy interest for vertical mergers. A distinctive feature of these businesses is that they coordinate broad ==== — i.e., groups of connected firms, often complementors — to lock-in their customers (Jacobides and Lianos, 2021).====Ecosystem gatekeepers gain power by being nodal and hard to replace (see, e.g., Cusumano et al. (2019)). But, to maintain a ‘bottleneck’ role and remain attractive to users, these platforms need to be constantly at the technological frontier to improve the quality of their final products and services, thereby benefitting all their participants — i.e., both final consumers and business users. Indeed, ecosystems often compete with each other. In addition, some of them are integrated while others are not. The latter, ====, feature intra-ecosystem competition among firms offering rivalrous, potentially substitute products within the same ecosystem (e.g., Bourreau and Perrot (2020)).====Modern hi-tech and digital industries share these features. Leading tech giants such as Apple, Amazon and Google compete by designing platforms with different business models and attributes to attract end-users (buyers and sellers) that trade and compete within and across these platforms. Alternative business models — i.e., organizational forms — are likely to impact the strength of inter- and intra-ecosystem competition and, importantly, the incentives to innovate at the ecosystem level. Yet, only few formal models investigate ecosystem business models and their effects of innovation (see, e.g., Tiwana (2013)). The present article contributes to filling this gap by examining the competitive and welfare effects of vertical integration in a competing ecosystem framework.====Recently, for example, NVIDIA tried to acquire Arm. The deal, worthed $40 billion, attracted media and regulatory interests worldwide and it was finally abandoned since competition policy authorities all around the globe were worried that the merger could consolidate NVIDIA competitive position in the PC and data-center markets.==== But, how would the acquisition have affected the merged platform’s quality and the delivery speed of critical computing technologies, including CPU, GPU, and AI? And how would rivals have reacted? These are questions of paramount importance both from a managerial and a policy standpoint.====Vertical mergers raise competitive concerns since they may result in the anticompetitive foreclosure of non-integrated rivals (see, e.g., Hart and Tirole (1990); Bolton and Whinston (1991); Rey and Tirole (2007), among many others). That is, vertically integrated suppliers may exploit their consolidated market power to exclude (partially or in full) their independent rivals to soften downstream competition: the so-called ‘foreclosure doctrine’. However, vertical mergers may also bring valuable efficiencies, mostly when they eliminate double marginalization or when they help upstream suppliers overcoming the hold-up problem emerging when contracts are incomplete and suppliers make non-contractible, relationship-specific investments before dealing with their downstream units (see, e.g., Motta (2004), and Riordan (2005), for insightful surveys of the literature).====The trade-off resulting from these forces is usually a complex phenomenon, hard to evaluate in practice. Mergers that bring upward pressure on wholesale prices are likely to be blocked absent efficiencies. Mergers that soften double marginalization or solve hold-up problems are more likely to bring efficiencies and thus less likely to raise competitive concerns. But does this tell the whole story about vertical mergers involving ecosystems? Is the absence of double marginalization sufficient to infer consumer harm in circumstances where ecosystem gatekeepers continuously innovate to improve the quality of their networks so as to steal business away from competing ecosystems?====We argue that in ecosystem-markets where innovation plays a public good role, the traditional assessment of the competitive effects of vertical mergers needs to be amended. To make this point, we consider a simple setting in which a vertically integrated incumbent (ruling a closed ecosystem) competes with an (open) ecosystem formed by two (horizontally differentiated) downstream firms and their essential input supplier (the gatekeeper of the open ecosystem). The incumbent has a competitive advantage (measured by a higher demand intercept) vis-à-vis its downstream rivals. Yet, the gatekeeper of the open ecosystem can fill up this gap by engaging in product innovation, thereby increasing the appeal, and thus demand, of all products in its ecosystem.====Under the hypothesis of secret and non-linear wholesale contracts (i.e., two-part tariffs), our model produces the following novel competitive and welfare insights. First, a vertical merger between the gatekeeper of the open ecosystem and one of its downstream retailers will never result in the complete foreclosure of the independent firm in the ecosystem, and the incentive to retain a duopoly within the ecosystem is increasing with the incumbent’s competitive advantage — i.e., the wholesale price charged by the merged entity to the non-integrated retailer is decreasing in the extent of incumbency advantage. Second, the vertically integrated platform has, nonetheless, an incentive to raise the costs of the independent downstream firm competing within its ecosystem. This incentive is tempered by fiercer inter-ecosystem competition. Third, in line with Cabral (2021) and Katz (2021) arguing that the proposal to tighten (horizontal) merger guidelines in the tech industry may discourage investment, we find that also a vertical merger increases the gatekeeper’s incentive to innovate and catch up with the incumbent, even in the absence of hold-up. This result squares with the evidence documented in Lee (2013) and Zhang and Tong (2021), who find that firms completing vertical M&As experience a growth in their rate of innovation, quality and consumer surplus. Again, the likelihood that the merger stimulates innovation is increasing in the incumbency advantage. Fourth, this positive innovation effect may offset any increase in prices and, hence, vertical integration may benefit consumers even in the absence of double marginalization efficiencies. This is more likely to happen when inter-ecosystem competition is relatively fiercer compared to intra-ecosystem competition.====Finally, while vertical integration always benefits the merging parties, its effect on the incumbent operating a closed ecosystem is ambiguous. On the one hand, it benefits from the softening of inter-ecosystem competition the merger brings about. On the other hand, it may lose its quality advantage vis-à-vis the two downstream firms in the open ecosystem. Notably, the vertical merger is likely to make the incumbent better off when it hurts consumers.====Furthermore, we show that the beneficial effect of vertical integration on consumer surplus is magnified under linear (wholesale) contracts because, in this case, the merger also eliminates double marginalization. However, with linear contracts, the effect of the merger on the incentive to innovate is ambiguous, as it depends on the degree of intra- and inter-ecosystem competition. Specifically, when the incumbent’s product is sufficiently differentiated from the ecosystem’s ones, the merger is profitable and can also stimulate investment. By contrast, when both inter- and intra-ecosystem competition is fierce enough, the merger can be profitable only if it reduces investments. In this case, the merger still enhances consumer surplus provided that the ‘incumbency’ advantage is not too large, so that the reduction in prices outweighs the lower ecosystem quality. Once again, as with two part tariffs, the merger is likely to benefit the incumbent when it is detrimental to consumers. By contrast, the non-integrated retailer within the open ecosystem can benefit from the merger only if it spurs investment — i.e., not every CS-increasing merger benefits the non-integrated unit, though its interests are more aligned with consumer welfare as compared to the incumbent.====The rest of the paper is organized as follows. After reviewing the related literature in Section 1.1, in Section 2 we set up the model, characterize and compare the pre- and post-merger equilibria, and discuss some extensions. Section 3 concludes. All proofs, and additional material, are in the Appendix.","Vertical integration, innovation and foreclosure with competing ecosystems",https://www.sciencedirect.com/science/article/pii/S0167624522000208,16 June 2022,2022,Research Article,21.0
"Sabatino Lorien,Sapi Geza","Department of Management and Production Engineering, Politecnico di Torino Italy,European Commission DG COMP - Chief Economist Team and Düsseldorf Institute for Competition Economics (DICE), Heinrich Heine University of Düsseldorf. Belgium","Received 28 January 2021, Revised 4 April 2022, Accepted 10 June 2022, Available online 12 June 2022, Version of Record 4 July 2022.",https://doi.org/10.1016/j.infoecopol.2022.100985,Cited by (2),"This paper investigates how privacy regulation affects the structure of online markets. We empirically analyse the effects of the 2009 ePrivacy Directive in Europe on firm revenues. Our results indicate that, if any, only large firms were weakly negatively affected by the implementation of the Directive. We also provide a simple theoretical model predicting an avenue how privacy regulation may predominantly influence the revenues and profits of larger firms, even if - as some of our evidence indicates - these larger firms may actually offer more privacy than smaller rivals. Our results suggest that while privacy regulation is not without costs to businesses, it need not distort competition to the favour of larger firms.","Firms in the digital economy collect customer data at an unprecedented rate. Electronic commerce in physical and digital goods is fuelled by recommendation engines: algorithms that rely on user data on demographics, previous purchases, and other preferences to predict products and services an online shopper may be interested in. Commentators often attribute a large share of the stellar success of internet giants like Amazon and Netflix to the ability of these firms to recommend products to their users based on data and analytics (Arora, 2016).====At the same time, consumers are increasingly mindful about online privacy. While in 2011 around ==== of surveyed Europeans were concerned about their behaviour being recorded through the internet when browsing, downloading files, and accessing content online (European Commission, 2011, page 67), in 2015 less than a quarter of Europeans reported trusting online businesses to protect their personal data (Eurobarometer, 2015, page 25).====As a response to the increased privacy concerns, the European Union (EU) put into force a series of privacy regulations since the early 2000s. The 2018 General Data Protection Regulation (GDPR) empowered European data protection authorities to issue hefty fines comparable to those in antitrust on firms violating data protection rules. In the same year, the European Commission put forward a proposal for an EU-wide ePrivacy Regulation to replace the current ePrivacy Directive of 2009 (Eur, 2018). The proposal received a lot of criticism from industry representatives, expressing concerns about the effect of stricter online privacy rules on the competitiveness of European businesses, adding that the regulation may benefit large firms.====We do three things in this paper. First, we empirically investigate the effect of the 2009 revision of the European ePrivacy Directive (2009/136/EC) on the market structure in e-commerce. In particular, we employ a ==== (DDD) model to identify the effect of increased privacy regulation on the revenues of European firms active in the online retail sector. We exploit time variation in the implementation of the ePrivacy Directive by EU Member States. Our data allow comparing European e-commerce businesses to a control group consisting of firms primarily active in North America (U.S. and Canada) as well as brick-and-mortar firms selling similar consumer discretionary products as their online counterparts. Second, we propose a simple theoretical model of competition in the online retail sector that captures the main trade-off between the informativeness of advertising and the degree of privacy intrusion (Tucker, 2012). Our model predicts that privacy regulation affects primarily the profits of larger firms, even if - as some of our evidence indicates - these larger firms may actually offer more privacy. Third, we review a large body of qualitative and quantitative evidence that relates to the assumptions in our theoretical model and empirical analysis.====Our empirical results indicate that the 2009 ePrivacy Directive had on average no significant effect on the revenues of European e-commerce firms. However, it had significant heterogeneous effects among large and small firms. Only the revenues of large firms reduced, while those of small firms were essentially unaffected. This stands in strong contrast to other empirical studies of (even the same) privacy regulation that tend to emphasise negative effects on the industry (Goldfarb, Tucker, 2011, Jia, Jin, Wagman, 2021, Lambrecht, 2017), and especially on small firms (Campbell et al., 2015).====Our results carry strong implications for the intersection of competition and data protection policy. They allow informing the ongoing debate regarding the most recent round of revision of the ePrivacy Directive in Europe. This revision is at the time of writing this article finally pushed out of a long stalemate stretching over several years in the European Council.==== Discussions came to a halt mostly due to concerns raised by industry groups regarding a loss of competitiveness vis-à-vis online firms outside Europe (Ghosh, 2018, Singer, 2018, Gwynn, 2017, McConnell, 2019). Our empirical results looking at the last (2009) round of revision of the very same Directive suggest a more nuanced and optimistic view for businesses. While revenue losses cannot be excluded, these were small and confined to large firms only. Second, we emphasise the potential role of technology - in particular, firms’ ability to monetise user data - to drive asymmetries in market shares and even impact which firms are affected most by privacy regulation.====The remainder of the paper is organised as follows. Section 2 introduces the relevant literature. Section 3 describes the institutional background. Section 4 introduces the data and the empirical model, and reports our main findings. Section 5 provides a theoretical model that rationalises our empirical results. Section 6 concludes.",Online privacy and market structure: Theory and evidence,https://www.sciencedirect.com/science/article/pii/S0167624522000245,12 June 2022,2022,Research Article,22.0
"Henriksen Alexandre Lauri,Zoghbi Ana Carolina,Tannuri-Pianto Maria,Terra Rafael","University of Brasilia: Universidade de Brasilia Brasília, Distrito Federal Brazil, Brazil","Received 23 January 2021, Revised 22 February 2022, Accepted 8 June 2022, Available online 10 June 2022, Version of Record 23 June 2022.",https://doi.org/10.1016/j.infoecopol.2022.100983,Cited by (1),"This study analyzes the effect of local Internet speed infrastructure (backhaul) on educational outcomes. In 2008, the Brazilian government implemented an Internet expansion policy that brought broadband to more than 3000 municipalities. The policy was designed with implementation criteria that make it a natural experiment that can be investigated through a ==== (RDD). The results suggest worsening proficiency, higher dropout and retention among students from municipalities served by more powerful backhauls, i.e., capable of supporting higher connection speeds over fiberoptic lines. These results are paralleled in the empirical literature, which predominantly indicates negative or neutral effects of Internet access on education. This study demonstrates the need for a deeper reflection on the domestic use of the Internet and its consequences on educational outcomes of school-age children and adolescents.","In 2008, Brazil launched a series of programs focused on broadband Internet expansion. The primary program involved implementing a backhaul to connect 3439 (==== of) Brazilian municipalities, which did not have access to this technology. A backhaul is an essential piece of telecommunications infrastructure that connects a municipality to the Internet and thus allows access to all of its inhabitants, particularly in urban areas====.====The objective of this study is to assess the effects of different backhaul connection speed capabilities on student outcomes – such as math and reading standardized scores, dropout, retention, and percentage of students scoring above or below reference thresholds. The present study explores a natural experiment, as some cities received a more powerful backhaul according to an exogenous regulation. The rule for Internet speed assignment is discontinuous in the municipal population – at several thresholds –, which allowed us to employ a parametric regression discontinuity design (RDD). To the best of our knowledge, this is one of the first studies to compare education outcomes across municipalities with different backhaul speed capacities using administrative data and a regression discontinuity strategy.====The introduction of broadband Internet can have a number of effects on the local economy and society. There may be effects on fields such as the productivity of labor (Jung and López-Bazo, 2020), business and the public sector (Van der Wee et al., 2015) – e.g., e-business and e-government –, elections (Campante et al., 2017) and the pursuit of distance education (Escueta et al., 2017).====There may have ambiguous effects of broadband Internet on education outcomes depending on the purpose of its use, namely, for learning or entertainment. In general, the literature associates Internet access at school with education and access from home with entertainment, although it can also be accessed for learning purposes from home====.The natural experiment explored in this study allows us to estimate the effects of increasing Internet speed capacity on education outcomes, especially through increasing household connections.====According to Escueta et al. (2017), the literature on educational information and communication technologies (ICTs) can be divided into four main areas or types of digital technology interventions: 1) access to technology, which includes access to computers, software and/or the Internet; 2) computer-assisted learning (CAL)====; 3) use of behavioral interventions; and 4) access to online courses.====Programs that only deliver ICT infrastructure to schools (such as “one laptop per child” or “connected schools” policies) are generally ineffective, yielding experimental evidence of neutral or, in some cases, even a negative effect on student grades (Bando, Gallego, Gertler, Romero, Barrera-Osorio, Linden, 2009, Cristia, Ibarraran, Cueto, Santiago, Severín, 2012, Fairlie, Robinson, 2013); these results are also corroborated by studies using quasi-experimental techniques with broader data on these types of public policies (Angrist, Lavy, 2002, Belo, Ferreira, Telang, 2014, de Melo, Machado, Miranda). Conversely, certain programs have been evaluated as effective, particularly using quasi-experimental methods (Alderete, Formichella, 2017, Dettling, Goodman, Smith, 2018, Prieto, 2014, Schmitt, Wadsworth, 2006, Spiezia, 2010).====Studies on the specific effects of internet access on student performance are equally varied, with a trend toward a lack of effect. A pioneering assessment on the subject addressed the subsidy program known as E-Rate in the United States (US), which was intended to change the incentives related to the decision to expand Internet investment in public schools in California. Goolsbee and Guryan (2006) find positive results – using regression discontinuity – for Internet investment, although there was no effect on grades. Belo et al. (2014) assess the impact of broadband Internet access at Portuguese schools on students’ grades. The authors find a reduction in grades of 0.78 of a standard deviation, while students at schools that blocked access to YouTube show better performance. On the other hand, data on Internet access and admission exams for US universities show that the Internet has a positive effect on university enrollment and acceptance decisions (Dettling et al., 2018).====Exploring differences in the infrastructure that supports Internet access can be a strategy for measuring the impact of the Internet on school performance. There may be changes in the infrastructure that are not correlated with other variables affecting educational indicators, offering a suitable instrumental variable for analysis. Faber et al. (2015) use data from more than 20,000 telephone exchange station catchment areas in the United Kingdom–invisible to the consumer but providing random jumps in the available residential Internet speed–to measure the effect of Internet availability at home on school performance. The authors find no effects, even for major changes in broadband connection speeds.====Recent studies seek to identify whether specific uses of the Internet may have an impact on school performance. Barbetta et al. (2019) analyze the use of Twitter for motivational purposes in high school literature classes in Italy. Although the tool is used extensively by certain groups of teachers and students, with positive perceptions from both sides, analysis using experimental methods shows that it was, in practice, detrimental to performance. More targeted uses of the Internet may yield better results (Comi et al., 2017), as it is possible that a freer use of the technology would be focused almost exclusively on entertainment activities, without positively influencing educational performance (Malamud et al., 2018).====For the purpose of discussion on the inter-relations between the Internet usage, education and entertainment, we propose a model of educational and entertainment production and consumption using the Internet as an input and agents with limited rationality. We then suggest the possibility that agents are behavioral, that is, they employ heuristics in their decision-making processes and are subject to behavioral biases, and finally discuss the policy implications. Such a discussion adds meaning to our results.====The results of our empirical model suggest that backhauls supporting higher Internet speeds have negative effects on educational outcomes for municipalities served with fiberoptic technology. The harmful effects of Internet use on education outcomes are not new and can be found in Lima et al. (2018), Vigdor et al. (2014), Belo et al. (2014), Malamud et al. (2018) and Barbetta et al. (2019). The Internet can be used for entertainment and take time away from more productive learning activities, so more research is needed on which uses of the Internet can be effective for educational performance and how families should allocate their time between education and entertainment to avoid the worsening of educational outcomes.====This study is divided into 7 sections, in addition to this introduction. Section 2 describes the backhaul program and presents basic information. Section 3 describes the dataset. Section 4 discusses a model of household production of education and entertainment. Section 5 presents the empirical strategy. Section 6 describes the preliminary robustness tests. Section 7 reports the results, and Section 8 discusses the findings and presents the conclusions.",Education outcomes of broadband expansion in Brazilian municipalities,https://www.sciencedirect.com/science/article/pii/S0167624522000221,10 June 2022,2022,Research Article,23.0
Lorenzon Emmanuel,"Univ. Grenoble Alpes, CNRS, INRAE, Grenoble INP, GAEL, Grenoble 38000, France,Governance and Regulation Chair, University Paris-Dauphine, PSL Research University, Paris 75016, France","Received 7 April 2021, Revised 20 January 2022, Accepted 25 January 2022, Available online 31 January 2022, Version of Record 8 March 2022.",https://doi.org/10.1016/j.infoecopol.2022.100965,Cited by (2),"We consider a departure from net neutrality by an Internet service provider (ISP) that financially discriminates among content providers through exclusive zero-rating contracts. Zero-rating is an instrument to distort competition between content providers and the manner in which consumers value content. We analyze its implications for the incentives to provide quality in the market for content and to invest in broadband infrastructure. Zero-rating makes content more expensive for consumers to use and imply a downward distortion of content quality. Content providers switch from minimal differentiation to a downward vertical differentiation outcome. Next, we find that zero-rating implies underprovision in the broadband infrastructure, which comes from a standard rent-extraction argument and a cost-alleviation channel related to the complementarity between network capacity and content quality. Finally, when implemented, zero-rating is found to be welfare reducing and detrimental to consumers.","Net neutrality, according to which an Internet service provider (ISP) should not discriminate among data packets sent on its network, has been a matter of heated debate during the past decade, with new developments in recent years. In Europe, the practice of zero-rating is pervasive (European Commission, 2017), but related legislation is still debated and opaque====. In September 2020, The Court of Justice of the European Union enshrined the net neutrality principle by ruling against a zero-rating practice used by the Hungarian telecom operator Telenos, which has been found to use zero-rating as a traffic management tool to discriminate against applications that were not part of its MyChat and MyMusic subscription plans====.====Zero-rating is a practice by which an ISP makes some content more expensive than others for consumers to access. Consumers subscribe to a monthly mobile data plan, which provides a data allowance. For all data packets consumed in excess of that allowance, consumers are charged marginal fees and/or the data usage is either blocked or restricted. Thus, zero-rating is a tool that an ISP can implement to price discriminate among content providers (CPs). The data from zero-rated content do not count against the cap. Once a consumer reaches his or her data cap, such content is exempted from per-unit surcharges and usage restrictions====.====Network operators advertise that zero-rating is beneficial for users because it purportedly allows users to consume more content at the same price for their mobile plan and allows operators to efficiently manage traffic and foster their incentives to invest in network quality (Schnurr and Wiewiorra, 2018, and Krämer, Peitz, 2018, Krämer, Peitz, 2018). Because CPs typically rely on traffic to generate revenue from either advertisements or user payments, opponents of such data management regimes contend that departures from net neutrality might steer consumers’ choices of online content toward providers included in the operator’s contracting offers, which could raise barriers to entry and impede incentives to provide high content quality in the market for content (Nurski, Nurski). Although the issues surrounding investments by ISPs in network infrastructure are crucial for regulators and well documented, especially with respect to priority pricing, their interplay with content quality has yet to be overlooked by the network neutrality literature regarding zero-rating practices (Krämer, Peitz, 2018, Krämer, Peitz, 2018). However, a tight relationship exists between the two dimensions. For instance, incentives to provide qualitative network infrastructure are positively related to consumers’ willingness to pay for content, which itself positively depends on the quality of the content provided within the network.====We add to the growing debate on data management regimes by focusing on investment decisions made by CPs and a profit-maximizing ISP. As in Gautier and Somogyi (2020), zero-rating is viewed as a tool to enhance content differentiation and to alter the competition in the market for content. However, the focus has been on the ability of an ISP to mitigate content asymmetry in environments in which CPs are passive and network investments are left separate====. CPs do not invest in content attractiveness or determine the amount of advertisement to which to expose their users, and ISPs face some given capacity constraints. Our aim is to study the implications that departure from a network neutrality regulation has for ISPs and CPs’ incentives to provide broadband facilities and content quality.====We compare two regulatory regimes: net neutrality and bilateral zero-rating contracts, which means that the ISP offers an exclusive zero-rating contract to a unique CP====. To study the impact of a move from the net neutrality regime on the investment decisions of CPs and the ISP, we consider a monopolistic ISP connecting two horizontally differentiated CPs to a unit mass of consumers with unit demand, which is consistent with the fact that users typically choose one CP at a time for each device or ISP. The ISP provides a network capacity constraint, and CPs compete ==== Hotelling to offer vertically differentiated services to consumers under an advertising-supported service model. The ISP charges consumers a connection fee to access its network and might price discriminate between CPs by charging consumers different per-unit fees for the two content offerings. CPs differ in their advertising revenues, and each imposes different advertising exposure levels on consumers on their respective websites. Thus, horizontal and vertical differentiation interact, and CPs compete on the level of advertisement to which they expose their users and from which they draw their revenues. Finally, we suppose that CPs are asymmetric with respect to the number of requests that users generate for their respective content, in that one CP obtains more content requests than its competitor.====In our model, ”quality” is understood as investments by CPs in the attributes or functionalities embodied in their content. In addition, consumers bear disutility per unit of content from being exposed to advertisements that, as in Calzada and Tselekounis (2018), interact with content quality. The idea is that higher content quality means that consumers spend more time using it and are more exposed to advertisements, and CPs benefit more from quality improvements. Hence, the per-unit advertising exposure rate is understood as a per-unit fee that CPs charge consumers, meaning that it possesses the same properties as a pure per-unit price to access the content. As a result, our setup not only allows for the investigation of quality and advertisement competition between CPs but also captures the business models of most services included in zero-rating====.====The main contribution of this work to the literature is to demonstrate that, in a static model with asymmetric CPs, a zero-rating makes content more expensive to access, implying a downward distortion of quality in the market for content and provision of broadband capacity attributable to complementarity between content quality and network capacity.====Our first result is to show that a departure from net neutrality softens quality competition and constitutes an impediment to quality improvement in the market for content. While in a neutral network, content quality is symmetric and CPs opt for minimal vertical differentiation, a departure from net neutrality reduces the overall level of content quality in the market and increases the degree of asymmetry between CPs. Incentives to invest in content quality are misaligned between CPs. The least attractive CP, which has a zero-rating contract with the ISP, benefits from greater market share and provides higher quality than its competitor to increase users’ willingness to switch to its content and reduce the disutility from congestion supported by its users. In contrast, the noncontracting CP opts for quality degradation to reduce the ISP’s ability to price discriminate between consumers and to prevent switching by its home users. However, investments provided by the zero-rated CP do not sufficiently compensate for the lack of investment from its rival, making the content sector less innovative. As a result, CPs switch from a minimal vertical differentiation outcome under net neutrality to an asymmetric equilibrium with a greater degree of downward vertical differentiation when zero-rating is allowed. In parallel, we find that zero-rating softens advertising competition in the market for content. Users are more exposed to advertising than in a neutral network. Because advertising exposure can be interpreted as a per-unit payment from consumers to CPs, content becomes more expensive to access when zero-rating is implemented.====Our second main result is to show that a departure from net neutrality reduces network investments. Alongside a price discrimination argument, which implies that the ISP offers a zero-rating contract to the least attractive CP, zero-rating reduces the ISP’s incentives to invest in network capacity through a cost-alleviation channel. Indeed, in contrast to the main claim from the industry, we find that a profit-maximizing ISP underinvests under a discriminatory regime. However, given the complementarity with content quality, we demonstrate that zero-rating reduces network congestion, which might constitute a traffic management instrument. This underinvestment is motivated by two explanatory channels. The first operates via standard price discrimination and relates to the rent-extraction effect derived by Choi and Kim (2010): the ISP increases resource scarcity, which allows for a higher per-unit surcharge on consumers of non-zero-rated content. Therefore, in equilibrium, as in Gautier and Somogyi (2020), the ISP offers a zero-rating contract to the least attractive CP. Reducing network capacity allows larger rents to be extracted from users of the most attractive content. The second channel operates via cost-reducing incentives: indirectly, the quality of the non-zero-rated content and network capacity are complements. By imposing a per-unit fee, a strong CP reduces the quality it provides to its users, also reducing congestion on the network and alleviating the need for the ISP to invest in the network; thus, it strategically reduces its investment. Hence, as a contribution to the debate surrounding the efficiency of broadband investment decisions, we find that, in contrast to Gautier and Somogyi (2020), investments in capacity are socially suboptimal under a discriminatory regime. In particular, an ISP underprovides network capacity with respect to the social optimum.====Finally, the social welfare analysis shows that a profit-maximizing ISP fails to adopt a socially efficient pricing policy and zero-rating is always implemented, which is detrimental to consumers and welfare-reducing. The ISP has incentives to engage in practices that make resources scarcer and always finds it profitable to financially discriminate between CPs, whereas consumers are always better off under net neutrality. Hence, a profit-maximizing ISP has pervasive incentives to implement zero-rating contracts, and these incentives are misaligned with consumer welfare.==== This work contributes to the significant literature on net neutrality and data management practices by ISPs that violate the principle====. In particular, this work is close to the body of work that models the impact of net neutrality on ISPs’ incentives to invest in network capacity with an emphasis on content innovation====.====As underlined by Goldfarb and Tucker (2019), the literature on net neutrality developed along the usage of data transmission technologies and the growing importance of the role of ISP strategies for the emergence of other businesses. Lee and Wu (2009) considered net neutrality as a type of subsidy for innovation to the extent that it does not impose transaction costs on CPs. In their vision, departures from net neutrality oblige CPs to negotiate with ISPs in a similar fashion as in other industries, such as cable TV, potentially discouraging several innovators from creating new services. They contend that in an industry of ”stars,” such as the CP industry, lower transaction costs are possible because CPs do not need to negotiate with distinct ISPs for access to their consumers and reduce barriers to entry, thereby enabling the emergence of new players. In contrast with this view, and as noted by Schnurr and Wiewiorra (2018), operators advertise paid prioritization and zero-rating as beneficial for users because the latter can consume more content while paying the same price for their mobile plan.====In a deregulated market, ISPs have incentives to depart from net neutrality because they can generate additional revenues from CPs by offering benefits in return (e.g., prioritization of data or exemptions from users’ data allowance), attract new customers from the network effects, and better discriminate among consumers on price and quality (Krämer, Peitz, 2018, Krämer, Peitz, 2018, and Goldfarb, Tucker, 2019, Goldfarb, Tucker, 2019). In Schnurr and Wiewiorra (2018), ==== practices might distort competition toward the CPs included in the sponsorship plan and might cause losses to those excluded. The authors consider two symmetric CPs generating revenues from advertising and connected to consumers via a monopolistic ISP. Consumers have preferences for one CP (”high-value”) or the other (”low-value”). The authors emphasize that, according to their results, both zero-rating and paid prioritization are distortionary—challenging the view that the latter should be more scrutinized than the former. This result is in line with the empirical findings in Nurski (2012) using data from the United Kingdom. The author finds that departures from net neutrality might steer consumers’ choice toward the CP included in the zero-rating plan. Once CPs typically rely on traffic-based revenues (advertising or consumer payment), zero-rating raises barriers to entry for CPs excluded from the plan and might reduce the variety of CPs available to users and the quality provided in the market for content. We contribute to this debate in line with the last observation because we find that a discriminatory regime reduces content quality and implies downward vertical differentiation. Given the interplay among content quality, congestion, and network capacity, zero-rating reduces managerial costs, increases CPs’ asymmetry, and allows the ISP to collect higher payments from users of the non-zero-rated CP.====One important aspect in this setting is that ==== data regimes might lead to excess data consumption, resulting in negative externalities in the form of congestion. Bourreau et al. (2015) noted that different services require distinct network capacities and that departing from net neutrality allows ISPs to manage data packages according to content, which in turn enables them to alleviate capacity constraints and congestion effects.====Our representation of zero-rating contract is close to that of Gautier and Somogyi (2020), and our modeling assumption about network investment decisions borrows from Choi and Kim (2010) and Bourreau et al. (2015). As in Gautier and Somogyi (2020), we find that the ISP always price discriminates and contracts with the least attractive CP in equilibrium. Although in their model zero-rating is implemented to reduce the asymmetry between CPs, in our model, the ISP uses zero-rating to distort vertical differentiation between content (which increases CPs’ asymmetry) and to affect how consumers value each CP. However, in contrast to their result that investment in capacity is aligned with the social optimum, this is never the case in our model. Private and public incentives to invest are aligned solely under net neutrality regulation, and a discriminatory regime reduces network capacity, which is in line with Choi and Kim (2010), who show that network investments are reduced in the long run====. Although for them, this strategic reduction stems from a rent-extraction effect (also presented in our model) arising from congestion, the reduction in capacity in our model follows from the fact that managerial costs are borne by the noncontracting CP because of the positive relationship between content quality and congestion and the complementarity between content quality and network capacity.====The literature providing a formal economic analysis of zero-rating contracts is scant. To the best of our knowledge, no work has yet investigated its parallel implications for network investment and content quality provision with ambient congestion. Much of the literature models a monopolistic ISP connecting consumers to CPs, drawing exogenous revenues from advertising and competing passively to attract users. Somogyi (2017) considered the interplay between congestion and increasing utility from consumption under open and exclusive zero-rating contracts. In this model, the attractiveness of content plays a key role. When content is attractive, the ISP always offers an open zero-rating. Jullien and Sand-Zantman (2018) considered zero-rating contracts as an instrument to screen among traffic-sensitive CPs and to enhance allocative efficiency. In equilibrium, sponsored data are selected only by high-type CPs and improve network efficiency because they induce more traffic to be directed to high-valued content. Its welfare implications are ambiguous in that they depend on the mass of existing high-type CPs and the distribution of low-type CP values. Whereas in Jullien and Sand-Zantman (2018), the ISP uses zero-rating to screen among CPs, in Inceoglu and Liu (2019), zero-rating is implemented to screen among consumers in an environment with multiproduct demand. The ISP uses zero-rating to screen consumers according to the quantity consumed and the composition of consumption. Zero-rating is found to be welfare-enhancing and cause network capacity expansion. Jeitschko et al. (2020) considered the implications of zero-rating with a vertically integrated ISP and asymmetric CPs with respect to some given quality parameter. Schnurr and Wiewiorra (2018), who analyzed two groups of consumers distinguished by their valuation of the content, find that when consumer groups are heterogeneous in their valuation of zero-rated content, they benefit from this practice. In contrast, when consumers are rather homogeneous in this preference, zero-rating might harm them. Gautier and Somogyi (2020) compared the market outcomes under both zero-rating and paid prioritization with two CPs that are horizontally differentiated and asymmetric—the ”stronger” CP has a larger natural market than its ”weaker” counterpart. The general conclusion is that paid prioritization is preferable when traffic is valuable for CPs and congestion is severe—in the other cases, ISPs tend toward zero-rating. Finally, Hoernig and Monteiro (2020) studied the role of network effects in an ISP’s rationale for implementing zero-rating. They noted that zero-rating is the profit-maximizing choice if network effects are strong enough and if the costs of increasing network capacity are low. They also noted that the result is similar under monopoly and duopoly conditions; however, in the latter case, the ISP with the larger consumer base benefits the most. We extend this literature by, first, finding that zero-rating makes content more expensive for consumers to access and implies a downward distortion of content quality by increasing downward vertical differentiation. Through zero-rating, the ISP is able to increase CPs’ asymmetry and affect consumers’ willingness to pay for content. CPs move from a minimal differentiation equilibrium to a downward vertical differentiation outcome. Second, we show that zero-rating is instituted to reduce congestion, whereas investments in broadband capacity are strictly lower than those under net neutrality. A complementarity effect between content quality and investment in network capacity is at play.","Zero-rating, content quality, and network capacity",https://www.sciencedirect.com/science/article/pii/S0167624522000026,31 January 2022,2022,Research Article,24.0
"Barrachina Alex,Forner-Carreras Teresa","Department of Finance and Accounting and LEE, Universitat Jaume I, Castellón, Spain,Department of Economic Analysis, Universitat de València, Valencia, Spain","Received 31 May 2020, Revised 23 December 2021, Accepted 5 January 2022, Available online 8 January 2022, Version of Record 8 March 2022.",https://doi.org/10.1016/j.infoecopol.2022.100964,Cited by (0)," whose product is supplied by domestic firms. Moreover, successful economic espionage implying market entry of foreign firms would harm domestic welfare. Considering counter-espionage policy as entry barrier and sufficient efficiency in espionage and counter-espionage efforts, the analysis of the benchmark case characterized by no foreign consumer and one foreign firm suggests that demand characteristics play an important role in the complex influence of competition in espionage. Irrespective of this, optimal counter-espionage effort is always positive although negatively affected by competition.","The illicit attempts by countries to acquire information on critical manufacturing processes and technologies employed by industries in other countries is considered ====.==== Although the information technology revolution that the world has undergone since the 1990s has facilitated economic espionage (Nasheri, 2005, pp. 1–2; Glitz and Meyersson, 2020, p. 1099), it is not a new phenomenon. For instance, almost 200 years ago Robert Fortune, on a covert mission, stole the Chinese tea processing technique, causing the fall of the China’s tea monopoly (Ben-Attar, 2004).====That branch of economic espionage can have extremely important consequences not only for the companies affected, but also for the general economy of the country being spied on. Although it is arduous to quantify the magnitude of these consequences (Nasheri, 2005, p. 52), the Commission on the Theft of American Intellectual Property (2017) estimated that the annual cost of economic espionage to the U.S. could be between $225 billion and $600 billion.==== Furthermore, National Counterintelligence and Security Center (2018) identified energy, biotechnology and defense technology sectors as the main targets for foreign intelligence collectors, and warned of the aggressiveness with which China, Russia and Iran would continue gathering information on sensitive technologies from the United States and its companies.====An important objective of these information-gathering activities, and the case of interest in the present paper, is the participation of firms from the spying country in a particular market. For instance, in the biotechnology sector, the participation of Chinese firms in pharmaceutical markets==== is based on the use of information about drugs from the United States (Lowe, 2011) and to copy them and commercialize generics (Atkinson, 2019), many of which are imported by this country (Palmer and Bermingham, 2019). Another example can be found in the bisphenol-A-free (BPA-free) coating market. As reported by Knox News (2019) and more recently by Bettenhausen (2021) and Goldsberry (2021), information about secret BPA-free technologies from American companies was stolen and passed to a Chinese firm supported by government programs (some of which are suspected of being designed for economic espionage purposes) looking to participate in the BPA-free coating market and compete with American companies.====The governments of advanced economies are concerned about this growing threat (Glitz and Meyersson, 2020, p. 1099) and countering economic espionage has become a priority, with more resources being devoted to establishing measures for the entire country’s economy. These measures can be classified, according to Grabiszewski and Minor (2018, pp. 271–272), into two general categories: ==== and ====. The objective of the first category of measures is to thwart the success of economic espionage activties and includes informing and raising awareness, developing different programs and protocols, and providing tools, training and assistance at company level. The second category focuses on tightening the law, not only directly increasing penalties but also broadening the limits of what is defined as economic espionage.====In this regard, Spain has its own plans and programs against economic espionage (Vilas-Rodríguez, 2017), and also recently passed the Ley 1/2019 de Secretos Empresariales which reinforces the protection against all types of espionage. United States, the paradigmatic example of an economic espionage target, increased its efforts to protect national industries with the 2012 Foreign and Economic Espionage Penalty Enhancement Act. Moreover, “countering espionage has been designated the FBI’s second highest priority, ranked only behind countering terrorism” (Overfield, 2016). In this sense, this intelligence and security service has defined a broad program to counter economic espionage.====The present paper advances the theoretical analysis of the interaction between economic espionage and counter-espionage based on ====. Focusing on the case of interest mentioned above, the main objective of this analysis is the most elemental effect of the level of competition in a particular spied market on the dynamics of this interaction. Given that counter-espionage measures are established for the entire country’s economy (in which different markets are interrelated), the general theoretical framework for the analysis assumes that the economy of the country countering espionage consists of only one market, although it is open to international trade.====In this study, the product commercialized in this international market is assumed to be initially supplied only by firms from this country (denoted by ====),==== while other firms from a foreign country (denoted by ====) are interested in participating in the market, supplying the product from their country. However, a secret technology only known by ====’s firms is needed to participate in this market. In this context, the government of ====, taking into account the welfare related to the participation of its economic agents in the market, decides what effort to exert in espionage activities to attempt to acquire the information necessary to replicate this secret technology. This decision is made considering the cost of the espionage activities and the probability of success in acquiring such information, as this also depends on the counter-espionage policy that might be deployed by the government of ====.====The government of ====, considering this possibility of economic espionage and knowing that (given the characteristics of the international market and the number of ====’s firms willing to participate in it) such participation is harmful for the aggregate welfare of ====’s market participants, decides the level of effort to exert in deploying a counter-espionage policy focused on ====. Weighing up the potential loss in welfare and the cost of this effort, the aim of ====’s counter-espionage policy is to thwart the foreign attempts at obtaining the necessary information to replicate the secret technology needed to participate in the market. Therefore, this policy is considered in its role as barrier to entry with the aim of protecting the welfare generated by the market to ====’s participants. In this context, ==== decides the counter-espionage effort taking into account that the failure of these attempts also depends on the effort exerted in the espionage activities. Nevertheless, this espionage effort is unobservable to ====, similarly as the counter-espionge effort is unobservable to ====.====As mentioned above, the analysis carried out in the present paper, as a first approximation towards studying the rationale behind the influence of the level of market competition on the dynamics of the interaction between espionage and counter-espionage, aims to highlight the most elemental aspects of this influence. This aim has two important implications. The general theoretical framework outlined above focuses on the simplest possible characterization of the international market and the aspects related to the product value chain such that the higher the initial level of market competition (that is, the larger the initial number of ====’s firms in the market), the higher the aggregate welfare of all the market participants. However, despite this simplicity, this general theoretical framework still implies some complexities which would obscure the aim of the paper, such as the existence of cases in which competition might not have such a positive effect on the aggregate welfare of ====’s market participants and the relatively high number of parameters.====So, although the paper develops in some depth the general theoretical framework, to avoid these complexities the analysis focuses on a benchmark case in which there is only one firm from ==== interested in participating in the international market and the product commercialized is only demanded by ====’s consumers. In this benchmark case, the initial level of market competition always has not only a positive effect on the aggregate welfare of ====’s market participants,==== but also a negative one on the welfare generated by the participation of ====’s firm in it. These effects are behind the results obtained which, despite avoiding the complexities mentioned above, suggest that the most elemental influence of the initial level of market competition is characterized by a non-trivial relationship between espionage and this level of competition.====More precisely, considering that both ==== and ==== are sufficiently efficient in exerting their respective efforts, these results show two basic patterns in their behavior. First, ==== is always willing to increase the effort in its counter-espionage policy the higher ====’s effort espionage, and ==== is always prepared to decrease this effort the higher the effort exerted by ====. Namely, ==== regards every espionage effort as a strategic complement while ==== regards every counter-espionage effort as a strategic substitute, and therefore, there is a strategic asymmetry (Tombak, 2006) between ==== and ====. And second, both ==== and ==== will exert smaller efforts in their respective espionage and counter-espionage activities given the effort of the rival the higher the initial level of competition in the market. In the case of ====, the profits its firm can obtain from participating in the market decrease with this level of competition. In the case of ====, the higher the initial level of competition, the smaller the loss in welfare of its domestic participants implied by the participation of ====’s firm in the market.====These reductions in efforts, together with the strategic asymmetry between ==== and ====, imply that, on the one hand, a higher initial level of market competition always has a negative effect on the equilibrium counter-espionage effort and, on the other hand, the effect of this higher level of competition on the equilibrium espionage effort depends on the relationship between both reductions, which is influenced by the characteristics of market demand. In particular, an elastic demand would enhance the positive effect of a higher initial level competition on the aggregate welfare of ====’s market participants, given that the decrease in prices would lead to a proportionally higher increase in sales. An elastic demand would also weaken the negative effect of this higher competition on the profits that ====’s firm would obtain from participating in the market. Similar positive effects of market’s willingness to pay come through in its influence in when the demand can be considered as sufficiently elastic.====This implies that if the demand were sufficiently elastic (inelastic), the reduction in ====’s counter-espionage effort due to a higher initial level of competition would be high (small) relative to ====’s reduction. Consequently, given the strategic asymmetry between ==== and ====, the latter would be better off increasing (decreasing) its espionage effort and, as a result, an increase in the initial level of market competition would have a positive (negative) effect on the equilibrium espionage effort. However, there are two crucial aspects related to this influence of elasticity of demand. Firstly, considering the demand as sufficiently elastic/inelastic in this sense also depends on the pair of initial number of firms in the market defining the increase in the initial level of competition. Secondly, there exists a sufficiently high initial number of firms in the market such that, no matter how elastic the demand is, the reduction in ====’s effort due to an increase in the initial level of competition is always small relative to ====’s reduction. Namely, there exists a critical initial number of firms from which the equilibrium espionage effort decreases with the initial level of competition.====This role played by price elasticity of demand, influenced by market’s willingness to pay and contingent to the initial number of firms in the market, is behind the complex dynamics of the equilibrium espionage effort under variations in the initial level of market competition. According to this role, the smaller the elasticity of demand, the fewer pairs of initial number of firms in the market exist with respect to which the demand can be considered sufficiently elastic and, therefore, the above-mentioned critical number of firms decreases. If the elasticity of demand is low enough, the equilibrium espionage effort decreases with the initial level of competition regardless of the initial number of firms competing in the market. However, when the elasticity of demand increases, the critical number of firms becomes larger and the behavior of the equilibrium espionage effort, for competitive intensities lower than the one defined by this critical number of firms, does not necessarily decrease. Furthermore, it strictly increases when elasticity of demand is sufficiently high.====The remainder of the paper is organized as follows. Section 2 presents the related literature and Section 3 sets out the general model. Section 4 develops this general model and analyzes the equilibrium of the benchmark case. Espionage and counter-espionage efforts in the equilibrium of the benchmark case are discussed in Section 5, while Section 6 is devoted to analyzing how the level of market competition affects these efforts. Finally, Section 7 concludes the paper and suggests several lines for future research.",Market must be defended: The role of counter-espionage policy in protecting domestic market welfare,https://www.sciencedirect.com/science/article/pii/S0167624522000014,8 January 2022,2022,Research Article,25.0
Mothobi Onkokame,"Department of Economics, Faculty of Social Science, University of Botswana, 4775 Notwane Road, Gaborone, Botswana,Research ICT Africa, Workshop 17, 17 Dock Road, V&A Water Front, Cape Town, South Africa","Received 10 March 2021, Revised 27 November 2021, Accepted 16 December 2021, Available online 18 December 2021, Version of Record 8 March 2022.",https://doi.org/10.1016/j.infoecopol.2021.100963,Cited by (1),0.27.,"Switching costs and mobile termination rates (MTRs) are the focal point of many telecommunication regulatory policies and antitrust cases.==== Switching costs bias consumers’ choices towards previously selected products and services. This, in turn, reduces their responsiveness to price and allows firms to charge higher prices. In an effort to reduce switching costs in mobile telecommunications markets, many regulatory authorities worldwide introduced mobile number portability (MNP), which allows consumers to take their mobile phone numbers with them when changing to a different mobile operator.====On the other hand, MTRs refer to charges which are set by mobile operators for terminating calls on each others’ networks. Although the MTRs have a direct impact on mobile retail prices, they are not observed by the consumers who make subscription decisions without taking them into consideration. Therefore, each network is a de facto monopoly for termination of calls, which can be a source of collusion.==== The regulatory authorities generally recognize this fact and intervene by regulating MTRs.====In spite of the importance of MNP and MTRs, there is a very short body of literature which provides an assessment of the effect which these policies have on prices and competition in low income countries. Among the few studies, Mothobi (2020) estimates the effect of MNP on own-and- cross price elasticities. Using quarterly data for 27 mobile operators in Sub Saharan Africa countries, he finds that MNP increases price elasticities of demand for mobile services. The paucity in this literature is largely due to the scarcity of data on the telecommunication market in these countries. Our study contributes to the literature by examining the effect of MNP and MTRs on pre-paid mobile phone service prices in Sub-Saharan African countries. Our approach is similar to Parker and Röller (1997) and Grzybowski (2005) who, assuming that mobile services are homogenous products, applied a static Cournot model to study competition in mobile telecommunication market. In particular, Grzybowski (2005) analyzes the impact of MNP on mobile retail prices for a number of European countries. However, in the estimation, he does not control for country-specific MTRs as a determinant of marginal costs, but instead uses country-specific cost dummies to take into account differences in marginal costs between countries. In this study, we control for differences in marginal costs in terms of country-specific MTRs.====We estimate a structural model of demand and supply using quarterly time series data between 2010:Q4 and 2014:Q4 for eight African countries. The data was constructed by aggregation of firm level information for 35 mobile operators which are active in these countries.====On the supply side, we find that MTRs have a significant and positive impact on mobile retail prices. On average a 10% increase (decrease) in MTRs will result in a 2.5% increase (fall) in prices. This result opposes the waterbed effect theory, which was tested for telecommunications markets by Genakos and Valletti (2011). The waterbed effect theory suggests that in two-sided markets when prices in one market are pushed down by regulatory controls, the prices in the unregulated market will increase towards monopoly prices. This holds when demand or marginal costs are interdependent; firms use non-linear pricing or there is a zero-profit constraint (see Schiff, 2008). Thus, pushing down the price in the regulated market, in other words, the termination rate, does not increase unregulated mobile retail prices in the group of countries used in this analysis. Our result supports the glide path termination rate policy. A glide path in termination rate refers to regulated price control where regulators mandate operators to reduce termination rate charges over time rather than an immediate move to to the cost-oriented level. This allows operators time to plan for the decreased revenue from mobile termination charges. This policy is expected to offer stability as compared to a one-off shock if the difference between the existing MTRs and the cost-orientated MTRs is great.==== Moreover, we do not find that MNP has a significant negative impact on retail prices for the selected African countries, which contrast with the results found by Grzybowski (2005), Park (2011) and Cho et al. (2013) for European countries. This may be due to less effective implementation of MNP in African countries and consequently lower attractiveness and take up of this option by consumers. For instance, even though it has been found that the effectiveness of MNP depends on porting time and charges, the porting process in Africa is characterized by long porting time. Furthermore, in some countries such as Nigeria, subscribers are not allowed to port again for the next three months.====On the demand side, we find that MNP does not change the responsiveness of consumers to price, a result which coincides with our findings on the supply side. This may be due to the fact that in many African countries, it is common to use multiple subscriber identity module (SIM) cards.==== A household survey, conducted by ResearchICTAfrica in different African countries in 2008, reports that 36.3% of adult mobile phone subscribers hold more than one SIM card in Benin, 25.8% in Kenya and only 2.9% in Mozambique. Hence, many consumers are connected to two or more operators with low demand for porting numbers. We estimate the price elasticity of demand to be on average ====0.27. We use the estimate of price elasticity to approximate the average market conduct parameter in the selected African countries, which takes value of 1.29.====The remainder of this paper is as follows. Section 2 discusses theoretical and empirical literature on MNP and MTRs. Section 3 provides an overview of MNP, regulation and termination rates. Section 4 presents our data. Section 5 introduces the empirical model. Section 7 presents estimation results and finally Section 8 concludes.",The impact of telecommunication regulatory policy on mobile retail price in Sub-Saharan African countries,https://www.sciencedirect.com/science/article/pii/S0167624521000512,18 December 2021,2021,Research Article,26.0
"Humbert Marc,Lambin Xavier,Villard Eric","Grenoble Ecole de Management, 12 rue Pierre Semard, Grenoble F-38000, France,ESSEC Business School and THEMA, 3 Av. Bernard Hirsch, B.P. – 50105, Cergy 95021, France","Received 11 September 2020, Revised 13 September 2021, Accepted 26 November 2021, Available online 4 December 2021, Version of Record 8 March 2022.",https://doi.org/10.1016/j.infoecopol.2021.100959,Cited by (0),"During the COVID-19 sanitary crisis, many exams were hastily moved to online mode. This revived a much needed debate over the privacy issues associated with online proctoring of exams, while the validity and fairness of unproctored exams were increasingly questioned. With a randomized control trial, we estimate the effectiveness of prior warnings as a means of discouraging academic dishonesty in exams. We use original, non-intrusive technologies to surreptitiously identify cheating in a series of unproctored assignments and send a targeted warning to half of the students who were identified as cheaters. We then compare their cheating behavior on the final exam with the behavior of the group of unwarned cheaters. The warning proves effective but does not completely eliminate cheating, as some students’ cheating strategies become more sophisticated following issuance of the warnings. We conclude that switching traditional exams to online mode should be accompanied by proctoring. When proctoring is not possible, credible and effective anti-cheating technologies should be deployed together with adequate warnings.","Online education has experienced sustained growth in recent decades. The 2020 global sanitary crisis caused by the COVID-19 virus suddenly rendered remote learning ubiquitous and paved the way for even more extensive use in the future. Naturally, these abrupt developments stimulated active debates over the benefits of online teaching and the associated risks, in particular the issue of academic dishonesty on distance exams. The stakes go beyond the already crucial issue of fairness in education, as several authors have noted a strong correlation between academic and professional dishonesty (====; ====).====Following the observation that unproctored online exams result in extensive cheating (====; ====), several strategies have been proposed. The randomization of questions (see e.g. (====)), when implementable, provides satisfying results but raises issues of fairness between students facing distinct sets of questions. This process also has technical limits, because an examiner may not be able to find enough variations of a given question to avoid repetition. Online proctoring is also a popular solution (====, ====, ====, ====, ====, ====) but faces strong public opposition reflecting concerns over students’ access to the necessary technologies (such as a webcam or a stable internet connection) and, most importantly, privacy.====We took advantage of the specific conditions under which online exams were administered during the COVID-19 crisis, where cheating was predictably widespread, to develop a new strategy to discourage cheating. The strategy, based on friendly warnings sent to students, is respectful of student privacy and does not require specific equipment. In this research, we seek an answer to the following research question “can targeted warnings discourage students from cheating on exams?”. We restrict our analysis to cheating in the form of illicit sharing of information between students.==== To that aim, we surreptitiously analyze similarities between copies of pre-exam assignments and various trick questions to identify (illicit) collaborations. Students are not aware of these techniques, which makes very accurate identification of cheaters and their strategies possible.==== To a randomly selected subgroup of students identified as cheaters on the assignments, we send a friendly warning stating that their copies were suspicious and reminding them that cheating on the final exam is prohibited. Mean comparisons indicate that only 9% of the members of our treated group (assignment-cheaters who received a warning) cheated on the final exam, down from 40% in the control group (unwarned assignment-cheaters). Controlling for various variables, the warning sent to cheaters decreases the cheating rate by 23 percentage points. By design of the experiment, this effect is causal. A warned cheater has a probability to cheat on the final exam 4 percentage points lower than a similar student who was not identified as assignment-cheater. We conclude that warnings are effective in discouraging cheating, insofar as warned cheaters behave similarly to non-cheaters. Cheating is, however, not eliminated entirely. We observe a similar effect when we restrict the analysis to “leaders” (students who gave their exam copies to other students). This effect is, however, subject to measurement errors, and properly testing leadership status would require a larger dataset to gain significance.====To the best of our knowledge the present paper is the first to exploit observational methods to analyze the dynamics of cheating in a series of distance exams. It is the first to analyze how suspected students respond to a targeted nudge warning. The experiment was pre-registered at the AEA's Social Science Registry.==== The rest of the paper is organized as follows. ==== reviews the literature. In ==== we describe our experimental setting. In ==== we report our results. ==== provides a discussion of our results and we conclude in ====.",The role of prior warnings when cheating is easy and punishment is credible,https://www.sciencedirect.com/science/article/pii/S0167624521000470,4 December 2021,2021,Research Article,27.0
"Mueller-Langer Frank,Gómez-Herrera Estrella","University of the Bundeswehr Munich, Neubiberg, Germany,Max Planck Institute for Innovation and Competition, Munich, Germany,University of the Balearic Islands, Palma de Mallorca, Spain","Received 17 December 2020, Revised 21 September 2021, Accepted 14 October 2021, Available online 16 October 2021, Version of Record 8 March 2022.",https://doi.org/10.1016/j.infoecopol.2021.100951,Cited by (2),"Intensified by the COVID-19 pandemic, online labour markets are at the core of the economic and policy debate about the future of work and the conditions under which we work online. We analyse the effects of an increase in the cost of on-site work induced by COVID-19-related mobility restrictions on the substitution between on-site and remote job postings and between on-site and remote hires. We benefit from the fact that the implementation of stay-at-home requirements varies by country, time and level. We use unique company data from a large European online labour market. We provide empirical evidence for a positive effect of stay-at-home restrictions on job postings and hires of remote work relative to on-site work. Overall, our results suggest that employers are substituting remote employment for on-site employment, while there is no substantial change in overall employment.","Lockdown measures and mobility restrictions that were implemented due to the COVID-19 pandemic have challenged the labour market, transforming working patterns and working conditions. Recent empirical evidence suggests that there has been a substantial substitution between on-site (in-person) work and remote (home office) work during the COVID-19 pandemic (Adams-Prassl et al., 2020a; Bartik et al., 2020, Bartik et al., 2020a; Brynjolfsson et al., 2020). In addition, COVID-19-induced mobility restrictions have increased the cost of working on-site relative to working from home (Bick et al., 2020). These mobility restrictions decreased the probability of workers to remain employed if their jobs are less likely to be performed remotely (Borjas and Cassidy, 2020). Recent works also find large negative effects of the COVID-19 pandemic on total employment (Adams-Prassl et al., 2020a, Adams-Prassl et al., 2020a; Brinca et al., 2020; Brodeur et al., 2021; Brynjolfsson et al., 2020; Del Rio-Chanona et al., 2020; International Monetary Fund 2020a, International Monetary Fund 2020b; Papanikolaou and Schmidt, 2020).====In parallel, online labour markets (henceforth, OLMs) have recently gained substantial importance (Farrell and Greig, 2016; Pesole et al., 2018; Kässi and Ledhonvirta, 2018). OLMs are digital marketplaces that allow clients and freelancers to communicate, hire, and work remotely. They allow market participants to solve information problems and gain from trade in remote labour services.==== OLMs are a growing field of research at the interface of platform economics and labour economics (Agrawal et al., 2013; Autor, 2001, 2013; Chen and Horton, 2016; Dube et al., 2020; Goldfarb and Tucker, 2019; Horton and Tambe, 2015).==== Intensified by the COVID-19 pandemic, OLMs are at the heart of the economic and policy debate about the future of work and the conditions under which we work online (Acemoglu and Autor, 2010; Balliester and Elshekhi, 2018; Berg, 2016; Berg et al., 2018; Codagnone et al., 2016; European Commission 2016a, European Commission 2016b; Gonzalez-Vazquez et al., 2019; Von der Leyen, 2019a, Von der Leyen, 2019b).====Using the unique company data of a large European OLM, we analyse the effects of an increase in the cost of on-site work induced by COVID-19-related mobility restrictions on the substitution between on-site and remote job postings and between on-site and remote hires. We define “remote work” as projects that can be done remotely, e.g., online from the home office. In contrast, “on-site work” is constituted by projects that should be done on-site, e.g., in-person on the clients’ premises.====Fig. 1 provides an overview of the timing of interactions on the OLM under study.====The timing of interactions is as follows. First, the client chooses whether a given project should be performed on-site or remotely, e.g., at the client's offices or via online work in the home office, and chooses eligible freelancers==== to which she sends a project request (henceforth, also referred to as requests). Then, the contacted freelancers choose whether to reply to a project request by sending quotes to the client, i.e., the price they charge to complete the project. Finally, the client chooses whether to start the project by accepting a quote. We divide our empirical analysis into three parts according to the timing of interactions: requests, quotes and hires.====Fig. 2 illustrates the weekly number of requests by project type (remote vs. on-site) and year (2019 and 2020). The two upper solid lines indicate the number of requests for remote projects (2020: red, 2019: blue). The two lower dashed lines indicate the number of requests for on-site projects (as before, 2020: red, 2019: blue). The vertical red line indicates 15 March 2020 in week 11, i.e., the day when most European countries started to implement their stay-at-home requirements.====Fig. 2 suggests that there was no positive trend in the number of requests for remote projects before 15 March 2020 but a substantial increase afterwards.==== It also suggests that the number of requests for on-site projects decreased after 15 March 2020. These findings contrast the results for 2019 where the numbers for both types of requests remain relatively stable over time.====Analysing the effects of stay-at-home requirements on requests, quotes and hires, we obtain the following results. First, we find evidence for a positive effect of stay-at-home requirements on the requests for remote work relative to on-site work. Our results suggest that the share of clients’ requests for remote projects amongst all requests increased by between 1.7 and 3.9 percentage points with the implementation or increase in intensity of the stay-at-home requirements. Hence, clients send relatively more requests for remote work and relatively fewer on-site requests. This is our main result. Second, conditional on these requests, we find no robust empirical evidence for a positive overall effect of stay-at-home requirements on the quotes for remote work relative to on-site work. As a consequence of these results, we find that the share of remote jobs amongst all hires increases with stay-at-home orders. Overall, our results suggest that employers are substituting remote employment for on-site employment, while there is no change in overall employment.====To the best of our knowledge, this paper is one of the first attempts to provide robust empirical evidence at the project level on the substitution between on-site (in-person) work vs. remote (home office) work in a setting that is already conducive to remote work, i.e., online labour markets.==== Our analysis benefits from the fact that the substitution is induced by an increase in the cost of working on-site due to the implementation of COVID-19-related mobility restrictions.====The remainder of the paper is organised as follows. Section 2 provides an overview of the related literature. In Section 3, we describe the data and empirical methodology. Section 4 provides an analysis of the substitution effects of stay-at-home requirements on-site work vs. remote work. In Section 5, we provide additional results. Section 6 discusses policy implications and conclusions.",Mobility restrictions and the substitution between on-site and remote work: Empirical evidence from a European online labour market,https://www.sciencedirect.com/science/article/pii/S0167624521000391,16 October 2021,2021,Research Article,29.0
"Choi Jay Pil,Yang Sangwoo","Department of Economics, Michigan State University, 220A Marshall-Adams Hall, East Lansing, MI 48824 -1038, United States,School of Economics, Yonsei University, Seoul, Korea,Graduate School of Economics, Yonsei University, 50 Yonsei-ro, Seodaemun-gu, Seoul, 03722, Korea","Received 22 August 2020, Revised 11 August 2021, Accepted 23 August 2021, Available online 25 August 2021, Version of Record 1 December 2021.",https://doi.org/10.1016/j.infoecopol.2021.100942,Cited by (2),"This paper develops a model of investigative journalism and media capture in the market for news with the depreciation of the value of news over time and a limited exclusive supply period of original news due to copying by other media outlets. We make distinctions between traditional media outlets that engage in investigative journalism and fringe digital media that mainly copy and spread original news created elsewhere. We show that the quantity and quality of news with investigative journalism decrease and media capture is more likely as digital technologies induce a lower fixed cost of entry for the fringe firms and a shorter exclusive supply period of news. These results may explain why there is scant evidence for the conventional view that more media outlets lead to higher quality news and less political capture, despite proliferation of news and information outlets in the digital age.","Media capture can be defined as a phenomenon in which “government or vested interests networked with politics” Mungiu-Pippidi (2012) or “the rich, special interest groups, political parties, governments, or any actors other than consumers” Petrova (2008a) violate media independence.==== In many democratic societies that have transitioned from authoritarian regime and military dictatorship, economic means can easily fill in the vacuum left by physical force. The harmful effects of media capture on society are obvious. Media capture violates the democratic system of checks and balances (Stiglitz, 2017), distorts collective decisions (Corneo, 2006), and deepens the social inequality of wealth (Petrova, 2008b).====However, with the rapid development of ICT (Information and Communications Technologies) and the emergence of the digital-online news media in the late 20th century, many scholars and journalists expected that media capture will reduce dramatically in the near future. Even Mitchell Stephens, a news media historian who thought that the 20th century was the most organized, effective and brutal era of media capture, expected that the development of ICT would allow news media to provide “more reliable information” as well as “much more information” to the masses (Stephens, 2007). Foster (2012) also argued that digital media could help users find many more sources of news than ever before and it may support high-quality news over time.====Economic research has also contributed to strengthening these sanguine views on the positive prospect of more sources of news in mitigating political capture. In a classic paper on political capture, Besley and Prat (2006) have shown that “more outlets/competition” would decrease media capture. Petrova (2008b) goes further and argues that when the alternative source of information becomes more accessible, then the probability of media capture decreases, so that the Internet has a stronger positive effect on media freedom. In fact, these expectations and beliefs are deeply rooted and go back as far as John Milton’s thought on “the open free marketplace of ideas” and “the self-righting process of truth.”==== More information sources and media competition thus have been a golden rule for most scholars and journalists attempting to find solutions against media capture so far.====Nonetheless, there is scant evidence that such expectations have been met and media capture has decreased with much more competition/outlets in the digital-online news age. In the past few decades, the number of media outlets in the world has skyrocketed largely due to entry by fringe digital media outlets. If we include popular online influencers in social media such as Facebook, Twitter, and YouTube as alternative sources of news, the total number of available information sources becomes even larger. Despite the existence of numerous alternative media outlets, we face a “shortage of local, professional, accountability reporting” which leads to the lack of accountability with “more government waste, more local corruption, less effective schools, and other serious community problems” (Wildman, 2011).====To explain this phenomenon of media abundance along with the lack of reporting that would inform the public through investigative journalism, we develop a model of political capture that reflects the realities of the media market in the digital age. In particular, the digital technology enables easy copying of rivals’ original content news. In this regard, Cagé (2016) observes that when news is repeated and replicated indefinitely, despite the huge cost of producing (original) news, and most of news are provided free of charge, the news media market staggers toward death. According to a study by the Pew Research Center's Project for Excellence in Journalism (2010) for Baltimore, for instance, the city had a proliferation of news and information outlets with a total of 53 different outlets ranging from blogs to talk radio to news sites created by former journalists. However, 95 percent of the stories were based on reporting done by mostly the Baltimore Sun, the traditional dominant media in the city. At the same time, the proliferation of media outlets has led to unprecedented and fierce competition, followed by a dramatic decrease in readership and income of legacy media. The Baltimore Sun produced 32% fewer stories on any subject in 2009 than it did in 1999, and 73% fewer stories than in 1991.==== As aptly expressed by Wildman (2011), “Abundance of Voices Does Not Necessarily Mean Abundance of Journalism.” In terms of newspaper revenues, a study on the media landscape in the US by the Federal Communications Commission (Wildman, 2011) reports that newspaper advertising revenue dropped 47 percent from 2005 to 2009 and daily newspapers cut their annual editorial spending by $1.6 billion per year, or more than a quarter between 2006 and 2009 with some major newspapers seeing half their staffs disappear in a matter of a few years, thanks to the emergence of the digital media enabled by the Internet.====We thus make a distinction between two types of the media in contrast to the most existing work that treats all media symmetrically when analyzing the effect of competition on political capture. One is traditional media with a budget to hire staff members to engage in investigative journalism and produce original content. The other is fringe firms of new digital media that mainly copy and share the news as opposed to carrying the heavier burden of reporting original news. With the emergence of the digital media along with massive reductions in the traditional news media revenue, we envision a situation in which the market can support only very few traditional news outlets.====To explore implications of the digital media in this new environment, we model the media market as competition for “attention” in which ad-financed media firms generate revenues by selling eyeballs to advertisers.==== We assume a market structure with one traditional media firm and fringe digital media firms whose number is endogenously determined by the zero-profit condition. The market for news is characterized by the depreciation of the value of news over time and a limited exclusive supply period of original news due to copying by other media outlets. In particular, our model explicitly considers the free-rider problem due to the rapid spillover of information and its effects on the incentives to invest in investigative journalism and political capture.====More specifically, we develop a model with three types of news: costly “high quality” news (====) that requires resources, news on political scandal (====) that can be investigated with a cost, but may not be reported due to potential political capture, and mundane “low quality” news with a minimal cost. The quality of ==== news depends on the amount resources devoted to it and the decision to engage in costly investigation on political scandal depends on the credibility of a lead that is stochastically received by the media firms. In the main model (sections 2 and 3) we assume that the media firm has enough resources and the decision on each type of news can be analyzed independently. In such a model, we analyze the media firms’ incentives to produce high quality news with investigative journalism and incentives to publish political news in the presence of political capture. We show that the possibility of political capture can enhance incentives to investigate political scandal, but the outcome of investigations is never reported.====A comparative static analysis in section 4 constitutes our key contribution, which is to explore implications of digital technologies for investigative journalism and political capture. We show that the quality of news with investigative journalism decrease and media capture is more likely as digital technologies induce a lower fixed cost of entry for the fringe firms and a shorter exclusive supply period of news, which enable more fringe media outlets to enter the market. We use these results to explain why there is scant evidence for the conventional view that more media outlets lead to higher quality news and less political capture, despite proliferation of news and information outlets in the digital age. It has been well documented that traditional media has been suffering in revenue in the digital age and media capture is a possibility. However, the possible connection between these two have not been noticed in the literature and we are the first to identify the revenue loss of the traditional media as a potential cause for more political capture in the digital age.====We also consider an extended model (section 5) in which the media firm has a budget constraint that would limit its ability to fully pursue both ==== and ==== news. In the presence of budget constraints, we uncover another channel through which the quality of ==== news can suffer with the possibility of political capture. When the exclusivity period is reduced with the emergence of the fringe digital media, political capture becomes more likely and incentives to pursue news ==== increases. With limited resources, this indirect channel can lead to a further erosion of the quality of other high quality news. In addition, the outcomes of investigations (==== news) are never published due to political capture. The overall outcome is abundance of low quality news at the expense of ==== and ==== news.====The seminal paper in the economics of media capture is Besley and Prat (2006). They show that more media outlets and the resulting competition increase the capture cost for the media capturer, thereby reducing the opportunity for media capture. Unlike them, this paper explains that more outlets/competition can increase capture under the assumption that only one (or a very limited number of) media outlet(s) can report the news damaging the capturer, which is more realistic in the digital age than the assumption employed by Besley and Prat (2006) that all media outlets can. Moreover, this paper assumes that the revenue of media, which is the basis for the cost of capture, depends on the exclusive supply period of news. Thereby, more outlets can increase the likelihood of media capture, overturning the result in Besley and Prat (2006).====While many scholars such as Gentzkow and Shapiro (2008), Prat and Strömberg (2013), Petrova (2008b), and Prat (2015) have argued along the lines of Besley and Prat (2006)’s view, there are very few papers contrary to it. In recent years, however, the views arguing that competition can facilitate media capture have gradually emerged. Vaidya and Gupta (2016), for instance, analyze the effect of media market competition on political capture by comparing two alternative market structures of monopoly and duopoly. Contrary to Besley and Prat who assume that the media have verifiable evidence, they consider a situation in which the information available to the media is not incontrovertible. This allows “information manipulation” where a captured firm can be used as a tool to discredit allegations raised by an independent media firm. In such a scenario, Vaidya and Gupta (2016) show the possibility that competition in the media market can facilitate political capture in that media is captured at lower bribes under duopoly relative to monopoly. Our paper maintains the assumption of verifiable evidence in Besley and Prat (2006) and rely on a different mechanism by which competition can facilitate political capture.====(Trombetta and Rossignoli, 2020) argue that competition has two effects in opposite directions in media capture. As in Besley and Prat (2006), more outlets increase the capture cost for the politician as there are more parties to silence with bribes. Another effect is that the competitive pressure decreases media firms’ profits, which makes firms with smaller financial margins more susceptible political capture. He shows that more competition can actually harm the media’s independence from political influence and the relationship between media competition and political capture can be non-monotonic with the interplay of the two opposing effects.====The rest of the paper is organized as follows. Section 2 describes the model that explicitly takes account for incentives to engage in investigative journalism to produce high quality news and to follow up on potential leads to uncover political corruption. In section 3, we solve the model to fully characterize the equilibrium. In particular, we analyze how the changes in the likelihood of media capture can be linked to the revenue associated with a report on political accountability. In section 4, we perform comparative statics analysis on how changes in underlying model parameters impact the revenues associated with each type of news, and thus affect the quality of ==== news and the likelihood of political capture. An extended model with the media firm’s resource constraint is analyzed in section 5. Section 6 discusses policy implications of the model. Section 7 contains concluding remarks.",Investigative journalism and media capture in the digital age,https://www.sciencedirect.com/science/article/pii/S0167624521000305,25 August 2021,2021,Research Article,33.0
"Lam Chungsang Tom,Liu Meng,Hui Xiang","Department of Economics，320L Wilbur O. and Ann Powers Hall， Clemson, SC, 29634, USA,Olin Business School 1 Brookings Drive, Knight Hall 406St. Louis, MO 63130, USA","Received 7 January 2021, Revised 29 June 2021, Accepted 25 July 2021, Available online 29 July 2021, Version of Record 1 December 2021.",https://doi.org/10.1016/j.infoecopol.2021.100941,Cited by (8),"Despite the popularity of ridesharing, there is limited empirical evidence on how ridesharing activities differ across regions with different levels of accessibility and the implication for consumers. In this paper, we study the market for rides across New York City neighborhoods. We construct a novel data set that contains massive API queries on route-specific estimates of pricing, wait time, and ==== of Uber, Lyft, and the public transit. After linking this data with actual trip records of taxis, Uber, and Lyft, we document a strong pattern that ridesharing has a larger market share relative to taxis in neighborhoods with lower accessibility, defined either in terms of geographic distance to Midtown Manhattan or “economic distance” to job opportunities. Next, we estimate a discrete-choice model of demand for rides and interpret the geography of ridesharing through the lens of the model. We find that ==== from ridesharing varies drastically across geography: passengers that are 5 to 15 miles (resp. more than 15 miles) from Midtown experience a 60% (resp. 19%) larger ==== relative to passengers that are within 5 miles from Midtown. Over half of these gains comes from reduced wait time.","Ridesharing platforms have gained popularity around the world in recent years and, alongside, scrutiny from policy makers. Policies and regulations about ridesharing across cities seem to be predominantly a binary decision: either ridesharing is allowed or it is banned. However, when policy makers ignore the potential distributional impacts of ridesharing across geographies, a one-size-fits-all policy will benefit some neighborhoods while being detrimental to others. Studying the geography of ridesharing and its implication for consumers helps us understand ==== ridesharing could benefit consumers, therefore creating opportunities for designing nuanced policies tailored to different geographies.====This paper studies how ridesharing affects consumer welfare across neighborhoods in a metropolitan setting. We aim to answer two research questions: First, how do ridesharing activities differ across neighborhoods with different levels of accessibility? Second, how might this geography translate into consumer welfare in different neighborhoods? To answer these questions, we construct a comprehensive data set that describes the market of rides in New York City (NYC hereafter). Specifically, we gather massive API queries of Uber, Lyft, and the NYC public transit, which include their pricing, wait time, and travel time. We link this data with actual trip records of taxis, Uber, and Lyft published by NYC Taxi and Limousine Commission (TLC hereafter). We further enrich the data by conducting a field collection of around 70,000 historical trip records from a sample of Uber and Lyft drivers. The resulting data set provides a comprehensive picture of the market that allows for analysis at the route level in real time.====We first summarize and visualize geographic patterns in ridesharing and taxi rides. We find that for both ridesharing and taxis, the total number of pickups is smaller in less accessible regions, where accessibility is defined either in terms of geographic distance to the city center or “economic distance”. The geographic distance is calculated as the distance between the pickup location and Manhattan Midtown, and economic distance is measured by the neighborhood’s total number of jobs accessible within one hour’s commute by public transit (Kaufman et al., 2014). However, the rate at which the number of pickups decreases with respect to distance, which we will refer to as the trip elasticity of distance henceforth, is smaller than that of taxis. Consequently, the market share of ridesharing relative to that of taxi is higher in low accessibility neighborhoods under both definitions of distance. In addition, we further highlight the geography of ridesharing by using green boro taxis as a benchmark, which follow the same pricing rule as yellow medallion taxis but are allowed to pick up passengers only outside Manhattan Core====, i.e., predominantly low accessibility neighborhoods. We find that the market share of ridesharing remains larger than that of green taxis in low accessibility neighborhoods, despite that green taxis are designed to serve low accessibility neighborhoods.====The geographical distribution of market shares is an equilibrium outcome, driven by both consumer preference toward various transportation modes and drivers’ decisions on when and where to provide the service. We provide empirical evidence that is consistent with a high degree of matching friction between taxi drivers and passengers, which likely gives rise to the low presence of taxi pickups in low accessibility neighborhoods. First, we show that low accessibility neighborhoods more frequently exhibit excess demand (the incidence of more people needing a ride than the number of available drivers in the area) instead of insufficient demand for taxis. Second, the trip elasticity of distance is larger for yellow taxis than for green taxis in low accessibility regions, even though both taxi services face the same demand due to their identical pricing rules outside of the Manhattan Core. This difference suggests that the matching friction in low accessibility neighborhoods relative to the Manhattan Core is even higher for yellow taxis than for green taxis.====The matching friction present among taxis may be alleviated by ridesharing through technology-enabled matching, incentives created by dynamic pricing, or both. Ridesharing platforms seem to reduce this information gap in low accessibility neighborhoods mainly through the use of advanced matching algorithms: According to our data, surge pricing is rare in low accessibility neighborhoods, although it is more frequently activated in Manhattan.====The second part of the paper aims at translating the observed geography to consumer welfare, leading us one step closer to answering relevant policy questions. We model consumer’s decision as a discrete choice problem between various competing transportation modes for a given route that is defined by a particular origin-destination-time combination. We use the logit framework to map the observed rides into consumer preference toward price, wait time, as well as other observed and unobserved characteristics. In the demand estimation, endogeneity arises when the price of a region is correlated with the unobserved demand condition in the same region, conditional on service type, route, and time fixed effects. To obtain a consistent estimate of price elasticity, we use an instrumental variable (IV) that affects the market share of a transportation mode on a given route ==== its impact on price, but it is otherwise uncorrelated with local demand shocks. We construct such an IV using a design feature of the ridesharing Apps that were present during our sample period: passengers see and commit to a surge multiplier on Uber and Lyft Apps ==== entering their destination on their phones.==== Exploiting this feature, we use the surge prices of trips into the focal zone to instrument for price in this zone. On the one hand, because these ridesharing Apps do not possess information on where the consumer is going, they cannot adjust their surge multiplier according to the consumer’s destination, which creates the uncorrelatedness of surge prices at the origin and the underlying demand condition at the destination. On the other hand, surge pricing at the origin will affect the price in the focal zone through its effect on the number of available drivers in the focal zone after their drop-offs.====Using this instrument, we identify heterogeneous consumer preference toward price and wait time. The parameter estimates show that consumers prefer lower price and less wait time. Also, the elasticity estimates across regions and different times of the day are consistent with the intuition that people are less price-sensitive and care more about short wait time during rush hours and in destinations with more offices. The fact that consumers prefer less wait time in certain locations during certain times of the day suggests that ridesharing platforms may increase consumer surplus if the service could effectively reduce wait time in these regions.====We use a highly stylized model to characterize taxi supply, where the equilibrium predictions are consistent with the data. Through the lens of the estimated demand and supply models, we translate the geography of ridesharing to the geography of consumer surplus. Conceptually, we adopt the idea of compensating variation and estimate how much consumers should be compensated if Uber and Lyft were to be removed from the market in order to maintain the same level of utility for consumers in different regions. We find that consumer surplus from ridesharing varies drastically across geography: passengers that are 5 to 15 miles (resp. more than 15 miles) from Midtown experience a 60% (resp. 19%) larger consumer surplus relative to passengers that are within 5 miles from Midtown. Additionally, over half of these gains comes from reduced wait time for getting a ride.====The distributional results on the relative market share and the uneven consumer gains from ridesharing have important implications for public policy makers. The results suggest that the impact of ridesharing can be highly uneven across regions with different levels of accessibility. It is important to note that metropolitan areas in the United States exhibit significant geographical disparity in transit access (Tomer, Kneebone, Puentes, Berube, Owen, Murphy), even for New York City, which has the nation’s best public transit system. Its neighborhoods at the 90th percentile in accessibility have access to 18 times more jobs via public transit within an hour than the neighborhoods at the 10th percentile in accessibility (using data provided by Kaufman et al. (2014)). This geographical disparity is unlikely to change in the near future, because it depends on long-run factors such as city planning and re-designing of public transit networks. Taxis are another important component of metropolitan transit systems, yet we show that they are highly concentrated in the dense areas, even for taxis that are specifically designed to pick up passengers in less accessible regions, due to the lack of technology that provides real-time information of demand.====Based on our findings, urban and transportation planners should recognize the important role of ridesharing in connecting neighborhoods with low accessibility and disproportionately create incentives for ridesharing in these neighborhoods, holding the social costs constant. A more inclusive transit network is important because mobility and proximity to markets could affect individuals’ labor market outcome and overall well-being, especially those of disadvantaged demographic groups and individuals without car ownership (e.g., Ihlanfeldt, Sjoquist, 1990, Ihlanfeldt, Sjoquist, 1991, Holzer, Ihlanfeldt, Sjoquist, 1994, Hering, Poncet, 2010, among others).",The geography of ridesharing: A case study on New York City,https://www.sciencedirect.com/science/article/pii/S0167624521000299,29 July 2021,2021,Research Article,34.0
Sun Meng,"Business School, Beijing Normal University, No. 19 Xinjiekouwai Street, Haidian District, Beijing 100875, China","Received 21 May 2020, Revised 10 May 2021, Accepted 29 June 2021, Available online 22 July 2021, Version of Record 1 December 2021.",https://doi.org/10.1016/j.infoecopol.2021.100940,Cited by (6),"This paper analyzes the effect of the development of the Internet on the share of small and medium-sized enterprises (SMEs) in total exports. We extend the Helpman, Melitz and Yeaple (2004) model to include the opportunity for firms to ==== fixed export costs by exporting indirectly via well-established e-commerce platforms. SMEs self-select into the indirect exporting mode. In response to the development of the Internet, fixed costs of indirect exporting fall at a higher rate than fixed costs of direct exporting. Consequently, SMEs tend to account for a larger share in total exports as the Internet develops. Using two samples from the Exporter Dynamics Database, we find supporting evidence that the development of the Internet in the exporting country has a significant and negative effect on the share of exports by the top 5% or 25% of exporters, implying a larger share of SMEs. Moreover, we find that improved submarine cable infrastructure in the exporting country also leads to a lower share of exports by large exporters. These are in contrast to the estimated positive effect of telephone development in the exporting country, which may not disproportionately benefit indirect exporting via e-commerce platforms over direct exporting. Finally, we find an imperfect substitution relationship between e-commerce platforms and traditional intermediaries.","Internet infrastructure nowadays underpins economic and social activity worldwide. Its presence alters the way firms conduct international business by dramatically reducing information frictions, search costs and communication costs and by forming the e-commerce==== landscape. Small and medium-sized enterprises (SMEs) have been well known for their contributions to economic development and social well-being, and better access to global markets is key to strengthening SME contributions (OECD, 2019b). Whether SMEs are given more opportunities in international trade through the spread of Internet technology then becomes an important issue.====How does the development of the Internet affect the share of SME exports in total exports? The removal of trade barriers by virtue of the development of the Internet could stimulate the exports of both large firms and SMEs, but the extent of enhancement may differ. One view is that large firms may become the main beneficiaries by accounting for an even higher share in total exports as the Internet develops. Given large firms’ advantages in scale and scope, they are able to exploit many new opportunities that are out of reach to individual SMEs.====The other view is that the share of SME exports is likely to rise in response to the increasing penetration of the Internet. Advances in digital technologies, differing from other trade facilitation measures such as lower tariffs====, have led to the emergence of online services and e-commerce platforms such as Alibaba, Amazon, Ecplaza, and Tradeindia. Their important intermediary role in international commerce has gained attention, as they lower barriers to starting and operating overseas businesses for SMEs, thereby realizing SMEs widespread participation in the global market (e.g., Kuwayama, 2001; OECD, 2019a; (World Trade Organization WTO, 2016) and offering them a level playing field for competing with their larger competitors (Cho and Tansuhaj, 2013). Although large firms have been early adopters of the Internet (World Bank Group, 2016), the development of the Internet may matter more to the exports of SMEs. Therefore, this is an empirical question to be explored.====To guide the empirical analysis, we propose an adaptation of the Helpman et al., 2004 model by incorporating the firm’s indirect exporting technology via e-commerce platforms. The model predicts that the increasing penetration of the Internet could enhance the share of SME exports. Specifically, we consider that firms exporting via e-commerce platforms pay lower fixed export costs, but higher service fees that translate into higher variable trade costs. In the presence of well-established e-commerce platforms, all SME exporters, or even some less productive large exporters, choose to export indirectly instead of directly exporting, and only the most productive firms choose to overcome the high fixed costs of directly exporting to foreign markets. As the development of the Internet propels e-intermediaries to keep evolving and revealing more cost advantages, the fixed costs of indirect exporting fall at a higher rate than the fixed costs of direct exporting, which leads to a larger share of SME exporters and a smaller share of exports by large firms in total exports.====Our empirical examination mainly relies on the Exporter Dynamics Database, which records the share of exports by the top 1%, 5%, and 25% of exporters from 1997 to 2014. One minus each share above is employed to characterize the share of SME exports in total exports. The standard deviation of the export value per exporter in the database is employed to control for the dispersion of exporter size. The development level of the Internet, which is the key explanatory variable, is measured by the percentage of individuals in the total population that use the Internet, which is based on the World Development Indicators.====The prediction that the development of the Internet boosts the share of SME exports in total exports is empirically supported in this work. The empirical analysis takes advantage of two data samples at the exporting country-product(HS2)-destination-year level (CYH2D) and at the exporting country-product(HS6)-year level (CYH6) separately. After controlling for various country characteristics and fixed effects at the lowest possible levels to partial out corresponding unobserved variables, we find that in the exporting country, the significantly negative effect of the development of the Internet on the share of exports by the top 5% or 25% of exporters remains in both samples.====This paper contributes to the burgeoning literature on the effects of information and communication technologies (ICTs) on international trade==== by exploring the nature of exporters in terms of their size. Although this line of literature has generally recognized the trade-liberalization effect of Internet technologies,==== the impact may not be the same across heterogeneous firms. We employ export share instead of export flow as the dependent variable in this work. The change in the share of SME exports sheds light on the resource allocation between SMEs and large exporters, especially when their exports both increase. In addition, some common determinants of the trade flows of SMEs and large exporters are eliminated when calculating the export share theoretically so that their potential confounding effects in explaining export share could be of less concern.====The following strategies and corresponding findings are also worth noting. First, we incorporate telephone development in the empirical specification to make a comparison with the development of the Internet, considering that they both fall into the category of national telecommunication infrastructure with active roles in reducing transaction costs. In contrast to the negative effect of the Internet on the share of large exporters, we find that telephone development exerts a statistically positive effect on the share of large exporters. We interpret the evidence as reflecting the key difference that the formation of e-commerce platforms relies fundamentally on the Internet, but not on phone calls.====Second, we construct a measure of the submarine cable infrastructure, and add it as an additional regressor to control for the qualitative differences of Internet traffic across countries that are not reflected by Internet penetration. This infrastructure is the key to cross-border data transfer. Due to the heavy reliance of e-commerce platforms on system capacity and data transfer speed for their services, the trade facilitation effect of improved Internet quality could be stronger for indirect exporting than direct exporting. A higher level of the submarine cable infrastructure is found to be associated with a larger share of SME exports. In this way, we provide suggestive evidence on the potential role of e-commerce platforms that is consistent with the baseline results.====Third, by exploiting the CYH2D sample further, we find that the share of SME exports increases with the development of the Internet to a lesser extent in sectors that have more traditional intermediaries relative to other sectors. Meanwhile, sectors that have more traditional intermediaries have larger shares of SME exports than other sectors, but the difference may lessen as the Internet develops. The results shed some light on the imperfect substitution relationship between e-commerce platforms and traditional intermediaries.====Empirical works that relate the development of the Internet exclusively to the trade performance of SMEs are scarce. Lendle and Olarreaga (2017) show that online markets provide smaller firms access to international markets by using data from eBay sellers. Lanz et al. (2018) find that SMEs tend to participate more in global value chains in countries where a higher share of the population has fixed broadband subscriptions. Our study provides further evidence that the impact could be biased in favor of SMEs, as it quantitatively assesses the impact of the Internet on SMEs in terms of their share in total exports using internationally comparable data.====Research on the traditional intermediaries in international trade (e.g., Blum et al., 2010; Ahn et al., 2011; Felbermayr and Jung, 2008 and Akerman et al., 2018) has investigated the influence of traditional intermediaries on the export behavior of SMEs. To some extent, e-intermediaries act similarly to wholesalers and other traditional intermediaries in international trade. However, e-intermediaries differ from traditional trade intermediaries inasmuch as the Internet plays an elementary role in the emergence and development of e-intermediaries.====The rest of the paper is organized as follows. Section 2 introduces the theoretical framework for modeling firm heterogeneity with e-commerce platforms. Section 3 describes the empirical strategy and the data in detail. The empirical results from the CYH2D sample are presented and discussed in Section 4, while those from the CYH6 sample in Section 5. Section 6 presents the conclusions.",The Internet and SME Participation in Exports,https://www.sciencedirect.com/science/article/pii/S0167624521000287,22 July 2021,2021,Research Article,35.0
Sawadogo Fayçal,"Université Clermont-Auvergne, CNRS, IRD, CERDI, F-63000 Clermont-Ferrand, France,Fondation pour les Etudes et Recherches sur le Développement International (FERDI), F-63000 Clermont-Ferrand, France","Received 23 August 2020, Revised 17 April 2021, Accepted 22 June 2021, Available online 5 July 2021, Version of Record 1 December 2021.",https://doi.org/10.1016/j.infoecopol.2021.100939,Cited by (2),"This study estimates the price elasticity of mobile voice communication in developed and developing countries using quarterly operator data from 2000 to 2017. Using a dynamic panel model through system-GMM, the study finds that the demand price elasticity is higher for operators in developed countries. Controlling for cross-price elasticity with internet data prices reveals that voice communication is a substitute for internet data usage in developed countries. Another important finding is that, for operators in developing countries, the price elasticity decreases with market development level, whereas it increases for those in developed countries. Demand for mobile voice communication is thus more sensitive to price changes in the less penetrated markets in developing countries and the mature markets in developed countries. Furthermore, over time, price elasticity has decreased across operators in developing countries, highlighting the need for updating regulatory frameworks for the telecommunications sector to reflect the sector's various developments. In addition, when formulating regulatory and tax policies, some important economic factors, such as income level and domestic market characteristics, should be considered to avoid losses in ====. The high estimated price elasticities suggest that operators do not have an obvious interest in engaging in collusive behavior that would hinder competition. Moreover, since there is no differential effect due to operators’ positions or market shares, asymmetric regulation of the dominant operators should be avoided.","Since the 2000s, the mobile telecommunications sector has undergone unprecedented development in many developing countries, with a proliferation of ""greenfield” operations and the opening up of the sector to competition. This has been accompanied by the creation of regulatory agencies that have become independent over time in most countries (ITU, 2018). All these factors have contributed to a significant decrease in communication tariffs in the sector, increasing consumer consumption, with concomitant welfare gains.====However, the OECD (2012), in a study on Mexico's telecommunications services, concludes that the high prices due to the sector's high concentration have led to losses in consumer welfare from 2005 to 2009. Hausman and Ros (2013), in response to the OECD study, contend that concentration and high market share are a necessary but not sufficient condition for a dominant position of power and higher prices; they show, through an estimation of the demand price elasticity, that the consumer surplus has been affected positively in the period. They argue that, in addition to competition, regulatory agencies should consider various more important factors, such as market share and the price elasticities of demand and supply. In addition, Jeanjean (2015) shows that investment in new technologies is more decisive in lowering prices than competition, which has a limited effect over time.====From another point of view, some authors, such as Matheson and Petit (2021), argue that mobile network operators (MNOs) extract a sort of rent through their exploitative behavior due to limited competition. This implies tacit collusive behavior among MNOs and some failures in regulatory processes. Carlton and Perloff (2004) and Dewenter and Haucap (2008) argue that the demand price elasticity in an industry is an important indicator of companies’ decision to engage in collusive behavior. Indeed, low price elasticities are a motivation to engage in collusive behavior, as operators are afforded the choice to set higher prices without losing demand, thus increasing their mark ups. Conversely, higher price elasticities are not conducive to collusion due to the possible “cheating” problem that may result in a great loss to the cheated firm. From all these studies, it may be noted that demand price elasticity is an important factor in an analysis of the demand for telecommunication services through regulatory processes, consumer welfare, and operators’ behaviors.====Many studies on the telecommunications sector have addressed the question of the demand for telecommunications services in the economics literature (Roller and Waverman, 2001; Martins, 2003; Waverman et al., 2005; Madden et al., 2004; Garbacz and Thompson, 2007; Dewenter and Haucap, 2008; Hausman and Ros, 2013). Some have considered market or operator characteristics (Koutroumpis et al., 2011; Kathuria et al., 2009; Karacuka et al., 2011; Dewenter and Haucap, 2008). However, most of these studies were based on operator data at country level and did not allow comparisons between different markets following the same approach. Furthermore, they used pre-2012 data, whereas the sector had undergone significant development in recent years.====Therefore, using quarterly mobile operator data from 2000 to 2017, taken from the GSMA Intelligence database, the objective of my study is to analyze demand price elasticity dynamics for operators in both developing and developed countries, and the extent to which these estimates could vary, depending on such factors as the country of location income level, region, market penetration level, or some other operator characteristics. Moreover, I analyze the relationship between voice communication and internet data usage, filling an important gap in the literature on this issue. To determine short- and long-run price elasticities, I consider a dynamic panel model that I estimate using a system-GMM that produces more efficient and consistent estimates than the first difference GMM. I find that demand for mobile voice communication is more elastic in developed countries, due to their market characteristics. Furthermore, my results show that mobile voice communication is a substitute for internet data usage only in these countries. An important finding is that the demand price elasticity has decreased over the years in developing countries, and that it decreases with market development level. In developed countries, it has remained constant over the years, and increases with market development level. Concerning debates on collusion behaviors or dominant market position power abuse, my results show that there is no differential price elasticity due to operators’ market shares or positions, suggesting that asymmetric regulation should be avoided, as proposed by Hausman and Ros (2013). Considering the first period of estimation (2000–2008), my results confirm the findings of previous studies (Martins, 2003; Lee and Lee, 2006; Kathuria et al., 2009; Koutroumps et al., 2011; Hakim and Neaime, 2014).====The remainder of the paper is organized as follows: Section 2 summarizes some stylized facts and presents a brief literature review. Section 3 presents the empirical strategy and the data used, while Section 4 is devoted to the results. Finally, Section 5 concludes with a discussion of the findings.",Demand price elasticity of mobile voice communication: A comparative firm level data analysis,https://www.sciencedirect.com/science/article/pii/S0167624521000275,5 July 2021,2021,Research Article,36.0
"Fabling Richard,Grimes Arthur","Independent researcher,Motu Economic & Public Policy Research; and Victoria University of Wellington, PO Box 24390, Wellington 6142, New Zealand","Received 31 August 2020, Revised 30 March 2021, Accepted 11 June 2021, Available online 12 June 2021, Version of Record 1 December 2021.",https://doi.org/10.1016/j.infoecopol.2021.100937,Cited by (3),"We estimate productivity gains and employment effects of ultrafast broadband (UFB) adoption and test whether effects differ when firms undertake complementary organizational investments. Using an ==== strategy based on proximity to schools (that were targeted in the UFB roll-out), we find that UFB adoption has a positive impact on multifactor productivity (MFP) over a four year horizon and a negative impact on employment (potentially due to increased outsourcing). The positive MFP effects are most clearly observed in firms that also make complementary investments to gain the most benefit from their UFB. The negative employment effects are observed especially in firms with initial low computer intensity.",None,Picking up speed: Does ultrafast broadband increase firm productivity?,https://www.sciencedirect.com/science/article/pii/S0167624521000251,12 June 2021,2021,Research Article,37.0
"Aldashev Alisher,Batkeyev Birzhan","International School of Economics, Kazakh-British Technical University, Almaty, Kazakhstan,Satbayev university, Almaty, Kazakhstan","Received 14 June 2020, Revised 10 June 2021, Accepted 11 June 2021, Available online 12 June 2021, Version of Record 1 December 2021.",https://doi.org/10.1016/j.infoecopol.2021.100936,Cited by (5),"Using unique data from a systematic roll-out of broadband infrastructure in rural areas, we provide evidence on the impact of high-speed internet via broadband on three main sectors of economy: retail, agriculture, and manufacturing. Our ==== model, which relies upon the timing of the roll-out at the first stage, shows that in general broadband access does not foster economic growth, but positively affects the retail sector, with no effect on the manufacturing and agricultural sectors. We also find that the biggest effects are found for speeds below 10 Mbps. We supplement our findings by exploring potential mechanisms that could drive the results.","Several studies have identified ageing telecommunications infrastructure based on analogue technology as being an impediment to productivity and competitiveness in transition economies (e.g., Madden and Savage, 1998). As such, access to high speed internet via broadband infrastructure, which has been rapidly deployed throughout the world in the last thirty years, is seen as a significant contributor to economic growth.====Previous literature highlighted that broadband infrastructure might be distinct from other forms of infrastructural investment which fosters economic growth, as high speed internet via broadband may further facilitate economy-wide growth by accelerating the distribution of information, reduction of transport and transaction costs, and fostering productivity and competition. Broadband may also affect the economy through development of new products, processes and adoption of new business models.====Because of its high cost, previous studies documented a gap in broadband usage between urban and rural areas. Prieger (2003), for instance, finds evidence of redlining, where broadband carriers avoid areas with high concentrations of poor and minority households. The so-called “digital divide” has received much attention from policymakers in both developed and developing countries. In our paper, we try to answer one simple question, that is whether rural areas that lack broadband access are economically deprived because broadband services are not available there.====A World Bank study (see, Qiang and Rossotto, 2009) reports that 10 percent increase in broadband penetration boosts GDP growth by 1.21 percentage points in developed countries. More importantly, the effects of broadband are even stronger for low- and middle-income countries – about 1.38 percentage points. Albeit, there are studies showing the positive impact of broadband on growth, further research is still needed especially for developing countries. On the one hand, broadband may not necessarily produce the desired effect without investment in complementary sectors, such as education or health. (see, Kelly and Rossotto, 2012). On the other hand, sometimes low-tech low-cost solutions may have a bigger effect (see, for example, Kenny, 2011).====Most of existing studies are based on cross-country variation and are conducted in context of developed countries, such as members of the OECD (e.g., Whitacre et al., 2014). The lack of access and difficulty of gathering information, especially in rural parts of the developing countries, might have contributed to this gap in the literature. In addition, the impact of broadband is hard to disentangle from other factors, as roll-out of broadband is usually driven by endogenous economic or social factors.====There is in fact scant systematic evidence relating broadband infrastructure to economic growth in developing countries, and more importantly, none examining its effect in rural areas. Yet, broadband can potentially be a bigger source of productivity in rural areas for several reasons. First of all, broadband may contribute to human capital formation. Access to information and various training courses online may help individuals in remote areas far from formal schools or training centers acquire new skills. Secondly, if broadband adoption can be characterized by diminishing returns to scale, as some authors point out (for example, Datta and Agarwal, 2004), broadband infrastructure might have a bigger effect in rural areas given relatively low levels of penetration. But at the same time, the benefits of this technology might not be fully utilized due to a host of other issues prevalent in developing regions, such as poor governance, lack of capital, low skill levels, and so on.====Moreover, given that allocation of investment funds to sector-specific infrastructure is an important policy issue for any country, in this paper, we try to analyze the relation between roll-out of broadband infrastructure and the regional product in rural areas of Kazakhstan by focusing on three major sectors of the economy: agriculture, retail, and manufacturing.====We exploit a unique dataset on program roll-out across the villages in Kazakhstan over a ten-year period from 2006 to 2016, combined with the data from the Kazakh Statistical Agency, to show the impact of broadband access on three major sectors of the economy. In our baseline results, we find a positive effect on the retail sector, with no effect on manufacturing output and mixed effects regarding the agricultural sector.====Given that the definitions of broadband differ across the studies, we also use different speed thresholds to define broadband. The evidence suggests that the biggest effect comes for speeds less than 10 Mbps. The effect of high speed of 10 Mbps and above is small and statistically insignificant.====Notwithstanding these baseline results, the uncovered relationships may be driven by omitted time-varying factors or be subject to reverse causality. Previous literature highlighted several such instances and addressed these issues, for instance, by utilizing the information on the availability of DSL broadband (e.g., Bertschek et al., 2013), or structure of the pre-existing voice-telephony network (e.g., Czernich, 2014) in an instrumental variable framework. Similarly, in order to address these concerns, we rely upon the time varying distance variable to the nearest district with broadband as an instrument. The IV results are qualitatively similar to our baseline results showing only a positive effect on retail, with no effect on manufacturing and agricultural sectors.====To better understand potential mechanisms at play, we also analyze both household and firm level data and show that in regions with higher broadband penetration rates households search more for products online, more actively use social networks, and tend to purchase more products from local sellers. The evidence from a supplementary survey of 643 local firms in rural areas show that in the retail sector, a substantial share of firms use broadband for advertising on social media platforms. Thus, the retail sector seems to benefit most from broadband roll-out through wider possibilities of marketing in social networks and easier access to potential clients.",Broadband Infrastructure and Economic Growth in Rural Areas,https://www.sciencedirect.com/science/article/pii/S016762452100024X,12 June 2021,2021,Research Article,38.0
"Kim Jin-Hyuk,Leung Tin Cheuk","University of Colorado at Boulder, Boulder, CO 80309, USA,Wake Forest University, Winston-Salem, NC 27109, USA","Received 21 August 2020, Revised 4 May 2021, Accepted 28 May 2021, Available online 3 June 2021, Version of Record 1 December 2021.",https://doi.org/10.1016/j.infoecopol.2021.100935,Cited by (0),"We examine the impact of removing Digital Rights Management (DRM) from electronic book devices. We derive a ==== hierarchical logit model based on the consumer’s utility maximization problem and estimate the model using data from a choice-based survey. We then simulate the counterfactual market outcomes when DRM is removed; on average, the ==== increases nontrivially holding everything else constant. However, the gain in ==== is diminished when we re-calibrate e-book device prices. Further, if there is a negative shock to content supply, then the consumer surplus could in fact decrease after DRM removal.","With the supply and demand for creative goods such as music, films, and books increasingly migrating from physical format to the digital, a new form of copyright management system has been widely adopted by market participants in the past couple of decades: Cryptographic engineering allowed rights holders to directly control access to and use of copyrighted works via Digital Rights Management (DRM). Specifically, a DRM system is an encryption scheme applied to digital content which can only be decrypted by authorized users and devices. The Digital Millennium Copyright Act of 1998 famously criminalized the act of circumventing DRM as well as disseminating knowledge about how to circumvent DRM.====While being perfectly legal, DRM has been criticized for over-protecting the rights holders beyond the scope of protection provided by traditional copyright laws. For instance, DRM systems most often prevent consumers from making a copy of purchased content for personal and other fair use, lending or sharing the content with others (except on a limited basis), and reselling the original copy to a third party.==== The handful of exemptions from anti-circumvention of DRM systems issued by the U.S. Copyright Office indicate that there is indeed a tension between the traditional copyright laws and the DRM technology (e.g., Bechtold, 2002, Feigenbaum and Weitzner, 2018).====Despite the criticisms from consumers and the arguments by e-commerce executives that DRM would not prevent unauthorized copying, all major e-book publishers have insisted on encrypting their content with DRM prior to distribution.==== The challenge is that we cannot readily predict what the counterfactual world without DRM systems would look like. If it led to more piracy, publishers’ profits would be reduced, and the incentive to create weakened, which would ultimately reduce the supply of content. The adoption (or removal) of DRM thus involves the familiar tradeoff between the benefits to consumers and the incentives provided to producers.====This paper aims to quantitatively assess how the removal of DRM systems would change the consumer surplus in the e-book platform market. Other scholars such as Rayna and Striukova (2008) have argued that consumers are not willing to pay as much for DRM-protected goods as for DRM-free ones. While empirical evidence on DRM is growing, how much DRM removal would positively affect consumer surplus is still a policy-relevant, open question that remains to be answered.====To answer this question, we solve the consumer’s utility maximization problem and derive an indirect utility function that theoretically incorporates the optimal consumption of digital content. This leads to a demand system for e-book readers that exhibits choice-specific hardware price coefficients, and we estimate the discrete-choice model using Bayesian techniques that allow for individual unit-level coefficients with flexible priors and show that it yields a plausible substitution pattern among the three major e-book platforms (the Kindle, the Nook, and the iPad).====For our counterfactual experiments, we start by turning off the DRM indicator for each of the three platforms, as well as for all of them at the same time, holding everything else constant. We find that the Kindle and the Nook can increase their market shares by a few percentage points if they unilaterally removed DRM, while the iPad’s market share is little affected. The average consumer surplus increases by $18 and $15 per person if the Kindle or the Nook unilaterally removed DRM, respectively; and it increases on average by $35 per person when all three devices removed DRM at the same time.====We then allow for a supply-side response in our counterfactuals. Because the e-book platform is a two-sided market, there are two layers of supply response: The first is that the platform could re-optimize the device price; and the second is that the publishers could pull contents if a platform were to remove DRM, which would lower the platform’s content availability. For the former, we use the inverse elasticity rule extended to the two-sided market (e.g., Weyl, 2010), and re-calibrate the equilibrium device prices in our counterfactual scenarios. For the latter, we assume hypothetical scenarios about the extent of a negative supply response (as we do not have any data on content supplies).====We find that the re-calibrated device prices dampen the consumer surplus gains, as the removal of DRM systems leads to an increase in residual demand elasticities which creates an upward pressure on the hardware prices. For instance, when DRM is removed from all three devices, the re-calibrated Kindle price increases by $14; the Nook increases by $13; and the iPad increases by $50. The average consumer surplus gain falls from $35 to $7 per person. We also find that a moderately negative supply response from publishers (say 20% reduction of available contents) can significantly offset the consumer surplus gains when the Kindle or the Nook removes DRM. When all three devices remove DRM, the negative supply response would in fact strictly decrease the average consumer surplus.====Some limitations to our approach need to be acknowledged upfront. First, our model is based on the premise that DRM cannot by itself increase consumer surplus. In other words, while some consumers might in fact prefer having DRM for such reason as that they believe DRM can reduce consumer piracy, our model assumes that consumers dislike having a DRM system, holding all other factors (such as the device price and content supply) constant. Our study is motivated by the general debate on this topic that tends to negatively portray the effect of DRM on consumers, thus creating tradeoffs. While our results indicate that consumers do not experience a significant disutility from DRM, we cannot test or quantify a positive impact of DRM per se.====Second, there are other impacts of DRM that cannot be built into our model. For instance, DRM could be used to limit content available to a rival platform. However, our model and data do not allow us to consider the effect of DRM on platform entry. Also, we cannot directly address how DRM removal changes the availability of contents, for instance, if a wider range of books can be made available across devices due to increased piracy. Further, we do not consider any potential antitrust issues, where DRM can be preferred by the platform-cum-device manufacturers to facilitate collusive pricing (Park and Scotchmer, 2005) or increase the hardware market power (Kim and Waldman, 2016).====The outline for the paper is as follows. Section 2 discusses some background information. Section 3 presents our model, and Section 4 describes the dataset. Section 5 discusses the estimation results, and Section 6 contains the counterfactual exercises. Section 7 concludes.",Eliminating digital rights management from the E-book market,https://www.sciencedirect.com/science/article/pii/S0167624521000238,3 June 2021,2021,Research Article,39.0
Hardy Wojciech,"University of Warsaw: Uniwersytet Warszawski, Warsaw, Poland","Received 23 August 2019, Revised 22 January 2021, Accepted 12 April 2021, Available online 9 May 2021, Version of Record 1 December 2021.",https://doi.org/10.1016/j.infoecopol.2021.100927,Cited by (3),"Much like the music and movies ==== before, the comic book industry has entered the digital markets and faces the unfair competition of unauthorized sources. I conduct a survey of comic book readers to analyse the relationship between the pirate channels and the sales of comic books from the top American publishers. My data allows me to construct a time panel of comics readers and calculate the substitution rate between the paid and unpaid channels of comics acquisition. I find that piracy is negatively related to comics sales though mainly those of limited series. Moreover, for digital comics the relationship is observable only for a smaller group of readers with high valuations of digital formats. I also show that the digital comics – both paid and unpaid – are typically considered as inferior by the readers. With the price of digitally released new comics set at the same level as their print versions, this suggests that readers who do not want to pay the full price for print copies are more likely to use pirate sources than to switch to legal digital channels. Still, both paid and unpaid readership is associated with higher interest in other types of comics-based media such as movies or video games. This suggests that comics piracy might carry some indirect positive effects. As this aspect could improve the negotiating position of comic book authors it should be studied further.","Like other creative industries, comics have entered the digital age, and face the challenges associated with digitalisation. Digitisation has made production costs lower, the creative process more decentralised, and distribution more efficient. Online stores have provided broader access to titles. Digital intermediaries have entered the market, facilitating the easy distribution of licensed content. These services have introduced mobile apps with broad access and user-tailored experiences.====With the introduction of digital formats, the high-quality piracy of comic books became effortless. Previously, sharing comics involved a tedious process of scanning, uploading, and searching through available sources. With the increasing access to high-speed internet and the introduction of digital formats, comic book file-sharing has become easier and faster. Currently, copies of comic books can be accessed directly through internet browsers or in downloadable formats for mobile applications.====Previous literature on piracy focused on the music and audio-visual sectors, largely omitting other types of content. Moreover, the exact effects of piracy remain disputed, with some studies showing that the effects might depend on the content characteristics. The American comics industry indeed differs from other creative industries. It is dominated by two publishers who focus on long-running, interconnected series with low prices and short stories. On the one hand, the serial nature of comic books is similar to that of TV series. On the other hand, their size and ease of consumption while traveling makes them more akin to songs, while the format itself most closely resembles that of traditional books. Finally, unlike in most creative sectors, the American comic book market has experienced growth in both its traditional and digital formats. Thus, the previous findings on piracy cannot be extended to comics without further study.====Considering the still limited scope of the literature on the effects of piracy, it is imperative to understand how piracy affects previously unstudied creative industries. The digital landscape is continuously changing, with new types and formats of content constantly being introduced. Understanding the effects of unpaid consumption on even the smaller markets, and the content characteristics that moderate these effects, can help to produce more general solutions and strategies for future content creators and distributors.====Moreover, despite the small size of the American comic book market, its significance is now larger than ever. The largest franchises, the highest grossing movies, and the most popular shows on streaming services are currently based on comic book stories. These trends also drive the growing popularity of comic conventions and huge sales of toys, clothing, and other gadgets related to the comic characters. These comic-based media and merchandise far surpass the size of the comic book industry itself.====I therefore seek to provide the first analysis of the readership patterns among American comic book readers, with a focus on the relationship between purchasing decisions and piracy. For this purpose, I conducted a three-month panel survey of comic book readers that is similar to prior studies conducted in the context of the film and music industries (albeit with a broader set of questions). Based on the resulting data, I am able to perform econometric analysis of the relationship between the unpaid and paid consumption of print and digital formats, as well as to identify some of the factors that moderate this relationship. Moreover, the serial nature of comic books allows me to examine this problem in the context of multiple connected goods, or, from another perspective, of single goods experienced over time. I estimate the cross-sectional and fixed effects models using information on comics types and reader valuations of digital formats to establish the relationship between comic books and piracy. As a robustness check, I also use lagged, unpaid consumption as an instrument for measuring the current levels and control of within-series changes in the quality of comic book issues. Finally, I verify the connection between the consumption of comics and the consumption of comics-based merchandise.====I find that an increase in unpaid consumption is associated with a (smaller) decrease in print and digital purchases, although the relationship is not homogeneous across items and respondents. A negative relationship with digital purchases is found only among the relatively few readers with high valuations of digital formats. Moreover, a negative relationship with piracy is detected mostly for the limited series, possibly because readers are less likely to switch formats when collecting a long-running, open-ended series. As comics consumption is positively related to the consumption of other comics-based media, further studies should analyse whether piracy can drive demand to other types of content.",Displacement from piracy in the American comic book market,https://www.sciencedirect.com/science/article/pii/S0167624521000159,9 May 2021,2021,Research Article,40.0
"Freitag Julian,Kerkhof Anna,Münster Johannes","ifo Institute for Economic Research, LMU Munich, CESifo, Germany,University of Cologne, Germany","Received 13 March 2020, Revised 9 March 2021, Accepted 5 April 2021, Available online 20 April 2021, Version of Record 31 August 2021.",https://doi.org/10.1016/j.infoecopol.2021.100926,Cited by (1),"We present a new measure for the political position of news outlets based on politicians’ selective sharing of news items. Politicians predominantly share news items that are in line with their political position, hence, one can infer the political position of news outlets from the politicians’ revealed preferences over news items. We apply our measure to twelve major German media outlets by analyzing tweets of German Members of Parliament (MPs) on Twitter. For each news outlet under consideration, we compute the correlation between the political position of the seven parties in the 19th German Bundestag and their MPs’ relative number of Twitter referrals to that outlet. We find that three outlets are positioned on the left, and two of them are positioned on the right. Several robustness checks support our results. We also apply our procedure to nine major media outlets from the USA and find that two outlets are positioned on the right, five are positioned on the left of the political spectrum.","State-of-the-art research shows that the media have a causal effect on the economic and political choices of individuals (DellaVigna and La Ferrara, 2016). The media are, however, repeatedly accused of being biased towards the political left or right. For instance, six-in-ten US citizens see political bias in the news media==== and four-in-ten German voters think that the government exerts pressure on the media.==== Are the media really biased? A growing body of literature addresses these questions by developing methods to assess political biases of news outlets (Groeling, 2013, Puglisi, Snyder, 2016).====Measuring the political position of news outlets is challenging, though. In particular, researchers must find ways to overcome problems of subjectivity and the absence of suitable baselines against which to assess bias (e.g., Groeling, 2013). Existing approaches are based on in-depth content analyses – either by human or by automated coding – or on determining the political position of the news outlets’ audience (Puglisi and Snyder, 2016). Many of these procedures are data demanding, computationally burdensome, and time consuming. Easy to implement methods to assess the political position of news outlets, in contrast, are rare.====We present a novel approach to measure the political position of online news outlets that is based on the selective sharing of news items by politicians on social media.==== Our central argument is that politicians predominantly share news items that are in line with their own political position, i.e., left-wing politicians prefer to share news items from left-wing news outlets, while right-wing politicians prefer to share news items from right-wing news outlets.==== Consequently, we can utilize the politicians’ revealed preferences over news items to infer the political position of the news outlets.====In an application to Germany, we compute a Spearman rank correlation coefficient for each news outlet under consideration. The Spearman rank correlation coefficient measures the correlation between the rank of the political position of the politicians’ parties (from most left-wing to most right-wing on a one-dimensional scale) on the one hand, and the rank of the politicians’ share of referrals to a particular news outlet – aggregated on the party level – on the other hand. A positive correlation indicates that the news outlet is positioned on the right, a negative correlation indicates that the news outlet is positioned on the left.====Our approach has several advantages. First, in comparison to existing approaches, it is relatively quick and easy to implement. We infer the political position of news outlets from the selective sharing of news items by politicians, whose political position is clear. Moreover, since sharing news items on social media is nowadays part of the politicians’ profession, we observe the politicians’ choices over news items in a setting where they have an incentive to reveal their preferences consciously and truthfully. Thus, our approach does not require any elaborate content analysis, but circumvents problems of subjectivity and the absence of suitable baselines against which to assess bias nonetheless. Second, the results from our procedure are straightforward to interpret. The Spearman rank correlation coefficient for a particular news outlet is either positive, negative, or equal to zero, whereby the news outlet can directly be classified as positioned on the left, on the right, or in the center of the political spectrum. Finally, our approach is applicable widely beyond this paper. In particular, while many existing procedures are limited to assessing political media bias in two-party democracies, our approach can also be applied to multi-party democracies, as long as the parties’ political position can be measured on an ordinal, one-dimensional scale.==== In addition to that, our approach is not data demanding and can thus be applied to small datasets, too.====We apply our procedure to twelve major online news outlets in Germany and consider the selective sharing of news items of German MPs on Twitter. The Spearman rank correlation coefficient is positive for five news outlets, but only statistically significant for two of them (==== and ====). The Spearman rank correlation coefficient is negative for seven further news outlets, and statistically significant for three of them (====, and ====). Following the above considerations, we conclude that ==== and ==== are positioned on the right, ====, and ==== are positioned on the left, and the remaining seven news outlets are positioned in the center of the political spectrum. Several robustness checks support our main results.====In an extension, we also analyze the selective sharing of news items of Members of the US Congress; this is particularly interesting, as reliable information on the ==== political position of Congress Members – in terms of ADA Scores==== – is available. Computing the correlation between individual political position and individual Twitter referral shares to nine major news outlets, we find that ====, and the ==== are positioned on the left, ==== and ==== are positioned on the right, and ==== and ==== are positioned in the center of the political spectrum. Moreover, we show that our results are highly correlated to the measures obtained in the seminal papers of Groseclose and Milyo (2005) and Gentzkow and Shapiro (2010).====The remainder of the paper is organized as follows. Section 2 discusses the related literature. Section 3 illustrates the application of the Spearman rank correlation coefficient as a measure of the political position of news outlets more closely. In Section 4, we describe the data collection procedure and the data preparation process. Section 5 presents the results of our application and compares them to existing measures of the political position of German news outlets. Section 6 demonstrates the robustness of our results by checking the tonality of the tweets, excluding extreme parties from the analysis, and by considering different numbers of news outlets. Moreover, we demonstrate that our results hold when we consider tweets by German MPs from a different time period and when we compute the political position of news outlets based on the selective sharing of news items by German Members of the European Parliament and Members of the German State Parliaments. Section 7 concludes.",Selective sharing of news items and the political position of news outlets,https://www.sciencedirect.com/science/article/pii/S0167624521000147,20 April 2021,2021,Research Article,41.0
Lindlacher Valentin,"University of Munich, 80539 Munich, Germany,ifo Institute, Poschingerstr. 5, 81679 Munich, Germany","Received 2 September 2020, Revised 20 February 2021, Accepted 12 March 2021, Available online 18 March 2021, Version of Record 31 August 2021.",https://doi.org/10.1016/j.infoecopol.2021.100924,Cited by (1),"Although the recent years have witnessed a stark increase in the availability of high-speed Internet, adoption rates remain low. One potential explanation is that for most users high-speed Internet does not increase their utility. Using a mixed logit discrete choice model, this paper analyzes whether high-speed and basic Internet are substitutes. I find that they are not. Users who do not need higher speeds, choose basic speeds regardless of high-speed availability. Therefore, high-speed Internet is not an infrastructure of general interest. Consequently, policy-makers cannot increase usage of high-speed Internet by solely fostering its rollout.","The Internet is widely regarded as a general-purpose technology as it is used on a large scale and has had significant social and economic effects. Czernich et al. (2011) and Kolko (2012), among others, have shown that the introduction of DSL at basic speeds accelerated economic growth. It is almost considered a truism that higher speeds will again go hand in hand with economic growth. It is thus unsurprising that policy-makers throughout Europe are concerned about raising broadband quality. The Digital Agenda for Europe from 2016 aims to connect all (50 percent) households to broadband Internet with at least 30 Mbps (100 Mbps) by 2020.==== Coverage rates for these bandwidths already reached 79.0 percent (55.1 percent) in 2017.==== In contrast to the adoption of Internet at basic speeds, however, the adoption of high-speed Internet remains low: only one-third (one-fifth) of households with higher available speeds have also subscribed to at least 30 Mbps (100 Mbps).==== This apparent subscription gap raises the question whether high-speed Internet is of general interest or only demanded by a very specific subset of users. Without a broad subscription, it is different from broadband Internet at basic speeds and cannot be described as a general-purpose technology.====There is still little research analyzing Internet at higher speeds. When investigating the effects of broadband Internet, the focus remains on basic speeds. More recent work on higher speeds relates to availability and therefore misses the role of adoption. This paper establishes a link between availability and adoption of high-speed Internet and provides one explanation for the lack of economic effects of high-speed Internet availability. It analyzes why high-speed Internet is not adopted widely in Austria, despite high coverage rates. I consider specifically contracts of more than 100 Mbps. The adoption of high-speed Internet depends on the substitutability over bandwidths and between stationary fixed-line and mobile technologies. Only if basic and higher speeds are considered substitutes, high-speed Internet will be adopted widely. If basic and higher speeds are not considered substitutes, there is only one specific type of Internet user with high benefits from high-speed Internet. Other users do not have such high benefits and will always choose a cheaper basic contract. In this case, the adoption rate will remain low because the high prices do not match the low utility that most users receive from higher bandwidths. Then, high-speed Internet would not be an infrastructure of general interest.====I apply a mixed logit discrete choice model, from which own-price and cross-price elasticities of demand are derived. While own-price elasticities show how price-sensitive consumers are, cross-price elasticities define substitutes. I apply a cross-sectional setting for Austria with regional differences in terms of active providers at the municipality level. The methodology applied in this paper is widely used in the market definition literature, where different technologies or bandwidths are analyzed in terms of their substitutability when deciding whether they form a single market or different markets.====I find that high-speed and basic Internet are not substitutes as cross-price elasticities show a different pattern for basic and higher speeds. Users who are satisfied with basic speeds refrain from adopting high-speed Internet even if it is available. I conclude that there is a certain type of users who needs higher speeds. So, increasing the availability of high-speed Internet will only increase the use of high-speed Internet if these users live in the area that gets an upgrade. By contrast, users who do not require higher speeds will always choose the relatively cheap basic-speed alternative, regardless of the availability of high-speed Internet. I find that for lower speeds, fixed-line and mobile technologies are substitutes. At higher speeds, these technologies do not function as substitutes. Furthermore, the derived own-price elasticities indicate that the demand is generally very elastic.====This paper uses novel geo-referenced data. I use extensive information from around 150,000 Austrian stationary broadband Internet tests conducted on the regulatory authority’s web site (RTR-NetTest) between April and November 2016. I observe the selected provider and the measured download and upload speed and can use this information to infer the selected contract. The number of Internet users testing their speed has increased sharply in recent years (Fig. A.1), making new methods such as the one presented in this paper feasible.==== The increase in speed tests is not due to the higher Internet penetration in Austria.==== Rather, the increase in tests shows that users are no longer satisfied with just being connected to the Internet, but are concerned with the speed at which they can use it. This data also makes it possible to track the increased average download speed (Fig. A.2). In addition, I observe the price of the selected contract as well as the competitive environment at the household’s place of residence, which enables me to define a choice set for each user in the sample.====Austria is examined as a representative country for Europe in terms of (bandwidth) coverage rates and Next Generation Access (NGA) coverage. As in many other countries, there is an incumbent provider (A1 Telekom Austria)==== that owns the copper network and is under regulation. The incumbent mainly offers high-speed contracts by copper-based VDSL technology. In addition, there are regional cable providers (kabelplus, Tele2, and UPC) that have historically been active in the TV market and therefore own a coaxial cable network. Their NGA technology is DOCSIS 3.0. In recent years, fiber network providers (Salzburg AG and LIWEST) have entered the market with very high-speed contracts at a more local level. Austria’s NGA mix contains only to a lesser extent fiber technologies (FTTP). Finally, Austria has one of the strongest mobile networks in Europe. Three providers (Hutchinson Drei, A1 Telekom Mobile, and T-Mobile) offer contracts for mobile broadband Internet at home. Importantly, A1 Telekom Mobile is one of them. So, the incumbent also offers mobile contracts.====This paper proceeds as follows. Section 2 gives an overview of the related literature. Section 3 defines the discrete choice mixed logit regression model and how the elasticities are derived. In Section 4, broadband coverage in Europe and the case of Austria are explained. Then, I introduce the data sets I use and merge. Section 5 presents the estimation results and the price elasticities of demand. Section 6 concludes.",Low demand despite broad supply: Is high-speed Internet an infrastructure of general interest?,https://www.sciencedirect.com/science/article/pii/S0167624521000123,18 March 2021,2021,Research Article,42.0
"Monteiro Natália P.,Straume Odd Rune,Valente Marieta","Department of Economics/NIPE, University of Minho, Campus de Gualtar, Braga 4710-057, Portugal,Department of Economics, University of Bergen, Norway","Received 7 April 2020, Revised 19 January 2021, Accepted 3 March 2021, Available online 7 March 2021, Version of Record 31 August 2021.",https://doi.org/10.1016/j.infoecopol.2021.100923,Cited by (2),"Whether or not the option to work remotely increases firm labour productivity is theoretically ambiguous. We use a rich and representative sample of Portuguese firms, and within-firm variation in the policy of remote electronic access – a key prerequisite for remote work – over the period 2011–2016, to empirically assess the relationship between remote access and firm labour productivity. Based on estimations of models with firm-fixed effects, we find a significantly negative association, on average, between remote access and productivity. However, we also find a substantial degree of heterogeneity across different categories of firms, where the association between remote access and productivity is significantly positive for firms that undertake R&D activities. Our findings suggest that the possibility of working remotely, as proxied by the possibility of remote access, is more likely to be harmful for productivity in non-exporting, small firms that do not do R&D, and that employ a workforce with a below-average ====.","In recent years, with the widespread use of cloud services and remote access to work applications, workers can perform their tasks outside the office (OECD, 2016). This provision of ‘remote work’ thus allows workers to perform what is often referred to as ‘telework’ or ‘telecommuting’. In 2015 in the US, nearly 4 million workers (representing 3 percent of the workforce) worked at least half of their time away from the office (GWA, 2017), and in the EU those who usually work from home constituted 5 percent of the employed workers (Eurostat, 2018). This trend, driven mainly by the digital revolution, has been changing the workplace organisation in a number of ways. Teleworkers may work at home but also turn to coffee shops or co-working spaces, or even travel around the world while maintaining their career goals. Video conferencing allows out-of-office workers to communicate and interact with each other in real time anywhere they are. Telework today also encompasses various full-time jobs in a wide set of occupations (not only highly educated) across multiple industries.====Technological advances in how work is performed may mean that “anywhere working” becomes business-as-usual (Blount, 2015). In the US, 70 percent of firms surveyed by the Society for Human Resource Management allowed telecommuting from an ad-hoc to a full-time basis (SHRM, 2018). Furthermore, around 75 percent of Europeans have access to some flexibility in their work in terms of schedule and location, and this is advocated as allowing better management of work and family life (OECD, 2016, Eurofound, 2017). To such end, the Work-Life Balance Directive (EU, 2019) was adopted in August 2019 by the European Parliament to allow parents and carers the right to remote work arrangements. Of course, the exceptional circumstances brought about by the COVID-19 pandemic in 2020 have strongly reinforced the trend of remote work and highlighted the health and safety advantages of such arrangements for firms where required social distancing at the workplace is not possible.====How does this global trend affect workplace performance? Do more flexible workplace arrangements translate into mutual benefits to both employees and employers? While anecdotal evidence might point to several advantages of remote work (to workers and firms alike), the existing empirical evidence on the effects of teleworking is less conclusive. In particular, an extensive body of work shows mixed evidence on the linkages between out-of-office work and various individual-level worker outcomes (such as turnover, job autonomy and satisfaction, and motivation).==== Regarding the effect on productivity (at worker or firm level), whereas the empirical evidence generally points to a positive effect of remote work, recent lab experiments provide evidence of negative or, at best, mixed effects of telework on productivity. Novel theoretical developments also show that the relationship between self-managed working time (which includes remote work), employee effort, and thus worker productivity, is not unambiguously positive, as commonly derived in various approaches from economics and related fields.====In the present paper, we contribute to the literature on remote work and productivity by empirically analysing the effect of a key prerequisite for the possibility of working remotely, namely ====. More specifically, we study how the possibility of remote access affects firm labour productivity, as measured by sales per worker, using firm-level data for Portugal, a country where the prevalence of telecommuters is higher than the EU-28 average. We conduct our analysis by first gathering information from the ==== (Eurostat, 2011) during the 2011–2016 period, which contains the following question: “Did your enterprise provide to the persons employed remote access to the enterprise’s e-mail system, documents and applications?’. This survey is then matched with data from the Portuguese ==== to recover data on firm characteristics, allowing us to build a panel covering the years 2011, 2012, 2013 and 2016.====A potentially important caveat for the interpretation of the results from this analysis is that we only observe whether or not workers in a firm are given remote electronic access. We do not observe whether there are any formal remote work arrangements in place, or the share of workers actually working remotely. Nevertheless, since in most cases remote access to e-mail, documents and applications is both a necessary and sufficient condition for working remotely, it seems reasonable to interpret the enablement of remote access as a proxy for remote work enablement, i.e., whether or not the firm allows (at least some of) their employees to work from home.====The main contribution of our analysis is related to the nature and richness of our data, which allows us to study the impact of firms” remote access policies across a wide range of firms and industries. This is in contrast to much of the existing literature that has looked at non-random or selected samples, usually large firms and in manufacturing.==== Importantly, the richness of the data enables us to analyse the possibility of ==== of remote access along several different dimensions related to firm, worker and job characteristics. In turn, this allows us to draw some conclusions regarding in which cases a policy of allowing remote access is likely to have a positive effect on firm labour productivity, and in which cases remote access is likely to be detrimental to productivity. Furthermore, the panel structure of the dataset improves upon the majority of empirical studies that are based on cross-sectional data. In particular, this feature combined with the within-firm variation in remote access policy allows us to control for firm-fixed heterogeneity, which in turn enables us to circumvent some potentially important endogeneity issues.====Our empirical strategy consists of estimating an augmented Cobb-Douglas production function on several firm characteristics, which is a standard approach in the literature (e.g., Black and Lynch, 2004, Bloom et al., 2011, Bloom et al., 2019). The effect of remote access on labour productivity is identified by a difference-in-differences estimator comparing the differences in productivity trends between two categories of firms: (i) those that changed their policy on remote access during the period of analysis (either adopting or abandoning such a policy), and (ii) those that always had the same remote access policy in place (either allowing it or not).====As a starting point and benchmark for comparison, we estimate the empirical model on the full sample of firms, finding a significantly negative association between remote access and firm labour productivity on average. However, our subsequent analysis reveals that this average association masks a considerable degree of heterogeneity. More specifically, we find that the negative association between remote access and productivity is mainly present for small and non-exporting firms, which do not perform any R&D activities and employ a workforce with a below-average skill level. On the other hand, for the subcategory of firms that undertake some R&D activities, we find a significantly ==== association between remote access and labour productivity. The finding of significantly opposite associations between remote access and productivity depending on the R&D status of the firm, which is one of the key results in our paper, has intriguing parallels to previous experimental evidence showing that remote work affects productivity differently for “routine” versus “creative” job tasks (Dutcher, 2012).====Another interesting feature of our results is that the inclusion of a firm-fixed effect in the empirical model makes a crucial difference. In our benchmark model using the full sample, the estimation of a specification without firm-fixed effects yields a positive relationship between remote access and labour productivity, although this association ceases to be statistically significant if we control for the full set of worker and firm characteristics. However, once we control for time-invariant firm-level heterogeneity, the sign of our key estimate reverses, indicating a significantly negative association between remote access and firm labour productivity on average. This result is particularly noteworthy in the light of the fact that a large portion of the existing empirical literature on remote work, often presenting a positive relation, is based on cross-sectional evidence.====The remainder of the paper is organised as follows. In Section 2 we review the relevant theoretical and empirical literature. We proceed in Section 3 by presenting the data and variables, including descriptive statistics, before introducing and discussing our empirical strategy in Section 4. The analysis based on the full sample of firms, including several robustness checks, is given in Section 5. In Section 6 we explore the possibility of inter-firm heterogeneity by re-estimating our preferred empirical model on a number of subsamples defined according to several relevant partition criteria. Finally, Section 7 closes the paper with some concluding remarks.",When does remote electronic access (not) boost productivity? Longitudinal evidence from Portugal,https://www.sciencedirect.com/science/article/pii/S0167624521000111,7 March 2021,2021,Research Article,43.0
"Xu Xu,Reed Markum","School of Business, Henderson State University, 1100 Henderson St, Arkadelphia, AR 71999, United States","Received 4 February 2020, Revised 4 November 2020, Accepted 22 January 2021, Available online 31 January 2021, Version of Record 31 August 2021.",https://doi.org/10.1016/j.infoecopol.2021.100914,Cited by (4),"There are large variations in research output among nations despite the rapid globalization progress. This article provides a new angle to help explain such variations. In this article, we study the impact of internet penetration on the research output of an economy. Using a country-level panel dataset, we find that higher internet penetration increases the volume of research output in an economy. The results are robust to a number of specifications, including an ==== approach that addresses the endogeneity of internet penetration. We also find some evidence showing that the impact of internet penetration on research output quantity decreases as the size of fixed broadband users increase in an economy. The effects of internet access on research quality is less conclusive. Results suggest that broadening the access of internet is important for research output boosting or innovation in general.","Research output, as an essential precursor of technological improvement, matters for the economic growth of an economy (Hatemi-J et al., 2016; Inglesi-Lotz and Pouris, 2013; Lee et al., 2011; Solarin and Yen, 2016; Vinkler, 2007). Publications are the major components of research output and are the most commonly used vehicles through which discoveries are broadcasted to the rest of the world. Traditionally, the United States has been leading the world in publication output. With increased globalization, however, other nations have been striving to improve their scientific capacity (Adams et al., 2013). Despite this trend towards internationalization and mobility, there remain significant variations in publication output among nations. The exact reasons for this variation disparity are primarily unknown (Man et al., 2004). Understanding the factors affecting the variations could help inequality among countries. In this paper, we offer new empirical evidence on the determinants of research output, exploring the role of the Internet in research productivity-boosting.====Previous studies present many benefits brought by the Internet to our economy. Empirical evidence has shown the positive relationship between Internet access and economic growth (Arvin and Pradhan, 2014; Czernich et al., 2011; Holt and Jamison, 2009; Jiménez et al., 2014; Kolko, 2012; Koutroumpis, 2009). Other social-economic benefits brought by the Internet include enhancing subjective well-being (Kavetsos and Koutroumpis, 2011; Pierewan and Tampubolon, 2014), reducing unemployment (Hjort and Poulsen, 2019; Jayakar and Park, 2013), and increasing labor productivity (Mack and Faggian, 2013; Najarzadeh et al., 2014). Research has also found that Internet use may facilitate knowledge generation and diffusion (Cowan and Jonard, 2004; Fadul, 2014; Pachi et al., 2012). Our paper is motivated by the productivity increasing and knowledge-generating background.====This paper relates to the literature focusing on the Internet and research productivity. Internet use could increase or decrease scholars’ research output. Barjak (2006) provided a summary of both sides. The productivity rising hypothesis states that the Internet makes access to information faster and easier and thereby lowering the discovery costs (Nentwich, 2003; Walsh et al., 2000). Communication and collaboration have also been made more accessible and therefore, help increase the efficiency of research (Nentwich, 2003; Steinmueller, 2000). There remains skepticism about the productivity-enhancing hypothesis. The productivity reducing hypothesis supporters worry that information overload may increase search costs instead of decreasing it. Also, the Internet could distract researchers and therefore decreasing their output (Walsh and Roselle, 1999).====Empirical results mainly find evidence supporting the productivity-enhancing hypothesis. Ynalvez et al. (2005) found the impact of ICT adoption on academic productivity to vary with the location of the scientists. Barjak (2006) used survey data on individual scientists to explain research output as a function of various inputs. Their results also showed a positive correlation between Internet use and research productivity, which indicates that there is no sign that the Internet distracts scientists from work and reduces their output. Pachi et al. (2012) used Brazil as a case study and found that an increase in Internet connectivity can positively affect academic productivity using Index number theory. Sooryamoorthy (2016) used survey data in South Africa and found that Internet use contributes to knowledge production. The author suggested that South Africa needs to provide faster and cheaper communication infrastructure to facilitate the growth of scientific research in the country. The above mentioned empirical studies all used individual-level data collected from surveys or interviews. None of them allows for cross-country comparison. Our paper contributes to the literature by testing the hypothesis using a country-level dataset.====Besides examing the impact of internet penetration on research output quantity, we also extend our main study in two ways. While the empirical evidence supports that the Internet can help increase research productivity, they did not consider the network effects of Internet penetration. We test if a country with a larger population of internet users can capture a greater research output gain from boosting internet penetration. Also, since research productivity can be reflected in both quantity and quality, we further consider the impact of Internet access on research quality proxied by citations and H-index.====This paper fills a gap in the literature by providing a new angle to understand why some countries publish more than others. We use a panel dataset to help explain the differences across countries in research output from the angle of Internet adoption. Our results reveal another benefit brought by the Internet by investigating its impacts on research outputs in this article. Also, through examing the impact of internet penetration network effects and distinguishing quantity and quality of research output, it contributes to the debate of the productivity-enhancing or productivity-reducing role of the Internet by offering a nuanced study. The conclusion that internet access increases research output is robust to several specifications, including an instrumental variable approach that addresses the endogeneity of Internet penetration. We found some evidence contradicting the hypothesized positive network effects of internet penetration. The evidence we obtained regarding research output quality is less robust. We, therefore, call for future research to look into the extensions of this line of research.====The remainder of this paper is organized as follows. Section 2 describes the data. Section 3 presents the empirical model. Section 4 presents the main results and robustness checks. Section 5 discusses two extensions of our main results. Section 6 concludes.",The impact of internet access on research output - a cross-country study,https://www.sciencedirect.com/science/article/pii/S0167624521000020,31 January 2021,2021,Research Article,44.0
"Bourreau Marc,Cambini Carlo,Hoernig Steffen,Vogelsang Ingo","Telecom Paris, Institut Polytechnique de Paris,Politecnico di Torino, DIGEP; EUI - Florence School of Regulation,Nova School of Business & Economics, Universidade Nova de Lisboa, Campus de Carcavelos, Carcavelos 2775-405, Portugal,Boston University, Department of Economics, Boston, MA","Received 11 September 2020, Revised 17 December 2020, Accepted 5 January 2021, Available online 8 January 2021, Version of Record 31 August 2021.",https://doi.org/10.1016/j.infoecopol.2021.100913,Cited by (2),"Caused largely by the recent technological changes towards digitalization, infrastructure investment in network industries has become the main issue for regulatory intervention. In this paper, we study the impact of co-investment between an incumbent and an entrant on the roll-out of network infrastructures under demand uncertainty. We show that if the entrant can wait to co-invest until demand is realized, the incumbent’s investment incentives are reduced, and total coverage can be lower than in a benchmark with earlier co-investment. We consider two remedies to correct these distortions: (i) co-investment options purchased ==== by the entrant from the incumbent, and (ii) risk premia paid ==== by the entrant. We show that co-investment options cannot fully reestablish total coverage, while premia can do so in most cases, though at the cost of less entry. Finally, we show that an appropriate combination of ==== and ==== remedies can improve welfare.",None,"Co-investment, uncertainty, and opportunism:",https://www.sciencedirect.com/science/article/pii/S0167624521000019,8 January 2021,2021,Research Article,45.0
"Conti Chiara,Reverberi Pierfrancesco","Department of Computer, Control and Management Engineering Antonio Ruberti (DIAG), Sapienza University of Rome, Roma, Italy","Received 2 August 2019, Revised 19 December 2020, Accepted 22 December 2020, Available online 27 December 2020, Version of Record 2 June 2021.",https://doi.org/10.1016/j.infoecopol.2020.100912,Cited by (2),"We study how an opt-in regime of privacy regulation, which limits the scope for online price discrimination, affects product quality and consumer surplus. When consumers decide to share personal data, they benefit from the complementarity between information and quality, but they pay personalized prices instead of a uniform price. We find that, if the complementarity is strong enough, then product quality is higher with than without the opt-in regime. Privacy regulation may have conflicting effects on consumers with different attitudes towards privacy, and an increase in quality is necessary to improve total consumer surplus. Interestingly, these results hold both under monopoly and imperfect product market competition. We thus recommend that privacy protection measures be grounded in the study of the relation between personal information and product quality.","This paper studies how privacy regulation affects product quality and consumer surplus, both under monopoly and imperfect competition. A distinguishing feature of our analysis is that we focus on the interplay between personal information and product quality.====The motivation for the paper is the vibrant academic and policy debate on the need to monitor the massive collection and use of personal data made possible by recent advances in information technology, which have raised privacy concerns.==== In the EU, the awareness of privacy risks has led to set out the General Data Protection Regulation (GDPR), which prescribes that the collection and use of personal data be allowed only after explicit consent of data subjects (==== regime).==== Firms are eager to obtain such data for profiling purposes, so as to set tailored prices, but the opt-in regime makes it easier and less costly for data subjects to preserve anonymity if they intend to do so. Hence, it is more difficult for firms to infer consumers’ valuations for the products and price discriminate.==== As long as this reduces profits, it may also challenge the incentives to invest in product quality.====In this framework, we shed light on the following issues: Does the opt-in regime, which reduces the room for price discrimination, come at the expense of product quality? Does enhanced privacy protection improve consumer surplus? Ultimately, should consumers be provided with more or less control over personal data? Despite the interest on these issues, economic analyses on the welfare effects of the opt-in regime are scarce (see Section 2 for a review of the relevant literature).====We develop a theoretical model to analyze how the amount of shared data is related to prices, product quality, and consumer surplus. In the baseline model, we consider a monopolist who sells online and invests in quality. We assume that, under the opt-in regime, consumers’ willingness to pay (hereafter, WTP) for the product is private information. In this sense, privacy regulation ensures that consumers are anonymous at the outset (at no cost). Having observed the quality level and the uniform price of the product, consumers decide whether or not to share personal data.====We consider two consumer groups. The first consists of consumers who, after evaluating the pros and cons, may opt for sharing data. We refer to these consumers as ====. Instead, the second group includes those who do not leave traces on the web, since they would receive more harm than benefit (e.g. they may have a ‘taste for privacy’).==== We refer to them as ====.==== Consumers in either group who are anonymous pay the uniform price for the product (if they buy).====When sharing personal data, consumers face a trade-off. On the one hand, they pay personalized prices based on their WTP. We assume that, having obtained personal data, the firm achieves a partial consumer identification (e.g. because it uses a tracking technology with some degree of inaccuracy). Hence, the firm has an imperfect ability to price discriminate and extract consumer surplus.====On the other hand, they enhance their consumption experience. We argue that (perceived) product quality increases with data sharing, and that the benefits of data sharing increase with product quality. Thus, personal information and product quality are complements from the consumers’ perspective.====There are several examples of this relationship in online markets. To fix ideas, consider electronic devices or software programs. Generally, firms offer bundles of the core product and add-on services such as delivery terms, after-sale assistance, training and tutoring. Customers are usually provided with add-on services if they share personal data (e.g. by registering on websites). Hence, data sharing increases utility, since it enables consumers to enjoy the benefits of add-on services.==== Moreover, these services are more valuable to users when the core product has a higher intrinsic quality. Thus, as long as the ‘quality’ of an Enterprise Resource Planning (ERP) system is related to a high degree of sophistication or complexity, each user in the organization strongly benefits from training and tutoring services, and the higher the product sophistication the higher the benefit from these services.====In this framework, we find that the opt-in regime may increase or decrease product quality and consumer surplus, depending on the degree of complementarity between information and quality.====Assume, first, that the complementarity is strong enough. In view of the benefits of data sharing, even privacy pragmatists with high WTP may be induced to pay personalized prices. For this purpose, the firm raises both product quality and the uniform price, so that anonymity is less attractive to these consumers. Despite the opt-in regime limits price discrimination, it does not necessarily reduce the incentives to innovate, and may even encourage the firm to provide higher quality.====Privacy pragmatists are the winners, whereas privacy fundamentalists are the losers from a quality improvement under the opt-in regime. Indeed, privacy pragmatists benefit from the higher quality and from data sharing (while avoiding to buy at the uniform price). Instead, privacy fundamentalists, who are not prone to sharing data, are affected by a negative externality. Specifically, the option to choose anonymity for privacy pragmatists puts an upward pressure on the uniform price that reduces the number of privacy fundamentalists who are active in the market. This negative price externality dominates the positive quality externality. Thus, somewhat paradoxically, the opt-in regime hurts exactly those consumers for which protecting personal data is more valuable.====Now, assume that the complementarity between information and quality is weak. Then, privacy pragmatists with high WTP can hardly be induced to share data. Thus, under the opt-in regime, the firm reduces quality (that is costly to obtain). As to privacy pragmatists, those who buy at personalized prices are worse off because of the lower quality, while those who maintain anonymity are often better off. Thus, the net effect of privacy regulation is ambiguous. As to privacy fundamentalists, fewer consumers are active under the opt-in regime, and they pay a higher price for a lower quality.====We show that an increase in product quality is a necessary condition for the total consumer surplus to increase under the opt-in regime. However, consumer surplus may decrease even in cases with a higher quality. Given that privacy regulation limits price discrimination, the firm may use quality to extract surplus. In this sense, product quality is an effective substitute for personalized prices.====We check the robustness of our results to a number of variants of the baseline model. We show that the main results are not altered when consumer groups are of different sizes, or under an alternative timing where privacy choices are made before the firm sets prices (so that the firm cannot use the uniform price to induce privacy pragmatists to share data). We also show that the opt-in regime more likely improves product quality and consumer surplus in the presence of further privacy protection measures, that is, when the firm has a limited ability to target consumers.====Finally, we extend our model to account for imperfect competition in a vertically differentiated product market. Perhaps surprisingly, we find that the main results of the baseline model still hold. This is mostly because, in a duopoly, the high-quality firm has incentives similar to the monopolist when choosing quality and prices under the opt-in regime, as compared to the benchmark case.====This paper is organized as follows. Section 2 examines the policy debate and reviews the literature. Section 3 introduces the model. Section 4 analyzes the opt-in regime. Section 5 evaluates the effects of this regime on quality and surplus. Section 6 checks the robustness of results. Section 7 deals with product market competition. Section 8 discusses policy implications and outlines future work.",Price discrimination and product quality under opt-in privacy regulation,https://www.sciencedirect.com/science/article/pii/S0167624520301566,27 December 2020,2020,Research Article,46.0
Cariolle Joël,"Research Officer, Fondation pour les études et recherches sur le développement international, 63 boulevard François Mitterrand, Clermont-Ferrand, France.","Received 29 July 2019, Revised 23 November 2020, Accepted 24 November 2020, Available online 1 December 2020, Version of Record 2 June 2021.",https://doi.org/10.1016/j.infoecopol.2020.100901,Cited by (12),"In recent decades, international connectivity has improved significantly with the worldwide deployment of some 400 fiber submarine cables (SMCs), transmitting more than 99% of international telecommunications. If sub-Saharan African (SSA) has long remained excluded from this interconnection process, the maritime infrastructure network has recently densified and spurred an African connectivity catch-up. This paper estimates the impact of SMC deployment on the digital divide in a sample of 45 SSA countries covering the period of 1990–2014. Difference in differences (DID) estimations are conducted and highlight the particular contribution of SEACOM and EASSy cables, laid in 2009–2010, to Internet penetration in Eastern and Southern Africa. According to ==== estimates, the rollout of these SMCs has yielded a 3–5 percentage point increase in Internet penetration rates in this region compared to the rest of SSA. This is a remarkable advancement, since this variation corresponds approximately to the level of Internet penetration in the subcontinent prior to their arrival.","Information and communication technologies (ICTs)==== are enabling production technologies with a proven impact on communication and transaction cost-reduction (Goldfarb & Tucker, 2019), particularly strong in the context of lacking infrastructures and large market failures that characterize many sub-Saharan African countries (Aker & Mbiti, 2010). By contributing to the emergence and dissemination of innovations in trade, agriculture, education, health, financial services, transportation, and public administrations (Aker & Mbiti, 2010; Aker & Blumenstock, 2014; World Bank, 2016;), ICTs, mobile and Internet technologies in particular, have the potential to act as general-purpose technologies in the African development process (Bresnahan & Tajtenberg, 1995; Cardona et al., 2013).====Mobile and Internet technologies facilitate economic actors’ access to simple as well as more complex telecommunications functions, from receiving health follow-up or agricultural extension programs through text-messaging, to exchanging digital content remotely through Internet, or facilitating different kinds of transfers with the government, citizens, or businesses through mobile money. In sub-Saharan Africa (SSA), mobile phones have spread in response to missing landline infrastructures (Aker & Mbiti, 2010), representing today the principal communication engine and the main platform for Internet access. However, while the uptake of mobile phone technology has fostered the multiplication of digital innovations throughout the subcontinent, this dynamic bumps into the large Internet divide in the population. In fact, African countries’ Internet penetration rates were not exceeding 55 percent of the population by 2015, with some countries like Niger, Sierra Leone, or Guinea-Bissau displaying penetration rates lower than five percent of the population (ITU, 2019). As a result, the expected dividends of digitization in the subcontinent, especially of Internet technologies diffusion, are slow to materialize and to benefit the whole population (World Bank, 2016).Identifying key determinants of mobile and Internet adoption is therefore critical to make digital technologies a lever for growth, productivity, and job creation in the subcontinent.====In this regard, during the last decade, global connectivity has improved significantly with the worldwide deployment of more than 400 fiber submarine cables (SMCs) over the period of 1990–2018, transmitting more than 99% of international telecommunications (broadband Internet, phone calls, videos, text messages, diplomatic messages). While SSA remained relatively spared by this process until 2009, its international connectivity, i.e. capacity for international telecommunications, has recently benefitted from this deployment, facilitating access to and reducing the cost of international telecommunications, in particular broadband Internet (OECD, 2014). Because SMCs are the first link into the international telecommunications access value chain (Schumann & Kende, 2013), a study of their impact on mobile phone and Internet penetration is therefore an important step to apprehend the potential for and the main obstacles to digitization in Africa. To our knowledge, such an empirical approach, focused on this critical connectivity infrastructure and applied to this geographic area, is missing.====While the economic literature has widely focused on the consequences of ICT diffusion,==== studies that reveal the conditions of their uptake, especially in Africa, are scanter. In regards to mobile and Internet technologies, micro- or local-level empirical research has identified levers or obstacles to their uptake in the subcontinent. These determinants encompass the private network size (Björkegren, 2019), geographic constraints (Buys et al., 2009), demographic and individual characteristics such as revenue, age, gender, education, and urbanity (Birba & Diagne, 2012), physical infrastructure proximity (Buys et al., 2009; Mothobi & Grzyboswki, 2017), and competition policy (Buys et al., 2009). Among international macro-level studies, Chinn and Fairlie (2007, 20210) highlight the importance of per capita income in explaining Internet penetration in a sample of 161 developed and developing countries. In addition, they stress the role of access to electricity, human capital, and institutional quality in reducing the Internet divide. Wallsten (2005) and Howard and Mazaheri (2009) have also examined the contribution of the regulatory framework to ICT access, but to our knowledge, none of these studies has emphasized the importance of improved international connectivity for digital technologies diffusion.====Yet, from the early works of Katz and Shapiro (1985) and Crémer and colleagues (2000), the quantity and quality of interconnections have been identified as critical determinants of the demand for telecommunications services. Thus, by increasing the number and the quality of interconnections linking African countries together and to the world telecommunications network, have SMCs spurred mobile telephony and Internet uptake?====This paper addresses this question by offering new empirical evidence on the consequence of improved international connectivity, resulting from SMC deployment, for mobile and Internet diffusion in SSA. The empirical analysis is conducted in two steps. First, within fixed-effect estimations of the effect of the successive arrivals of SMCs along African coasts over the 1990–2014 period on Internet and mobile phone penetration is conducted. Baseline estimates support that one additional SMC laid on the African coast is associated with an average two percentage point increase in Internet penetration rates in the recipient country. By contrast, mobile phone penetration is found to be rather unresponsive to the deployment of this infrastructure. Second, the concern for endogeneity in the timing and location of SMC rollout is addressed by narrowing the focus on the specific contribution of the SEACOM cable and the East African Submarine Cable System (EASSy), deployed along Eastern and Southern African coasts in 2009-2010, using a difference in differences (DID) approach.====These cables’ arrival is considered as a quasi-experiment for recipient countries for two main reasons. First, they serve the whole Eastern and Southern-East African region, suggesting that their deployment follows regional rather than country-level considerations. In fact, SEACOM and EASSy cables have been laid all along the southern and eastern African coasts, with no exception among coastal countries, whether in mature telecommunications markets like Kenya and South-Africa, or less-mature ones like Somalia or Madagascar. Second, the timing of this wave is also associated with a new generation of broadband cables (Weller & Woodcock, 2013; OECD, 2014) that induced an exogenous technological shift in the capacity for carrying international telecommunications in this area.====These assertions are supported by the existence of long-period pre-treatment parallel trends in Internet penetration rates in treated and untreated countries, diverging after SEACOM/EASSy arrival. According to DID estimations, the deployment of these cables has yielded a 3 to 5 percentage point increase in Internet penetration rates in recipient countries compared to rest of the subcontinent. This growth in Internet penetration represents approximately the SSA's average Internet penetration rate preceding the SEACOM/EASSy laying. These estimates are robust to various sample restrictions, aimed at addressing an eventual sample selection bias, consisting in excluding from the estimation sample countries hosting non-regional cables, emerging telecommunication markets, and landlocked countries.====The next section describes the institutional background of SMC deployment in SSA. The section 3 presents the baseline estimations results, while the section 4 exposes our identification strategy and the resulting estimates. The section 5 concludes.",International connectivity and the digital divide in Sub-Saharan Africa,https://www.sciencedirect.com/science/article/pii/S0167624520301451,1 December 2020,2020,Research Article,47.0
"Denzer Manuel,Schank Thorsten,Upward Richard","Gutenberg School of Management and Economics, Johannes Gutenberg-University Mainz, Jakob-Welder-Weg 4, Mainz 55128, Germany,Gutenberg School of Management and Economics, Johannes Gutenberg-University Mainz, IWH and IZA, Jakob-Welder-Weg 4, Mainz 55128, Germany,School of Economics, Sir Clive Granger Building, University Park, University of Nottingham, IWH, Nottingham NG7 2RD, United Kingdom","Received 26 February 2019, Revised 23 July 2020, Accepted 24 November 2020, Available online 28 November 2020, Version of Record 2 June 2021.",https://doi.org/10.1016/j.infoecopol.2020.100900,Cited by (9),"We examine the impact of household access to the internet on job finding rates in Germany during a period (2006–2009) in which the share of households with a broadband connection increased by 31 percentage points, and job-seekers increased their use of the internet as a search tool. During this period, household access to broadband internet was almost completely dependent on the availability of a particular technology (DSL). We therefore exploit the variation in DSL availability across municipalities as an instrument for household access to the internet. OLS estimates which control for differences in individual and local area characteristics suggest a job finding advantage of about six percentage points. The IV estimates are substantially larger, but much less precisely estimated. However, we cannot reject the hypothesis that, conditional on observables, residential computer access with internet was as good as randomly assigned with respect to the job finding rate. The hypothesis that residential internet access helped job-seekers find work because of its effect on the job search process is supported by the finding that residential internet access greatly increased the use of the internet as a search method. We find some evidence that household access to the internet reduced the use of traditional job search methods, but this effect is outweighed by the increase in internet-based search methods.","Since the late 1990s, the internet has transformed the ways in which job-seekers look for work and the ways in which they contact potential employers. It has also changed the ways in which employers advertise positions, search for and screen suitable applicants. During the period 2006–2009 (the setting for this paper) the proportion of households with high-speed internet access increased from 34% to 65%.==== Survey evidence shows that over this period job-seekers and employers both increased their use of online search technologies.==== The internet seems to offer a number of significant advantages as a search and matching technology (Autor, 2001). It allows job-seekers to search more quickly for a larger number of vacancies over a wider geographical area, while also allowing them to search and screen for vacancies which meet certain characteristics. It allows applicants to contact employers and make multiple applications more quickly and at much lower cost. It also allows the creation of information networks which enable job-seekers to broadcast their skills and availability to potential employers. In an environment where not everyone has access to this new technology (as in the 2000s in Germany), we expect that job-seekers who have access will have better job search outcomes compared to those who do not. Better job search outcomes will imply shorter unemployment durations if the benefits of the new technology do not increase reservation wages sufficiently to outweigh the increased rate at which job opportunities can be located.====A number of empirical studies have examined whether using the internet as a job search method is more effective than traditional job search methods. We review these studies in Section 2. A key issue is the endogeneity of the choice of search method. More recent papers in this literature have attempted to solve the endogeneity problem by using the timing and location of internet availability as an instrument for the use of the internet as a search method. In this paper we also use the process of internet expansion to measure the causal impact of the internet on the job finding rate of the unemployed. We use a relatively new survey of unemployed job-seekers in Germany which provides information on household access to the internet, job search methods and job search outcomes. But, in contrast to other papers in this literature, which focus on the internet as a search method, we ask a simpler but distinct question: does having a computer at home with internet access increase the job finding rate?====There are a number of reasons why focussing on this question is interesting: if we were to allocate a computer with internet access to a group of unemployed job-seekers, would that group have a higher job finding rate than those not allocated a computer? In contrast, it is difficult to imagine a policy which allocated a search method: by definition, search methods will be chosen on the basis of their expected costs and benefits which makes the average treatment effect on the treated (ATT) a less interesting value, since further expansion of the technology will tend to impact on those with lower benefits. The second advantage is that we use a very straightforward survey question which is less likely to be affected by measurement error. Instead of requiring survey respondents to recall what methods they used to search for a job, and whether they used the internet to do so, the question we use is “do you have a computer at home with internet access?” The third advantage of focusing on computer access with an internet connection rather than search methods is that search methods may be complementary with internet access, as noted by Kuhn and Skuterud (2004, p. 223). For example, traditional search methods such as “contacting employers directly” or “answering advertisements in newspapers” may be made more effective with internet access.==== Since we wish to measure the total benefit of internet access on search outcomes, it seems natural to consider internet access as our treatment and particular search methods as intermediate outcomes which are themselves affected by internet access.====Although our research question has advantages, our results do require a different interpretation. First, our estimates should be interpreted as the “intention to treat” effect of having a computer with internet access: it seems likely that some of those who have a computer will not actually use it for job search, either because they are unable to, or because it offers no actual benefit. Second, having a computer with internet access may help job-seekers find a job through mechanisms other than job search channels. For example, computer users may learn skills which are valued in the workplace, or internet access may be used as a signal of increased productivity by employers (Bertrand and Mullainathan, 2004). Our research question is also inherently partial equilibrium in nature. Finding that job-seekers with internet access have better job search outcomes than those without does not necessarily imply that a labour market with internet access has a higher matching rate than one without. In the extreme case, if the new technology does not increase the aggregate matching efficiency, but merely re-allocates which job-seekers are successful in finding matches, access to the new technology will be beneficial to those that have it only insofar as others do not have access (Fountain, 2005). Finding that there is an individual effect is a necessary but not sufficient condition for establishing that the technology increases the matching rate overall.====Fig. 1 shows the key technological developments which occurred in Germany in the 2000s which we exploit in our analysis. In panel (a) we plot the share of households in Germany with a broadband internet connection, which increased from about 10% in 2003 to nearly 80% in 2010. During this period, access to broadband internet was essentially only possible via a technology called ==== (DSL), which was first made available to subscribers in Germany in 1999 (Kopf, 2012). In panel (b) we plot the number of broadband subscriptions which shows that almost all access to broadband was via DSL. Over the same time period, the proportion of job-seekers using the internet as a job-search method increased considerably: our own calculations from survey data show that the share of unemployed job-seekers using the internet for job search increased from 33% in 2003 to 73% in 2015.====The crucial feature of DSL we exploit is the fact that this technology was not available to all households at the same time. We use data on availability of DSL at the municipality level as an instrument for household access to a computer with internet.==== As noted by Falck et al. (2014, p. 2245), key features which determined DSL availability included technological peculiarities such as the location of buildings which were determined in the 1960s during the roll-out of public switched telephone network. We show that DSL availability is a strong predictor of whether a household has a computer with internet access, and we argue that DSL availability is also plausibly exogenous with respect to the job finding rate of unemployed job-seekers since we control for a set of observable local area characteristics like the employment rate, the unemployment rate and its change, the density of firms, population density, surface area, pupil shares by education level and GDP per capita.====A further distinguishing feature of our study is that we consider the job finding behaviour of a group of relatively disadvantaged job-seekers. The survey we use (described fully in Section 4) oversamples job-seekers who have exhausted all savings, assets and other state transfers (such as insurance based unemployment benefit) and who are eligible for minimal means-tested benefits.==== In this sample, job finding rates are very low: only 30% of the unemployed sample are in employment a year later.====Nevertheless, we find that there are large differences in employment rates between those who have a computer with internet access and those who do not. Those who have a computer at home with internet access have a job finding rate some nine percentage points larger than those who do not. OLS estimates which control for differences in observable individual and local area characteristics suggest a job finding advantage of about six percentage points. The IV estimates are substantially larger, but much less precisely estimated. However, we cannot reject the hypothesis that, conditional on observables, individual computer access was as good as randomly assigned with respect to the job finding rate. To relate our results back to the conventional question considered in the literature, we show that having a computer at home is indeed strongly related to the use of the internet as a search method. This reassures us that having a computer at home with internet access is an important tool for internet job search.====In Section 2 we review the existing empirical literature on the effect of the internet on individual job search outcomes. In Section 3 we explain the methods we use, and in Section 4 we describe our data and provide some descriptive statistics. Our results are reported in Section 5 and Section 6 concludes.",Does the internet increase the job finding rate? Evidence from a period of expansion in internet use,https://www.sciencedirect.com/science/article/pii/S016762452030144X,28 November 2020,2020,Research Article,48.0
"Jeitschko Thomas D.,Kim Soo Jin,Yankelevich Aleksandr","Graduate School, Michigan State University, 466 W. Circle Dr. Rm. 212, East Lansing, MI 48824,School of Entrepreneurship and Management, ShanghaiTech University, 393 Middle Huaxia Road, Shanghai, China,Federal Communications Commission, 445 12th Street SW, Washington, DC 20554","Received 12 February 2020, Revised 2 September 2020, Accepted 12 November 2020, Available online 19 November 2020, Version of Record 2 June 2021.",https://doi.org/10.1016/j.infoecopol.2020.100899,Cited by (6),"We study zero-rating, a practice whereby an Internet service provider (ISP) that limits data consumption exempts certain content from that limit. This practice is particularly controversial when an ISP zero-rates its own vertically integrated content, because the data limit and ensuing overage charges impose an additional cost on rival content. We find that zero-rating and vertical integration are complementary in improving social welfare, though potentially at the expense of lower profit to an unaffiliated content provider. Moreover, allowing content providers to pay for zero-rating via a sponsored data plan raises welfare by inducing the ISP to zero-rate more content.","Internet service providers (ISPs) offer subscribers a menu of service plans, many of which consist of a periodic fee and overage charges that apply when exceeding a predetermined limit or cap on data consumption (Nevo et al., 2016). Among mobile wireless ISPs like Verizon Wireless, a typical plan involves a monthly fee for a preset amount of data and an overage charge for additional gigabytes of data beyond the preset amount.==== Home Internet service providers have also started to limit the service that their monthly subscription fee buys, but the limits are typically much higher than those of mobile wireless providers.====We study a hybrid pricing strategy that several ISPs have introduced to distinguish their service offers whereby the ISPs do not subject a subset of available content to caps or overage charges. Such content is said to be zero-rated, meaning that its consumption is not counted when tabulating consumers’ monthly data consumption toward or beyond the cap. Additionally, ISPs may offer to zero-rate certain content providers’ data in exchange for a fee, a practice referred to as sponsored data.====There are numerous examples of zero-rating and sponsored data programs. T-Mobile’s “Binge On” allows consumers to watch unlimited HBO, Hulu, Netflix, Sling, and other content without eating into their data allowances. To offer the service, T-Mobile reduces video quality to 480p+ for zero-rated content, though it does not charge content providers affiliated with this service.==== In contrast, under the now defunct “Go90” sponsored data program, Verizon charged content providers to zero-rate their content.==== Comcast’s Stream TV service presents an example of zero-rating by an ISP that is vertically integrated into content. Stream competes with other streaming services like Amazon Video, Hulu, and Netflix, but does not count toward Comcast’s data allowance (see Comcast Corporation, 2016, Knowledge, 2016). More generally, any ISP that sets a cap on Internet service but also provides other content using a means beside the Internet (i.e., cable) effectively zero-rates the other content.====On the surface, zero-rating appears to benefit consumers by allowing them to consume certain content without being concerned about overage charges. In principle, this can increase broadband consumption and foster greater innovation and competition among CPs. Nevertheless, zero-rating has spurred a heated debate over its merits among scholars, public interest groups, and industry advocates,==== and raised regulatory concerns as a potentially harmful discriminatory practice. For instance, possibly worried that zero-rating was a violation of net neutrality antidiscrimination principles, the Federal Communications Commission (FCC) in 2016 conditioned its approval of the merger between Charter Communications and Time Warner on a condition that the parties not impose data caps or usage-based pricing until mid-2023, a condition that Charter, in 2020, sought to end two years early.==== In 2017, the FCC released a report (later retracted) putting forward a framework for evaluating mobile zero-rated offerings (see FCC, 2016 ====457, Federal Communications Commission, 2017a, Federal Communications Commission, 2017a, Federal Communications Commission, 2017b).==== After the FCC abandoned net neutrality (FCC, 2018a), California unveiled broad net neutrality legislation which, among other things, sought to ban zero-rating and sponsored data.==== The California legislation is presently being challenged by the U.S. Justice Department.====Regulations in various countries outside the U.S. have likewise curtailed zero-rating. In 2016, India prohibited data service providers from offering or charging different prices for data—even if offered for free. This had the effect of banning Facebook’s Internet.org Free Basics program, which provided a pared-down version of Facebook and weather and job listings.==== Similarly, regulators in Canada, Chile, Norway, the Netherlands, and Slovenia have made explicit statements against zero-rating as anti-competitive or contravening national net neutrality regulation (OECD, 2015; Lohninger et al., 2019). A primary concern is that zero-rating can give an unfair advantage to zero-rated services, allowing ISPs to favor some content over other.====We address several research questions ensuing from the debate over zero-rating. On what grounds will ISPs and CPs agree to a zero-rating deal if CPs are asymmetric in the quality of content that they provide? Under what conditions is zero-rating harmful, or alternatively, beneficial to content competition and social welfare? Finally, how does vertical integration together with zero-rating of affiliated content alter competition from rival CPs and how does vertical integration impact ISP incentives to offer sponsored data options?====To address the questions above, we consider a model in which a monopolistic ISP offers consumers access to content from two asymmetric CPs using a two-part tariff consisting of a hookup fee ==== and a linear data overage charge ====.==== We view CPs as asymmetric in content quality, but also view their content as substitutable to a degree. We characterize and compare the set of equilibria when zero-rating is banned as well as when it is permitted with and without monetary transfers between the ISP and CPs. If content is zero-rated, the ISP does not charge the overage fee (i.e., ====).====We find that zero-rating leads to two opposing effects on an ISP’s profit, one operating through the hookup fee, the other through the overage charge. Moreover, for each CP, zero-rating not only directly affects content demand, but also indirectly influences demand by affecting the content price. The aggregate effect of zero-rating on both the ISP’s and CPs’ profits depends on content quality and the degree of content substitutability.====Suppose first that CPs cannot offer monetary transfers for zero-rating. Then, in equilibrium, the ISP zero-rates the lower quality CP to take advantage of the higher overage charge that it can set for higher quality content. A zero-rating equilibrium only emerges under a sufficiently large level of substitutability. The intuition is that if the ISP zero-rates any content when content is highly differentiated (low level of substitutability), the loss to the ISP from an overage charge that could be charged on low quality content is relatively large: there are distinct demands for both CPs regardless of content quality. Consequently, the ISP chooses not to zero-rate to take advantage of consumers’ relatively inelastic demand.====If instead, CPs must pay to be zero-rated—i.e., sponsored data programs—both CPs end up being zero-rated in equilibrium, which we refer to as full zero-rating in the paper. If content is sufficiently differentiated, both CPs always pay a positive fee for zero-rating, which increases the ISP’s incentive to lead to full zero-rating. As content becomes more substitutable, however, the low quality CP loses its incentive to pay a positive fee. When substitutability is sufficiently high, the ISP finds it optimal to subsidize the low quality CP to induce full zero-rating, permitting the ISP to more than make up for the subsidy with a higher fee to the high quality CP.====In Section 4, we permit the ISP to integrate with one of the CPs. The ISP optimizes by vertically integrating with the high quality CP and zero-rating its content to profit from revenue tied to the sale of content. Moreover, without a monetary transfer, the integrated firm only wants to zero-rate its affiliated content while optimally not zero-rating the rival’s content in an attempt to vertically foreclose the rival. However, if there is a monetary transfer, full zero-rating emerges in equilibrium. Thus, as long as there is a monetary transfer for zero-rating, vertical integration does not exclude full zero-rating. Nevertheless, the low quality CP is left worse off under vertical integration.====We also find that full zero-rating tends to lead to the highest level of social welfare. Zero-rating softens content competition compared to no zero-rating, but ultimately leads consumers to pay less per unit of content (by reducing the overage charge to zero) and to consume more. Thus, zero-rating with monetary transfers (sponsored data plans) is welfare-enhancing relative to zero-rating without monetary transfers because the latter does not induce full zero-rating. Additionally, vertical integration is welfare-enhancing because it gives the ISP an incentive and the ability to raise affiliated content consumption, but possibly at the expense of the unaffiliated CP.====Our model setup leans on the framework of Economides and Hermalin (2015), who analyze a monopoly ISP that can impose download limits on symmetric rival CPs selling content that consumers perceive to be ==== independent. Economides and Hermalin show that these limits can place downward pressure on CP prices, permitting ISPs to profit from an increase in demand.==== Using a variant of the model in which content is ==== substitutable (e.g., to account for limits on consumers’ time that can be devoted to content) and content quality is asymmetric we investigate when an ISP might wish to relax download limits. We find that zero-rating allows the ISP to fine-tune how it wants different, asymmetric CPs to behave by adjusting its pricing to consumers, allowing it to discriminate across CPs without charging them different termination prices.====Substantial recent work has investigated the impact of zero-rating in various settings. Krämer and Peitz (2018) draw policy implications on zero-rating practices from the European Union’s regulatory perspectives. Jaunaux and Lebourges (2019) analyze how zero-rating affects end-users’ content usage. Neither study relies on economic models of two-sided markets to derive potential implications of zero-rating. Likewise, whereas Inceoglu and Liu (2019) and Hoernig and Monteiro (2020) model a monopolistic ISP that offers consumers access to content to study how the ISP can use zero-rating to benefit from, respectively, price discrimination and network effects, neither article models profit maximizing competing CPs.====As in our work, several papers do study zero-rating using economic models of two-sided markets. Somogyi (2017) finds that zero-rating is an optimal ISP strategy when CP revenue per click is relatively large and the ISP subscription fee is relatively small, in which case the ISP trades off serving a greater number of consumers by zero-rating the CP that derives a higher amount of revenue per click in order to extract revenue from that CP directly.==== Jullien and Sand-Zantman (2018) find that absent a ban on zero-rating, the ISP can use sponsored data to improve efficiency by facilitating the transmission of information between CPs and consumers. Nevertheless, this mechanism results in socially suboptimal consumption levels because the ISP charges excessive prices to CPs. Gautier and Somogyi (2018) investigate paid prioritization and zero-rating (or sponsored data) within a unified framework to show that the ISP choice between these two practices depends on how valuable content traffic is to CPs and the degree of network congestion. Schnurr and Wiewiorra (2018) show that zero-rating with (without) monetary transfers may enhance (lower) consumer welfare, while harming non zero-rated CPs by distorting consumers’ content consumption. Finally, Maillé and Tuffin (2019) setup a model with competing ISPs.==== They find that sponsored data may harm CPs but benefit ISPs.====While the above work offers important insights into zero-rating and sponsored data, our model differs from most existing work along at least two important dimensions. First, following Economides and Hermalin (2015), we suppose that CPs can charge consumers directly. Although we notice that there is a significant amount of content available to consumers for free, this modeling choice permits us to focus on major providers of streaming services and to also account for the important case of cable ISPs who set data caps.==== Second, and perhaps more importantly, we extend our results to a scenario where the ISP can vertically integrate into content provision in order to study how zero-rating could be used by major ISPs like AT&T and Comcast, who have integrated into content, to vertically foreclose rival content.====Aside from being broadly related to the theoretical literature on pricing in multi-sided markets (Armstrong, 2006; Rochet, Tirole, 2003, Rochet, Tirole, 2006)—the ISP in our work is a platform connecting content sellers to content buyers—the analysis in this manuscript is closely related to the study of net neutrality. The static and dynamic impact of violations of net neutrality—simply put, a ban on discrimination at the point where content terminates—has been shown to vary widely according to the framework under analysis (i.e., the means of modeling prioritization, the level of ISP competition, etc.). For example, Economides and Hermalin (2012) show that price discrimination via paid prioritization diminishes welfare if it diminishes content diversity and Choi and Kim (2010) and Cheng et al. (2011) show that prioritization could incentivize ISPs to keep network capacity scarce. Conversely, paid prioritization has been shown to lead to higher broadband investment and increased diversity of content (Krämer and Wiewiorra, 2012; Bourreau, Pereira, and Vareda, 2015).==== Of particular interest here, Guo et al. (2010) and Brito et al. (2014) investigate the combined effects of vertical integration between an ISP and CP together with net neutrality. Both studies show that the the affect of vertical integration along with paid prioritization depends on how well an affiliated (integrated) CP can generate ad revenue relative to the unaffiliated CP.====Paid prioritization differs from zero-rating from both a technical/economic perspective and a legal one. The central technical distinction is that paid prioritization permits an ISP to offer different service quality tiers to different CPs, whereas zero-rating operates via the opposite end of the market, by presenting consumers with a pricing distinction between different CPs. Besides having the potential to lead to quantitatively different outcomes, this distinction has clearly been scrutinized by regulators who have made different determinations with regard to whether or not zero-rating violates net neutrality (Koning and Yankelevich, 2018).",Zero-Rating and Vertical Content Foreclosure,https://www.sciencedirect.com/science/article/pii/S0167624520301438,19 November 2020,2020,Research Article,49.0
"Arato Hiroki,Hori Takeo,Nakamura Tomoya","Tokyo Metropolitan University, 1-1 Minami-Osawa, Hachioji-shi, Tokyo, Japan,Tokyo Institute of Technology 2-12-1, Ookayama, Meguro-ku, Tokyo 152-8552, Japan,Meiji Gakuin University, 1-2-37 Shirokanedai Minato-ku, Tokyo, Japan","Received 20 May 2020, Revised 7 October 2020, Accepted 9 October 2020, Available online 14 October 2020, Version of Record 2 June 2021.",https://doi.org/10.1016/j.infoecopol.2020.100898,Cited by (2),"We consider the implementability and welfare effects of a partial announcement policy using a beauty contest model in which agents’ actions are strategic complements and their decisions on public information acquisition are endogenous. We obtain the following results: (i) if a social planner sells public information at a constant price, then multiple equilibria emerge and the partial announcement equilibrium becomes unstable; (ii) there exist pricing rules that ensure a unique and stable equilibrium partial publicity level, which indicates that a partial announcement policy can be implemented; and (iii) as the precision of public information increases, the optimal price rises due to higher optimal publicity level. To realize the higher optimal publicity level in equilibrium, the social planner must reset the pricing rules to lower the price of public information for each publicity level.","Proper disclosure of information by the authorities is essential in the management and development of financial markets. However, proper information disclosure is not an easy task. If the authorities adopt policies that place too much emphasis on information transparency, then market participants may overreact to that information, leading to unintended consequences for the authorities. In contrast, if they adopt a policy that disregards information transparency, then the necessary information will not be communicated and market participants will be unable to respond appropriately to changes in economic fundamentals. From the social welfare maximization perspective, the authorities must adopt a balanced policy.====Several academic studies address whether publicly provided information improves social welfare. In their seminal paper, Morris and Shin (2002) show that public information may induce excessive coordination of agents’ actions, leading to a detrimental welfare effect if those actions are strategic complements. This is because coordination is not socially beneficial in the beauty contest model. They conclude that an opaque policy can improve social welfare if the precision of public information is sufficiently low.====Many researchers challenged the results of Morris and Shin (2002). Among them, Cornand and Heinemann (2008) demonstrate that a partial announcement policy====, under which a social planner that disseminates public information to a certain fraction of agents can alleviate the problem of excess coordination using the beauty contest model of Morris and Shin (2002). Cornand and Heinemann (2008) conclude that under an optimal partial announcement, a transparent policy can ameliorate social welfare regardless of the precision of public information.====For policymakers, an important issue is how to pursue a partial announcement policy. In the present study, we endogenize the fraction of information users and then design a simple and realistic policy instrument to help create a socially optimal partial announcement policy. To endogenize the fraction of information users, we assume that each agent must pay a usage fee to acquire public information. If the usage of public information generates a larger payoff than not using public information, then an agent decides to pay the fee and becomes an information user. Consequently, the fraction of information users is endogenized.====We think this method makes partial disclosure easier to implement and is fairer because even if it is random, it is difficult for the authorities to justify restricting the release of important information to a limited number of people. The methods we examine in this study are based on the voluntary decision-making of economic agents and on the benefit principle.====Using this simple framework, we examine the features of usage fees that implement the partial announcement policy and then characterize the socially optimal usage fee for public information. Initially, we find that it is not easy for the social planner to implement a partial announcement policy by selling information at a certain price. When agents’ actions are strategic complements, information acquisitions are also strategic complements, as demonstrated by Hellwig and Veldkamp (2009) and Hellwig et al. (2012). If the social planner sells public information at a certain price, then such strategic complementarity causes multiple equilibria, which consist of two symmetric pure strategy equilibria (the full- and no-announcement equilibria) and an asymmetric pure strategy equilibrium (the partial announcement equilibrium). The partial announcement equilibrium is unstable. Therefore, unless the social planner can coordinate the beliefs of all agents completely, it will be difficult to realize the partial announcement equilibrium by selling information at a certain price.====To ensure the uniqueness and stability of the partial announcement equilibrium, we propose another pricing rule for information. If the social planner offers a pricing rule that counteracts the strategic complementarities of information acquisition, then the partial announcement is implementable. One such pricing rule is that the price of public information increases sufficiently in relation to the number of public information users. This method involves a strategic substitution effect.====From a theoretical perspective, our information-purchasing game is in a class of anonymous games introduced by Schmeidler (1973). Anonymous games are those in which each player’s payoff depends on their actions and on the aggregate behavior of the other players, but not on the exact action profile. In particular, our information-purchasing game belongs to the class of binary-choice anonymous games; that is, each agent’s action set consists of two alternatives. In our model, the two alternatives correspond to whether to buy or not to buy. The partial announcement equilibrium is an equilibrium in which some players buy information and other players do not. This is an asymmetric pure Nash equilibrium. We find some properties of the pricing rule that make the best-reply dynamic converge to an asymmetric equilibrium. Hence, the social planner can implement the partial announcement policy under this pricing rule.====We then show that there exists a socially optimal usage fee that implements the socially optimal level of publicity. The socially optimal usage fee is determined by the precision of public information, the one of private information, and the degree of the strategic complementarities. We show that if the quality of private information decreases or if the quality of public information increases, then the optimal degree of publicity and the optimal usage fee increases.====We characterize a simple pricing rule under which equilibrium publicity level is socially optimal and stable. Moreover, we find three properties of this optimal pricing rule, assuming that this rule is rapidly increasing in the ratio of public information users. First, when the degree of strategic complementarity rises, the pricing schedule should shift upward because the optimal publicity level falls and an excess demand for public information occurs. Second, when public information becomes more precise, the pricing schedule should shift downward because the optimal publicity level rises and an inefficiently low demand for public information occurs. Third, when private information becomes more precise, the pricing schedule should shift upward because the optimal publicity level decreases and excess demand for public information occurs.====Our modeling strategy of endogenizing the fraction of public information users is closely related to a method discussed in Cornand and Heinemann (2008). More specifically, they state that by “selling data at prices that not all agents are willing to pay,” the social planner could implement a partial announcement policy.==== The present study complements the discussion of Cornand and Heinemann (2008) because we provide a model-based analysis of “selling data at prices that not all agents are willing to pay” by extending their model.====The remainder of this paper is organized as follows. Section 2 describes the model and shows the multiplicity of equilibria and the instability of the partial announcement equilibrium under certain prices. In Section 3, we present a solution for surviving a partial announcement equilibrium. We analyze the welfare implications in Section 4. In Section 5, we extend the model in terms of payoff and information structure to confirm the robustness of our results. Finally, in Section 6, we provide our conclusions. Appendix contains all proofs and supplementary analyses.",Endogenous information acquisition and the partial announcement policy,https://www.sciencedirect.com/science/article/pii/S0167624520301426,14 October 2020,2020,Research Article,50.0
Henriques David,"London School of Economics and Political Science, London, United Kingdom,RBB Economics, London, United Kingdom","Received 13 January 2020, Revised 13 June 2020, Accepted 29 September 2020, Available online 2 October 2020, Version of Record 2 June 2021.",https://doi.org/10.1016/j.infoecopol.2020.100897,Cited by (1),"This paper investigates how regulations that limit advertising airtime may affect advertising quality and social welfare. I show, first, conditions under which an advertising cap may reduce or improve the average quality of advertising broadcast on a free-to-air TV platform. Second, an advertising cap may reduce TV platform’s and firms’ profits, while the net effect on viewers’ welfare is ambiguous because the ad quality may decrease as a result of a regulatory cap offsetting the direct gain from watching fewer ads. The results suggest that a regulator that is trying to increase social welfare via regulation of the volume of advertising on TV should take the effect of advertising quality into consideration. Implementing an advertising cap without regard to ad quality may result in lower social welfare than leaving advertising airtime unregulated."," The quality of TV ads matters not only for commercial purposes, but also because it influences the quality of the TV viewing experience. TV viewers may consider some ads as entertaining, while other ads may cause a nuisance. Advertising represents a remarkable proportion of airtime of various TV channels, and regulators limit advertising airtime on TV in various countries. But what if these caps drive to a change in the quality of the TV ads? This paper sheds light on potential mechanisms through which ad caps may affect ad quality positively or negatively, and it discusses the welfare impacts of such caps.====Individuals in developed countries spend a significant share of their time watching TV. For example, in the first quarter of 2019, the average US American adult spent almost four hours per day watching live TV (The Nielsen Company, 2019), while in the UK, the average viewer aged 4+ watched slightly more than three hours and twelve minutes per day in 2018 (Ofcom, 2019). In Japan, the average time was three hours and twenty-one minutes per day in 2017 (eMarketer Report, 2018). TV broadcasting offers an opportunity for firms to advertise to a large pool of consumers. In 2018, TV ad spending increased to more than USD 140 billion across twelve countries including Australia, Brazil, Canada, China, France, Germany, India, Italy, Japan, Russia, UK and US. TV ads account for c. 42% of all worldwide advertising spending (Digital TV Europe, 2018).====In the US, the frequency and length of commercial breaks are generally unregulated with some programs on major TV channels recording advertising levels in excess of twenty minutes per hour. In many other countries, regulators limit advertising airtime on TV. These time restrictions are usually intended to ensure that viewers are not exposed to excessive amounts of ads, and that the overall viewing experience meets a certain quality standard. For example, in Australia, regulations limit ads airtime to ten minutes in any single hour, while in Europe, the revised EU Audiovisual Media Services Directive published in 2018 sets a daily cap on advertisements of 20% of broadcasting time. Some countries, such as France, Germany and the UK, have implemented even stricter national regulations, particularly on public service broadcasters with ad caps below 10 minutes per hour.==== I utilize a basic free-to-air TV model supported by advertising revenues that yields predictions on how advertising quality is determined by firms and influenced by advertising caps. In this paper, advertising quality expands the demand for an advertised product (via persuasion), while it also reduces the viewers’ nuisance of watching a TV ad. In the extreme, if the advertising quality of a TV ad is sufficiently high, the viewer may even enjoy watching that ad. Within this context, higher ad quality could result from adding valuable entertainment such as humor or upbeat music to the ad, or a celebrity endorsement of the product to be advertised (Ford, 2018, Carrillat, Ilicic, 2019). This may not necessarily be synonymous with a higher art form, but I take the standard assumption that advertising costs increase with ad quality.====The reasoning for a free-to-air model is two-fold. First, free-to-air TV platforms are more likely to be affected by advertising caps than paid services such as cable TV and subscription video on demand. Note that a pay-TV platform by setting lower advertising levels may be able to generate higher subscription revenues, which do not exist in a free-to-air model. In other words, subscription revenues may act as an incentive for platforms to moderate their volume of ads. Li and Zhang (2016) show that pay-TV platforms can broadcast too few ads, while free-to-air TV might act in the opposite manner. Second, historically, free-to-air has represented a significant share of TV viewing across a number of countries. Pay-TV and on demand services such as Netflix and Amazon Prime Video services are taken into account in the model as an alternative option to free-to-air TV (see “Partial participation of viewers” in Section 4).====In the model, viewers are concerned about their total exposure to TV ads. The impact of such exposure on the utility of a viewer depends on the aggregate ad quality, net of nuisance cost, across all ads watched or, equivalently, the net average ad quality multiplied by the volume of TV ads. I show how the average ad quality in a TV platform may be increasing or decreasing with the volume of ads broadcast. The former will be the case if information and persuasion are substitute inputs for firms. In other words, the marginal product of investing in ad quality is lower for firms with a stronger informative effect. Under mild assumptions, the marginal firm can exhibit higher ad quality compared to inframarginal firms with a higher informative effect. Consequently, if the TV platform sells more advertising slots, such sales will increase the average ad quality. If so, an advertising cap confines advertisement slots to firms with a higher informative effect, which are also the firms with higher willingness to pay for an ad slot, to the detriment of firms that would invest in ads of higher quality but are now excluded from the advertising market due to the cap. On the contrary, if information and persuasion are complementary inputs for firms, then a cap will increase the average ad quality because the excluded firms from TV ads are those willing to invest less in ad quality. Also, if ad quality and the number of ad slots purchased by a firm are complementary (substitute) inputs, firms may choose to invest less (more) in ad quality as the advertising fees increase with a tighter cap.====An advertising cap may entail the following welfare effects. A tighter cap incentivizes the TV platform to set higher advertising fees, as a lower fee cannot increase the volume of advertising sales in view of the cap. A tighter cap will, thus, make firms pay more per ad slot. However, if a cap expands the mass of viewers, some firms may boost their sales in the product market and increase profits as a result. The net effect on viewers’ welfare is ambiguous. Although there are fewer ads when an advertising cap is imposed, the average advertising quality may also be reduced. Such possibility suggests that a regulator that is trying to increase social welfare via regulation of the volume of advertising on TV should take the effect of advertising quality into consideration. This paper contributes to the literature on entertainment in advertising by showing that implementing a cap without regard to a potential change in ad quality may result in lower social welfare than leaving advertising airtime unregulated.==== To the author’s knowledge there has been no previous research on the link between regulation limiting the advertising airtime and advertising quality on TV. This paper brings together three topics that have been approached separately in the literature: normative work on the optimal choice of advertising levels; the role of quality (entertainment) in TV advertisements; and how ad quality on TV can empirically be measured.====In the first stream of related literature, seminal normative work on advertising, such as Steiner (1952) and Spence and Owen (1977), tended to focus on the benefits that ads generate to the audience but ignored the surplus obtained by the firms. The assumptions of fixed levels of advertising airtime and prices prevent the analysis of whether the market under- or over-provisions advertisements. Anderson and Coate (2005) explored market failures in the broadcasting industry by modeling how media platforms fulfill their role of providing content to viewers and simultaneously supplying eye-balls to firms. Their work connects the product market to the advertising market and analyzes the trade-off between the nuisance stemming from commercial breaks during the broadcasts and the informational gains generated by the content of these commercials. They show that the market equilibrium may under- or over-provide advertising airtime, depending on the nuisance cost to viewers, the substitutability of programs, and the expected benefits to firms from contacting viewers. My model builds on Anderson and Coate (2005) by allowing firms to invest in ad quality (persuasion). Anderson and Jullien (2015) provide an excellent survey on advertising-financed business models, including normative aspects. A common assumption within this stream of literature is that ads are a nuisance to viewers. In my model, because firms can choose their ad quality, they can reduce ad nuisance (or even turn ads into a net benefit) to viewers.====Over the last two decades, several other articles have studied the welfare effects of advertising caps. On the theoretical literature, Dukes (2004) showed that less product differentiation or more media differentiation leads to higher market levels of advertising. In particular, if media is sufficiently differentiated, the advertising levels will surpass the social optimal level. Rothbauer and Sieg (2015) studied the welfare effects of caps in free-to-air TV channels, one of them being less content differentiation. They identified circumstances in which leaving the market unregulated is optimal. Kerkhof and Münster (2015) showed that ad caps can increase the per-viewer price of ads. Consequently, media content can improve for viewers. If so, welfare can potentially increase, even if viewers are not ad averse. Greiner and Sahm (2018) considered a duopoly of TV channels, which are vertically differentiated in content, and compete in prices for both viewers and firms. They analyzed the case where the high-quality content broadcaster is subject to an advertising ban. They showed that preventing the high-quality broadcaster from selling ad slots leads to fewer viewers watching it. This is because after the ad ban the broadcaster is unable to sell its viewership to firms, relying on subscription revenues only. The two co-authors found that the ad ban can result in a welfare decrease. On the empirical literature, Filistrucchi et al. (2012) analyzed some of the impacts of an advertising ban that happened on the French public TV in 2009. They provided empirical evidence that such ban had no significant impact on the public TV’s share of viewers or on the private TVs’ level of advertising. They suggested that such result can be explained by viewers’ heterogeneity across different TV channels. Zhang (2018), using a dataset from twelve TV broadcasters in France, found that ad deregulation in France would increase profit of TV broadcasters, but could reduce the surplus of both viewers and firms.====The second body of related literature suggests that the quality (entertainment) in TV ads affects the quality of the viewing experience. Aaker and Bruzzone (1985), based on a nationwide sample of viewers that responded to a questionnaire on TV ads in the US, found that viewers experienced lower levels of irritation when ads featured a credible spokesperson, or included humor. Elpers et al. (2003) found that viewers choose to stop watching ads with low levels of entertainment or high information content, while Wilbur (2016) showed that viewers tend to avoid some ads more than others. Teixeira et al. (2014) found that entertainment in ads impacts purchase intentions via persuasion, and increases the ad’s attractiveness. They construe entertainment consistently with other literature on the role of entertainment in ads. In particular, entertainment means that the TV “ad itself is attractive and induces pleasure throughout its viewing”.====The third stream of related literature examines how advertising quality on TV can be measured. Advertising quality is hard to verify and measure. Institutions such as the Australian Broadcasting Tribunal (ABT) have used costs as a proxy for quality. However, the ABT acknowledged that cost should not necessarily be equated with quality (Wright, 1994). Early attempts to generate quality scores for TV ads have relied on surveys to infer the best-liked ads and the most-recalled ads. Since 2007, Google has been exploring ways to measure the quality of TV ads. Google aggregates data describing the precise second-by-second tuning behavior for millions of TV set-top boxes, covering millions of US households, doing so for several thousand TV ads every day. From this data, Interian et al. (2009) and Zigmond et al. (2009) developed measures that can be used to gauge how appealing and relevant commercials appear to be to TV viewers. One such measure is the percentage of initial audience retained: how much of the audience tuned in to an ad when it began airing and remained tuned to the same channel until the ad finishes. More recently, Teixeira et al. (2014) attempted to measure the level of entertainment of TV ads by: (i) filming the viewers’ facial reactions, (ii) checking whether viewers watched the ad until the end, and (iii) asking, after the full or partial view of each ad, whether viewers intend to purchase the brand.====The rest of the paper is organized as follows. Section 2 sets up an advertising-supported TV model that allows to study the impact of ad caps on ad quality and welfare. Section 3 describes the model equilibria and key results. Section 4 discusses in further detail some of the assumptions in the model, and addresses the robustness of the key results. Section 5 concludes the paper. Proofs and further technical details are in the Appendix.",Effects of TV airtime regulation on advertising quality and welfare,https://www.sciencedirect.com/science/article/pii/S0167624520301414,2 October 2020,2020,Research Article,51.0
Varian Hal R.,"Google, 1600 Amphitheatre Parkway Mountain View, CA 94043, United States","Received 2 September 2019, Revised 2 September 2020, Accepted 4 September 2020, Available online 15 September 2020, Version of Record 5 March 2021.",https://doi.org/10.1016/j.infoecopol.2020.100893,Cited by (9),"There is currently a great deal of interest in online competition, particularly involving large tech firms such as Google, Apple, Facebook, Amazon, and Microsoft (a group commonly known as “GAFAM”). In this essay I examine several issues involving these firms that have often come up both in the popular press and academic discussions. The goal of this paper is to examine the facts about the ",None,Seven deadly sins of tech?,https://www.sciencedirect.com/science/article/pii/S0167624520301372,15 September 2020,2020,Research Article,52.0
"O’Connor Jason,Wilson Nathan E.","University of Pittsburgh, United States,Federal Trade Commission, United States","Received 29 August 2019, Revised 7 August 2020, Accepted 22 August 2020, Available online 5 September 2020, Version of Record 5 March 2021.",https://doi.org/10.1016/j.infoecopol.2020.100882,Cited by (10),"We model how a technology that perfectly predicts one of two stochastic demand shocks alters the character and sustainability of collusion. Our results show that mechanisms that reduce firms’ uncertainty about the true level of demand have ambiguous welfare implications for consumers and firms alike. An exogenous improvement in firms’ ability to predict demand may make collusion possible where it was previously unsustainable or more profitable where it previously existed. However, an increase in transparency also may make collusion impracticable where it had been possible. The intuition for this ambiguity is that greater clarity about the true state of demand raises the payoffs both to colluding and to cheating. Our findings on the ambiguous welfare implications of reduced uncertainty contribute to the emerging literature on how algorithms, artificial intelligence (AI), and “big data” in market intelligence applications may affect competition.","In the real world, competition has always occurred under conditions of at least somewhat imperfect information. Firms have not been able to observe the true level of demand nor all competitively relevant actions by their rivals. However, the recent proliferation of large data sets and the availability of algorithmic tools to analyze them have caused some commentators to conclude that the prevalence of uncertainty may be changing, and that these developments have potentially baleful implications for competitive intensity. For example, in October 2018, the US Assistant Attorney General for antitrust intimated that a price-fixing case involving algorithmic price-setting tools might be brought soon.==== At roughly the same time, the UK’s Competition and Markets Authority released a white paper describing risk factors associated with the use of data-driven algorithms.====In this paper, we bring the tools of formal theory to bear on the question of how real-time collection of big data, plus techniques to make timely, more accurate predictions from these, could alter the incidence and character of collusion. To do this, we develop a framework derived from the seminal Green and Porter (1984) model of dynamic competition to consider how changes to firms’ ability to observe the true level of demand may affect market outcomes.==== In our model, a market with two firms is subject to two independent negative demand shocks. We consider the sustainability of collusive equilibrium strategies depending on whether firms have access to a technology enabling them to perfectly forecast or observe one of the shocks. We generally refer to the uncertainty-reducing technology as “AI” in keeping with how the recent literature (e.g., Agrawal et al., 2018) describes the use of artificial intelligence algorithms and data to improve “nowcasting” capabilities.====Like all oligopoly models that fix the number of competitors and abstract from entry and exit, our framework is a stylized abstraction from the “real world.” However, our model of competition with separate sources of demand uncertainty could characterize many industries where final prices and quantities are imperfectly observable by rivals. Indeed, the possibility that a market may experience multiple independent shocks can arise any time demand for a given product is a multivariate function of (at least partially) stochastic factors. One common demand factor is consumer income, which is subject to shocks that are likely independent from shocks to a different typical factor, the availability of complementary or substitute products. This rough pattern characterizes disparate industry settings. For example, the spot market for coal is affected by uncertain demand factors like the weather, which shifts demand for electricity. An unexpectedly cool summer will depress coal demand relative to the norm by reducing the use of air conditioners. Simultaneously, demand for coal will also be impacted by fluctuating supply conditions in alternative sources of power. An unexpectedly wet winter may mean that hydro power is likely to take a larger than typical share of the overall electricity market. Similar dynamics can be seen in as different a market from coal as online retail. There, demand will be affected by shocks to the overall economy that affect consumers income and consumption priorities. Simultaneously, changes affecting the desirability of buying through brick and mortar outlets will affect demand for products available via online distribution, as seen in the 2020 COVID-19 pandemic.====In both the coal and online retail examples, market participants likely can infer something about the incidence of demand shocks, but there will remain some degree of uncertainty about their realized magnitudes that leaves open other possible explanations for lower than expected sales. In particular, it will be difficult to rule out that low sales reflect undercutting on price by a rival firm. For example, an individual coal seller may have priors about the prices offered by rival mining companies but will not be able to observe the outcome of individual sales to utilities. Similarly, even if an online retailer’s list prices are public, they may engage in large-scale targeted private discounts that enable them to capture more of the market. Thus, in either setting, a firm’s failure to achieve the sales it expected may reflect an unobserved decline in demand or undercutting. Sellers will not be able to perfectly distinguish these possibilities insofar as they only directly observe their own sales and profits. Algorithms that can reliably collect and analyze data correlated with the true state of demand could reduce some of this uncertainty.====The model that we develop in this paper shows that the exogenous adoption of nowcasting AI by sellers has ambiguous implications for both firms and consumers. On the one hand, reduced demand uncertainty may benefit firms in many cases. By clarifying what the true demand conditions are, AI enables firms to more precisely differentiate rivals’ cheating from unobserved negative demand shocks. Furthermore, increased clarity about the true state of demand allows colluding firms to better tailor their collusive prices to demand conditions, increasing the average per period profit earned during periods of collusion. These factors may make collusion sustainable when it previously was not, and also may cause the duration of punishment to shrink relative to the duration required in the pre-AI period (assuming collusion was possible then). On the other hand, once firms have better knowledge of the true state of demand, they may better time their decision to deviate from a collusive agreement. All else equal, this knowledge can make collusion unsustainable even if it had been sustainable before AI. The net effect of the coordination-facilitating and coordination-inhibiting effects of greater transparency will depend on market characteristics as described by the market’s location in the model’s parameter space.====To provide greater insight into how different market characteristics interact in sustaining collusion in the two information environments, we implement our model in a linear demand setting. In this context, we show that consumers gain most from the introduction of AI when the probability of the now forecastable shock is large relative to the probability of the one that is still unobservable. Returning to the coal example, this might occur if weather prediction algorithms end up being better able to predict local temperatures than rainfall in areas with hydro plants. If the likelihood of cool temperatures is large relative to the likelihood of higher rainfall, then our model suggests the introduction of AI is likely to make collusion unsustainable. Specifically, the firms will want to take advantage of rare hot (i.e., high demand) seasons and undercut the collusive price.====As in other treatments of collusion, we also find that consumers are more likely to gain, all else equal, when firms’ (common) discount rate is lower. Finally, we show that whenever collusion is sustainable both before and after AI, producer surplus increases while consumer (and total) surplus declines.====In extensions, we show that our baseline results are qualitatively robust to modifying our model to allow colluding firms to optimize simultaneously over the collusive prices in addition to the length of punishment periods. However, there are some differences when prices and punishment are optimized in tandem relative to our baseline model. In particular, if firms are allowed to collude at prices below the monopoly level in the post-AI environment, the possibility of a total breakdown in coordination is dramatically reduced. That said, in some cases where collusion is sustainable pre-AI the prices needed to sustain collusion after AI adoption must be set so low that consumer surplus increases.====Overall, we contribute to the large literature on conditions conducive to collusion and coordinated conduct (Tirole, 1988, Ivaldi, Jullien, Rey, Seabright, Tirole, 2003, Kovacic, Marshall, Marx, White, 2011), particularly with respect to the role played by uncertainty (Robson, 1981, Green, Porter, 1984, Kandori, 1992, Raith, 1996, Athey, Bagwell, 2001). Our work shows that reducing uncertainty has ambiguous effects, making collusion more attractive in some cases, but also sometimes more difficult to sustain. These results dovetail with the equally ambiguous results in an emerging empirical literature considering the connection between transparency and collusion. In this literature, some papers, like Lemus and Luco (2019), find that increased transparency may hinder coordination. However, other contributions, such as Byrne and De Roos (2019), have found evidence that increased transparency may facilitate coordination.====In addition, our paper contributes to the ongoing debate about how antitrust policy should address the competitive effects of recent developments in data analytics. This literature already contains contributions reflecting a wide variety of opinions (see, e.g., the partial bibliography of Ritter (2017)), but many of the most vocal commentators have suggested that these phenomena threaten consumer welfare. In our view, these critics have focused on two related, but distinct, theories of harm: (1) an increased ability to personalize prices will allow firms to better extract consumer surplus (Ezrachi, Stucke, 2016, Ezrachi, Stucke, 2017), and (2) the use of algorithms will facilitate collusion (Ezrachi, Stucke, 2016, Mehra, 2015).==== While our paper only addresses the latter theory, we worry that the critics have proceeded to judgment without necessarily establishing a rigorous basis for either theory of harm in the economic or computer science literatures.====By rooting consideration of AI in the game theoretic treatment of uncertainty and coordination, our paper shows that the effects of AI and related technologies are more nuanced than most commentators have heretofore considered. Our results show that even without endogenizing entry, exit, and repositioning on the supply-side, let alone demand-side technological adaptation (Gal and Elkin-Koren, 2016), the implications of improved analytical capabilities are varied, and depend heavily on market primitives. This nuanced conclusion stands in contrast to much of the discussion in the policy-oriented literature.====Within the literature on algorithms and competition, our paper fits with other emerging contributions taking a more technical approach (Ittoo, Petit, 2017, Kuhn, Tadelis, 2017, Calvano, Calzolari, Denicolò, Pastorello, 2019, Miklós-Thal, Tucker, 2019, Harrington, 2020). In particular, our work is similar in motivation to that of Miklós-Thal and Tucker (2019), who also assume algorithms allow firms to better predict demand rather than directly set prices. However, their model extends the Rotemberg and Saloner (1986) model of collusion where firms rely on a signal of future demand in setting prices, while we modify Green and Porter (1984). An important difference between our modeling frameworks is that we make different assumptions about competitors’ ability to monitor adherence to the collusive strategy. Whereas Miklós-Thal and Tucker (2019) focus on markets where firms can perfectly monitor each other, we consider situations where firms can only imperfectly infer each others’ behavior by considering market outcomes. This means that the arrival of uncertainty-reducing technologies not only enables more tailored pricing but also improves monitoring. Despite our distinct modeling frameworks, both we and Miklós-Thal and Tucker (2019) find that AI has ambiguous effects on consumer welfare and profits. That our respective papers reach broadly similar conclusions supports a cautious policy approach on the potential threat of AI to consumer welfare.====The paper is organized as follows. Section 2 outlines the modeling framework. Section 3 compares the equilibria that will result in the pre- and post-AI states. Section 4 presents the results of a less restrictive version of our model, which allows firms to set collusive prices and punishment periods simultaneously. Section 5 briefly discusses the validity of thinking of AI adoption as exogenous. Finally, Section 6 concludes.",Reduced demand uncertainty and the sustainability of collusion: How AI could affect competition,https://www.sciencedirect.com/science/article/pii/S0167624520301268,5 September 2020,2020,Research Article,53.0
"Gautier Axel,Lamesch Joe","HEC Liege, University of Liege and LCII, Belgium,CORE (UCLouvain) and CESifo, Germany,Luxembourg Competition Authority, Belgium","Available online 2 September 2020, Version of Record 5 March 2021.",https://doi.org/10.1016/j.infoecopol.2020.100890,Cited by (17),"Over the period 2015–2017, the five giant technologically leading firms, Google, Amazon, Facebook, Apple and Microsoft (GAFAM) acquired 175 companies, from small startups to billion dollar deals. In this paper, we provide detailed information and statistics on the merger activity of the GAFAM and on the characteristics of the firms they acquire. One of the most intriguing features of these acquisitions is that, in the majority of cases, the product of the target is discontinued under its original brand name post acquisition and this is especially true for the youngest firms. There are three reasons to discontinue a product post acquisition: the product is not as successful as expected, the acquisition was not motivated by the product itself but by the target’s assets or R&D effort, or by the elimination of a potential competitive threat. While our data does not enable us to screen between these explanations, the present analysis shows that most of the startups are killed in their infancy. This important phenomenon calls for tighter intervention by competition authorities in merger cases involving big techs.","The five largest tech giants, Apple, Alphabet (Google), Amazon, Facebook and Microsoft, known as GAFAM, are among the largest market capitalization firms worldwide. Operating as multi-sided platforms, they have created a large ecosystem of products, applications, services, content and users. They generate value by offering services to the various user groups gravitating around the platform and by enabling interaction between and within them.====The GAFAM have known tremendous internal and external growth over the last two decades. Their investment in research and development is huge with a cumulated investment of over $ 71 billion for the year 2017. In addition to these important investments, they have an extremely intense mergers and acquisitions (M&A) activity. In 2017, for instance, they made 55 (different) acquisitions altogether, most of which were young and innovative startups.====There are several reasons for one of the GAFAM platforms to acquire an innovative startup. First, the platform might be interested in the products developed by the startup. The GAFAM have developed a large ecosystem of products and services and are increasingly competing for attention, i.e. to retain consumers on their platform. In this context, adding new products or functionalities is part of the competitive process, acquisition therefore is one way of developing the firm’s ecosystem. Second, the platform might be interested in the startup’s inputs. They, indeed, have valuable assets (innovation, patent, engineer, talent====, customer base) that could be of interest to the platform. Last, acquisition may be a way of restricting competition and consolidating the platform’s position on the market. As, in the digital economy, an important source of value comes from network effects, a firm with a substantial user base can eventually turn into a competitor of the incumbent network even if at the time of its acquisition there was no product overlap.. Hence, the preemptive acquisition of a small and promising startup can be used to restrict potential competition on the market. Nowadays, there are growing fears that the GAFAM acquire startups to protect their already strong market position.====Despite their intense merger activities and the vivid debates they generate, little is known about the GAFAM’s merger strategies. The present research ambitions to fill this gap. To this end, we have collected detailed information on the acquisitions of the GAFAM over the years 2015–2017 and on the GAFAM themselves. We have extracted all the necessary information from the firms’ 10-k files==== and the Crunchbase database.====In this paper, we provide detailed information and statistics on the GAFAM’s merger activity and on the characteristics of the firms they acquire. We focus in particular on the age, the funding and the origin of the target. We also identify the products they offer. To that end, we classify products in segments broadly defined according to the group of customers targeted. Six different user segments are identified: products offered to advertisers, businesses, consumers, merchants, content editors and platform products (mainly hardware and operating systems).====This product classification is used to identify the main segments of the platforms and their main income source. Given their multi-sided nature, some segments do not directly generate revenues for the platform. This is particularly true for social media, Facebook and Google, for which users are extremely important. However, matching revenue with segments is important to identify the money side of the platform. For all the GAFAM, the revenue streams are extremely concentrated with most of the revenues coming from one or two segments: platform products (devices) for Apple, merchants for Amazon, advertising for Facebook and Google, business and platform products for Microsoft.====In a second step, we classify acquisitions and allocate each to one of the six business segments. Unsurprisingly, we observe that the firms acquire a lot in their main income segment. For instance, Microsoft used acquisitions to reinforce its business offers with 65% of the acquisitions in this segment. We also observe that there are two segments where the merger activity is quite intense: the digital content segment with 26% of all acquisitions and all firms being extremely active and, the business segment, where all firms, except Facebook and to a lesser extent Apple, make a lot of acquisitions. The intense merger activity in these two segments could be a sign of increasing rivalry for business customers and for digital content.====We further analyze the acquisition strategies of the GAFAM firms by looking at the evolution of the target post-acquisition. We observe that in the vast majority of cases, the acquirer discontinues the acquired brands. A product is considered to be discontinued if it is no longer supplied, maintained or upgraded under its original brand name. This practice is far from being systematic in the digital world and there are plenty of examples of products which continue to be supplied under their original name after an acquisition by one of the GAFAM.==== In our sample, we observe that in more than 60% of the acquisitions, the acquired products were discontinued. Apple and Facebook seem to have a more systematic discontinuation policy than the other firms.====There are three main reasons to discontinue a product post-acquisition. First, the product may not be as successful as expected and the acquirer gives up the project. Second, the motivation for the acquisition was not the product or the brand in itself but the assets of the company or its innovation effort. Following the acquisition, the targeted assets are transferred to the acquirer and the target is shut down. Puranam and Srikanth (2007) explains that when acquisition is motivated by asset acquisition, the target is more likely to be integrated with the acquirer while when it is motivated by product acquisition, the target is more likely to be kept independent. Last, the product may be discontinued to protect the acquirer’s market position. Such a merger followed by the disappearance of the acquired firm is now referred to as a killer merger. The firm acquires a target which develops a technology that can be used to compete with its own products in the future and the acquisition kills the competitive threat.==== Killing rather than continuing a project competing with the acquirer’s own product depends on the existence of demand and supply side complementarities. With strong complementarities, the acquirer is better off if it continues to develop the acquired project and supplies it along with its own product. Otherwise, the acquirer is better off killing the project and only develops its own version of the product.====We run Probit regressions to better understand the determinants of product discontinuation. In our estimation, the age of the target appears to be an important determinant of product discontinuation: younger firms are more likely to be discontinued. We also find that acquisitions in the platform’s core segment, defined as the main income segment plus the user segment for the social medias, are more likely to be discontinued than acquisitions in the other segments. This suggests that products which are more closely related to the (broadly defined) main products of the platform are more likely to be discontinued. However, from our data, we cannot screen between the two explanations for product discontinuation: technology acquisition or the elimination of a potential rival. A more detailed analysis, product by product, should be carried on to understand the motivations for the merger. But our paper shows that most of the startups are killed in their infancy and this important phenomenon calls for a tighter intervention by competition authorities in merger cases involving big techs.====In the literature, there is, to our knowledge, no systematic analysis of the merger activity of the main digital platforms, Argentesi, Buccirossi, Calvano, Duso, Marrazzo, Nava, Argentesi, Buccirossi, Calvano, Duso, Marrazzo, Nava being exceptions. Both papers make a critical assessment of several merger decisions taken by the Competition Market Authority (UK) in the digital economy, and suggests reforms to take better account of the specificities of digital markets. Furthermore Argentesi et al. (2019b) systematically review the mergers of Google, Amazon and Facebook (GAF) for the period 2008–2018. They classify mergers into eight segments, not according to the targeted user group as we did, but according to the products’ purpose or functionality. They observe an intense acquisition activity in the AI, data science and analytics segments which raise concerns as data analytics technology combined with the huge amount of data collected by the GAF may constitute a barrier to entry for competitors. Finally, their analysis converge with ours in noticing that Google has a more intense and more diversified acquisition strategy than Amazon and Facebook who have a more focused acquisition pattern.====Few papers explicitly consider the striking features of the digital economy in a merger model. Motta and Peitz (2020) develop a model of acquisition by big tech firms. In their set-up, the startup (the target) is potentially financially constrained and may lack of the necessary resources to complete its innovative project. Acquisition by a less financially constrained big tech may remove this financially constraint and brings the new project to an end. Acquisition, however, has two drawbacks. First, the big tech may acquire the startup and stop the project (a killer acquisition). Second, acquisition could occur despite the fact that the startup has enough ressource. In this case, the innovative project would be developed when the startup remains independent and acquisition only reduces competition on the market. Finally, on the basis of their modeling, Motta and Peitz develop theories of harm that integrate specific features of the digital economy like zero-price products or network effects.====Prat and Valletti (2019) develop a model of attention oligopoly in which platforms that may ==== look like different are competing for the attention of the targeted consumers, attention that will be sold to the advertisers and retailers. In this context, they consider a merger between two competing networks and show that the larger the overlap between the user bases, the larger the welfare losses resulting from the merger. Indeed, a merger between overlapping networks is more detrimental than a merger from dissociated networks. Hence, a merger between two networks offering different products to the same user groups can be used to substantially restrict competition on the market, even if the products offered to capture consumer attention are different.====Recently, the literature has considered the impact of a merger on innovation efforts.==== Cabral (2018) develops a model where tech giants are competing with fringe firms. The focus of the model is on innovation and the impact of mergers on incentives to innovate. He distinguishes between radical and incremental innovations, showing that mergers decrease the former but favor the latter. The idea is that incremental innovation has more value if it is transferred to the dominant firm, as is the case of a merger. Anticipating a transfer, the startup partially internalizes the full benefit of its innovation and has more incentives to invest. On the contrary, startups have fewer incentives to invest in radical innovations that would allow them to replace the dominant firm. The reason is that increasing the benefit of incremental innovation also increases the opportunity cost of a radical innovation. Therefore, a merger may boost investment yet also reinforce the incumbent’s dominance. Bryan and Hovenkamp (2020) reach a similar conclusion. They develop a model of startup acquisitions by dominant firms where startups innovate and develop components to be used by a tech giant. They show that technological leaders have more incentives to buy the startups to maintain their leadership and that this persistence of leadership through acquisition may not be welfare improving. Furthermore, startups may bias their research efforts towards the improvement of the technological leader, and in so doing reinforce its leadership.====Complementarities are important in the digital economy as many startups develop products or features that are complements to the platform’s ecosystem. Wen and Zhu (2018) show that the entry threat of the platform in a complementary market changes the incentives to innovate and the complementor’s pricing strategy. Rather than entry, a platform can buy the complementor to expand its ecosystem. Etro (2019) shows that such a merger between complements increases the innovation effort, as it solves the Cournot complement problem but restricts competition by making entry less likely.====The paper is organized as follows. In Section 2, we present the platform’s business model, the users group gravitating around it and the main revenue sources. In Section 3, we provide detailed information on the GAFAM firms’ merger activities over 2015–2017. In Section 4, we analyze the product continuation decision and we conclude in Section 5. In the appendices, we describe the data source in greater detail (Appendix A), provide a list of acquisitions (Appendix B) and additional statistics (Appendix C).",Mergers in the digital economy,https://www.sciencedirect.com/science/article/pii/S0167624520301347,2 September 2020,2020,Research Article,54.0
Katz Michael L.,"University of California, Berkeley, CA USA","Received 16 March 2020, Revised 22 August 2020, Accepted 26 August 2020, Available online 1 September 2020, Version of Record 5 March 2021.",https://doi.org/10.1016/j.infoecopol.2020.100883,Cited by (22),There is broad concern that merger policy toward Big Tech has been too lenient. Big Tech typically operates in markets characterized by innovation-driven “competition for the market.” I show that this fact provides a rationale for heightened scrutiny of incumbents’ acquisitions of emerging or potential competitors. I also address the widespread argument that permissive merger policy promotes innovative entry by facilitating entry for buyout. I show that permissive merger policy can also discourage entrant innovation. One way is by diminishing entrants’ incentives to invest in marginal product improvements when such improvements reduce the gains from merger. A second way is by facilitating i==== under which an incumbent makes investments in order to extract rents from an entrant through merger.,"Commentators across the political spectrum have called for new and more forceful approaches to antitrust enforcement with respect to Big Tech—especially Amazon, Facebook, and Google. A wide range of proposals have been put forth, including breaking firms up, subjecting them to pervasive regulation, and being much more wary of mergers between incumbents and either recent or potential entrants.==== The last concern is driven, in part, by the combination of apparently dominant market positions coupled with large numbers of acquisitions. For example, Amazon is estimated to have accounted for almost 40 percent of all 2019 ecommerce sales in the U.S., and Facebook and Google together account for over 60 percent of U.S. digital ad spending.==== These companies made hundreds of acquisitions in the previous decade.====To determine if acquisitions by Big Tech are a big problem that justifies heightened or different scrutiny, it is useful to begin by asking: what is special about these firms, beyond their tremendous success? One answer is that all are in industries with very strong increasing returns and positive feedback loops. There are several sources of increasing returns—often present simultaneously. One source is network effects. Another is the collection and use of big data, which can give rise to economies of scale, scope, and experience. Lastly, the creation of software and intellectual property (including proprietary hardware designs) is typically characterized by large economies of scale, with high fixed costs and very low marginal costs.====The presence of such strong increasing returns can limit the number of viable competitors and even create a tendency to tip toward monopoly. When increasing returns are large relative to product differentiation, competition may be ==== the market rather than ==== the market. Competition for the market, whereby firms compete by innovating to attain temporary market dominance, is often referred to as ====The need for scale can make entry difficult in the very markets in which entry is critical because competition takes place for the market. In a market in which a large base of users is essential to a firm's ability to offer an attractive value proposition—say because network effects are strong and/or the value of user big data is high—the only economically viable means of entry may be to build up a base of users in an adjacent market and then provide the new service to that base of users—what is sometimes called a two-stage entry strategy.==== Some commentators believe that Instagram and WhatsApp would have used two-stage entry to become strong competitors to Facebook in social networking if that firm had not acquired them in 2012 and 2014, respectively.====Another strategy, to use alone or in conjunction with two-stage entry, is to offer a product with higher quality than that of the incumbent firm. Most directly, an entrant's quality advantage may outweigh its scale and installed-base disadvantages. There can also be an indirect effect. In markets subject to strong network effects, a firm may gain significant competitive advantage from favorable consumer expectations—when consumers expect that firm to have high sales, their expected value of patronizing that firm rises due to the anticipated network benefits associated with a larger user base. One might expect incumbents generally to have expectations advantages. However, a highly visible innovation might tip expectations in favor of an entrant.==== If this pattern prevails, then leapfrog innovation might allow an entrant that is small today to generate a positive feedback cycle and overcome an incumbent's various scale advantages.====Both two-stage and innovative entry strategies require the entrant to amass complementary resources (e.g., users and intellectual property) to have a chance of overcoming the advantages of incumbency. An entrant also needs a strong growth trajectory that allows it to achieve viable scale. An entrant's need to acquire complementary assets and attain a promising growth trajectory may allow an incumbent to identify potential rivals before they become major actual competitors.====Below, I explore the role of merger policy in a model in which competition is for the market and an incumbent can identify—and merge with—an emerging or potential competitor before the entrant becomes the new dominant firm. After Section 2 briefly reviews related literature, Section 3 presents the overall analytical framework: a discrete-time, infinite-horizon game in which each period a new potential entry opportunity arises with exogenous probability. Entry requires making a sunk investment in product development. If entry occurs and the firms do not merge, then the incumbent and entrant compete for the market, and the market is ultimately monopolized as one of the firms is driven to exit.====Section 4 argues that incumbents’ acquisitions of emerging or potential competitors should be subject to heightened antitrust scrutiny when competition is for the market. Entry is a critically important means of promoting market performance under Schumpeterian competition but acquisition of a nascent competitor can be an especially effective way to avoid Schumpeterian competition, to the detriment of consumers. Marino and Zábojník (2006) have shown that, when competition is ==== the market (i.e., multiple incumbents can be profitable simultaneously), the threat of rapid entry can sometimes serve as a substitute for merger policy by making merger unprofitable absent efficiencies. To see why, suppose that there are two incumbents. In the absence of additional entry, merging to monopoly raises profits by eliminating product-market competition. However, there can be an offsetting ==== when the two firms merge, they may create room for a subsequent firm profitably to enter the market because they weaken their bargaining position—instead of collectively receiving two-thirds of the continuation profits in symmetric bargaining over merger with the entrant, they receive only half. When subsequent entry is rapid, the share-dilution effect renders mergers unprofitable absent efficiencies. By contrast, there is no share-dilution effect when competition is ==== the market: even absent merger, the market is monopolized after a period of competing for dominance following entry. Because it eliminates a period of competition to be the dominant firm, merger is profitable even if ensuing entry is rapid.====Sections 5-8 examine the effects of merger policy on potential entrants’ pre-merger innovation incentives. When mergers are banned, entry occurs only when a potential entrant has a sufficiently valuable innovation that it can overcome any incumbency advantages and become the new dominant firm. As Rasmussen (1988) identified, when mergers are feasible, a firm may enter the market solely to induce the incumbent to purchase the entrant in order to avoid dissipating product-market profits through competition—so-called ====.==== It is often argued that a benefit of allowing mergers is that, by facilitating entry for buyout, they can promote innovative entry.==== Below, I show that allowing mergers can also discourage entrant innovation.====As long as the incumbent's actions conditional on not merging are independent of whether mergers are permitted, the option to merge can only increase an entrant's profits. This finding, however, does not imply that this option everywhere increases an entrant's incentives to ==== improve its product. In fact, the possibility of merger may diminish innovation by increasing the relative profitability of entering with a less ambitious product. Moreover, the option to merge might reduce an entrant's profits by facilitating ==== whereby an incumbent invests in strengthening its competitive position solely to induce the entrant to merge on more favorable terms. Finally, a permissive policy that also applies to future mergers can reduce a current entrant's profits by facilitating entry for buyout by subsequent entrants.====Section 5 analyzes the effects of merger policy on an entrant's incentives to invest in marginal product improvements. When there is no incumbency for buyout and certain other conditions are satisfied, an entrant's disagreement profits when bargaining over whether to merge equal its profits when mergers are banned. Hence, the difference between the entrant's profits with and without the possibility of merger equals the entrant's share of the gains from merger. The effect of permissive merger policy on innovation incentives thus depends on how a change in the entrant's product quality affects those gains. In general, increasing the entrant's product quality can raise or lower the gains from merger, which implies that option to merge can raise or lower an entrant's incentives to invest in quality.====The analysis summarized above relies on reduced-form profit functions. Sections 6-8 present examples in which product-market competition is explicitly modeled. These examples illustrate how permissive merger policy can harm innovation incentives and that the competitive effects of mergers can be complex and highly fact specific. The example of Section 6 exhibits same-side network effects, such as arise with social networks. Among other things, this example demonstrates that, in situations where the merged firm retains the older technology (to avoid splitting users and losing network benefits), an increase in the entrant's product quality lowers the gains to merger. Intuitively, the higher is the entrant's product quality, the greater the opportunity cost of not using it. The example of Section 7 considers a market without network effects in which a firm must make a new investment each period in order to be an active seller. In this example, the entrant chooses a higher product quality when mergers are banned because the possibility of merger leads the entrant to put weight on the effects of its innovation on monopoly profits, which are less sensitive to the entrant's quality than are the profits the entrant earns when competing with the incumbent. Section 8 shows by example that permissive merger policy can facilitate incumbency for buyout that discourages innovative entry.====Section 9 discusses some policy implications of this analysis. Although acquisitions of emerging competitors in Schumpeterian markets should be subject to heightened scrutiny generally, in some cases merger promote innovation and efficiency. It is thus important to examine the facts of each case at hand even if doing so is difficult.====Finally, a technical appendix examines a benchmark case of ==== entry to compete ==== the market, which serves as a benchmark for the text's model of innovative entry to compete for the market.","Big Tech mergers: Innovation, competition for the market, and the acquisition of emerging competitors",https://www.sciencedirect.com/science/article/pii/S016762452030127X,1 September 2020,2020,Research Article,55.0
"Jullien Bruno,Sand-Zantman Wilfried","CNRS, Toulouse School of Economics, University of Toulouse Capitole, 1 Esplanade de l’Universite, Toulouse Cedex 6 31080, France,Toulouse School of Economics, University of Toulouse Capitole, 1 Esplanade de l’Universite, 31080 Toulouse Cedex 6, France","Received 14 January 2020, Revised 17 July 2020, Accepted 11 August 2020, Available online 17 August 2020, Version of Record 5 March 2021.",https://doi.org/10.1016/j.infoecopol.2020.100880,Cited by (26),"We propose an analysis of platform competition based on the academic literature with a view towards competition policy. First, we discuss to which extent competition can emerge in digital markets and show which forms it can take. In particular, we underline the role of dynamics, but also of platform differentiation, consumers multi-homing and beliefs to allow competition in platform markets. Second, we analyze competition policy issues and discuss how rules designed for standard markets can perform in two-sided markets. We show that multi-sided externalities create new opportunities for anti-competitive conducts, often related to pricing and contractual imperfections.","Designed primarily as a means of communication, computers and the internet have revolutionized the way we produce and exchange services and goods, whether they are digital or not. As a consequence, at the turn of the century a first wave of digital firms like Apple and Microsoft have paved their ways among the major companies in the world. Then, another wave has emerged in the last 15 years, with companies like Amazon, Alibaba, Facebook, Google, Airbnb, Baidu, Booking.com or Uber, also leaping into the front ranks of the world’s corporate giants.==== These companies rely on different business models but all aim at facilitating interactions between individuals and/or firms, and as such they belong to the same general category, now known as platforms. The objective of this article is to take some insights from the academic literature first to understand how platforms compete and second to discuss some elements of competition policy in the digital world.====As a preamble, let us define more precisely what we mean by a platform.==== For this consider the firms mentioned above: Facebook is a social media website, Google a search engine, Amazon and Alibaba are marketplaces, whereas Uber matches drivers and riders. Despite these very different activities, all these firms have built their models to act as an intermediary. Platforms, just like classical intermediaries, act as matching devices, allowing each side of the market, or at least one side of the market, to find the best agent on the other side—that is, the one that generates the highest surplus. The fact that so much information is available on the internet facilitates this matching process. The emergence of matching platforms is part of the “dis-intermediation” process that has led to potential separation of information management from production/storage and delivery. Hence, some platforms are pure “infomediaries”, focusing on information, acting as aggregators (like TripAdvisor) or sharing platforms (Airbnb). Others (like Amazon Marketplace) provide matching as well as support for transactions and delivery. But a common feature that distinguishes these platforms from traditional resellers is that they allow autonomy for agents on both sides of the market to meet and define trade (see Hagiu and Wright, 2014).====Consequently, potential users of a platform are often concerned about the size of the other side’s population to which they will be matched. A seller is more likely to visit a platform if there are many potential buyers, and vice-versa. These network effects explain why the market tends to favor large firms. Even if network effects existed before the emergence of digital firms, for example with telephone communication systems, they often operate differently in the case of platforms. Indeed, potential sellers value a platform more if there are more buyers, not if there are more sellers. So platforms are able to generate or take advantage of inter-group or multi-sided network effects (or indirect network effects) rather than intra-group or direct network effects. Through network effects, every agent, by subscribing to a platform, generates value for some other agents on the platform, which can be positive or negative. This generated value is a common feature of platforms and what are now called two-sided markets.====Regarding pricing, some groups are important for the other sides, whereas other groups are less important—for example, users are essential for advertisers on a platform while users may care less about advertisers. If a platform can identify each user, and in particular the group to which each user belongs, then it can price the various groups differently. One important subject of study is how platforms set their prices on the different sides of the market, and we will see that the value of the externality is a key component of the optimal pricing formula.==== The presence of network effects also increases the significance of the beliefs every agent holds about the future behavior of the other agents. Since the benefit every agent gains depends on the number of agents he can trade with, there is a potential coordination issue when it comes to choosing whether to join a platform or which platform to join. This means that the success or failure of a platform depends to a large extent on the beliefs consumers hold about its future success or failure. This belief dependency explains the additional uncertainty that exists about the fate of platforms, compared with firms in more standard markets. The problem is even more acute when a firm attempts to enter a market where there is already another firm. In contrast to standard markets where agents can be gained individually, it is necessary to attract groups of agents, triggering collective moves and achieving a critical mass. Success here depends on the ability to set smart prices but also to change consumers’ beliefs about market dynamics.====The first part of this article aims at reviewing the major theories of platform competition, to understand whether and how effective competition can occur between platforms. There is growing debate about the possible break-up of many so-called “dominant platforms,” in particular Amazon, Facebook and Google. So the issue of the economic sustainability of competition in those markets needs to be addressed. More precisely, we will discuss the features that make a market more likely to be monopolized. When the market in which platforms operate is likely to be a monopoly, we discuss the extent to which this market can be contested. Indeed, the fact that a market is a monopoly does not prevent contesting the position of the monopolist. While this discussion will concern markets with “tipping” on one platform, the lessons will also be insightful about the ability of new entrants to emerge in markets that can only accommodate a few firms. If the market under scrutiny is not a monopoly, it will be crucial to analyze in more details the different forms platform competition can take. The most common form to be analyzed is price competition, and we will discuss the likely competitive prices and their normative properties. We will see how price competition between platforms depends on the behavior of the agents, in particular on their decision to stick to one platform – i.e., to “single-home” – or to potentially visit more than one platform – i.e., to “multi-home.” A last, we will emphasize the role played by consumers’ belief in shaping platform competition.====The second part of this article will focus on competition policy issues in two-sided markets. As competition is not solely based on prices, we will have to discuss some non-price dimensions related to the design of the service chosen by competing platforms. Markets where platforms are prevalent have been scrutinized by many competition agencies over the past 20 years and some conducts have been considered anti-competitive. However, the rules generally used to assess the competitive or anti-competitive nature of firms’ policies have been designed for standard markets. We will therefore revisit competition policy issues in the context of two-sided markets where the competing firms are platforms. In particular, we will discuss the notion of market power, the essential definition of markets, and the consequences of practices such as tying, exclusionary pricing, exclusivity, foreclosure, collusion, and mergers. We will see that multi-sided externalities create new opportunities for anti-competitive conducts. These new conducts can often be related to some pricing or contractual imperfections, in particular zero-pricing and imperfect coordination between sides.",The Economics of Platforms: A Theory Guide for Competition Policy,https://www.sciencedirect.com/science/article/pii/S0167624520301244,17 August 2020,2020,Research Article,56.0
Gilbert Richard J.,"Emeritus Professor of Economics, Univerity of California, Berkeley United States","Received 9 January 2020, Revised 8 June 2020, Accepted 9 June 2020, Available online 16 June 2020, Version of Record 5 March 2021.",https://doi.org/10.1016/j.infoecopol.2020.100876,Cited by (14),"Political candidates, legislators, and academics have made proposals to separate services provided by dominant digital platforms from activities that rely on these services. Although the platforms have different economic and technical characteristics, common themes that motivate these proposals are the incentives of platforms to favor their own products and to suppress investment by imitating rivals. As has been shown in other contexts, this paper demonstrates that structural separation does not eliminate incentives for platforms to discriminate in the provision of service quality. Furthermore, the ability of vertically integrated platforms to imitate rivals does not necessarily harm consumers. Structural or functional separation can address some complaints lodged against activities by dominant platforms, but experience demonstrates that separation requirements are difficult to administer and can harm innovation. Public policy should rely on a mix of antitrust enforcement and regulation to address concerns about privacy, data security, and potential influence of major platforms in politics and the media, as well as the abuse of market power.","The ability of the major digital platforms to engage in conduct that can harm rivals and distort competitive outcomes has led to calls for major reforms. Senator Elizabeth Warren proposed that companies with an annual global revenue of $25 billion or more and that offer to the public an online marketplace, an exchange, or a platform for connecting third parties should be regulated as “platform utilities.” These companies would be prohibited from owning participants on the platform. Platform utilities also would be required to meet a standard of fair, reasonable, and nondiscriminatory dealing with users and would not be allowed to transfer or share data with third parties.==== This proposal would classify Amazon Marketplace, Google Search, and Google’s ad exchange as platform utilities and would require them to operate as separate entities.====Others, including legal scholars and Representative David Cicilline, the current head of the House Antitrust Subcommittee, have expressed concerns about anticompetitive conduct by vertically integrated platforms.==== There are also calls for regulators to unwind acquisitions by the major platforms that have reinforced their dominance, such as Facebook’s acquisitions of Instagram and WhatsApp and Google’s acquisition of YouTube and the ad manager DoubleClick. Courts have the power to dissolve mergers, but that has rarely occurred if the merging parties notified the antitrust authorities prior to the merger and cooperated with the authorities during the approval process (Patel, 2020).====My focus in this paper is on the economic arguments for and against structural or functional separation of services provided by the major digital platforms. I use the term “digital platform” loosely because companies that operate in the internet economy have different business models and provide different types of services. Some, such as Google and Facebook, are multi-sided platforms that generate revenues from advertisers and connect them with potential consumers. Others, such as Amazon, provide online merchant services that are analogous to services provided by traditional retailers. I apply the term “dominant” to a digital platform in this paper if it provides services for which there are no close substitutes that others rely on to develop or market their own products or services. The stylized models in this paper do not address interactions between different sides of a platform, although a more formal analysis should account for these effects if they are relevant to antitrust allegations.====Structural separation refers to the division of operations into separate companies. Functional separation allows for operations to be conducted by subsidiaries of the same company subject to restrictions on the transfer of information, intermediate products, or services between the subsidiaries. From the perspective of economic efficiency, there are three reasons to pursue structural or functional separation:====Antitrust authorities often condition approval of a merger or acquisition on an agreement to divest assets to restore competition and innovation lost by the proposed transaction. Divestiture is a preferred merger remedy because it breaks apart what should not have been joined together in the first place. In contrast, U.S. courts have been reluctant to dissolve existing entities to remedy the abuse of monopoly power (Waller, 2009). European Union antitrust law allows for structural remedies to address monopolization only when there is no equally effective behavioral remedy or where a behavioral remedy would be more burdensome.==== Crandall (2001) identified 336 civil cases in which U.S. government antitrust enforcers secured judgments of unlawful monopolization between 1890 and 1996. Courts approved remedies that required structural or functional separation in 95 of these cases, but nearly all of them were mergers or acquisitions or they involved coordinated conduct. Only a handful of the cases that were resolved with a separation order solely addressed conduct by a single firm.====In 1911 the U.S. Supreme Court held that the Standard Oil Trust was an unlawful monopoly and ordered the trust to be split into separate geographic companies.==== In 1948 the Supreme Court affirmed an order to divorce the ownership of movie theaters by major producers of films.==== The 1984 Modified Final Judgment (MFJ) that settled an antitrust case brought by the U.S. Department of Justice (DOJ) against AT&T split the company into separate geographic and functional units.==== More recently, a district court approved a request by the DOJ and several states to separate Microsoft’s Windows operating system from the company’s personal computer applications to remedy the finding that Microsoft monopolized markets for personal computer operating systems and internet browsers. But the court of appeals ordered a re-hearing, which the parties preempted with a settlement that contained behavioral conditions and did not require divestiture.====Legislators have pursued divestitures to open regulated markets to competition, such as England did to separate electric power generation from regulated electricity transmission and distribution (Newbery, 2002). In the U.S. regulators ordered, but subsequently relaxed, separation requirements in several industries. The Glass-Steagall Act of 1933 required the separation of retail and investment banking, but the legislation was weakened over time and repealed in 1999. The Public Utility Holding Company Act of 1935 required interstate electricity and natural gas holding companies to re-organize as single integrated systems that served a limited geographic area,==== but the restrictions were eased over time and eliminated in 2005.==== In 1970 the Federal Communications Commission (FCC) promulgated financial interest and syndication rules that prevented the three major television networks from owning programming that they aired in prime time or syndicated programming in which they had a financial stake. The FCC abolished these restrictions in 1993.==== The 1996 Telecommunications Act eliminated many of the restrictions imposed in the 1984 MFJ.====Section 2 of this paper explores whether a vertically integrated platform has incentives to favor its own products or services and whether separation eliminates such incentives. An online platform provides retail services to independent merchants and may offer its own products for sale. The platform can invest to lower the cost that consumers incur to find and purchase products on the platform. In a related paper, Brito et al. (2012) analyze the incentives of a vertically integrated monopoly supplier of an input to discriminate in the quality of service provided to downstream rivals. The input is supplied at a fixed price in their model and the monopoly supplier can degrade the quality of service offered to rivals. They show that vertical separation does not necessarily eliminate incentives for the input supplier to choose a quality that differs for downstream buyers. Section 2 shows that the platform has incentives to discriminate between firms that rely on its services even if it does not compete with these users and structural separation of a vertically integrated platform need not result in less discrimination.====Section 3 addresses the concern that online retail platforms such as Amazon can use information they collect in the course of business to imitate their rivals’ products and more effectively promote sales of their proprietary private label products. Google or Apple can do the same for apps downloaded from their app stores. Google has been criticized for populating its own services, such as product reviews and travel sites, with information copied from competing websites. This section identifies the potential consumer harm and benefit from imitation with application to Amazon’s private label sales.====Section 4 draws on experience with deregulation of the vertically integrated telecommunications giant AT&T to inform the likelihood that separation requirements imposed on the major digital platforms would have significant consumer benefits. The AT&T history identifies several conditions that are necessary, but not sufficient, for separation to promote competition and innovation. They include the ability to impose line-of-business restrictions that do not become obsolete with the progress of technology and the ability to define, monitor and enforce rules for access to separated services.====Many of the activities provided by the dominant digital platforms fail to satisfy these conditions. Moreover, there are economic incentives to re-establish the vertically integrated organizational structures that existed before separation. Industry regulation also was a factor in the breakup of AT&T that is absent for the digital platforms. The MFJ enabled competition in long distance telecommunications and equipment manufacturing that regulation had foreclosed. Furthermore, the deregulation of AT&T benefited from the support of a specialized regulatory body, which does not presently exist for the digital platforms.====Nonetheless, it is feasible to separate some platform activities. Amazon could be required to offer separate retail platforms for merchant and private label sales with a bar on the communication of information between the platforms. Google could be required to separate its Ad Manager from its other properties. However, separation can compromise benefits from vertical integration and separation would not eliminate the need for continued antitrust oversight to address anticompetitive conduct. It is also important to recognize that antitrust enforcement is not a substitute for regulation to address concerns such as privacy, data security, and the ability of the major platforms to influence public policy and the flow of information and disinformation (Moss, 2019). There is little in the way of theory or empirical evidence that establishes a clear connection between competition and protection against these other ills of the information economy or whether structural or functional separation would mitigate these concerns.==== Section 5 concludes.",Separation: A Cure for Abuse of Platform Dominance?,https://www.sciencedirect.com/science/article/pii/S0167624520301207,16 June 2020,2020,Research Article,57.0
Cabral Luís,"Paganelli-Bull Professor of Economics, Stern School of Business New York University United States,CEPR, London, United Kingdom","Received 23 December 2019, Revised 18 May 2020, Accepted 21 May 2020, Available online 7 June 2020, Version of Record 5 March 2021.",https://doi.org/10.1016/j.infoecopol.2020.100866,Cited by (23),"I present a cautionary note on the proposal to tighten ==== policy in the high-tech space. The discouragement effect on innovation could be significant. This is not to say that increased policy enforcement is not called for. On the contrary. My point is that it should primarily take the form of checking for abuses of dominant position, tightening consumer protection, and directly regulating dominant firms.","Digital industries — or whatever definition includes GAFAM (Google, Amazon, Facebook, Apple, Microsoft) — have been the source of intense debate in academic, policy and political circles. This is not without reason: never in history have large corporations like the American giants been so much part of our daily lives and concerns, from privacy to security to quality of service to concentration of political power to freedom of speech.====Proposals to solve the “GAFAM problem” abound. In this paper I focus on the role of competition policy, in particular merger policy. Some argue that, when it comes to high-tech giants, antitrust has been “asleep”. For example, Streitfeld (2019) remarks that====This may suggest that an “antitrust revolution” must take place in response to the “digital revolution” we’ve seen unfold. By contrast, a number of competition policy scholars argue that it’s all a matter of tightening the screws on existing policy instruments. For example, Crémer et al. (2019) claim that====In particular, there is a growing consensus among policy makers that we need to tighten merger policy in the digital space. See, for example, Scott-Morton et al. (2019) (“Stigler Report”), Furman et al. (2019) (“Furman Report”), and Crémer et al. (2019) (“EU Report”).====In this paper, I agree with Crémer et al. (2019) and others that the current competition policy and law framework are fundamentally sound and useful. I also agree that vigorous enforcement is required in order to curb the increasing power wielded by GAFAM and other giants. However, in light of the very specific features of high-tech industries, I disagree with the Stigler, Furman and EU reports that merger policy is the area where substantial reform is required.====Specifically, I make six points regarding digital industries. First, these are industries where it’s very hard to predict the evolution of business models. Second, related to the first point, preemptive actions are difficult to target, given the poor definition of markets and potential rivals. Third, IP rights are difficult to protect in the software space, so that imitation is a real threat. Fourth, related to the previous point, markets for technology transfer in the form of licensing work poorly.==== Fifth, related to the two previous points, technology transfer is frequently accomplished by means of firm acquisition. Finally, the prospect of such acquisitions provides a strong innovation incentive for startups.====These considerations have important implications for competition policy, in particular for merger policy. Unlike other, more stable industries, where the business model is better defined and market positions easier to predict, in digital industries the pace of innovation is too fast for any serious long-run forecast to be possible. In this context, it makes sense for competition policy to be based primarily on ex-post remedies rather than on ex-ante rules and analysis as traditional merger policy is.====The above does not deny a role for merger enforcement in high-tech. If fact, considering that none of the GAFAM acquisitions were blocked, one must conclude that, if anything, there has been under-enforcement. In part, this results from the nature of high tech and of the current system, which allows most of the proposed mergers to fly under the regulatory radar (Wollmann, 2019).==== My point is that the tone of the current proposals risks swinging the pendulum too far into the opposite direction of over-enforcement. Tightening merger policy not only is a relatively less efficient approach but also one that has enormous costs in terms of innovation incentives.====The above also does not preclude the closer scrutiny of digital industries. My point is that competition should primarily take the form of checking for abuses of dominant position, tightening consumer protection, and directly regulating dominant firms.====The rest of the paper is divided as follows. In Section 2, I argue that the pre-emptive motive of acquisitions varies across industries and is relatively lower in digital industries. In Section 3 I suggest that firm acquisitions may play an important role as a form of technology transfer, and that this is particularly true in digital industries. In Section 4 I touch on the issue of innovation, in particular incentives for “innovation for buyout”. Section 5 presents the main argument regarding merger policy in digital industries, whereas Section 6 concludes the paper.",Merger policy in digital industries,https://www.sciencedirect.com/science/article/pii/S0167624519302537,7 June 2020,2020,Research Article,58.0
"Motta Massimo,Peitz Martin","ICREA-Universitat Pompeu Fabra and Barcelona Graduate School of Economics, Spain,University of Mannheim and MaCCI, Germany","Received 19 January 2020, Revised 29 April 2020, Accepted 25 May 2020, Available online 26 May 2020, Version of Record 5 March 2021.",https://doi.org/10.1016/j.infoecopol.2020.100868,Cited by (25),"Big tech mergers are frequently occurring events. What are the competitive effects of these mergers? With the help of a simple model we identify the acquisition of potential competitors as a pressing issue for merger control in digital industries. We also sketch a few recent theories of harm of horizontal and conglomerate mergers that are potentially relevant in digital industries. Finally, we draw some policy recommendations on how to deal with mergers in such industries.","Big tech mergers happen frequently. Taking a look at the acquisitions by the “Big Five” over the last five years (Amazon, Apple, Facebook, Google, and Microsoft), Amazon is reported to have made 42 acquisitions, Apple 33, Facebook 21, Google (Alphabet) 48 and Microsoft 53.==== The vast majority of these and earlier mergers were under the radar of Antitrust Authorities (AAs)==== and the very few that have come under their scrutiny have been approved, among them the prominent mergers of Google/Youtube, Google/Waze, Google/Doubleclick, Facebook/Instagram, Facebook/Whatsapp, Microsoft/Linkedin.====AAs and governments have become increasingly nervous at the perceived concentration in some digital markets and the persistent and increasing market power of some firms operating in digital industries. This has led to a number of recent high-profile reports on digital markets—e.g. ACCC (2019), Crémer et al. (2019), Furman et al. (2019), or Scott Morton et al. (2019)—to better understand their functioning and to formulate possible ways to promote competition.==== There is also concern that recent mergers were investigated using an inadequate methodology possibly leading to wrong decisions; for instance, in a report commissioned by the UK Competition and Markets Authority (CMA), Argentesi et al. (2019) provide a critical ex-post evaluation of recent UK merger cases in digital industries.====Policy proposals may include a mix of regulatory measures (e.g. obligations for certain firms regarding data portability and interoperability, transparency, as well as not to discriminate), stricter antitrust enforcement, and—relevant for the scope of the present paper—possible changes in merger control. Indeed, some of the (many) mergers in digital industries may well have made the entrenchment of large firms’ market positions easier. This applies not only to acquisition of horizontal nature, but also to acquisition of firms which may appear as conglomerate or vertical. Indeed, challenges to an established incumbent may also arise from often small, but quickly growing firms in adjacent markets. The acquisition of potential competitors is therefore a pressing issue for merger control.====In Section 2, which is grounded in Fumagalli et al. (2020), we develop a simple reduced-form framework to address the possible anti- and pro-competitive effects of the acquisition of ==== competitors. In our setting, a start-up can develop a project that succeeds with some probability. Whenever the start-up has the ability to pursue its project, the merger will be anti-competitive. The acquisition then becomes either a “killer acquisition” or an upgrade with suppressed competition. The merger can only be pro-competitive if the start-up would not be able to pursue its project absent the merger and if the incumbent will have an incentive to develop the project after acquiring the start-up. We shall also see that the acquisition may also have beneficial ex ante innovation effects: a merger may increase the expected benefit from innovation, and hence stimulate effort to obtain it. In extensions we address conglomerate mergers, the presence of outside investors, and exclusionary conduct by the incumbent.====In Section 3, we look at six recent theories of harm of big tech mergers which remove ==== competitors.==== These theories rely on some features that figure prominently in digital industries and that include network effects; two-sidedness and free services to one side; the prominence of big data). They are based on Anderson and Coate (2005) and Anderson and Peitz (2020) in Section 3.1, Nocke and Whinston (2013) in Section 3.2, Prat and Valletti (2019) in Section 3.3, de Cornière and Taylor (2019) in Section 3.4, Rhodes and Zhou (2019) in Section 3.5, and Choi and Jeon (2020) in Section 3.6.====In Section 4, we conclude by making some policy recommendations based on our analysis and some further considerations.",Big tech mergers,https://www.sciencedirect.com/science/article/pii/S0167624520300111,26 May 2020,2020,Research Article,59.0
"Jin Ginger Zhe,Wagman Liad","University of Maryland & NBER, United States,Illinois Institute of Technology, United States","Received 12 April 2020, Accepted 13 April 2020, Available online 18 April 2020, Version of Record 5 March 2021.",https://doi.org/10.1016/j.infoecopol.2020.100865,Cited by (14),"The rise of big data in the global economy has led to concerns about antitrust and consumer protection, but policy makers often treat the two areas separately. The separate treatment is justified in classical markets because antitrust tends to focus on firm-to-firm interactions, while consumer protection deals with firm-to-consumer interfaces. The two areas may also be subject to different laws, and any crossovers between the two have tended to be small. However, big data blurs the distinction between the two, causing them to intertwine, complement or even conflict with each other. This paper uses examples to illustrate why that is the case and identifies areas that would benefit from more economic research.","Many countries use two sets of laws to address market failures. One is antitrust laws that aim to promote competition for the benefits of consumers. Typical concerns are price-fixing and other collusive agreements among competitors, mergers and acquisitions that substantially lessen competition, and the abuse of monopoly power. The second set of laws assure the rights of consumers. To do so, agencies target fraudulent sellers, police deceptive information in the marketplace, implement minimum product quality, and watch out for unfair behavior that may harm consumers.====Antitrust and consumer protection are often treated separately. For example, in Canada, antitrust laws are enforced by Competition Bureau Canada while consumer protection is under the authority of the Office of Consumer Affairs. In the US, both the Department of Justice and the Federal Trade Commission (FTC) enforce antitrust laws, but consumer protection laws are mainly enforced by the FTC, under a bureau separate from antitrust. For financial products and services, consumer protection is also regulated by the Consumer Financial Protection Bureau (CFPB). In the United Kingdom, antitrust and consumer protection functions used to be distributed between the Competition Commission and the Office for Fair Trading, before the two merged to form the Competition and Market Authority (CMA) in 2014. As of today, CMA still has separate senior directors for antitrust and consumer protection.====These separate treatments are justified in classical markets, because antitrust tends to focus on firm-to-firm interactions, while consumer protection deals with firm-to-consumer interfaces. From an economic perspective, antitrust is designed to address the market failure in market power, and consumer protection is more focused on other market failures such as information asymmetry, negative externalities, and bounded rationality. From a legal perspective, the two areas may be subject to different laws (e.g., the Sherman Act and Clayton Act for antitrust, and Truth in Advertising for consumer protection), and any crossovers between the two have tended to be small.====However, the distinction between antitrust and consumer protection has been blurred by big data, or more precisely, by technological advances that allow one to collect, store, and use high-volume, high-velocity and high-variety data. This occurs for several reasons.====First, data exacerbates the information asymmetry between firms and consumers. Consumers face an information overload in the digital era and constantly seek ways that would help them allocate their scarce attention. In contrast, firms welcome data as an input in their business practices and have strong incentives to collect, use, store, or trade consumer data. They are also more resourceful than individual consumers in developing, innovating, utilizing, and commercializing data-driven infrastructures and technologies, such as artificial intelligence, for purposes that may or may not be aligned with the interests of consumers.====Second, users stand to both benefit and lose from the externalities that are associated with data processing and provision, but the specific pecuniary and non-pecuniary harms and benefits to users vis-à-vis firms’ data practices are often difficult to quantify. Many firms in the digital economy are multi-sided platforms that depend on network externalities among users on different sides. Typically, one side is individual consumers, while other sides include entities such as sellers, advertisers, content providers, and other platforms. The multisidedness enables new, direct interactions between firms and consumers as well as among firms, and these interactions feed back into the platform’s business decisions through real-time data generated in the process. The data can also feed back into the different users on the platform and influence future interactions to the benefit and detriment of the users, including firms and consumers.====Third, the nature of data storage and usage raises new questions about property rights and data ownership, data portability and accessibility, data concentration and security, data-related disclosures and transparency, as well as privacy and the ease of data de-anonymization—questions that are at the crossroads of antitrust and consumer protection because they can lead to harms or benefits for consumers on the one hand and alter market power for firms on the other.====More broadly, all of the classical market failures – asymmetric information, negative externalities, market power, and bounded rationality – are potentially exacerbated or face new complications due to data. For instance, data can make every product a credence good with future uncertainty; firms have incentives to extract value from data, but tend to ignore the negative externalities of data; data could be an essential input that an incumbent platform uses against potential competition; and consumers, facing an expanding overload of information, are likely subject to more bounded rationality.====We argue that antitrust and consumer protection could intertwine, complement or even conflict with each other because of big data. To articulate this point, Section 2 presents several examples that highlight data-driven entwinements between antitrust and consumer protection. In Section 3, we connect those examples to a list of incentives that drive market behavior pertaining to data. In Section 4, we review the ongoing literature as to how data affects the classical market failures. Section 5 concludes with a call for more economic research at the crossroads of antitrust and consumer protection.",Big data at the crossroads of antitrust and consumer protection,https://www.sciencedirect.com/science/article/pii/S0167624520300792,18 April 2020,2020,Research Article,60.0
"Calvano Emilio,Polo Michele","Università di Bologna, Toulouse School of Economics, CEPR and CSEF,GREEN and IGIER, Università Bocconi","Received 7 September 2019, Revised 18 December 2019, Accepted 16 February 2020, Available online 18 February 2020, Version of Record 5 March 2021.",https://doi.org/10.1016/j.infoecopol.2020.100853,Cited by (39),"This article focuses on the economics of digital markets with particular emphasis on those features that are commonly deemed critical for Antitrust. Digital markets are often concentrated due to network effects and due to the need of large amounts of Data for production. We review papers characterizing the nature of social harms caused by market power and the role of competition FOR the market and IN the market to relief some of that harm. Special emphasis is given to the role of (i) human attention (which is monetized and is a key input in advertising markets), (ii) Data (which is the oil that powers these markets) and (iii) innovation (incentives, entry for buyout and killer acquisitions).","Digital markets are at the forefront of the public policy debate because of the central role played by tech giants in today's economy and because of their influence, among other things, on cultural diversity, political pluralism and privacy. Any policy argument requires to understand deeply the functioning of these markets. This article focuses on the economics of digital markets with particular emphasis on those characterizing features that industry observers commonly deem as being particularly thorny from an antitrust perspective.====Section 2 focuses on network effects. Network effects are an obvious source of concentration due to a “rich get richer” dynamics, whereby more users enhance the dominant firm's attractiveness leading to even more users. In extreme cases, network effects cause markets to tip to monopoly. Concentration maximizes gross consumers’ surplus when network effects are in place, but its benefits have to be weighed against costs due to market power. Multi-sidedness (i.e. indirect network effects) enriches the picture. The literature on multi-sided markets shows that the interplay of cross-side externalities affects firms’ pricing in distinctive ways.====The fact that digital services are often offered for free does not mean that concerns over concentration are unfounded. Section 2 concludes by reviewing platforms’ incentives to exert their market power in dimensions other than price. Particular emphasis is given to the concept of “intermediation bias” and its consequences. The basic trade-off platforms face is that between revenues per interaction and quantity of interactions. Do search engines or social media outlets provide us with the best possible product? In other words, do firms have an incentive to `tweak’ their algorithms to increase revenues at the expense of consumers’ surplus?====Section 3 presents a more recent literature bound together by the idea that in a world with rapid innovation, potential and actual entry may mitigate the social costs of concentration. That is, we look at the potential of competition FOR the market to discipline incumbents. In particular we present the notion of “incumbency advantage,” capturing the idea that an installed base of consumers may prevent entrants from penetrating the market despite these latter being endowed with better quality products. The three key questions that arise in this context are: what is the source of the incumbency advantage? How and to what extent can such advantage be exploited to extract supra competitive rents? Are there factors that can mitigate the anticompetitive potential of network effects by fostering competition FOR the market? What about multi-homing? That is, what about the widely documented habit of trying out new services ==== quitting the old ones or patronizing two competing platforms at the same time?====Despite concentration being a widespread phenomenon in digital markets, there are plenty of cases with competition occurring IN the market, perhaps because consumers have idiosyncratic preferences for quality or varieties. Section 4 reviews the standard case with different platforms coexisting although often adopting different business models.====Section 5 looks at a particular class of actors, referred to as “attention platforms.” A large fraction of the Internet is basically powered by advertising money. Many websites and apps are in the business of harvesting and reselling human attention. These firms are essentially platforms operating in multi-sided markets: advertisers wish to place their creatives on outlets that have a large audience while consumers typically dislike ads. These markets received a special treatment in the economics literature for a number of reasons that go beyond their obvious relevance. There are often no prices on the consumer side of the market, so firms do not internalize consumers’ willingness to pay. Multi-homing is widespread, and this makes it difficult to use traditional measures of market power. New technologies that use data to profile users and follow them as they traverse the internet (e.g. cookies) scale down competition for attention at the individual level. Finally, advertising is a key input in product market competition when consumers are not informed about the quality and quantity of products on the marketplace.====Section 6 focuses on innovation. The potential threat to incumbents coming from innovative start-ups requires to look at the incumbent's strategic response with a focus on foreclosing strategies and the use of mergers and acquisitions to protect market dominance. Can firms protect their rents by targeting innovative start-ups before they enter the market? How does this change the incentives to innovate? Moreover, the relevance of innovation in digital markets warrants a fresh look at an old theme: the potential impact of mergers on research activity. Indeed, this issue has been hotly debated in the antitrust community after the ==== EC case sparking a flurry of theories.====A final and fundamental piece of the picture refers to the role of data. The quintessential task of many digital platforms is that of making prediction of various sort Agrawal et al., 2018. Search engines need to predict the relevance of URLs to a consumer query. Matchmakers need to predict the value of a match in order to find good prospects for their users (for instance, employees and employers, single men and single women and so on); content distributors, such as Spotify, need to predict their user tastes to keep them entertained; mapping services need to predict traffic conditions and so on. Data is the oil that powers these predictions. Section 7 reviews empirical and theoretical papers shedding light on the map from the quantity and quality of Data to prediction accuracy (thus ultimately to product quality), on the incentives to sell, share or license Data and, finally, on the potential for Data to act as a barrier to entry.====Many enforcers and think tanks have moved their attention to the competition policy in digital markets and several reports have been recently released.==== They share a common concern for the intrinsic tendency to concentration of these ecosystems and suggest, as a general approach, reducing the risk of under-enforcement. Competition policy remains a central player and traditional tools should be adapted in light of a true understanding of the functioning of these markets and of the empirical evidence of the different effects simultaneously at play. A stricter merger control and a focus on foreclosing strategies are the central areas of intervention. At the same time there is a common view in these reports that tech giants should be monitored also through ex-ante regulation, defining a code of conduct that may guide their practices. Data mobility and interoperability, open standards and data openness are key in keeping open these markets to new rivals.====The positive theoretical and empirical results reviewed in this paper offer a rich set of economic insights that should help enlighten the debate and assist policy makers in drawing sensible public policies.","Market power, competition and innovation in digital markets: A survey",https://www.sciencedirect.com/science/article/pii/S0167624519301994,18 February 2020,2020,Research Article,61.0
"Spiegel Yossi,Waldfogel Joel","Coller School of Management, Tel Aviv University, CEPR, and ZEW, Israel,Carlson School of Management, Department of Economics, University of Minnesota, NBER, and ZEW, United States","Available online 8 September 2020, Version of Record 5 March 2021.",https://doi.org/10.1016/j.infoecopol.2020.100894,Cited by (3),None,"Bruno Jullien and Wilfried Sand-Zantman (“The Economics of Platforms: A Theory Guide for Competition Policy”) review the major theories of platform competition with an eye toward competition policy. They explore conditions favoring the emergence of competition ==== the market, versus competition ==== the market (when the market is a natural monopoly or oligopoly), and revisit competition policy for digital platforms. They discuss market power, the essential definition of markets, and the consequences of platform practices such as tying, exclusionary pricing, exclusivity, foreclosure, collusion, and mergers, and show that multi-sided externalities create new opportunities for anti-competitive conduct. These new forms of conduct can be related to pricing or contractual imperfections, in particular zero-pricing and imperfect coordination between sides.====Emilio Calvano and Michele Polo (“Market Power, Competition and Innovation in Digital Markets: A Survey”) survey the economics of digital markets with particular emphasis on features that are commonly deemed critical for antitrust. They pay particular attention to the role of network effects, the “incumbency advantage” of an installed base of consumers for incumbents, differentiation in the business models that different platforms adopt, and the potential impact of mergers on research activity. They also consider competition over limited human attention, the incentive of platforms to “tweak” their algorithms to increase revenues at the expense of consumers’ surplus, and the competitive effect of big data, its potential to serve as a barrier to entry, and the incentives to sell, share, or license it.====Hal Varian (“Seven Deadly Sins of Tech?”) provides an industry perspective on competition in digital markets and offers a counterpoint to critics’ concerns about potentially anti-competitive effects of Google.",Introduction to the special issue of ,https://www.sciencedirect.com/science/article/pii/S0167624520301384,8 September 2020,2020,Research Article,66.0
Martínez-Sánchez Francisco,"Departamento de Métodos Cuantitativos para la Economía y la Empresa, Facultad de Economía y Empresa, Universidad de Murcia, Murcia 30100, Spain","Received 13 February 2019, Revised 15 September 2020, Accepted 18 September 2020, Available online 28 September 2020, Version of Record 7 December 2020.",https://doi.org/10.1016/j.infoecopol.2020.100896,Cited by (3),"I analyze how the loss aversion of consumers affects the strategies of the government and the incumbent for preventing commercial piracy. To that end, I develop a sequential ==== model of vertical product differentiation with price competition in which consumers have a reference-dependent utility. Regardless of the quality of the illegal copy, conventional models that do not take into account the loss aversion of consumers overestimate the government’s effort to deter piracy but underestimate the incumbent’s effort. Contrary to conventional wisdom, I find that blocking the entry of a pirate by the government can provide more welfare than accommodating it. However, the government will not block it because socially it is better to encourage the incumbent to establish a price low enough to deter the pirate from entering.","The arrival of personal computers in homes around the world in the last quarter of the twentieth century opened up the possibility of copying digital goods without the consent of their owners. This action, known as piracy, reduced profits because it became widespread throughout the world, as can be seen in Table 1. For example, the Global Software Survey (2003) reports that in 1994 97% of the software installed in China was not licensed, i.e. was pirated. Even in developed countries such as France and Germany, pirated software accounted for half of all installed software. Given how widespread piracy has become, firms and governments have carried out strategies to curb and reduce it. As a result the piracy rate worldwide has been reduced from 49% in 1994 to 37% in 2017, according to the latest Global Software Survey (2018) conducted by the Business Software Alliance. Despite the progress made, piracy rates remain high, as can be seen in Table 1. Recent papers have analyzed differences in piracy rates between countries.==== Martínez-Sánchez and Romeu (2018) find that in more developed countries there are fewer incentives to pirate products. They also find that countries with smaller, more efficient bureaucracies are likely to protect intellectual property more effectively. These results are backed up by Athey and Stern (2015), who suggest that the quality of the institutional environment is more closely linked with piracy than income per se.====On the other hand, behavioral economics has shown that humans behave non-rationally.==== In particular, we are averse to losses (Kahneman, Tversky, 1979, Tversky, Kahneman, 1991). This means that the pain of a loss is greater than the pleasure of a gain of equal size. Neumann and Böckenholt (2014) have empirically demonstrated that loss aversion manifests when consumers decide to buy a product. Models developed recently in the field of industrial economics incorporate these discoveries about the non-rational behavior of people.==== They consider that consumers obtain utility not only when consuming a product but also when comparing it with the reference product. Therefore, if a consumer buys a more expensive (or lower quality) product, she experiences this as a loss in the price (quality) dimension. Previous literature has considered that consumers compare original products. However, this paper considers that consumers compare the original and the copy of a product. As far as I know, there are no papers that study piracy when consumers are averse to losses. This paper sets out to fill that gap and analyze the effect of loss aversion among consumers on strategies to prevent piracy. It also seek to confirm whether making the model a little more realistic invalidates the conclusions of the traditional models that analyze piracy, and whether it provides new findings.====From the marketing literature, we know that lower price of illegal copies appears to be the main factor driving sales (Chiu and Leng, 2016), and that the more aware consumers are for paying lower prices, subject to some quality constraints, the more favorable they are to piracy (Ang et al., 2001) and (Wang et al., 2005). Illegal copies evoke positive emotions such as the pleasure of buying a product at a cheaper price (Marticotte and Arcand, 2017), but the availability of a cheaper illegal copy causes consumer dissatisfaction with the original brand (Juggessur and Cohen, 2009). Consumers are therefore very likely to be loss aversion when they decide to buy the original or the copy of a product.====In this paper, I analyze commercial piracy of digital goods, which occurs when some firms reproduce and illegally sell copies of original products without the authorization of the owner.==== To that end, I develop a sequential duopoly model of vertical product differentiation with price competition.==== This model is similar to those presented in Banerjee (2003); Martínez-Sánchez (2010) and López-Cuñat and Martínez-Sánchez (2015). Banerjee (2003) analyzes the role of government in combating piracy. Among other results, he finds that blocking the entry of a pirate so that the incumbent can set monopoly prices is an equilibrium. This result contrasts with that obtained by Martínez-Sánchez (2010) and López-Cuñat and Martínez-Sánchez (2015). Martínez-Sánchez (2010) shows that the government will not help the incumbent to become a pure monopolist even if it installs an antipiracy system. It will let the pirate enter as either a follower or a leader, or encourage the incumbent to set a low enough price to successfully deter the pirate from entering the market, depending on its technology for monitoring the pirate. In a common framework, but with different assumptions, López-Cuñat and Martínez-Sánchez (2015) confirm that deterred or accommodated piracy can occur in equilibrium, but pure monopoly cannot occur in any anti-piracy policy.====Commercial piracy has been and continues to be widely analyzed. Among the relevant publications,==== Lu and Poddar (2012) study the case when the incumbent makes a costly investment to deter a commercial pirate in a given regime of intellectual property rights (IPR) protection. They find that when the consumers’ tastes are sufficiently diverse and the IPR protection is weak, it is profitable for the incumbent to accommodate the pirate. In all other cases it is profitable to deter the pirate. Häckner and Muren (2015) consider two types of consumption externalities: i) consumers do not like the quantity of items (copies or originals) sold of a product; and ii) consumers do not like copies more than the originals. They show that there seem to be welfare gains from counterfeiting, but the government would typically want to keep counterfeiting at a low level, at least when externalities are strong and enforcement costs low. Recently, Madio (2018) presents a model for analyzing the competition between a subscription-based content provider and many pirate providers. He shows that the incumbent and the government always find it optimal to tolerate some degree of piracy. Finally, Klein (2020) presents a model that incorporates endogenous product quality and the interaction between public and private enforcement efforts. He shows that when public intellectual property enforcement is low, the authentic product firm optimally accommodates counterfeit entry by choosing low private enforcement and relatively high product quality. However, under high public enforcement the firm deters entry through high private enforcement. Although piracy hurts incumbents, these papers find that piracy has a positive effect on consumers because it forces owners to set a lower price. Therefore, tolerating piracy can be socially desirable. In the present paper, I check whether these conclusions hold when consumers are loss averse.====This research is related to recent literature that analyzes how the loss aversion of consumers affects price competition. Using the approach introduced by Köszegi and Rabin (2006); Heidhues and Köszegi (2008) modify the model introduced by Salop (1979) to consider that consumers are loss averse in relation to a reference point given by their recent expectations about the purchase. They find that consumers’ loss aversion in terms of money increases the intensity of competition, reducing or eliminating price variation. Karle and Peitz (2014) modify the model developed by Heidhues and Köszegi (2008) to consider that firms commit to deterministic prices before consumers form their reference points. They find that loss aversion in price is procompetitive, while loss aversion in taste is anticompetitive.====These papers consider that the reference point arises endogenously, but I assume that it is determined exogenously, as in Zhou (2011) and Amaldoss and He (2018). In a duopoly à la Hotelling (1929); Zhou (2011) finds that the firm whose product takes more consumers as a point of reference has an incentive to randomize its price. He also shows that loss aversion in price is procompetitive, while loss aversion in taste is anticompetitive. On the other hand, Amaldoss and He (2018) include reference-dependent utility in the spokes model developed by Chen and Riordan (2007). They find that loss aversion in price increases competition among low-value goods, whereas loss aversion in taste softens competition among high-value goods only if consumer sensitivity to the difference in taste is high enough.====Previous papers consider models in which firms are horizontally differentiated. However, in the model that I present in this paper firms are differentiated vertically because I assume that the quality of the copy is inferior to that of the original product. Recent papers have developed monopoly models with vertically differentiated products and loss averse consumers. In this framework, Carbajal and Ely (2016) study optimal price discrimination when consumers have reference-dependent preferences for the quality of the product. They find that, depending on the reference plan, optimal price discrimination may exhibit efficiency gains relative to second-best contracts without loss aversion. Hahn et al. (2018) also study price discrimination but they consider that consumers have reference-dependent preferences for the quality and price of the product. They show that offering menus with a small number of bundles is consistent with profit-maximizing firms that face loss averse consumers. Finally, Courty and Nasiry (2018) apply loss aversion within a class of products of the same quality but not across quality classes. They show that uniform pricing can be optimal across quality classes up to a quality threshold.====Here I analyze how the loss aversion of consumers affects the strategies of the government and the incumbent for preventing commercial piracy. To that end, I develop a sequential duopoly model of vertical product differentiation with price competition in which consumers are loss averse. I consider that consumers have a reference-dependent utility for the hedonic price of the product, where a hedonic price is defined as the price/quality ratio of a product. Loss aversion thus depends on the price and quality of a product, but the degree of loss aversion is the same for both characteristics. This last contention is empirically supported by Neumann and Böckenholt (2014), who find no general differences in loss aversion between price and quality. I find that, regardless of the quality of the illegal copy, conventional models that do not take into account the loss aversion of consumers overestimate the government’s effort to deter piracy but underestimate the incumbent’s effort. Contrary to conventional wisdom, I find that action by the government to block the entry of a pirate can provide more welfare than accommodating it. However, the government will not block it because socially it is better to encourage the incumbent to establish a price low enough to deter the pirate from entering.====The rest of the paper is organized as follows: Section 2 describes the model formally. Section 3 presents the equilibrium. Section 4 draws comparative statics. Finally, Section 5 concludes.",Preventing commercial piracy when consumers are loss averse,https://www.sciencedirect.com/science/article/pii/S0167624520301402,28 September 2020,2020,Research Article,67.0
"Phuc Nguyen Canh,Dinh Su Thanh,Doytch Nadia","School of Banking, University of Economics Ho Chi Minh City, Ho Chi Minh City, Vietnam,School of Public Finance, University of Economics Ho Chi Minh City, Ho Chi Minh City, Vietnam,Koppelman School of Business, CUNY-Brooklyn College, New York, USA,PhD Program in Economics, CUNY-Graduate Center, New York, USA,School of Government, Ateneo de Manila University, Manila, Philippines","Received 11 December 2019, Revised 23 June 2020, Accepted 3 September 2020, Available online 15 September 2020, Version of Record 7 December 2020.",https://doi.org/10.1016/j.infoecopol.2020.100892,Cited by (21)," and financial development. We find that internet usage has a significant negative impact on overall financial development, which could be attributed to a negative impact on financial institutions with all their three dimensions, depth, access, and efficiency. At the same time, internet has significant positive impact on financial markets with its three dimensions. Contrary to the opposing effects internet usage, mobile usage has a significant positive impact on all nine indices of financial development. The PMG ARDL and PDOLS estimations clarify that the positive impact of the internet is a short run effect, while the negative effect is a long-run one. The mobile usage impact is a long-run phenomenon. The estimations for two sub-samples show consistently positive impact of mobile phones in HIEs, whereas the results for LMEs are less robust.","The impact of new technologies on financial development (FD) has not been sufficiently examined. Most of the existing literature focuses on social development indicators as determinants of FD (Datta and Singh, 2019; Ductor and Grechyna, 2015; Mlachila and Ouedraogo, 2019; Zhang and Ben Naceur, 2019). Moreover, previous studies tend to examine a single indicator, such as bank credit or stock market capitalization, as a proxy for FD, which leads to failure in capturing the complex multidimensional nature of the FD process. In this study, we use a system of FD indicators proposed by Svirydzenka (2016). These indicators include separate indices for financial institutions and financial markets and sub-indices capturing their depth, access, and efficiency. Since these different dimensions of FD have been largely unexplored by the literature, we seek not only to examine a new relation- the relationship between technology and FD, but also to explore FD in depth by using the novel FD measures.====Although the link between the new technologies and FD has not been extensively studied, there is existing evidence that utilization of new technologies, such as internet and mobile phones, have been playing a very important role in socioeconomic activities during the past three decades (Stockemer, 2018; Visser, 2019; Wang and Hao, 2018). There are several studies on the impact of these technologies on economic activity, such as on income inequality (Jaumotte et al., 2013), productivity (Akerman et al., 2015), and innovations and entrepreneurship (Abraham, 2006; Feldman, 2002; Madon, 2000). Economists agree that internet and mobile phones technologies are shaping up as important new factors in human and economic activities (Wang and Hao, 2018). They are credited with reducing communication time and communication costs and with providing means for efficient high-quality communication (Suvankulov et al., 2012).====There are some studies on the nexus between information and communication technologies (ICTs), FD and economic growth. Sassi and Goaied (2013) note that FD and ICTs are expected to have positive impacts on economic growth, but their empirical results, which are based on a sample of MENA countries, show mixed effects. They find that ICTs stimulate economic growth, while FD has a negative impact. However, they do not explore the linkages between ICTs and FD. Another study, Mushtaq and Bruneau (2019), documents a positive impact of ICTs diffusion on financial inclusion for a sample of 62 countries. Overall, however, the literature on the linkages between ICT and FD, in general, or internet and mobile usage and FD specifically, is limited and incomplete. This is the gap we are seeking to fill.====The two phenomena, internet/mobile use penetration, and development of financial institutions and markets have been happening simultaneously around the world. Table 1 describes some stylized facts about internet and mobile technology use over time. In 1998, there only 5.1% of the population used the internet. In 2017, this rate was close to 61%. In term of mobile usage, there were only 9.6 mobile cellular subscriptions per 100 people on average in 1998, but this number was around 119 in 2017. The internet and mobile use rates were higher in high income countries (HIEs), respectively, 11% and 20 per 100 in 1998, and 84% and 134 per 100 in 2017. For comparison, these rates for low- and middle-income countries (LMEs) were respectively 0.6% and 1.4 per 100 in 1998, and 43% and 107 per 100 in 2017. Along with the spread and utilization of internet and mobile technology, financial institutions and markets also developed over time. Therefore, we hypothesize that there could be possible links between internet/mobile usage and financial development.====Our study explores the impact of internet and mobile technology on different dimensions of FD with a panel 109 economies over the period of 1998–2017. We choose this period because of the sharp rise in the internet and mobile phones usage around the world (Group, 2019; VOA, 2019). We apply panel Granger causality tests following Dumitrescu and Hurlin (2012) to investigate the causal nexus between Internet/mobile usage and FD. After reporting five panel unit-root tests on stationarity of the variables, we apply a two-step system generalized method of moments (GMM). Our model controls for other known drivers of FD, such as economic growth, capital formation, government expenditure, domestic savings, inflation, trade openness, and human capital. Additionally, we conduct long-run analysis by first, applying three panel cointegration tests to examine the long-run cointegration between Internet/mobile usage, income level, and FD, and then apply a pooled mean group autoregressive distributed lag estimator (PMG ARDL) and a panel dynamic ordinary least squares estimator (PDOLS). We conduct the above short-run and long-run analyses within two subsamples: 62 high income economies (HIEs) and 47 low- and middle-income economies (LMEs).====Our empirical results show that internet use has opposite effects on financial institutions and financial markets. It exhibits a significant negative effect on the depth, access, and efficiency of financial institutions, while having a significant positive impact on the development of financial markets, including their depth, access, and efficiency. Meanwhile, mobile phone use exhibits significant positive impacts on both, financial institutions, and financial markets, as well as overall financial development.====We also perform long-run analysis, which allows us to differentiate between effects occurring in different time horizons. Accordingly, we find that the negative impact of internet on FD is associated with the long run. Similarly, the positive effect of mobile phone use is also associated with the long run. When we conduct the analysis by sub-samples (HIEs and LMEs), the results show consistently positive impacts of mobile phones in HIEs, while the influences of internet are sensitive to the estimation methods. Moreover, the influences of both, internet, and mobile phones on financial development in LMEs are sensitive to the different estimation methods and different indexes of FD.====The study is structured as follows: in Section 2 we conduct a literature review, in Section 3 we describe the methodology and the data used in the study, in Section 4 we present the empirical results and finally Section 5 concludes.",The drivers of financial development: Global evidence from internet and mobile usage,https://www.sciencedirect.com/science/article/pii/S0167624520301360,15 September 2020,2020,Research Article,68.0
"Kim Byung-Cheol,Ahmed Mishal","Department of Economics, Finance & Legal Studies. Culverhosue College of Business University of Alabama, United States,Center for Economics Studies, U.S. Census Bureau, United States","Received 5 February 2020, Revised 28 August 2020, Accepted 30 August 2020, Available online 3 September 2020, Version of Record 7 December 2020.",https://doi.org/10.1016/j.infoecopol.2020.100891,Cited by (1),We consider ==== sequential price competition between a low-cost online firm and a high-cost brick-and-mortar firm that decides whether to price-match the low-cost rival. We study how price-match guarantees affect the incentives of both firms to invest in cost reduction and quality enhancement. We find that price-match guarantees in our model weaken these incentives in most cases. Our research reveals more reasons to suspect that seemingly pro-competitive price-matching by many offline rivals to online sellers may have hidden social costs.,"Price-match guarantees (henceforth, PMGs) have been used by firms since at least 1947 (Edlin and Emch, 1999). Recent data show 12% of the top 500 retailers selling electronics and computers price-match, while 50% of the hardware and home improvement stores and 100% of office supplies stores commit to such policies (Jiang et al., 2016). To consumers, this may appear to be beneficial but economists, policymakers, and media have long debated over whether PMGs are pro-competitive or anti-competitive.====Recently, more attentions have been paid to so-called “showrooming” practices and how firms attempt to avoid potential free-riding by buyers at their expense. At large, showrooming can take place in two different contexts: 1) buyers wishing to analyze a product first-hand may visit a brick-and-mortar showroom, but then purchase the product online at a lower price; 2) buyers may search on an online marketplace ‘showroom’ to find the lowest-price or best-quality seller, then purchase the product directly from the seller or through a different platform at a lower price. Brick-and-mortar stores have used PMGs to address the first type of showrooming.==== And, online platforms have used “price parity clauses” (PPCs) to deal with the second type.==== In both situations, one of the main issues is how PMGs or PPCs affect investment incentives of sellers and platforms.====In this paper, we use a standard Hotelling-type model of duopoly competition between a low-cost online firm and a high-cost brick-and-mortar firm and consumers with heterogeneous preferences for online versus offline shopping. In our model, the offline high-cost firm decides whether to adopt a PMG or not, after which both firms engage in the Stackelberg price competition with the online firm moving first. The model reflects reality in that usually higher-cost offline firms have been price-matching their lower-cost online opponents.====After we show how PMGs affect the online rival, the consumers, and social welfare (Proposition 1), we focus on its effects on investment incentives. We find a new type of inefficiency resulting from PMGs. First, we show that PMGs may dampen the high-cost offline firm’s investment incentive to decrease its cost disadvantage. Intuitively, there are two channels through which the high-cost firm’s incentive is affected: (i) a lower cost expands its market share through more competitive pricing, which we refer to as the ==== and (ii) it also raises the mark-up per unit of sales, which is the ====. With PMGs, however, the high-cost firm already secures half of the market, and further market expansion from lower cost is not possible. Besides, the high-cost firm as the follower in a sequential price-setting passes the entire reduction in its cost to the consumers through a lower price, which means no intensive margin effect either. Consequently, on both accounts, the high-cost firm has weaker investment incentives for cost-reduction with PMGs (Proposition 2). We find a similar result for the low-cost firm, but the reason is slightly different. Again, PMGs make the extensive margin effect null. There is some positive intensive margin effect for the low-cost firm from its cost reduction, but the intensive margin effect is smaller with PMGs compared to that without PMGs. As a result, PMGs result in a low-cost firm to have a weaker investment incentive also (Proposition 3).====Second, we study firms’ incentives to invest in quality improvement. We find that PMGs weaken the high-cost firm’s investment incentive for a sufficiently small cost disadvantage. Why does the size of cost difference matter? The logic is as follows. Let ==== denote the cost difference.==== Under PMGs, the investment incentive is independent of ==== because the market share and the mark-up for the high-cost firm are independent of ====. By contrast, without PMGs, the intensive margin effect and extensive margin effect both increase with ==== if the low-cost firm’s quality improves. In other words, the investment incentives for quality are stronger when its cost disadvantage is bigger, which is intuitive. As a result, the high-cost firm’s investment incentive is smaller under PMGs for a small cost disadvantage, whereas the opposite is the case for a large ==== (Proposition 4). Regarding the low-cost firm’s incentive to improve its quality, we find that PMGs unconditionally weakens its quality innovation incentives (Proposition 5). Under PMGs, the market share is independent of ====; however, the intensive margin effect becomes negative because the price-match leads to an even lower mark-up when the quality difference enlarges. Without PMGs, both the intensive margin and extensive margin effects turn positive. As a result, PMGs do not increase investment incentives for the low-cost firm.====Our research reveals reasons to suspect that seemingly pro-competitive price-matching by many offline rivals to online sellers may have additional hidden social costs. These hidden social costs strengthen the case for policymakers to intervene in specific contexts and limit such practices when empirical evidence supports our theoretical findings.",Price-match guarantees and investment incentives,https://www.sciencedirect.com/science/article/pii/S0167624520301359,3 September 2020,2020,Research Article,69.0
"Lo Yuen C.,Medda Francesca","Institute of Finance and Technology, UCL, University of London, Room 213, Chadwick Building, Gower Street, London, WC1 E6BT, United Kingdom","Received 20 August 2019, Revised 3 July 2020, Accepted 17 August 2020, Available online 22 August 2020, Version of Record 7 December 2020.",https://doi.org/10.1016/j.infoecopol.2020.100881,Cited by (15),"Digital tokens linked to financial and economic ventures may have multiple functions and uses. In this work, we examine the relationship between various token functions and the market price of the corresponding token. We consider 86 venture related blockchain tokens, and develop the analysis through a stepwise testing of four hypotheses using panel ordinary least squares with cluster-robust standard errors. We find that token functions are statistically significant in relation to token prices. In the absence of an established legal framework, we argue that our results complements recent regulatory actions identifying tokens to be investment contracts in a common venture.","Sales of blockchain tokens, often referred to as initial coin offerings (ICOs), raised $5.3 billion in 2017 (Adhami et al., 2018). These funds are testament to the financial significance of crypto tokens. The token market capitalizations of a subset of these tokens, used in this study, are shown in Fig. 1 to help visualize their size and recent performance. Following the seminal work of Nakamoto (2009), researchers have built a solid understanding of the underlying technology (Yli-Huuomo, Ko, Choi, Park, Smolander, 2016, Narayanan, Bonneau, Felten, Miller, Goldfeder, 2016). The information layer of the Bitcoin blockchain consists of blocks of transaction data linked together, such that an attempt to change one transaction, requires changing all those chained after it. This is the censorship resistant record. Any processor (aka miner), authentic or adversarial, can compete for the right to add a block to the chain, and garner a reward of cryptocurrency, by solving a computing intensive cryptographic puzzle. Miners are able to enter freely and the system shows no bias between existing miners and new entrants. This is the mechanism behind the system decentralization, with the longest chain of proofs determining consensus around the state of the world the blockchain describes. Public keys, known as addresses, link transactions with pseudonymous address owners (who control their assets via private cryptographic keys). Together, this web of computer science, game theory and cryptography delivers the provably scare digital asset of Bitcoin. In a key paper connecting the technology to the economics of blockchain, Cong and He (2019) provides a formal proof of how a blockchain based consensus, that includes smart contract based prices contingent on delivery, can support new entrants. In their framework, new entrants signal quality by trustlessly guaranteeing buyers compensation if the product fails, explicitly enlarging the contract space.====Bitcoin and Ether are cryptotokens native to the Bitcoin blockchain and the Ethereum blockchain respectively. Buterin (2013)’s Ethereum platform formalized smart contracts on a blockchain. Smart contracts are shared computer objects that can manipulate state e.g. token balances. Smart contracts enable many contingent actions, including the issuance and trading of non-native tokens by third parties on blockchains such as Ethereum in under 30 min.==== Bartoletti and Pompianu (2017) surveys various smart contract platforms and highlights the diminished barriers to issuing tradeable digital tokens.====These digital tokens may claim a link to a real world venture. Cong and He (2019), Catalini and Gans (2018) and Canidio (2018) connect blockchain consensus with new token structures and economic models. One functional category of this is the utility token, which is exchangeable for a service provided by the venture. Catalini and Gans (2018) use economic proofs to show how such a utility token, limited in quantity, and the sole medium of exchange, can appropriate the returns to a given platform. Under these conditions, the token price can appreciate in proportion to a rise in demand for the service on the platform. They note how these platforms are typically open source software protocols that equate to shared infrastructure among ecosystem participants. They use this framework to contrast token fund raising and venture capital. Importantly, equity investment offers the returns on all current and future projects of a firm, whereas token investment is solely in the current platform. These tokens therefore represent a more circumscribed package of entrepreneurship, value creation and value capture than an equity. Canidio (2018) addresses the possibility of exit scams, where an issuer steals the funds raised, with retained token holdings and mixed strategies. In Chod and Lyandres (2018), retained token holdings are one way for issuers to address asymmetric information.====However, despite the scope and scale of cryptoassets, there is little understanding of what a token holder has acquired in practice. This is partly because unregulated blockchain based tokens rarely include any legal obligations. Users are being asked to fund a business, which raises the question: what they are receiving relative to what they are promised? Cohney et al. (2019) explores this by comparing marketing promises with smart contract code. Nevertheless most empirical work on blockchain tokens so far have been focused on cryptocurrency prices (Briere, Oosterlinck, Szafarz, 2015, Brandvold, Molnar, Vagstad, Valstad, 2015, Bouri, Molnar, Azzi, Roubaud, Hagfors, 2017, Lo, 2017, Pieters, Vivanco, 2017) or ICO success and size (Benedetti, Kostovetsky, 2018, Amsden, Schweizer, 2018, Howell, Niessner, Yermack, 2018, Adhami, Giudici, Martinazzi, 2018). Overall the academic literature is responding to this new capital structure of a venture, yet so far lacks evidence that its newest components authentically link project and token. The latter becomes the first objective of our analysis: to investigate the linkage between a project that has a value, and a token that has a price. The second objective is to investigate differences between functional types of blockchain tokens. A result that shows token type impacts prices imply that legislators and policy makers should create a framework that differentiates between different types of tokens. A result that shows no difference would imply that regulators can focus on ICOs as a broad category of venture capital funding, and pay less attention to the nature of the token.====We develop our paper through a stepwise approach testing four hypotheses that investigate this novel link between venture and price. We start by parsing out the relationships between our sample blockchain tokens and two major cryptocurrencies, then by isolating the effect of different token functions, token features and token distribution characteristics highlighted in the literature (Fig. 2). Ultimately our examination of the impact of common token structures on token price is intended to contribute to the wider question of what can these new tokens embody and connect, and touches on the possibility of new forms of information discovery.====The cryptotoken space is dominated by two specific cryptocurrencies. Bitcoin is the primary means of buying and selling tokens, while Ethereum smart contracts are often the technological basis of many of these tokens, and sometimes a fee component of the application business an ICO is building. Bitcoin and Ether are also the main currencies used in funding ICOs. Both underpin distribution of tokens following a token generation event. For example, ERC-20 is the technical standard for issuing third party tokens on the Ethereum platform. Therefore buying an ERC-20 token with Ether occurs on a single blockchain platform, and is the easiest way to engage in such a transaction. Buying an ERC-20 token with other cryptocurrencies requires additional steps or coding, because the transaction crosses between two blockchains. Our first hypothesis emerges from the importance of these two cryptocurrencies.====In our analysis we assume a direct relationship between Bitcoin, Ether and ICO prices. We then move on to our core hypothesis, that the functions that constitute a token create an economic link between a project that has a value, with a token that has a price. The idea that price and value can be separate is commonplace in the finance literature, though the definition of intrinsic value varies with the object of study (Lee, Myers, Swaminathan, 1999, Froot, Ramadorai, 2005).====Our paper classifies the sample of tokens according to the token’s functionality. We look specifically for the influence of functional dummies on a token’s trading price. The four key functions we study are: (1) payment, (2) utility, (3) asset and (4) yield. We distinguish between share of profits type yields, and proof of stake blockchain type rewards (that distribute tokens to nodes that participate in consensus generation). One difference between the two is that the former is paid in a separate currency out of platform income (e.g. a dividend in Ether), while staking is in the token currently held, and numerically dilutive. Because of this, we consider share of profits to be a function and staking to be a feature. For reference, yield only tokens do exist in the dataset, but staking is typically associated with other functionalities as it reflects an underlying technical choice e.g. a proof of stake blockchain variant. There are other ways to interpret the promise of staking, including the potential elimination of platform mining costs and lower electricity consumption. However we are wary of categorizing a promise of stake rewards as usage of proof of stake technology (versus proof of work), because sampled project tokens typically begin trading on a proof of work blockchain platform such as Ethereum.====With respect to a token being the sole medium of exchange on a platform, in theory it is necessary for a utility token to adhere to this in order to appropriate the benefits of the platform (Catalini and Gans, 2018). We collect data on whether or not a utility token is intended as the sole medium of exchange for the related platform and test for any relationship between this factor and the token’s trading price. The features of staking and sole medium of exchange are bundled together in our third hypothesis.====Designating a token as the sole medium of exchange does not always resonate with investors. A prominent example of this is how Basic Attention Token (BAT) has faced calls to make payments in US Dollars or Bitcoin instead of BAT tokens====. Sole medium of exchange requirements may slow wider adoption, as changing in and out of the utility coin becomes a barrier to use.====Our final hypothesis revolves around the criticality of token distribution at the time of ICO. This focuses on the split in economics between investors, relative to insiders and future service providers. These token distribution decisions are likely endogenous, and operate in two contrasting ways. It is plausible that the better the project, the higher the share of tokens reserved for insiders - and therefore signals quality to outsiders. Conversely, an increase in either of these reserved token categories decrease the share of platform economics received by funders. Our framework enables us to test for which of these effects dominate in terms of price.====We use the proportion of tokens reserved for founders / team members / company controlled foundation, or reserved for mining, future marketing or future partnership building.====The paper is structured as follows. Section 2 explores different blockchain token functions and features. Section 3 addresses the data, including an examination of selection biases. Section 4 describes our empirical methodology. Section 5 presents our results, including a discussion that compares our results with our hypotheses. We close with a conclusion.",Assets on the blockchain: An empirical study of Tokenomics,https://www.sciencedirect.com/science/article/pii/S0167624520301256,22 August 2020,2020,Research Article,70.0
"Tyrowicz Joanna,Krawczyk Michal,Hardy Wojciech","FAME/GRAPE, Poland,University of Warsaw, Poland,IZA, Germany","Received 13 August 2017, Revised 27 December 2018, Accepted 24 June 2020, Available online 30 June 2020, Version of Record 7 December 2020.",https://doi.org/10.1016/j.infoecopol.2020.100879,Cited by (4),"Economic theory posits that consumers’ response to technological innovations in distribution of cultural goods may have both positive and negative effects on the sales of these goods. Access to unauthorized cultural content – often referred to as “online piracy” – may reduce demand for the authorized distribution through substitution or raise it (through complementarity and similar effects). The empirical evidence speaks to both positive and negative correlation between unauthorized distribution and authorized sales. We review and discuss the accomplishments in the field so far and provide a quantitative analysis of the empirical literature, the meta-analysis. While numerous and interesting measures and methods have been developed, estimating reliable ==== effect remains a challenge. There are also substantial differences between film and music industry. On the whole the literature fails to reject the null hypothesis of no effects on sales.","This study focuses on the link between activity referred to as “online piracy” and the sales of cultural goods. Analyzing this relationship is problematic and at the same time interesting for three main reasons. First, while “online piracy” is a popular phenomenon, its effects on sales have received considerably less attention from the scientific community than from commercial or civic actors. Second, “online pirates” typically try to hide their activity, making data relatively scarce. Moreover, the technological progress in production and distribution cultural goods introduces substantial changes to the nature of authorized and unauthorized acquisition of cultural content.==== Third, funding in the field is often provided by the creative industries, who generally show interest in studies confirming the damaging character of the unauthorized distribution.====The field has no clear consensus about the sign and the magnitude of the displacement effect from unauthorized consumption to authorized sales. On the one hand, careful readers will find at least a few articles arguing in favor of a positive effect of unauthorized digital consumption. They rely on a range of arguments such as sampling and exploration, “dipping the toe”, increased revenues for complementary goods and services, etc. On the other hand, some studies argue forcefully that “online piracy” actually substitutes legitimate sales. Indeed, zero or low price competition from unauthorized distributors can effectively reduce demand for usually higher-priced CDs, MP3 files, DVDs, movie theater tickets, etc.====In addition to conflicting results, there is also a disparity in measurement. Indeed, even the measures of legitimate sales or proceeds from authorized sales are often unavailable or proxied (e.g. by store survival). Naturally, “online piracy” is rarely directly observable, which necessitates the use of indirect measures or proxies. As a matter of fact, the very term of “online piracy” – though commonly used – is highly imprecise. Intuitively, most people identify it with acquisition of content from unauthorized sources, which is why the focus is placed on consumer choice. However, there is wide discrepancy in how this type of action is perceived: from a rational response of a consumer to technological progress with contested legal status to a crime punishable by imprisonment. In fact, acquiring copyrighted content is legal in many countries regardless of whether the distribution was authorized or not. For example, in many European countries – e.g. Germany, Spain, and Poland – only unauthorized distribution violates the law,==== while acquisition is either entirely legal, or merely a tort, subject to typically troublesome civil lawsuit. By contrast, in the United States, acquiring copyrighted content from unauthorized sources is considered a crime. Naturally, the legal status of “online piracy” may also change over time.====Defining “piracy” is difficult also due to technological progress. Within the last decade, more and more unauthorized distributors make content available through streaming, which implies that the “downloader” actually never acquires the file but rather experiences the content in real time, from a server often located outside the jurisdiction of countries where most copyright holders reside. These services differ in business models: some rely on commercials, others request the so-called transfer payment, finally some distributors expect a regular fee for maintaining the account; to avoid criminal charges most unauthorized distributors offer the actual content for free. These are models similar to the authorized distribution; as a result the final users often find it hard to distinguish between these two types of sources (Liebowitz, 2013).====In addition to the measurement issues, there are also conceptual complexities. Typically one would interpret content distributed with and without authorization as substitutes if cross-elasticity of demand was positive. However, in the case of acquisition of content from unauthorized distributors, the (marginal) cost is typically zero. Consequently, even if data was available on per-good per-client basis, cross-elasticity could not be computed.==== Moreover, there are endogeneity issues: a positive correlation between (a proxy of) “online piracy” and legitimate sales cannot be meaningfully interpreted, because typically more attractive content will be more frequently acquired from both authorized and unauthorized distributors. Similarly, a consumer with a greater taste for culture may demand more from both types of sources.====With these two paramount problems (popularity driving both legitimate sales and “piracy” as well as near-zero price for “pirated” content), the field has offered a variety of methodological solutions. While some of them remain controversial – see for example Oberholzer-Gee and Strumpf (2007) and the subsequent critique by Liebowitz (2017) – they are designed with the intention to discriminate between substitution effects and complementarity effects. A strand of research attempted to instrument for “piracy” with variables seemingly unrelated to legal sales or to identify the effects through event studies (such as the introduction of new laws or the shut down of major unauthorized distributors). To avoid some of the delineation issues, studies typically analyze fairly similar cultural goods (e.g. only music or movies). Some of the studies also try to narrow the analyzed topic geographically (e.g. analyzing sales within a university campus or in shops in certain districts). Methodologically, it seemed relevant to several authors to control for individual tastes for culture or to apply fixed effects estimates to obtain estimates conditional on the unobservable taste for culture. This proliferation of methods, subjects and identification strategies constitutes yet another motivation to engage in a meta-analysis.====Our objective in this paper is to provide a systematic and quantitative evaluation of the achievements in this field of economics. We analyze the available research papers with two major goals. First, we aim to shed some light on the developing standards in this strand of the literature, emphasizing the relevance of endogeneity bias and the need for a strong identification strategy. We review measurements and methods used in the literature with the objective to evaluate external validity of some positive and some negative results. Second, we test the contention that with more robust methods and more comprehensive measures, the actual substitution effect is strong and large. Popularity of “piracy” is, after all, yet another example of a concept referred to in the literature as an enforcement problem. The ability to protect one’s property from unauthorized use is not a new topic in economics – its traces may be found in numerous applications of empirical institutional economics, conflict economics and endogenous growth theories to name just a few.====Our study is not the first one to look at this growing body of literature (see Oberholzer-Gee, Strumpf, 2010, Liebowitz, 2013, Danaher, Smith, Telang, 2014a, Liebowitz, 2016). However, there are some aspects that differentiate this study from the earlier contributions. First, we provide a systematic overview. This implies that the basis for our review is a universe of earlier studies, rather than an arbitrary selection. In fact, in total, we have identified as many as 45 studies (with 425 estimates), which is substantially more than covered by any of the previous reviews. Second, unlike previous studies, we employ a meta-analysis methodology, i.e. quantify the size effects in the literature using objective statistical methods. Danaher et al. (2014a) argue that a vast majority of published papers demonstrate a negative effect of “piracy” on sales. Meanwhile, of the 40 studies we identify, 21 interpret their findings as evidence for substitution rather than complementarity effects. However, there are also studies which find the opposite, studies which demonstrate no significant results and studies which argue that the direction of the link depends on the type of content or analyzed sample. In addition, in most papers, at least some of the specifications were insignificant or had a sign opposite to the claimed result. Unlike reviews, a meta-analysis provides tools to understand these discrepancies.====Following the literature in the field, we focus on consumer behavior. Thus, the term ==== will be a shorthand for the acquisition of cultural content from unauthorized sources (e.g. downloading, streaming, copying, etc.). Given the mixed legal character of acquisition from unauthorized sources, as well as differences in the ethical perception of this activity, we also use the term ==== in the interest of brevity. Whenever we speak of ==== without consent of copyright holders, it will be explicitly indicated. In the next section we discuss the insights from the available literature in a broader context of institutional economics and property rights protection. In Section 3 we discuss the literature sampling procedure and discuss of the insights from the available estimates. After this exploratory analysis, in Section 4 we move to the meta-regressions, assessing quantitatively the role of robustness for the conclusions from the literature. The policy and research implications of our study are discussed in the conclusions.",Friends or foes? A meta-analysis of the relationship between “online piracy” and the sales of cultural goods,https://www.sciencedirect.com/science/article/pii/S0167624520301232,30 June 2020,2020,Research Article,71.0
Landgraf Steven W.,"Department of Economics, Florida State University, 600 W College Ave, Tallahassee, FL 32306, USA","Received 4 July 2019, Revised 21 December 2019, Accepted 23 June 2020, Available online 30 June 2020, Version of Record 23 September 2020.",https://doi.org/10.1016/j.infoecopol.2020.100878,Cited by (3),"Deploying municipally-run broadband Internet to deliver high-speed access is an increasingly popular idea among local communities. These public networks often operate alongside private incumbent networks and may have substantial effects on the competitive landscape. This research design exploits variation in cities that have a municipal electric utility (MEU), which reduces barriers to entry for municipal providers, and state-level regulation, which restricts public entry into broadband markets. Using these market differences, I investigate whether incumbents offer higher speeds to deter public entry or underinvest in speed due to crowding out. Estimates indicate that the presence of an MEU is associated with lower maximum upload and download speeds offered by private cable and DSL providers. In states where municipal entry is made more difficult by regulation, these effects disappear. Therefore, restrictive regulation of municipal broadband has a non-trivial effect on competition. Negative effects are reduced in communities with lower or less dense populations.","Is there a role for local governments to play in promoting the development of Internet infrastructure? There is little doubt that having an adequate Internet infrastructure is key to supporting economic growth (Katz, 2012) and making a community an attractive place to live (Molnar et al., 2015). While it is relatively easy to attract private investment in denser, wealthier urban areas, there is a great concern that rural areas are being left behind.==== Stenberg et al. (2009) finds that areas with low population size, low population density, and a declining or aging population have difficulty in attracting broadband Internet service providers (ISPs).==== These demand factors limit the profitability of deploying new networks or substantially upgrading existing ones in these communities. There is evidence that large gaps between the social and private benefits of high-speed fiber Internet drives underinvestment by private ISPs (Nevo et al., 2016).====Many communities across the U.S. have deployed or are considering deploying a publicly-owned network. The provision of these networks raises questions about the way in which public and private firms compete in the marketplace. If a public firm adequately maximizes social welfare, then it can indirectly regulate competitive conduct in the “public oligopoly” vein of Merrill and Schneider (1966) and Cremer et al. (1989). This indirect regulation can be especially helpful in local Internet markets, where local authorities have little to no way to directly regulate service quality or price. Potential entry alone may be enough to tame market power in a “contestable market” along the lines of Baumol et al. (1982). On the other hand, public investment, actual or potential, can crowd out private investment which might reduce innovation and scale and scope economies. Social welfare can also be harmed entry occurs and the public firm is poorly-managed or otherwise inefficient.====In this paper, I estimate how private broadband Cable and Digital-Subscriber Line (DSL)==== ISPs react to the threat of entry by a public firm and how this threat interacts with state-level regulation. I exploit variation in the probability of municipal entry, measured by the presence of a municipal electric utility (MEU), and variation in state regulatory statutes to estimate the relationship between network quality and the entry threat. Using an estimation strategy that controls for market structure, I find that the threat of entry by a public firm leads to lower maximum speeds offered by private firms in states that do not regulate municipal networks. Thus, investment by public firms may be crowding out private investment. In states that restrict municipal entry into broadband markets, these negative effect disappear as the entry threat is weakened. Given that these laws are currently the subject of intense political and legal debate, these results provide some useful evidence that these laws do impact market conduct. Estimates of the model on a subset of communities with lower or less dense populations suggest that crowding out effects are less likely to occur in these places.====The remainder of this paper is as follows: Section 2 reviews the state of municipal broadband and electric utilities while providing a brief discussion of technology and cost factors. Section 3 reviews theoretical and empirical literature that motivates the approach I take in examining this issue. I then discuss the empirical model and data employed in Sections 4 and 5. Finally, I report and discuss estimation results and robustness checks in Section 6 and conclude with suggestions for policy and future research in Section 7.",Entry threats from municipal broadband Internet and impacts on private provider quality,https://www.sciencedirect.com/science/article/pii/S0167624520301220,30 June 2020,2020,Research Article,72.0
"Carroni Elias,Paolini Dimitri","DSE, Almamater Studiorum Università di Bologna, Italy,DiSEA - Università di Sassari, CRENoS, Italy and CORE, Université Catholique de Louvain, Belgium","Received 29 May 2019, Revised 20 June 2020, Accepted 23 June 2020, Available online 30 June 2020, Version of Record 23 September 2020.",https://doi.org/10.1016/j.infoecopol.2020.100877,Cited by (9),"A streaming platform obtains contents from artists and offers commercial spaces to advertisers. Users value contents’ variety and quality of the service and are heterogeneously bothered by ads. Two solutions can be proposed to users. If they pay a positive price, they subscribe to a commercial-free service with an upgrade of quality (====). Otherwise, they have free access to a service of a basic quality. We find that a wider audience gives incentives to the platform to increase both the advertising intensity and the quality upgrade in the ====. As a consequence, some people move to the ====. At the limit, the platform opts for a purely subscription-based business model as the audience reaches a certain level. The parsimonious model we propose is able to give a rationale to the emergence of different business models in the streaming market as well as to the (end of the) disputes between artists and the Spotify model.","The online world offers many business opportunities to companies that run platforms turning web-users into subscribers. In particular, online media markets have boomed dramatically, with many big players currently competing (e.g., Google, Amazon, Spotify, Apple, YouTube, Netflix). These players behave in different ways ==== each side of the market, with the consequence of a rich variety of business models. On the user side, Google and Apple Music opt for the offer of a paying subscription, YouTube follows an ad-based business model whereas Spotify presents a mixed model with users self-selecting into their preferred subscription (second-degree price discrimination).====Media platforms are characterized by the interaction of different groups of agents exhibiting cross-group externalities. Namely, a user enjoys more (less) a platform’s service when the variety of contents (number of commercials) increases and, in turn, a content provider and an advertiser have stronger incentives to join a platform in which they can meet a wider audience. In terms of business strategy, this brings us to what Caillaud and Jullien (2003) defined as a chicken–and–egg problem: the company needs to find the most profitable way to attract a critical mass in each group. In the music streaming market, Eller (2015) reports that both Spotify and Apple Music offer at least a 30 million–song library. This makes them very attractive to users and, in turn, selling an artistic production to a platform “offering” a broad set of users is valuable to providers, who want their contents to reach the widest possible audience. Moreover, the value of subscribing the platform service for a user depends not only on the variety of contents but also on other features offered by the platform. The latter represent the quality of the platform service, brought by recommendation systems; creation, access off-line and sharing of playlists; synchronization on several devices; quality of page layout/video/sound.====On top of that, real streaming markets show different subscribing solutions, which prove to be different ways to account for these cross–group interactions. For instance, consider the cases of Spotify, Youtube and Deezer. Their free-of-charge solution, the so–called basic subscription, entails frequent commercial interruptions after a few songs. Somehow, users are compensated for the nuisance of ads with free access to music. Contextually, users are given the opportunity to upgrade to a paying solution with quality improvements and absence of commercial interruptions. This business model is commonly called ====.====Differently, in the purely subscription-based business model of Apple and Google Music, users pay a price and they are allowed to access the contents’ catalogue available on the platform. The absence of commercials is usually associated with quality improvements similar to the ones proposed by the upgraded version of Spotify.==== Hereafter, we will refer to this business model as ====.====These platforms have been perceived with suspicion by artists, especially when the offer of contents is completely free-of-charge.==== Indeed, artists may look at the streaming market as a threat to the sale of their artistic productions through alternative channels. Recent empirical articles such as Aguiar and Waldfogel (2018), Wlömert and Papies (2016), and Hiller (2016) show that streaming and purchasing tend to be substitutes. Differently, Aguiar (2017) and Aguiar and Martens (2016) give evidence of complementarity due to an effect described by Belleflamme (2016) as “discovery,” that is, streaming is used by subscribers to discover high–value music and match value, leading to an ultimate increase in music consumption.==== Our model assumes artists to have heterogeneous outside options, and so accounts for the “cannibalization effect” and the fact that this effect may be different among artists.====This paper aims at giving a rationale to the following stylized facts related to streaming market. First, the disputes against the Spotify model have been resolved in the last few years, leading important artists to join the platform (e.g., Radiohead and Taylor Swift). Second, both the number of active users and the share of ==== users boomed dramatically in the same period, as documented in Figs. 1 and 2. These two aspects are linked with each other and highlight the pivotal role of Spotify’s market share. Indeed, the increase in the number of active users (200% in the period 2015-2018) has been associated with the joining decisions of important artists as well as with a boost of the share of users upgrading to the ==== subscription (more than 400% increase in the same period).====In the present paper, we provide a parsimonious model in which a monopolistic platform allows the interaction between users, advertisers and content providers. Users are assumed to receive utility from the variety of contents they can stream by subscribing to the platform’s service and are heterogeneous according to their aversion towards advertisement. The platform decides on four dimensions. First, it pays per–user royalties to content providers, which are heterogenous with respect to their outside option. Second, it sets the advertising intensity. Moreover, it sets the subscription price for ==== users. Finally, it decides the quality upgrade offered in the ==== segment. Our results depend on an exogenous parameter which represents the share of people the platform is able to reach, i.e., its audience.====This share is key in two dimensions. On the one hand, a larger audience results in a lower royalty necessary to bring contents on board. As a consequence, a larger share of consumers results in a larger proportion of contents present in the platform at equilibrium. On the other hand, if the platform reaches a wide audience, offering only the paying subscription is always dominating. This is because a wider audience gives incentives to the platform to increase both the quality upgrade of the ==== (so to increase subscription price) and the advertising intensity (so to increase unitary profits from advertising). As a consequence, some people move to the ==== subscription. As a sufficient share of consumers can be reached by the platform, this mechanism leads to a situation in which it is optimal to opt for a purely subscription-based model, eliminating advertising and the free subscription.====In conclusion, the present paper explains the relationship between audience, content providers and business models in the streaming markets. All in all, our model predicts that a platform with a wide audience will only offer a ==== subscription, whereas a platform having access to a narrower share of consumers will offer a menu of subscriptions. This is in line with what happens in online markets, where a widening of the audience is usually accompanied to a gradual passage from an advertising-based to a subscription-based business model. Moreover, in our model, content providers would prefer a purely subscription-based system, which explains artists’ reluctance to participate in the ====. Our baseline model builds on the assumptions that the platform cannot set a positive price in the basic segment and on the impossibility to discriminate royalties among content providers. Relaxing these assumptions, we show that the positive relationship between the emergence of a ==== model and the size of the audience is confirmed, even though the parameter regions compatible with each scenario may obviously change. Importantly, we also show that whenever a menu of subscriptions is the best solution for a platform, the price of the basic subscription is optimally set to zero.====The rest of the article is organized as follows. The next section presents the related literature. Thereafter, the baseline model is introduced in Sections 3 and 4 presents the analysis. Finally, Section 5 provides a discussion and Section 6 extends the baseline model before drawing the conclusions in Section 7.","Business models for streaming platforms: Content acquisition, advertising and users",https://www.sciencedirect.com/science/article/pii/S0167624520301219,30 June 2020,2020,Research Article,73.0
"Anderson Simon P.,Celik Levent","University of Virginia, Charlottesville, VA 22904, USA,CEPR, London, UK,City, University of London, London EC1V 0HB, UK,CERGE-EI, a joint workplace of Charles University and the Economics Institute of the Czech Academy of Sciences, Prague 1, Czechia,National Research University Higher School of Economics, Moscow, Russia","Received 15 May 2019, Revised 26 March 2020, Accepted 27 May 2020, Available online 7 June 2020, Version of Record 23 September 2020.",https://doi.org/10.1016/j.infoecopol.2020.100869,Cited by (2),"We study “opaque” selling in multiproduct environments – a marketing practice in which sellers strategically withhold product information by keeping important characteristics of their products hidden until after purchase. We show that a monopolist will always use opaque selling, but it is not first-best optimal to do so. However, opaque selling might be used at the constrained optimum (with the monopolist’s pricing behavior taken as given). For linear disutility costs, it is optimal for a monopolist to offer a single opaque product.","We study “opaque” selling in multiproduct environments – a marketing practice in which sellers strategically withhold product information by keeping important characteristics of their products hidden until after purchase. Opaque selling is particularly prevalent and growing in the travel/tourism industry.==== Online intermediaries such as Hotwire.com and Priceline.com engage in opaque selling by concealing hotel names and locations or airlines and departure/arrival times. Economycarrentals.com, an online car rental intermediary, reveals the name of the car rental company only after the customer pays for the service. Other venues where opaque selling is employed include Japanese “fukubukuro” or “omakase”, subscription beer or wine boxes, etc.====Focusing on market segmentation (and thereby price discrimination) as a motive for withholding information, we investigate in this paper the equilibrium and welfare properties of opaque selling. We consider a standard Hotelling (1929) model with a continuum of consumers who differ with respect to their ideal tastes and a monopoly seller. In the baseline model, we assume that the seller is equipped with two base products that are located at the two end-points of the unit line [0,1]. We then extend the analysis to the case of many products. Besides offering each base product individually for sale, the firm can also design and sell any number of lotteries that award one of the base products as the final prize, but the consumer cannot observe the outcome until after purchase. The questions we address are: When can the seller profit from selling opaque products? How are base product prices affected? How many opaque products does the seller offer concurrently? Does opaque selling improve social welfare?====The literature on opaque selling is quite recent. The price discrimination motive for a monopolist is addressed in Jiang (2007) and Fay and Xie (2008) in a symmetric two-product Hotelling framework. They find the conditions under which offering a given opaque product improves profits. In a similar setting, Balestrieri et al. (2017) solve the optimal selling mechanism allowing non-uniform pricing and an endogenous number of lotteries. They show that, depending on the shape of the transportation cost function, the monopolist may offer a single lottery, a continuum, or lotteries with positive probabilities of no sale. Thanassoulis (2004) and Pavlov (2011) reach similar results in a random utility setting.====The main elements of the above models are similar to ours. We contribute to this literature by offering a tractable framework that relies heavily on graphical tools and economic reasoning. Building on the methodology we developed in Anderson and Celik (2015) (henceforth AC), we provide a simple graphical characterization of a monopolist’s optimal strategy using the elementary tools of virtual valuation curves. Given a set of base products, a monopolist will offer only those opaque products that extend the upper envelope of virtual valuations. When the transportation costs are linear, it is optimal to offer a single opaque product that offers the same expected utility to all consumers. This approach greatly simplifies the standard mechanism design approach used in Balestrieri et al. (2017) while allowing for possibly asymmetric base products and non-uniform consumer distributions.==== Moreover, we are able to show that this result extends to multiple base products when the monopolist is free to offer any number of lotteries. To the best of our knowledge, none of the above papers consider lotteries of more than two products. Finally, our methodology enables us to show that opaque selling is never first-best optimal, but might improve welfare in a second-best sense. We are not aware of any earlier papers that offer any welfare analysis of opaque products.",Opaque selling,https://www.sciencedirect.com/science/article/pii/S0167624519300988,7 June 2020,2020,Research Article,74.0
"Chiou Lesley,Kafali E. Nilay,Rysman Marc","Economics Department, Occidental College, CA, United States,Boston University, Boston, MA United States","Received 28 May 2019, Revised 11 November 2019, Accepted 22 May 2020, Available online 25 May 2020, Version of Record 23 September 2020.",https://doi.org/10.1016/j.infoecopol.2020.100867,Cited by (3),"This paper examines the effect of Internet penetration on competition and prices in the market for Yellow Pages advertising. We find that the diffusion of the Internet is associated with a decrease in the number of competitors and average prices for printed advertisements in the long-run. However, the decrease in prices is attenuated both by increasing market concentration as firms exit and by geographic rescoping as remaining firms reposition their products.","Since the 1990s, the diffusion of the Internet reshaped the way consumers search for goods and services, as well as the markets for goods and services themselves. This led to significant interest in the effect of the Internet on offline markets, particularly those markets that are well-served by online sellers, such as books, CDs, and computers. Some examples are Brynjolfsson and Smith (2000), Goolsbee (2001), Ellison and Ellison (2006), Prince (2007), and Chandra and Kaiser (2014).==== We examine how competition and prices evolve in the market for print Yellow Pages advertising as Internet usage rises over a fifteen-year period.====Yellow Pages advertising provides an ideal setting for studying the effects of Internet diffusion on competition and prices. First, the Internet provides a clear alternative to Yellow Pages’ primary service—providing search and information. Second, detailed data on prices and locations allows us to study both market structure and prices. Because geographic scoping of directories is a publisher’s most important product characteristic, we can also document how publishers repositioned their products in response to Internet competition. Third, Yellow Pages advertising consists of a large number of local geographic markets, which is amenable to statistical analysis.====Finally, studying this industry allows us to overcome challenges in identifying the influence of the Internet on offline competition. In prior studies that focus on the decline of offline retailers for leisure goods such as books or CDs, the diffusion of the Internet may lead to a decline in the number of offline book or music stores if consumers purchase books or CDs from an online retailer instead of an offline retailer (residual demand for a retailer falls), or if consumers substitute their leisure time away from reading books and listening to CDs towards surfing the Internet (overall demand for books falls). The advantage of our study is that Yellow Pages advertising is not a leisure activity, so we can isolate the effect of the Internet on residual demand for a retailer.====We empirically test the theoretical predictions of competition between online and traditional retail sectors (Alba, Lynch, Weitz, Janiszewski, Lutz, Sawyer, Wood, 1997, Bakos, 1997, Pan, Ratchford, Shankar, 2002, Lal, Sarvary, 1999, Viswanathan, 2005, Chun, Kim, 2005). The growth of the Internet may decrease the number of competitors in offline markets; as low-cost online retailers enter, existing offline retailers may exit the market. The expansion of the Internet may have an ambiguous effect on prices. On one hand, prices may fall if the demand for traditional retailers falls as online retailers become an attractive alternative. On the other hand, prices may rise if market concentration increases as traditional retailers exit the market, if particularly inelastic consumers remain in the market, or if remaining retailers reposition their products to maintain prices in the presence of Internet competition.====We also highlight the importance of product repositioning. Although repositioning is presumably an important general response to market shocks such as entry, it receives limited study that we are aware of. Some exceptions include choice of retail formats and circulation of newspapers (Ellickson, Misra, Nair, 2012, George, Waldfogel, 2006) as well as repositioning in the context of mergers between airlines, radio stations, and ice cream manufacturers (Li, Mazur, Park, Roberts, Sweeting, Zhang, Sweeting, 2010, Mazzeo, Seim, Varela, 2018). We are not aware of any studies on repositioning in the context of offline response to the Internet. Because one of the most important characteristics of a Yellow Pages directory is its geographic scope, we show that publishers appear to adjust geographic scope more intensively in markets with more Internet usage, and that doing so prevents some of the decrease in prices that we might otherwise observe.====We construct a unique dataset that covers printed directories by all publishers in the years 1999 and 2014, a period when the industry underwent massive changes. Our timespan allows us to capture the long-term trends in the industry. We combine data on prices from the Rate and Data publication of the Yellow Pages Publishers Association (YPPA, now known as the Local Search Association) with data on Internet usage and demographics for the distribution areas. We test whether locales with relatively fast Internet growth also experienced relatively large changes in the Yellow Pages market, focusing on outcome variables such as the number of competitors and prices.====Our results illustrate how competition falls and market concentration rises. With the expansion of the Internet, the number of directories produced by smaller independent publishers decreases. Due to the overall decline in the number of firms, industry concentration rises by approximately 20%. Our results also indicate that the Internet has a direct effect of decreasing average prices of printed advertisements by providing a substitute to online advertisements, particularly for smaller print advertisements. However, remaining firms did not decrease prices as substantially as they otherwise would, because the exit of smaller independent firms led to a rise in market concentration.====Further, we study the repositioning of Yellow Pages directories. We provide several measures of geographic repositioning and show that markets with more repositioning have higher prices, and the negative effect of the Internet on prices is higher when controlling for repositioning. Thus, it appears that the Internet induced publishers to reposition their products, which prevented price declines to some extent. Our results are consistent with increased Internet usage inducing consumers to switch away from usage of Yellow Pages, and publishers now finding rescoping to be valuable as their consumer base changed.====Our paper has implications for how policymakers evaluate competition. In order to evaluate the level of competition between rivals, policymakers often focus on whether rivals affect prices. For example, merger authorities often assess whether firms compete in the same market by measuring whether the presence of one firm affects the prices of another.==== Our results show that even if price effects are not evident, competition may manifest in other strategic variables. Similarly, the Federal Communication Commission is often called upon to regulate the competitive transition from one technology to another, and often relies on price effects to determine whether competition in one technology disciplines another.==== However, our results suggest that even when competition between technologies is important, price effects may appear small, and instead competition may more strongly affect other strategic variables.====Our paper relates to several strands of literature. It connects to prior work that analyzes how the rise of the Internet led to a decline in competition for leisure goods (Goldmanis, Hortacsu, Syverson, Emre, 2010, Forman, Ghose, Goldfarb, 2009). As mentioned previously, because Yellow Pages are a non-leisure good, we isolate the effect of the Internet on a firms’ residual demand. Our study also links to work that examines how Internet penetration affects price dispersion and competition (Orlov, 2011, Ater, Orlov, 2015), advertising (Seamans, Zhu, 2014, Goldfarb, Tucker, 2011, Chandra, Kaiser, 2014), and consumption of offline substitutes (Gentzkow, 2007, Liebowitz, Zentner). In addition, our paper relates more generally to research on the printed Yellow Pages market (Rysman, 2004, Busse, Rysman, 2005). Our paper contributes to the few studies that examine a firm’s decision to reposition its products. We also provide an overview of the changes in competition, pricing, and product positioning in a market over a fifteen year period in response to a large and sustained competitive shock.","Internet use, competition, and geographical rescoping in Yellow Pages advertising",https://www.sciencedirect.com/science/article/pii/S016762451930109X,25 May 2020,2020,Research Article,75.0
"Sharma Priyanka,Wagman Liad","Stuart School of Business, Illinois Institute of Technology, United States","Received 2 January 2019, Revised 5 April 2020, Accepted 6 April 2020, Available online 1 May 2020, Version of Record 23 September 2020.",https://doi.org/10.1016/j.infoecopol.2020.100864,Cited by (1),"We study a political contest where two candidates advertise on a platform to persuade voters to vote in their favor. Voters a priori favor one of the candidates. The extent of a candidate’s favorability can be ascertained by a data intermediary who can decide to sell this information to one, both or neither of the candidates. We contrast the intermediary’s incentives for selling information with the platform’s incentives for maximizing candidates’ advertising expenditures, and show that the two are always at conflict. Our findings suggest that tensions may exist between social-media platforms, which often generate data that an intermediary may collect, and an intermediary whose data sale choice can lower the platform’s profit from advertisements. We characterize conditions under which the intermediary can influence the outcome of the contest.","Recent revelations about the extent to which voter data can be collected have fueled growing concerns about the harvesting and use of voter data in political campaigns. These concerns are exacerbated by recent reports from the Federal Trade Commission which demonstrate the proliferation of data intermediaries—entities that collect information from a wide range of sources and form detailed individualized profiles of the citizenry, which, among other things, can be used to discern political preferences.==== A concomitant increase in expenditures on political advertising (approaching $7 billion in the US in 2016, and, in particular, exceeding $1 billion on social-media platforms) has further amplified these concerns.==== The literature on campaign spending includes earlier foundational works by  Brams and Davis (1973), Snyder (1989), and Nagler and Leighley (1992). While these works predate the ascent of the digital economy and social-media platforms, tensions among data intermediaries, media platforms, and political campaigns have long persisted. However, they are heightened by the degree to which data intermediaries and media platforms are now interlinked, with those linkages encompassing voter interactions, voter profiling, and political advertising. Our aim in this paper is to provide an analysis of voter data flow between a data intermediary and candidates in a political contest.====With social media quickly becoming a core advertising platform for political campaigns, some policy commentators and consumer groups have urged governmental authorities to deal with privacy issues that arise in a variety of contexts, and the literature in the area has been rapidly expanding  (Acquisti et al., 2016).==== It has also been shown that information revealed about political preferences on social networks combined with other information aggregated by data intermediaries can help assess voter predisposition (Jernigan and Mistree, 2009). Our analysis assumes that voter data had already been collected, and we study the drivers behind a political candidate’s access to this data. That is, our focus is on which political candidate gains access to election-pertinent data, and how that access can influence the profits of the advertising platform and the information intermediary, and the outcome of the election.====The model we study can be described as follows. Two candidates, Alice and Bob, are competing for the same political office. Before voters go to the polls, each candidate can buy campaign advertisements on a social-media platform. A priori, Alice is favored to win among voters. This is exhibited in a predisposition of voters towards voting for her. Bob, consequently, would need to spend more than Alice on campaigning in order to have a fair shot at winning. An intermediary owns data that reveals more precise information about Alice’s favorability relative to Bob—information that can be helpful in determining candidates’ advertising spending. The intermediary can decide whether to sell the data to both Alice and Bob or to sell the data exclusively to one of the candidates. The advertising platform, on the other hand, has its separate objective of maximizing the candidates’ total advertising spending on the platform. Under this setup, we ask: How much should candidates spend on advertising? To which of the candidates should the intermediary sell access to its data, and for how much? What are the advertising platform’s preferences over candidates’ access to data? Will the existence of the intermediary improve or worsen the platform’s payoff? Does the sale of data influence the outcome of the election and, if so, in what way?====To answer these questions in our framework, we assume that the price of advertisements can be ascertained up front.==== A Tullock contest is used to determine a winner based on relative spending  (Tullock, 1980), and voters’ political predispositions, which are modeled in an overall population sense for simplicity, weigh the relative efficacy of campaign spending (i.e., Alice is the ‘favored to win’ in the context of Dixit, 1987). Candidates’ spending decisions depend on what information they have about voters and what information they expect their opponent to have. In other words, when deciding how much money to spend on advertising, Alice must take into account not only her own access to information, but also Bob’s, because Bob’s spending will determine the marginal impact of Alice’s own campaign dollars. Both candidates share a common prior about the extent of voters’ predisposition towards voting for Alice, but neither knows it precisely. The intermediary seeks to maximize its profit from selling information about voters’ precise predisposition parameter, whereas the advertising platform profits from candidates’ advertising outlays.====We show that, in equilibrium, the intermediary and the platform are always at conflict with respect to candidates’ information access. For instance, if the intermediary decides to sign a contract to sell the information exclusively to Bob, then the platform, in contrast, prefers that the intermediary either sells non-exclusively to both candidates, or instead signs an exclusive contract with Alice. If the intermediary decides to sign a non-exclusive contract to sell the information to both candidates, then the platform prefers that the intermediary instead signs an exclusive agreement with one of the candidates. We then proceed to identify conditions under which the intermediary’s existence can influence the outcome of the contest.====Our findings have practical implications. Most prominently, either the intermediary or the platform always has an incentive for voter information to be shared exclusively with only one of the candidates. The incentives to grant access to voter information exclusively may affect the outcome of political contests by either hurting the winning chances of the favored candidate or help cement her victory. Furthermore, our analysis indicates that a social-media platform that is also used for advertising may have incentives to hinder an intermediary’s access to its data, whether by encouraging regulation or by way of limiting the intermediary’s access to its platform’s data hose. In the market for data, such actions may have additional implications with respect to data concentration and data portability.",Advertising and Voter Data in Asymmetric Political Contests,https://www.sciencedirect.com/science/article/pii/S0167624519300022,1 May 2020,2020,Research Article,76.0
Tropeano Jean-Philippe,Paris School of Economics and University of Paris 1 France,"Received 12 February 2019, Revised 19 March 2020, Accepted 22 March 2020, Available online 25 March 2020, Version of Record 23 September 2020.",https://doi.org/10.1016/j.infoecopol.2020.100862,Cited by (0),We develop a framework in which the timing of the merger control is left to the merging firms’ discretion: before the completion of the merger (ex ante) or afterwards (ex post). We show that the choice of merger control timing by the firms always dominates the ex ante control in terms of expected ====. The choice of merger control timing also dominates the ex post control except if the expected merger outcome is very anti-competitive.,"Information is crucial for the effectiveness of merger control. Merging firms are typically supposed to have better information beforehand than the Competition Authority on the true impact of the merger in terms of consumer surplus. In contrast, after the completion of the merger (ex post), many aspects of the merger are publicly observed so that it becomes easier to assess the impact of the merger (see for instance Ashenfelter, Hosken, 2010, Blonigen, Pierce, 2016). This has led (Ottaviani and Wickelgren, 2011), to examine the theoretical implications of an ex post merger control. Nevertheless, they show that the risk of an ex post costlier rejection may make the ex post merger control worse than the ex ante control by deterring some price-reducing mergers.====In this paper, we suggest a mix of both systems: we allow the merging firms to choose the timing of the assessment. We develop a model of merger control by a competition authority where ex ante (before the completion of the merger), both the merging firms and the authority have imperfect information on the true type of the merger. In case of ex post control (after the completion of the merger), the competition authority has better information but if the merger is rejected, the merging firms incur an additional cost due to this late rejection.====We first show that such a flexible merger control (ex post control or ex ante control according to the merging firms’ choice) increases the expected consumer surplus with respect to the ex ante control. Indeed, the flexible merger control preserves the possibility for the firms to be controlled ex ante and also allows the firms to be controlled ex post. In addition, we show that the flexible merger control does not always dominate the ex post control. The flexible merger control combines the informational benefit of the ex post control for the firms that actually choose the ex post control and the riskless benefit of the ex ante control but it also gives lower incentives to firms to adopt the ex post control. Eventually, we show that the flexible merger control is the most efficient control timing except if the firms are well informed ex ante or if the welfare cost of anticompetitive mergers is high. In that case it is preferable for the Competition Authority to impose the ex post control. As a policy implication of our framework, we suggest that a Competition Agency (denoted CA henceforth) that wants to avoid anticompetitive mergers should adopt an ex post control. Instead, a CA that wants to avoid deterring pro-competitive mergers should adopt a flexible merger control.====Few papers examine the role of the timing on the effectiveness of the merger control. A recent debate on the possible tightening of US merger control led some scholars to advocate some sort of ex post review of mergers==== (see Salop, Shapiro, Shapiro, 2019), but without any formal analysis to assess the pros and cons of that ex post review. The possibility of a regime allowing the merging firms themselves to choose the timing of control of their merger is not considered either. Ottaviani and Wickelgren (2011) formally compare both timings. A distinct but related approach is taken by Choe and Shekhar (2010), who study the trade-off between voluntary and mandatory notification==== They show that the non mandatory notification acts as a screening device since the most efficient mergers will not be notified. However, their ex post merger assessment in case of voluntary notification involves no regulatory control by the agency but only costly litigation without information revelation.====The paper proceeds as follows: the model is presented next, and the main results derived in Section 3.",Ex ante or Ex post? When the timing of merger assessment is up to the merging firms,https://www.sciencedirect.com/science/article/pii/S0167624519300289,25 March 2020,2020,Research Article,77.0
Gnangnon Sèna Kimm,"World Trade Organization, Rue de Lausanne 154, CH-1211 Geneva, Switzerland","Received 8 April 2019, Revised 16 December 2019, Accepted 11 January 2020, Available online 30 January 2020, Version of Record 13 June 2020.",https://doi.org/10.1016/j.infoecopol.2020.100850,Cited by (14),"This article investigates whether greater access to the Internet influences tax reform in developing countries. Tax reform entails here the change of the tax structure in favour of domestic tax revenue, and at the expense of ==== tax revenue. The analysis has used an unbalanced panel dataset of 102 developing countries over the period 1995-2015. The empirical exercise, based on the two-step system Generalized Methods of Moments (GMM), shows that the rise in the Internet usage is associated with a higher extent of tax reform in developing countries. Specifically, low-income countries enjoy a higher positive effect of the Internet access on tax reform, compared to other groups of countries. Furthermore, the positive effect of the Internet usage on the extent of tax reform increases as countries' degree of openness to international trade rises.","In the wake of the liberalization of their trade regimes, developing countries have experienced significant public revenue losses (e.g., Khattry and Rao, 2002; Berg and Krueger, 2003; Keen and Simone, 2004; Longoni, 2009; Baunsgaard and Keen, 2010; Castanheira et al., 2011; Waglé, 2011; Hisali, 2012; Crivelli, 2016; Moller, 2016; Cagé and Gadenne, 2018). Therefore, they have been confronted with the important challenge of how to replace the public revenue losses so as to secure a sustainable stream of their public revenue. Many researchers as well as international institutions, including the International Monetary Fund (IMF) and the World Bank have advised policymakers of developing countries to embark in a revenue-replacement strategy so as to recoup and, eventually to more than compensate the revenue losses due to greater trade openness. The proposed tax reform, also referred to as “tax transition reform” (e.g., Chambas, 2005; Brun and Chambas, 2014) involves a change of the tax structure in favour of a high dependence on domestic tax revenue at the expense of international trade tax revenue. Some studies (e.g., Baunsgaard and Keen, 2010; Waglé, 2011; and Crivelli, 2016; Moller, 2016) have been conducted on whether the revenue replacement strategy has been successful. Those studies have not explicitly focused on the extent of the tax transition reform (that would replace the public revenue losses), although the studies on the revenue replacement strategy could provide an insight into the extent of the tax transition reform. For example, Baunsgaard and Keen (2010) have obtained empirical evidence that middle-income countries and high-income countries have recouped the lost trade tax revenue from other sources. In contrast, for low-income countries (LICs), the replacement rate is low, although signs of recovery vary across these countries. Waglé (2011) has obtained that the tax recovery in LICs is much more robust than shown by Baunsgaard and Keen (2010), although long term replacement is statistically significant only for few LICs. Moller (2016) has reported that low-income countries that have enjoyed a significant tax recovery are those that have simultaneously initiated a process of democratization. Based on a sample of transition economies in Eastern Europe, the former Soviet Union, and North Africa and the Middle East, Crivelli (2016) has provided empirical support for the strong revenue replacement of total domestic tax revenue with trade tax revenue lost, including through the Value-added Tax (VAT), and the personal income tax (PIT).====While some studies have examined the determinants of tax reform considered in a general sense (or the extent of such a tax reform) (e.g., Mahon, 2004; Sánchez, 2006; Di John, 2006; Attila et al., 2009; Lora, 2012; Focanti et al., 2013; Gnangnon, 2017; Adandohoin, 2018), few of these (including Attila et al., 2009; Adandohoin, 2018) have looked at the determinants of tax transition reform, as defined above. Adandohoin (2018) has investigated the role of Value Added Tax (VAT) and excises as first wave tax transition tools in developing countries.====Even though scarce studies have looked at the effect of the Internet usage (the share in the total population of the number of individuals using the Internet) on public revenue==== (Gnangnon and Brun, 2018, 2019), the few existing studies on the determinants of tax transition reform have paid little attention to the effect of the Internet usage - which underpins the technology-driven industrial revolution, i.e., the digitization of economies around the world - on tax transition reform in developing countries. In particular, even though the role of digital revolution in transforming public finances has received a particular attention in recent years (e.g., Bird and Zolt, 2008; OECD, 2016; Cotton et al., 2017a,b; OECD, 2017; IMF, 2017), to best of our knowledge, no empirical study exists on the effect of the Internet usage (among other Information and Communication Technology (ICT) tools) on the implementation of tax transition reform in developing countries. The current article examines whether one important characteristic of the digital revolution, that is, the substantial rise in the Internet usage could influence the extent of tax transition reform in developing countries. Exploring this topic is all the more relevant that lies on the fact that some recent studies highlighted above have argued that the rise of the digital economy brought about by the fourth industrial revolution is having significant influences on public finances (e.g., Bird and Zolt, 2008; OECD, 2016; Cotton et al., 2017a,b; OECD, 2017; IMF, 2017).====The current study measures the extent of tax transition reform by an indicator that reflects the degree of convergence of developing countries' tax structure towards the tax structure of developed countries. The empirical analysis is performed using an unbalanced panel dataset of 102 developing countries, over the period 1995-2015. The empirical exercise shows that a rise in the Internet usage exerts a positive and significant effect on tax transition reform in developing countries. This positive effect appears to be higher in low-income countries than in other countries. Additionally, evidence is obtained that countries that further open up to international trade (including thanks, ====, to the Internet) experience a higher positive effect of the Internet on the extent of tax transition reform.====The rest of the paper is structured as follows. Section 2 discusses the channels through which the Internet access could influence tax transition reform, as well as whether this impact could depend on countries' degree of international trade openness. Section 3 describes the measure of tax transition reform. Section 4 presents the model specification, while Section 5 discusses the econometric strategy. Section 6 interprets empirical results, and Section 7 concludes.",Internet and tax reform in developing countries,https://www.sciencedirect.com/science/article/pii/S0167624519300757,30 January 2020,2020,Research Article,78.0
"Okada Yoshimi,Nagaoka Sadao","Institute of Innovation Research, Hitotsubashi University, 2-1 Naka, Kunitachi, Tokyo 186-8601, Japan,Tokyo Keizai University / Research Institute of Economy, Trade and Industry, 1-7-34, Minami-cho, Kokubunji-shi, Tokyo 185-8502, Japan","Received 21 January 2018, Revised 26 November 2019, Accepted 24 January 2020, Available online 25 January 2020, Version of Record 13 June 2020.",https://doi.org/10.1016/j.infoecopol.2020.100852,Cited by (0),"In order to assess effects of early publication of ==== applications on the dissemination of the disclosed technological knowledge, this study examines the impacts of the 18-month pre-grant publication system introduced in the United States in 2000. It focuses on the variations of the applicant non-self-citations driven by the actual policy reform, unlike prior studies, after demonstrating that including examiner citations causes a systematic bias toward early knowledge flows to subsequent inventors. It finds that the citation probability rose significantly in early stage following the early publication, which shows that the reform accelerated knowledge diffusion significantly. Furthermore, the effect was stronger in the fields with longer publication lag before the reform (strongest in Computers & Communications, followed by in Drugs and Medicals). The reform looks to have helped inventors to recognize potential duplication and follow-on invention opportunities earlier. In addition, the publications of eventually abandoned patent applications, which had not been published before the reform, are found to have become significant new knowledge sources.",None,Effects of early patent publication on knowledge dissemination: Evidence from U.S. patent law reform,https://www.sciencedirect.com/science/article/pii/S0167624518300131,25 January 2020,2020,Research Article,79.0
Peng-Ju Su Alice,"Department of Economics, National Taipei University, 151 University Rd., Sanxia Dist., New Taipei City237, Taiwan","Received 19 August 2018, Revised 13 October 2019, Accepted 14 January 2020, Available online 15 January 2020, Version of Record 11 March 2020.",https://doi.org/10.1016/j.infoecopol.2020.100851,Cited by (1),"In this study, I consider the effects of a minimum wage policy on an employment contract in the presence of asymmetric information. The key channel comprises how a minimum wage acts jointly with incentive compatibility (truthful revelation) given different levels of information advantage after signing the employment contract. This information advantage is captured by the timing of information arrival either before or after employment contracting. The minimum wage manipulates the rent-efficiency tradeoff with ex-ante information arrival, and it introduces a binding incentive compatibility with ex-post information arrival. I also discuss the implications of minimum wage ","In this study, by recognizing that the labor market comprises employment contracts, I study the impacts of the minimum wage policy in a contract theoretical framework with asymmetric information. The information structure at the contracting stage plays a role in the policy evaluation for minimum wages because it is related to the fundamental tradeoff in the contracting relationship with the minimum wage legislation. I characterize the effects of minimum wage legislation on the efficiency of productivity and welfare distribution, as well as how these effects are related to the employee’s information advantage after signing the contract.====In a contractual relationship where the employee has private information based on his ability measured by the cost of production, the optimal employment contract is a menu of options, where each comprises a pair of outputs from the task and associated wage. How binding minimum wage legislation affects this contract depends on how it shapes the tradeoff behind the optimal contract, which depends on the timing of information arrival. If the employee’s private information arrives before signing the contract, e.g., the employee is well aware of his own ability, then the employee has an information advantage upon contracting. The optimal employment contract is a result of the rent-efficiency tradeoff. Efficiency is traded off to reduce the information rent required for binding incentive compatibility. An effective minimum wage acts as a counter force that manipulates this tradeoff by imposing a lower bound on the distorted efficiency.====If the employee’s private information arrives after signing the contract but before conducting the task, e.g., a fresh graduate who is imperfectly informed about his match with the task until it is encountered, then the employee does not have an information advantage upon contracting. The optimal contract results in no tradeoff in terms of efficiency with slacking incentive compatibility. An incentive is induced by an ex-post exploitative contract, which the low-skilled employee who has a high cost of production is unable to reject due to late information arrival. An effective minimum wage introduces a new tradeoff with respect to this employment contract. It mitigates the employer’s ability to lower the monetary wage paid to the high-cost employee even if the latter does not have an information advantage when signing this contract. Providing a higher wage to this employee violates the incentive compatibility, which would be slacking otherwise. A binding minimum wage acts as a binding constraint itself and indirectly introduces a binding incentive compatibility constraint into the contracting problem.====With a sufficiently low minimum wage such that separating the employment contract is optimal, a binding minimum wage results in a binding incentive compatibility even when the employee does not have an information advantage upon contracting, and it tightens the incentive compatibility when the employee has an information advantage. In the former scenario, production by the high-cost employee exhibits inefficient upward distortion. In the latter scenario, the binding minimum wage restores the output by the high-cost employee toward efficiency because it countervails the rent-efficiency tradeoff. A socially optimal binding minimum wage exists only when the employee has an information advantage when signing the contract.====In terms of welfare and monetary wage inequality, a binding minimum wage together with a more restrictive incentive compatibility reduces inequality in the monetary wage but increases the inequality in terms of the cost of production from the upward distortion of the output. The magnitude of the increased inequality in the cost of production is higher than that of the reduced wage inequality. Thus, welfare inequality is more severe with a binding minimum wage. Inequality of welfare is a result of binding incentive compatibility, which occurs after signing the contract when information is asymmetric, arriving early or late. This inequality result does not qualitatively depend on the timing of private information arrival, whereas the distribution of the welfare inequality does.====With an information advantage upon contracting, the high-cost employee is sufficiently well informed to reject an ex-post exploitative contract. The optimal contract to induce participation requires that the high-cost employee earns his reservation utility. With a binding minimum wage, incentive compatibility is satisfied solely by a spillover information rent to the high-skilled employee who has a low cost of production. Without an information advantage upon contracting, the high-cost employee is not sufficiently perfectly informed to reject an ex-post exploitative contract. The optimal contract to induce participation requires that the employee expects to earn his reservation utility when signing the contract. With a binding minimum wage, incentive compatibility is satisfied by a spillover rent to the low-cost employee as well as by further lowering the negative information rent to the high-cost employee. Raising the minimum wage only improves the welfare of the low-cost employee, and whether it is detrimental to the employee who earns the minimum wage depends on the employee’s information advantage when signing the contract.====With an intermediate minimum wage such that it is still optimal to employ both types of employee, failure of screening occurs as an additional form of inefficiency. With a sufficiently high minimum wage, it is not feasible to confine the employment contract to the minimum wage legislation while also screening the low-cost employee from the high-cost employee. Binding individual rationality is not relaxed with a sufficiently high minimum wage. This implies that the high-cost employee earns his reservation utility with an information advantage upon contracting, whereas he suffers a higher loss without an information advantage upon contracting.",Information advantage and minimum wage,https://www.sciencedirect.com/science/article/pii/S0167624518301914,15 January 2020,2020,Research Article,80.0
Jung Juan,Universidad Complutense de Madrid and Centro de Estudios de Telecomunicaciones de América Latina (cet.la),"Received 10 April 2019, Revised 6 December 2019, Accepted 7 January 2020, Available online 8 January 2020, Version of Record 11 March 2020.",https://doi.org/10.1016/j.infoecopol.2020.100849,Cited by (10),"Closing the digital divide and fostering the digital economy is considered one of the keys for the countries to increase productivity and economic growth. To achieve those objectives, investment in telecommunications networks is crucial. This paper develops a theoretical framework to explain the link between public institutions and telecommunications investment. This model is estimated for a sample of 13 European countries during the period 2007–2015. Results were clear in verifying a positive association between institutional quality and investment levels. These findings were robust to different specifications of the model, and to the control of potential endogeneity linked to the institutional variable. Novel findings also pointed out at institutional quality being more relevant for most disadvantaged countries, in terms of development and digital connectivity. Furthermore, we found evidence of Property Rights being the main cause of concern for telecom operators, followed by ====, judicial independence, transparency, and in a lesser degree, by political favoritism and trust.","Telecommunications diffusion has been identified in the economic literature as a potential source for raising productivity and economic growth. Following the relevant contribution of Aschauer (1989) about public capital as a productivity determinant, several authors have found empirical evidence of the economic impact of voice telecommunications (Röller and Waverman, 2001), and more recently, broadband connectivity (Koutroumpis, 2009; Qiang et al., 2009; Czernich et al., 2011; among many others). There is little doubt that broadband nowadays constitutes a key part of the necessary infrastructure for development, in the same way as railroads, motorways and electricity. Current and forthcoming advances, such as Internet of Things, Big Data, Artificial Intelligence and automatization will only make this topic even more relevant in the future.====Then, it is no surprise that most countries have promoted in recent years Digital Agendas or Broadband Plans intending to promote investments in network deployments to massify connectivity. The fact that massive funds are required for these deployments==== and that networks are largely irreversible, make uncertainties play a critical role in investment decisions in the telecommunications sector. Therefore, such a considerable effort will surely require of a propitious regulatory and institutional environment to become feasible. Thus, the analysis of the determinants of telecommunications investment should be a top priority for researchers and policymakers, especially in those countries that still have a considerable digital divide to close.====While the impact of regulation intensity on investment decisions has been analyzed in the past (see for instance the relevant contribution provided by Alesina et al., 2005), it is surprising to find out that there is very little empirical evidence regarding the role of institutional quality for investment in telecommunications, being this such a critical sector for the economy. This article intends to fill this gap in the empirical literature.====The very few articles that have analyzed the link of institutional variables on the telecommunication sector, have focused on its effect on penetration levels for alternative services, rather than in investment decisions by telecommunications operators (see for instance Henisz and Zelner, 2001; Andonova, 2006). Another important difference with most related papers is that they rely on ad-hoc empirical specifications, while on the contrary, through these lines we will develop a theoretical model with the specific purpose of understanding the main drivers of investment decisions in the telecommunications sector, specifically disentangling the different effects attributable to regulatory and institutional environment.====This paper is structured as follows: next section reviews the main literature regarding institutions and economic performance, Section 3 develops a theoretical model to understand the main drivers of investment decisions, Section 4 presents the dataset with its main descriptive statistics, Section 5 specifies the empirical specification and reports the estimation results, and finally Section 6 ends with some concluding remarks.",Institutions and Telecommunications Investment,https://www.sciencedirect.com/science/article/pii/S0167624519300770,8 January 2020,2020,Research Article,81.0
Zennyo Yusuke,"Graduate School of Business Administration, Kobe University, 2-1 Rokkodai, Nada, Kobe, 657-8501, Japan","Received 7 January 2019, Revised 4 December 2019, Accepted 9 December 2019, Available online 10 December 2019, Version of Record 11 March 2020.",https://doi.org/10.1016/j.infoecopol.2019.100848,Cited by (8),"This paper studies competition between ad-sponsored platforms that strategically determine business models. In addition to basic services including annoying advertisements, each platform decides whether to introduce an ad-free premium service (i.e., a freemium business model). Freemium platforms encounter a trade-off between increasing the number of premium users for the subscription-based revenues and increasing the number of basic users for the ad-sponsored revenues. I characterize how the freemium platforms should segment their users into basic and premium services. Moreover, I show that the equilibrium business model choice depends on the extent of fixed costs for introducing a premium service. When the fixed cost is at an intermediate level, asymmetric equilibria may arise: i.e., only one platform introduces the premium service. Competing platforms may have an incentive to coordinate their choices toward asymmetric market structures; however, these structures can be harmful to both consumers and advertisers.","Many ad-sponsored platforms, such as Spotify, Deezer, Pandora, YouTube, Flickr, and many smartphone applications, offer basic (or free) services including annoying advertisements as well as ad-free premium services.==== For example, Spotify is a music-streaming service providing both free and premium services. In the free service, some advertisements are inserted between songs, but if users upgrade to the premium service by paying $9.99/month, they can eliminate these annoying advertisements. On October 28, 2015, Google introduced a new ad-free membership named YouTube Red, which allows users to enjoy all YouTube videos without advertisements for $9.99/month. The above business model can be considered a form of ====.====In reality, not all ad-sponsored platforms adopt the freemium model. For instance, many mobile applications rely on revenue from in-app advertisements.==== The video-streaming platform, Dailymotion, is largely financed by ad revenue. A possible factor determining the business model is the extent of fixed costs for introducing an ad-free paid service additionally. For example, when introducing an ad-free premium service, a computer program/system must be built to prevent any advertisements from being displayed to their premium users. In another instance, if basic services are provided for free, platforms must newly consolidate a payment system for their paid users. These initial investment costs influence the platforms’ decision on whether to introduce ad-free premium services. Many mobile applications are developed by small firms (or even individuals) that appear to have insufficient funds to prepare for their initial fixed costs,==== whereas streaming services are typically operated by relatively large companies that are well equipped to incur the related fixed costs.====From the perspective of strategic relationships among competing platforms, however, it is not clear if all platforms introduce premium services even when the fixed costs are not too high. If all of them adopt the freemium business model, fierce competition may take place not only between their basic services but also between their premium services. In contrast, different business model choices may serve to ease the intense competition by inducing the competing platforms to target different user segments. Therefore, the purpose of this paper is to examine the strategic decision of ad-sponsored platforms on whether to adopt the freemium model or not. In addition, I also address whether the resulting market structure in equilibrium is beneficial to consumers and advertisers.====To this end, I develop a model of competition between two ad-sponsored platforms, which contains the following three features: (a) users are heterogeneous with respect to their utility losses from advertising; (b) platforms can provide both an ad-sponsored service (i.e., basic service) and an ad-free service (i.e., premium service) simultaneously; and (c) platforms endogenously determine whether to introduce the premium services or not.====Feature (a) creates users’ endogenous decision-making for the basic or premium service. That is, if a platform decides to use the freemium model (i.e., features [b] and [c]), there exists an interesting trade-off between increasing the number of premium users for subscription-based revenues and increasing the number of basic users for ad-sponsored revenues, which is crucially related to the strategy on how to segment users into basic and premium services.====In this regard, this paper provides the characterization of the equilibrium strategy of freemium platforms. How to segment users into basic and premium services significantly depends on the distribution of users’ disutility from advertisements. In equilibrium, the price discount for the basic service is set to be equal to the advertising revenue that the platform obtains from a unit of basic users. That is, the advertising revenues are fully utilized to discount the price of the basic service. Therefore, the equilibrium price of the basic service can take a negative value (i.e., ====) if the platform can gain a lot from advertising revenues. This is typical of two-sided pricing when indirect network externalities across two sides are divergent.====Moreover, I demonstrate the equilibrium business model choices. Adopting the freemium model is the dominant strategy when the fixed cost of introducing a premium service is small enough. Interestingly, when the fixed cost is at an intermediate level, asymmetric equilibria can arise, in which only one platform introduces the premium service. The asymmetric market structures enable the competing platforms to target different user segments than each other, which mitigates the platform competition. The freemium platform targets two polar user segments who are either tolerant or allergic to advertisements, whereas the ad-sponsored platform effectively deals with the remaining users with midrange preferences for advertisements.====From a different viewpoint, the lower competition under asymmetric market structures incentivizes the platforms to coordinately choose different business models with a monetary transfer that satisfies the incentive compatibility constraints for them. However, I also find that such coordination is detrimental to consumers and advertisers. These findings indicate the important caveat that anticompetitive coordination may emerge.",Freemium competition among ad-sponsored platforms,https://www.sciencedirect.com/science/article/pii/S016762451930006X,10 December 2019,2019,Research Article,82.0
Savelkoul Ruben,"KU Leuven, Belgium","Received 12 April 2019, Revised 21 November 2019, Accepted 5 December 2019, Available online 5 December 2019, Version of Record 11 March 2020.",https://doi.org/10.1016/j.infoecopol.2019.100847,Cited by (8),"This paper estimates the causal effect of illegal downloading on recorded music sales volumes. We explicitly allow for differing effects of piracy on superstars versus other songs (i.e. songs or artists that are respectively ranked at the top or lower in the sales distribution), with an extension about product variety. We use a difference-in-difference approach, exploiting the natural experiment of the introduction of the HADOPI anti-piracy law in France in 2009, using Belgium and the Netherlands as a control group. We find a positive effect on music sales after the introduction of the law, thus implying a negative effect of music piracy. The effect is greater for top selling songs compared to lower ranked songs. It is stronger shortly after the introduction of the law and diminishes in later periods for all songs except the top sellers or superstars. After the introduction of the law, consumption became more concentrated in terms of musical genre and style, indicating that piracy increases consumed product variety.","Ever since the rise of illegal downloading on the internet, there have been efforts to estimate its impact on music sales. It has especially proved difficult to separate the effect on sales from more general effects related to digitization. Another point of discussion is the influence of piracy (and more generally the digital economy) on the distribution of sales between superstar artists and niche artists and its impact on product variety. This paper disentangles the effect of piracy from more general digitization trends on music sales, while also considering how this effect is distributed over groups of songs based on sales (from superstars to smaller, potentially niche, artists). The empirical strategy is based on the natural experiment of the introduction of the HADOPI anti-piracy law in France. In order to answer questions about product variety, we extend our analysis by looking at the effect of the law on concentration of sales in terms of musical genres and styles.====HADOPI was passed as an anti-piracy law in 2009 in France. The law empowers the administrative authority to monitor online infringement and act against acts of piracy based on information by rights holders. The voting of the law caused a lot of controversy and media attention. Further, it was accompanied by a campaign to increase awareness among citizens about the illegality of file sharing and its possible impact on future generation of content. Therefore, it is to be expected that the introduction of HADOPI had a significant impact on illegal downloading relative to countries that did not pass comparable laws.====It is interesting to investigate the effect of the introduction of HADOPI because it is unclear whether it is positive or negative, especially for smaller or niche artists. The reason for this is that piracy enables consumers to sample music for free. This may lead consumers to discover more (niche) music that they would otherwise never listen to. If some of these consumers buy the music after sampling, sales increase because of piracy. The total effect of piracy on sales however also depends on the size of the substitution effect. This effect occurs when consumers use piracy to acquire music without the intention to buy it through a legal channel later. In that case the download does not act as a sample, but rather as a substitute for legal purchases. Since superstars have little to gain from free sampling, because they are already well known, it is possible that the effect is different between them and smaller artists.====The results from this paper indicate that there is a significantly different effect of piracy on music sales between superstars and other artists. We find that all songs are positively affected by HADOPI (and thus negatively by piracy), but the effect is significantly greater for songs that are ranked in the top of the sales distribution. Apart from concentration of sales in the top of the distribution, we find that the effect of HADOPI dies out over time for all but the songs with the highest sales. When focussing explicitly on product variety, we also find that HADOPI led to further concentration of sales in terms of musical genres and styles. This suggests that piracy does have a positive effect on variety or product discovery.====Some empirical work has been done related to the distributional and variety aspect of our research question, but most of it studies the shock of the introduction of Napster in 1999. This is not an ideal approach since the introduction took place all over the world simultaneously and it coincided with other technological advances that affected the music industry, limiting most of the analyses to ’before-after’ comparisons. Mortimer et al. (2012) rank artists according to revenue and find lower negative growth on recorded music sales for lower ranked artists following the introduction of Napster, as well as a more positive effect on revenues from live concerts. Waldfogel, 2012, Waldfogel, 2015 finds that quality of supplied music is stable or increasing and that artists signed to independent record labels reach the (top of the) charts more frequently after 1999. Aguiar and Waldfogel (2016) further find that more products with modest prospects reach the top of the charts, but that this does not decrease concentration in sales. Related to this observation, Benner and Waldfogel (2016) find that major record labels now concentrate even more on artists with high prospects, effectively leading to (even) more chart success for superstars. Handke (2012) finds an increase in the number of releases for the same period.====These analyses are interesting and suggest links with piracy. However, they can not distinguish between the effects of illegal downloading, cheaper ways to record and release music, new promotional channels that make use of the internet, and other developments related to digitization. This makes it difficult to infer causal links between recent trends in the music industry. A solution could be to exploit a natural experiment such as the introduction of HADOPI. If we assume that the HADOPI law only affects downloading behavior in France and has no effect on technological advances, it can help us isolate the effect of piracy on our variables of interest.====Other papers have taken the natural experiment approach before, disentangling the effect of piracy from more general digitization trends. Danaher et al. (2014) showed that introducing HADOPI had a significant positive impact on iTunes sales for major label releases in France. Adermon and Liang (2014) have come to a similar conclusion regarding the introduction of the similar IPRED law in Sweden in 2009. Other approaches that effectively isolate the causal effect of piracy on music sales include surveys (Rob and Waldfogel, 2006) or an IV approach (Oberholzer-Gee and Strumpf, 2007). Although these papers’ approaches succeed in disentangling the effect of piracy from other trends, none of them attempt to identify any differential effect of piracy between superstars and niche artists (i.e. songs or artists that are respectively ranked high and low in the sales distribution). Therefore these papers do not provide an answer to the distributional aspect of our research question. The contribution of this paper thus lies in the combination of an approach that allows for isolating the causal effect from piracy, while still allowing for differential effects between different types of artists or songs. To the best of our knowledge, this question has never been studied before.====The results in this paper are somewhat in line with the results of Lee (2018), who estimates the causal effect of private-network music sharing on aggregate music sales and finds that superstars are affected negatively while mid-tier artists experience a positive effect from piracy. While this paper does not find a positive effect for smaller or mid-tier artists from piracy, it confirms that the effects from piracy are different between superstars and mid-tier or smaller artists. Note that the conclusions in Lee (2018) apply to private-network sharing specifically, whereas this paper focusses on the effect of all peer-to-peer networks.====Strictly speaking, we could directly interpret our findings about the effect of piracy on sales distribution in terms of product variety, since it is often argued that every artist (or even song) is different from any other artist (or song). Any increase in sales for new or niche songs compared to top hits could then be interpreted as an increase in product variety consumed. Although this reasoning makes sense in theory, it is easy to see its limitations: even when observing that sales are spread across more different artists/songs, consumers might gravitate towards more similar songs. In order to explicitly consider product variety, we extend our analysis by looking at the effect of piracy on the concentration of sales in terms of musical genres and styles. This question has also not been studied before.====The paper continues as follows: Section 2 summarizes theoretical work about the impact of piracy and formulates hypotheses. Section 3 provides background of the HADOPI introduction. Section 4 describes the data. Section 5 discusses the empirical strategy and Section Appendix F shows results. Section 7 provides an extension on product variety. A broader discussion of our results relating to other topics is given in Section 7 and 8 concludes.",Superstars vs the long tail: How does music piracy affect digital song sales for different segments of the industry?,https://www.sciencedirect.com/science/article/pii/S0167624519300782,5 December 2019,2019,Research Article,83.0
"Chillemi Ottorino,Galavotti Stefano,Gui Benedetto","Department of Economics and Management, University of Padua, Italy,Department of Economics, Management and Corporate Law, University of Bari, largo Abbazia Santa Scolastica 53, Bari 70124, Italy,Department of Economics and Management, Sophia University Institute, Loppiano, Figline e Incisa Valdarno (Florence), Italy","Received 7 January 2019, Revised 22 November 2019, Accepted 24 November 2019, Available online 25 November 2019, Version of Record 11 March 2020.",https://doi.org/10.1016/j.infoecopol.2019.100843,Cited by (5),"We study the properties of simple mobile broadband Internet plans characterized by a data cap and an up-front price. Assuming demand uncertainty and a positive correlation between users’ valuations and (expected) demands, we show that a monopolistic provider does always find it convenient to offer one plan with binding cap together with an unlimited one. We characterize all the elements of the profit-maximizing plans, showing that medium users, but not heavy users, will incur the risk of being capped, and that, relative to the benchmark case of unlimited access only, the introduction of a plan with cap leads to a redistribution of surplus from high users to low users. The overall effect on social welfare and users’ surplus is, in general, ambiguous, but is certainly negative if low valuation users have a sufficiently homogeneous demand for Internet traffic.","If you shop for an Internet plan for your mobile phone, you can realize how commonplace are data caps. In the simplest data plans, a customer’s service is terminated if she has depleted her traffic allowance; in other plans, the connection speed is greatly reduced. Plans with data caps are the rule also in the market for home Internet connections, where they are often accompanied by overage charges: if the user exceeds her allowance, she will be charged additional fees to go on using the Internet.====The surge of data caps in recent years has prompted an intense debate on their pros and cons.==== Advocates of data caps – primarily the Internet Service Providers (ISPs) themselves – argue that data caps, by disciplining consumers’ usage, reduce network congestion and favor a more efficient usage of the network.==== On the other hand, some interest groups claim that caps, by discouraging the use of certain data-intensive applications, are often intended to protect the ISP’s other services from competition.====Quite soon, some scholars and stakeholders pointed out that the presence of several service tiers in a provider’s portfolio of offerings is clear evidence of a price discrimination strategy: in this view, data caps should be interpreted as a screening device employed by providers to extract surplus from subscribers.====Whichever motive prompted providers to adopt them, the introduction of data caps in a world that was formerly dominated by unrestricted access has certainly produced welfare consequences. The ISPs claim that caps actually benefit most users, for at least two reasons. First, with a flat-price plan with no cap (all users pay the same price for unlimited Internet access), light users implicitly subsidize heavy bandwidth users. The introduction of plans with data caps has then determined a redistribution of surplus from the latter to the former. Second, the introduction of small plans has favored the expansion of the market, by attracting consumers with low willingness to pay. In rebuttal, opponents complain that caps hurt most users, especially the heavy ones, either because they are not able to make the desired usage of the network and have to unwillingly modify their network consumption habits, or, when overage charges are present, because they experience bill shocks as they exceed their usage thresholds. In this sense, plans with unlimited traffic represent an insurance against fluctuations in users’ demand. Some also argue that data caps enable the provider to levy extra-revenues by exploiting the subscribers’ biased tendency to choose higher data caps than necessary.====Although the price discrimination scope of data caps has been recognized by many, a thorough and formal understanding of this strategy and its welfare consequences is still missing. We thus contribute to fill this gap by theoretically studying the problem of a provider who optimally sets a menu of plans with data caps in order to maximize profits.====The key elements of our model are two. First, we assume that a capped user – a user who exhausts her traffic allowance – suffers a discrete loss of utility. This is in contrast with much of the related literature, where it is usually assumed that a user derives utility from each unit of data downloaded or uploaded. However, we believe that our novel assumption is more appropriate in many cases. Think of an individual who needs Internet access to perform a task through her mobile phone. For example, this task could be participating in a conference call, downloading a dataset or uploading a buy/sell order. In these cases, it is reasonable to believe that the individual will suffer a significant loss in utility if she is not able to complete the desired task. In other words, we share the idea that Internet consumption is characterized by important indivisibilities, as many activities require a single download or upload of substantial size (see Nevo et al., 2016). Also, the evidence that consumers show a preference for flat-rate plans with unlimited data transfer (see Lambrecht, Skiera, 2006, Lambrecht, Seim, Skiera, 2007, Liu, Prince, Wallsten, 2018) seems to indirectly support the idea that hitting the cap is costly to users. The second fundamental ingredient of the model is demand uncertainty: at the time of subscribing a plan, a user does not know the exact amount of traffic she will need in the following period. Specifically, we assume that a user’s (expected) demand of Internet traffic is positively correlated with her valuation, an assumption that appears reasonable and is also empirically supported (see, e.g., Liu et al., 2018). As we will show, it is the interplay of these two key elements that makes data caps a powerful tool for the provider to profitably discriminate across users, and this eventually produces significant welfare and distributional effects.====For the sake of tractability, we also introduce a few simplifying assumptions. To isolate the pure incentive to impose caps from other competitive goals, we focus on a monopolistic provider. Besides, to abstract from issues related to congestion and network management, we deliberately treat Internet access as a homogenuous product that is delivered at zero marginal cost and without capacity constraints. Finally, we concentrate on plans with tight data caps, i.e. plans that envisage an interruption of the service once the user has exhausted her traffic allowance. While most fixed broadband providers allow subscribers reaching the monthly threshold to go on surfing by paying overage charges, the practice of blocking access is fairly common in the mobile industry: for example, looking at the four largest mobile operators in Italy, plans with tight caps are the rule, at least for those users who do not opt for paying through credit card or bank account. The likely reason is simple: these users are liable for their residual credit only. Therefore, obtaining extra-payments could be problematic (or very costly) for the provider, whereas she can easily and immediately interrupt the service (or implement a lower connection speed) when the cap is reached.====Under the above assumptions, we determine how a profit-maximizing provider should design her plans, and analyze the effects on welfare in comparison with a world where only an unlimited flat-rate plan is offered. Focusing on a situation where the provider can offer at most two plans, our main findings can be summarized as follows. First, the provider will always find it optimal to offer one plan with a non-binding cap – a cap that is so high that no user will run the risk of being capped – and one with binding cap: the former will be subscribed by high demand users, while the latter will be subscribed by intermediate/low demand ones. As a result, it is not heavy users, but medium users that will bear the risk of being capped. This suggests that, even in a world of unlimited transmission capacity and no congestion problems, the ISPs would still find it profitable to maintain data caps. Second, the effects on social welfare and users’ surplus crucially depend on the choice of the binding cap. In particular, a sufficiently low cap would be a win-win situation: not only the provider’s profit would be larger, but no user will be worse-off and some will be strictly better-off. However, this Pareto-improving outcome will not emerge if the provider is free to set the binding cap at her preferred level: in this case, we show that, while the introduction of a plan with cap attracts some former non-subscribers and benefits some intermediate users who now find a plan that better suits their needs, high valuation users are strictly worse-off than previously, either because they have to pay a higher price for unlimited access, or because they have to reluctantly switch to the plan with cap, running the risk of being interrupted. Therefore, while there is certainly a redistribution of surplus from high to low users, the overall effect on social welfare and users’ surplus may be positive or negative, and this depends on the structure of the demand side. In this respect, our analysis suggests that a crucial element is the relation between users’ valuations and demands: we show that, under certain conditions, when the demand of Internet traffic by low valuation users is sufficiently homogeneous, the introduction of data caps certainly reduces social welfare and users’ surplus.====In light of the results from our welfare analysis, we conclude the paper with some policy recommendations: in particular, we suggest that a proper regulation of the caps that can be imposed by providers and/or a rule that allows users to transfer the unconsumed traffic in one period to the next period’s allowance are viable ways to contrast the potentially harmful effects of data caps on users.",The impact of data caps on mobile broadband Internet access: A welfare analysis,https://www.sciencedirect.com/science/article/pii/S0167624519300046,25 November 2019,2019,Research Article,84.0
"Belhadj Nada,Laussel Didier,Resende Joana","ISG, University of Tunis, Tunisia,Aix-Marseille Univ, CNRS, EHESS, Centrale Marseille, AMSE, France,Cef. Up, Economics Department, University of Porto, Portugal","Received 30 April 2018, Revised 29 October 2019, Accepted 16 November 2019, Available online 21 November 2019, Version of Record 11 March 2020.",https://doi.org/10.1016/j.infoecopol.2019.100834,Cited by (14),"This paper shows that the platforms’ ==== on demand may explain the empirical observation that platforms like Amazon resell high-demand products, while acting as marketplace for low-demand goods. More precisely, the paper examines the strategic interaction between a seller and a better informed platform within a signalling game. We consider that the platform may choose between two distinct business models: act as a reseller or work as a pure marketplace between the buyers and the seller. The marketplace mode, which allows to internalize the ==== between the platform’s sales and the seller’s direct sales is always preferred for a low-value good. The reselling mode, which allows the platform to take advantage of its ====, may be selected in the case of high-value goods provided that (i) the externalities between direct sales and platform sales are not too strong and (ii) the difference between consumers’ willingness to pay for the high and the low-value goods is large enough. Under these conditions, the game displays a Least-Cost Separating Equilibrium in which the platform works as a marketplace for low-demand goods, while it acts as a reseller in the case of high-demand goods.","In the last years, many economic sectors have experienced considerable changes. While in some cases there has been a significant disintermediation process, in other cases new intermediation players have emerged. Digital platforms are a good example of this last phenomenon (e.g. Amazon, Ebay or Alibaba). These platforms have adopted different types of business models: some of them act as resellers (retailers), whereas others are pure marketplaces, which bring together sellers and buyers (without making any sales decisions). Finally, in some other cases, digital platforms act both as a marketplace (for some products) and as a reseller (for other products). For example, Rysman (2009) reports that the Internet giant Amazon sometimes acts as a conventional reseller (Amazon Retail), while on other occasions it offers a marketplace to independent buyers and sellers (Amazon Marketplace). For some particular products (e.g. specific books), Amazon basically buys the product at a wholesale price and then resells it. However, for many other products,==== Amazon adopts a two-sided business model, acting as a platform which connects buyers and sellers, with sellers choosing both the sales level and the price of the product.====Jiang et al. (2011) provided significant empirical evidence that platforms like Amazon tend to resell high-demand products while letting third-party independent sellers sell low-demand goods (with the platform opting for the marketplace mode). For example, in the category of electronics, they noticed that Amazon was reselling only 7% of the products but that those products included 64% of the 100 best sellers. In the categories of Tools and Home improvement, Sports and Outdoors, Jewelry, Toys and Games and Shoes the respective figures were 5.8% and 88%, 3.1% and 76%, 3.2% and 34%, 5.9% and 66%, 16.7% and 72%.==== These findings have been confirmed by Zhu and Liu (2016) who concluded from a large scale empirical study using data from Amazon.com that ====.====The present paper proposes a very stylized model to examine whether asymmetric information on demand may constitute one possible determinant explaining why the platform may prefer to resell a high-value good, while acting as a marketplace for low-value goods.====To this end, we analyze the signalling game played between an intermediary platform and a representative seller (thereafter “she”), who may be interested in using the platform to distribute its product. More precisely, we assume that the seller may use (either separately or together) two distribution channels==== to reach its final consumers: ==== its own direct sales channel, such as a brick and mortar shop or an online website; ==== an independent platform. One may find plethoric evidence of the combination of these business practices. For example, HP computers are for sale on HP online store as well as on Amazon and other platforms, Nike shoes may be purchased on Nike official site as well as on Amazon, Dior perfumes are available on Dior’s official site as well as on many intermediaries sites including Amazon.====In our set-up, the platform has private information about demand. This modelling option intends to account (in a very simple way) for the digital platforms’ ability to collect, gather and process enormous volumes of data==== on their customers (e.g. demographic data, data on consumers’ online behavior, or data on consumers’ previous online searches and purchases). In this respect, Petro (2017) actually argues that ==== In a completely different set-up, Heiko and Peitz (2017) find that an advertising agency or an Internet portal can make use of a tracking technology which allows it to perfectly identify a consumer’s preferred product category, giving support to our assumption that the platform may be better informed about demand than the sellers.====The strategic interaction between the representative seller and the privately informed platform is here examined within a sequential game. In the first stage of this game, the platform first (privately) learns the true value of the good for consumers and then proposes a trading contract to the seller. This contract may be either a marketplace contract (in which all the commercial decisions are transferred to the seller) or a reselling contract (in which the platform itself defines the platform’s sales level). In the second stage, after revising her beliefs about the consumers’ tastes using the Bayes’ Rule, the seller decides to accept or reject the platform’s proposition. Finally, in the last stage, the seller takes her sales decisions: in the case of a marketplace contract, she chooses both the level of direct sales and the level of sales intermediated by the platform, whereas in the case of a reselling contract, she only chooses the level of direct sales. In this context, the choice of the platform sales is a “transferable action”====: it is made by the platform under the reselling mode and by the seller under the marketplace mode. Differently, the choice of the direct sales level is a “non-transferable action”: it is determined by the seller in the reselling mode as well as in the marketplace one.====We find that asymmetric information (with the platform being better informed than the seller about final demand) may indeed lead the platform to prefer to resell a high-value good rather than acting as a pure marketplace. More precisely, we find that there is a separating equilibrium in which, all other things equal, the platform is willing to act as a reseller for high demand products but not for low demand products. This separating equilibrium is more likely to occur when: (i) the consumers’ willingness to pay for the high-value good is sufficiently larger than their willingness to pay for a low demand product (which opens the door to potentially large information rents); (ii) the negative spillover between the two vending sites (due to some degree of substitution between them) is not too large.====In the literature, it is well established that the marketplace mode allows for the internalization of the spillovers between two vending sites since this business mode transfers to the seller the sales’ choice in both channels (direct sales and platform sales), thereby eliminating any double marginalization issues (Tirole, 1988). For this very reason, under complete information, we find that, as expected, the marketplace business model generates, at the same time, greater aggregate profits and greater platform’s profits (in comparison with the reselling business model).====When the platform has private information on final demand, the business model choice is no longer exclusively determined by the need to eliminate double marginalization issues. Indeed, the platform shall also consider the possibility of obtaining informational rents (everything else the same). More precisely, the platform faces a ==== between: (i) letting the informed party (the platform) adjust its own sales level to the state of demand (by proposing a reselling contract), at the cost of not internalizing the externalities between direct sales and platform sales, or (ii) letting the uninformed party (the seller) choose the level of sales in the two sites (by offering a marketplace contract) at the cost of not adapting them to the true state of demand.====We find that the platform will never be interested in reselling a low demand product. In this case, there is not much room for informational rent and therefore it prefers to offer a marketplace contract. Now, what about a high-value good? In the context of our model, the platform may prefer to offer a reselling contract, depending on two parameters: (i) the degree of substitutability between the direct vending site and the platform’s vending site and (ii) the ratio between the consumers’ willingness to pay for a high-value good and their willingness to pay for a low-value good. When the former is not too important and the latter is large enough, the only Perfect Bayesian Equilibrium satisfying the Cho and Kreps (1987) Intuitive Criterion is a separating equilibrium in which the platform always resells a high quality good. The rational behind this result is the following: the strategic cost of reselling under these circumstances is small (as the spillover between the two vending sites is limited) while the informational benefit is important. When the sites are strong substitutes and/or there is a small gap in the consumers’ willingness to pay for a high-value versus a low-value good, the only Perfect Bayesian Equilibrium is a pooling equilibrium where the platform always acts as marketplace for both types of goods (high-value and low-value goods). Finally, for intermediate parameter values, there exist three Perfect Bayesian Equilibrium: a separating one, a pooling one and an hybrid equilibrium where the platform resells the high value good with a certain probability, which is strictly positive and lower than 1.",Marketplace or reselling? A signalling model,https://www.sciencedirect.com/science/article/pii/S0167624518300982,21 November 2019,2019,Research Article,85.0
"George Lisa M.,Peukert Christian","Department of Economics, Hunter College and the Graduate Center, CUNY, 695 Park Ave., New York, NY 10065, USA,Católica-Lisbon School of Business and Economics, Universidade Católica Portuguesa, Palma de Cima, Lisbon, Portugal,ETH Zürich, Center for Law and Economics, Haldeneggsteig 4, 8092 Zürich, Switzerland","Received 24 October 2017, Revised 13 September 2019, Accepted 11 October 2019, Available online 1 November 2019, Version of Record 27 November 2019.",https://doi.org/10.1016/j.infoecopol.2019.100833,Cited by (1),"Economic research has documented a robust, positive relationship between media consumption among minority individuals and the size of the minority population in local markets. The theoretical mechanism behind these “preference externaltities” has been understood to be the supply incentive to cater to large groups when fixed costs or other ","Economic research documents a robust relationship between the demographic composition of media markets and minority media consumption. Metropolitan areas with a larger African-American population, for example, see higher per capita newspaper readership among black individuals than markets with a smaller black population, all else equal. The relationship has been documented across racial and ethnic minorities in the context of news, music and television.====The relationship between the size of a minority population and media consumption has been understood to arise from supplier incentives to target large groups. These incentives arise when fixed production costs or other scale economics limit the number of differentiated products a market can support. Empirical evidence supports the supply-side mechanism: markets with a larger African-American population offer more black-targeted radio and newspaper content, while markets with more Hispanics provide more Spanish language television news programs.====Yet despite results linking group size to the supply of targeted media, other evidence suggests that supply-side explanations for what have come to be known as “preference externalities” remain incomplete. In news markets, for example, a gap in consumption between minority and majority readers has persisted into the digital era despite massive reductions in fixed costs and proliferation of targeted outlets. The well known Pew Research Center surveys show that the gap in newspaper readership between African-American and white readers has remained stable at about 5–6% between 1999 and 2016.==== At the same time, the economics literature has amassed evidence that local social networks affect a wide range of individual decisions in education, trade and public choice.==== Since communication lies at the heart of news markets, it seems reasonably that larger peer populations might increase the tendency to seek, and to share, information. Social networks might thus deepen, or offset, the relationship between group size and media consumption that arise from targeted entry and help explain the persistence of a “digital divide” in news consumption.====This paper examines the relationship between community composition, online social networks and the demand for news. We have two goals. The first is to demonstrate that patterns of minority news consumption cannot be explained solely by supplier incentives to target large groups. We do this by showing that per capita consumption of national news online among black households increases with the local black population, contradicting theoretical predictions of the strict supply-driven model. The second goal is to provide evidence that social networks can explain consumption patterns observed in the data but unexplained by the standard model. We do this by linking local community composition with social network size and utilization. Taken together, our results indicate a broader scope for individuals with shared preferences to influence each other through media markets than previously recognized.====Uncovering the mechanism behind consumption inequality is important for understanding the effects of digitization. Supply-side models of preference externalities are grounded in a spatial framework where fixed production costs limit entry, so they naturally apply in local settings with few products. But if social networks influence the demand for information, then minority groups can see lower consumption of ==== targeted media, not solely local outlets. In other words, the important determinant of consumption inequality would not be the presence of ====, but rather the presence of ====. Moreover, in digital markets, firms increasingly steer resources to content most readily shared, forwarded and “liked.” If larger groups have a greater tendency to share information, then even when fixed costs are very low suppliers will face a differential incentive to serve the majority at the expense of minority preferences. Simply put, if the tendency to share information depends on local social networks, groups with a higher tendency to share will find themselves better served and better informed.====Distinguishing the role of supply-side and demand-side factors in consumption inequality is also important for policy. Diversity and localism are founding principles motivating policies of the Federal Communications Commission. Supply-side factors in consumption inequality imply under-provision of targeted media, ameliorated with the minority ownership rules or production supports. Demand-side mechanisms suggest social foundations for under-consumption, best tackled with policies to increase broadband access, digital literacy, and community cohesion. Policies aimed at strengthening local social networks are now advocated by economists for improving a broad range of social outcomes (Jackson, 2011).====Our empirical approach for the two goals is as follows. We first establish that the relationship between local minority population and local news consumption documented in the literature holds in online news markets. Closely following the empirical strategy in the literature, we show using a sample of 35,997 internet households that a larger local black population is associated with more online news visits among black households relative to white households. A larger white population is similarly associated with fewer news visits among blacks relative to whites. We then extend the analysis to national news outlets. With the same sample of internet households, we show that a larger local black population is associated with higher online consumption of national news among blacks relative to whites. The standard supply-side mechanism cannot explain these results, predicting instead that national media targeted at the median consumer should not be positively correlated with local minority populations.====After showing that the supply-side explanation of preference externalities is incomplete, we look for evidence of a demand-side mechanism. Following the literature on social networks, we hypothesize that larger communities with shared tastes lead to larger social networks and more information sharing. We study this in the context of social media, asking whether a larger minority population is associated with more intense social media use. We offer two sets of evidence. Using our internet sample, we first show that a larger minority population in a locality is associated with more frequent visits to the social media outlets Facebook and Twitter. The result suggests that local populations affect social network use overall, but does not allow us to measure social network size or distinguish information sharing from more passive consumption. To study this we assemble a unique sample of 11,479 Twitter users coded for race. We find that a larger local black population is associated with a larger number of sources followed and a higher number of messages posted. A larger local white population is similarly associated with a larger number of sources followed and higher network use. Cross effects, the effect of a larger white population on a black users and vice versa, tend to reduce network size and participation, especially for minority users. We conclude that a larger local population with shared preferences increases the tendency to seek, and to share, information on online social networks.====Our identification strategy for the news analysis closely follows the industrial organization literature on preference externalities in using independent variation in the minority and majority populations at the MSA level to capture shared tastes for media products (Waldfogel, 2003, George, Waldfogel, 2003). Unlike some earlier studies of preference externalities, we measure behavior with household data and thus can control for observable household characteristics. In studying social media, we follow the literature on peer effects and identify results from variation in community composition at a more localized geography (zip code or census place) (Bertrand, Luttmer, Mullainathan, 2000, Aizer, Currie, 2004). Our approach effectively compares social network size and activity among minority individuals in places with a larger minority population to social network size and activity among minority individuals in places with a smaller minority population.====In addition to our direct contribution to the literature on preference externalities in media markets, our results also relates to a now-substantial literature on social network communications. Perhaps most closely related to our study are List and Price (2009) and Zhang and Zhu (2011), which consider the relationship between group size and the incentives to contribute to public goods. Halberstam and Knight (2016) investigate political communications in social networks, linking group size to majority and minority political interests. Our results are also relevant to the growing literature on the relationship between online social media and news readership (e.g. Sismeiro and Mahmood, 2018), online social media and political participation (e.g. Petrova et al., 2017), and the effects of user-generated content in news media (e.g. Yildirim et al., 2013). Our focus on the link between the demographic composition of local communities and online behavior also contributes to a now substantial literature on geography and the internet, initiated by Sinai and Waldfogel (2004) and continued with, for example, Agrawal et al. (2015). Our work also contributes to research in marketing that studies the extent to which social activity acts as a complement or substitute to other activities, for example as in Seiler et al. (2017) and Zhang et al. (2017).====We emphasize at this stage that our work relies on broad classifications of race rooted in the census definitions of African-American/black and non-Hispanic white populations. Like other studies, our aim is ==== to study race ====. We study consumption differences across race because of well-documented evidence that media preferences differ by race in measurable ways (e.g., George, Waldfogel, 2003, Waldfogel, 2003) and because of extensive evidence of that race plays a substantial role in peer groups and local social networks (e.g., Bertrand, Luttmer, Mullainathan, 2000, Aizer, Currie, 2004). While individuals with different racial background may prefer different news topics due to a complex mix of socioeconomic, ethnic and cultural factors, race captures this mix of factors that together constitute a distinct set of preferences of interest in research, policy and practice. Population by race thus offers a clear and practical measure of shared tastes in a market that links our results to policy and the literature. For economy of language we refer throughout the paper to black and white populations and individuals, recognizing that these are simplified, shorthand references to complex, culturally-based classifications.====The paper proceeds as follows. Sections 2 and 3 describe our working data and empirical approach. Section 4 reports results on the demand for online news, demonstrating that observed patterns of preference externalities cannot be explained solely by supply incentives. Section 5 shows that community composition affects both the size and utilization of users’ social networks. Section 6 concludes the paper.",Social networks and the demand for news,https://www.sciencedirect.com/science/article/pii/S016762451730183X,1 November 2019,2019,Research Article,90.0
Foster Joshua,"Department of Economics, University of Wisconsin - Oshkosh, Sage Hall 800 Algoma Blvd. Oshkosh, WI 54901, United States","Received 15 January 2019, Revised 18 September 2019, Accepted 1 October 2019, Available online 2 October 2019, Version of Record 27 November 2019.",https://doi.org/10.1016/j.infoecopol.2019.100832,Cited by (12),"This paper uses daily panel data to study the effects that entrepreneurs’ social networks have on the success of their projects seeking capital from a potentially large group of individual investors (i.e. crowdfunding). Much of the literature to date demonstrates both theoretically and empirically that the benefit of large social networks accrues at the beginning of the crowdfunding campaign and are commonly the initial contributions that the project receives. We find this is consistent with unsuccessful campaigns, however, among successful campaigns many of the benefits of large online social networks occur only after the project has met its funding goal. In particular, we find that entrepreneurs with relatively large online social networks receive a statistically significantly larger number of backers only after the project is successfully funded. It is hypothesized this result is due to the composition of strong and weak ties in the entrepreneur’s social network. Importantly, when a project reaches its funding goal a positive signal of its quality is sent to those in the entrepreneur’s social network and motivates the relatively large group of weak ties in it to contribute. As a result, it puts into question the value that strong ties can have in aiding entrepreneurs in reaching their funding goal.","In a matter of years, entrepreneurs have seen dramatic growth in their ability to raise capital with crowdfunding campaigns. This alternative method of financing new business ventures relies on collecting pledges from a dispersed group of individuals for the purposes of funding a proposed project. The most widespread form of crowdfunding, and the focus of this paper, involves reward-based campaigns, in which an entrepreneur seeks pre-purchases from their customers for a product or service. There are several reasons why entrepreneurs would prefer to raise capital through crowdfunding. First, it allows them to finance new ventures while retaining equity and avoid taking on debt (Belleflamme et al., 2014). Second, it helps businesses be first to market by locking customers in before production can be completed. Third, it provides a unique way of engaging their customer base by creating a conversation around the product or service, which can lead to feedback on design and features that would otherwise never have taken place with more traditional forms of financing (Schwienbacher and Larralde, 2010). Fourth, it can help minimize the negative impact that implicit biases have against under-represented entrepreneurs (Marom, Robb, Sade, Gorbatai, Nelson, 2015). And fifth, it allows entrepreneurs to leverage their social networks in a way that can costlessly promote their new project and find backers for their campaign.====In this paper, we focus on the social component of crowdfunding to better understand the dynamics of backer growth through time. Much of the literature to date has theorized that the benefits of large social networks accrue at the beginning of the crowdfunding campaign by seeding the project with an initial investment (Stuart, Sorenson, 2007, Zheng, Li, Wu, Xu, 2014, Agrawal, Catalini, Goldfarb, 2015). However, while these early investments serve to benefit the project, successful campaigns will often require contributions from those who have weak or no ties with the entrepreneur. Furthermore, since these prospective backers cannot always ascertain their value for the project or the quality of the entrepreneur, an asymmetric information problem exists, which has the potential of limiting the amount of funding the project receives (Belleflamme et al., 2015). One obvious method that all campaigns can use to overcome this market failure is to share information on having recently reached their funding goal as a positive signal of quality. Thus, this paper considers the possibility that entrepreneurs will find greater benefits from their social networks only after they reach their funding goal and thus can advertise this positive signal. Specifically, it is hypothesized that entrepreneurs with larger social networks will attract more backers once their project is funded.====The daily panel data and corresponding econometric analyses presented in this paper strongly support this hypothesized dynamic among project backers. Namely, while it is found that the size of the entrepreneur’s social network is positively related to the likelihood they run a successful campaign, it is only after the project reaches its funding goal that larger networks containing many weak ties begin to help in attracting new backers. Thus, entrepreneurs will often find the most benefit from their social networks once they can send a positive signal (i.e. reaching their funding goal) to those prospective backers within it, which then can beget additional backers through others’ social networks, and so on. Along the lines of the existing literature, the data finds that unsuccessful campaigns attract additional backers through an increase in the number of strong ties in the entrepreneur’s social network (e.g. close friends and family). However, increasing the number of weak ties in the unsuccessful entrepreneur’s social network has no effect, which we hypothesize is due to their sensitivity to the asymmetric information on project quality that is never adequately resolved.",Thank you for being a friend: The roles of strong and weak social network ties in attracting backers to crowdfunded campaigns,https://www.sciencedirect.com/science/article/pii/S0167624519300083,2 October 2019,2019,Research Article,91.0
Vélez-Velásquez Juan Sebastián,"Banco de la República de Colombia, Cali, South America","Received 29 May 2018, Revised 20 September 2019, Accepted 22 September 2019, Available online 24 September 2019, Version of Record 27 November 2019.",https://doi.org/10.1016/j.infoecopol.2019.100831,Cited by (0)," of firms producing complementary products entail two opposing effects: Lower prices, because they may internalize the impact of the complementarity, and higher prices, because the firms gain the ability to price discriminate. I use Colombian data to analyze mergers between firms providing complementary ====. I estimate a discrete choice model of demand for bundled fixed-line and mobile internet services, in which the degree of either substitutability or complementarity among products is a parameter of interest. Counterfactual experiments using the estimated model indicate pro-competitive effects of mergers with complements: despite a small increase in the price of standalone goods, ==== increases.","In recent years, there has been a wave of mergers of firms producing differentiated and potentially complementary products. Take, for instance, the merger between Apple and Beats.==== Some consumers enjoy having a smartphone and a pair of headphones simultaneously, but others are content having just either one or the other. For consumers who enjoy having both, the utility of consuming them together is higher than is the sum of their individual components. The products sold by Apple and Beats are textbook examples of products that consumers perceive as complements and, as such, the merger between these firms does not quite fit the traditional framework used to analyze horizontal or vertical mergers. However, the real world is seldom as simple as textbook examples, and regulators often have to scrutinize proposed mergers between firms that produce many goods, some of which are substitutes and some of which are complements. Mergers like this, in particular among the market leaders in their respective fields, often stir a policy debate fueled by concerns about anti-competitive effects, but public opinion tends to ignore the aspects that could potentially attenuate such concerns. Consider Colombia’s telecommunications industries as an example. The 2012 Claro merger involved Comcel, the largest mobile carrier, and Telmex, the largest Internet Service Provider (ISP), in the country. Consumers were understandably concerned due to the massive market power of the merging parties. This article emphasizes the importance of understanding the pre-merger rivalry of the goods involved in the merger, in particular when there might be complementarities between the goods provided by the merging parties.====The antitrust literature (Masson et al., 2014) has highlighted the difficulty of predicting the effects of mergers involving complementary goods and, in particular, Choi (2008) describes their ambivalent nature in detail. On the one hand, the merger may cause a decrease in prices. The price reduction occurs because the demands of firms producing complementary goods are decreasing in each other’s prices, so a reduction in the price of either firm increases the revenue of the other. When acting alone, a firm reducing prices does not reap the extra profits it generates; thus, firms have no incentives to reduce prices. A merger internalizes those extra profits, making it optimal to lower the price. On the other hand, the prices could also rise because the merger brings the possibility of bundling, with which the merging firm gains the ability to exert price discrimination. The merged firm might find it optimal to decrease the price of bundles and increase the price of standalone goods, and, by doing so, it would harm consumers who prefer to build their own bundle (mix-and-match). If enough people prefer to mix-and-match, price discrimination may reduce consumer welfare relative to its pre-merger level.====This article explores the dual effects of mergers with complementary goods using panel data from Colombia on market shares, prices, and product characteristics for the universe of internet plans sold in the country’s three largest metropolitan areas (MAs) between 2013 and 2015. I begin by estimating the demand for internet, allowing consumers to choose whether to buy standalone fixed- line internet (accessed by either DSL, ADSL, fiber, or coaxial), standalone mobile internet (using a mobile broadband dongle), or a bundle of both, in a discrete choice framework that permits complementarities between the two types of connections. Then, I use the estimated demand model and an assumption of Bertrand competition to evaluate whether mergers between wired and wireless providers would be either pro- or anti-competitive.====There are two reasons why the telecommunications sector in Colombia is an interesting enough setting for studying these mergers. First, several momentous events rocked the sector during its recent past, including a few mergers of companies delivering potentially complementary services. For instance, early in 2012, Telefónica (mobile) consolidated its control over Coltel (mobile and fixed-line), and a few months later, Telmex (fixed-line) and Comcel (mobile) merged under the Claro banner. In 2014, Tigo (mobile) and Une-EPM (fixed-line) merged, and DirecTV started providing mobile Internet. The second reason is the availability of detailed data that regulators were resolute in collecting and making available for a brief period.====I use a comprehensive administrative dataset put together by Colombia’s telecommunications regulator (Comisión de Regulación de Comunicaciones, henceforth CRC).==== The dataset contains information about the universe of firms and plans offered, because all providers of telecommunications’ services in Colombia must file CRC’s Form 5 quarterly. In Form 5, providers report bundled and standalone services sold in every market, the number of households that subscribe to each service, and the service’s most relevant characteristics. The key feature of the data, besides the high detail of information about characteristics of the products, is the ability to observe market shares for standalone and bundled services, as these kinds of data are necessary to determine whether the services are complements or substitutes. As usual, the dataset is not perfect. Because the providers themselves report the information, it is impossible to know how many households buy services from several providers, effectively creating their own bundles. This is a common drawback of the kind of data used for demand estimation. In this case, this does not seem to be particularly worrisome, because, as I show below, there is strong evidence that Colombian households seldom buy internet plans from two providers. However, I discuss this issue further in the identification section, because estimating complementarity is a main goal of the paper.====Studying the effects of mergers with complements empirically is important for two separate reasons. First, there is an important economic theory question of what occurs with these mergers. It is well understood that mergers between complements can solve double-marginalization problems and, hence, are pro-competitive. When the levels of complementarities are large enough, they can be pro-competitive, even in the absence of cost synergies. But, do products in different sectors have enough complementarities to overcome the upward pricing pressure effects from bundling? Economic theory alone cannot resolve this question, and, hence, empirical research can add valuable evidence. Second, around the world, there have been some proposed mergers between providers of complementary services in telecommunications and other sectors. For instance, in the U.S., AT&T and Comcast recently announced their intentions to merge. Exactly as in the Claro case in Colombia, a merger that I study here, AT&T focuses on wireless services, whereas Comcast focuses on wired services. To evaluate the price impact of any of these mergers, one would need to understand the level of complementarity between these providers and how it translates into changes in welfare following the merger.====The framework I use to obtain the substitution patterns builds on Gentzkow (2007). I estimate a discrete-choice model for internet demand, in which the utility is written to allow consumption of standalone and bundled services. I adjust the indirect utility function of bundles to incorporate the additional utility of joint consumption by including a parameter that contains information about the substitution patterns between mobile and fixed-line internet. The sign of the estimate conveys information about the substitutability or complementarity of the goods: an estimated negative (positive) sign means that the goods in the bundle are substitutes (complements).====The main challenge in implementing this framework is to avoid confusing complementarity either with correlated tastes or with the fact that the goods are, simply, bad substitutes. Observing that subscribers to fixed-line internet often subscribe to mobile can be evidence that the two are complements. But it could also mean that the preferences for both are correlated, like when a household of YouTube enthusiasts subscribes to both as a means to watch videos either at home or while taking the tube to commute. Similarly, one observes joint purchases of goods that are bad substitutes, for instance, when households purchase salt and detergent jointly because they are bad substitutes and not because they are complements. To recover true complementarities, one must have market shares for both bundled and standalone goods as well as variation in the characteristics of the goods. Identification is further enhanced when characteristics that shift the utility of one good do not enter the utility of the other. All these desirable traits are present in the data collected by the CRC.====I find that Colombian households tend to derive extra utility from the joint consumption of wired and mobile internet. Every month, households that subscribe to both get, on average, $3.3 worth of additional utility. Knowing that wireless and wired internet are complementary services allows me to simulate several scenarios of interest. I start by studying alternative ways in which the Claro merger could have unfolded, simulating one scenario in which the regulators block the merger altogether and another in which they allow the merger but ban bundling. In these simulated equilibria, I re-optimize the prices of all firms in a market, assuming that the merging firms decide prices independently when the merger is blocked, and to maximize joint profits when they merge. Besides, I simulate a merger between ETB and Avantel, providers of wired and wireless services, respectively. The interest in this simulation is twofold, as it contributes to understanding the effects of mergers with complements and to an ongoing debate around the sale of the state-owned ETB.====In the simulations, I assume that the firms decide prices conditional on their rivals’ characteristics. I use the First Order Conditions (FOC) of the game to simulate the alternative scenarios and solve for the new equilibrium prices. The results of simulating a merger with complements in Colombia’s telecommunications context confirms what Berry and Haile (2014) expected for the AT&T and DirectTV merger in the US: The prices of standalone products increase, while the bundles become cheaper. Our calculations suggest that overall consumer welfare increased as a result of the Claro merger.====The findings presented here have important implications for regulatory agencies, as these agencies often prescribe divestiture and proscribe bundling to counter the potential adverse effects of mergers. Given the potential for pro-competitive effects, acquiring firms producing complements could be an alternative remedy to divestiture, or, if the merger under scrutiny involves firms producing complements and consumers have strong preferences for bundles, the regulatory agencies can be more lenient toward bundling. For the particular case of selling ETB, those deciding whether to sell it also need also to decide to whom to sell it. The relation between ETB’s products and those of its eventual buyer matters. The insights gained here imply that it is more desirable to sell the company to a mobile carrier because consumer surplus increases and because, potentially, a mobile carrier would pay a higher price for it. The structure of the rest of the article is as follows. The next section describes how this article relates to the existing literature. Then, I introduce the data and some relevant features of Colombia’s telecommunications sector. Next, I present the empirical model of demand used to estimate substitution patterns. Finally, I present and discuss the results of estimating the model and simulating the counterfactuals.",Merger effects with product complementarity: Evidence from Colombia’s telecommunications,https://www.sciencedirect.com/science/article/pii/S0167624518301033,24 September 2019,2019,Research Article,92.0
"Igna Ioana A.,Rincon-Aznar Ana,Venturini Francesco","National Institute for Public Policies Analysis (INAPP), Rome, Italy,National Institute of Economic and Social Research (NIESR), London, UK,University of Perugia, Perugia, Italy","Received 3 November 2018, Revised 12 July 2019, Accepted 19 July 2019, Available online 19 July 2019, Version of Record 27 November 2019.",https://doi.org/10.1016/j.infoecopol.2019.07.002,Cited by (1), – mainly reducing efficiency levels in network industries – and ,"The Great Recession has renewed the interest of academics and policymakers on what is the best institutional design to restore the growth process and increase competitiveness. Along with the financial market (OECD, 2010), one key area of policy intervention in OECD countries has been the reform of the product and factor markets (Arnold et al., 2011b). Changing the set of rules that governs firms’ behaviour, and their incentives to compete, is a long-lasting process that involves various levels of political and economic action. These policies are likely to produce relevant economic outcomes only over the long-term horizon (Conway and Nicoletti, 2006). Since the late 1990s, active market reforming has involved service industries as they are characterized by high regulation/concentration, are not exposed (or to a minor extent) to international competition, and account for an increasing share of national income. Service deregulation is thus expected to produce significant aggregate benefits, in the light of intense inter-industry linkages between these sectors and the rest of economy (Conway et al., 2006). The productivity effects of market liberalisation are amplified by the larger economies of scale induced by the increasing integration of global markets (Nicodeme and Sauner-Leroy, 2007). From a policy-making perspective, product market reforms have been central to the European Union’s single market program, as greater competition raises firms’ incentives to innovate and improve productivity levels (Griffith et al., 2010). Service deregulation is found to yield important efficiency gains also in those countries, such as the EU New Member States, that have opened their economies to the free market only more recently (Hagemejer et al., 2014).====An increasing number of research works have investigated how service regulation, hereinafter defined as upstream regulation, transmits to downstream firms that use these inputs in their production. Earlier analyses have focused on productivity growth (Barone, Cingano, 2011, Bourlés, Cette, Lopez, Mairesse, Nicoletti, 2013), investment patterns (Alesina, Ardagna, Nicoletti, Schiantarelli, 2005, Cette, Lopez, Mairesse, 2017), labour market outcomes (Fiori et al., 2012) and innovation performance (Franco, Pieri, Venturini, 2016, Amable, Demmou, Ledezma, 2010, Amable, Ledezma, Robin, 2016). Attention has also been paid to the distributive effects of product market regulation (PMR), i.e. how entry barriers or public ownership in services, can influence the dynamics of the labour share on income (Azmat, Manning, Reenen, 2012, O’Mahony, Vecchi, Venturini, 2019) Service market liberalisation is found to improve different dimensions of economic activity. However, the existing research works have focused on the effects of service regulation on different dimensions of economic activity taken separately and have not paid much attention to the mechanisms through which upstream PMR transmits to productivity performance.====Not less relevant is the fact that these studies have overlooked cross-industry differences in the impact of upstream (service) regulation. The effect of upstream PMR may not only reflect the intensive use of the regulated factors, but may also depend on more fundamental technology conditions (see Eberhardt and Teal, 2018 for a discussion on heterogeneous technologies). In other words, firms may react to the strictness of upstream regulation differently, depending on the nature of their production, the type of factors used and the possibility to substitute regulated (service) inputs with internal factors, i.e. performing intangible tasks on their own. Failing to account for industry heterogeneity in the impact of regulation and the effect of un-observable shocks (globalisation, financial factors, etc.) may lead to misleading inference (Chudik and Pesaran, 2015).====The goal of this paper is to study how service regulation shapes long-run productivity trends at the industry level. To this aim, we develop a simple theoretical set-up where upstream regulation has a twofold impact on production activities. First, PMR reduces productivity ==== by depressing efficiency levels (the productivity channel). Second, PMR generates frictions in intermediate input markets and, by changing the demand for investment or labour inputs via a relative prices’ mechanism, it affects productivity ==== (the factor demand channel). Based on international industry data from 1975 to 2007, we assess the predictions of the model using up-to-date panel time-series regression techniques, that allow for (parameter) heterogeneity in the effect of regulation and control for the impact of un-observable common shocks.====We find that upstream regulation reverberates on industrial production through both conduits, i.e. the productivity channel and the factor demand channel. Service regulation has detrimental effects on productivity primarily in the network industries, whilst lowers labour demand mainly in the manufacturing sector; the negative impact on traditional investment goods is statistically weaker. As a novel evidence, the paper shows that downstream firms respond to high anti-competition barriers (and costs) in intermediate inputs (services) markets by investing more intensively in ICT capital goods. This effect shows up mainly in the manufacturing industries where firms are more exposed to international competition and hence are forced to adopt stricter cost-saving strategies. We quantify that, at the level of the total economy, a 10% decrease in service regulation would yield a permanent increase in industrial output by 1.8% via the productivity channel, and by 4.1% and 1.3% via the demand for labour and traditional capital. However, according to our estimates, the indirect productivity cost associated with the deregulation wave, channelled by a lower ICT investment demand, would be of 2.8%.====Our work contributes to the literature by showing that accounting for technology conditions (industry heterogeneity) is fundamental to understand how firms respond to stringent upstream regulation. One of the most consolidated findings of the literature is that service regulation lowers ICT investment in downstream industries, taken as an aggregate. However, Papaioannou (2017) illustrates that the regulation impact is non-linear (U-shaped), being positive at the left tail of the distribution and negative elsewhere: it implies that industries subject to less stringent service regulation invest more in ICT. Our analysis therefore demonstrates that, when one fully accounts for industry heterogeneity in terms of exposure to upstream regulation and impact, the positive effect of upstream PMR on ICT investment may dominate ====.====The plan of the paper is the following. Section 2 provides a brief literature review. Section 3 draws the theoretical framework and derives the empirical specifications. Section 4 describes data and presents summary statistics. Section 5 shows the econometric results. Section 6 concludes.","Upstream regulation, factor demand and productivity: Cross-industry differences in OECD countries, 1975–2007",https://www.sciencedirect.com/science/article/pii/S0167624518302385,19 July 2019,2019,Research Article,93.0
"Garcia-Swartz Daniel D.,Campbell-Kelly Martin","Charles River Associates, One South Wacker, Suite 3400, Chicago, IL, 60606, United States,Department of Computer Science, Warwick University, CV4, 7AL, UK","Received 11 June 2018, Revised 7 July 2019, Accepted 10 July 2019, Available online 11 July 2019, Version of Record 29 August 2019.",https://doi.org/10.1016/j.infoecopol.2019.07.001,Cited by (2),"In this paper we pose a simple question: For firms that own or sponsor a computer or smartphone standard, is it better to make the standard open or closed? In other words, has openness paid as a business strategy? We explore the issue by examining the history of operating systems in computing and mobile phones, and rely on four different notions of openness: open systems, open innovation, open-source software, and open governance. We conclude that the truly successful operating systems have been those whose owner or sponsor has managed to combine some degree of openness with some measure of control.","In 1987, H. Landis Gabel published an often cited paper on X/OPEN, the consortium of (mostly European) computer vendors created to promote the UNIX operating system. He described the essence of his study in the following terms: “The objective of this paper is to answer a deceptively simple question. If open standards in the computer industry are as desirable as the conventional wisdom suggests, why do we not see more of them?” (Gabel, 1987: 91).====Although UNIX was developed in the late 1960s and early 1970s, it did not become truly popular as a commercial product until the mid-1980s. Thus, it could be argued that Gabel conducted his research on UNIX and X/OPEN at a time when UNIX was viewed at most as a partial failure if measured purely in terms of diffusion. It is not surprising, some would argue, that Gabel raised the issue of the limited diffusion of open systems at the very beginning of his study. Today he would surely write a different paper.====But would he? What operating environments have dominated the computer industry in recent years? How open are they? Table 1 gives us the answer. The table presents information on worldwide packaged software revenues for the top 50 vendors by operating environment in 2011. Specifically, it shows the share of each operating environment in total packaged software revenues. In other words, Table 1 provides an answer to the following question: What proportion of the value of all packaged software created by the top 50 world vendors and deployed on various computing platforms in 2011 was accounted for by each operating environment?====Table 1 conveys a couple of fundamental messages. First, almost 57% of the value of all packaged software in 2011 was deployed on a Windows operating environment. Second, an additional 7.5% of the value of software was deployed on a mainframe operating system. Even without making the (quite reasonable) assumption that a large portion of mainframe operating environments were proprietary, the table suggests that proprietary operating systems were the dominant operating environments in terms of the value of software deployed in 2011, with UNIX and LINUX accounting for only about 29%.====So perhaps the question posed by Gabel is as relevant today as it was in 1987: If open systems are so beneficial, why are they not dominant? In this paper we explore this question but from the perspective of the companies that own or sponsor a standard. The question thus becomes: Is openness a good business strategy? Put differently, from the perspective of a company that owns or sponsors a computer or smartphone standard, is it better to make that standard open or closed?====For our exploration of these issues, we select a number of important operating systems in the history of computers and smartphones. Further, we distinguish between, and rely on, several notions of openness (West, 2007, Simcoe, 2008). First, there is the idea of “open systems” or “open standards.” Open systems can be open in the process or in the outcome. A system is open in the process if the process of shaping the system is open to the participation of all interested stakeholders. A system is open in the outcome if the system's key specifications are available to everybody, and especially if the system is licensed to all interested licensees at a reasonable rate.====Second, there is the idea of “open innovation” (Chesbrough, 2005, Chesbrough et al., 2008), which implies that innovation happens not only within the boundaries of the firm that owns or sponsors the standard but also outside those boundaries. In other words, open innovation suggests the presence of an array of companies that cooperate with one another in generating the innovation that drives the growth and diffusion of the standard. Open innovation may happen around a standard that is more or less open, although the need to facilitate open innovation may force the owner of the standard to introduce more openness than it would otherwise.====Third, there is “open-source software.” When we talk about open-source software in 1950–1980 we mean that, during that period, computer customers and independent software vendors had access to the source code and used it for a variety of purposes. Customers, for example, modified the source code according to need. More recently, free software and open-source software (OSS) have somewhat more technical connotations. The key feature of free software and most OSS is “that the source code is available for public access, open to study and modification, and available for redistribution to others with few constraints, except the right to ensure these freedoms” (Scacchi, 2011: 614–615). Free software is usually licensed under the GNU general public license (GPL), whereas OSS may use either the GPL or other licensing models that allow for the integration of software that may not be free software. Free software is always available as OSS, but OSS is not always free software (Scacchi et al., 2006).====And then there is, finally, the idea of “open governance” of open-source projects (Laffan, 2012). The open-governance index (OGI) measures whether the decision-making process within an OSS project is open, accessible, and transparent to all users or, alternatively, concentrated in a specific set of users. More specifically, the OGI comprises 13 governance measures in four areas: (a) access (availability of latest source code, developer support mechanisms, public roadmap, and transparency of decision-making process); (b) development (the ability of developers to influence the content and direction of the project); (c) derivatives (the ability of developers to create and distribute derivatives of the source code); and (d) community (a community structure that does not discriminate across developers).====The literature on open systems, discussed in the next section, has identified two fundamental trade-offs associated with standard openness: first, diffusion versus appropriability, and second, diversity versus control. More openness usually leads to more diffusion but may compromise appropriability of returns. More openness leads to more participation by members of the ecosystem, and usually to more diversity, but it may also lead to loss of control and fragmentation, which in turn may conspire against further diffusion. The main contribution of this study is that we provide a comprehensive examination of the history of computers and mobile phones in light of these trade-offs and, in the process, we provide answers to the question whether openness pays or not. Our key finding is that the most successful operating systems in history are “partially open:” they have managed to combine the right dose of openness with the right measure of control.====The paper is organized as follows. Section 2 examines the main concepts and trade-offs identified in the literature on open systems. Section 3 focuses on IBM and its operating systems between 1950 and 1980. Section 4 covers the UNIX operating system between 1970 and 1990. Section 5 examines Microsoft's attempt to bring UNIX under control on the personal computer in the 1980s, and Section 6 studies a similar attempt by Sun Microsystems on the scientific workstation. Sections 7, 8, and 9 examine openness in smartphone operating systems—Symbian, the iOS, and Android. Section 10 presents our conclusions.",Openness as a business strategy: Historical perspectives on openness in computing and mobile phones,https://www.sciencedirect.com/science/article/pii/S0167624518301331,11 July 2019,2019,Research Article,94.0
"Liu Yixuan,Whinston Andrew B.","McCombs School of Business, University of Texas at Austin, United States,Department of Computer Science, University of Texas at Austin, United States","Available online 14 June 2019, Version of Record 9 July 2019.",https://doi.org/10.1016/j.infoecopol.2019.05.005,Cited by (9),"In this paper, we propose an information-based approach to eliminate inefficiency in traffic systems in the era of autonomous vehicles. We build up theoretical models to coordinate vehicles through Waze, a pervasive crowdsourcing mapping app. We apply the idea of ","Traffic congestion is an inevitable problem for people in urban areas across the world. Economists and transportation researchers have devoted decades to understanding the human behavioral interactions involved and looking for solutions to reduce it. Unfortunately, the usual solution of expanding road system capacity does not always work. As Pigou (1932) and Braess (Braess et al., 2005) have discovered, if every driver chooses the path that looks the most favorable to her, increasing capacity can lead to longer travel time overall. This counterintuitive finding stems from the fact that each driver’s selfish decision in the aggregate can lead to a suboptimal social outcome. Roughgarden and Tardos (2002) and Roughgarden (2005) provide a quantitative measure of the inefficiency resulting from drivers selfish routing behavior. They demonstrate that, generally, inefficiency, also known as “the price of anarchy”, can be extremely severe. Apparently, the inefficiency of anarchy can be eliminated through coordination. Nevertheless, nowadays, there seems to be no viable tool that can achieve it. However, as technology advances, especially in the era of the Internet of Things (IoT), is there anything people can do to handle the inefficiency of the transportation systems?====When we think about the future of transportation, we cannot ignore the development of autonomous vehicles. Tesla, one of the pioneers of self-driving cars, has attracted exuberant investors through its potential to revolutionize the auto industry. Its stock valuation was thus boosted to surpass Ford and become comparable to that of GM. Other automakers, such as Volkswagen, BMW, and Ford, and tech companies such as Apple, Google, and Uber, are also competing fiercely in the autonomous driving market. It seems that autonomous vehicles are the future of transportation. While the various advantages of autonomous vehicles, including increasing safety and the facilitation of mobility services, are frequently discussed, their effect on improving the efficiency of the traffic systems is rarely mentioned.====In this paper, we propose an information-based approach stemming from game theory to eliminate inefficiency in the era of autonomous vehicles. We believe robotics is an ideal area to apply theoretical economic methods. In fact, though numerous theorems have been derived by economists who study game theory, the theoretical results are hardly applied directly to guiding human behaviors because of their complexness and randomness. However, in the era of the IoT, since robots act as programmed, theories are suitable for determining the optimal robotic behaviors. Thus, to guide the routing decisions for autonomous vehicles, we apply the idea of Bayesian persuasion (Kamenica and Gentzkow, 2011) and information design (Bergemann and Morris, 2017) and illustrate it through Waze, a pervasive crowdsourcing mapping app for coordinating driverless vehicles. In general, our approach can be implemented based on any pervasive mapping apps including Google Maps,==== Apple Maps (developed by Apple Inc as the default map system of iOS and macOS), HERE WeGO (which is a maps and navigations applications for Android and iOS by HERE Global B.V.), etc.====Under Waze’s current practice of providing the same traffic information to all vehicles, negative externalities from vehicles’ myopic and selfish decisions can occur, hampering the efficiency of the entire traffic system efficiency. When selecting its route, a vehicle does not usually anticipate other vehicles’ strategies or consider its impact on the other vehicles behind it. For instance, when traveling on a highway during rush hours, vehicles are notified by Waze about traffic slowdowns on their route ahead. Given this information, most vehicles decide to make a detour to a local road that currently has no congestion. Consequently, these rerouted vehicles suffer from an overall longer travel time due to the congestion they create on the local route. This example by Pigou (1932) is one of the earliest displays of the suboptimality of selfish routing. Given this issue, the information-based approach we propose provides direction that varies with vehicles. Receiving different traffic data, vehicles find distinctive optimal routes from their perspectives. In this way, they are coordinated through customized information conveyed by Waze.====Since Waze is the basis for the information source, it is important that the information source be reliable. However, a vehicle currently searches for the best route based only on the aggregated traffic information provided by Waze at the time of departure, which may end up with a suboptimal route. Since roads are connected and vehicles can switch from one road to another (especially in urban areas), the traffic situation along a route varies continuously. A route that seems optimal based on Waze’s data when a vehicle departs may no longer be optimal after the vehicle has been traveling for a while. A dramatic traffic volume change could occur on some section of the route ahead. For example, when a vehicle departs, it may choose to go on the highway which has less congestion than the local road, as shown by Waze. At the same time, an accident happens nearby. Although the accident seems irrelevant to this vehicle, a number of vehicles deviate from their original routes and merge onto the highway to avoid the accident area. The vehicle therefore experiences a longer travel time than expected. It could have arrived at the destination earlier if Waze had anticipated the traffic volume increase on the highway and had indicated to the vehicle that the local road would be a better choice. In other words, under the current practice, Waze’s traffic data become “outdated” and could be misleading if Waze does not prepare for possible future traffic situation changes.====Given the two aforementioned issues, we are motivated to obtain better routing instructions and make Waze a more efficient data source for autonomous vehicles. The primary cause of the issues is that the current Waze does not provide onboard computing systems with ==== and ==== traffic information to steer vehicles in an optimal fashion. Accordingly, we propose an innovative Waze to improve on the current one in two ways. Waze has very rich individual-level traffic data, which should be exploited for a broader usage. In fact, Waze knows the starting point, destination, and planned route of each vehicle. Based on this, we first suggest that, using the queueing theory, the innovative Waze can calculate the moment that any vehicle arrives at a specific section of a route and can thus estimate and predict the future traffic flow. Hence, Waze will be more advanced, since it has more accurate and predictive traffic information. In addition, Waze will be improved to provide information that varies with each vehicle. Receiving different traffic data, vehicles can be provided with distinctive optimal routes from their own perspectives. They are thus coordinated through customized information conveyed by Waze. Therefore, externality problems such as the Braess’ paradox can be eliminated, and the “price of anarchy” is reduced.====The innovative Waze we propose works according to the following procedure. Firstly, just as the current version, Waze collects and clusters inclusive crowdsourced real-time traffic data. The data are then incorporated into the queueing model of the congestion to precisely predicts the traffic. Then, upon several vehicles’ setting their destinations and departing, besides current traffic situation, additional personalized information is computed for each vehicle. The information is conveyed to each vehicle by implementing augmented reality, a way to supplement/manipulate autonomous vehicles’ onboard computers’ vision of the current (or even future) traffic conditions. Since the routing problem is decentralized to be solved by each vehicle, the onboard computer with the vision that uses Waze’s information makes decisions in the best interest of that vehicle. After these vehicles have departed and are on the route selected by their own onboard computers, Waze updates its data about these vehicles, as well as the entire traffic system, and then works on newly departing vehicles.==== The most important step is therefore to solve for the optimal information provided to each vehicle. Thus, in this study, we build up theoretical models and look into the dynamic information design problem for the innovative Waze to minimize the total waiting time of vehicles. To improve the efficiency of the traffic system, the optimal information structure is designed to enable vehicles to foresee the changing traffic conditions and take into account other vehicles on the road. Before discussing our models in detail, we present a traffic game based on the Wheatstone network that induces the Braess’ Paradox and illustrate the potential benefits of the innovative Waze proposed in this paper. The original traffic network is shown in the left panel of Fig. 1. A unit measure of vehicles simultaneously choose one of two paths connecting node ==== and node ====, namely ==== → ==== → ==== and ==== → ==== → ====. Each edge is labeled with the cost function ====( · ) denoting the cost (e.g., travel time) of traveling along the edge as a function of the amount of traffic. Obviously, at equilibrium, half of the vehicles chooses the upper path and the other half chooses the lower path. Thus, the total cost of all vehicles is 3/2. Note that the equilibrium coincides with the social optimal outcome. Now, the transportation department wants to expand the road capacity by adding an express lane (which has no latency) from ==== to ==== shown in the right panel of Fig. 1. Consider the decisions of the unit measure of vehicles. At equilibrium, all traffic moves on the path ==== → ==== → ==== → ==== and the total cost is 2. It illustrates the idea of the Braess’ Paradox that ==== However, if the vehicles are equipped with the innovative Waze, it can give instructions from the beginning, when the vehicles depart from ====, to achieve the social optimal outcome. By providing different information to the two vehicles, Waze makes half of the vehicles travel along ==== → ==== → ==== and the other along ==== → ==== → ====. The Braess’ paradox is thus avoided and the roads are utilized in the best way possible.====To study the real-time traffic information design problem for Waze, we build a discrete-time dynamic programming model. We apply the idea of Bayesian persuasion (Kamenica and Gentzkow, 2011) for the single-vehicle case and apply the unified information design framework of Bergemann and Morris (2017) in a general model. Then, we extend the models to a dynamic setting. The framework can be used to study incomplete information games with a designer who commits to providing information to several players. The information designer’s problem is to identify the information structure and equilibrium such that, when the players maximize their ex ante expected utilities, the designer’s payoff is maximized. In our setting, since autonomous vehicles (players)==== are robots with or without human beings in them, instead of maximizing their utility functions, we assume that vehicles minimize their expected waiting time. The feasible information structures, defined as Bayes correlated equilibria, satisfy the obedience condition, which requires that, if the designer privately communicates information as stochastic action recommendations,==== the vehicles will want to follow the recommendations. Among these information structures, Waze (information designer) chooses the one that minimizes the expected total waiting time of all vehicles during the entire rush hour.====Our discrete-time model is more applicable to routing problem on open roads. To consider a more complex traffic system in urban areas, we propose a continuous-time traffic network model. In a city road network, a vehicle can choose from a number of paths that connect the starting point and the destination. We model vehicles departing as a point process (Daley and Vere-Jones, 2007) in which the time interval between two successive vehicles is a random variable with a known distribution. Besides the challenge of selecting the shortest path in a traffic network, the vehicle faces challenges from the highly random nature of future traffic flow along each possible path. A huge volume of data is clustered for Waze. Waze must be able to clean, analyze, and make predictions and recommendations for each vehicle in real-time. The information design problem for urban traffic appears to remain challenging. For the problem to be fully tackled, processors of extremely high computing power need to be onboard for each autonomous vehicle.====More broadly speaking, we study the economic approach of robotic coordination. We emphasize that, in the era of the IoT, normative economic models can be applied to directing robots’ behavior. Instead of understanding human behavior, economists may want to take the opportunity to guide optimal robotic actions. For example, besides the routing problem studied in this paper, the information design framework can be applied in many different settings, including the parking problems, autonomous freight trains and trucks, and aircraft take-off and landing control, etc. In general, we can introduce utility functions for robots as a high-level specification to regulate and direct their behaviors. We believe it is a proper approach since, firstly, introducing utility functions for robots is helpful to build up a framework for regulations and resolve the concerns about robotic moral and legal issues; secondly, introducing utility functions is necessary to direct robots’ optimal behavior from an economic point of view. The utility function of a robot can be designed based on its owner’s needs and the level of autonomy the owner allows the robot to have. For example, the owner can design the robot’s utility such that it is fully obedient, or the owner can design the utility such that the robot can partially consider the guidance from a human and solves for its own best action.====In the next section, we start with a static model to present preliminary approach and to illustrate the idea of information design. In Section 3, we study the general dynamic framework of either a single vehicle departing at a time or multiple vehicles departing simultaneously. These two models are more applicable to the problem of intercity or long-distance travel. In Section 4, we study the model of travel in urban areas. In Section 5, we discuss the results and conclude the paper.",Efficient real-time routing for autonomous vehicles through Bayes correlated equilibrium: An information design framework,https://www.sciencedirect.com/science/article/pii/S0167624518301161,14 June 2019,2019,Research Article,95.0
"Colombo Emilio,Mercorio Fabio,Mezzanzanica Mario","Università Cattolica del Sacro Cuore and CRISP, Italy,University Milano-Bicocca and CRISP, Italy","Available online 29 May 2019, Version of Record 9 July 2019.",https://doi.org/10.1016/j.infoecopol.2019.05.003,Cited by (53),"This paper develops a set of innovative tools for labor market intelligence by applying machine learning techniques to web vacancies on the Italian labor market. Our approach allows to calculate, for each occupation, the different types of skills required by the market alongside a set of relevant variables such as region, sector, education and level of experience. We construct a taxonomy for skills and map it into the recently developed ESCO classification system. We subsequently develop measures of the relevance of soft and hard skills and we analyze their detailed composition. We apply the dataset constructed to the debate on computerization of work. We show that soft and digital skills are related to the probability of automation of a given occupation and we shed some light on the complementarity/substitutability of hard and soft skills.","It is indisputable that in the past few decades significant forces and factors have dramatically changed the nature and characteristics of the labor market in both advanced and developing countries. Technical progress, globalization and the re-organization of the production process with outsourcing and offshoring have radically altered the demand for certain skills.==== In addition, population aging in advanced economies intensifies the need for continued training, and is likely to affect the structural demand for certain competences, in particular those related to the health and care of the elderly.==== The overall impact of these factors on the labor market is multifaceted. On the one hand several jobs are disappearing while new jobs are emerging; of these some are simply a variant of existing jobs, others are genuinely new jobs that were nonexistent until few years ago. On the other hand the quantity and quality of the demand for skills and qualifications associated to the new labor market has changed substantially. New skills are needed not only to perform new jobs but also the skill requirements of existing jobs have changed considerably. Which occupations will grow in the future and where? What skills will be demanded the most in the next years? Those are the questions that are at the forefront of the policy debate both among economists and policymakers. In order to address these questions specific data need to be collected. This calls for new tools that integrate and complement existing labor market instruments for grasping the complexity and the variability of new labor market trends.====In this paper we develop a set of innovative tools for labor market intelligence by applying machine learning techniques to web vacancies on the Italian labor market. Those tools are at the forefront of research in computer science and are able to address a number of technical and methodological challenges dealing with large volumes of unstructured data, mainly in textual form. In particular they are specifically designed for analyzing firms’ skill needs. In this way we can calculate the skills required by the market, alongside a set of relevant variables such as region, sector, education and level of experience.====Our approach allows to shed light on a number of issues. First we can calculate, for each occupation, the different types of skills required and their frequency. Furthermore we are able to classify those skills into a standard classification system and assess the relevance of digital skills and of soft-hard skills constructing measures of specific “skill degree”. We then apply the dataset constructed to the debate on computerization of work. We show that soft and digital skills are related to the probability of automation of a given occupation and we shed some light on the complementarity/substitutability of hard and soft skills.====The remainder of the paper is structured as follows. Section 2 analyses the advantages and limits of using web vacancies with respect to other more traditional methods. Section 3 describes the tools and the methodology used, Section 4 presents the results. Finally Section 5 concludes.",AI meets labor market: Exploring the link between automation and skills,https://www.sciencedirect.com/science/article/pii/S0167624518301318,29 May 2019,2019,Research Article,96.0
"Agrawal Ajay,Gans Joshua S.,Goldfarb Avi","Rotman School of Management, University of Toronto, 105 St George St, Toronto, ON M5S 3E6, Canada","Available online 29 May 2019, Version of Record 9 July 2019.",https://doi.org/10.1016/j.infoecopol.2019.05.001,Cited by (71),"Based on recent developments in the field of artificial intelligence (AI), we examine what type of human labor will be a substitute versus a complement to emerging technologies. We argue that these recent developments reduce the costs of providing a particular set of tasks – prediction tasks. Prediction about uncertain states of the world is an input into decision-making. We show that prediction allows riskier decisions to be taken and this is its impact on observed productivity although it could also increase the variance of outcomes as well. We consider the role of human judgment in decision-making as prediction technology improves. Judgment is exercised when the objective function for a particular set of decisions cannot be described (i.e., coded). However, we demonstrate that better prediction impacts the returns to different types of judgment in opposite ways. Hence, not all human judgment will be a complement to AI. Finally, we show that humans will delegate some decisions to machines even when the decision would be superior with human input.","Artificial intelligence (AI) has advanced markedly in the past decade. With advances in machine learning – particularly deep learning and reinforcement learning – AI has conquered image recognition, language translation, and games such as Go (Brynjolfsson and McAfee, 2014). This has raised the usual questions with regard to the impact of such new general purpose technologies on human productivity (Cockburn et al., 2018, Brynjolfsson et al., 2018). Will AI substitute or complement humans in the workforce? (Autor, 2015, Markov, 2015, Acemoglu and Restrepo, 2016). In this paper, we build a simple model that takes a careful approach to precisely what new advances in AI have generated in a technological sense and applies this to a microeconomic model of task production. In so doing, we are able to provide some insight on the complements/substitutes question as well as where the dividing line between human and machine performance of cognitive tasks might be. Our approach is, naturally, a first step in exploring the impact of AI but we believe sets the stage for more substantive investigations.====At the core of our approach is noting that recent developments in AI are all advances in prediction – in its statistical sense. Prediction is when you use information you do have to produce information you do not have. For instance, using past weather data to predict the weather tomorrow. Or using past classification of images with labels to predict the labels that apply to an image you are currently looking at. This is all machine learning does. It does not establish causal relationships and it must be used with care in the face of model uncertainty and limited data (Ng, 2016, Agrawal et al., 2018a). But in an economic sense, if we were to model the impact of AI, the starting point would be a dramatic fall in the cost of providing quality predictions.====With this insight, we embed these changes in a model of decision making under uncertainty. As might be expected, having better predictions leads to better and more nuanced decisions – in particular, signal-contingent decisions that vary depending upon predictions received. But we note, however, that better predictions also change the returns to understanding payoffs or rewards will be realized from different actions in distinct states. We term this process of understanding payoffs, ‘judgment’. At the moment, it is uniquely human as no machine can form those payoffs. Depending on their frequency, there will be different returns to judgment for particular states. For instance, if a state is so rare that you never take an action based on it, then there is a diminished return to understanding the payoff in that state. Having this set up that makes a sharp distinction between prediction and judgment allows us to better examine how a reduction in the cost of prediction will change the value and scope of human judgment.====In this paper, our approach is to delve into the weeds of what is happening currently in the field of artificial intelligence (AI) to examine precisely what type of human labor will be a substitute versus a complement to emerging technologies. In Section 2, we show that prediction allows riskier decisions to be taken and this is its impact on observed productivity although it could increase the variance of outcomes as well. In Section 3, we make our stand on the issue of “what computers cannot do” and consider the role of human judgment in decision-making. Judgment is exercised when the objective function for a set of decisions cannot be described (i.e., coded). However, we demonstrate that better prediction impacts the returns to different types of judgment in opposite ways. Hence, not all human judgment will be a complement to AI. Section 4 then considers the design of prediction technology when prediction may be unreliable. Section 5 then examines span of attention issues for human judgment, demonstrating that humans may give the machines real authority even when human input would lead to better decisions. Finally, in the conclusion, we conjecture what will happen when AI learns to predict the judgment of humans.",Exploring the impact of artificial Intelligence: Prediction versus judgment,https://www.sciencedirect.com/science/article/pii/S0167624518301136,29 May 2019,2019,Research Article,98.0
"Gonçalves Rui,Ribeiro Vitor Miguel,Pereira Fernando Lobo,Rocha Ana Paula","Department of Electrical and Computer Engineering, School of Engineering, Porto University (FEUP), Portugal,Department of Informatics Engineering, School of Engineering, Porto University (FEUP), Portugal,Department of Economics, Aveiro University (DEGEIT), Portugal","Received 31 March 2018, Revised 15 May 2019, Accepted 28 May 2019, Available online 29 May 2019, Version of Record 9 July 2019.",https://doi.org/10.1016/j.infoecopol.2019.05.002,Cited by (18),"We present the implementation of a short-term forecasting system of price movements in exchange markets using market depth data and a systematic procedure to enable a fully automated trading system. Three types of Deep Learning (DL) Neural Network (NN) methodologies are trained and tested: Deep NN Classifier (DNNC), Long Short-Term Memory (LSTM) and Convolutional NN (CNN). Although the LSTM is more suitable for multivariate ==== from a theoretical point of view, test results indicate that the CNN has on average the best predictive power in the case study under analysis, which is the UK to Win Horse Racing market during pre-live stage in the world’s most relevant betting exchange. Implications from the generalized use of automated trading systems in betting exchange markets are discussed.","The increasing amount of data reveals that the Big Data era is here to stay and constitutes a new form of strategic behavior and business interaction. Data is currently considered one of the most valuable intangible assets in the world. The domain of data analytical techniques is a key step not only to facilitate the transformation and growth of firms but also to boost the level of digital literacy. Goodfellow et al. (2016) recognize that the use of Deep Learning (DL) constitutes an enabler of disruptive change for businesses due to its power of association, regression, classification and clustering. Machine learning incorporates a vast array of algorithmic implementations, which not all of them can be classified as DL. Indeed, the later only corresponds to a subset of the former field of research.====Historically emerging from cognitive and information theories, DL aims at imitating the learning process of human neurons and creates complex interconnected neuronal structures similar to human synapses. Hence, DL consists of the application of multi-neuron, multi-layer Neural Networks (NN) to perform learning tasks such as regression, classification, clustering or encoding/decoding. The ability for a NN to be used in a wide variety of data and learn indiscriminately implies that the DL approach can be applied to a considerable number of case studies rather than requiring the development of a structure for each new analysis. Varian (2014) recognizes the relevance of DL NN architectures for the economics field. Proficiency with data mining, data visualization tools and artificial intelligence rank as one of the most important skills in determining business success, thus, any effort to educate stakeholders is clearly advised.====This study presents a framework for short-term forecasting of price movements in exchange markets and, therefore, the main core relies on time series forecasting. It is focused on modeling predictors of future values of a time series based on past observations. The relationship between past and future observations in the domain of financial markets is stochastic or non-deterministic, which implies that the conditional probability distribution of a matrix of inputs (X) as a function of past observations is generically given by:====DL NN models have built-in properties that make them suitable for multivariate time series analysis. Firstly, in their basis, they are robust to noise in input data and can support learning and prediction in the presence of missing values (Dixon et al., 2015). Secondly, they do not make strong assumptions about the mapping function and learn either linear or non-linear relationships (Dorffner, 1996). This implies that they add the capability to learn non-linear relationships with arbitrarily defined, though fixed, number of inputs and outputs (Huck, 2009, Huck, 2010). This is extremely important because most real-life events are characterized by complex relationships. Thirdly, they have generalization power due to the recognition of unobserved relationships in data after learning from a set of inputs. Fourthly, they are not excessively rigid on the treatment of input data (e.g., forcing the persistence of a certain distribution). Fifthly, they deal better with heteroskedasticity due to their ability to learn hidden relationships in data without the imposition of additional constraints. Finally, in the case of recurrent neural networks (RNNs), they learn temporal dependence from context. The study of Hochreiter and Schmidhuber (1997) is the seminal contribution responsible for the introduction of Long Short-Term Memory (LSTM), which is a particular type of RNN that has played a key role in recent DL advances due to its learning ability in long run dependencies. From a theoretical point of view, the LSTM learns adequately long-term dependencies between time steps of sequence data and, therefore, it is frequently considered the benchmarking DL NN model for multivariate time series analysis (Fischer and Krauss, 2018).====The DL approach contemplates a meaningful set of application fields (Hatcher and Yu, 2018). Financial exchange markets have only recently been subject to the ability of DL NN models to learn stochastic data. In general, researchers intend to implement predictive mechanisms through DL NN architectures in order to recognize trends and detect anomalous behavior. Ding et al. (2015) analyze stock exchange market price predictions through the implementation of a deep NN to learn event embedding and a Convolutional NN (CNN) for short-, medium- and long-term analysis. Their model improves accuracy and profit relatively to baseline NN methods, especially for firms with a lower number of available news. Heaton et al. (2016) consider a DL auto-encoding technique that is based on the principal component analysis (PCA) for high-dimensionality data reduction, which allows feature extraction. This way, they define a smart index (i.e. a fit approximation of a subset of stocks to a single index). Korczak and Hemes (2017) show that, compared to simple NN multilayer perceptron (MLP), a CNN in the H2O algorithmic trading framework significantly increases the average rate of return per transaction in the FOREX exchange market. Hu et al. (2018) consider a Hybrid Attention Network (HAN) to predict stock exchange market trends based on the report of news. Their natural language processing (NLP) framework internalizes attention to values of temporal news vectors and uses LSTM for sequential modeling prediction. Their training mechanism increases accuracy and outperforms competing methods in simulation. Zhao et al. (2017) ensembles different models extracted with a cross validation process (i.e. division of the training dataset into multiple subsets) to forecast the West Texas Intermediate crude oil spot price using stacked auto-encoders (SAE). Fischer and Krauss (2018) consider LSTM networks for predicting price movements for the constituent stocks of the S&P 500. In their case study, LSTM networks have a better predictive power than memory-free classification networks. They also reveal returns close to zero after 2009 due to the low exposure of the trained model to systemic risks.====The main objective of this study is to implement a short-term price movement forecast system in betting exchange markets (i.e. classify changes in odds). This type of exchange shares common grounds with financial exchange markets, namely in terms of raw data format and framework interaction, as clarified in Section 2. We developed agents that execute trades of bets on the United Kingdom (UK) to Win Horse Racing market of the Betfair betting exchange. The period of actuation is the 10 min time window before the start of a race. During this period, the odds (i.e. prices) of bets are subject to speculation. Our agents try to establish a profit by buying and selling bets at different prices before the beginning of a race. Three types of DL NN architectures are trained, tested and compared against each other to find the one that discloses the best price movement prediction: Deep NN Classifier (DNNC), LSTM and CNN.====Two main results are provided. First, after exposing the implementation procedure of the three distinct DL NN architectures, we conclude that the CNN ensures the highest level of accuracy. This implies that, although the LSTM is more suitable for multivariate time series analysis from a theoretical point of view, the CNN outperforms the LSTM. Therefore, in our case study, the use of models with memory cells (e.g., LSTM) can be negligible since the time series under analysis change context periodically (i.e. each 10 min pre-race event constitutes a different context). Second, validation results show that all DL NN models ensure a positive, though low, profit and loss (PL) at the end of a simulation conducted during 30 days.====The remaining of the article is organized as follows. Section 2 presents the case study. Section 3 provides an overview of the different DL NN architectures considered in this study. Section 4 describes the methodology. Section 5 exposes the results. Conclusions are summarized in Section 6.",Deep learning in exchange markets,https://www.sciencedirect.com/science/article/pii/S0167624518300702,29 May 2019,2019,Research Article,99.0
Mustonen Mikko,Aalto University School of Business and HECER,"Received 26 November 2015, Revised 10 October 2017, Accepted 11 October 2017, Available online 10 April 2019, Version of Record 29 August 2019.",https://doi.org/10.1016/j.infoecopol.2017.10.003,Cited by (1),"An Author may directly publish some of his output for free use to increase consumer awareness and thus his profits from performances. The Publisher receives only a share of the profits of the remaining copyright work. We analyze the Author's actions and their effect on the bargaining between the Author and the Publisher. The Publisher voluntarily gives in the bargaining. She accommodates direct publishing so that its level maximizes his profit or she deters direct publishing. The Publisher's profit may be increasing in the distribution cost, due to the threat of direct publishing. Compared to bargaining, joint decision-making, a “360 deal”, reduces the appearance and volume of direct publishing and yields higher surplus. A high level of welfare arises if parties jointly publish directly all output.","Authors of books and music artists, among others, increasingly publish their work circumventing the intermediating publishers and record labels. In the digital world, for example Smashwords offers authors inexpensive publishing and distribution services for books (Smashwords 2017, Waldfogel and Reimers, 2015). Soundcloud and Jamendo are equivalent services for music (Soundcloud 2017, Jamendo 2017). Within these services authors can offer their works at zero or very low price. This has become popular. About 16 percent of Smashwords offerings, over 70.000 books, are free. And the reason is obvious. The number of downloads for free books is forty times higher than for priced books (Smashwords 2017). Yet at the same time the services of the traditional publishers are still important. They protect the copyright of the work, market, advertise and manage distribution. For the author, a trade-off emerges: zero-priced direct publishing creates awareness with indirect or future income. With standard publishing, there are profits but they are shared between the author and the publisher. As a contractual response, vertical integration in the form of a “360 deal” (Marshall 2013) has emerged. Under it, the author and publisher agree to share all profits. When does the author find it profitable to utilize direct publishing and for what share of his output? How does the option of direct publishing impact the bargaining between the author and the publisher over the division of the profits from the output? What changes under a “360 deal” ?====In our model, an Author has created a fixed amount of artistic copyrightable output. He decides what share of that he publishes directly with zero price. This freely available DP work creates consumer awareness, which in turn yields profits to the Author from eg. performances. The intermediating Publisher has the bargaining power and is aware of the Author's decision above. She determines the profit shares to the Author and herself from the sales of the remaining share of the output as standard copyright (CR) work. Alternatively, the Author and the Publisher make a “360 deal” where they share all profits and determine the share of DP work jointly. Consumers bear a constant distribution cost. The optimal share of DP work is increasing in the value of the awareness and the distribution cost but decreasing in the Author's share of the CR work profits. The Author's option to publish directly affects the bargaining and the Publisher will voluntarily concede in bargaining. With low distribution costs, the Publisher accommodates DP work. The Publisher's optimal profit sharing is a trade-off. She wants to receive a large share of the profits from the CR work and yet keep the size of the DP work small. With high distribution costs, it is more profitable for the Publisher to deter the appearance of DP work than to accommodate it. A consequence of the threat of DP work to the Publisher in bargaining is that her profit is increasing in the distribution cost, for certain parameter values. If it is possible to contract over the Author's outside profit in bargaining, to do a “360 deal”, the parties prefer the arrangement. Given a bargaining outcome over the profits from CR work, it is always possible to agree on sharing rule over all profits that does not decrease the profits of either party. Under a “360 deal”, the choice for DP work appears on a smaller region of the parameter space, since all profits are under bargaining and the Author and Publisher have the same incentives for total profit maximization. Finally, under the “360 deal”, if the value of awareness is high and distribution costs are also high, the parties find it optimal to publish all output as DP work. Now increased awareness for performances dominates profits from CR work. We compare the above results to the benchmark surplus-maximizing outcome. Below a threshold distribution cost, the Author chooses direct publishing while surplus is maximized with no direct publishing. Above the threshold this is reversed. An outcome where all output directly published as DP work yields a high level of surplus, because the deadweight loss resulting from property rights is eliminated.====Bazen et al. (2015) surveyed how artists that publish in Jamendo assess the idea of the zero-priced work and found that emerging musicians value the large audience. Firms are active in the field. For example, the publishing house Elsevier (Elsevier 2017) offers authors either standard copyright publication or open access direct publication. Somewhat in line with our set-up, an academic author trades off larger awareness with costly open access to standard publication with no cost to the author. McCabe and Snyder (2015) analyzed open access within academic journals, and found that for some open on-line platforms, publication there had a significant positive effect on citations. Gayer and Shy (2006) considered piracy in a setting where an author and a publisher share the profits from copyright works but where the author receives all profit from performances. They found that the author may accept illegal copying since it enlarges the potential audience for performances. Our approach builds on this analysis. We, however, model the Author's decision of the share of DP work and consider the consequences of such decisions to the bargaining between parties. Caves (2003) described the contractual relationships between authors and publishers. He asserted the bilateral nature of such contracts and also that the contracts are signed well before actual sales. In addition, Towse (2006) pointed out the weak position of the authors in bargaining and the authors’ rights to decide over their work even after contracting. Krueger (2005) and Connolly and Krueger (2006) found that performance revenues are significant and have grown faster than revenue from copyright works.",Direct publishing and the bargaining between the author and the publisher,https://www.sciencedirect.com/science/article/pii/S0167624517301804,10 April 2019,2019,Research Article,100.0
"Baake Pio,Sudaric Slobodan","DIW Berlin, Mohrenstr. 58, Berlin 10117, Germany,Humboldt-Universität zu Berlin, HU Berlin, Spandauer Str. 1, Berlin 10178, Germany","Received 23 August 2017, Revised 24 November 2018, Accepted 19 January 2019, Available online 30 January 2019, Version of Record 23 March 2019.",https://doi.org/10.1016/j.infoecopol.2019.01.003,Cited by (6),"We analyze competition between Internet Service Providers (ISPs) where consumers demand heterogeneous content within two Quality-of-Service (QoS) regimes, Net Neutrality and Paid Prioritization, and show that paid prioritization increases the static efficiency compared to a neutral network. We also consider paid prioritization intermediated by Content Delivery Networks (CDNs). While the use of CDNs is welfare neutral, it results in higher consumer prices for internet access. Regarding incentives to invest in network capacity we show that prioritization regimes lead to higher incentives than the neutral regime as long as capacity is scarce, while investment is highest in the presence of CDNs.","This paper contributes on the ongoing debate on ‘net neutrality’ — a concept that broadly requires that all internet traffic should be treated equally (Wu, 2003). One central aspect within the debate revolves around differentiation with respect to Quality-of-Service (QoS), i.e. whether or not all content classes should face identical service quality within the network. While opponents of net neutrality argue that QoS differentiation is part of reasonable network management and should therefore be allowed if not encouraged, net neutrality proponents argue that this benefits mainly network providers as it opens up new revenue models, and picks a few winners amongst the landscape of content providers (CPs). Indeed this ambivalence can be found e.g. in EU guidelines (EP and Council of the EU, BEREC) where a neutral treatment of internet traffic appears as a central pillar of the new regulation, while internet service providers (ISPs) may still offer differentiated QoS under certain conditions.==== While there are various ways of QoS alterations within the management of a network, we would like to focus on the practice of ‘paid prioritization’ where CPs pay ISPs directly for prioritization of their content. We also consider the impact of Content Delivery Networks (CDNs) such as Akamai or Limelight. Instead of contracting with network operators directly, content providers can contract with an intermediary, the CDN, which then delivers the traffic to the ISPs.====The purpose of this paper is to analyze how paid prioritization affects the static efficiency for a given network infrastructure and the dynamic efficiency regarding incentives for investment in network capacity. We further analyze whether these effects are altered by a CDN. In a neutral regime ISPs are only allowed to offer one quality level, i.e. all participants experience potential network congestion to the same extent. In a paid prioritization regime ISPs can charge CPs for bypassing the network congestion by having access to a ‘priority lane’. In a CDN environment ISPs offer access to their priority lanes to CDNs instead, which then resell the access to CPs. This setup reflects the idea of capacity bottlenecks in the regional or last-mile segment where congestion occurs because of high consumer demand (e.g. in legacy copper or coaxial networks).====We present a two-sided market model where two symmetric ISPs compete for consumers and CPs. Consumers are assumed to single-home, i.e. they purchase internet access only once, while CPs are free to multi-home with respect to their QoS choice. Content is differentiated with respect to connection quality sensitivity and quality levels are derived from a M/M/1 queuing system, where the non-priority quality (‘best-effort’) always remains free of charge, while the priority quality becomes a possible revenue source.====Using this framework, we show that the two regimes of QoS differentiation are welfare superior to the neutral regime. As content is differentiated, a tiered quality regime allocates priority to highly sensitive content classes while it leaves content classes with low quality sensitivity in the waiting queue, resulting in a more efficient use of existing network capacity. In particular we show that from a welfare perspective it is irrelevant whether this is achieved by direct paid prioritization or through the use of a CDN. Differences emerge once we take into account strategic effects of the QoS regimes on competition for consumers. Here we argue that QoS differentiation makes the consumer market more elastic leading to lower consumer prices in regimes of QoS differentiation compared to the neutral regime. Comparing paid prioritization offered by ISPs to the intermediated prioritization by a CDN shows that consumer prices are higher with CDNs. This is the case because ISPs do not internalize the negative effect of a consumer price increase on the CP market in the presence of CDNs. Lastly, we analyze unilateral incentives to increase network capacity from a symmetric equilibrium perspective and show that as long as network capacity is scarce, both QoS regimes lead to higher investment in network capacity than the neutral regime. The use of a CDN leads to the highest investment incentives irrespective of the initial capacity level.",Net neutrality and CDN intermediation,https://www.sciencedirect.com/science/article/pii/S0167624517301488,March 2019,2019,Research Article,101.0
"Farajallah Mehdi,Hammond Robert G.,Pénard Thierry","Rennes School of Business, France,North Carolina State University, USA,CREM, University of Rennes 1, France","Received 28 November 2017, Revised 2 December 2018, Accepted 10 January 2019, Available online 11 January 2019, Version of Record 29 August 2019.",https://doi.org/10.1016/j.infoecopol.2019.01.002,Cited by (26),"We examine how price and demand are determined on peer-to-peer platforms and whether experience and reputation have the same impact as in traditional markets. We use data from the world's leading intercity carsharing platform, BlaBlaCar, which connects drivers with empty seats to riders. We find that pricing decisions evolve as drivers gain experience with the platform. More-experienced drivers set lower prices and, controlling for price, sell more seats. Our interpretation is that more-experienced drivers on BlaBlaCar learn to lower their prices as they gain experience; accordingly, more-experienced drivers earn more revenue per trip. In total, our results suggest that peer-to-peer markets such as BlaBlaCar share some characteristics with other types of peer-to-peer markets such as eBay but remain a unique and rich setting in which there are many new insights to be gained.","The rise of the “sharing economy” (also known as the “collaborative” or “on-demand” economy), and the success of platforms like AirBnB and Uber have attracted the attention of economists and other academics as well as the popular press (Horton and Zeckhauser, 2016). Einav et al. (2016) define these platforms as peer-to-peer markets and emphasize their role in connecting individuals that can be either sellers, buyers, drivers, or workers, and enabling transactions that would not be possible in traditional markets.==== Peer-to-peer markets are increasingly used for transportation, lending, accommodation, home services, deliveries, or task assignments (Sundararajan, 2016). Peer-to-peer platforms have been argued to provide important efficiency gains (Edelman and Geradin, 2016). Specifically, they lower search and transaction costs (e.g., reduce information asymmetries) and allow fuller use of resources (e.g., increase car occupancy). However, our understanding about pricing behavior and market outcomes on peer-to-peer platforms is still limited.====Our paper investigates how price and demand are determined on peer-to-peer markets and whether experience and reputation have the same impact as in traditional markets. We define experience as a seller with a long tenure in the market and reputation as a seller with a lot of good feedback from past buyers. There is a lot of evidence that buyers are willing to pay more for items sold by sellers with good reputation (Bolton et al., 2013, Cabral and Hortaçsu, 2010, Jin and Kato, 2006, Melnik and Alm, 2002, Resnick et al., 2006). Sellers also care about their own reputation. Inexperienced sellers use reputation-building strategies, and they tend to charge higher prices as they accumulate experience and ratings (Jolivet et al., 2016). These questions have been extensively studied in the context of marketplaces like eBay or AmazonMarketplace (e.g., Cabral, 2012, Dellarocas, 2003, Tadelis, 2016). It is important to investigate whether experience and reputation can have similar effects on platforms where sharing is a key element of the transaction, most sellers are non-professional, and users’ feedback concerns the social experience rather than exclusively the product (Zervas et al., 2015, Fradkin et al., 2014).====In this paper, we focus on intercity ridesharing platforms. With intercity carpooling, drivers and riders share the space within the car together for up to a few hours. Because of these face-to-face interactions, there is a much larger scope for driver and rider characteristics to matter and for behavior to evolve in interesting ways as users gain experience on the platform. Our data come from the leading carpooling platform, BlaBlaCar, which is valued at $1.5 billion as of 2015. BlaBlaCar connects a driver with empty seats to riders to share an intercity trip. The importance of BlaBlaCar has been emphasized by Sundararajan (2016), calling it “the company that dominates [the intercity carsharing] market” and noting that BlaBlaCar moves “as of 2015, more people every day than the US national rail system Amtrak” (Sundararajan, 2016 p. 12).====We use this empirical setting to study the determinants of price setting and demand behavior in order to understand how we should expect these types of peer-to-peer markets to evolve moving forward. The BlaBlaCar setting is uniquely well suited for our study because its price-setting environment is different from other carsharing peer-to-peer markets. On platforms like Uber and Lyft, pricing is centralized by the market maker and thus price is the same for any driver offering a given trip at a given moment. In contrast, on BlaBlaCar, pricing is decentralized: drivers set their own price for each trip. Further, all drivers on BlaBlaCar are non-professional. This provides rich price variation and interesting price dynamics as drivers gain experience on the platform.====We collected a large data set on the French carsharing market, which is the home market of BlaBlaCar. Our econometric model addresses the endogeneity of the driver's price and, controlling for price, models demand. Our two sets of main results concern a driver's level of experience and demographic characteristics. First, we focus on how drivers’ price-setting behavior evolves as they gain experience on the platform. The results suggest that more-experienced drivers set lower prices and sell more seats than less-experienced drivers. The price result is counter to evidence from other offline and online markets: firms with more experience in the market commonly charge higher prices.====Our finding that drivers lower their prices as they gain experience suggests that learning is important in understanding price setting on sharing platforms like BlaBlaCar. Specifically, we find that drivers learn to set lower prices and earn more revenue per trip over time. The effects we document are not being driven by riders’ responses to drivers’ reputation. These new insights are important relative to the large literature on peer-to-peer markets such as eBay, where demand-side responses to reputation have been the focus (Cabral, 2012). Our results highlight the important role of supply-side changes in pricing behavior as drivers gain experience on the carpooling platform. We conclude that prices and market outcomes on “sharing platforms” such as BlaBlaCar are determined differently than on marketplaces such as eBay.====Second, we find the demographic characteristics of a driver have strong predictive power for her price and the demand for her seats. Matching drivers’ first/given names to a database of names and their predominant country or region of origin, we classify drivers based on names that are common in our setting of interest, France. Comparing French-sounding names to Arabic-sounding names, drivers with a predominantly French name sell more seats and drivers with a predominantly Arabic name sell fewer seats. An Arabic name is associated with a particularly strong negative effect on demand and reduces driver revenue substantially. Our findings are related to the growing empirical literature on digital discrimination, including studies of AirBnB (Cui et al., 2016; Edelman and Luca, 2014; Edelman et al., 2017 and Kakar et al., 2018), Craigslist (Doléac and Stein, 2013), Uber (Ge et al., 2016), and Prosper.com (Pope and Sydnor, 2011). These studies show evidence of discrimination on both sides of the market (toward suppliers and demanders) and seek to understand the underlying mechanisms (statistical versus taste-based discrimination).====The paper is organized as follows. The next section introduces BlaBlaCar. Section 2 describes our data and Section 3 explains the empirical methodology to analyze market outcomes and address the simultaneity of price setting and demand. Section 4 presents the main econometric results and Section 5 presents revenue results. Section 6 explores the mechanisms that drive our results, while Section 7 concludes.",What drives pricing behavior in Peer-to-Peer markets? Evidence from the carsharing platform BlaBlaCar,https://www.sciencedirect.com/science/article/pii/S0167624517302135,11 January 2019,2019,Research Article,102.0
"Briglauer Wolfgang,Dürr Niklas S.,Falck Oliver,Hüschelrath Kai","ZEW Centre for European Economic Research and MaCCI Mannheim Centre for Competition and Innovation, P.O. Box 10 34 43, D-68034 Mannheim, Germany,ECOAustria, Am Heumarkt10, 1030 Vienna, Austria,Vienna University of Economics and Business, Welthandelsplatz 11, 1020 Vienna, Austria,University of Munich and Ifo Institute–Leibniz Institute for Economic Research at the University of Munich, Munich, Germany,CESifo, Poschingerstraße 5, D-81679 Munich, Germany,University of Applied Sciences, Faculty of Business and Economics, Blechhammer 9, D-98574 Schmalkalden, Germany,ZEW Centre for European Economic Research, P.O. Box 10 34 43, D-68034, Mannheim, Germany","Received 9 March 2018, Revised 2 January 2019, Accepted 3 January 2019, Available online 4 January 2019, Version of Record 23 March 2019.",https://doi.org/10.1016/j.infoecopol.2019.01.001,Cited by (41),"We evaluate the impact of a major European state aid programme for speed upgrades in broadband internet availability applied to rural areas in the German state of Bavaria throughout 2010 and 2011. We use a matched difference-in-differences estimation strategy that compares treated and untreated municipalities that are balanced with respect to funding criteria. We find that aided municipalities have a significantly higher broadband coverage at higher speed than non-aided municipalities. This increase in broadband coverage does not, on average, benefit the local population in terms of increase in local jobs per resident or wages. However, these effects are heterogeneous across skill groups. Furthermore, we find positive effects on the employment rate among residents. Taking these results together suggests that an increase in broadband coverage through state aid protects rural areas from depopulation (while residents work at other locations), but does not contribute to a further closing of the economic divide in the form of creating new jobs.","The interrelationship between various types of infrastructure investment and economic development has fascinated generations of researchers. While there appears to be little dispute about the positive impact of the general provision of infrastructures such as transportation or communication networks for employment, innovation and growth,==== the question of the socially optimal degree of infrastructure deployment and most suitable financing options are much more controversial.====While historically the (seemingly) public good character of many infrastructures suggested their entirely public provision, the liberalisation processes in many network industries in the 1980s and 1990s broadened the financing options to entirely private or public-private investment projects. The public provision of infrastructures is more and more seen as limited to cases of market imperfection – i.e. situations in which market forces alone are unlikely to provide the socially optimal level of network deployment.====In the European Union, belief in the strategic importance of broadband infrastructures for economic development has long been affecting policy making. Although the European Commission aims at strengthening the incentives of private companies to invest in both the deployment of broadband infrastructures and subscriptions through the design and implementation of appropriate regulatory frameworks, since 2003 this general strategy has included the granting of state aid. This applies particularly to rural areas where the private investment incentives are considered insufficient due to the interference of large deployment costs and limited revenue potentials.====In fact, between 2003 and 2014, the European Commission approved a total of 136 state aid applications==== – mostly from regions, but also entire (smaller) countries. This aid was given for the deployment of broadband networks in rural areas, aiming at closing the digital divide and triggering welfare-enhancing externalities that are expected from a well-established broadband infrastructure as ‘general purpose technology’ (see Bresnahan and Trajtenberg, 1995). From a political-economy perspective, local policy makers in rural areas are very much in favour of broadband-deployment programs. Many people in rural areas hope that broadband availability counteracts agglomeration forces and keeps economic activity in remote areas. At the very least, it is a consumptive amenity. Shopping from home, media and entertainment consumption, internet phone calls or the option to work from home may hinder people from moving away from such remote areas. The negative externalities often related to infrastructure (e.g., noise) are minimal in the case of broadband deployment (wires are installed underground in many European countries).====Despite the huge interest in such policies, their rigorous ex-post evaluations are still rare and are limited to few countries. Beyond, studies are often limited to the analysis of the effectiveness of such policies, that is whether a policy increased broadband availability or adoption. Those studies that also evaluate indirect effects of a policy concentrate on outcomes reflecting economic activity such as value added or job creation but oversee the role of broadband as a consumptive amenity. For an overview of the literature, we refer the interested reader to Section 2 as well as to What Works Center for Local Economic Growth (2015a). We contribute to narrowing this gap in the literature by evaluating such a state aid programme for broadband deployment introduced in the German state of Bavaria in the years 2010 and 2011 on broadband coverage, local employment, and the decision of where to live. This program was meant to bring speed upgrades to rural areas that were underprovided with broadband access at more than 1 Mbit/s. The analysis of speed upgrades is especially informative for policy-makers nowadays since low-speed broadband infrastructure is available for the largest part of the population in most countries.====We apply a difference-in-differences (DiD) estimation strategy based on a matched sample of 1845 aided and non-aided rural municipalities. The matching strategy guarantees balanced initial broadband availability, as well as balanced economic characteristics of treated and non-treated municipalities. Since the main funding criterion was that the respective municipality was underprovided in terms of access to broadband, treated and matched untreated municipalities had the same chance of receiving subsidies for broadband deployment. Thus, we argue that the application for state aid was driven by factors like varying availability of information about the program or differences in individual preferences of local politicians for new technologies, rather than by economic considerations. We find that the state aid program was indeed effective, and contributed to closing the digital divide. Aided municipalities have, depending on broadband quality, between 18.4 and 25.4 percentage points higher broadband coverage at higher speed than non-aided municipalities.====We find no evidence that the policy benefited the local population in terms of local jobs per resident (or self-employment) or wages in the short-term. However, this zero effect masks significant positive effects on the share of jobs with qualification. We also find positive effects on the employment rate among residents (irrespective of where they work). Overall, we conclude that an increase in broadband coverage allows workers to live in these rural municipalities – and prevents these areas from depopulation, respectively – but without attracting substantial additional economic activity necessary to create jobs and to close the economic divide.====The remainder of the paper is organised as follows. In Section 2, we present our view on related literature and discuss our contribution to this strand of literature. The third section continues with a description of the institutional structure of broadband state aid in the European Union in general, and in particular, its implementation in the German state of Bavaria. The fourth section provides a detailed characterisation of our empirical strategy. The fifth section describes our data, followed by the presentation and discussion of our estimation results in Section 6. Section 7 concludes the paper with a review of its main results and the identification of avenues for future research along with the limitations of our analysis.",Does state aid for broadband deployment in rural areas close the digital and economic divide?,https://www.sciencedirect.com/science/article/pii/S0167624518300556,March 2019,2019,Research Article,103.0
"Kuroda Toshifumi,Koguchi Teppei,Ida Takanori","Tokyo Keizai University, 1-7-32, Minami-cho, Kokubunji-shi, Tokyo 185-8502, Japan,Shizuoka University, Japan,Kyoto University, Japan","Received 24 July 2017, Revised 19 August 2018, Accepted 19 December 2018, Available online 27 December 2018, Version of Record 23 March 2019.",https://doi.org/10.1016/j.infoecopol.2018.12.002,Cited by (1),"Modern economic theory predicts that tying can serve as a tool for leveraging market power. In line with this economic theory, competition authorities regulate the tying of Microsoft Windows with its Media Player or Internet browser in the EU and Japan. The authorities also take note of the market power of mobile handset operating systems (OSs) over competition in the app and services markets. However, no empirical evidence has thus far been presented on the success of government intervention in the Microsoft case. To assess the effectiveness of government intervention on mobile handset OSs, we identify the extent to which complementarity and consumer preferences affect the correlation between mobile handset OSs and mobile service app markets (mail, search, and map). We find significant positive complementarity between the mail, search, and map services, and mobile handset OSs. However, the elasticities of the mobile handset OS–mobile service are rather small. We conclude that taking action to restrict mobile handset OSs is less effective than acting on mobile services market directly.","There has been stiff opposition to Microsoft's excessive control of the personal computer (PC) operating system (OS) market in the United States, Europe, and Japan. The US Department of Justice brought a lawsuit against Microsoft, accusing the company of abusing its market power and threatening rival companies, thus harming consumers. To redress such anti-competitive conduct, the European Commission prohibited Microsoft from offering its Windows OS with Media Player included. Moreover, the Japan Fair Trade Commission admonished Microsoft for bundling its Office software with its Internet browser.====In July 2018, the European Commission fined Google €4.34 billion for the abuse of dominance in general Internet search on Android device manufacturers and mobile network operators to exclusively pre-installed the Google Search apps on their devices (European Commission, 2018). In October 2015, the Russian Federation Antimonopoly Service approved a lawsuit filed by the competitor search site Yandex against Google for violating the rules designed to protect competition and ordered Google to pay fines.====At issue is whether the OS market can be controlled by tie-ins of application software with smartphone OSs, which are then used to leverage application software. The Chicago School theory (Posner, 1976) holds that monopolies using tie-ins do not raise their profits. However, recent theoretical research has found that tie-ins can be used to leverage market control (Whinston, 1990, Carlton et al., 2010) as well as prevent market entry (Nalebuff, 2004, Choi and Stefanadis, 2001).====Accordingly, the effect on the market of Microsoft tying its Media Player and Internet browser to its OS is unclear, as are the results likely to be achieved by the actions of the competition authorities. In particular, the extent to which OS tie-ins or interventions affect economic welfare is not known (Gilbert and Katz, 2001, Klein, 2001, Whinston, 2001). Moreover, although Microsoft's share of the PC OS market has remained high, whether its market power has extended to the smartphone OS and Internet service markets and the nature of its response to governmental interventions remain unclear.====To consider the effectiveness of governmental intervention in the smartphone market, the present study quantifies whether a complementary relationship exists between mobile phone OSs and services. For our analysis, we use data taken from a consumer survey that examined the choice of mobile phone OSs and services at two points in time (2012 and 2015) in Japan, which has seen a rapid spread in mobile Internet usage. Specifically, the mobile phone OS in our analysis comprises the three OSs of Japanese mobile phone carriers at that time (iMode, EZWeb, and Yahoo! Ketai) as well as Google Android, Apple iOS, and Microsoft OS (Windows Mobile) commonly used in today's market.====The users of a mobile phone OS produced by a certain company tend to use the services of that company more than do other handset users. However, it remains unclear whether this is because of product quality (e.g., the complementarity of the mobile phone OS and services)==== or a result of consumer preferences (e.g., brand preferences for certain carriers). To identify the correlation between the mobile handset OS and service owing to brand preference and the complementarity of services and handsets, the present study examines the market price for mobile handsets, which affects handset choice but not service choice, as instrumental variables. This approach enables us to divide the correlation between mobile handset OSs and services into complementarity with consumer preference correlations.====We find that the choice of a mobile phone OS significantly affects the choice of mail, search, and map services. Furthermore, while the OSs of Japanese mobile phone carriers were influential in guiding customers to the company's services in 2012, this influence weakened in 2015 with the rising presence of the OSs of Google and Apple. Between 2012 and 2015, the elasticity of the increase in users of Google Search produced by an increase in users of the Google OS rose from 0.06 to 0.11. Moreover, in 2012 Apple began tying in its search (Siri) and map (Map) OSs. The increase in OS users in 2015 thus produced elasticities of the increase in service users of 0.59 and 0.53, respectively.====However, while these estimation results are statistically significant, the size of their influence on market competition is not meaningful. The search utilization rate using mobile phones was 66.8% in 2015 with Google's share of this 35.7%. Moreover, the usage rate for map services was 52.7% (70.4%). A factor explaining this breakdown of market share was the fact that the effects of the complementarity of handset OSs and services were negligible, while the larger share of the service market resulted from the quality of these services themselves.====Concerning the Microsoft issue, the European Commission intervened, unbundling Media Player from Windows and citing standard web browser alternatives with regards to the default settings. However, because of the declining complementarity between the Google OS and its services, despite the intervention to prohibit the pre-installment of a company's own services on its OS, there was a negligible effect on market share. Put another way, although the excess market control of the handset, services, and app markets in the OS market became apparent, the remedies in the services and app markets makes it important to consider plausible interventions besides prohibiting pre-installment.====The framework of the remainder of the paper is as follows. In Section 2, we discuss mobile Internet in the Japanese phone market and the transition to smartphones, which forms the backdrop to the analysis. In Section 3, we examine the Competition Review in the Telecommunications Business Field report used in this analysis as well as the survey data from 2012 and 2015. In Section 4, we present econometric models that use control functions to identify the correlations between complementarity and consumer preferences. In Section 5, we compare the mobile phone OSs for both 2012 and 2015. Lastly, Section 6 contains a conclusion and policy implications.",Identifying the effect of mobile operating systems on the mobile services market,https://www.sciencedirect.com/science/article/pii/S0167624517301191,March 2019,2019,Research Article,104.0
"Courty Pascal,Ozel Sinan","University of Victoria, Canada,CEPR, Europe","Received 28 July 2017, Revised 7 November 2018, Accepted 20 December 2018, Available online 21 December 2018, Version of Record 23 March 2019.",https://doi.org/10.1016/j.infoecopol.2018.12.003,Cited by (4),"Online retailers use scarcity cues to increase sales. Many fear that these pressure tactics are meant to manipulate behavioral biases by creating a sense of urgency. At the same time, scarcity cues could also convey valuable information. We measure the value of the scarcity messages posted by Expedia to a ==== rational consumer. A signal reveals information on the number of seats available at the posted price. Consumers can use this information to optimally time when they purchase a ticket. The maximum increase in expected utility for a naive consumer, who does not use publicly available information, is 8%. For a sophisticated consumer, the increase is between 4–7%. Scarcity signals have a negligible impact on seller revenue and consumption.","Scarcity cues and pressure tactics are widely used by online retailers to increase sales (Nagpal, 2014).==== According to marketers and some social scientists, scarcity creates a sense of urgency, it increases desirability and gives a perceived benefit of acting quickly (Worchel, Lee, Adewole, 1975, Lynn, 1991, Verhallen, Robben, 1994, Mullainathan, Shafir, 2013). Although many fear that sellers manipulate the psychology of consumers, scarcity messages can also deliver information that is not available otherwise. A Bayesian consumer could benefit from this information even if messages are meant to manipulate behavioral consumers subject to decision biases.====This paper measures the informational value of scarcity messages in the context of air travel. Airfares can vary dramatically from day to day. Many travelers have to choose whether to book a non-refundable ticket without knowing future fares and whether it would be wise to postpone purchase. Airlines try to influence travelers by presenting scarcity signals next to airfares. For example, fares displayed on Expedia sometimes mention that there are few seats left at the posted price.==== We develop a Bayesian rational framework to evaluate the value of such signals. This approach is supported by evidence, from similar contexts as ours, that shows that some consumers behave as rational optimizers (Li, Granados, Netessine, 2014, Cui, Zhang, Bassamboo). Clearly, the rationality assumption may not apply to all travelers, but for our purpose, it is a starting point to derive empirical predictions that can be tested using only information on prices and signals. Our approach offers a relevant benchmark for the consumers who respond to messages as expected utility maximizers. Although we cannot test this assumption (because we do not observe consumer bookings), our measure delivers an upper bound for the value of signals.====For our application, we collect an original dataset using a web scraping script that submits queries to the Online Travel Agent (OTA) Expedia (Edelman, 2012). As a descriptive step, we compare the distributions of price changes conditional on the signal realization; whether or not ‘few seats left’ is posted next to the fare. The posterior with the scarcity signal first order stochastically dominates the posterior without a scarcity signal. A scarcity signal lowers the chances that the posted fare will decrease and that it will remains constant, two pieces of information consumers care about. Showing that Expedia signals are informative is a contribution in itself and it establishes the basis for the rest of the paper.====In the core of the analysis, we consider a simple one-off purchase-delay decision. The traveler can buy a ticket now or postpone her purchase decision by one week and she can do so ====. The only source of uncertainty is regarding what the price will be next week. This stylized scenario is consistent with recent related works (Li et al., 2014). We also present results where the consumer can delay her purchase twice and the main insights do not change. If the consumer believes that the price increases in expectation, which is often the case for airfares, her purchase decision depends on how much she values traveling relative to the current price. We obtain a threshold rule: the consumer buys at the current price if and only if her current utility, denoted ==== where ==== is her valuation and ==== the current price, is above a fixed positive threshold. A week later, the consumer who waited buys if and only if she receives a non-negative utility, that is, ==== ≥ ====, where ==== is the updated fare.====Fig. 1 plots the traveler’s utility (vertical axis) under three scenarios as a function of her valuation (horizontal axis). The current price is set at ==== which corresponds to the average price in our sample. The figure features valuations in the range [$306, $322] because consumers outside that range do not respond to the signal. The dotted line plots the consumer surplus if she cannot delay. The consumer does not care about the signal realization. Her utility is zero for valuations below $300 (outside the range of the plot) and follows a 45 degree line for valuations above $300. The dashed line plots the expected surplus if the consumer is uninformed and has the option to delay purchase. An uninformed consumer does not update her decision based on the signal’s realization. The consumer with ==== is indifferent between buying at the current price and delaying. This consumer is located at the kink of the dashed line. The solid line plots the same surplus for the informed consumer. The two kinks in the solid line correspond to the indifferent consumers conditional on the signal realization. Consumers with valuations far from the indifferent uninformed consumer do not benefit from the signal (the dashed and solid lines coincide) and the increase in surplus from the signal is maximum for the indifferent consumer. Fig. 1 also reveals that for most travelers the gains from waiting is significantly higher than the gains from the signal.====An important contribution of this paper is to show that the increase in consumer surplus from the signal can be obtained by estimating non-parametrically the price distributions conditional on the signal realization. This method to compute consumer surpluses generalizes to travelers who conditions their decisions on publicly available information in addition to the signal. The signal increases the expected utility of an unsophisticated traveler, who does not condition her decision on any publicly observable information, by at most 8 percent. This corresponds to the percentage utility gain for traveler ==== in Fig. 1. We also compute the value of information for a sophisticated consumer, who uses public information in addition to the signal to predict future fares (Mantin and Gillen, 2011). For a traveler who conditions her decision on the number of days remaining till departure, the increase in expected utility is between 4 and 7%.====Finally, we compute the impact of scarcity signals on seller revenue and on the number of tickets sold. To do so, we assume that there is a uniform distribution of consumer valuation in the neighborhood of the indifferent consumer. Scarcity signals have a small negative impact on seller revenue for some subsamples of the data and little impact on the number of tickets sold. This surprising result calls for more work to understand the supply side of signals. A natural extension would be to add non-Bayesian travelers who respond to the signal in ways that systematically benefit the supplier.====This work is related to several strands of literature. The model touches upon the literatures on price discrimination with information revelation (Lewis and Sappington, 1994) and Bayesian persuasion (Gentzkow and Kamenica, 2011).==== The literature on scarcity signals is mixed. Scarcity theory in psychology and marketing argues that signals are largely used to exploit consumer biases (Brock, 1968, Aggarwal, Jun, Huh, 2011, Aguirre-Rodriguez, 2013, DellaVigna, Gentzkow, 2009, Mullainathan, Shafir, 2013). At the same time, Cui et al. (2016) offer convincing evidence that consumers respond, as assumed in this work, rationally and strategically to real-time Online information.==== Finally, the empirical application is related to the airline literature which is reviewed in the next section.====The rest of this paper is organized as follows. Section 3 presents a model of consumer decision making under price uncertainty, derives a measure of the value of information and computes the impact of the signal on seller revenues and consumption. The following section presents the data and descriptive statistics. Section 5 presents our main results and the last section concludes.",The value of online scarcity signals,https://www.sciencedirect.com/science/article/pii/S0167624517301257,March 2019,2019,Research Article,105.0
Visser Robin,"Deakin University, Department of Economics, 70 Elgar Road, Burwood, VIC 3125, Australia","Received 11 September 2017, Revised 13 October 2018, Accepted 17 December 2018, Available online 18 December 2018, Version of Record 23 March 2019.",https://doi.org/10.1016/j.infoecopol.2018.12.001,Cited by (22),"This paper investigates the effect of internet penetration in the form of broadband subscriptions on the extensive and intensive margins of differentiated exports, and assesses whether the internet bridges the linguistic gap in trade. Results tentatively indicate that there is a positive association between an increase in internet penetration and the extensive and intensive margins of differentiated exports. Splitting the sample into development levels, internet penetration may facilitate the extensive margin of exports between low and high income countries, but not within these groups. Lastly, an increase in internet penetration may decrease the effect of linguistic distance on the extensive and intensive margins of differentiated exports.","Since its inception in 1983, the internet has played a major part in society. The development of the World Wide Web in 1990 has facilitated rapid digitisation and globalisation and today, around 3.5 billion internet users exist globally. It is therefore not surprising that researchers have shown considerable interest in the effects of the internet on different aspects of the economy such as economic growth (Choi and Yi, 2009), consumer behaviour (Castañeda and Montoro, 2007), competitiveness (Brown and Goolsbee, 2000), and productivity (Oliner et al., 2008, Gust and Marquez, 2004). In the field of international trade, the effect of the internet has seen especially extensive research. Here, substantial evidence exists to suggest that development and increased use of the internet has a positive and significant effect on trade, especially so when it comes to developing economies (Clarke and Wallsten, 2006, Clarke, 2008, Kurihara and Fukushima, 2013, Yadav, 2014). Generally, a 10% increase in the number of internet users in a country leads to a 2% increase in FDI (Choi, 2003), and a 0.2–0.4% growth in exports (Freund and Weinhold, 2004, Bojnec and Fertö, 2009, Choi, 2010).==== While the evidence base for the effect of internet penetration on the overall volume of trade and investment flows is substantive, three key areas require further study.====First, little is known about the effect of the internet on the margins of trade, while it is at these margins rather than overall trade flows that the internet's trade cost alleviating effect (Freund and Weinhold, 2004) arises according to models of firm productivity by Melitz (2003) and Chaney (2008). As such, the focus in the literature on the internet as a search cost alleviator following Freund and Weinhold (2004) renders its role as a communication tool to be under-investigated. As such a tool, it affects both the fixed and variable costs of information exchange between suppliers and consumers as both are elements of communications costs (Fink et al., 2005). There are two broad ways to study the margins of trade. The extensive margin can be measured at the country level, i.e. in terms of the number of product categories exported, and at the firm level, i.e. firm-level transactions and export decisions. Likewise, the intensive margin can be defined as the value traded per product category or per transaction (Lawless, 2010, Felbermayr and Kohler, 2006). The use of firm-level data is not feasible in this paper because such data is heterogeneously available across countries; studies of the firm-level margins focus on a single country due to effort required in constructing such a dataset. Moreover, a country-level set of margins of trade also ensures comparability of such measures with broader developmental issues regarding internet penetration rather than firm behaviour.====The adoption of communication cost alleviators (among which the internet) into trade cost models occurs first in Lawless (2010), who develops a heterogeneous firm productivity model based on Melitz (2003) and Chaney (2008). In her model, and such models in general, fixed and variable trade costs negatively affect the extensive margin and ambiguously affect the intensive margin. In particular, based on a cross-sectional model of U.S. firms' exports she finds that an increase in internet use has a positive effect on the extensive margin, and a negative effect on the intensive margin.==== More recently, Osnago and Tan (2016) use data for 2001–2013 on internet use to find that a 10% increase in internet adoption increases bilateral exports by 1.9% (for exporters), that an exporter's extensive margin, in terms of number of goods exported, increases more so than the intensive margin as a result of internet use, and that internet use affects differentiated goods more than homogeneous goods. Their study departs from the trade creation versus displacement finding in Lawless (2010). Other studies along these lines focus on whether firm-level use of internet technologies affects the export likelihood of firms from developing countries (Yadav, 2014), and on the exporting decisions of eBay firms versus offline firms (Lendle and Vezina, 2015). Overall, research on the internet and the margins of trade does not yet reflect the global nature of the internet, nor does it comprehensively address the models outlined by Freund and Weinhold (2004) and Lawless (2010). The question of export diversification versus intensification matters especially from a development perspective where export diversification is especially beneficial to growth of countries in early stages of development (Cadot et al., 2011), although others again point to the dominating role of the intensive margin (Helpman et al., 2008, Felbermayr and Kohler, 2006).====Second, the manner in which developing countries benefit from increased internet penetration is as yet unclear. From a small number of studies it is clear that developing countries are the main beneficiaries of increased internet development. This goes for transition economies in Europe (Clarke, 2008), developing economies in Asia and sub-Saharan Africa (Clarke, 2008, Kurihara and Fukushima, 2013, Yadav, 2014), as well as developing economies in general (Clarke and Wallsten, 2006). These studies are based on the firm level (Clarke, 2008) and the overall trade flow level (Kurihara and Fukushima, 2013, Yadav, 2014, Clarke and Wallsten, 2006). Based on their results, Clarke and Wallsten (2006) conjecture that internet growth in a developing economy redirects developed economies' trade towards this economy, and away from developing countries where no internet growth has taken place; they thus suggest that trade displacement takes place, rather than trade creation. Utilising the margins of trade as outlined above addresses this by allowing for the identification of such trade displacement and creation effects along the export diversification and export intensity dimensions between countries in different development levels.====Third, the internet being a communication tool, its role in alleviating the costs arising from intercultural communication is under-investigated. According to the institutional literature, trade costs arise in part from differences in informal constraints such as socially shared rules (North, 1990). The negative (positive) impact of culture differences (similarities) on trade is, unsurprisingly, well established in terms of linguistic (dis)similarities (e.g. Melitz and Toubal, 2014, Egger and Lassmann, 2015), religious (dis)similarities (e.g. Lewer and van den Berg, 2007), trust emanating from cultural differences (e.g. Boisso and Ferrantino, 1997), and broader indicators of shared cultural values (e.g. Lankhuizen et al., 2011). Given the increase in trade costs caused by communication costs, and the reduction of communication costs by the internet, the extent to which internet penetration mitigates the negative effects of linguistic distance may be substantial. The only study that looks into this is by Lee (2015), who utilises a cross-sectional dataset on OECD countries to identify the effect of the interaction between cultural dissimilarity and internet use on bilateral trade. However, this estimation is based on Hofstede's (1980) cultural dimensions, which have met substantial criticism (Fang, 2003, Williamson, 2002, Baskerville, 2003, McSweeney, 2002). As such, the extent to which the internet alleviates communication costs arising from linguistic dissimilarities requires additional research to clarify, utilising the margins of trade, how this occurs.====The aim of the current research is to address these three gaps in the empirical literature. In doing so it answers four questions. First, how does the internet affect the extensive and intensive margins of trade? Second, what is the effect of the internet on the margins of trade taking into account differing development levels? Third, does internet growth in developing countries facilitate trade creation with or trade redirection to these countries? Fourth, does the internet alleviate the costs of communication arising from linguistic dissimilarity? This study contributes to the empirical literature on the effect of the internet on trade by addressing the questions outlined above, the answers to which remain under-developed or absent. An additional contribution is the dataset itself, which is the largest to date in this field. Moreover, the trade data collected for this study is the most detailed data available for country-level trade flows, which adds a new dimension to a body of literature which has so far largely relied on aggregate trade flows, and to a large extent on cross-sectional studies. Moreover, to the extent that the empirical literature has performed panel data studies of the effect of the internet on trade (Bojnec and Fertö, 2009, Bojnec and Fertö, 2010, Lin, 2015, Vemuri and Siddiqi, 2009), none have done so taking into account the margins of trade.====To achieve the stated aim this study uses a gravity model panel dataset for the extensive and intensive margin of exports from 162 countries to 175 destinations for the period 1998–2014. The results (1) tentatively indicate that there is a positive association between increased internet penetration and both the extensive and intensive margins of differentiated goods exports, (2) suggest that increased internet penetration in the form of broadband subscriptions in developing countries allows for expansion along the extensive and intensive margins in their exports to higher income countries, while exports to other low income countries only increase along the intensive margin, and (3) suggest that increased broadband subscriptions may reduce the negative effects of linguistic distance on the extensive and intensive margins of differentiated goods exports.====The remainder of this paper is divided into three sections. First is an overview of the data and methodology in Section 2, followed by the results in Section 3, and a conclusion in Section 4.",The effect of the internet on the margins of trade,https://www.sciencedirect.com/science/article/pii/S0167624517301580,March 2019,2019,Research Article,106.0
"Chesnes Matthew,Jin Ginger Zhe","Federal Trade Commission, Bureau of Economics, Washington, DC, USA,University of Maryland and NBER, Department of Economics, College Park, MD, USA","Received 19 July 2017, Revised 29 May 2018, Accepted 9 November 2018, Available online 16 November 2018, Version of Record 23 March 2019.",https://doi.org/10.1016/j.infoecopol.2018.11.001,Cited by (5),"Beginning in 1997, the Food and Drug Administration (FDA) allowed television advertisements to make major statements about a prescription drug, while referring to detailed information on the internet. The hope was that consumers would seek additional information online to understand the risks and benefits of taking the medication. This policy motivates us to analyze direct-to-consumer advertising (DTCA) and search engine click-through data on a set of drugs over the period from 2008 to 2011.==== shows that DTCA on a prescription drug is associated with a higher frequency of online search and subsequent clicks for that drug, as well as search for other drugs in the same class. While the FDA’s policy was intended to direct consumers to the internet for drug information, the effects of DTCA are larger for paid clicks relative to organic clicks and for clicks on promotional websites relative to informational websites. We also find the relationship between DTCA and search is heterogeneous: the effects are stronger for younger drugs and drugs that treat acute conditions.","Advertising regulations have extended from traditional medias to the internet. Take prescription drugs for example: in 1997, the Food and Drug Administration (FDA) allowed a television advertisement to focus on the essential efficacy and side effect information of a prescription drug as long as the manufacturer provided detailed drug information on the internet and in other publications Administration, Drug, Administration, Drug.==== The key assumptions are that (1) a television-watching consumer will seek more information on the internet, and (2) this process will result in a balanced understanding of the drug.====Our paper contributes to the recent literature assessing both assumptions. On the first, Kim (2015) analyzed warning letters issued to pharmaceutical companies regarding problems with their online search ads and found that most violations were for a lack of adequate risk information on branded drug websites and in online paid advertisements. After receiving these letters, many of the advertisements were removed. Chiou and Tucker (2016) looked at how search patterns changed as a result and found that consumers were more likely to click on websites that featured user-generated content and online pharmacies.====Beyond drug advertising, a number of papers have looked at the relationship between television advertising and online search. Joo et al. (2014) look at how television advertisements for financial services companies affected search for both individual brands and more general product categories. They find a large and significant effect on brand searches and a smaller effect on category searches.==== Lewis and Reiley (2013) show similar positive effects focusing on a range of consumer products advertised during the Super Bowl. Dinner et al. (2014) find cross-channel effects on sales: offline (online) advertising affects online (offline) sales.==== The interaction of offline and online advertising is studied in Goldfarb and Tucker (2011) where they show that online ads can be a substitute for offline ads in affecting sales of alcoholic beverages.====An intensive debate also targets the second assumption that consumers receive a balanced understanding of the risks and benefits of taking a particular drug. One side of the debate argues that it is misleading to provide drug information to consumers as they cannot directly choose their prescription. To make things worse, pharmaceutical manufacturers may not have the incentive to provide “balanced” information: researchers show that television advertisements tend to emphasize drug benefits over risk information (Kopp, Bang, 2000, Day, 2006) and although most prescription drug websites provide both risk and benefit information, the risk information is presented in a less prominent and accessible way (Huh and Cude, 2004). Other evidence confirms the concern that consumer pressure for advertised drugs may compromise doctors’ prescription choices (Kravitz et al., 2005) or lead to adverse drug-related events David et al. (2010).====The other side of the debate stresses the educational value of drug advertising: it informs consumers of a drug’s existence, which may prompt consumers to research the drug, associate it with self-observed symptoms and eventually seek treatment. For example, Jin and Iizuka (2005) show that DTCA is associated with increased doctor visits, while Avery et al. (2007) show that DTCA has a positive and significant impact on antidepressant use. Two other papers conclude the net effect of DTCA is positive for consumers, focusing on cholesterol-reducing drugs, statins. Jayawardhana (2013) finds positive consumer welfare effects of DTCA, while Sinkinson and Starc (2015) show that DTCA is a cost-effective solution from a societal perspective.====Implicit in the FDA’s 1997 guidelines regarding the adequate provision of package labeling in broadcast advertisements is that consumers could access detailed and balanced information about a drug at an alternative source, including the internet. While some consumers may directly navigate to the drug or pharmaceutical company’s website featured in the advertisment, it is likely that others may use a search engine to query the drug or condition. Our paper first addresses that question: do consumers search for online information upon exposure to direct-to-consumer advertising (DTCA)? If they do, the search engine will provide a mixture of organic and paid links, some of which are primarily informational, while others could be more focused on promoting drug sales. Therefore, we examine the type of websites that consumers click to determine if the websites are more informational in nature (likely consistent with the FDA’s intention) or more promotional.==== The types of websites that consumers click play an important role on both sides of the DTCA debate. Finally, we consider the question of advertising spillovers: will DTCA encourage consumers to go beyond the advertised drug and research competing drugs?====Our analysis focuses on the click behavior of consumers using comScore’s click-through data from the five largest search engines. According Fallows et al. (2004), search engines like Google and Yahoo! are an important gateway to the internet. Use of the internet in the U.S. increased from 52% of all American adults in 2000 to 84% in 2015 (Perrin and Duggan, 2015).==== When consumers go online, using a search engine is a very popular online activity: 91% of internet users visited a search engine in 2012 (Purcell et al., 2012). While consumers formerly relied on their doctor as the primary source of medical information, now they increasingly turn to the internet. A study by Pew Research found that 72% of internet users searched for health-related information on the internet in the past 12-months and 77% of users began at a search engine Fox and Duggan (2013).====Interestingly, while most DTCA for prescription drugs are conveyed via traditional medias such as television, radio, magazines, newspapers, and billboards, a small but growing portion is devoted to display (banner) advertisements and sponsored search on the internet. For example, pharmaceutical manufacturers spent $270 million on online display ads in 2010, 6.4% of overall DTCA spending.==== Overall, total DTCA spending on prescription drugs increased from $662 million in 1996 (the year before the FDA’s new guidance) to $4.2 billion in 2010, a 542% increase. According to Nielsen, in 2014, pharmaceuticals were the third largest category of ad spending.====Our analysis shows that advertising on a prescription drug serves to increase the frequency of online search for that drug as well as search for other drugs in the same class.==== The number of clicks following drug queries is positively associated with DTCA and the effect is significantly larger for paid clicks compared to organic clicks.==== Clicks (both organic and paid) on promotional websites are more strongly associated with DTCA compared to clicks on informational websites. The magnitude of the effect of DTCA varies significantly by media type. Broadcast, print, and internet advertising all show positive and significant effects, through broadcast ads show the largest effect on organic clicks while internet ads has the strongest association with paid clicks.====Because our comScore data are only available monthly, we supplement our analysis using daily Google Trends data and daily DTCA on television. Our findings using the comScore data are confirmed by a positive relationship between the higher-frequency DTCA and Google Trends index, suggesting that consumers’ internet search responds to television ads quickly.====We also find the relationship between DTCA and search is stronger for younger drugs and for those drugs that treat acute conditions. This group of drugs may be particularly important from the FDA’s point of view because consumers may be first-time users and lack experience taking a drug. As expected, drugs that are less likely to be covered by insurance plans also show a stronger (though insignificant) relationship between DTCA and search activity, particularly paid clicks on promotional websites.====Ippolito and Mathios (1991) show that older and potentially more informationally-disadvantaged populations are more responsive to advertising.==== If the advertising is associated with more clicks on promotional websites, it may expose these populations to relatively more biased and less balanced information. However, if the advertising is associated with clicks on informative websites, they also may benefit more than other populations if they lack awareness of alternatives. Our results show more evidence of the first hypothesis, primarily for older searchers. Promotional clicks are more strongly associated with DTCA for older searchers. In contrast, the relationship between DTCA and promotional clicks is reduced for less wealthy searchers. Neither age nor income level appears to affect the relationship between DTCA and clicks on informational websites.====The remainder of this paper is organized as follows. In Section 2, we provide a description of the data, which includes click-through data from comScore, drug information from the FDA, and advertising data from Kantar Media. Section 3 presents summary statistics on drug-related searches and advertising. Regression results are presented in Section 4 that show how DTCA is associated with the frequency of search and clicks following drug queries. We then show that our results are confirmed using a higher-frequency search index from Google Trends. We also consider spillovers of drug DTCA to other drugs in the same class and how drug attributes and searcher demographics affect the relationship between advertising and search. Section 5 concludes.",Direct-to-consumer advertising and online search,https://www.sciencedirect.com/science/article/pii/S0167624517301208,March 2019,2019,Research Article,107.0
Jacqmin Julien,"HEC Liège, University of Liège, B. 31 Place des Orateurs 3, 4000, Liège, Belgium","Received 25 April 2018, Revised 29 August 2018, Accepted 25 October 2018, Available online 30 October 2018, Version of Record 29 August 2019.",https://doi.org/10.1016/j.infoecopol.2018.10.002,Cited by (7),"This paper aims at assessing whether universities improve their enrollment in traditional taught in-person programs by providing MOOCs (Massive Open Online Courses). Focusing on French universities, we look at the impact on enrollment figures of providing MOOC on France Université Numérique (FUN), a MOOC platform. We find that new students intake of universities offering MOOCs rises over 2% the following academic year, all else being equal. Analyzing data related to the characteristics of students enrolled in online and traditional programs as well to media coverage, we see two intertwined explanations for this positive relationship. One relates to the enhanced information available to students when making their enrollment decision; the other lies in the attention-grabbing - and likely persuasive - character of the event created around MOOCs.","In 2011, two Stanford professors, Peter Norvig and Sebastian Thrun created a website offering a free access to their “Introduction to Artificial Intelligence” course. The course consisted of short videos, quizzes and online discussion forums. With over 160,000 students enrolled worldwide, including 23,000 of whom received a certificate of completion, they gained unprecedented success. From there, Sebastian Thrun went on to create Udacity, a platform hosting other MOOCs. Following the development and success of two other MOOC platforms, edX and Coursera, The New York Times declared 2012 as the “Year of the MOOCs”. Clayton Christensen, who first coined the concept of “disruptive innovation”, further predicted “‘wholesale bankruptcies’ over the next decade among standard universities” (The Economist, 2012).====Nonetheless, it remains questionable that MOOCs or other online educational programs will soon substitute sometimes centuries-old, higher education institutions. Various empirical studies (Figlio, Rush, Yin, 2013, Xu, Jaggars, 2013, Alpert, Couch, Harmon, 2016, Bettinger, Fox, Loed, Taylor, 2017) have questioned the quality equivalence between the online and live mode of delivery. Distance education technologies do have an impact on higher education institutions, although of a different nature, yet as complements rather than substitutes for them. First, as argued by Deslauriers et al. (2011) and Belleflamme and Jacqmin (2016), internet technologies facilitate the implementation of new pedagogical approaches e.g. by providing students with (quasi) instantaneous feedback and customizing courses enhancing the quality of the programs. Second, providing online courses can reduce the costs of traditional higher education programs by curbing Baumol’s cost disease (Bowen et al., 2014). For example, basing themselves on data relating to U.S. post-secondary education, Deming et al. (2015) find that institutions providing online courses - and especially for-profit and low-ranked public ones - are charging lower tuition fees. Finally, MOOCs can be seen as a means of improving the visibility of higher education institutions (Howarth et al., 2017), i.e. as a sample/stepping stone towards traditional programs.====Based on this last argument, this paper studies the effects of online courses provision on subsequent enrollment figures in traditional programs, focusing on French universities, some of which have provided MOOCs for free on the France Université Numérique (FUN) platform. Since its creation by the French government in collaboration with French universities in 2013, FUN has hosted close to 1.2 million different students in over 350 courses. To analyse these figures, we elaborate a panel dataset with data aggregated at the university level. We believe that the French open enrollment system and the absence of editorial line on the platform make for a suitable context for studying the impact of MOOC provision on future enrollments.====Using a fixed-effect identification strategy that controls for year- and university-specific unobserved heterogeneity, we observe that MOOC provision has a positive and significant impact on new students intake. The effect for the universities concerned is roughly equivalent to climbing up 75 ranks in the Shanghai ranking. We also find that non-Parisian universities and universities with a good Shanghai ranking observe a comparatively larger effect on their enrollments. Finally, students enrolled in high school programs preparing them for university are more likely to be influenced in their enrollment decision by the MOOC provision.====The study coming closest to the present one is that of Goodman et al. (2019) bearing on the parallel offer at Georgia Institute of Technology of a fully online and a taught in person Master’s programs in computer science, and analysing the impact of the former on enrollment on the latter. While both programs are similar contentwise, the former is less selective and six times less costly to students. What comes out of this study is first, that owing to this disparity, both programs appear to attract non-overlapping pools of candidates, but also that introducing the online program has increased the amount of applications in the traditional one. Thus these authors observe that this higher education institution, by investing the field of online education, did not by any means ”cannibalize” its traditional activities, on the contrary.====In addition, considering traditional student characteristics, media coverage and enrollment data on the MOOC platform, brings further evidences of the two underlying mechanisms behind this positive relationship. The first one finds its roots in the informational asymmetries between students and prospective traditional higher education institutions. By providing MOOCs, universities inform students about imperfectly observed characteristics, such as their will to innovate in new forms of pedagogy, their fields of specialization or their scholastic tradition.==== As they hear about or try out the free online samples of the traditional programs available, MOOCs may function as a means of informing students and impacting their enrollment decision. Hence, this channel sees MOOCs as a way to better inform students in their enrollment decision. This explanation comes close to a recent strand of the literature which examines the effect of information provision on students’ enrollment decision. So far though, the majority of papers have considered the provision of information about financial aid policies (Bettinger et al., 2012), personalized counseling (Pistolesi, 2017), university rankings (Luca and Smith, 2013), student satisfaction surveys (Chevalier and Jia, 2016) or about job prospects (Hurwitz and Smith, 2018).====The second explanation lies in the fact that attention is a scarce resource. The buzz surrounding MOOCs in the social and traditional media has caught students’ attention and influenced their enrollment decision. Accordingly, MOOCs are a persuasive rather than an informative marketing tool. Similar attention-catching events have been analyzed in the higher education context: e.g. the performance of college sports teams (Pope and Pope, 2014) or negative incidents like campus scandals (Luca et al., 2016).====This paper develops as follows: Section 2 describes the French higher education system. Section 3 presents the data; Section 4 develops our estimation strategy; Section 5 presents our results and Section 6 our conclusions.",Providing MOOCs: A FUN way to enroll students?,https://www.sciencedirect.com/science/article/pii/S0167624518300933,30 October 2018,2018,Research Article,108.0
