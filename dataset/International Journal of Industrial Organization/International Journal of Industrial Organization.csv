name,institution,publish_date,doi,cite,abstract,introduction,Title,Url,Time,Year,Type,Unnamed: 0
"Love Brian J.,Helmers Christian","Santa Clara University, United States","Received 3 November 2022, Revised 26 May 2023, Accepted 29 May 2023, Available online 12 June 2023.",https://doi.org/10.1016/j.ijindorg.2023.102978,Cited by (0),"The theory of patent “hold-out” posits that frictions in the market for licensing standard-essential patents (SEPs) provide incentives for prospective licensees to opportunistically delay taking licenses with the goal of avoiding or reducing royalty payments. We construct measures of pre- and in-litigation hold-out from information disclosed in U.S. cases filed 2010-2019. Relying on both SEP and a matched control set of non-SEP disputes, we explore whether frictions in the market for licensing are associated with hold-out. We find some evidence of an association between hold-out and both SEP portfolio size and enforcement uncertainty; however, we find no evidence associating pre- or in-litigation hold-out with the international breadth of SEP rights.","For more than two decades, academic and policy debates concerning “standard-essential patents” (SEPs)—i.e., patents covering essential features of ubiquitous technology standards, like WiFi and mobile broadband—have focused on the potential for SEP owners to leverage their patent rights to exclude competitors or otherwise “hold-up” companies that produce or sell standard-compliant products or services and, thereby, extract profits or royalties disproportionate to the pre-standardization value of their technological contributions (see for example, Shapiro, 2001, Lemley, Shapiro, 2007; DOJ/FTC, 2007; FTC, 2011, Scott Morton, Shapiro, 2016). Heeding these concerns, courts, antitrust regulators, and standard-setting organizations (SSOs) responded by placing limits on SEP enforcement in an attempt to curb SEPs’ potential anticompetitive effects (Shapiro and Lemley, 2020).====In recent years, however, the wisdom of regulatory reactions to hold-up has been fiercely contested by the advancement of countervailing concerns about the resulting potential for “hold-out” by prospective SEP licensees. According to SEP hold-out theory, current limits on SEP licensors’ ability to enforce their rights encourage prospective SEP licensees to opportunistically delay licensing SEPs in an effort to minimize royalty payments or avoid them entirely. Though principally supported to date by theoretical arguments (e.g., Layne-Farrar, 2016, Heiden, Petit, 2017, Llobet, Padilla, 2023) and anecdotal discussions of specific court cases (e.g. Epstein, Noroozi, 2017, Gupta, Devaiah, Bhushan, 2019, Barnett, Kappos, 2023, Heiden, Rappaport, 2023), warnings about the deleterious effects of hold-out are at present commonly expressed not just by SEP owners, but also by policymakers and media outlets, much as concerns about hold-up were widely aired just a few years ago.====We contribute to this active debate by investigating the prevalence of potential hold-out in patent disputes, as well as factors associated with such behavior. Specifically, we collect data related to hold-out from the dockets of U.S. district court cases filed 2010-2019 to enforce a declared SEP or one of a matched sample of non-SEPs (NSEPs). We determine whether and to what extent each accused infringer received pre-suit notice of the patent enforcer’s rights and additionally define and construct multiple variables suggestive of opportunistic delay. While both courts and the literature have focused to date on delays or refusals to negotiate ==== to litigation (which we refer to as “pre-litigation hold-out”), we additionally collect evidence of delay- or cost-related gamesmanship by accused infringers ==== the course of litigation (“in-litigation hold-out”).====Using these data, we analyze whether the motivations for hold-out identified in the literature and asserted in ongoing policy debates==== are associated with observing hold-out in practice. Specifically, we ask whether hold-out is associated with the size and international scope of asserted patent portfolios and families, as well as with metrics of patent quality and standard essentiality. We assess hold-out determinants in two ways. First, we examine variation among our sample of cases filed to enforce or challenge at least one declared SEP. This approach allows us to analyze potential determinants that are specific to the SEP context; however, it does not produce results relative to a baseline comparison. In a second approach, we provide comparative results by incorporating data drawn from a matched control sample of cases that involve NSEPs.====Considered in the aggregate, our results offer some evidence of an association between allegations of dilatory conduct by accused infringers and both the size of SEP enforcers’ patent portfolios and measures of enforcement uncertainty. However, we find little evidence of a link between alleged hold-out and the international breadth of a licenor’s SEP portfolio. Additionally, our findings exhibit a moderate degree of heterogeneity across our analyses of pre- and in-litigation delay metrics, as well as across our approaches that do and do not incorporate NSEP data. Accordingly, our results chiefly indicate a need for additional theoretical analysis of hold-out, both with respect to hold-out’s causes and with respect to how prospective licensees’ incentives change once litigation is initiated.====From a policy perspective, our findings are broadly consistent with efforts to facilitate portfolio licensing in the SEP context, as well as efforts aimed to increase the quality and better assess the essentiality of patents declared to SSOs. Otherwise, our approach suggests restraint in the face of calls (often forcefully made in recent years) to take sweeping action at the legislative, judicial, and SSO level to counteract perceived hold-out. While our analysis does reveal the occurrence of conduct that may qualify as hold-out under at least some existing legal frameworks, we caution that an improved understanding of the mechanisms driving licensor and licensee behavior (both before and during litigation) may be a predicate to the formulation of effective policy reforms.",Patent Hold-out and Licensing Frictions: Evidence from Litigation of Standard Essential Patents,https://www.sciencedirect.com/science/article/pii/S0167718723000590,Available online 12 June 2023,2023,Research Article,0.0
"Mansley Ryan,Miller Nathan H.,Sheu Gloria,Weinberg Matthew C.","Georgetown University, Department of Economics, 37th and O Streets NW, Washington DC 20057, USA,Georgetown University, McDonough School of Business, 37th and O Streets NW, Washington DC 20057, USA,Federal Reserve Board, Board of Governors of the Federal Reserve System, 20th Street and Constitution Avenue NW, Washington DC 20551, USA,The Ohio State University, 410 Arps Hall, 1945 N High Street, Columbus OH 43210, USA","Received 23 September 2022, Revised 18 May 2023, Accepted 19 May 2023, Available online 2 June 2023, Version of Record 10 June 2023.",https://doi.org/10.1016/j.ijindorg.2023.102975,Cited by (0),"We provide a methodology to simulate the coordinated effects of a proposed ==== using data commonly available to antitrust authorities. The model follows the price leadership structure in Miller, Sheu, and Weinberg (2021) in an environment with logit or nested logit demand. The model calibration leverages profit margin data to separately identify the extent of coordinated pricing from marginal costs. Using this framework, we demonstrate how mergers can shift incentive compatibility constraints and thereby lead to adverse competitive effects. The incentive compatibility constraints also affect the extent to which cost efficiencies and divestitures mitigate competitive harms.","Antitrust authorities often use model-based simulations to quantify the competitive effects of mergers and to evaluate the possible tradeoffs between adverse competitive effects and efficiencies such as marginal cost reductions (Davis, Garcés, 2009, Miller, Sheu, 2021). This approach was developed in the academic literature under the Bertrand assumption that each firm sets its prices to maximize its current profit, conditional on the prices of competitors (e.g., Berry, Pakes, 1993, Hausman, Leonard, Zona, 1994, Werden, Froeb, 1994, Nevo, 2000). As such, the simulations are intended to capture the changes in unilateral pricing incentives that are created by a merger.====However, it is well understood that mergers also can facilitate or enhance collusion and thereby have “coordinated effects” on market outcomes. The 2010 ==== of the Department of Justice (DOJ) and Federal Trade Commission (FTC) describe coordinated effects as follows:====The ==== also clarify that coordinated effects could, but need not, involve price-fixing or other explicit agreements:====The economics of coordinated effects have received considerable attention in the literature (e.g., Porter, 2020) and, indeed, coordinated effects may manifest differently than unilateral effects. As one of many examples, if a merger occurs in a market that already features monopoly prices due to collusion, then it is unlikely to cause prices to increase (e.g., Verboven, 1995).====Less well-developed are simulation methods for coordinated effects. There are a number of challenges. For example, it can be unclear how to select among multiple equilibria, given that the folk theorems indicate that there can be many different pricing strategies that constitute a subgame perfect equilibrium (SPE) in repeated pricing games. Furthermore, many canonical models of collusion allow for the possibility that incentive compatibility (IC) constrains the prices that can be sustained in SPE (e.g., Green, Porter, 1984, Rotemberg, Saloner, 1986). Thus, it can be necessary to model how punishment might unfold and how firms tradeoff current and future profit. In practice, coordinated effects analyses often default to identifying characteristics of an industry that make it more or less susceptible to collusion.==== This “checklist” approach is grounded in economic theory but does not quantify harm, and also can yield ambiguity if different aspects of an industry or merger have different implications for the viability of collusion.====In this paper, we examine a modeling framework that can overcome these challenges in specific settings: the model of ==== developed in Miller, Sheu and Weinberg (2021) (henceforth “MSW”). Our contribution is practical, as we seek to facilitate the appropriate use of the model in merger review. To that end, we simplify the MSW framework to a setting with a single market and logit or nested logit demand. We then show how the model can be calibrated with information that often becomes available to antitrust authorities during the course of merger review. We explore how a number of different mergers affect equilibrium outcomes in the model, and examine the conditions under which efficiencies and divestitures mitigate adverse competitive effects. We also discuss the settings for which the modeling framework may be appropriate.====We organize the paper as follows. We start with the model of price leadership in Section 2. The model is a repeated two-stage game of perfect information. In the first stage of each period, the leader announces a ==== to be applied above Bertrand prices by members of a coalition. In the second stage, all firms set prices simultaneously. Along the equilibrium path, the leader selects the supermarkup that maximizes its profit subject to the IC constraints of the coalition firms, and the coalition firms adopt the supermarkup in the subsequent pricing stage. Thus, although the leader’s announcement is cheap talk, it provides a coordination device that resolves (by assumption) the multiple-equilibrium problem. The model incorporates a set of fringe firms that price to maximize static profit functions. A merger in this context can affect the IC constraints and thus the magnitude of the supermarkup that emerges in SPE.====The model is intended to provide a reasonable representation of a pricing practice that has been observed across a range of settings. In 1946, the United States Supreme Court ruled that price leadership in the tobacco industry violated the Sherman Act, and this motivated the earliest academic articles on the subject (e.g., Stigler, 1947, Markham, 1951, Oxenfeldt, 1952, Bain, 1960). Anecdotal examples of price leadership are discussed in Scherer (1980), Lanzillotti (2017), and Harrington (2017), and evidence of price leadership has been considered in a number of price-fixing lawsuits when courts have weighed whether discovery should be granted to the plaintiffs.==== Recent empirical applications document follow-the-leader pricing in retail industries including supermarkets, pharmacies, gasoline, and beer (Clark, Houde, 2013, Seaton, Waterson, 2013, Chilet, Lemus, Luco, 2021, Byrne, de Roos, 2019, Miller, Sheu, Weinberg, 2021). We suspect price leadership may be (at least somewhat) prevalent because it simplifies the process through which coordinating firms select prices. Thus, what is convenient from a modeling standpoint—that the leader’s announcement effectively selects among the SPE that are generally available under the folk theorems—may also be convenient for the firms involved.====In Section 3, we develop how the primitives of the model can be calibrated to match information that often becomes available to antitrust authorities during merger review. Here we depart from MSW, which pairs demand estimates for the beer market with a supply-side orthogonality condition.==== Our focus reflects that what is possible in merger review can differ from what is possible for academic research. Specifically, in merger review, the time and data necessary for sophisticated demand estimation may be unavailable whereas, subject to the usual caveats, data on equilibrium objects such as margins and diversion may be possible to obtain from proprietary data or confidential business documents (e.g., Davis, Garcés, 2009, Miller, Sheu, 2021). With logit demand, calibration can be accomplished with as little as market shares, prices, two margins, and a diversion ratio. With somewhat more information, a nested logit demand system can be calibrated to allow for richer consumer substitution patterns. We illustrate by calibrating a version of the model using statistics on the beer industry presented in MSW; our merger simulation results are similar to what is obtained using the estimation-based approach of MSW.====We end the paper by exploring the antitrust implications of horizontal mergers in industries characterized by price leadership (Section 4). Using simulations, we first show how prices change as concentration increases in a market with symmetric firms. Under price leadership, prices can rise more quickly with concentration than under a standard Bertrand equilibrium, and near-monopoly prices can be attained at lower levels of concentration. This points to the additional observation that, to the extent price leadership allows firms to realize monopoly prices, mergers can lead to smaller price increases than would arise with Bertrand equilibrium (for a given market configuration). We then examine a variety of horizontal mergers in a market with asymmetric firms. Mergers involving the coalition firm with the binding IC constraint can have outsize effects on prices and welfare, as these mergers tend to relax the IC constraint and thereby result in higher equilibrium supermarkups. Another class of mergers that can have large effects involves the acquisition of a fringe firm by a coalition firm—the merged firm internalizes that the price of the fringe product affects IC constraints within the coalition, and this can amplify price changes.====We also show that price leadership can impact the ability of efficiencies and divestitures to mitigate the effects of a merger. For example, a merger that creates efficiencies for the firm with the binding IC constraint also creates slack in that firm’s IC constraint, and this can lead to ==== supermarkups in equilibrium. Such an effect does not arise if the merger does not involve the firm with the binding constraint. With regard to divestitures, we provide examples in which they can amplify, diminish, be neutral for the coordinated effects of mergers. What appears to matter is who has the binding IC constraint—one of the merging firms, the divestiture recipient, or some other firm. Putting these results together, the intuitions for efficiencies and divestitures that arise under Bertrand competition do not always extend cleanly to models of pricing coordination.====We conclude in Section 5 with a summary, a discussion about the applicability of the model, and directions for future research. Our work contributes to a broad literature that develops and evaluates methodologies with which to analyze the competitive effects of mergers. Two contributions that are close to our own are Moresi et al. (2015) and Igami and Sugaya (2021), which focus on understanding coordinated effects outside the price leadership context. More research analyzes unilateral effects. Among the topics that have been examined are upward pricing pressure (e.g., Farrell, Shapiro, 2010, Jaffe, Weyl, 2013, Moresi, Salop, 2013, Miller, Remer, Ryan, Sheu, 2016, Miller, Remer, Ryan, Sheu, 2017, Greenfield, Sandford, 2021) and auction models and bargaining (e.g., Waehrer, Perry, 2003, Miller, 2014, Loertscher, Marx, 2019, Sheu, Taragin, 2021). Other research explores the accuracy of modeling or the extent to which specific modeling choices affect inferences (e.g., Peters, 2006, Weinberg, Hosken, 2013, Garmon, 2017, Slade, 2021, Panhans, Taragin).",A price leadership model for merger analysis,https://www.sciencedirect.com/science/article/pii/S0167718723000565,2 June 2023,2023,Research Article,1.0
"Luo Jinjing,Moschini GianCarlo,Perry Edward D.","Citibank, Tampa, FL, United States,Iowa State University, Ames, IA, United States,Kansas State University, Manhattan, KS, United States","Received 17 July 2020, Revised 23 May 2023, Accepted 25 May 2023, Available online 27 May 2023.",https://doi.org/10.1016/j.ijindorg.2023.102977,Cited by (0),"We evaluate the role of brand and technology switching costs in the US soybean seed industry using a unique dataset of actual seed purchases by about 28,000 farmers from 1996 to 2016. Using a random coefficients logit model of demand, we estimate brand and technology switching costs, characterize the distributions of buyers’ willingness to pay for seed brands and the glyphosate tolerance (GT) trait, and assess the implications of brand and technology switching costs for farmers’ welfare, technology adoption, firm profits, and firm market shares. We find that farmers are willing to pay large premiums for brand labels, and even larger premiums for the GT trait, although there is considerable heterogeneity in these values. Switching costs play an important role in the soybean seed industry. Eliminating these costs would significantly increase buyers’ welfare, reduce seed prices and firm profits, decrease adoption of the GT trait, and impact industry consolidation by expanding smaller firms’ market shares.","There is a long-standing recognition that buyers often prefer products they have experienced in the past, relative to products with similar or even identical characteristics. Beyond the role of idiosyncratic preferences, the resulting inertia in observed consumers’ demand may reflect switching costs associated with moving away from past choices, where switching costs can be broadly construed to include learning costs, contractual or transaction costs, and also psychological elements. Switching costs have important implications for imperfectly competitive markets. By altering the responsiveness of consumer demand, switching costs directly affect equilibrium prices. Because these frictions make demand more inelastic, firms can exploit their loyal customer base by charging higher prices. Dynamically, however, switching costs also provide the opportunity for customer base expansion through aggressive pricing.====How such contrasting harvesting and investment incentives actually play out remains an open question (Farrell and Klemperer 2007, Cabral 2016), and is ultimately an empirical matter that depends on the specifics of a given market. A key challenge in this setting is to distinguish true ====—defined as the causal dependency of an individual's future choices on their current state (Heckman 1981a)—from the effects of unobserved preference heterogeneity (spurious state dependence). When present, true state dependence directly alters the returns to different choices and thus influences market equilibrium.====In this article, we study the nature and extent of switching costs in a novel context—soybean seed demand over a period characterized by the advent of genetically engineered (GE) varieties. This sweeping new technology led to early acquisitions and consolidation in the industry, as established brands had to adapt to the innovator's (Monsanto) entry and the tremendous success of GE soybeans with farmers. As such, it provides a useful context to empirically assess the extent to which inertia due to switching costs matters with respect to both brand and technology.====Our study adds to an extensive literature on demand inertia in both economics and marketing. Following Keane (1997), a large body of research, focused on the consumer-packaged goods (CPG) industry, has shown that the presence of state dependence can have important implications for the extent of market power and pricing behavior (Dubé, Hitsch, and Rossi 2010), market structure (Dubé, Hitsch, and Rossi 2009), the price effects of mergers (MacKay and Remer 2022), the pro-competitive role of advertising (Shum 2004), and the persistence of brand shares (Bronnenberg, Dhar, and Dubé 2009; Bronnenberg, Dubé, and Gentzkow 2012; Grzybowski and Nicolle 2021). Among non-CPG applications, Sudhir and Yang (2014), and Train and Winston (2007), study the automobile industry; Handel (2013) and Yeo and Miller (2018) analyze the health insurance industry; Honka (2014) investigates automobile insurance; and, Barone, Felici, and Pagnini (2011) study local credit markets.====Unlike much of related existing work, which has privileged consumer demand, in this paper we study state dependence for competitive producers’ (farmers’) choices. Furthermore, the industry and period we study is characterized by a major innovation, which permits us to investigate both brand and technology inertia, as well as their interplay.====First introduced in 1996, GE varieties provided farmers with drastically new technological solutions for weed and pest management, which led to rapid widespread adoption (Moschini 2008). Among the major US crops, the soybean seed industry has perhaps undergone the largest transformation. The once-common farming practices of saving harvested soybeans for seed use, and/or purchasing publicly developed varieties, have been replaced by the almost complete reliance on new proprietary commercial soybean varieties that embed the GE trait for glyphosate tolerance (GT). The GT trait was developed by Monsanto, who, as an outside proprietor, faced major challenges in converting their technology into a commercial product. Successful seed varieties required access to two complementary building blocks: the new GE traits, and elite germplasm in existing varieties (Graff, Rausser, and Small 2003). Monsanto made some early key seed brand acquisitions, providing them with direct access to farmers (Fernandez-Cornejo 2004). Furthermore, they also aggressively licensed the GT trait to other seed suppliers.====To characterize the pattern of farmers’ seed choices and technology adoption, we estimate a random coefficients logit model of US soybean seed demand. The model characterizes the demand for seed brands and the GE trait, and allows for the presence of both state dependence and unobserved heterogeneity in farmers’ preferences for brand labels and for the GT technology trait. For estimation, we draw on a unique dataset containing more than 200,000 seed purchase decisions by roughly 28,000 US soybean farmers from 1996 to 2016. These (unbalanced panel) data provide information on seed purchase histories, seed characteristics, and prices. In developing and estimating the model, our main objectives are to: (====) identify the extent to which seed demand is affected by state dependence for brand labels and the GT trait; (====) characterize the importance of farmer heterogeneity for seed demand; (====) quantify farmers’ willingness to pay (WTP) distributions for different brands, the GT trait, and switching costs (state dependence); and, (====) assess the consequences of switching costs, through empirical counterfactual scenarios, for farmers’ welfare, GT trait adoption, and for the leading brands’ market shares.====Overall, we find that farmers are willing to pay large premiums for brand labels and for the GE trait, and these WTP estimates can vary widely across farmers. For example, from 2010 to 2016, farmers’ mean WTP for the GT trait was $21.8/acre, with 10% of farmers valuing GT at $40.2/acre or more, and another 10% of farmers valuing it at $3.5/acre or less. We also find significant evidence of structural state dependence, even after controlling for persistent unobserved farm-level heterogeneity. The mean marginal switching cost associated with brands is estimated at $13.1/acre; the corresponding estimates for trait switching costs are $6.8/acre (conventional to GT products) and $14.7/acre (GT to conventional products). We also uncover considerable heterogeneity across buyers for these WTP estimates, and for farmers’ valuations of the various brands.====Using the model estimates and a differentiated product model of the supply side, we evaluate counterfactual scenarios that shed additional light on the implications of inertia for the industry. Removing all switching costs would decrease seed prices by 6%, on average, and increase buyers’ welfare by $7.7/acre/year, on average, with most of this value arising from the removal of brand switching costs. At an aggregate level, farmers’ surplus would increase by $463.5 million per year, whereas firm profits would decrease by $225.9 million per year. Furthermore, without switching costs, the adoption of the GT trait would have been higher in the first few years, and lower thereafter, with the trend away from GT products more pronounced in recent years. The counterfactual simulations also provide evidence that inertia, in a context characterized by rapid innovation such as the US seed industry, can affect market shares in a substantial way.====The analysis and results of this article contribute to the literature in several ways. First, we provide evidence on the importance of switching costs for competitive firms’ choices over a key input. Most existing studies focus on the CPG industry and largely ignore the extent and implications of switching costs in a production context. Second, we contribute to the empirical literature on the implications of state dependence, especially when combined with the introduction of a major technological innovation. Finally, we provide new evidence that the sizeable WTP estimates for GE seed trait innovation are characterized by considerable heterogeneity across farmers.====The rest of this article is organized as follows. Section 2 provides background information on the US soybean seed market. Section 3 presents the data used in the econometric regression. Section 4 develops the demand model, discusses the identification strategy, and presents the estimation strategy. Section 5 presents the demand estimation results, and the computation of WTP distributions and demand elasticities. Section 6 uses counterfactual simulations to assess some empirical implications of state dependence and switching costs. Section 7 concludes.",Switching Costs in the US Seed Industry: Technology Adoption and Welfare Impacts,https://www.sciencedirect.com/science/article/pii/S0167718723000589,Available online 27 May 2023,2023,Research Article,2.0
Kasberger Bernhard,"Düsseldorf Institute for Competition Economics (DICE), Heinrich Heine University Düsseldorf, Universitätsstraße 1, Düsseldorf 40225, Germany","Received 9 August 2021, Revised 12 May 2023, Accepted 15 May 2023, Available online 21 May 2023, Version of Record 29 May 2023.",https://doi.org/10.1016/j.ijindorg.2023.102972,Cited by (0),"I study auctions in which firms bid for licenses that reduce their marginal costs in a post-auction downstream market. When there are three or more firms, I show that the Vickrey–Clarke–Groves (VCG) auction maximizes consumer surplus in dominant strategies if and only if it maximizes producer surplus in dominant strategies. With two firms, the effect on consumer surplus is ambiguous. When the VCG auction does not maximize consumer surplus, I show that consumer surplus can be maximized by adding caps, i.e., restricting the number of licenses a bidder can win. This might lower producer surplus.","Many multi-unit auctions allocate essential production inputs to firms. Importantly, the firms compete not only in the auction, but also in a downstream market after the auction. For example, many governments auction spectrum licenses to telecommunication companies. The companies require these licenses for the provision of their services, as only the electromagnetic spectrum can transmit mobile-phone calls and data. Winning more spectrum licenses in an auction allows a firm to transmit more data through its cell towers, which reduces the number of required towers and the cost of maintaining a certain level of capacity (Rey and Salant, 2017).====An important theoretical benchmark, but also an auction format used in practice, is the Vickrey–Clarke–Groves (VCG) auction.==== The VCG auction was designed to be efficient in the auction-theoretic sense of maximizing bidder welfare in dominant strategies. However, if the bidders’ values for the auctioned goods are the expected profits in a post-auction downstream market, then efficiency corresponds to maximizing downstream producer surplus. I study the impact of this “efficient” (i.e., industry-profit maximizing) auction on downstream consumer surplus.====I address this issue with a model in which the auction first allocates multiple marginal cost reducing licenses among a fixed number of firms. After the auction, the firms are in Cournot competition. Explicitly modeling the downstream market allows me to determine endogenously the bidders’ values and to study the auction’s impact on downstream consumers. Other papers on auctions with a downstream market have largely focused on the extensive margin: How does consumer surplus depend on the auction outcome when the license allocation determines the number or set of post-auction active firms?==== In contrast, I focus on the intensive margin by assuming that the cost functions are such that all bidders are active in equilibrium after any auction outcome.==== An important element of my model is the cost-reduction technology, which specifies how the licenses reduce marginal costs.====The first main result shows that the VCG auction maximizes both consumer and producer surplus in dominant strategies when the cost-reduction technology is linear. Thus, the VCG auction is not only “efficient” in the auction-theoretic sense of maximizing bidder welfare in dominant strategies, but also truly socially efficient in dominant strategies. It may be surprising that the auction outcome is optimal for consumers even though they do not bid. The optimality follows from the consumers being indifferent between the license allocations: The sum of marginal costs determines aggregate output in Cournot competition and, hence, consumer surplus (Bergstrom and Varian, 1985). When the cost-reduction technology is linear, then the sum of marginal costs is the same for all allocations in which all licenses are sold, which implies the consumers’ indifference between these allocations. I also prove the converse: A linear cost-reduction is necessary for the VCG auction to maximize consumer and producer surplus in dominant strategies.====Consumers prefer more equal license allocations when the cost-reduction technology is strictly convex. However, the VCG auction may lead to very unequal allocations and may even minimize consumer surplus. In this setting, I show how caps can improve outcomes in favor of consumers; caps limit how many licenses a bidder can win.==== Under certain conditions, when consumers’ downstream demand is sufficiently high, industry profits have a local maximum at the consumer-surplus maximizing allocation. The VCG auction implements this allocation when caps rule out a potential global maximum on the boundary. The result provides a formal justification for pairing an “efficient” format with caps: The auction locally maximizes industry profits, but the caps ensure that this local maximum is “close” to the consumer-optimal allocation.==== Importantly, caps do not need to be binding to be effective. However, when downstream demand is not high enough, caps may be binding and consumer surplus may increase with the caps’ restrictiveness.====In a related two-firm Hotelling model in which both firms are always active, Mayo and Sappington (2016) analyze an auction that allocates a marginal change in marginal costs to the firm with a more substantial increase in profits. They find that it is not always socially efficient to assign the cost reduction to this firm. In contrast, I analyze the effect of selling multiple licenses to several firms in downstream Cournot competition. Jehiel and Moldovanu (2000) focus on the externalities that arise in the sale of a single good when the seller may sometimes keep the object. Related allocative and identity-dependent externalities also play a role in my model, in particular when the cost-reduction technology is nonlinear. In this case, a dominant strategy exists only when there are two firms.====As mentioned, the majority of studies of auctions with an after-market focus on the extensive margin: The licenses are entry licenses that determine the set of active firms (see also footnote 2). Rey and Salant (2017) analyze the sale of divisible cost reductions to an incumbent and an entrant in post-auction homogeneous Bertrand competition. In the downstream market, only the firm with the post-auction lower marginal costs will be active in equilibrium, making the licenses essentially entry licenses. For consumers, it is optimal to reduce the costs of the (inactive) firm with the second-highest marginal cost, which might be challenging to implement in practice. Janssen and Karamychev (2010) show in a unit-demand setting that auctions do not necessarily select the firms with the lowest marginal cost. This is in contrast to the case in which there is a single monopoly license for sale (Demsetz, 1968, Laffont, Tirole, 1999). Eső et al. (2010) allocate capacity constraints to ex ante symmetric firms in capacity-constrained Cournot competition with complete information with a bidder-welfare maximizing auction. Note that exogenous capacity constraints also resemble entry licenses. Martimort and Pouyet (2020) study two upstream firms bidding in a second-price auction for additional capacity. While their focus is on how initial capacity impacts bidding, my main interest is how “efficient” multi-unit auctions impact downstream consumers.====The next section presents the model. Section 3 analyzes linear cost-reduction technologies. Section 4 analyzes strictly convex cost-reduction technologies and investigates the role of caps. In addition, I show how set-asides, i.e., non-competitively awarded licenses, may increase consumer surplus in an extension in which not all firms are active after all license allocations. Section 5 concludes. Appendix A provides the omitted proofs. Appendix B characterizes the differentiated Bertrand and Cournot markets for which consumers are indifferent between auction outcomes and for which a dominant strategy exists.",When can auctions maximize post-auction welfare?,https://www.sciencedirect.com/science/article/pii/S016771872300053X,21 May 2023,2023,Research Article,3.0
Goetz Daniel,"Rotman School of Management, University of Toronto Mississauga, Mississauga, ON L5L1C6, Canada","Received 17 June 2022, Revised 30 March 2023, Accepted 18 April 2023, Available online 15 May 2023, Version of Record 23 May 2023.",https://doi.org/10.1016/j.ijindorg.2023.102956,Cited by (0),"This paper examines how new telemedicine competitors affected incumbent health care providers during the first waves of COVID-19. Using data from the largest mental health provider search platform in Canada, I show that increased telemedicine competition in a market caused incumbent providers in that market to stop offering income-based discounts to patients. I isolate the causal effect of competition in a difference-in-differences framework, comparing providers before and after a supply shock on the platform that exogenously assigned some markets new telemedicine search results. I find that higher-quality providers are more likely to stop income-based discounts when facing new telemedicine entrants, while lower-quality providers are more likely to exit the platform, which is consistent with telemedicine providers competing for more price-sensitive patients. The results suggest that expanding telemedicine options had a heterogeneous effect on the ==== of care.","Since COVID-19, there has been a dramatic expansion in the prevalence of telemedicine—the provision of health care services virtually or by phone. For mental health care, the expansion has been especially sharp: only 20% of U.S. providers of talk therapy services offered a telemedicine option pre-COVID, a share which rose to 96% by August 2020.==== While there are many reasons why teletherapy offerings grew so much during the pandemic, a consequence of this growth is that patients’ choice sets for therapists have expanded. If a patient is willing to meet with a therapist virtually, then they now have access to vastly more providers than they did prior to COVID, a situation that is likely to persist into the future (Calkins, 2022).====In this paper, I isolate talk therapists’ response to an increase in telemedicine competition after the advent of COVID. When patients’ choice sets expand—as they have with the explosion of telemedicine offerings by providers—then one may reasonably expect competitive pressures to intensify. In other industries, such as retail with e-commerce or banks with mobile banking, online entrants have forced incumbents to compete on prices, adopt an online channel themselves, or otherwise change their strategy.==== Given rising healthcare costs and limited mental health care access, whether telemedicine competition can lower costs and improve access to mental health care providers is an important question.====My main empirical finding is that new competition from teletherapists caused incumbent therapists to change how they price discriminate, but did not change their base prices, and did not increase their adoption of the online channel. In particular, the entry of new teletherapists into a market caused incumbents to stop offering discounts to lower income patients. The results are consistent with telemedicine providers skimming price-sensitive patients, and echo results on competition in the pharmaceutical industry, where branded pharmaceuticals raise prices after cheaper generics enter (Frank, Salkever, 1997, Ching, 2010).====Isolating the causal effect of greater telemedicine competition on therapists after the advent of COVID presents several empirical challenges. First, market structure is endogenous to unobserved market-level heterogeneity, so regressing outcomes like price and technology adoption on market structure variables will typically lead to biased coefficients (e.g., Orhun (2013).) Second, patients can access online providers regardless of distance, so telemedicine providers typically enter many—or all—geographic markets simultaneously. This entry pattern implies that there are no control markets that are not exposed to new entrants, which makes it difficult to isolate the effects of entry from other time-varying shocks. Third, during COVID there were large demand shocks for talk therapy in particular: aggregate demand expanded as anxiety and depression increased, and both patients’ and therapists’ preferences shifted towards telemedicine and away from in-office visits. If the cross-market pattern of these and other COVID shocks are correlated with market entry choices of new teletherapists and decision-making of incumbents, then the estimated competition effect of teletherapist entry will be biased.====To address these challenges, I gather novel panel data from the largest therapist search platform in Canada. The platform brings together two groups. Potential patients can freely search for therapists within their local forward sortation area (FSA), a geographic area defined using the first three digits of the six-digit postal code. Meanwhile, private-practice therapists can pay to list themselves on the platform to solicit new clients. At monthly intervals from January through December 2020, I observe rich pricing, location, and telemedicine-offering data on roughly 12 000 therapists.====I identify the competitive effect of new telemedicine entry in a difference-in-differences analysis, where I leverage a unique supply shock on the platform to generate exogenous variation in telemedicine exposure across therapists and over time. Starting in June 2020, for markets (FSAs) with fewer than 20 therapist search results, the platform began padding the results to 20 by adding an explicitly labelled, new category of results: teletherapists from geographically distant markets. By making new teletherapists easier to find in markets with fewer than 20 results, the supply shock increased competition from teletherapists in those markets (Ghose, Yang, 2009, Agarwal, Hosanagar, Smith, 2011).====Treated therapists are those whose markets receive the new, exogenously added telemedicine search results on the platform. Therapist and month-level fixed effects, combined with exogenous cross-market variation in competition generated by the shock, straightforwardly deal with the identification issues caused by endogenous market structure and a lack of cross-market variation in online entrants. To address the fact that treated therapists are non-random and may therefore face different COVID shocks, I construct a propensity-score matched sample of therapists, taking care to match on variables that capture shocks to demand for therapy in general and shifting preferences for online therapy in particular.====I find that therapists exposed to the telemedicine supply shock (7 new telemedicine competitors on average) decrease their propensity to offer income discounts by 8.9% but do not change their base price. Telemedicine entry thus raises incumbent prices for the worst-off patients. Therapists facing new telemedicine competitors also do not adopt telemedicine themselves. This latter finding is intuitive, given that therapists who do not already offer a virtual option by June 2020 are a selected sample with high costs of adoption relative to the benefits. I find that the competition effects I identify are stable across a battery of robustness checks; in particular, when doing a regression discontinuity style analysis around the platform’s arbitrary padding threshold, and when allowing for completely flexible unobserved shocks at the city-month level.====Turning to heterogeneity analysis, I show that more intense telemedicine competition leads to greater exit from the platform, consistent with telemedicine entrants diverting profits from incumbents. Moreover, the effects of competition depend on therapist quality: telemedicine entrants cause the lowest quality therapists to exit the platform, while higher quality therapists remain on the platform but stop offering income discounts. This exercise suggests that telemedicine entrants compete for price-sensitive clients with lower quality therapists, while higher quality therapists can avoid competing with the new entrants by focusing on price-insensitive patients. I also show that a therapist’s price response to new telemedicine entry depends on when they adopted telemedicine themselves, with late adopters reducing income discounting by the most.====To understand why telemedicine competition leads incumbents to stop price discriminating, I write a simple applied theory model of therapist behaviour that expands on the price discrimination framework of Stole (2007). Under the assumption that telemedicine entrants compete for price-sensitive patients in particular, the model predicts that incumbents stop trying to serve those patients with income discounts. Therapists’ capacity constraints play a key role in this prediction: incumbents prefer the option value of an open slot to a potentially long-lived match with a price-sensitive patient who has become less profitable to serve after the competition shock.====The paper makes two novel contributions. Principally, this paper is the first to identify the causal effect of increased telemedicine competition on incumbents’ prices and technology adoption choices. The magnitude of the competition effect I estimate is specific to the unique environment of mental health care in 2020; however, the mechanism is not COVID-related, which suggests a more generalizable insight into how providers’ incentives change in response to increased telemedicine competition. Given the persistence of telemedicine availability and usage post-COVID, it is crucial to understand how this digital disruption to market structure affects incumbent providers’ behaviour. Second, policymakers are increasingly interested in expanding mental health care provision through telemedicine, as countries worldwide grapple with the costs of untreated mental illness (Lancet, 2020). My results suggest that increasing telemedicine options reduces access to local providers, which could harm patients who require in-person therapy.","Telemedicine competition, pricing, and technology adoption: Evidence from talk therapists",https://www.sciencedirect.com/science/article/pii/S0167718723000371,15 May 2023,2023,Research Article,4.0
"Chambolle Claire,Christin Clémence,Molina Hugo","Université Paris-Saclay, INRAE, AgroParisTech, Paris-Saclay Applied Economics, Palaiseau, 91120, France,CREST, École polytechnique, France,Normandie Université, IDEES-UMR CNRS 6266, UFR SEGGAT, Esplanade de la Paix, Caen 14000, France","Received 15 December 2022, Revised 19 April 2023, Accepted 1 May 2023, Available online 14 May 2023.",https://doi.org/10.1016/j.ijindorg.2023.102969,Cited by (0),"This article presents recent advances in the analysis of buyer-seller networks, with a particular focus on the role of buyer power on exclusion. We first examine simple vertical structures and highlight that either upstream or downstream firms may have incentives to engage in exclusionary practices to either counteract or leverage buyer power. We then review current work attempting to revisit this issue in “interlocking relationships”. Based on an ongoing research project, we show that the same exclusion mechanisms arise when retail substitution is soft.","In a large number of industries, manufacturers deal with retailers (or intermediaries) to access final consumers.==== Understanding how bilateral agreements are formed in such vertical structures is of great interest to policymakers as it determines product variety, prices, and ultimately welfare. In this context, the role of buyer power which refers to the ability of retailers to influence the formation of trading relationships and terms of trade with manufacturers has attracted considerable attention among scholars and antitrust practitioners these last decades.====This article reviews recent advances in the analysis of buyer-seller networks from simple to complex vertical market structures, with an emphasis on the potential exclusionary effects of buyer power. Most of the models that we introduce share the common assumption that the buyer-seller network is formed before negotiations take place. We distinguish two types of exclusionary practices: (i) exclusivity clauses whereby a firm requires to be the exclusive trading partner in the vertical relationship (single branding or exclusive dealing), and (ii) restriction of resources whereby a firm strategically restricts its (production or distribution) capacity which, in turn, limits the set of its trading partners. We highlight that most theories presented in this review concur on a unifying message along which exclusionary practices are used either to leverage or counteract buyer power, which leads to inefficient distribution networks. In particular, a restriction of resources is used by a firm with limited bargaining power to strengthen its bargaining leverage, while buyer power facilitates the emergence of exclusivity clauses (either imposed by a manufacturer or a retailer).====To capture the main essence of each theory in a simple and concise way, we develop numerical examples where, for any given set of trading relationships, the profits generated by retailers are taken as primitives of the analysis. While this allows us to abstract away from specifying a particular demand system, it is worth noting that most articles discussed in our review illustrate their results using demand systems in which consumers have a taste for variety, implying that exclusion always harms consumers.====For the sake of conciseness, we do not review the literature studying the effect of market structure changes (e.g., horizontal or vertical mergers) on buyer-seller networks.==== We also rule out incomplete information environments which are a source of inefficient contracting (double-marginalization) that often leads to exclusionary practices (e.g., Choné, Linnemer, 2016, Calzolari, Denicolò, Zanchettin, 2020, de Cornière, Taylor, 2021).====The remainder of this article is organized as follows. Section 2 examines the role of buyer power on exclusion in simple vertical structures, namely markets with a monopoly either upstream or downstream and competing vertical chains. Section 3 extends the analysis to vertical structures involving “interlocking relationships”, that is when manufacturers distribute their products through the same competing retailers.",Buyer power and exclusion: A progress report,https://www.sciencedirect.com/science/article/pii/S0167718723000504,Available online 14 May 2023,2023,Research Article,5.0
"Bernard Christophe,Mitraille Sébastien","TBS Business School, 20 BD Lascrosses - BP 7010, Toulouse Cedex 7, 31068, France","Received 10 June 2021, Revised 4 May 2023, Accepted 9 May 2023, Available online 13 May 2023, Version of Record 23 May 2023.",https://doi.org/10.1016/j.ijindorg.2023.102971,Cited by (0),"We explore how asymmetric information affects task assignment between a manufacturer and its supplier when tasks are horizontally differentiated, and when the comparative advantage in terms of marginal costs differs during the production process. We show that the manufacturer ==== to a generalist supplier and ==== to a specialist supplier depending on its level of efficiency. The presence of countervailing incentives drives these results. When the manufacturer’s internal costs are sufficiently low, it can externalize some of its best tasks and internalize its worst tasks. These two distortions simultaneously affect the contract offered to the generalist supplier.","In a 1989 ==== article entitled ‘Sell the Mailroom’, Peter F. Drucker notoriously advocated for the optimization of a company’s monopolized internal activities by turning to outsourcing: ====. More than three decades later, finding the right mix between internal and external activities remains a critical issue for companies such as Nike, Samsung, Airbus, and Toyota, and elicits various responses from a diverse range of sectors and industries such as the health sector, airline, mechanical engineering, construction, or the automotive industries, more generally.====Many strategic considerations govern the decision whether or not to outsource activities that would otherwise be performed internally, such as optimizing a firm’s costs, maintaining internal key competencies, avoiding under-investment in specific assets, managing supply chain risks or product complexity, fostering/softening market competition, or any combination of these. The financial returns from an outsourcing decision - no matter its strategic objectives - are strongly affected by a firm’s ability to accurately determine the set of activities that can be profitably externalized. The increasing importance of the optimization of business processes====, through lean manufacturing and/or business process management, allows an organization to more precisely identify its processes and subprocesses, and to carefully modify the set of tasks to keep in-house.==== Therefore, the successful selection of potential outsourcing partners can be achieved by more strategically choosing the set of tasks to be externalized.====In this paper, we study the decision of a manufacturer - who currently assigns different tasks of a production process to its internal division and to an external supplier -, in an agency model, in which the external supplier’s cost is unknown to the manufacturer. Tasks are horizontally differentiated on the unit segment (====), and all tasks must be completed for the product to have a value. The marginal costs of production for all agents are monotonic on the unit segment, but the manufacturer and its supplier’s specializations differ: the manufacturer has a lower marginal cost than its supplier to carry out the tasks located at one end of the unit segment, while the supplier has a marginal cost advantage to perform the tasks at the other end. The unit segment represents a ==== which can be fragmented in smaller segments, or ====. The supplier can be one of two unknown types: either a “specialist”, or a “generalist”. On the part of the unit segment (or subprocess) where both types are more efficient than the manufacturer, a specialist has a marginal cost of production lower than the generalist at one end, and higher at the other. On the contrary, the marginal cost of a generalist is more balanced at both ends of this segment. To optimize the externalization to these different types of supplier, the manufacturer offers a menu of contracts that specifies the set of tasks each type will realize, as well as the associated payments it will receive.====In such a setting, the incentive of a specialist or a generalist to misreport its type depends on its marginal cost advantage to perform the ==== of each set the manufacturer seeks to externalize in its menu of contracts. If this average task belongs to the subprocess where a type of supplier has a marginal cost advantage, this supplier could pretend it is the other type. The type of supplier a manufacturer prefers depends on how strong its own comparative advantage is: a ==== (respectively, ====) manufacturer has the lowest marginal cost on a narrow (respectively, wide) set of tasks and prefers to externalize to a generalist (respectively, to a specialist). To induce the supplier type it prefers to report it truthfully and to ensure that this type produces the first best set of tasks, we show that the manufacturer can introduce two kinds of distortions into the contract offered to the type it dislikes: either, (1) the manufacturer increases the proportion of tasks the type of supplier it prefers has no marginal cost advantage for, or (2) the manufacturer internalizes the tasks that provide the highest rents to the type of supplier it prefers.====The distortion imposed by (1) shifts the average task of this contract towards the tasks for which the type it prefers has a lower comparative advantage. Compared to the first best, it leads a high-cost (respectively, low-cost) manufacturer that seeks to obtain the production of the first best set of tasks from a generalist (respectively, a specialist) to ====-outsource (respectively, ====-outsource) to a specialist (respectively, a generalist). The distortion imposed by (2) leads the manufacturer to externalize the tasks it is the most proficient at performing, as well as to internalize the tasks that it does not perform well. Hence, if strategy (2) is implemented, the two possible mismatches between tasks and marginal costs of production coexist for the same type. We conclude by studying how the manufacturer’s expected total cost at the second best changes with its internal cost: incomplete information amplifies the effect of upward internal cost shocks a high-cost manufacturer might suffer, while it reduces the benefits of internal cost reductions a low-cost manufacturer could achieve.====The over- and under-outsourcing distortions that we characterize are determined by the differences between the marginal costs of production of the two types of supplier in the production process. These differences introduce countervailing incentives in our model. For any given contract offered by the manufacturer that covers the cost of production of one type of supplier, the second type of supplier benefits from completing the tasks in this contract in which it specializes. Whether or not this contract is more profitable for this second type depends on the marginal cost of production of the average task in this contract. When both first best contracts are such that one type is the most efficient supplier, the average task in the contract offered to the other type must be shifted towards tasks that the first type does not benefit from. In other words, the manufacturer reduces the information rent of the specialist by increasing the average task in the contract offered to the generalist. Analogously, it reduces the information rent of the generalist by decreasing the average task in the contract offered to the specialist. By doing so, the manufacturer prevents the specialist (respectively, generalist) that realizes the first best from overstating (respectively, understating) its type. The source of countervailing incentives therefore differs from previous works, where they result from type-dependent participation constraints,==== and hence novel distortions appear.====Inefficient task assignment due to asymmetric information has been extensively studied in multitask principal agent models, especially in personnel economics.==== In particular, Prasad (2009) extends the work of MacDonald and Marx (2001) to examine job assignments to specialists or generalists, whose efforts increase the likelihood of success in (at most) two complementary tasks. The benefit of the principal increases with the number of tasks successfully completed. Screening contracts must penalize success in one single task when the contract chosen includes two tasks, in order that specialists do not choose the contract designed for generalists. Agastya and Birulin (2023) consider a project whose value is much higher if all its complementary tasks are successfully completed. The optimal scheduling of tasks specifies a precise order by which specialists - with unknown specific skills and whose efforts are unobservable - must act. Our article differs from these earlier studies in that the manufacturer can assign any set out of a continuum of tasks to its supplier without moral hazard. This also allows us to more precisely determine the scope and nature of the tasks that must be outsourced, as well as the distortions it requires.====Another part of the literature analyses how employers strategically use their informational advantage in relation to their workers’ abilities, leading to inefficient job or task assignments by employers, insufficient mobility, and inadequate matching of workers in the labour market.==== Amongst others, Bidwell (2011) finds that external hires are paid more and perform worse than workers promoted internally, while Ferreira and Nikolowa (2019) show that unobserved differences of productivity can lead rational firms to poach less productive but experienced managers for new jobs they offer, rather than inexperienced managers whose productivity is perceived as more uncertain. Bar-Isaac and Leaver (2021) show that this negative effect of asymmetric information on matching disappears when firms disclose information on the general skills of workers who are a bad fit for them. This disclosure induces an increase in the provision of general training, as well as better matching in the labour market. In our paper, the negative effects of asymmetric information on the matching between tasks and marginal costs appear - for rent extraction reasons - in a model with countervailing incentives, without a strategic use of information by the manufacturer.====Our study of the boundaries of a business process relates to the classical studies of the boundaries of the firm in which the roles of transaction costs, property-rights on assets, access to critical resources, and agency costs are highlighted.==== Fixing its boundaries is of concern to a firm’s general strategy. Our analysis of the limit of its internal processes contributes to the definition of a firm’s operations strategy, which is one of the variations of a firm’s general strategy. Therefore, we provide an alternative explanation to the classical ones: we identify the boundary to set between internal and external operations in order for truthful revelation to occur, and so that the total cost of the organization is minimized. Recent empirical findings, such as David et al. (2013) in the context of patient care, show evidence that some tasks that are not asset specific may be performed by different organizations (hospital, home health services, etc.) at different costs, all of which are allocated sub-optimally when these organizations are not vertically integrated. Parmigiani (2007) describes a similar phenomenon in the steel industry. Our study of the sensitivity of the organization’s expected total cost with regard to the manufacturer’s internal cost also relates to the literature on the trade-off between mark-up reduction and incentives to reduce costs in make-and-buy models (see, for example, Sappington, Riordan, 1987, Loertscher, Riordan, 2018). In our paper, the benefit of reducing costs can be mitigated by the increase in outsourcing distortions.====Finally, the assignment of a task to an external resource is also studied in different incomplete contracts settings, where the type of input (intellectual or capital), the bargaining power, and the cash constraints Aghion and Tirole, 1994) or market competition concerns Jungbauer et al. (2021); Galdon-Sanchez et al. (2015) and Gil and Ruzzier (2018) drive the outsourcing decision. None of these forces are present in our model, where an excessive externalization or internalization is, instead, driven by adverse selection and the heterogeneity of a manufacturer and its supplier’s comparative advantage, combined with the horizontal differentiation of tasks.====The remainder of our paper is organized as follows. Section 2 presents the model, and Section 3 presents the equilibrium contract. Section 4.1 studies over- and under-outsourcing, while the internalization of the manufacturer’s most costly tasks is analyzed in Section 4.2. Section 5 studies the pass-through of shocks on the manufacturer’s internal cost into its total cost, and Section 6 concludes.",Outsourcing horizontally differentiated tasks under asymmetric information,https://www.sciencedirect.com/science/article/pii/S0167718723000528,13 May 2023,2023,Research Article,6.0
Weichselbaumer Michael,"Vienna University of Economics and Business, Department of Economics, Welthandelsplatz 1, Vienna A-1020, Austria","Received 8 April 2022, Revised 30 April 2023, Accepted 2 May 2023, Available online 13 May 2023, Version of Record 27 May 2023.",https://doi.org/10.1016/j.ijindorg.2023.102970,Cited by (0),"With novel data on ex-post quality after used-car purchases, I evaluate left-digit bias, a type of inattention bias, for underlying quality of used cars. My main result is that the used-car market exhibits a quality discontinuity in parallel to a price discontinuity at 10,000 multiples of mileage. I discuss explanations for how quality adjusts to the price discontinuity. The explanations differentiate between exogenous and endogenous quality, and by the extent of the ","Inattention bias represents a group of behavioral biases that has been documented in several markets. One variant is the overemphasis of leading digits, as in the mileage of a used car. In the extreme case, buyers would value two otherwise equal used cars significantly different when one car displays 9999 and the other 10,000 miles. Such price discontinuities at multiples of 10,000 miles have been found in Lacetera et al. (2012). Similar discontinuities exist for months when the year of registration of a used car increases (Englmaier et al., 2018). Repetto and Solís (2020) find a discontinuous price change for apartments; when the initial asking price has a leading digit that is close to increase, the final selling price is significantly higher, conditional on observables.====Used cars, apartments, and many other goods and services have quality about which buyers and sellers have different sets of information. Unobserved quality in a market with a behavioral (price) anomaly raises two fundamental questions. First, does quality exhibit a similar pattern? Second, what motivates a quality discontinuity? I address these two questions.====I draw on a large-scale consumer study on the quality that buyers experienced after a used car purchase. Buyers reported the number of problems for the year after the purchase. Regarding the first research question, I find a discontinuous increase in the number of problems (quality drop) when mileage switches to the next 10,000s. Because many respondents rounded kilometer values to a 10,000 or 5000 multiple, I also evaluate a subsample excluding major rounding. In this undiluted sample, the number of problems increases by 0.64 problems, from an average number of problems of 1.29, which is an increase of 50 percent. The price discontinuity, which is well-documented in the literature, is also present in the consumer survey data. Prices drop on average by 1.5 percent when the mileage crosses a 10,000 multiple. The quality discontinuity in parallel to the price discontinuity interferes with the interpretation of the price-mileage-discontinuity as a bias that introduces inefficiency in decision making.====To address the second question, I discuss possible explanations why I expect a quality discontinuity as a consequence of the price discontinuity. The explanations are based on some basic motives of sellers and buyers. With “smooth” quality, a seller of a car at or above the threshold experiences a sharp decline in the value of the car. The seller can decide not to sell such a car, which is a typical case of adverse selection. The seller can sell the car at a later point in time when price and quality are more in line, or simply anticipate the price drop and sell before the threshold in the first place. Finally, the seller may have the possibility to influence the quality of the car, by adjusting the timing and extent of maintenance and repair.====Buyers have an incentive to buy cars that are underpriced relative to quality. A buyer can be in a better position to become informed about car quality than an econometrician. Remedies have emerged to deal with information asymmetry. They are called quality assurance mechanisms (Dranove and Jin, 2010) or counteracting institutions (Akerlof, 1970), for example certification, knowledge, or trust. While the remedies in markets with unobserved quality may be imperfect, their existence may well be based on being useful by increasing information. Buyers that are able to become informed about quality can reject cars that are overpriced, or be satisfied to accept a car with lower price also having lower quality. With quality assurance mechanisms, buyers can judge quality and reveal if sellers have taken measures to offer a car where a lower price, e.g. at a 10,000 switch, is compensated with lower quality.====Due to data limitations, I cannot clearly disentangle the relevance of the different motives. I provide evidence on the effectiveness of quality assurance mechanisms. Buyers that have more car knowledge than the median end up with cars of higher quality. Also, buyers’ partly reliance on trust increases car quality. I interpret this as evidence that buyers actively enforce behavior of sellers that leads to quality adjustment — adverse selection, timing of the sale, and quality investment by sellers.====The results contribute to the literature that studies behavioral bias in decision making (see the overview in DellaVigna (2009) and Gabaix (2019), the latter suggesting that many behavioral biases may be attributed to inattention). Inattention bias is not the only type of behavioral bias where unobserved heterogeneity plays a role. Genesove and Mayer (2001) find evidence for loss aversion in the housing market — a market also characterized by unobserved quality. The case of used cars shows that unobserved quality can exhibit the same discontinuity as prices. Quality counteracts the price anomaly, such that the cost of the bias for buyers and sellers may even disappear. An important complement to the literature on behavioral biases is the question if and how consumers deal with such biases actively. My results are tightly connected to the debate on the meaning and implications of behavioral biases, relating to the behavior of experts on markets (List, 2003) and approaches of rational inattention (Basu, 1997).====The results on quality assurance mechanisms are an additional contribution to the literature on asymmetric information markets, and market functioning overall. My results provide novel microeconomic evidence on partly neglected market mechanisms. Relying on trust to reduce asymmetric information problems is evidence of the positive role of trust in market transactions. Guiso et al. (2008) find a lower level of investment on the stock market for people with lower trust, which is in line with the positive effect of trust on income found in Butler et al. (2016). Used car purchases constitute a concrete transaction where trust is advantageous, because it is effective in dealing with an asymmetric information problem. Trust increases car quality and can offset the need to eliminate bias or increase attention.====A limitation is that the data rely on self-reported information. Two major concerns about misreporting are recall bias and a heterogeneity bias, the latter arising from the specific way the sample was collected. Reporting heterogeneity could arise because there are respondents that only provide rounded values. Additional analysis suggests that these two concerns are not responsible for my results. It is impossible to rebut all concerns conceivable with self-reported data. The strength of the empirical analysis is that it is generally hard to come up with post-purchase quality data and that both the EC and the experienced research firm may be considered reliable sources. The mechanisms in support of the findings rely on standard economic arguments and basic motives.",Unobserved heterogeneity and adjustment to behavioral bias: The case of used cars,https://www.sciencedirect.com/science/article/pii/S0167718723000516,13 May 2023,2023,Research Article,7.0
"Cullen Joseph A.,Reynolds Stanley S.","Uber Technologies, Inc., USA,Department of Economics, Eller College of Management, University of Arizona, Tucson, AZ, USA","Received 9 June 2022, Revised 26 January 2023, Accepted 6 April 2023, Available online 4 May 2023, Version of Record 13 May 2023.",https://doi.org/10.1016/j.ijindorg.2023.102954,Cited by (0),"A transition to a low carbon future will include a medium-to-long run period in which intermittent renewables co-exist with conventional fossil fuel electricity generators. Fossil fuel generators have frequent startups and shut-downs during the transition. A dynamic competition model is developed that allows for costly cycling of conventional generators. We analyze long run effects of renewable subsidies and carbon prices in the Electric Reliability Council of Texas system using the dynamic model. Accounting for costly generator cycling leads to large changes in equilibrium outcomes and changes policy predictions. The dynamic model predicts higher subsidies or carbon taxes are required to achieve CO2 reduction targets compared to a static model without costly generator cycling. The dynamic model predicts the cost of CO2 reduction is 40 - 80% greater than the static model prediction. The dynamic model predicts a much larger gap between CO2 reduction costs for carbon taxes and renewable subsidies; $303 million/year, compared to a static model prediction of $209 million/year.","Production of electricity and heat is the largest source of greenhouse gas (GhG) emissions world-wide; see IPCC (2014). Many countries and states around the world have implemented incentives for expansion of renewable energy generation from sources such as wind and solar, as a way of mitigating GhG emissions. Such policies, coupled with falling prices for renewable technologies, have spurred rapid growth in renewable electricity generation. Electricity markets are highly regulated and will likely experience increased government involvement to reshape the grid towards a cleaner production portfolio. In this paper we analyze the medium-to-long run economic effects of renewable energy incentives in a market in which solar and on-shore wind are important renewable energy sources. Understanding the economic effects of these incentives is critical for evaluation of policies that promote a transition to a low carbon electricity system. The economic effects we consider include impacts on energy prices, welfare costs of emissions reductions, and investment in electricity generation capacity.====An important aspect of this research is the role played by startup (or, cycling) costs for conventional fossil-fuel electricity generators. These generators are likely to remain as a significant part of the generation portfolio for several decades during a transition to a low or no-carbon future. As penetration of intermittent renewables increases, conventional generators must startup and shut-down more frequently to offset fluctuations in renewable generation; see Troy et al. (2010) and Perez-Arriaga and Batlle (2012). Greater penetration of renewables and more frequent cycling of conventional generators may lead to significant changes in the volatility of wholesale energy prices, in profits of conventional generators, and ultimately to changes in investment incentives for various types of conventional generators. We seek to understand how the economic forces unleashed by a large scale renewable energy expansion play out in long run equilibrium.====We address our research question by first developing and analyzing a formal dynamic competitive equilibrium model that incorporates both short-run operating dynamics associated with electricity generator startup costs and long-run generator investment decisions. We apply our analysis to the electricity system of the Electric Reliability Council of Texas (ERCOT). Data from ERCOT and other sources are used to parameterize our theoretical model. We utilize counterfactual simulations to examine the impact of two policy options: a renewable investment subsidy and a carbon tax. We compare outcomes under these policies for several CO2 emissions reduction targets. The counterfactual simulations permit us to identify the role that startup costs play in the response of investments, energy prices, and welfare cost of emissions reduction to renewable incentives.====What does our economic analysis contribute? While there is a large literature that examines the impact of renewable energy incentives, our analysis is unique in employing a dynamic market equilibrium framework to address the research question. This framework allows us to capture short-run operating dynamics and long-run investment incentives. Inclusion of conventional generator startup costs will alter the distribution of wholesale electricity prices, for example by raising prices during peak and/or shoulder hours and lowering prices during off-peak hours. These effects on prices change long run generator investment incentives and affect the mix of generators in the portfolio and the welfare cost of emissions reduction. The approach developed here may be used to analyze the impact of other renewable policies, such as feed-in tariffs and clean energy standards. Moreover, an economic model such as the one we develop here is useful for assessing the value of complementary policies, such as demand-response policies and energy storage incentives.====Our dynamic competition model allows for aggregate demand shocks, one-time capacity investment decisions from a menu of technologies, repeated generator startup/shut-down decisions, and production decisions for active firms. Firms incur a lump sum cost upon each generator startup, which introduces dynamic linkages across periods. Our model formulation presents theoretical and computational challenges, due to production non-convexities and the absence of an easily derived steady state equilibrium. We show that a dynamic competitive equilibrium is the solution of a social planner’s problem. We characterize properties of competitive equilibrium and use the planner’s problem as a computational platform for our application to electricity market analysis.====In the theoretical portion of the paper we prove equivalence between dynamic perfectly competitive allocations and the solution to a particular stochastic dynamic programming (DP) problem for a planner via a ‘small firms’ assumption. This assumption allows us to side-step equilibrium existence difficulties posed by production non-convexities, while retaining key economic implications of non-convexities. We use the equivalence result to show existence of a competitive equilibrium. We allow heterogeneity in types to emerge endogenously in equilibrium via firms’ initial capacity investment decisions. All firms with a given type of technology are symmetric, but the number (or, measure) of firms of each type is endogenous. We further characterize properties of equilibrium allocations analytically using a special case of the model. Here we show how demand variability coupled with the magnitude of startup costs influence the extent of heterogeneity. Specifically, more frequent demand shifts and/or higher startup costs for baseload technology make it more likely that a higher cost, but more flexible, technology will be invested in.====The electricity system of the Electric Reliability Council of Texas (ERCOT) serves as a test bed for our analysis. ERCOT operates a self-contained grid that serves most of the state of Texas. Strong wind resources and favorable public policies have resulted in Texas having the highest wind turbine capacity of all U.S. states; LaRiviere and Lu (2022). Texas has also experienced rapid growth in solar photovoltaic (PV) capacity in the last decade; Potomac-Economics (2020). We apply our theoretical model to analyze wholesale electricity market competition in ERCOT, focusing on the long-run impact of policies that promote investment in renewable energy. We show that incorporating startup costs for conventional natural gas fired generators yields more volatile electricity prices and a different mix of generator investments, compared to predictions from a conventional ‘grid-stack’ model of generator operations; we refer to this type of model as a static model.==== These differences are magnified as renewable penetration increases. The presence of startup costs in the model changes equilibrium wholesale electricity prices in ways that reduce the profitability of renewable generators, implying that higher subsidies or higher carbon prices would be required to achieve CO2 reduction targets. We find that investment subsidies required to achieve a 60% CO2 reduction target are 31% higher for our dynamic model with startup costs than for the static model. A similar difference between dynamic and static model results arises for a carbon pricing policy. The presence of startup costs in the model has significant effects on equilibrium outcomes even though total startup costs represent a modest percentage of firms’ overall costs.====The effectiveness of policies may be measured and compared by finding the reduction of total (consumer plus producer) surplus per ton of CO2 emissions reduction. We find that a carbon pricing policy is more effective than a renewable investment subsidy policy. For a 60% CO2 reduction target the average cost of CO2 emissions reduction is 27% higher under a renewable subsidy policy than under a carbon pricing policy. This yields a $303 million per year cost difference between the policies for the ERCOT system. While there is a substantial cost difference between these two policies, some other studies find larger differences in policy effectiveness. For example, using ERCOT data from 2008, (Fell and Linn, 2013) find that a renewable production subsidy or a renewable portfolio standard would be 4 - 6 times more costly than a carbon pricing policy. Our finding is likely driven by the fact that our long run equilibrium has no investment in coal-fired generators; thus, carbon pricing does not serve to induce coal-to-gas switching as it does in other studies. We also find that dynamic frictions affect estimates of policy effectiveness. Our estimates of welfare cost per ton of CO2 emissions reductions for high CO2 reduction are 40% higher in a dynamic model with startup costs than in a static model, for both types of renewable policies.====Our analysis builds on the dynamic competition literature; see Lucas and Prescott (1971), Jovanovic (1982), Rob (1991), and Hopenhayn (1990). As in these papers, firms in our model are price-takers who face stochastically varying market conditions and make forward-looking decisions based on rational expectations of future outcomes. Our model departs from these papers in two economically important ways, however. First, we specify a fixed, lump sum cost each time a firm starts a generator for production. This formulation is in contrast to continuity assumptions for firms’ state transitions used in other papers.==== Our assumption of fixed, lump sum startup costs permits us to parameterize the model in the application without resorting to specifying distributions for startup costs. We prove existence of a dynamic competitive equilibrium for our model, in the absence of continuity for firms’ state transitions. Second, many dynamic competition papers permit firms to make one-time entry and exit decisions. Our model of startups and shut-downs may be interpreted as a model in which individual firms are persistent, and repeatedly enter and exit the market. This is important in settings where firms are long lived and can wait for optimal startup (entry) and shut-down (exit) times. Unlike prior work, firms are ex-ante heterogeneous when startup (entry) and shut-down (exit) decisions are made. Combined with persistence, this permits us to model situations where differences between types are fixed and permanent once established, but at the same time allows the distribution of types which arises via equilibrium investments to be endogenous.====Several prior studies assess the long run impact of large scale renewable energy penetration. A common approach is to use a perfect competition model with a structure similar to our model; a short-run component - capturing market clearing prices and operation of generators under conditions of fluctuating demand - as well as a long-run component - capturing profit maximizing investment in generation capacity. Bushnell (2010) uses this type of model to examine the impact of increasing wind penetration on fossil fuel generation investment and wholesale prices in U.S. Western Electricity Coordinating Council regions. In a similar vein, Green and Vasilakos (2011) analyze the impact of an increase in wind capacity on the mix of generating capacity, wholesale prices, and emissions in Great Britain. Fell and Linn (2013) analyze the impact of several different types of renewable support policies on long run wholesale market outcomes, and apply the analysis to ERCOT. Blanford et al. (2014) analyze the effects of a clean energy standard on generation investment in the U.S., using the US-REGEN model. Gowrisankaran et al. (2016) analyze the effects of a renewable energy mandate for electricity generation in Arizona. They focus on how increased solar photovoltaic penetration affects operating reserves, investment in conventional generators, and emissions. However, none of these papers incorporate dynamic generator operating constraints into a model with optimizing agents, as we do in this paper.==== The static competition model that we report on and use for comparison purposes in the application embodies the main features of the models used in the electricity studies cited above.==== Our comparisons of static and dynamic model results for the ERCOT application demonstrate key differences between our results and the kind of results that emerge from prior long run studies of electricity market investment.====We note two limitations of our analysis of electricity markets. First, we invoke a small-firms assumption to facilitate our dynamic competition analysis. This implies that firms are price-takers, so that supplier market power is not considered in our analysis. There is a large literature that analyzes the extent and effects of market power in restructured wholesale electricity markets; see for example Wolfram (1999), Borenstein et al. (2002), Hortacsu and Puller (2008). The small-firms assumption smooths out aggregate production non-convexities, which allows us to characterize a competitive equilibrium with linear energy prices. If firms are not small then non-convexities may require modifications to the equilibrium formulation. For instance, O’Neill et al. (2005) show that efficient market-clearing prices may be obtained by complementing energy prices with additional prices for generator startups.==== Second, we do not consider electricity transmission costs or constraints in our analysis, and instead analyze a single pooled wholesale electricity market. Transmission constraints can limit energy flows between nodes at some hours and can also be a factor in siting decisions for renewable energy generation facilities. In some regions the most productive renewable energy resources are far from demand centers, so that new or expanded transmission lines are required to increase renewable energy penetration. We discuss how these limitations may bias our policy results at the end of Section 5.====Finally, we contribute to the emerging literature on the effects of supply frictions in electricity markets. Cho and Meyn (2011) analyze a dynamic model of a wholesale electricity market and show that generator ramping constraints may lead to significant wholesale price volatility and sustained periods of prices that deviate from marginal generation cost. Reguant (2014) and Cullen (2015) estimate dynamic structural models of electric generator operations that include startup decisions and startup costs. Both papers find economically significant estimated generator startup costs. Cullen (2015) uses estimated generation and startup costs to simulate a dynamic competitive equilibrium, which is used for counterfactual policy analysis. The counterfactual analysis of Cullen (2015) applies to short-run competitive equilibrium with fixed generation capacity, in contrast to the long-run investment analysis of the present paper. Reguant (2014) shows that accounting for generator startup costs yields smaller estimates of price-cost markups during peak demand periods.====The rest of the paper proceeds as follows. In Section 2 we characterize the model of competition. In Section 3 we develop the connection between market equilibrium and the solution to the planner’s problem. In Section 4 we describe the data we use, explain how we set parameter values, and describe the computation approach. In Section 5 we report and discuss results. The conclusion is in Section 6.",Market dynamics and investment in the electricity sector,https://www.sciencedirect.com/science/article/pii/S0167718723000358,4 May 2023,2023,Research Article,8.0
"Do Jihwan,Miklós-Thal Jeanine","School of Economics, Yonsei University, South Korea,Simon Business School, University of Rochester, United States","Received 14 December 2022, Revised 21 March 2023, Accepted 13 April 2023, Available online 24 April 2023.",https://doi.org/10.1016/j.ijindorg.2023.102955,Cited by (0),"This paper introduces a notion of partial secrecy in bilateral contracting games between one upstream firm and several competing downstream firms. The supplier’s offer quantities are subject to trembles, and each downstream firm observes a noisy signal about the offer received by its competitor before deciding whether to accept its offer. A downstream firm’s belief about its competitor’s quantity is determined endogenously as a weighted average of the competitor’s expected equilibrium quantity and the signal about the actual quantity that the competitor was offered. The degree of contract secrecy is captured by the weight that this belief puts on the competitor’s expected equilibrium quantity. We find that a higher degree of secrecy implies a more competitive equilibrium outcome, both in a game with simultaneous offers and in a dynamic game with alternating offers similar to the one in Do and Miklós-Thal (2022, “Opportunism in Vertical Contracting: A Dynamic Perspective,” CEPR Discussion Paper No. DP16951).","Models of bilateral contracting between an upstream monopolist and multiple competing downstream firms are widely used in the industrial organization literature, and their analyses have generated important insights about the effects of vertical restraints and vertical mergers. A well-known result of this literature is that the equilibrium outcomes of games in which the upstream supplier makes simultaneous contract offers to the downstream firms depend on whether offers are public (i.e., observed by all downstream firms) or secret (i.e., observed only by the firm receiving an offer). When offers are public, the supplier exerts its full market power in equilibrium. When offers are secret, however, the supplier faces a temptation to behave opportunistically by offering contracts that raise the bilateral surplus with one downstream firm at the expense of other downstream firms. Equilibrium outcomes then depend on a downstream firm’s beliefs about other downstream firms’ offers after obtaining an out-of-equilibrium offer; under the commonly used passive-beliefs assumption, the supplier is unable to fully exert its market power. Vertical mergers and vertical restraints like exclusive dealing can re-establish the supplier’s market power in that case, to the detriment of consumers and social welfare.====The extant literature treats contract secrecy as binary. Contract offers are either public or secret. In the former case, all downstream firms observe all offers without any noise. In the latter case, a downstream firm receives no information at all about the actual offers received by its competitors. In practice, however, neither of these two extremes, public or secret contract offers, is likely to be a good description of the informational environment faced by firms. Public offers lack realism because negotiations of bilateral contracts typically involve private communication between the contracting parties, and suppliers, even if they wanted to, cannot credibly commit to making all offers publicly observable. Secret offers, on the other hand, ignore that information leakages across downstream firms can occur even if contracts offers are made privately. Moreover, in dynamic settings, accepted supply contracts are unlikely to remain fully secret as time passes, because downstream firms can draw inferences about their competitors’ current supply terms from observed market behavior and market outcomes.====This paper takes a first step towards modeling ==== in vertical contracting games and analyzing its implications for equilibrium outcomes.==== Our analysis builds on two new modeling elements. First, the supplier’s offer quantities are subject to mistakes or trembles. Such mistakes could arise due to coordination failures or communication frictions within the supplier’s organization, for instance, or due to human error. Second, each downstream firm obtains a noisy signal about its competitor’s quantity before deciding whether to accept its offer. Given these two elements, a downstream firm’s posterior mean belief about its competitor’s quantity, after observing the signal, becomes a weighted average of the competitor’s expected equilibrium quantity and the signal about the actual quantity that the competitor was offered. The degree of contract secrecy is captured by the weight that this belief puts on the competitor’s expected equilibrium quantity, which depends on the ratio of the variance of the signal over the variance of the trembles in the offer quantities. As the signal about the actual quantity becomes more precise (the signal variance falls), the degree of secrecy falls and the belief moves closer to the true quantity that the competitor was offered. As the signal becomes less precise, the degree of secrecy rises and the belief moves closer to the competing firm’s expected equilibrium quantity.====Since all quantities are offered with positive—albeit possibly very small—probability in equilibrium, our analysis and results do not rely on any assumptions about beliefs following out-of-equilibrium offer quantities.==== Passive beliefs, which are widely used in the literature but have been lacking a theoretical foundation, arise endogenously as the limiting case in which the ratio of the variance of the trembles over the variance of the signals goes to zero.====We analyze the implications of partial secrecy in two vertical contracting games: first, a game with simultaneous offers, extending the standard modeling approach in the literature; and second, a dynamic game with alternating offers, following our modeling approach in Do and Miklós-Thal (2022). In the dynamic game, the supplier alternates between making offers to two competing downstream firms over an infinite horizon, and the analysis focuses on Markov perfect equilibria. While Do and Miklós-Thal (2022) assume that each downstream firm is aware of its competitor’s current contracted quantity when deciding whether to accept an offer, either because contract offers are public or because the firm can perfectly infer its competitor’s current quantity from observed market outcomes in the time that elapses between offers, in the present paper we assume instead that contract offers are secret and that each downstream firm’s inferences about its competitor’s current quantity are imperfect.====In both the simultaneous-offers game and the alternating-offers game, we find that a greater degree of secrecy leads to a more competitive outcome with a higher expected total quantity sold in equilibrium. In other words, a greater degree of secrecy implies a greater degree of equilibrium opportunism. Intuitively, this is because a change in the quantity offered to one downstream firm has a lesser effect on the competing downstream firm’s posterior belief when the degree of secrecy is higher, which implies that the supplier’s equilibrium offers internalize less of the competitive externalities across downstream firms.====The equilibrium degree of opportunism varies continuously with the degree of secrecy in the two games analyzed in this paper. In the simultaneous-offers game, the expected equilibrium outcomes span the entire range between the integrated monopoly outcome, corresponding to the equilibrium under simultaneous public offers, and the competitive (pairwise-proof) outcome, corresponding to the equilibrium under secret offers and passive beliefs. In the alternating-offers game, the expected steady-state equilibrium outcome is more competitive than the integrated monopoly outcome even in the limit case corresponding to public offers, and it approaches the competitive (pairwise-proof) outcome for any discount factor as the degree of contract secrecy rises. Moreover, the combination of secrecy and dynamic recontracting implies stronger opportunism than secrecy alone. For any given degree of secrecy, the equilibrium outcome is more competitive at the steady state of the dynamic model with recontracting than in the static model with simultaneous offers. The findings thus complement the results in Do and Miklós-Thal (2022), where we analyze how the equilibrium degree of opportunism varies with the firms’ discount factor and the speeds at which the supplier’s contract with one retailer reacts to changes in the other retailer’s contract.====Understanding the equilibrium degree of opportunism is useful for vertical merger policy and for public policy on vertical restraints. This is because when opportunism is greater, then the competitive damage arising from strategies like vertical mergers or vertical restraints that lead to the monopoly outcome will be worse, and the supplier’s incentives to use such strategies will be stronger. Our findings suggest that a greater degree of secrecy raises both the supplier’s incentive to employ strategies aimed at dampening competition and the harm to competition from allowing the supplier to do so. This is true both with and without the possibility of future recontracting.====The rest of this paper will be organized as follows. Section 2 introduces partial secrecy into an otherwise standard model of simultaneous contract offers. Section 3 analyzes the implications of partial secrecy in an infinite-horizon dynamic model with alternating offers. Section 4 concludes. All formal proofs are relegated to Appendix A.",Partial secrecy in vertical contracting,https://www.sciencedirect.com/science/article/pii/S016771872300036X,Available online 24 April 2023,2023,Research Article,9.0
"Brown David P.,Eckert Andrew,Shaffer Blake","Department of Economics, University of Alberta, Edmonton, Alberta, Canada,Department of Economics and School of Public Policy, University of Calgary, Calgary, Alberta, Canada","Received 8 June 2022, Revised 28 January 2023, Accepted 6 April 2023, Available online 13 April 2023, Version of Record 9 May 2023.",https://doi.org/10.1016/j.ijindorg.2023.102953,Cited by (0),"Asset divestitures play a central role in antitrust and competition policy. Despite their importance, empirical evidence on their impacts on market competition is limited. We analyze market power in Alberta’s wholesale electricity market, where transitional arrangements that virtually divested generation assets from large incumbents were put in place during market restructuring in the early 2000’s and expired at the end of 2020. Subsequently, average peak hour prices rose by 120% the year after their expiry. We demonstrate that nearly two-thirds of this increase can be explained by elevated market power from the large suppliers. Further, exploiting variation in the allocation of the divested assets across heterogeneous firms, we demonstrate that market power execution is elevated when the divested assets are controlled by large strategic firms. Our findings highlight the important role that asset divestitures and their allocations can have on market competition. Our analysis also raises concerns over the ability of restructured electricity markets to facilitate sufficient competition through entry and the potential need for regulatory intervention.","Competition and antitrust authorities can have an important impact on shaping the structure of markets. Prominent examples include structural remedies that require firms to divest assets in proposed merger and acquisition cases in Europe and the United States.==== Mandated divestitures also played a critical role during the initial phases of market restructuring in the electricity sector. In particular, the introduction of competition involved transitional arrangements to ensure the heavily concentrated markets did not lead to excessive market power or impede the development of competition. These policies often involved breaking up firms’ vertical arrangements (i.e., generation from transmission and distribution) and forced generation asset divestitures to reduce wholesale market concentration (Borenstein and Bushnell, 2015).====Despite the widespread use of divestitures in competition policy, empirical evaluations of their impacts are limited. In this paper, we consider the case of the Alberta electricity sector, where the government chose a path of ==== to facilitate restructuring through the creation of 20-year Power Purchase Arrangements (PPAs).==== The PPAs effectively acted as a lease on the generation units, allowing the holder to choose both the price and quantity offered into the wholesale market, while the original owner maintained physical operation. These virtual divestitures expired at the end of 2020 and the offer control of these assets reverted back to their original owners leading to a large increase in market concentration. In addition to this large change in market structure, the holders of the PPAs varied throughout our sample. In particular, starting in 2016 the PPAs were transferred from large strategic firms to a government entity called the Balancing Pool.==== The Balancing Pool offered these units at short-run marginal cost, as if it were a perfectly competitive firm. This provides us with unique variation to identify how the allocation of divested assets to heterogeneous firms can impact market outcomes.====Alberta’s electricity market has a regulatory framework that places limited restrictions on firm behaviour. This provides us with an ideal environment to analyze market power. We use detailed data from the Alberta Electric System Operator (AESO) for the period 2013–2021 that includes price and quantity wholesale market bids from all generators at the hourly level. We also have detailed data from various sources that allow us to estimate the short-run marginal cost of each generation unit. We use these data to construct a counterfactual “competitive benchmark” that represents the market outcome if all firms behaved as price-takers in the hourly uniform-priced wholesale procurement auction. This allows us to compare the competitive counterfactual to realized equilibrium outcomes to quantify changes to the extent of market power over time and to consider how these changes relate to changes in offer control over the PPAs.====We find that the expiry of the PPAs in 2021 corresponded with a substantial increase in the degree of market power execution. Of the $70/MWh increase in average peak-hour prices from 2020 to 2021, representing a 120% increase in prices, we find that approximately 63% is due to firms raising their offer prices above marginal cost. The increase in market power that corresponds with the expiry of the PPAs remains when we focus on a narrow window around the expiry event, during which there were no other changes to the configuration of generation assets in the market. Similarly, we demonstrate that the high prices that occurred early in our sample were the result of strategic behaviour. This arises despite the fact that virtual divestitures existed to reduce market concentration. We show that this market power arises because the PPAs were offered into the market by large strategic firms.====We show that starting in 2016 until the expiry of the PPAs, market outcomes more closely reflected the competitive counterfactual. These more competitive outcomes reflect a combination of two forces. First, overall market conditions became less tight as a large natural gas unit entered the market. Second, driven in part by the entry of new generation and low prices, the divested assets were transferred to the Balancing Pool, a firm that offered these units at prices near their marginal cost. We use several empirical methods to demonstrate that market power execution is distinctly higher in the periods in which the PPAs were offered by large strategic generators, controlling for prevailing market demand and available generation capacity.====We complement these market-level results with a firm-level analysis. We demonstrate that the allocation of the PPA assets resulted in several large firms becoming “pivotal” (i.e., necessary to clear the market). Further, we use methods developed by Wolak (2000) to quantify the generators’ ability and incentive to exercise unilateral market power based on the properties of the residual demand function each firm faces. Both of these analyses demonstrate that the large generators had a significant ability to exercise market power over certain periods. This ability was mitigated when the PPAs were offered by the Balancing Pool. We examine firm-level bidding behaviour and find that the generators that had offer control over the PPAs were the generators that typically exercised market power, coinciding with periods in which there were elevated wholesale prices.====Our findings illustrate that virtual divestitures can have a substantial impact on market outcomes. When these assets are offered by large firms, either by their original owners after expiry or when the PPAs were allocated to large oligopolists, market prices exceed the competitive counterfactual by a considerable margin. We demonstrate that this leads to a considerable transfer of rents from consumers to producers. Our findings highlight that structural remedies in the form of asset divestitures can serve an important role in reducing short-run market power in concentrated electricity markets, but the allocation of these assets is critically important.====Our analysis provides a number of contributions to the literature. First, structural remedies that include asset divestitures are the dominant tool employed by the United States Department of Justice and the European Commission in merger cases (DOJ, 2020, Maier-Rigaud, Loertscher, 2020). Despite their dominant role, there are relatively few studies that quantify the impacts of asset divestitures. Competition agencies have carried out a number of descriptive studies that detail different divestiture cases and approaches, including in the United States (FTC, 1999, FTC, 2017), Canada (Competition Bureau Canada, 2011), and the United Kingdom (PwC Economics, 2005).====There is an emerging literature that carries out empirical ==== analyses of merger cases with divestitures. These studies consider several industries including the Johnson & Johnson acquisition of Pfizer’s customer health division (Tenn and Yun, 2011), licenses for highway retail gasoline stations held by the Dutch government (Soetevent et al., 2014), a merger in the Swedish beer market (Friberg and Romahn, 2015), and a merger of casinos in Missouri (Osinski and Sandford, 2020). These studies often find that the divestitures were successful at mitigating the price increasing impacts of the mergers. Soetevent et al. (2014) and Friberg and Romahn (2015) find that the divested products set lower prices and have competitive spill-over effects. Osinski and Sandford (2020) find that while prices were lower post-merger, the authors attribute this to realized efficiencies and find the divested casino performance decreased post-merger.====In addition to providing among the first empirical evaluation of the effects of asset divestitures on market competition in the electricity sector, our setting provides us with unique advantages.==== The four empirical studies cited above primarily rely on a difference-in-difference (DID) empirical framework to evaluate the impacts of mergers and asset divestitures on market prices.==== Davies and Ormosi (2012) highlight empirical challenges with this approach because the control group includes non-merging rivals which are likely to be exposed to spill-over competition effects post-merger. This makes it difficult to find a clear control group in a DID framework. A unique advantage of analyzing the electricity sector is our ability to estimate the marginal cost of production using a well-established engineering formula.==== We are able to directly infer how the degree of market power varies over time, controlling for changes in underlying supply and demand conditions, by constructing a competitive counterfactual benchmark. Further, we observe the transfer of the divested assets to heterogeneous firms over time. This allows us to estimate the impact of variation in divested asset ownership on market outcomes.====Our analysis adds to the large body of work that analyzes and documents the presence of market power in wholesale electricity markets (e.g., Wolfram, 1999, Wolak, 2000, Wolak, 2003, Borenstein, Bushnell, Wolak, 2002, Mansur, 2007, Bushnell, Mansur, Savaria, 2008, Reguant, 2014, Brown, Olmstead, 2017). While the precise methods, applications, and objectives of these studies differ, they all present empirical evidence that electricity markets are susceptible to market power. We follow the work of Borenstein et al. (2002), Mansur (2007), and Brown and Olmstead (2017) and use a competitive benchmark approach to quantify strategic behaviour.====Our work also relates to two recent contributions that use variation in transmission capacity constraints to identify the relationship between market structure and market power (McDermott, Woerman). Both studies use quasi-experimental empirical strategies and find a strong relationship between market concentration and market power. Unlike their setting, which relies on temporary hourly variation in transmission bottlenecks, our analysis focuses on large persistent changes to Alberta’s market structure as the result of virtual asset divestitures.====Finally, our analysis contributes to the literature that analyzes the impacts of fixed-priced forward contracts on competition (e.g., Allaz, Vila, 1993, Bushnell, Mansur, Savaria, 2008). When a generator holds a forward contact, it has the incentive to bid the associated quantity covered by that contract at or below short-run marginal cost (Hortacsu and Puller, 2008). Consequently, the transfer of the PPAs from the strategic PPA buyers to the Balancing Pool, which bids these assets in at marginal cost, could be viewed as the PPA buyers entering into a forward contract that is proportional to the size of the PPA units. Then, when the PPAs expired, these forward contracts were removed.====Consistent with the forward contracting literature, we observe a reduction in market power as the PPAs were transferred to the Balancing Pool and then elevated market power when the PPAs expired. This behaviour is consistent with the empirical literature demonstrating the pro-competitive impact of forward contracts (Wolak, 2000, Bushnell, Mansur, Savaria, 2008). However, the PPAs differ from standard fixed-priced forward contracts in two key ways. First, the PPAs were effectively a lease on the units to other generators and placed no restrictions on the bids on the unit. As a result, the holder of the PPAs chose whether the asset would generate electricity, and if so, how much. The forward contracting analogy is a result of the Balancing Pool’s bidding behaviour given its interpretation of its mandate by the government. Second, forward contracting decisions are endogenous. We anticipate that the generators adjusted their broader forward contract positions in response to changes in the PPA allocation. Brown and Eckert (2017) and Miller and Podwol (2020) develop a model to demonstrate that firms adjust their forward positions as the market structure changes. Therefore, we are unable to make assertions about the overall level of forward contracting in the market as the allocations of the PPAs varied.====Our analysis proceeds as follows. Section 2 describes features of Alberta’s electricity market and how it has evolved over time, including a detailed discussion and timeline of the PPAs. The data are detailed in Section 3. Section 4 summarizes our empirical methodology. Results are presented in Section 5. Section 6 concludes.",Evaluating the impact of divestitures on competition: Evidence from Alberta’s wholesale electricity market,https://www.sciencedirect.com/science/article/pii/S0167718723000346,13 April 2023,2023,Research Article,10.0
"Pál László,Sándor Zsolt","Department of Economic Sciences, Sapientia Hungarian University of Transylvania, Piata Libertatii 1, Miercurea Ciuc 530104, Romania,Department of Business Sciences, Sapientia Hungarian University of Transylvania, Piata Libertatii 1, Miercurea Ciuc 530104, Romania","Received 11 October 2022, Revised 22 March 2023, Accepted 23 March 2023, Available online 2 April 2023, Version of Record 10 April 2023.",https://doi.org/10.1016/j.ijindorg.2023.102950,Cited by (0),"We compare several nested fixed point and optimization procedures for computing the estimator of the widely-used empirical market demand model developed by Berry et al. (1995). It is well-known that the optimization may often lead to multiple local optima, which, if ignored, can lead to erroneous policy conclusions. By combining the frequencies of finding the global minima and the computing times, we propose a new indicator that provides the computing time needed for obtaining the global minima. Using this indicator, we find that the Spectral and Squarem methods (Reynaerts et al., 2012) outperform the benchmark contraction iterations method and the MPEC (Dubé et al., 2012) and ABLP (Lee and Seo, 2015) methods. Moreover, when the share of the outside alternative is relatively large, two derivative-free optimization algorithms, which require less calculations and coding than derivative-based algorithms, outperform the best derivative-based methods. A simple argument suggests that the latter statement is likely to be true for other versions of the model as well.","The empirical model of market demand and its estimation procedure proposed by Berry et al. (1995), BLP henceforth have become a workhorse in industrial organization for in-depth analysis of various competition policy issues. These include, but are not limited to, analyzing the welfare effects of mergers, tax changes or the introduction of new products. For example, Brennan (2021), in his assessment of how the method of defining relevant markets has evolved for competition law assessments, argues that this estimation procedure is one of the three fundamental achievements of economics that allow merger analysis to be applied in markets with differentiated products.====BLP constructed a discrete choice demand model of the car market with price endogeneity, unobserved heterogeneity of consumers’ preferences, and unobserved product characteristics, which are different from the previously used idiosyncratic errors because they are allowed to correlate with prices. This model can be estimated with market share and product characteristics data and without individual-level choice data. This model has been applied to various markets and extended in several ways.====Estimation of the model usually proceeds by GMM (i.e., generalized method of moments) based on unconditional moments of the unobserved product characteristics. Computation of the GMM estimator, however, is not a trivial problem. First, the unobserved product characteristics need to be computed from the observed market shares by using the market shares predicted by the model. These predicted market shares cannot be computed analytically, so they need to be approximated numerically. As Brunner et al. (2020) point out, this approximation must be sufficiently precise; otherwise not only the parameter estimates and various indicators derived from them become imprecise but also the outcome of the estimation procedure becomes misleading in the sense that the minimization procedure converges at values that are not local minima and the number of local minima obtained is too large. Second, the unobserved product characteristics cannot be computed analytically either, so they also need to be approximated numerically. Dubé et al. (2012) study how this approximation error propagates to the GMM estimators and argue that the unobserved product characteristics should be computed very precisely. According to Knittel and Metaxoglou (2014) imprecise approximation of the unobserved product characteristics may cause convergence and local minima issues similar to imprecise approximation of the predicted market shares, which leads to wrong elasticity values and erroneous conclusions regarding mergers. Third, due to price endogeneity the moments need to be constructed based on appropriate instruments. The literature has recently made significant progress in advising researchers on how to construct the instruments (Armstrong, 2016, Gandhi, Houde, 2020), but as such instruments still depend on the available data, there is no guarantee that they will be sufficiently strong. In such a case the GMM objective function may not be sufficiently peaked around the global minimum, which may lead to a large number of local minima.====Under such circumstances it is not a surprise that several empirical papers report finding local minima that differ from the global minimum (e.g., Berry, Levinsohn, Pakes, 1999, Goldberg, Hellerstein, 2013, Reynaert, Verboven, 2014, Reynaert, 2021). Further, by studying the original BLP car data, Brunner et al. (2020), Brunner, Heiss, Romahn, Weiser, 2020, Table 1 find that more precise approximation of the predicted market shares used for computing the unobserved product characteristics tends to reduce the number of different local minima and tends to eliminate cases when convergence occurs at values that are not local minima, although they still report several local minima even when they use the most precise approximations. Based on Amemiya (1985) an arbitrary local minimum is not necessarily consistent while the global minimum is, so in order to obtain reliable results it is crucial to find the global minimum of the GMM objective function. Unfortunately, it is rather difficult to determine the global minimum of an objective function that is not known to be globally quasi-convex. A common practice is to run the GMM minimization algorithm several times, each time starting from different values of the parameters, and choosing the estimator that corresponds to the smallest objective function value.====These facts highlight the trade-off between computing time and obtaining reliable results in this class of estimation problems. In this regard, by computing predicted market shares and unobserved product characteristics more precisely as well as by running the GMM minimization more times with different starting values, the researcher is likely to obtain more reliable estimates at an increased computational cost. In order to reduce the computational cost, it is crucial to use the most efficient procedures for the various problems involved in the estimation. Specifically, on the one hand, one may think of algorithms that are able to compute the unobserved product characteristics faster than the original contraction iteration approach proposed by BLP. On the other hand, there may be optimization algorithms that outperform the Nelder-Mead nonderivative simplex search algorithm used by BLP in terms of convergence and in terms of the likelihood of finding the global minimum in a given number of executions of the algorithm.====Several papers have proposed solutions to these computational problems. Dubé et al. (2012) propose the so-called MPEC (i.e., mathematical programming with equilibrium constraints) approach that treats the unobserved product characteristics as variables in the GMM minimization and regards the system of market shares as a constraint of the optimization problem. They recommend solving the GMM minimization problem by a derivative-based method and propose the KNITRO solver for the practical implementation. Reynaerts et al. (2012) propose several methods among which the more recently developed Spectral and Squarem methods. These are methods for solving nonlinear equations and fixed point equations, respectively, that mimic quasi-Newton methods but replace the Jacobian by a scalar multiplied by the identity matrix. Lee and Seo (2015) propose the so-called approximate BLP (ABLP henceforth) procedure, which provides an analytic approximation of the unobserved characteristics by linearizing the logarithms of the predicted market shares. Conlon and Gortmaker (2020) provide vast comparisons of various methods used for computing the predicted market shares, the unobserved product characteristics and methods used for GMM minimization, as well as comparisons of various sets of instruments.====In these papers the most important indicators for evaluating the proposed methods are convergence and computing time, where the latter refers to the time needed for minimizing the GMM objective function, irrespective of whether the result is the global minimum or not. This indicator is useful under the assumption that all the methods under comparison find the global minimum with similar relative frequency. However, when two methods take equal computing time but differ in the relative frequencies of finding the global minimum, computing time determined in this way provides misleading information about the relative times needed for finding the global minimum, which is actually what needs to be measured.====To the best of our knowledge the relative frequencies of finding the global minimum in BLP estimation have not been studied before. In this paper we conduct several Monte Carlo experiments in which we estimate the model and compare several methods. Specifically, for computing the unobserved characteristics, besides the contraction iterations (BLP), we consider the Spectral and the Squarem algorithms (Reynaerts et al., 2012). For minimizing the GMM objective function we consider the derivative-based method of KNITRO used by Dubé et al. (2012), the Nelder-Mead method and two other derivative-free methods, namely the numerical derivative version of the derivative-based KNITRO method and the so-called BOBYQA (i.e., Bound Optimization BY Quadratic Approximation) algorithm. We study these GMM minimization algorithms in combination with the contraction iteration, Spectral and Squarem algorithms and compare these to MPEC (Dubé et al., 2012) and ABLP (Lee and Seo, 2015). It is important to note that derivative-free algorithms have an advantage compared to derivative-based algorithms due to the fact that no calculation and coding of the gradient and Hessian is needed for the estimation of the parameters.==== One can regard this advantage as a reduction in fixed computing time as opposed to marginal computing time.====Our Monte Carlo experiments are based on those from Dubé et al. (2012) and Lee and Seo (2015) updated in order to implement advice from the literature on how to construct the instruments. Specifically, we include a cost shifter in the pricing equation in order to instrument for price, as Armstrong (2016) recommends, and we construct so-called differentiation instruments of quadratic type proposed by Gandhi and Houde (2020) to instrument for the random coefficients.====In these Monte Carlo experiments we report, among others, the frequencies with which the global minima are found as well as the computing times for different methods.==== By combining these two indicators by means of the binomial distribution, we propose a new indicator that provides an estimate of the computing time needed for obtaining the global minima and use this as the main criterion for comparing the methods. This indicator turns out to be useful in our analysis because we do find methods that perform well in terms of computing time but are poor in terms of finding the global minima; hence they need more time to find the global minima. In this regard, similar to Lee and Seo (2015), we also find ABLP to be faster than both the contraction iterations and MPEC in all the cases, but still in several cases it turns out that ABLP needs more time to find the global minimum than the contraction iterations. This is because ABLP finds the global minimum less frequently in those cases. We regard developing this indicator as the first contribution of the paper.====By employing this indicator to compare the performance of the various methods, when we use the derivative-based algorithm for GMM minimization, we find that the Spectral and the Squarem methods have the best performance in most of the cases while in one case they are slightly outperformed by ABLP. When we use derivative-free algorithms for GMM minimization we find that in some practically highly relevant cases the numerical derivative-based KNITRO algorithm combined with the Spectral and the BOBYQA algorithm combined with the Squarem algorithm perform equally well as the best derivative-based methods, and remarkably, in some cases they even outperform them. These cases occur when the share of the outside alternative is relatively large. We provide a simple explanation why derivative-free algorithms may be competitive compared to derivative-based methods in certain cases and why this is not expected to occur in other cases. This explanation suggests that the relative good performance of the derivative-free algorithms is not specific to the setup of our paper but it is likely to hold more generally. We regard these findings as the second contribution of our paper.====From the literature on computational aspects of BLP estimation, our paper is the closest to Conlon and Gortmaker (2020). Although these authors provide more comprehensive comparisons of existing methods than we do, they do not consider the ABLP and MPEC methods and they do not consider derivative-free optimization methods other than the Nelder-Mead. Further, they do not report the frequencies of finding the global minimum; in fact they admit that in their Monte Carlo experiments they deliberately work with simple models that can be estimated thousands of times for which the issue of local minima is less of a problem. Another related paper is Sun and Ishihara (2019), which proposes a computationally efficient MCMC algorithm to estimate the model in a Bayesian framework. The efficiency of the algorithm stems from the fact that it avoids full computation of the unobserved characteristics for new draws of parameter values. Doi (2022) addresses the problem of computing the unobserved product characteristics in the special case of discrete random coefficients when data on total sales are available for each consumer type. The paper finds that the unobserved product characteristics can be computed analytically by means of a simple formula, which is not time consuming to compute in practice.====The rest of the paper has the following structure. Section 2 presents the model and Section 3 describes the estimation procedures we consider in the paper. Section 4 presents the Monte Carlo simulations and discusses the results. Section 5 concludes.",Comparing procedures for estimating random coefficient logit demand models with a special focus on obtaining global optima,https://www.sciencedirect.com/science/article/pii/S0167718723000310,2 April 2023,2023,Research Article,11.0
"Fu Xiao,Tan Guofu,Wang Jin","School of Management, Fudan University, 670 Guoshun Road, Shanghai 200433, China,Department of Economics, University of Southern California, 3620 South Vermont Ave., Los Angeles, CA 90089, United States,Uber Technologies Inc., United States","Received 28 April 2022, Revised 26 March 2023, Accepted 27 March 2023, Available online 30 March 2023, Version of Record 11 April 2023.",https://doi.org/10.1016/j.ijindorg.2023.102952,Cited by (0),"Standard-setting organizations (SSOs) exhibit a variety of policy orientations toward the conflicting interests of technology developers and adopters. In this paper, we analyze a model that incorporates the technology choices of SSOs in standards wars and royalty determinations made by the developers of essential technologies. We show that both policy orientations toward developers relative to adopters and coordinated standard setting by SSOs that issue competing standards may result in a more-than-optimal number of essential technologies. Furthermore, we examine how SSOs’ technology choices may be affected by both network effects in standard adoption and coalition formation among developers.","A standard is a document that contains technical specifications that regulate the characteristics of products and services. In the modern economy, there exists a large variety of standards that have a tremendous impact on the development of particular industries. Examples of important standards include various generations of mobile phone standards (2G, 3G, 4G, etc.), Wi-Fi for wireless Internet technologies, USB for electronic connectors and portable devices, etc.==== The opportunity to collect royalties motivates technology firms to submit their inventions for consideration for new standards.==== To be incorporated in a standard, various technological approaches must undergo a long process of examination and approval by standard-setting organizations (SSOs). Because there are typically multiple approaches to address the same technical problem, standards developed by different organizations are often in competition for user adoption.====SSOs are open, voluntary, and consensus-based organizations specializing in the development of standards. Most SSOs declare that their main objective is to establish standards with the best technical performance.==== However, in practice, there are often significant disagreements within SSOs regarding the appropriate selection of technologies. Technology developers, who rely on patent licenses to commercialize their inventions, typically have technology preferences that differ from those of standard adopters, who are the producers of final products conforming to the standards. To help their members reach a coordinated decision regarding technology choices, SSOs establish various rules to balance the conflicting interests of their members.==== Nonetheless, over the past decade, antitrust regulators, courts, and firms have devoted considerable time and resources to patent licensing disputes between standard adopters and the holders of patents that cover standardized technologies (“standard-essential patents”, SEPs).==== A major concern frequently raised by researchers (e.g., Lerner and Tirole, 2015) is that although patents differ in their marginal contributions to the technical performance of a standard, once being included in the standard, they may become SEPs and, therefore, are equally indispensable.====How do SSOs’ diverse preferences regarding the conflicting interests of developers and adopters affect their technology choices and licensing fees? In particular, what determines whether an SSO has an incentive to design a “technically efficient” standard that achieves the best technical performance while minimizing efficiency losses from royalty stacking?==== Moreover, how does negotiation in standard-setting processes, such as coordinated standard setting by multiple SSOs and the formation of a coalition by developers, affect technology choices? To address these questions, in this paper, we analyze a theoretical model that incorporates the technology choices of SSOs and patent royalty determinations by SEP holders.====We focus on how SSOs’ policy orientations affect technology choices and licensing fees in the presence of competition among differentiated standards in the context of standards wars. Specifically, we examine the following model of a two-stage game. In the first stage, multiple SSOs design competing standards by selecting essential technologies. In the second stage, the developers of the selected technologies (i.e., SEP holders) independently set licensing fees for implementing the standards. It is commonly acknowledged that real-world negotiations between SSO members in standard-setting processes are highly complex. For tractability, we do not explicitly model the process through which SSO members reach an agreement but assume that SSOs’ standard-setting policies, which are characterized by different levels of orientations between user surplus and developers’ total licensing revenues, determine technology choices. Once the standards have been set, users make adoption decisions based on their net utility derived from implementing a particular standard, which depends on its technical performance, the licensing fee, and other factors.====Our analysis leads to the following main findings. First, we show that an SSO that is relatively more oriented toward developers prefers to incorporate a larger number of essential technologies, resulting in higher licensing fees and licensing revenues of its standard. Second, we find that SSOs’ policy orientations also affect the technical efficiency of standards. Compared to the technically efficient choice of technologies, an SSO that is fully oriented toward adopters (hereinafter, a user-friendly SSO) may choose too few (“suboptimal”) technologies to avoid inducing high licensing fees; by contrast, an SSO fully oriented toward developers (hereinafter, a developer-oriented SSO) may choose an excessive (“supraoptimal”) number of technologies, with the aim of softening the competition from competing standards. Third, we obtain that a supraoptimal selection of technologies is more likely to arise when developers can make sufficiently large marginal contributions to the technical value of a standard. The intuition behind this result is that an increase in the standard’s technical value not only induces the existing SEP holders to charge higher licensing fees, but may also give a developer-orienteddeveloper-oriented SSO an incentive to incorporate additional SEP holders in order to further raise total licensing fees. These findings may provide an alternative explanation for why standard adopters frequently express concerns that SSOs incorporate too many low-value patents in important standards.====Moreover, we extend our analysis to examine other important issues, namely, competing SSOs’ coordination on standard setting, coalition formation regarding technology choices, the impact of network effects (i.e., both within-standard and cross-standard positive externalities) among standard adopters on standard setting, and the existence of overlapping SEPs in competing standards. We show that, all else being equal, when developer-oriented SSOs are able to coordinate their technology choices to maximize their combined licensing revenues, a strictly larger number of technologies could be included in their standards than would be included in the case of noncooperative standard setting. In contrast, a coalition of developers that aims to maximize participants’ average licensing revenue would make a suboptimal technology choice and incorporate fewer technologies than a developer-oriented SSO would.==== Our findings contribute to the functioning of SSOs and standards wars.==== We note that standards can emerge in various ways. The early theoretical models concerning standards mainly focused on the so-called ==== standardization, where standards emerge as competing firms (or their alliances) invent incompatible solutions to an industry-wide problem (see Baron and Spulber 2018 and Shapiro and Varian 1999 for an overview). Throughout this paper, we focus on ==== standards (also called SSO standards in the literature) created by formal SSOs with certain regulatory power.====Our paper is closely related to the recent and growing literature concerning the role of SSOs in establishing standards, which starts with the pioneer works by Lerner and Tirole (2006) and Chiao et al. (2007). In both papers, the authors consider a continuum of SSOs that exhibit different levels of policy orientation toward developers relative to users. With developers self-selecting an SSO to commercialize their inventions, the authors show that owners of more valuable technologies submit inventions to a more friendly SSO that requires fewer concessions on the licensing of patents (such as royalty-free licensing).====More recently, several studies have further addressed the possibility that the conflict between technical efficiency and the interests of SSO members may result in inefficient standard setting. Schmalensee (2009) suggests that an SSO that favors adopters may issue a technically suboptimal standard to avoid hold-up and royalty-stacking problems in the market for SEP licenses. Farrell and Simcoe (2012) and Simcoe (2012) show that conflicting interests in the standard setting process can lead to a “war of attrition” in consensus building, decreasing the speed at which a standard is developed. Llanes and Poblete (2014) analyze the interplay between patent-pool formation and standard setting within a monopoly SSO. They find that the prospect of patent-pool formation may dampen the incentives to include firms with low-value technologies in the standard.==== Thus, the standard may not include all valuable patents, leading to a technically suboptimal standard-setting selection due to a lack of regulation for the licensing of SEPs. They show that a monopoly SSO’s choice of technologies can never be supraoptimal in a technical sense, regardless of whether its objective is to maximize users’ net utility or to achieve maximum profit per developer. Spulber (2019) presents an analytical framework of voting in an SSO and predicts that the interaction between the forces of particular voting rules and the interests of SSO members may balance each other, resulting in the efficient selection of technologies.====In the context of standards wars, Llanes and Poblete (2020) study the efficiency of standard setting when standards result from competition between groups of technology sponsors. In their model, technology sponsors that hold patents form coalitions, and each coalition develops a standard based on the patents it owns. These authors analyze various profit-sharing rules for coalition participants and find that ==== negotiations determining the ==== distribution of profits may not be socially efficient in the context of standards wars.==== These findings contrast with the results of previous studies examining standard setting within a monopoly SSO (e.g., Llanes and Poblete 2014 and Lerner and Tirole 2015), in which pre-standard-setting negotiations between developers are found to be welfare increasing.====The existing studies on SSOs suggest that the efficiency of standard setting is subject to a broad set of influencing factors, including standard-setting rules and procedures, the value of technology sponsors’ contributions, and competitive forces from other standards. Following this literature, we assume that SSO policies determine how standard-essential technologies are selected, which then establishes the technical characteristics of standards (i.e., the ownership of SEPs and standards’ technical qualities). To draw an important distinction between our paper and this line of work on how SSO policies affect technology choices, we emphasize that none of the papers mentioned above considers the possibility that SSOs’ policy orientations toward developers may result in an overinclusive choice of technologies in the context of standards wars.====Our analysis is also related to, but distinct from, the literature concerning strategic divisionalization (i.e., the strategic incentives of final-product producers to establish independent divisions, each offering one or several complementary components). For example, Rey and Stiglitz (1995) find that manufacturers have incentives to separate themselves from retailers to soften downstream competition. Baye et al. (1996) find that oligopolists producing homogeneous products have incentives to establish competing divisions, each independently operating in the market. Tan and Yuan (2003) study the strategic incentives of two competing firms offering differentiated products to divest into independent, complementary divisions and delegate pricing decisions to them. Considering the set of divisions exogenous, Quint (2014) explores the price and welfare effects of various arrangements to supply these complementary components as a package.==== A common feature in these papers is that consumers derive their values based on the entire final product rather than the composition of its inputs, i.e., divestiture decisions are assumed to be irrelevant to the product quality. In comparison, in our analysis, the technical performance of a particular standard varies with the number of complementary technologies incorporated in the standard, allowing us to assess the technical efficiency of SSOs’ technology choices.====The remainder of this article is organized as follows. Section 2 introduces our model. Section 3 characterizes the equilibrium outcome and offers our main findings. Section 4 discusses some extensions of our analysis. Section 5 discusses some empirical and public policy implications. Section 6 concludes. All omitted proofs are provided in the Appendix.",Policy orientations and technology choices in standards wars,https://www.sciencedirect.com/science/article/pii/S0167718723000334,30 March 2023,2023,Research Article,12.0
"Moresi Serge,Schwartz Marius","Charles River Associates, Inc., Washington DC 2004, USA,Department of Economics, Georgetown University, Washington DC 20057, USA","Received 28 September 2022, Revised 31 January 2023, Accepted 22 March 2023, Available online 25 March 2023.",https://doi.org/10.1016/j.ijindorg.2023.102951,Cited by (1),"This paper compares the incentives for product innovation across different market structures when the new product is vertically differentiated and of ====y, a common case empirically. We show that innovation incentive rankings across market structures can differ substantially when the new product is of lower rather than higher quality. In particular, the incentive to add the new product can be greater for a monopolist over the old product than for a firm that would face any degree of competition from the old product. This incentive ranking cannot occur when, instead, the new product is of higher quality as has been analyzed in previous work. Moreover, in that case, the incentive ranking is the same whether the market is covered or not covered, whereas in our setting the ranking can differ. With the market covered, our setting provides another environment where the monopolist can have the greatest incentive to innovate, as previously shown when the new product is ","The effect of market structure on incentives for innovation is of longstanding interest to economists and policymakers. In a seminal early analysis, Arrow (1962) considered a perfectly patentable ==== that reduces the constant marginal cost for an existing homogeneous product. He compared the innovator's gross gain (before subtracting the fixed cost of obtaining the innovation) under two alternative market structures. One is a secure monopolist.==== In the other market structure, homogeneous Bertrand competitors initially set price at their common marginal cost. The innovator obtains a lower marginal cost but faces potential competition from the higher-cost firms. If the innovation is drastic—the old technology does not constrain the innovator's profit—the incentive to innovate clearly is lower under monopoly than under competition. In both cases, the innovator earns the unconstrained monopoly profit with the innovation, whereas only the monopolist earns positive profit without the innovation.====If instead the innovation is ====, then there is an opposing effect: post-innovation profit will be greater to a monopolist that also controls the old technology than to an innovator that is constrained by competition from the old technology. Arrow (1962) nevertheless showed that the monopolist's gross gain from a process innovation again is lower because the reduction in marginal cost applies to a smaller industry output than under perfect competition (see also Tirole, 1988).====However, for a ====—yielding a differentiated substitute product that coexists with the old product—total output is no longer sufficient to rank the innovation incentives across alternative market structures. ==== The innovator's gain will depend also on the output mix and prices. Compared to a firm that initially earns no profit (an existing competitive firm or a new entrant), a monopolist's gain from adding a new product is subject to opposing forces. The monopolist diverts sales and profit from its old product (the “replacement effect”), but can coordinate its product prices to increase overall profit. In particular, it might raise the price of the old product to boost profit from the new product. A priori, either effect might dominate—sales diversion or price coordination—depending on the specifics of product differentiation.====Our paper considers innovation incentives for a nondrastic product innovation that yields a vertically-differentiated product of ====.==== In practice, a product innovation certainly can take this form: stripping out costly features or using lower-grade materials to create a more affordable product offers consumers beneficial variety. For example, the cost and quality of a disposable camera are lower than for a regular camera,==== and the same is true of cubic zirconia compared to diamonds.==== Other potential examples include frozen vs. fresh vegetables, and clothing made from polyester vs. natural fibers such as cotton. Also, it is common for manufacturers of smartphones or consumer electronics to introduce lower-quality models at lower prices.====We compare innovation incentives across three different market structures.====The Competition and Duopoly market structures (“regimes”) span the polar extremes of rivalry that an innovator might face from the old good: the latter is sold by perfectly competitive firms or by a monopolist. Thus, ==== and these are the lowest and highest incentive to add the new good for a firm that is not the initial monopolist. Consequently, if ==== then the monopolist's innovation incentive is never greater than under product market rivalry, whereas its incentive is always greater if ====. Besides academic interest, comparing ==== with ==== or ==== can also be relevant for policy interventions in certain scenarios (see Section 6).====Our model is similar to Greenstein and Ramey (1998, hereafter G&R) who consider the case where the new product has ==== (not lower) quality. They assume the market is ==== (i.e., a reduction in the price of the old good would expand sales) and show that ==== under general distributions of consumer types (willingness to pay for higher quality). When instead the new product has lower quality, we find the same ranking if consumer types are uniformly distributed (Proposition 1); but for general distributions ==== and ==== are possible (Proposition 2). Thus, one contribution of our paper is to show that innovation incentive rankings across market structures can differ substantially when the new product is of lower rather than higher quality.====A second contribution is to show that, when the new good has lower quality, innovation incentive rankings can differ when the market is not covered versus covered (i.e., a reduction in the price of the old good would ==== expand sales). With uniformly distributed types, ==== if the market is not covered (Proposition 1); if instead the market is covered then ==== (Proposition 4). Interestingly, in G&R's case we show that ==== continues to hold (under general distributions) if the market is ==== (Proposition 3).====Chen and Schwartz (2013) consider a standard Hotelling setting where the new product is ==== differentiated. They only consider the case where the market is covered and find that ==== is possible, which shows that the incentive for product innovation can be greater under monopoly than under any other market structure. Our setting with a lower-quality new product provides an additional environment where the monopolist has the greatest incentive to innovate.====Note that our Monopoly regime assumes a secure monopolist, not a preempting monopolist as in Gilbert and Newbery (1982). There, the monopolist expects that if it does not acquire the innovation an entrant will, leading to duopoly profits (====,====). A preempting monopolist thus has an added incentive to acquire the innovation: to prevent a fall in its profit from the monopoly level (==== to the duopoly level (====). Let ==== denote the monopoly profit with both goods. A preempting monopolist's incentive is ==== while for a secure monopolist ====, hence ==== Gilbert and Newbery showed that the incentive of a preempting monopolist to acquire the innovation exceeds an entrant's incentive (====), whenever industry profit is greater under monopoly than under duopoly, ====. However, since ==== Gilbert and Newbery's result is not sufficient to rank ==== versus ====Turning to welfare, we find that consumers are always worse off under Monopoly, even when the product innovation occurs only under Monopoly (Proposition 5). However, in the latter cases total welfare can be lower or higher under Monopoly (Proposition 6).====The paper is organized as follows. Section 2 presents the model. In Section 3, we compare innovation incentives across the three alternative market structures when the market is always not covered, and in Section 4 when it is always covered. Section 5 provides some intuition for our results and a summary of the incentive rankings in the various cases. Section 6 discusses welfare and potential policy implications of our findings. Section 7 concludes.",Product innovation with vertical differentiation: Is a monopolist's incentive weaker?,https://www.sciencedirect.com/science/article/pii/S0167718723000322,Available online 25 March 2023,2023,Research Article,13.0
"Nielsen Carsten Krabbe,Weinrich Gerd","Dipartimento di Economia e Finanza, Catholic University of Milan, Via Necchi 5, 20123 Milano, Italy,Dipartimento di Matematica per le Scienze Economiche, Finanziarie ed Attuariali, Catholic University of Milan, Largo Gemelli 1, Milano 20123, Italy","Received 13 January 2022, Revised 23 February 2023, Accepted 26 February 2023, Available online 3 March 2023, Version of Record 20 March 2023.",https://doi.org/10.1016/j.ijindorg.2023.102944,Cited by (0),"In our model, banks, heterogeneous in terms of entry costs, compete à la Salop for depositors on the unit circle. When capital requirements, intended to prevent risk shifting, are increased, the resulting costs are passed on to depositors in the form of reduced deposit rates or quality of service. This may induce depositors to migrate to unregulated shadow banks, the consequence being a change in the market structure for regulated banks: for low levels of capital requirements we observe ","Studying a firm in isolation may not always provide a full understanding of the effects and effectiveness of regulation. Compliance and its associated costs are likely to affect the profitability of firms and hence the number of firms operating in the market as well as how these interact, in short what we shall call the market structure.==== Not only are those effects potentially important for the social and private benefits of the market in question, but in addition they may feed back to how the individual firm reacts to the regulation imposed. Without studying the regulated market in its entirety, this kind of feedback would be left out of the picture making an assessment of the regulation in question incomplete.====Here we study a particular type of regulation, capital requirements imposed on banks to discourage excessive risk taking (i.e. risk shifting), and how it affects the size and competitive structure of the banking market. Bank profitability, intermediation and competition may decrease when banks are required to hold more capital, and the feedback from these induced changes can render capital requirements less effective or even counterproductive in preventing risk taking.====The model of bank behavior we use here is embedded in the classic Salop (1979) circular-model of monopolistic competition, however enhanced by assuming heterogenous entry costs for banks. As a consequence, and in contrast to previous versions of the Salop model, the model now allows for three distinct, endogenously determined market structures: monopolistic competition, constrained oligopoly and (local) monopoly. By the ==== we shall broadly refer to the impact of increased capital requirements, via the profitability of banks, on which of these structures materializes and thus on the number of banks in the market as well as the number of depositors being served by these banks. This effect may then feed back to the effectiveness of capital requirements.====Our setting allows us to distinguish between three incentive effects of capital requirements imposed on banks, of which the direct one is the ====, which, as intended, reduces risk-taking by forcing banks to have a larger stake in the returns from their investments. The two other, indirect effects, the ==== and the ====, whose presence depends on the type of market structure banks operate in, however make short-termism, i.e. investing in risky assets, more attractive.==== The distinct nature of these indirect effects is rooted in the trade-off from investing in a risky asset: on the one hand it provides a higher immediate pay-off per unit invested (i.e. per depositor) when the investment is successful, but it also implies a higher probability of failure, with a resulting loss of what we shall call the franchise value of the bank, that is, the discounted value of future profits.==== We show that these two indirect effects may individually dominate the direct effect, thus rendering capital requirements counterproductive.====Central to our results is the existence of an alternative placement of depositors’ funds which we shall interpret as shadow banks (or fintech companies), but which may also encompass banks outside the (geographical or juridical) reach of regulators or other relatively close substitutes for bank deposits. This alternative becomes attractive to depositors when the cost of capital regulation translates into lower deposit rates.==== While traditional banks offer products tailored to their clients (here summarized by their location on the circle), we assume that the alternative (i.e. fintech companies or shadow banks), offers a generic (non localized) product and is not subject to regulation. This captures the idea that for instance fintech companies operate via the internet and offer more standardized products and procedures to their costumers.====As capital requirements increase, to stay profitable banks initially, that is, when they only monopolistically compete with each other, decrease the deposit rate offered. However at some point the rate becomes so low that, on the margin, regulated banks are competing with shadow banks - we name this situation constrained oligopoly. Further tightenings of capital requirements force weaker banks to exit the market and the remaining regulated banks are now facing shadow banks and thus a more elastic demand. As a consequence, it is now optimal for the regulated banks still in the market to increase their market share by offering a higher deposit rate resulting in a situation with fewer and larger regulated banks, each with reduced profit margins per depositor. As we explain later, these larger banks earn higher aggregate returns if they choose risky investments and are successful, and capital requirements may therefore in this indirect way provide incentives to engage in risky activities. This development can however only continue until a certain point, where the capital requirements become so large that regulated banks are forced to retreat due to competition from shadow banks. This means that the market structure changes to that of (local) monopoly, where regulated banks no longer compete with each other, but only with shadow banks, and where some depositors use shadow banks.====The franchise value effect derives from the reduction in the franchise value of the individual bank, and hence in the loss in case of default (thus reducing the downside risk of risky investments). This reduction happens since, due again to the presence of shadow banks, banks are constrained from passing on all the costs of regulation to depositors. We show that this effect becomes particularly potent once capital requirements are so high that banks have become local monopolies. The franchise value effect, which is much referred to in the literature on bank regulation, was first identified, in a partial equilibrium model, as a potential counterforce to capital requirements by Hellmann et al. (2000). Since the profits earned by individual banks are determined by the number of banks and the type of competition in the market, the franchise value is best studied from the point of view of the market, as was done by Repullo (2004), a precursor to our study. He however found this effect to be absent in a Salop model with a fixed number of banks engaged in monopolistic competition (as we show, this is also so with free entry of banks). To the best of our knowledge, we are the first to demonstrate, in a model of banking markets where future profits are endogenously determined, that the franchise value effect may render capital requirements counterproductive.====If one relates to the current situation in banking markets, of the three possible regimes in our model, constrained oligopoly, where regulated banks not only compete with each other but also with shadow banks, seems most pertinent.==== In this type of market structure, due to the high costs of regulation, weaker banks have left the market leaving each of the remaining banks with a larger market share but also lower profits per depositor and lower future profits. In such a scenario banks may have greater incentives to pursue a risky investment strategy, despite the high capital requirements in place.====We note that there are other important repercussions of the market structure effect which, although arguably negative, we cannot directly evaluate in the context of our model. The most obvious one is the reduced relevance of regulated banks due to market leakage, whereby depositors choose unregulated shadow banks for their deposits, implying that regulation covers less of the total intermediation activity. Since, in our model, shadow banks choose higher-risk, lower-quality investments, this market leakage effect has a negative effect on the well functioning of bank intermediation. Secondly, since increased requirements make some banks leave the market, as explained above the remaining ones increase in size. This goes against the regulator’s objective of avoiding the presence in the market of banks which are too-big-to-fail.",Bank regulation and market structure,https://www.sciencedirect.com/science/article/pii/S0167718723000267,3 March 2023,2023,Research Article,14.0
Rabbani Maysam,"Feliciano School of Business, Montclair State University, 1 Normal Ave., Montclair, NJ, 07043 (SBUS 532), United States","Received 18 December 2021, Revised 12 February 2023, Accepted 15 February 2023, Available online 24 February 2023, Version of Record 4 March 2023.",https://doi.org/10.1016/j.ijindorg.2023.102934,Cited by (0),"It is theoretically shown that mergers between incumbents and future rivals can boost prices and harm consumers. But in the absence of empirical evidence, no merger has been litigated on this basis. To offer empirical insights, I study the acquisition case of a promising future rival by a large incumbent pharmaceutical firm. First, there is strong and causal evidence that the merger has enabled higher prices for the incumbent. Mergers with future rivals are practically unregulated and, if wisely exploited, they can circumvent antitrust enforcement and serve as entry barriers. Second, in contrast to the mainstream prediction that mergers with future rivals do not alter market concentration, I report a large post-merger increase in the market concentration. I introduce advertisement expenditure as a possible channel of effect between the merger and market concentration. Third, I document spillover effect of the merger on the incumbent's immediate rivals without affecting its distant rivals.","Economic theory predicts that nascent rivals (including future rivals and fringe firms====) impose competitive forces beyond their zero or negligible market shares. Subsequently, an incumbent's acquisition==== of a future rival may harm consumers==== either by postponing future price reductions that could improve welfare, or by boosting current prices. This concern has been explicitly stated in the latest version of the Horizontal Merger Guidelines (FTC, 2010a). Without empirical evidence, however, no merger has been litigated on this basis. In this study, I empirically show that mergers with future rivals can cause an immediate price increase, act as an entry barrier by taking advantage of a regulatory loophole, and, contrary to what the current literature presumes, largely boost market concentration.====I study Gilead's 2012 acquisition of Pharmasset. With five patented drugs, Gilead was the largest provider of medication for the human immunodeficiency virus (HIV) and hepatitis B virus (HBV). At the time, Pharmasset was not in the market, but it had two promising drugs – one for HIV and one for HBV – in phase II clinical trials that were expected to enter the market in 2013 (more in Section 2).====For three reasons, this merger is an excellent case to study the effect of mergers with future rivals. First, mergers with future rivals are most concerning when the incumbent is dominant, and the future rival is promising (FTC, 2010a). On the one hand, before the merger, Gilead was serving 40.83%==== of patients and generating 55.14%==== of revenues in the entire United States (US) HIV/HBV drug market. On the other, the competitive threat that Pharmasset imposed was so strong that Gilead spent 37% of its entire assets to acquire it (Gilead, 2012). Second, the elimination of the threat of entry is the only compelling channel for the merger to impact the HIV/HBV market because neither Pharmasset nor its drugs ever entered the market, and the study is not confounded by factors such as a partial presence of the acquired firm in the affected market or other markets. Third, for several months around the merger, Gilead did not have any acquisitions or major discoveries. Thus, if effects are found that coincide with the merger, the causality of the relationship will be more credible.====I use Truven Health MarketScan Research Databases (MarketScan data) provided by Truven Health Analytics®, part of the IBM Watson Health™ business==== in a difference-in-difference specification. I examine the merger's effect on six outcome measures, namely, the Average Wholesale Price (AWP) per pill, retail price per pill, out-of-pocket payment (OOP) per pill, the daily number of pills sold, the daily number of prescriptions filled, and the daily value of the Herfindahl-Hirschman Index (====) in the HIV/HBV drug market.====The first contribution of the study is empirically showing that a merger with a future rival can boost current prices (mechanisms of effect discussed in Section 3.3). I find that the merger has increased Gilead's AWP by $3.74 per pill, which immediately led to a $3.37 increase in its retail price per pill, and a $0.11 increase in the OOP per pill. Gilead drugs are chronically used. So, the price increase translates to $39.37 higher OOP and $1189.46 higher insurer payments per patient to utilize Gilead drugs for a year. Follow-up analyses in Section 5.4 support the robustness of the findings. High-frequency event studies (Figures A1 to A5) reveal that Gilead's AWP increased first, and the retail price and OOP responded within days.====The second contribution is pointing to the possibility that mergers with future rivals can make viable entry barriers. On the one hand, Modern Merger Analysis (MMA) flatly approves all mergers with future rivals because they are believed to leave the ==== intact (more in Section 3.4). On the other, incumbents carefully monitor the upcoming rivals, forecast market developments, and take action to safeguard their long-run profitability and market dominance (more in Section 3.2). So, incumbents have the motives and information to acquire promising future rivals to suppress future competition whereas no antitrust procedure exists to prevent or even detect it. This creates an antitrust loophole that, if exploited consistently, allows incumbents to deter entry and extend their market dominance indefinitely.====The third contribution is showing that, in contrast to what current literature presumes, a merger with a future rival may boost market concentration. I empirically support it by showing a statistically significant 249-unit increase in the ==== after the merger. The causality of the relationship between the merger and the ==== increase remains inconclusive. But, in an attempt to pin down a causal link, I introduce advertisement expenditure as a possible underlying mechanism (Section 3.4).====I document evidence of spillover effects. Gilead's branded rivals increased their AWP, retail price, and OOP after the merger but to a lesser extent than Gilead did. Event studies reveal that Gilead has made the first move, and the branded rivals have followed suit within days. I found no effect on Gilead's generic rivals. This local presence of spillover effects is consistent with the literature (Gaynor et al., 2015; Clougherty and Duso, 2009).====The paper proceeds as follows. Section 2 elaborates on the merging parties, the studied drug market, and the merger. Section 3 explains the mechanisms through which mergers (particularly those that involve future rivals) can increase current prices, evidence that firms proactively respond to anticipated threats, and the state of antitrust enforcement regarding mergers with future rivals. Section 4 specifies the tested hypotheses, model specification, and data. Section 5 presents the results and robustness checks. Section 6 covers several discussion topics including estimating the welfare effect, event studies, a framework to analyze mergers with future rival, and generalizing to mergers with nascent rivals. Section 7 concludes.","Mergers with future rivals can boost prices, bar entry, and intensify market concentration",https://www.sciencedirect.com/science/article/pii/S0167718723000164,24 February 2023,2023,Research Article,15.0
Kadner-Graziano Alessandro S.,"Faculty of Law, Business and Economics, University of Bayreuth, Universitätsstr. 30, Bayreuth 95440, Germany","Received 29 September 2022, Revised 13 February 2023, Accepted 15 February 2023, Available online 24 February 2023.",https://doi.org/10.1016/j.ijindorg.2023.102935,Cited by (0)," of complements are widely thought to decrease prices and thereby benefit consumers. Benefits materialise when the merging parties are monopolists but not when they face perfect competition. What about all cases between those competitive extremes? I model a vertically related ==== where every supplier may face competition. I show that, for general demand functions, pre-merger margins can reveal whether a merger would decrease prices. Then I develop a simple, practicable merger test and identify when the standard prediction of merger benefits is inconsistent with observable facts. Instead of yielding benefits, profitable mergers of complements can cause unambiguous consumer harm.","Absent anticompetitive effects, mergers of complements are typically thought to benefit consumers through the Cournot effect (Etro, 2019). According to this effect (sometimes referred to as the ‘internalisation of double markups’), mergers of complements eliminate negative externalities and decrease consumer prices.==== Antitrust authorities acknowledge this benefit (in particular, the United States Department of Justice, the Federal Trade Commission, and the European Commission in its merger guidelines).====Meanwhile, mounting evidence of insufficient antitrust enforcement has fuelled widespread concerns that merger policy is too lax.==== This calls for re-examining what are deemed to be merger benefits. In this paper, I show how to identify whether a merger of complements does – or does not – yield the beneficial Cournot effect. This can help antitrust authorities identify mergers that do not yield this benefit and help detect mergers that harm consumers.====Cournot’s (1838) now standard argument can be synthesised as follows. Suppose that two monopolists each supply one input to downstream consumers, that each monopolist sets the unit price for the input it supplies, and that the two inputs are perfect complements. Then, the two suppliers each set their unit price without considering that a higher price diminishes the profit of the other firm (through a lower, common demand). If the two monopolists merge, this negative externality is internalised. After the merger, prices fall and demand increases.==== This benefits all: the merging parties and downstream consumers.====In contrast, when there is perfect competition in each input market (and with constant returns to scale), suppliers earn zero profit pre-merger. Post-merger, the merged entity does not decrease price (doing so would be unprofitable). Hence, the Cournot effect does not materialise.====For all competitive landscapes in input markets between the two polar extremes (of monopoly and perfect competition), there is no general result on whether the Cournot effect materialises. This gap is problematic for antitrust authorities and applied theorists alike. During a merger review, an antitrust authority needs to weigh positive merger effects against negative effects from any theories of harm it may have. However, it may lack data to ascertain the net impact. More fundamentally, it does not have a test or tool to determine when exactly it can outright dismiss positive Cournot effects. The problem for an applied theorist is knowing how to recognise whether a model correctly predicts that a merger benefits consumers via Cournot effects or whether that prediction is inconsistent with observable facts.====I re-examine the Cournot effect in a vertically related industry. In my baseline model, a downstream firm needs different inputs to manufacture a consumer good. Each of those inputs is produced by one or more upstream producers. In stage 1, each upstream producer sets the unit price (the bid) at which it offers to supply an input to the downstream firm.==== In stage 2, based on input prices, the downstream firm selects its suppliers and sets the price of its consumer product. This model differs from Cournot’s original model (Cournot (1838), p.112-7) in two crucial ways. First, whereas Cournot exclusively modelled suppliers, I consider both upstream and downstream firms as profit-maximising entities. This enables me to obtain results on upstream versus downstream margins. Second, I allow for any number of producers of each input. This captures the polar cases of monopoly and perfect competition but also intermediate cases of competition. With this model, I develop solutions to the problems laid out in the previous two paragraphs.====Theorem 1 closes the gap in the literature: it can be used to determine whether a proposed merger among suppliers of complements would benefit consumers for all competitive landscapes, from the polar extreme of monopoly to the polar extreme of perfect competition. Theorem 1 can also be used to identify whether particular models of mergers of complements, and their predictions, are consistent with observable facts of an industry under analysis.====The test, a corollary of Theorem 1, constitutes a practicable new merger tool.==== Antitrust authorities can use it to determine whether proposed mergers would benefit consumers. If a particular merger would not yield the Cournot effect, an antitrust authority can dismiss claims that the merger would benefit consumers via this effect. Absent other claimed benefits, and if the antitrust authority has a theory of harm, the authority would then have unambiguous grounds to act against the proposed merger.====The intuition for the test goes as follows. When a downstream firm passes through to consumers less than 100% of an input price increase (i.e. when it “absorbs” some of the increase====), the elasticity of demand upstream is lower than it is downstream. Consequently, by the intuition of the Lerner index,==== any supplier’s unit dollar margin must exceed the downstream firm’s margin.==== If a suppliers’ unit dollar margin is smaller than that of the downstream firm, then the supplier must be constrained (by some outside option of the downstream firm). Such alternatives can, for example, be to source the input from an alternative supplier, to produce the input in-house, or not to purchase and use the input at all. A constrained supplier maximises profit by maximising price: it sets its unit price to the level beyond which the downstream firm would resort to an outside option. In contrast, an unconstrained supplier does not maximise price; it trades off an increase in the unit price it charges with a decrease in consumer demand. Lerner and Tirole (2004), who analyse patent pools, refer to the former case as “the competition margin is binding” and to the latter as “the demand margin is binding”.====The test comes down to a comparison of pre-merger margins. It states that if the merging suppliers earn a combined unit dollar margin smaller than the downstream firm’s unit dollar margin, then prices would not decrease post-merger. Hence, consumers would not benefit from the merger. Effectively, the test verifies and reveals whether the merging suppliers are collectively, sufficiently constrained pre-merger to rule out a price decrease post-merger. (It is necessary but insufficient that each merging party is constrained: the merging parties need to be sufficiently constrained.) Margins can contain sufficient information on outside options to predict merger effects.====As a practical illustration: data shows that Apple earns more dollars of profit per iPhone sold than all of its suppliers combined. (The price of an iPhone, e.g. the iPhone 11, is around $1100, whereas its total unit cost to Apple is estimated at $490: less than half.)==== Hence, according to the test, if any two of Apple’s suppliers were to merge – or even if all of its suppliers were to merge – the merged entity would not find it profitable to lower input prices.====Theorem 1 and the test can identify the absence of consumer benefits but not the presence of harm. In Section 4, I augment the model to allow for profitable strategies that harm consumers. I present two distinct theories of harm. In the first theory, I add a compatibility parameter to the model. In the second, I apply a central idea from Whinston (1990). In each, the merged entity weakens the downstream firm’s outside options. Consequently, the merger relaxes price constraints, raises the consumer price, and causes unambiguous consumer harm.====The results I develop are useful precisely because they are robust to extensions which capture the intertwined nature of real supply chains. Among others, results continue to hold with downstream competition and when upstream firms supply multiple competing downstream firms.====The remainder of the paper is structured as follows. After a literature review, I describe the baseline model and develop the main results in Section 2. Section 3 addresses the robustness of the results. In Section 4, I deal with merger harm (in addition to the absence of benefits). In Section 5, I discuss settings in which my results do not apply. Finally, I conclude in Section 6. The appendix contains proofs. The online appendix contains workings for Section 5 as well as additional results.",Mergers of Complements: On the Absence of Consumer Benefits,https://www.sciencedirect.com/science/article/pii/S0167718723000176,Available online 24 February 2023,2023,Research Article,16.0
"Doan Thanh,Manenti Fabio M.,Mariuzzo Franco","Office of Communications UK,Dipartimento di Scienze Economiche ed Aziendali “M. Fanno”, Università di Padova, Italy,School of Economics and Centre for Competition Policy, University of East Anglia, UK","Received 8 August 2022, Revised 6 February 2023, Accepted 7 February 2023, Available online 19 February 2023, Version of Record 15 March 2023.",https://doi.org/10.1016/j.ijindorg.2023.102930,Cited by (0),"The tablet PC market is dominated by two platforms: iOS and Android. In this paper, we combine tablet-level data with data on the quality of the top 1000 mobile applications from these platforms and estimate a structural demand model. We exploit variations over three periods and five European countries to find whether the application quality affects tablet demand. We then run two counterfactuals. The first counterfactual suggests that an improvement in application quality benefits the tablet producers on that platform with a more pronounced effect on the demand for Android-based tablets. The second counterfactual discusses the policy of leveling the app quality of the two stores. It shows that such a policy favors the tablet producers adopting the lowest quality app store (Google) and stimulates the adoption of tablet PCs. This generates ==== in tablet demand.","The introduction of digital distribution platforms for mobile operating systems, developed by Apple and Google in 2008, marked a milestone in the rapid growth of the mobile app market. Since their launch, the number of mobile applications (apps) using Apple and Google operating systems has grown exponentially. By the end of the third quarter of 2022, more than 3.5 million apps were available in Google Play and nearly 1.7 million in Apple App Store (Statista.com), the two largest app stores by far. This astonishing app growth brought a new generation of hardware to the market - the tablet PC. Apple delivered its first generation of iPad devices in early 2010, followed closely by several other manufacturers.====The wide availability of applications is undoubtedly one of the explanations for the success of tablets; nonetheless, an empirical evaluation of the exact role of apps and the relevant policy in the tablet market is still missing. Filling this gap is essential, especially considering the characteristics of the tablet market tightly dominated by two alternative platforms: the iOS-based platform, entirely controlled by Apple, which both produces the devices and manages the app marketplace (App Store), and the Android-based platform, with competitive and independent manufacturers producing the devices and Google managing the app store (Google Play).====We study how the quality of the apps distributed in each dedicated app store affects the outcome of the tablet market. In particular, we show how the quality of the apps in the two online stores has a differential impact on tablet producers due to the different levels of competition in the two platforms. Following the previous literature (Binken, Stremersch, 2009, Kim, Prince, Qiu, 2014), we employ the average user rating to measure application quality. For each store in each period, we consider the average rating of the top 1000 apps, weighted by the total downloads. This measure allows us to capture the heterogeneity in the popularity or attractiveness of the apps to users and, hence, to account for the well-known “superstar” effect in the hardware-software market, according to which the availability of top software applications is one of the main drivers in hardware demand (Binken and Stremersch, 2009).====We construct an econometric model relying on the discrete choice literature for product differentiation to estimate the impact of app quality on tablet PC demand. More specifically, we choose the random coefficients nested logit model proposed by Grigolon and Verboven (2014) where, here, the nest captures the heterogeneity in operating systems. We account for observable and unobservable price sensitivity, as in Nevo (2001). This allows us to estimate richer own-price and cross-price elasticities of demand for both Apple and Android tablets and relate those to profitability and welfare.====The sample used in our estimation consists of three waves of quarterly product-level data for tablets and apps distributed in five European countries (Germany, France, Italy, Spain, and the UK) over 2013Q3-2014Q1. We recover information on income from a Eurostat dataset. We jointly estimate demand and pricing equations to investigate the role of app quality in the tablet market.====In the last part of the paper, we use the estimates of the random coefficient nested logit model to conduct two counterfactual analyses aimed at studying two alternative policies affecting app quality. First, we evaluate how tablet manufacturers’ equilibrium prices, market shares, profits, and consumer surplus would change if platforms opt to increase the quality of the available applications. Then, we study what would happen if the quality of the apps were the same in the two stores. We conduct these two counterfactuals on practical grounds. Guaranteeing a certain average quality of their apps has always been one of the objectives of the platforms. Since the onset, Apple has adopted a strict quality control system, and only developers meeting specific requirements can publish apps on the Apple App Store (see Comino et al., 2019). This policy has played a central role in Apple’s strategy toward app quality. On top of this, in 2016, Apple removed thousands of outdated and non-compliant applications from its online store. Google, which, unlike Apple, does not have a similar quality check, periodically removes low-quality, malware, and abandoned apps (see Wang et al., 2018).==== Our first counterfactual embraces these platform-quality strategies by discussing the impact of increasing the weighted average quality of app stores on the tablet market.====We obtain an increase in app quality by removing the lowest quality apps from the app stores, one store at a time until the weighted average of the app quality increment grows approximately by one standard deviation, and then recompute the new equilibrium in the tablet market. Our findings confirm that improving app quality impacts tablet demand. For example, in the UK, Apple’s market shares and profits are estimated to increase by 9.44 (base points) and €2.52 million (about 2%), respectively. Android tablet producers would improve their market shares and profits by 17.81 (base points) and €1.39 million (again, about 2%), respectively; consumers surplus would grow by €1.42 million when the quality of iOS apps increases, and by €2.44 million when the quality of Google Play apps increases by the same amount. Our estimates reveal that due to the different levels of competition between the two platforms, Android tablets’ demand expansion tends to be larger than Apple’s. We find that the increase in app quality in one store leads to a higher gain from the outside good than from demand from other competitors. Even though we do not model them explicitly, we interpret this evidence as due to switching costs in tablet PCs demand.====With the second counterfactual, we analyze how the demand for tablets, firms’ profits, and consumer surplus change when the average quality of the apps is the same in the two stores. With this counterfactual, we can, at least partially and in the short run, replicate what could happen if the two platforms were interoperable.==== In this case, an app developed for the Android system could also run in the iOS environment, and tablet users could access the apps of both stores, regardless of which tablet they choose. Consequently, app quality differences between platforms are leveled.====The effect of imposing the same quality in the two stores depends on the level of the common quality in relation to the level of quality before the policy introduction. For this reason, we study various scenarios depending on where the common level of app quality is set (minimum quality, average quality, and maximum quality). Our data reveal that the average app quality in Google Play, measured in terms of weighted average users rating, is lower than in the App Store. We thus find that imposing the same app quality at the level of Google Play would benefit Android tablet producers. For example, to remain in the UK again, if the common weighted average app quality is set at the average quality of the two stores, the profits of Android tablet manufacturers grow by almost €2 million, while those of Apple are reduced by around €3.5 million. Hence, we claim that a policy that leans toward interoperability and reasonably increases the app quality for Android users tends to stimulate, in the short term, the demand for Android tablets and depress that of Apple tablets. We find that unless the policy lowers the average overall app quality, it stimulates tablet adoption and enhances consumer welfare.====To give an intuition for our empirical analysis, we describe, in the appendix, a stylized theoretical model of competition among tablet producers. We perform a two-stage game, where consumers first select the platform (operating system) and then, according to their choice, purchase their most preferred tablet model. Preferences for tablets are affected by the quality of the applications available for that platform. To reinforce our findings, we compare the simulations of the two counterfactuals obtained with our discrete random coefficient nested logit (RCNL) model with those calibrating the theoretical model. The conclusions reached by these calibrations are qualitatively in line with, and quantitatively close to, those of the RCNL model.====The outline of this paper is the following. In the next section, we briefly review the relevant literature. We discuss the testable predictions in Section 3. Section 4 develops the econometric model, while Section 5 briefly describes the data and provides summary statistics. Section 6 illustrates the estimation strategy and discusses the main results. Section 7 concludes and discusses limitations and extensions.",Platform competition in the tablet PC market: The effect of application quality,https://www.sciencedirect.com/science/article/pii/S0167718723000115,19 February 2023,2023,Research Article,17.0
"Huang Yangguang,Xie Yu","Hong Kong University of Science and Technology, Hong Kong,University of Hong Kong, Hong Kong","Received 25 May 2022, Revised 7 February 2023, Accepted 12 February 2023, Available online 15 February 2023, Version of Record 20 February 2023.",https://doi.org/10.1016/j.ijindorg.2023.102933,Cited by (0),"A prominent feature of online sales is that buyers rely on the search tools offered by platforms to process information when searching for products. We develop a model that captures how the search algorithm affects buyers’ search processes, which further influences market equilibrium and welfare. If a platform adopts a highly unequal search algorithm, buyers are likely to obtain repetitive information about a small group of sellers, which causes buyers to consider fewer options and suppresses competition. By using data from food delivery platforms, we provide empirical evidence that markets with less equal distributions of store rankings in search results have higher average prices and more concentrated sales. We suggest that regulators should restrict search algorithms from showing repetitive information.","Online e-commerce platforms often host a large number of sellers that offer heterogeneous products with a substantial amount of information. It is impossible for consumers to thoroughly search and study all available products due to information overload (Anderson and De Palma, 2009). One of the most important features of online sales is that consumers rely on the search tools provided by platforms to search for and learn about products. Search tools such as search engines, recommender systems, and price-comparison shopbots use information technology to assist buyers in searching for products and learning about product characteristics. These search tools have strong impacts on consumer searches (Teh, Wright, 2020, Chen, Tsai, 2021, Teh, 2022). For example, Backus et al. (2014) shows that the eBay search algorithm can cause identical items to have vastly different visibility, leading to dispersion in prices and the number of bidders. Dinerstein et al. (2018) find that transaction prices on eBay fell significantly after the platform redesigned the search process to promote price competition.====Because search tools are designed and operated by for-profit online platforms, they might not be designed to boost competition. Regulators and antitrust authorities have started to regulate the search tools controlled by online platforms. For example, in June 2017, the Commission (2017) fined Google € 2.42 billion for “abusing dominance as search engine by giving illegal advantage to own comparison shopping service.” Figure 1 illustrates Google’s ability to control users’ attention (clicks). The Commission (2013) requires search engines to “distinguish between advertisements and search results,” because sponsored searches can profoundly influence buyers’ decisions and cause potential efficiency losses (Ghose and Yang, 2009). China passed a law in 2021 restricting platforms from exercising algorithmic discrimination against frequent customers.====Many researchers have studied the private incentives of online platforms in designing search algorithms and the related consequences. Hagiu and Jullien (2011) note that platforms have two main motives in diverting searches: obtaining higher revenues from participating users and affecting sellers’ choices in pricing or other strategic variables. A platform has a clear incentive to favor sellers who are vertically integrated with it (De Corniere and Taylor, 2019). In designing the search algorithm, platforms might bias the search results toward their own content or that of sponsored sellers (De Corniere, Taylor, 2014, De Corniere, Taylor, 2019). Chen and Tsai (2021) find empirical evidence suggesting that Amazon’s products are recommended in frequently-bought-together lists considerably more often than are the same products carried by third-party sellers. Search advertising also deeply affects consumers’ search behaviors and social welfare (Athey, Ellison, 2011, Chen, He, 2011, Eliaz, Spiegler, 2011, Blake, Nosko, Tadelis, 2015).==== De Corniere (2016) shows that even with neck-and-neck competition among search engines, suboptimal sponsored links persist, and welfare can be worsened.====In this paper, we construct a model that demonstrates how the search algorithm influences the consumer search process on the platform, which further affects the sales distribution among sellers and social welfare. Buyers search for products with the search tool offered by the platform. After the search process, each buyer spends a limited amount of effort on searching and obtains a consideration set (Goeree, 2008, Honka, Hortaçsu, Vitorino, 2017) that contains several options. Then, the buyer chooses his or her favorite option from within the consideration set (Eliaz and Spiegler, 2011a).====The search algorithm determines the ranking and composition of the product information in the search results. Most search results are lists of products or sellers with rankings that determine the probability of each product/seller being considered by the buyers. The algorithm usually does not treat sellers equally. Some products may receive preferential treatment and appear frequently at the top of search results. Moreover, the same product or seller may appear multiple times in the search results. In practice, search algorithms commonly yield search results with repetitive information. For example, Fig. 2 shows the results from a trial search on Meituan, the largest food delivery platform in China. The search results display the information of some chain restaurants multiple times. Chain restaurants mostly offer identical food items at the same prices across different stores. Store prices are usually set by the central office of the brand, so repetitive listings of chain stores with the same brand do not impose any competitive pressure between them. Instead, these repetitive listings may reduce the number of options considered by buyers. As the number of repetitive listings increases, a buyer obtains fewer effective options after spending a limited amount of effort searching. Similar phenomenon of repetitive listings can also be found on major food delivery platforms in the US, e.g., Uber Eats (Fig. B.1).====Our model shows that consumers are more likely to obtain repetitive information about the same set of sellers when using a search algorithm that favors a small set of sellers. This phenomenon leads to smaller consideration sets, softens price competition, and reduces consumer surplus. The expected size of the consideration set increases as the algorithm becomes more equal in the sense of Lorenz ordering. The buyer-side surplus and total welfare improve if the platform adopts a more equal search algorithm. However, both the platform and sellers have private incentives to make the search algorithm unequal, which causes repetitive listings to appear more frequently in the search results.====We use data from food delivery platforms to explore how the search algorithm affects sales in practice. Based on trial search results, we construct two variables to measure how equal and repetitive the search algorithm is. For each store, we record its default position and how many times stores with the same brand appear in the search results. We show that restaurant revenues are critically determined by the ranking in search results. Repetitive listings of stores with the same brand help each store, on average, earn more revenue. Based on market-level regressions, we find that markets with less equal search results have higher average prices and more skewed revenue distributions.====This paper mainly contributes to the growing literature on the search design of online platforms. The model regarding how the search algorithm affects the composition and size of the consideration set is closely related to two papers. Dukes and Liu (2015) consider a model in which consumers must decide their search breadth and depth. They find that platforms have the incentive to strategically increase search costs to discourage buyers from evaluating too many sellers. Dinerstein et al. (2018) note the trade-off between navigating consumers to more desired products and promoting price competition. If the search algorithm lists homogeneous products by price, sellers face fierce price competition, but consumers are more likely to be matched with undesirable products. In contrast, if the search algorithm lists products that are more heterogeneous, price competition softens, but consumers can more easily find desirable products. Compared to previous studies, our model emphasizes the role of repetitive listings and demonstrates how the features of a search algorithm affect welfare measures.","Search algorithm, repetitive information, and sales on online platforms",https://www.sciencedirect.com/science/article/pii/S0167718723000152,15 February 2023,2023,Research Article,18.0
"Silveira Douglas,de Moraes Lucas B.,Fiuza Eduardo P.S.,Cajueiro Daniel O.","Department of Economics, University of Alberta, Canada,Territorial and Sectoral Analysis Laboratory - LATES, Brazil,Institute for Applied Economic Research (Ipea), Brazil,Department of Economics, University of Brasilia, Brazil,National Institute of Science and Technology for Complex Systems (INCT-SC), Brazil,Machine Learning Laboratory in Finance and Organizations (LAMFO), Brazil","Received 25 May 2022, Revised 23 January 2023, Accepted 6 February 2023, Available online 15 February 2023, Version of Record 26 February 2023.",https://doi.org/10.1016/j.ijindorg.2023.102931,Cited by (1),"We propose a data-driven machine learning approach to flag bid-rigging cartels in the Brazilian road maintenance sector. First, we apply a clustering algorithm to group the tenders based on their attributes. Second, we use the labels created by the clustering algorithm as a target variable to predict them using a classifier. We rank the screens according to their relevance to decrease the number of false positive (detecting cartel when it does not exist) and false negative (not detecting cartel when it does exist) predictions. Our results shed light on the need to use a range of screens to recognize the vast profile of strategies practiced by bid-rigging cartels, such as misleading competitive dynamics, bid combination, and cover bidding behavior. Our method can improve cartels’ deterrence in different economic sectors, especially when labeled data are not available. In a controlled environment with a simulated labeled dataset, the overall average accuracy of the algorithm is 99.33%. In a real-world cartel case with a labeled dataset, the overall average accuracy is 80.25%. When applied to the road maintenance unlabeled dataset, our model identified a group containing 273 (31% of the total) suspicious tenders. We conclude by offering a policy prescription discussion for antitrust authorities.","One of the fundamental issues in Industrial Organization and competition policy is detection, punishment, and deterrence of collusive behavior. Although it is a consensus that collusive agreements reduce social welfare, firms have incentives to coordinate their decisions and increase their returns (Levenstein and Suslow, 2006). The increasing amount of electronic databases has been crucial for developing methods that integrate economic analysis and data-driven approaches to combat such behavior effectively.====In this paper, we propose a Machine Learning (ML) approach to identify bidding patterns consistent with bid-rigging cartels related to road maintenance in Brazil. The analysis is based on an unlabeled dataset from open tenders between 2012 to 2020, covering 891 tenders from all states and the Capital District of Brazil. We combine ingredients from unsupervised and supervised ML with statistical and economic analysis. First, we apply a clustering algorithm (i.e., unsupervised ML) to group the tenders according to their attributes.==== Second, we use the labels created by the clustering algorithm as a target variable and try to predict these labels using a classifier (i.e., supervised ML). Third, we find out the most relevant input variables to characterize each cluster and the typical values of these variables. Fourth, based on the attributes that shape these clusters of tenders, we identify the ones that present behavior consistent with the existence of bid-rigging cartels. Thereby, our last step is strongly dependent on previous evidence about the detection of cartels using statistical screens.====Certain supervised ML methods have been proposed to detect cartels using statistical screens (Huber, Imhof, 2019, Imhof, Wallimann, 2021). They depend on two basic steps: (1) labeled data collection; (2) model parameters estimations. However, labeled data are frequently unavailable. In general, they are costly and depend on previous investigations of the antitrust authority. In addition, even after the cartel investigation, the pieces of evidence collected may not be conclusive. In the case of the tenders related to road maintenance activities in Brazil, the available data is insufficient to train supervised machine learning models. Our ML approach intends to identify collusive patterns without labeled data.====We use the Gaussian Mixture Clustering Model (GMCM) for the unsupervised ML stage and the Quadratic Discriminant Analysis (QDA) for the supervised ML stage. The choice of the GMCM explicitly assumes that the data-generating process of the open tenders attributes comes from a Gaussian mixture. Thus, due to the finiteness property of the first and second moments of each mode of the Gaussian mixture, the GMCM tends to group tenders with similar attributes. QDA also assumes that each class follows a Gaussian distribution and uses the expected value of each input variable as a classification criterion. The Permutation Importance (PI) technique is used in supervised ML to assess the relationship between the input variables and the target variable (the labels generated via the clustering analysis in our unsupervised ML stage). This step in our approach finds out the most “skilled variables” to separate the clusters into groups of tenders with higher and lower probabilities of anti-competitive behavior. Furthermore, we generate (additional) results to support our findings using different combinations of the GMCM and ====-means (unsupervised ML stage) and QDA and logistic regression (supervised ML stage).====We first validate our approach using Monte Carlo simulations to generate labeled data in a controlled environment. Then, we provide a similar analysis using the gasoline cartel labeled dataset due to Silveira et al. (2022) and compare our predictions with their actual labels. Since the Monte Carlo simulations exercise presents a data-generating process close to the one used in our model, the model accuracy is around 100%. The overall average accuracy to detecting the correct label in the gasoline cartel dataset is 80.25%.====Then we investigate an unlabeled dataset of open tenders related to road maintenance activities in Brazil. We use six statistical screens in our model. These are grouped into: (a)“elementary” screens (total number of firms, the total number of bids, the average number of bids, and the number of firms offering single bids) to detect possible misleading competitive dynamics; (b)“variance-based” screen (coefficient of variation) to capture bid combination strategies; (c)“cover bidding” screen (skewness) to identify situations where the cartel members submit bids in excess of the cartel member designated to win the contract. Explanatory variables similar to (a), (b), and (c) have been used in works dedicated to studying bid-rigging strategies, either using labeled (Porter, Zona, 1999, Tóth, Fazekas, Czibik, Tóth, 2014, Huber, Imhof, 2019) or unlabeled data (Bajari, Ye, 2003, Chassang, Kawai, Nakabayashi, Ortner, 2019, Kawai, Nakabayashi, 2022). Our findings suggest the need to use a range of statistical screens to recognize the vast profile of strategies practiced by bid-rigging cartels, such as misleading competitive dynamics, bid combination, and cover bidding behavior. Guided by the patterns found by these statistical screens, our approach indicates a cluster of bidding data indicative/suspicious of anti-competitive practices in approximately 31% of public tenders distributed across all Brazilian states. The replication package is available at the Zenodo repository (Silveira et al., 2023).====In summary, this work combines economic and statistical analysis with a data-driven framework to investigate collusive market behavior without using labeled datasets. Even (indirectly) relying on patterns found in previous studies on the strategies of bid-rigging cartels, this screening approach can be a valuable tool for competition authorities to identify potential collusion in different markets and circumstances.====The remainder of this paper is organized as follows. Sections 2 – 4 review the literature, detail the methodology, describe the dataset, and introduce the screens, respectively. Sections 5 and 6 present and discuss our main findings, respectively. Section 7 concludes.",Who are you? Cartel detection using unlabeled data,https://www.sciencedirect.com/science/article/pii/S0167718723000139,15 February 2023,2023,Research Article,19.0
"Forsbacka Tove,Le Coq Chloé,Marvão Catarina","Stockholm School of Economics, Sweden,University of Paris-Pantheon-Assas (CRED), France,Stockholm School of Economics (SITE), Sweden,Technological University Dublin, City Campus, Ireland","Received 20 September 2022, Revised 8 February 2023, Accepted 9 February 2023, Available online 12 February 2023.",https://doi.org/10.1016/j.ijindorg.2023.102932,Cited by (0),"This paper examines how a gradual tightening of antitrust enforcement impacts cartels’ births and deaths. To avoid the inherent sample selection bias in prosecuted cartel studies, we use a unique dataset of Swedish legal cartels registered between 1946 and 1993. We compare estimates from a count model (considering only registered cartels) and a Hidden Markov Model (allowing for potentially unregistered cartels) to identify observed and hidden cartel dynamics. The count model suggests that strengthening antitrust enforcement has a deterrent effect, but the Hidden Markov Model suggests otherwise. Despite stricter competition laws and a credible threat of cartel prohibition, cartels continue to form, but do so undercover. Additionally, our results suggest that the strengthening of competition law has little impact on destabilizing existing cartels.","Cartel detection and deterrence are two of the main goals of competition authorities worldwide, and significant resources are devoted to them to develop various tools to detect, investigate and prosecute cartels (OECD, 2022). However, despite the strengthening of competition law, cartels continue to form.==== Furthermore, little is known about the true population and the determinants of cartels.====This paper analyzes how the legal environment impacts cartels’ birth and death. We draw on a gradual tightening of competition law, which provides a rare opportunity to study how cartels react to changes in the legal environment. We find that strengthening the competition law significantly impacted cartels’ births and deaths. As expected, stronger cartel enforcement led to fewer cartels. However, cartels continue to form undercover as the competition law becomes stricter and the threat of cartel prohibition becomes likely.====Our legal framework analysis is characterized by a gradual strengthening of antitrust enforcement. We consider three legal changes (in 1953, 1956, and 1982), each narrowing the definition of a “legal” cartel. The 1946 law, which effectively created the Swedish cartel register (SCR), was first strengthened with the 1953 Competition restraints law, establishing two new regulatory bodies, implementing a ban on bid-rigging cartels and resale price maintenance, and allowing a non-cartel firm member to report an unregistered cartel. This law was extended in 1956, notably establishing a new regulatory body and forcing firms to rewrite any “harmful” agreements previously signed. Finally, the first formal Competition law was implemented in 1982, allowing the Market Court to counteract or dissolve cartel agreements.====Our estimations are based on a new and extensive dataset covering the population of legal cartels in Sweden between 1946 and 1993 enrolled in the SCR.====We use two empirical strategies to analyze the effect of the legal environment on cartel formation and break-up. The two approaches are applied to the same data and use similar cartel birth and death definitions. We estimate a count model, which considers only registered cartels, and a Hidden Markov Model (HMM), which also accounts for any potentially unregistered cartels. The count model is a standard framework for modeling count data. It allows us to measure how the intensity of collusion varies with the legal environment. More precisely, we estimate the impact of the legal environment on the yearly number of cartels’ births and deaths.====It is, however, possible that some cartels did not register in the SCR. We, therefore, apply the HMM framework developed by Hyytinen et al. (2018b). Based on what we observe in the data, the HMM distinguishes between observed and hidden states of a market (which could be collusive or not). For a given year, the state is unknown when there is no information in our dataset that the cartel is active.==== The probability of cartels’ birth and death is then estimated separately from the likelihood of observing cartels.====The count model finds that stronger antitrust enforcement has a deterrent effect, i.e., it lowers registered cartel births. However, this deterrent effect vanishes when we allow for the possibility of incomplete data within an HMM framework. Indeed, we find the opposite (and significant) result after the 1982 Competition law: cartels continue to form but do not register.====We argue that firms may have anticipated the cartel ban, particularly after the stricter 1982 law. This result arises as registering a legal cartel involves a trade-off. On the one hand, a cartel ban will force members to dismantle the cartel if an agreement is registered. On the other hand, not registering a cartel entails the risk of being fined or reputational loss, but it allows members to continue to collude even after a cartel ban. As the competition law became stricter, the threat of a cartel ban became more credible, and the gap between the registered cartel population and the true cartel population widened. Only by comparing the estimates from the two models are we able to find this result.====On the desistance effect of competition law, our results suggest that strengthening competition law did little to destabilize existing cartels. The 1956 law led to some cartel stabilization. A similar effect was documented by Bigoni et al. (2012), who found that leniency programs have both a deterrent and a stabilizing effect on (illegal) cartels.====Taken together, these results indicate that strengthening competition law may have a limited deterrent effect. While the NegBin models the registered (compliant) cartels, the HMM estimates the overall level of cartelization in the economy by accounting for cartels that were potentially unknown to the authorities. Only by combining the two modeling approaches with different assumptions on the data’s completeness can we reach this conclusion. Our study appears to be the first to use different modeling approaches to examine the determinants of the number and probability of cartels’ births and deaths. This allows us to provide new insights into why cartels form and break apart.====This paper contributes to the literature in three ways. First, to the best of our knowledge, we are the first to provide empirical evidence on the impact of a gradual strengthening of antitrust enforcement on cartels’ births and deaths: we show that it may hinder and even destabilize some cartels, but it also leads to hidden cartel activity. Our findings complement the extensive economic literature on illegal cartels and cartel legislation.==== More significantly, we contribute to the empirical literature on the effect of legislation prohibiting collusion, which is scarce and conflicting (Levenstein and Suslow, 2006). Many previous studies focus on the National Industrial Recovery Act of 1933, which effectively legalized collusion in many industries in the US, but studies have found conflicting results on whether the law facilitated collusion (Alexander, 1994, Krepps, 1997, Bittlingmayer, 1995, Taylor, 2002).==== To overcome the usual issue of sample selection bias, we consider legal cartels and exploit that cartel enforcement changed over time, restricting the type of cartel agreements allowed.====Our paper, thus, speaks to the debate on whether antitrust enforcement can be effective at detecting and preventing collusion, where there is little evidence of efficient enforcement (e.g., Miller, 2009, Bos, Davies, Harrington, Ormosi, 2018, Marvão, Spagnolo, 2023). While the laws designed to regulate legal and illegal cartels cannot be seamlessly paralleled, one can establish a parallel between the two. The law changes discussed in this paper resonate with recent changes in antitrust law, such as the increase in the budget of competition authorities (OECD, 2022), revisions to the EC and US leniency programs (e.g., 2002, 2006, 2022), and the ECN+ directive (2019/1).====Our second contribution relates to the literature on how firms learn to collude. In our case, continuing to collude but undercover can be rational if firms have foreseen the cartel ban but want to take the opportunity to form a cartel ahead of it, as communication was still easy and less risky. This echoes the literature showing that initiating collusion takes time (e.g., Byrne and De Roos, 2019) and that coordinating on one collusive outcome requires some learning through trial-and-error (e.g., Huck et al., 2004). Nevertheless, firms would not register the cartel agreement to avoid advertising an agreement that would soon be illegal. Cases of cartels colluding undercover before the implementation of the cartel ban lend support to this interpretation. The asphalt cartel, for example, was prosecuted in 2009, and the court established that the cartel had been initiated before the 1993 cartel ban.==== The cartel was registered in 1963 and deregistered in 1975. However, the firms continued colluding secretly and were prosecuted in 2003 for illegal collusive behavior, which was ongoing until the investigation in 2001.====Third, our paper contributes to the literature on the issue of sample bias in prosecuted cartel studies, which are often hampered by the lack of data on undetected cartels.==== We address this sample bias issue in two ways. Our study period corresponds to the time when all collusive agreements were legal but had an obligation to register. As such, our dataset contains the full population of registered (legal) cartels. However, we also account for possibly unregistered cartels using an HMM framework.====Limited access to data makes the literature on legal cartels scarce. To account for incompleteness in the data, Porter (1983) and Lee and Porter (1984) introduced “hidden states” where it is unknown whether a cartel is active or not. Ellison (1994) develops these states into a model of hidden regimes with Markov transitions between collusive and non-collusive states. These studies all study a single cartel: the Joint Executive Committee railroad cartel in the US in the 1880s.====Hyytinen et al. (2018b) go further by developing a hidden Markov modeling structure to model collusion across a panel of industries. They focus on characterizing the hidden cartel dynamics and analyze the prevalence of cartels in the Finnish manufacturing industry, using data on 364 manufacturing cartels.====Instead, this paper focuses on showing how a series of antitrust enforcement changes affect the observed but also hidden cartel dynamics, using two different empirical approaches. We are also the first to apply this HMM framework to study the impact of the legal framework on cartels’ births and deaths and across all sectors==== of the economy.====Few studies have focused on Swedish legal cartels. A description of a subset of the register is provided by Fölster and Peltzman (1995), Ciarreta, 2012 and Berg (2011). These papers examine the effect of cartels and regulation on prices, output and/or productivity. Berg (2011) uses a sub-sample of 300 Swedish cartel agreements (corresponding to 9% of the cartel population) and analyzes these cartels’ institutional design and characteristics. Le Coq and Marvão (2019) describe the SCR dataset in detail and examine the survival rate of cartels, given different firm and cartel characteristics.====The SCR covers the entire Swedish economy between 1946 and 1993. Since then, globalization and technological advances have possibly changed the economy and firms’ behavior. However, cartels convicted by the European Commission and the US Department of Justice continue to occur in similar industries==== and many of the firms in the SCR continue to be fined for illegal collusive behavior.====It is worth emphasizing that legal and illegal cartels share many similarities in terms of goals, characteristics, and types of firms (see Le Coq and Marvão (2019) for an extensive analysis of this comparability),====. Therefore, our results may be relevant to current cartels and related antitrust issues.====The rest of the paper is structured as follows. We first describe the development of Swedish competition policy and the Swedish cartel register in Section 2. We describe the count and Hidden Markov models in Section 3. Our data variables are described in Section 4. Section 5 presents our results on the effect of the legal environment on cartel dynamics. Section 6 concludes.",Cartel birth and death dynamics: Empirical evidence,https://www.sciencedirect.com/science/article/pii/S0167718723000140,Available online 12 February 2023,2023,Research Article,20.0
"Muñoz-Acevedo Ángela,Grzybowski Lukasz","Institut Polytechnique de Paris, Telecom Paris, Department of Economics and Social Sciences, 19 Place Marguerite Perey, 91120 Palaiseau, France & University of Cape Town, School of Economics, Rondebosch, 7701, Cape Town, South Africa,Deloitte Finance, 6 Place de La Pyramide Tour Majunga Deloitte, 92800, Puteaux, France","Received 9 November 2021, Revised 30 January 2023, Accepted 5 February 2023, Available online 10 February 2023.",https://doi.org/10.1016/j.ijindorg.2023.102927,Cited by (0),"We use a difference-in-differences approach to assess the impact of the EU roaming regulation on mobile operators’ average revenues per user (ARPU) and the retail prices of mobile services. Our results suggest that due to the regulation the ARPU of EU mobile operators decreased since 2007 on average by 9.1%. When considering ====, the decline of ARPU is estimated on average at 5.8%, but in this case we cannot reject that there was no decrease at all. We also find that the impact of the regulation on ARPU depends on traffic imbalances, which may be related to tourism flows, and has a stronger negative impact on operators from countries with a surplus in tourism traffic. There is however no difference in the impact of the regulation on cross-country and national operators. Moreover, our results suggest that the Roam Like at Home (RLAH) regulation implemented in June 2017 had no impact on the tariffs of national mobile plans.","Roaming regulation implemented in years 2007–2017 is considered a success story in the European Union (EU).==== Back in the middle 2000s, following complaints about excessive prices for roaming services and lack of transparency in the market, the European Commission started monitoring prices and evaluating different policy options for regulation of roaming within the EU.==== The European Commission’s investigation revealed that, on average, international roaming prices were four times higher than those of national mobile calls. Such price differences could not be explained by differences in the costs of service provision. According to the European Commission’s findings, on average retail charges for a roamed call were five times higher than the actual cost of providing the wholesale service (50% higher than the average inter-operator tariffs).==== In this context, the EU roaming regulations were conceived with the objective of achieving substantial reductions in international roaming charges in the EU.====First, in 2007, the European Commission introduced regulations which capped the maximal roaming fee that mobile users had to pay for voice services as well as the wholesale tariffs for outgoing calls. In the following years, further regulations capped roaming charges for SMS and data, both at the retail and wholesale levels. Finally, in June 2017, the Commission implemented Roam Like at Home (RLAH) regulation, which equalized roaming fees for voice, SMS and data with prices that mobile users pay for these services in their home countries. Since this last regulation was implemented, users of mobile services in the EU do not need to worry about being surprised by high mobile bills after trips abroad within the European Economic Area (EEA).==== The EU roaming regulations are considered an important step towards intensifying competition in the European mobile telecommunications markets and an important effort towards the consolidation of a digital single market in the EU.====While the regulations have greatly benefited millions of travelling consumers in the EU, they do not come without important risks and challenges. In the words of the regulatory agency of the telecommunication market in the European Union, BEREC, ====.==== First, there exist diverse travel and consumption patterns across Member States, coupled with significant variations on the level of tariffs and costs of different mobile operators across Member States.==== Thus, imposing the exact same price caps across Member States, and ultimately equalizing retail roaming charges with domestic prices, could introduce distortions in national competition and challenge the sustainability of certain mobile operators. Second, there was an increase in consumers’ use of roaming services encouraged by the regulation, which increased the wholesale bill of the operators and put pressure on the network capacity.==== This increase in demand and investment needs arrives in a context where wholesale tariffs are also capped and decrease over time. Such dynamics can significantly affect the competition conditions at the national level, impose constraints on investment incentives and lead to potential strategic reactions by mobile operators. In particular, mobile operators could attempt to recoup lost revenues and profits by adjusting domestic prices upwards. In such cases, consumers who do not travel abroad would be worse off as a result of roaming regulation with higher domestic bills and no compensation through lower roaming fees.==== This unintended distributional effect was one of the main concerns expressed by BEREC during the consultation period preceding the adoption of RLAH regulation.==== Note that this distributional effect can be rationalized as a “waterbed” effect like the one discussed in the literature studying regulation of mobile termination rates.====Even though roaming regulation was a successful political move and had a great economic impact on both consumers and mobile operators, there is a shortage of rigorous empirical studies on its impact on the prices of mobile services, and the consequences for revenues and profits of mobile operators. There are only two other empirical papers on roaming regulation by Quinn et al. (2021) and Canzian et al. (2021), as discussed below. Our paper provides evidence of the effects of roaming regulation using detailed operator-level data on the average revenue per user (ARPU) and the national tariff plans of a large number of mobile operators from the OECD countries. The non-EU operators were not affected by roaming regulation and are included in this analysis as a control group.====There are reasons for such a shortage of rigorous empirical studies. First, reliable information on ARPU is not easily available. We combine such data for the years 2004–2018 for a large number of operators using different sources. Second, the pricing of mobile services is complex including a range of different services sold as bundles, which also change frequently. Likewise, international price information is not easily available and requires a substantial data collection effort. We use a large database of tariff plans from a number of operators for the years 2014–2017. Moreover, there are also substantial differences across EU countries with respect to consumption patterns of mobile services and traveling patterns, which make it difficult to assess how the benefits of roaming regulation are distributed across countries and between segments of the population within a given country.==== Due to lack of detailed information on the demand side, we are not able to assess the welfare effects of roaming regulation and how it is distributed across Member States, which is addressed in Canzian et al. (2021).====Our analysis is carried out in the following steps. First, we put together a database including information on ARPU for 111 European and non-European mobile operators from 33 OECD countries on a quarterly basis for the years 2004–2018. The data covers the period of roaming regulations implemented step-wise in the years 2007–2017. We estimate different specifications using a difference-in-differences approach, in which we assess whether roaming regulations impacted the ARPU of mobile operators in the EU. We compare the quarters before and during the roaming regulation and as a control group, we use information on several mobile operators from non-EU countries, which were not affected by the regulation. We estimate model specifications with ARPU in Euros for all countries, either using average quarterly exchange rates or purchasing power parities (PPP) conversion rates. There are some differences in the results depending on the exchange rates used. In the first model specification, we assess the impact of roaming regulation from its start in 2007 and find that it led to lower ARPU of mobile operators in the EU. The inclusion of dummy variables for mergers which took place during this period and other control variables reduces the magnitude of the negative impact. In the second model specification, we explore the effect of different phases of the regulation and find that the decrease in ARPU is gradual. We also find that the impact of the regulation on ARPU depends on traffic imbalances, which may be related to tourism flows, and has a stronger negative impact on operators from countries which have a surplus in tourism traffic. There is however no difference in the impact of the regulation between cross-country and national operators.====Next, we use the hedonic regression time-dummy approach to construct quality-adjusted price indices for mobile services. We use a large number of tariff plans on a quarterly basis between January 2014 and December 2017 for 12 mobile operators from selected OECD countries including 6 EU Member States. The pricing information is much shorter than our data on ARPU because such data is not readily available on a cross-country basis. Thus, the earlier roaming regulation was already in place for the whole period and we can only assess the impact on prices of the latest roaming regulation (RLAH regime) which took place in June 2017. In the first stage, the regressions include several variables to control for differences in the quality of mobile services offered in different countries. In particular, we create a set of dummy variables, which account for different data and minutes allowance included in tariff plans, as well as the length of the contract. We estimate the regressions for all tariff plans, including prepaid and postpaid segments. We then use time-dummy coefficients to create quality-adjusted price indices. In the second stage, we use these price indices to estimate several difference-in-differences models with non-EU countries as the control group. Our results suggest that the latest phase of the regulation had no impact on mobile operators’ tariffs, consistent with Canzian et al. (2021). Thus, the potential upward adjustment of domestic prices by mobile operators - feared by the authorities - does not seem to have taken place after the entry into force of the RLAH regime.====Our paper contributes to the following streams of literature. First, we contribute to the short empirical literature on roaming regulation. Two recent working papers mentioned above study the effects of the RLAH regulation. In the first one, Quinn et al. (2021) use individual-level data from one European mobile operator to study how RLAH impacted consumer surplus from using mobile internet. They find that there is an increase in consumer surplus when individuals use mobile internet more intensively when traveling. In the second paper, Canzian et al. (2021) evaluate the impact of the RLAH regulation on roaming traffic and revenues in the European Economic Area. They find that RLAH has substantially increased international wholesale roaming volumes and revenues with some differences across countries. Their empirical analysis is completed with a theoretical framework, which lets them conclude that gains in consumer surplus from the regulation are large and compensate for profit losses. Finally, they study whether RLAH resulted in a “waterbed” effect. In line with our findings, they do not observe increases in domestic prices as a result of the regulation. These studies are complementary to our work on four fronts. First, they focus mainly on roaming consumption (data traffic), while we study the revenues and prices of mobile operators. Second, they study specifically the effects of the RLAH regulation (implemented in 2017), while our analysis of revenues covers the whole set of roaming regulations implemented since 2007. We think that it is important to consider the entire period because roaming regulation was a process rather than one-time intervention and the adjustment of revenues and prices might have been gradual and spread over time. Third, they use different data sources and empirical strategies. Fourth, Canzian et al. (2021) can disentangle the regulations effects on wholesale and retail roaming revenue, while due to data limitations we focus on overall revenues.====Second, our paper contributes to the literature studying the impact of regulation on outcomes in telecommunications markets. Among studies on the impact of regulation on prices of telecommunications services, Genakos and Valletti (2011) analyze how the regulatory intervention to cut fixed-to-mobile (F2M) termination rates impacts mobile retail prices. Using panel data of prices and profit margins for mobile operators in more than 20 countries in a period of over six years, they find that a reduction in F2M termination rates leads to an increase in retail prices, which they call the “waterbed” effect.==== In a more recent paper by the same authors, Genakos and Valletti (2015) estimate the impact of regulation of F2M termination rates on mobile phone bills using a large panel covering 27 countries. They find that the “waterbed” phenomenon becomes insignificant on average over the 10-year period between 2002 and 2011. They argue that this is due to the changing nature of the industry, whereby mobile-to-mobile traffic surpassed fixed-to-mobile traffic.====Third, we contribute to the literature using hedonic price regressions in application to the telecommunications industry. The hedonic price model is based on the idea that any product can be viewed as a bundle of attributes. Firms and consumers trade with each other to determine the price attached to each attribute (see Griliches, 1961 and Rosen, 1974 for a formal presentation of this model in perfectly competitive framework). Hedonic price regressions were commonly used to study price changes in different industries. There is also several empirical studies for telecommunications markets including Karamti and Grzybowski (2010) and Nicolle et al. (2018) for mobile prices in France; Greenstein and McDevitt (2011) for broadband industry in the U.S.; Wallsten and Riso (2014) for broadband services in the OECD countries and Calzada and Martinez-Santos (2014) for broadband prices in 15 EU countries.====The remainder of this paper is organized as follows. Section 2 discusses the main changes in the mobile telecommunications industry in the EU and the expected impact of regulations on mobile operators’ revenues. Section 3 presents the data which we use in the estimations. Section 4 introduces the econometric framework. Section 5 presents the estimation results. Finally, Section 6 concludes.",Impact of roaming regulation on revenues and prices of mobile operators in the EU,https://www.sciencedirect.com/science/article/pii/S0167718723000097,Available online 10 February 2023,2023,Research Article,21.0
"Wong Tak-Yuen,Wong Ho-Po Crystal","Department of Quantitative Finance, National Tsing Hua University, Taiwan,Department of Economics, National Tsing Hua University, Taiwan","Received 28 May 2021, Revised 31 January 2023, Accepted 2 February 2023, Available online 10 February 2023, Version of Record 25 February 2023.",https://doi.org/10.1016/j.ijindorg.2023.102929,Cited by (0),"This paper analyzes securities auctions in which bidders have an option to acquire information after winning the right to develop a project. The payment consists of an up-front cash bid and a contingent security bid, which distorts investment and information acquisition relative to the first-best. We order securities in terms of their steepness: the payment of a steeper security is more sensitive to high project values. The agent’s incentives to acquire information that prevents either cost overruns (Type I errors) or false cancellations (Type II errors) decrease with the steepness of securities. The optimal limited-liability securities auction involves bidding debt that minimizes the distortions in the agent’s incentives to acquire performance-enhancing information. The model delivers implications on the practices commonly observed in oil lease auctions.","In many important economic environments, a seller auctions off the right to use an asset or to develop a project. For example, in oil lease auctions, the U.S. federal government sells the lease to drill oil fields, of which the payment typically comprises an up-front bonus (cash) bid and a royalty on the sales revenue from oil production. In mergers and acquisitions, the payment to a target firm often takes the form of a cash payment and the acquirer’s equity. A common feature of these examples is that the ex-post cash flows generated by the auctioned asset or project are observable and verifiable, and bidders compete by submitting security bids for which the values and actual payments are contingent on future cash flows of the asset or project.====In modern business, pre-project planning is a crucial step in project management. Gibson et al. (1995) and Gibson et al. (2006) demonstrate that an effective pre-project planning, which produces information that firms can use carefully before the final project implementation, is critical for project success. In oil lease auctions, Porter (1995) argues that post-auction information, including forecasts of oil and gas prices and drilling outcomes in the neighbor tracts, is influential for exploratory drilling decisions, as such information is a reliable predictor of tract profitability.====Motivated by the fact that pre-project planning affects future investment performances and the widespread practice that securities auctions are employed to allocate the rights to develop projects, we seek to understand the connection between security bids, post-auction investment, and information management. To the auctioneer, this linkage is particularly important and relevant. The reason is that if security designs affect information acquisition and firms’ ability to generate future revenues, then the payment mechanism would affect firms’ willingness to bid at the auction stage, and thus, the seller’s expected revenue. Secondly, such payment mechanism could also divert much of the cost of learning the expected values or net benefit of the underlying resources or project to the bidders, which could be extremely expensive, as in the case of oil drilling (Hendricks and Porter, 1996). Practically, how should a seller auction a project if delicate pre-project investigation are the keys to successful investment decisions? This paper addresses these questions by developing a tractable mechanism design model that integrates the optimal design of securities auctions and optimal contracting with information acquisition.====In the model, bidders compete for a project, and a seller auctions the right to develop the project by running a securities auction. The payment made to the seller in the securities auction contains two components: an unconditional up-front cash bid and a contingent payment for which the value is conditional on the verifiable future value generated from the project development. Before the auction takes place, the bidders possess private information on the costs of implementing the project. At the auction stage, the bidders compete by submitting security bids. Post-auction, the winning bidder observes a signal that provides an estimate of the project value. The option to acquire information appears at this stage: the winner can learn perfectly the project value at a fixed cost. Then, with or without the additional information about the project value, she has to decide whether to execute the project. Finally, the value realizes and the winner makes the contingent payment to the seller.====Without precise information about the return of the project, it is efficient to undertake the project if the signal is good. However, the signal is noisy, and thus, the additional information can improvethe post-auction investment decision. Pre-project planning prevents two types of implementation errors. Type I errors: when the signal indicates a favorable prospect of the project, it is still possible that the project value is low, and executing the project may lead to cost overruns. Type II errors: when the signal is unfavorable, it is still possible that the project turns out to be a profitable one, and project cancellation would then result in values foregone. In either case, information acquisition prevents incorrect decisions. The information that prevents Type I (II) errors is referred to Type I (II) information. Costly information acquisition does not occur when the signal is either highly favorable or unfavorable. In these extremes, cost overruns or false cancellations are not likely.====Since pre-project information helps generate a higher surplus from an investment, the seller has incentives to design an auction to encourage information acquisition. There are two key frictions that affect the optimal design of the securities auction. First, there is ex-ante adverse selection: the agents have superior knowledge of the costs of project implementation in the auction stage. Second, there is ex-post moral hazard: once the right to develop the project is in the winner’s hand, it is in the agent’s discretion to manage information and investment. Non-contractibility of the post-auction decisions and the use of contingent payment together drive a wedge between the social value and the agent’s value of information. The optimal securities and auction design must provide incentives tomitigate the ex-ante asymmetric information and ex-post moral hazard in order to help the seller maximizes her expected revenue.====The main results are as follows. First, we characterize the distortions in information acquisition and investment relative to the socially efficient benchmark with no contingent payment. Given that any contingent payment positively depends on the project value, it necessarily reduces investment incentives. However, contingent payments produce a non-monotonic effect on information acquisition. On the one side, the agent excessively acquires information that prevents Type I errors (cost overruns). By correctly calling off the project, the agent can save the additional contingent payment. Thus, the agent’s value of Type I information is higher than its social value. On the other side, the agent acquires an insufficient amount of Type II information. By correctly resuming the project, the agent will need to make the additional contingent payment. Therefore, the agent’s value of Type II information is lower than its social value.====Second, a steeper security weakens incentives for the agent to acquire both types of information. Intuitively, the steepness of a security measures the sensitivity of the contingent payment to the investment performance, and the sensitivity is more important for high-value projects as it generates a larger impact on the absolute gains. We have in mind that steep securities, even if start out low, eventually rise fast as opposed to flat securities. We capture this notion of steepness using a single-crossing relation: a security is steeper than another security if the former single crosses the latter from the bottom.==== As such, standard securities are ranked as follows: a call option is steeper than an equity, and an equity is steeper than a debt, which in turn is steeper than a cash payment. Under this ranking, we show that improving the investment decision using the additional information may mean a higher expected contingent payment from resuming the project, and thus, a lower value of Type II information. Similarly, the additional information may result in a lower expected cost saving from correctly calling off the project, and therefore, a lower value of Type I information. Consequently, the agent has weaker incentives to acquire either Type I or Type II information facing a steep contingent payment.====Finally, we derive the optimal securities auction. Since the contingent payment influences the agent’s post-auction decisions, revenue equivalence along the security design dimension fails to hold. However, type-dependent contingent payment still helps to extract information rents. Conditional on the revelation of the private costs, the seller optimally uses the flattest possible security to mitigate the distortions in investment and information acquisition. The optimal contingent payment is then an additional cash payment, with its level fully dependent on the bidder’s private cost, upon investment and is independent of the realization of the project value.====With incomplete information regarding the project value, limited liability becomes a concern. In particular, the winning bidder makes a negative profit when the project value is lower than the optimal cash payment. We then show that limited liability drives a tension between investment and information acquisition. The optimal design takes this trade-off into account, and it is optimal for the seller to ask potential developers to bid in terms of debt contracts in the auction stage. In other words, the winning bidder is made the residual claimant of the project in that her incentives for investment and information acquisition are minimally distorted away from the seller’s. Furthermore, we show that the optimal limited-liability securities auction can be implemented by a first-price or a second-price auction in which the agents bid on an ordered set of debt securities.==== Our paper contributes to the literature on securities auctions and optimal contracting with information acquisition, and we provide the first attempt to link these two lines of research. Early research on securities auctions includes Hansen (1985), Crmer (1987), Riley (1988), Rhodes-Kropf and Viswanathan (2000), and recently Abhishek et al. (2013), Deb and Mishra (2014). DeMarzo et al. (2005) discuss the revenue ranking of all security designs and show that ǣsteeperǥ security bids yield higher expected revenue in formal auctions. The reason is that the expected payment under a steeper contingent contract is more sensitive to the bidder’s private information, and thus, the seller can extract more rents from such a contract.In contrast, our model does not assume the dependence of security values on the bidder’s type, effectively removing the mechanism in DeMarzo et al. (2005) and allowing us to focus on ex-post moral hazard.====Subsequent works largely focus on other economic factors that could invalidate this revenue ranking. Che and Kim (2010) argue that when a higher project return requires a higher investment cost, steeper securities yield lower expected revenues. In Gorbenko and Malenko (2011), sellers, who compete for potential bidders, commit to flatter securities to encourage bidders to enter in their auction. Our work highlights the use of flat securities to facilitate information acquisition. Notably, Liu and Bernhardt (2021) show that a cash-plus-equity mechanism can extract full surplus in the context of merger. Their results exploits how synergy is related to bidder’s type. Indeed, the curvature of synergy affects the total values of the securities with different steepness. Our setting does not include such a component, and thus, the seller cannot extract all surplus by screening bidders using different combinations of securities with different steepness. Our paper is also related to Liu (2016), who studies the optimal design of equity auction given bidders’ heterogeneity. Bidders in our model are homogeneous and we allow for a general set of securities.====There are a few papers that study the perverse incentives for bidders’ post-auction decisions created by steep securities. Kogan and Morgan (2010) and Jun and Wolfstetter (2014) study agent’s effort decision, and Board (2007a) and Cong (2020) analyze how security design affects investment timing. Our paper adds to this last group by characterizing how security designs could affect a different type ofpost-auction moral hazard problem, that is, the acquisition of pre-project information. Thus, our study produces a novel result for revenue ranking of security designs and identifies frictions other than post-auction effort and timing decision as a driver for such a ranking.====There is a vast literature on optimal contracting with information acquisition. For example, Crmer and Khalil (1992), Lewis and Sappington (1997), Trestiege (2016) study the agent’s incentives to acquire information before she accepts the contract. We study post-contractual information management similar to Crmer et al. (1998). A closely related paper is Krhmer and Strausz (2011) who study pre-project planning in the optimal procurement contract. Our model differs from theirs in two key aspects. First, in their model, the information obtained by the agent is private information, and their optimal contract features sequential screening. Second, their model, and other works in this literature, focus exclusively on cash payment. The novelty of our paper is to introduce contingent payments in the optimal contracting with information acquisition literature.",Securities auctions with pre-project information management,https://www.sciencedirect.com/science/article/pii/S0167718723000103,10 February 2023,2023,Research Article,22.0
"Andreu Enrique,Neven Damien,Piccolo Salvatore","Compass Lexecon, Spain,Graduate Institute of International and Development Studies, Geneva, CEPR and Compass Lexecon, Belgium,Bergamo University and Compass Lexecon, Italy","Received 29 March 2022, Revised 24 January 2023, Accepted 29 January 2023, Available online 8 February 2023, Version of Record 21 February 2023.",https://doi.org/10.1016/j.ijindorg.2023.102926,Cited by (0),"We characterize the degree of price discretion that two competing manufacturers grant their retailers in a framework where demand is uncertain and privately observed by the retailers, while manufacturers only learn it probabilistically. In contrast with the consolidated vertical contracting literature, we assume that manufacturers cannot use monetary incentives to align the retailers’ incentives to pass on their unverifiable distribution costs to consumers. Our objective is to study how, in this context, an information-sharing agreement according to which manufacturers share their demand information affects prices, profits and ====. While equilibria with full price delegation never exist, regardless of whether manufacturers share information, partial delegation equilibria may exist with and without the exchange of information. These equilibria feature binding price caps (list prices) that prevent retailers from passing on their distribution costs to consumers, and are more likely to occur when manufacturers exchange demand information than when they do not share this information. Manufacturers profit from exchanging demand information when products are sufficiently differentiated, and retailers’ distribution costs are high enough. Yet, expected prices are unambiguously lower when manufacturers exchange demand information than when they don’t, making the information exchange beneficial to consumers.","Information sharing agreements are common in many prominent sectors and have been under close antitrust scrutiny on both sides of the Atlantic for many years.==== An established literature has extensively studied the competitive and welfare effects of communication in oligopoly games where firms first decide whether to share their private information (about demand or costs) and then compete à la Cournot or Bertrand (see, e.g., Kühn and Vives, 1995, and Vives, 2006, for surveys of this literature).====These models rest on the traditional hypothesis that firms are profit-maximizing black boxes and are thus silent on the interplay between firms’ internal delegation decisions and their incentives to exchange information. Yet, when firms are viewed as organizations composed of different individuals with dispersed information, responsibilities, and non-congruent interests, the allocation of decision rights becomes an integral component of their internal architecture and competitive strategies.====To investigate the interaction between information sharing and internal organization, in this paper, we study the link between manufacturers’ incentives to enter an information-sharing agreement, wherewith they exchange demand information, and the strategies according to which they delegate price authority to their downstream retailers. However, in contrast to the existing vertical contracting literature, where similar questions have been addressed under the hypothesis that monetary incentives are enforceable, we consider a competitive environment where manufacturers cannot internalize retailers’ objectives through monetary transfers but can only decide what they are entitled to do by designing permission sets from which retailers can select prices.==== Hence, manufacturers face the so-called ‘delegation dilemma’: giving up vertical control to gain flexibility, or imposing rigid rules unresponsive to changes of the environment?====We consider a stylized two-stage game, with linear demand functions, in which two upstream manufacturers compete by producing differentiated products and choose simultaneously, in the first stage, how much price authority to grant their downstream retailers who are privately informed about an aggregate, additive and binary demand shock — i.e., being closer to the final market, these retailers are better informed than manufacturers about consumers’ willingness to pay. In the second stage, after demand has been realized, retailers simultaneously set actual prices given the constraints (if any) imposed by the manufacturers in the first stage. We assume that retailers incur an observable but not verifiable distribution cost to introduce a simple wedge between upstream and downstream objectives.==== The presence of this cost, together with retailers’ private information, creates a natural misalignment of preferences that is the core of our theory, and makes it different from traditional vertical contracting models with privately informed retailers. Since by assumption manufacturers cannot internalize the non-verifiable distribution cost through monetary transfers, retailers will pass on this cost to consumers at the expense of sale volumes, profits and consumer surplus. Therefore, although retailers are better informed than manufacturers on demand and can tailor prices to this information, they tend to charge excessive prices compared to what upstream competition mandates. Yet, in contrast to other delegation models, manufacturers are not totally uninformed in our framework: with some probability, they observe the state of demand (i.e., high or low willingness to pay) and can, therefore, condition the degree of price authority granted to their retailers on this information. We study the incentives of these manufacturers to exchange demand information and examine the competitive and welfare effects of this agreement — i.e., its impact on prices, profits and consumer surplus. We show the following results.====First, we establish that full delegation never occurs in equilibrium regardless of whether manufacturers share demand information or not. That is, as long as the distribution cost is positive (even negligible), manufacturers will never allow retailers to set prices in all demand states freely. Specifically, manufacturers always have an incentive to constrain their retailers’ pricing decisions (in at least one state of nature) to prevent them from charging excessive prices compared to what upstream profit maximization requires.====Second, given that full delegation is never an equilibrium, we characterize equilibria with partial delegation in both information-sharing regimes. In these equilibria, retailers are entitled to choose their preferred price only when demand is low and are, instead, constrained to charge a price lower than what they would like in the high-demand state. Hence, partial delegation equilibria feature a price cap or, equivalently, a list price: a result broadly consistent with customized pricing with discretion (see, e.g., Phillips et al., 2021).==== As intuition suggests, partial delegation equilibria exist when distribution costs are not too high, so that the conflict of interest between manufacturers and retailers is not too pronounced. The region of parameters where partial delegation occurs in equilibrium shrinks when products become closer substitutes because retailers are more incentivized to pass on their distribution costs to consumers to shield against increased competition. On the contrary, this region of parameters expands as manufacturers’ information accuracy and the significance of demand uncertainty increase. The higher the probability that manufacturers are informed, the easier for them to sustain an equilibrium with partial delegation. Specifically, a manufacturer that expects its rival to be informed with a higher probability will be keener to delegate because it expects a lower opponent’s price, which, by strategic complementarity, also makes his retailer willing to charge a lower price, endogenously reducing its bias towards an excessive price. The opponent will, in turn, expect a lower price and accordingly reduce its price as well. Moreover, the greater the significance of demand uncertainty, the higher the cost for the manufacturers to give up flexibility and implement a rigid pricing rule (pooling) that makes prices unresponsive to demand shocks — i.e., the greater the cost of not allowing the retailers to engage in price tailoring.====Third, we show that manufacturers mutually benefit from exchanging their demand information only when products are sufficiently differentiated, when their information accuracy is high, and distribution costs are not too low. The following effects determine this result. When manufacturers share information, each learns the demand shock with greater probability than without information sharing (because under the information-sharing agreement an uninformed manufacturer learns the state of demand when the rival is informed). Hence, other things being equal, the information exchange benefits manufacturers because it mitigates the conflict of interest with their retailers. Yet, when both manufacturers are uninformed and know this is the case because the information-sharing agreement is in place, retailers charge higher prices in the low-demand state. In this state of nature, each retailer is sure that the opponent will pass on its cost to consumers and will be keener to raise its price, as opposed to the no information sharing regime where there is uncertainty on whether the rival manufacturer is informed or not. This price-increasing effect has an ambiguous impact on the manufacturers’ profits: it benefits manufacturers since it softens competition; but, when distribution costs are excessively high, there is excessive pass-through, reducing sales and thus profits. Overall, exchanging demand information benefits manufacturers if retailers’ distribution costs are not too small and products are sufficiently differentiated. In this case, solving internal agency conflicts is relatively more important than softening competition. We also show that expected prices are lower with than without information sharing. Essentially, by aligning incentives within organizations, information sharing reduces the pass-through rate according to which distribution costs are passed on to consumers. Hence, from an ex ante point of view, the exchange of information unambiguously benefits consumers.====Finally, to discuss the robustness and the limitations of these results, in the online Appendix, we also examine (pooling) equilibria in which, when uninformed, manufacturers retain full price authority — i.e., they force a singleton price irrespective of the demand state. In these equilibria, results align with the existing literature (see, e.g., Kühn and Vives, 1995 and Vives, 2006) since manufacturers de facto behave as uninformed, vertically integrated oligopolists. When a pooling equilibrium arises with and without information sharing, the exchange of information has a neutral impact on expected prices and consumer surplus. However, sharing information unambiguously benefits manufacturers because it allows them to tailor prices to demand, while increasing the extent of vertical control. By contrast, when the equilibrium features partial delegation under information sharing and pooling without information sharing, we find that consumers are hurt by information sharing. Yet, in this hybrid scenario, manufacturers will not share information when products are sufficiently differentiated and the significance of demand uncertainty is not too high. We also argue why our results remain qualitatively valid under alternative information structures, demand specifications, and organizational structures (i.e., inter-brand competition), identifying new avenues for future research whenever possible.====The rest of the paper is organized as follows. Section 2 lays down the baseline model. In Section 3, we develop two useful benchmarks and show that full delegation cannot occur in equilibrium irrespective of the information sharing regime. In Section 4, we characterize and show the existence of partial delegation equilibria with and without information sharing and then study the effect of these agreements on equilibrium prices, profits and consumer surplus. In Section 5, we discuss robustness and relate our results to the existing work. Section 6 concludes by discussing possible avenues for future research. Proofs are in the Appendix. In the online Appendix, we provide additional material and robustness checks.",Price authority and information sharing with competing supply chains,https://www.sciencedirect.com/science/article/pii/S0167718723000085,8 February 2023,2023,Research Article,23.0
"Linde Sebastian,Siebert Ralph B.","Department of Medicine, Medical College of Wisconsin, 8701 Watertown Plank Rd., Milwaukee, WI 53226, USA,Center for Advancing Population Science, Medical College of Wisconsin, 8701 Watertown Plank Rd., Milwaukee, WI 53226, USA,Purdue University, Department of Economics, Krannert School of Management, 403 West State Street, West Lafayette, IN 47907-2056, USA,CESifo, Munich, Germany","Received 30 August 2021, Revised 2 December 2022, Accepted 23 January 2023, Available online 2 February 2023, Version of Record 11 February 2023.",https://doi.org/10.1016/j.ijindorg.2023.102925,Cited by (0),"We address the question through which channels ==== create incremental value to merging firms and consider various product market and technological arguments. Based on the pairwise stable allocation concept, we estimate firms’ pair-specific (incremental) merger value functions. Our results show that technological arguments contribute to the majority of added merger value. We also find that market power arguments across multiple markets contribute to explaining incremental post-merger value. In contrast, multimarket strategic effects do not add merger value. Our estimated match values are aligned with the merging firms’ post-merger stock market performance.","Mergers have long been a popular strategy among firms, and they have become increasingly important over time, involving trillions of dollars spent on merger transactions every year.==== A well-established fact in the merger literature is that a consolidation can create additional value for merging firms (see, for example, Stigler, 1950, Williamson, 1968, Perry, Porter, 1985, Farrell, Shapiro, 1990, Hitt, Ireland, Harrison, 2001, King, et al., 2004 among many others). However, while several mergers are highly profitable for consolidating firms, other mergers are less profitable for merging firms.==== Our study examines to what extent mergers increase value for merging firms. We focus on the gains of mergers to merging firms or the extent to which merging firms gain additional profits, which characterizes their incentives to merge.==== We specifically examine through which channels mergers predominantly add value.====Seminal theoretical merger studies have explored various determinants that create additional merger value and promote firms’ incentives to merge (see Deneckere and Davidson, 1985; Perry and Porter, 1985). Studies identified product market arguments (market power effect) and technology arguments (efficiency gains and synergy effects) that can create additional firm value from merging (see Williamson, 1968). Regarding the market power argument, most mergers are formed among firms operating in multiple product markets and it appears natural for firms to sort themselves into mergers based on market power and merger value created across various product markets. Until now, little attention has been devoted to merger value creation by firms competing and operating in multiple product markets.==== Our study puts special attention to the question whether multi(product)-market firms (simply referred to as multimarket arguments from now onward) are able to further benefit from merger value gains. We also control for merger value enhancement caused by technology arguments such as firms’ synergy and innovation capabilities. The main objective of our empirical study is to test the extent to which merger value is raised by various multimarket and technology arguments. We establish variables that relate to various multimarket arguments and empirically test to what extent they determine incremental merger value.====Previous merger studies have shown that a key motivation for firms to engage in mergers is the internalization of competitive externalities which allows firms to increase prices and create incremental merger value, often referred to as the market power effect (see Stigler, 1950, Salant, Switzer, Reynolds, 1983, Farrell, Shapiro, 1990). The extent to which the market power effect adds value to mergers depends on firm and product market attributes. For example, mergers among larger firms operating in markets with more inelastic demands result in larger market power effects (see Farrell and Shapiro, 1990; Froeb and Werden, 1998) and this increases merger value. Market power effects scale in the number of product markets that merging firms compete in. Hence, firms competing in multiple product markets may realize higher market power effects and generate higher merger value. The market power effect and the extent to which competitive externalities can be internalized through mergers will likely be augmented in the number of markets that merging firms operate in; this defines the ====.====We consider a further argument that relates to the creation of post-merger efficiency gains, which are realized by production rationalization and economies of scale. For example, mergers have the opportunity to reallocate production units to more efficient units to achieve efficiency gains (see Bergstrom and Varian, 1985; Salant and Shaffer, 1999; Roeller et al., 2007). In the multimarket merger context, it is reasonable to expect that merger value is composed of the sum of efficiency gains across markets, and this defines the ====.====A third argument relates to firms’ multimarket presence in product markets that can exert several strategic implications on firms’ behavior. Firms competing against each other in multiple markets can refrain from engaging in aggressive pricing behavior in one market to avoid aggressive responses in other mutual markets, also referred to as “coordinated effects”, or “mutual forbearance” (see Bernheim and Whinston, 1990). Hence, firm contact across multiple markets weakens competition since firms avoid retaliation actions across multiple markets. In the context of mergers, multimarket contact among merging firms can alter firms’ behavior in markets and have ambiguous effects on merger value. Some studies show it can result in higher merger value (see Kim and Singal, 1993; Hughes and Oughton, 1993; Evans and Kessides, 1994) while other studies show that it can reduce merger value (see, for example, Ciliberto and Williams, 2014). The strategic merger argument depends on the degree of firm interactions across markets, and it defines the ====.====We also control for technology attributes that control for merger value stemming from efficiency and synergy effects and are independent of product market interactions. Merger-specific cost efficiencies could be caused by the unification of knowledge, innovation, or other technological complementarities, see also Ravenscraft and Scherer (1989) and Farrell and Shapiro (1990). For example, mergers between firms with closely related technologies have further potential to add merger value from generating post-merger synergy effects. Moreover, highly innovative firms have better opportunities to absorb external knowledge and are able to further benefit from post-merger efficiency effects and incremental merger value.====Our study uses a comprehensive dataset on the semiconductor industry. The semiconductor industry and the dataset is particularly useful for our purposes since a large number of firms are present in multiple product submarkets (such as dynamic random access memories, static random access memories, flash memories, etc.). Varying product market activity levels across firms result in differential potential merger gains across merging firm-pairs originated by the multimarket arguments introduced above.====The dataset comprises detailed information on firm-level production across multiple submarkets within the semiconductor industry, patents, and mergers. We test the extent to which above mentioned multimarket and technology arguments create incremental merger value and therefore increase firms’ merger incentives. One challenge in merger studies is that the formation of mergers is endogenous since firms sort into mergers with other firms depending on their attributes and their expected merger gains. Hence, merger decisions and merger gains can be related to firm attributes. We deal with this endogeneity problem by using a matching framework and by considering firms’ sorting patterns into mergers (i.e., who merges with whom) to determine heterogeneous merger values and arguments that drive this additional merger value.====The empirical model builds on the notion of a merger as a bilateral agreement between two firms. The model relates to the classic (one-sided) roommates problem (see Gale, Shapley, 1962, Roth, Sotomayor, 1990, Roth, Sotomayor, 1992). Based on the pairwise stable allocation concept, we estimate firms’ pair-specific merger value functions. The estimation procedure adopts a semiparametric maximum score estimation technique following Manski (1975) and Fox, 2010, Fox, 2018.====The estimation results of firms’ pair-specific merger value functions show that incremental merger values vary substantially across merging firms. Our estimation results provide evidence that firms match into merger pairs based on multimarket and technological relatedness arguments. We find that technology market arguments explain (on average) 81 percent of the incremental merger value. Multimarket arguments contribute about 17 percent to the added merger value. Our results show that multimarket strategic arguments add no incremental merger values once the remaining arguments are considered. Overall, our findings highlight the importance of technological and multimarket power arguments when evaluating merger values. We also find that our estimated pair-specific incremental merger values are positively correlated with the acquiring firms’ post-merger stock market performance. This result supports that the merger value determinants in our model capture large parts of the expected merger synergies that are realized in the market.====Our study is related to several strands of literature. The first literature strand relates to the market power effects and the internalization of competitive externalities. Several theoretical studies show that mergers among larger firms operating in more inelastic markets are more profitable (see Salant, Switzer, Reynolds, 1983, Farrell, Shapiro, 1990, among others). Several merger studies provided evidence that market power has been an dominant force in a number of markets, see Weinberg (2008) and Duso et al. (2013). Relatedly, Gugler and Szuecs (2016) find that the return on assets of rival firms increases significantly after a merger. Siebert (2022) shows that merging firms realize substantial heterogeneous post-merger effects on production and prices depending on firm efficiencies and price elasticities.====The second strand of literature relates to post-merger efficiency gains that play a relevant role in merger formation and merger value creation. For further information on efficiency gains in horizontal mergers and homogeneous product markets, see also Jovanovic and Wey (2012), Motta (2004), and Whinston (2007). Several merger studies evaluate the trade-off between market power and efficiency gains, see Williamson (1968). Efficiency-dominated mergers are beneficial to producer and consumer surplus, as output units are transferred from less efficient to more efficient production facilities (see Salant and Shaffer, 1998, 1999).====The third strand of literature relates to firms competing in multiple product markets (====). Edwards (1955) proposed the idea of “mutual forbearance”, that is, multimarket contact weakens competition since firms avoid retaliation actions across markets. The majority of empirical studies finds that repeated firm contact across markets weakens product market competition and enables firms to sustain higher levels of profits and prices (see, e.g., Busse, 2000, Parker, Roller, 1997 on the telecommunications industry, Evans, Kessides, 1994, Singal, 1996; Ciliberto and Williams, 2014 on the airline industry, and Heggestad and Rhoades, 1978; Rhoades and Heggestad, 1985 on the banking industry). In the context of multimarket mergers, Kim and Singal (1993) contend that merging firms competing in several markets can facilitate coordination effects and lead to higher merger value. Ciliberto and Williams (2014) remark that multimarket mergers can result in lower prices (and lower merger value) if firms’ coordinated behavior already existed premerger. Similarly, if a merger is formed among multimarket firms, these firms will forego the possibility to engage in tacit collusion or mutual forbearance practices with the merging partner, which might then reduce the value of the merger.====The fourth strand of literature relates to technological arguments and synergy effects. Mergers allow merging firms to learn from each other and achieve further efficiency and synergy effects (see e.g., Bena, Li, 2014, Focarelli, Panetta, 2003; Harrison et al., 1991; Hitt, Harrison, et al., 1998, Larsson, Finkelstein, 1999, Datta, Pinches, Narayanan, 1992, Ramaswamy, 1997, Sheen, 2014, Shelton, 1988, Singh, Montgomery, 1987). Ornaghi (2009) considers mergers in the pharmaceutical industry and finds that merged companies reduce their R&D activities compared to non-merging firms. He also finds that higher levels of technological relatedness are not associated with better R&D outcomes. Szuecs (2014) finds that acquiring and target firms in a merger reduce R&D intensities.====A fifth strand of literature relates to the semiconductor market. Cabral and Leiblein (2001) state: “While new process technology can be used in the production of different types of semiconductors (i.e., memories, microprocessors, analogue integrated circuits, etc.) these submarkets have different market structures. For instance, while memory devices are frequently considered as commodities, there is significant product differentiation within the microprocessor industry.” The merger study by Gugler and Siebert (2007) find that mergers achieve dominant efficiency gains. The merger study by Harris and Siebert (2017) show that firm-specific discount factors have an effect on merger formation and merger outcomes. They also show that highly efficient firms merge with other firms that are highly efficient and innovative.====Finally, a growing literature addresses the empirical estimation of mergers using a matching framework. This body of work has drawn on the empirical work by Sorensen (2007) and Fox (2018) and applied it to the study of mergers between mutual fund management companies (Park, 2013) and banks (Akkus et al., 2016).==== In our work, we follow the approach by Fox (2018), as is also done by Fox and Bajari (2013) and Baccara et al. (2012), that does not require transfer data for the purpose of estimation. We employ a one-sided matching setup where any set of firms can choose to merge with any other firm.====To the best of our knowledge, little is known about whether merger value creation is mostly stemming from product market or technology market and efficiency arguments. Moreover, we are not aware of merger studies that consider several multi(product)market arguments including multimarket strategic, multimarket power, and multimarket efficiency arguments.====The remainder of this paper is organized as follows: Section 2 presents our basic model and our main hypotheses. Section 3 outlines the empirical matching model. Section 4 describes the industry and data sources, outlines our variable definitions, and presents data descriptives. We report our results in Section 5, and we conclude in Section 6.",Exploring the incremental merger value from multimarket and technology arguments,https://www.sciencedirect.com/science/article/pii/S0167718723000073,2 February 2023,2023,Research Article,24.0
Clavorà Braulin Francesco,"ZEW – Leibniz Centre for European Economic Research, L7, 1 68161 Mannheim, Baden-Württemberg Germany","Received 29 July 2021, Revised 16 January 2023, Accepted 22 January 2023, Available online 25 January 2023, Version of Record 3 February 2023.",https://doi.org/10.1016/j.ijindorg.2023.102923,Cited by (1),"This article studies the effects of consumer information on the intensity of competition. In a two dimensional ==== model of horizontal product differentiation, firms use consumer information to price discriminate. I contrast a full privacy and a no privacy benchmark with intermediate regimes in which the firms can profile consumers only partially. I show that with partial privacy firms are always better-off with price discrimination: the relationship between information and profits is hump-shaped. In particular, competing firms prefer to target consumers with partial but asymmetric information about preferences. Instead, consumers prefer either no or full privacy in aggregate, but the effects of information on individual surplus are ambiguous: there are always winners and losers. Finally, I study the information acquisition incentives of the firms when there is an external data seller. When this upstream data broker holds partially informative data, an exclusive allocation arises. Instead, when data is fully informative, each competitor acquires consumer data but on a different dimension. These findings are relevant for the strategic decisions of firms active in digital markets and contribute to the policy debate surrounding privacy, exclusive access to data and competition.","Consumer data collection in digital markets is pervasive as data is a crucial asset for many businesses. Data is highly valuable when it allows firms to change strategies in a profitable way and it opens up the possibility of making personalized offers based on user characteristics (Stucke, 2018). Firms can provide customized services and personalized recommendations, deliver targeted advertising or even personalize prices shown to consumers. Clearly, firms need information about preferences to implement sophisticated pricing strategies: they collect consumer data by themselves or acquire it from data brokers (Montes et al., 2018).====Focusing on price personalization, this article studies firms which know something but not everything about consumer preferences: a regulation may prevent them from exploiting complete consumer profiles by requesting anonymisation, or retailers may not be able to infer perfectly each consumer’s willingness to pay. I employ a model of horizontal product differentiation with two dimensional consumer information (Liu and Shuai, 2013) to investigate how different privacy regimes impact competition, showing that partial price targeting ==== decreases competitive pressure. The main novelties are a comprehensive analysis of all cases in which firms have granular data on none, only one, or both data dimensions, and the characterization of a data seller’s incentives to trade data with firms in this multi-dimensional context.====The addressed research questions are: which privacy regimes relax competition and increase industry profits? Which data strategy ensures that a firm does not lag behind its competitor? How much and what type of data does a business need to maintain a strategic advantage in the market? Then, introducing data sales: should a revenue-maximizing data owner sell data exclusively? If not, should this broker trade similar or different type of data with the retailers?====Hence, the main focus of the article is on firms that observe only certain consumer characteristics, correlated with the willingness to pay of each consumer. Other relevant variables may not be observed at all. Consider a situation in which two attributes, age and location, entirely capture consumers’ willingness to pay, but only one of these characteristics is easily observed: individual demographics appear in public records or are even self-declared, whereas information on location can be tracked via the GPS or the IP address of the user’s device but it could be concealed, at least by some customers.====Alternatively, consider two outlets selling technological products: the first specialized in Apple devices while the second one in Windows personal computers. A data broker tracking the technical characteristics of users’ smartphones is likely to infer consumers’ brand preferences: an iPhone user could more likely buy another Apple product, and knowing it is valuable to the retailers. On the other hand, preferences for a smaller sized but portable laptop or for a larger screen but heavier product may be not so easily observable.====There is evidence, although limited====, of price discrimination in online markets (Hindermann, 2018). Consumer targeting revolves around user-based, technical and location-based features. Hannak et al. (2014) find limited evidence of price discrimination in the hotel sector, and Hupperich et al. (2018) find some evidence also in the rental car sector. Lewis (2021) identifies discrimination in the airline industry, showing that targeted discounts are often offered when firms provide differentiated products. Relying on experimental data, Dube and Misra (2017) show that profits always increase when machine learning techniques are used to target prices. In a competitive setting, Dube et al. (2017) find that profitability of price discrimination crucially depends on whether the competitor’s price response is symmetric or not.====Concerning data sources, the role of data collectors and sellers, generally named data brokers, is far-reaching in the markets for information. Access to data increases the ability of firms to segment customers, but sellers could discriminate against data buyers on the basis of their willingness to pay for data (Pancras and Sudhir, 2007). Their strategic behavior and exclusive data access may harm consumers and the society as a whole (Duch-Brown et al., 2017).====This article contributes to the debate on price discrimination, privacy and data sales in competitive markets, arguing that the effects of information on competition depend on the ==== of data available to the retailers. With some notable exceptions reviewed below, the prevalent idea is that more information intensifies competition and benefits consumers. Therefore, a privacy enforcement by means of a ban on price discrimination could backfire if competitive forces work well. However, in a strategic setting, it is not obvious what “more” information really means. Assuming that consumer preferences are captured by two attributes, this approach delivers some managerial implications for deciding what type of data to acquire on the market.====My first contribution is to show that exploitation of partial information on consumer preferences (i.e. partial privacy) ==== generates a redistribution of surplus from consumers to firms. The relationship between information and industry profits is hump-shaped. The inability to observe one consumers’ attribute is key, as the standard argument of Bertrand competition in transport costs that holds with no privacy breaks down under partial privacy.====When only one dimension is known, consumers are correctly and oppositely ranked, but only with respect to that piece of information. Firms identify a continuum of consumers rather than single users, and while some of them have a better match with the rival in the unobserved dimension, the competitor is unable to steal all these consumers even with aggressive discounts. Differently from a one dimensional model in which a firm pricing too high would immediately lose all consumers targeted by the rival with suitable discounts, here the rival cannot be as aggressive as it would be necessary to reap all the other firm’s loyal customers, as there will always be users not willing to switch. Thus, compared to a scenario without information, each firm can safely set higher prices to all customers close in the observed dimension, which more than offsets any eventual business stealing effect, rising industry profits.====The second contribution is to show that when the data allocation is induced by an upstream seller, data is awarded exclusively only if the data broker has information on a single dimension. If the dataset is fully informative, the incentive is to sell information non exclusively, awarding different data dimensions to different competitors. This asymmetric equilibrium survives when firms are allowed to gather data by themselves, but only if collection costs are low.====Consumers prefer no privacy in aggregate but, if individual targeting is unfeasible, full privacy should be welcomed as a second best solution: intermediate levels of privacy are always detrimental to consumers. In this context, the conclusion is that privacy protection should be made either very hard or very easy. However, I also find that the impact of data exploitation on individual consumer surplus is ambiguous across different regimes: some consumers are better-off but others are always made worse-off, even under no privacy.==== This article contributes to two main strands of the literature. Firstly, my work builds on models of price personalization in spatial competitive settings. Behavioral-based price discrimination models are a building block of this literature and focus on firms’ dynamic incentives to collect and exploit consumer data (Villas-Boas, 1999, Fudenberg, Tirole, 2000, Choe, Matsushima, Tremblay, 2022). In static settings, it has been widely shown that if all firms personalize prices in models characterized by best-response asymmetry (Corts, 1998), price competition is strengthened. Under full market coverage, industry profits decrease below the no information level due to discounts targeted to all consumers (Thisse, Vives, 1988, Cooper, Froeb, O’Brien, Tschantz, 2005, Armstrong, 2006).====More recently, this result was revisited in the context of the current privacy debate in digital markets.==== Taylor and Wagman (2014) show that no privacy is detrimental to the firms and beneficial to all consumers. The intuition that more information should be welcomed because consumers are better-off without privacy is confirmed by Belleflamme and Vergote (2016) even under monopoly, when buyers can anonymise themselves incurring a privacy cost. However, in a similar setting, Conitzer et al. (2012) show that more privacy can benefit consumers even when anonymisation is costly, although only up to a threshold.====Some notable exceptions exist also in competitive settings, and my article fits into this line of research. By relaxing full market coverage, Rhodes and Zhou (2022) show that competitive personalized pricing can benefit firms. When firms can address individual consumers with pinpoint discounts but customers may eventually decide whether to opt in or out, Anderson et al. (2022) show that targeting may be advantageous to firms. Active disclosure of information is key also in Casadesus-Masanell and Hervas-Drane (2015). They show that privacy policies may be effectively used to soften competition. When consumers are active in their data management, Chen et al. (2020) find that these practices may backfire at the benefit of competing firms.====In this paper consumers do not have an active role and there is full market coverage. Rather, I mainly focus on the impact of symmetric or asymmetric data allocations on targeting profitability. Exclusive access to data has been show to play a significant role in boosting industry profits (Gu et al., 2019). More generally, privacy is investigated also in consumer search models (Shy and Stenbacka, 2015) and online advertising (de Cornière and de Nijs, 2016).====Marotta et al. (2021) consider heterogeneous consumers in a horizontal and a vertical dimension to study the impact of alternative information regimes on competition in the online advertising market. Their setup is richer than the one analyzed here, as they allow for vertical differentiation. However, they study only symmetric regimes, whereas I also allow for asymmetric scenarios. Moreover, in Marotta et al. (2021) firms are not necessarily competing in the product market but rather for consumer attention, with customers end up seeing one advertisement at a time. Information can improve matching and expand supply, as well as allow the firm to target the price linked to the advertisement. In my paper, consumers are aware of both offers and can switch between firms. Information can either strengthen price competition at the product market level or relax it. They find that industry profits are maximized when advertisers have full information, whereas here that scenario benefits consumers.====In the context of personalized prices, this paper is methodologically close to Baye et al. (2018), Esteves (2009) and Liu and Shuai (2013). The first article, which builds on Jentzsch et al. (2013), proposes a two period model where consumers differ in their geographical position and flexibility in transport costs. Locations are perfectly observed but individual disutility costs are unknown. However, first period purchases are imperfectly informative about consumer flexibility. They show that firms can be better-off by combining location information with behavioral data for price discrimination purposes when consumers are moderately heterogeneous in flexibility. Here, the type of available information is different: transport costs are homogeneous across consumers, but outlets rank them differently with respect to both dimensions. Moreover, firms are allowed to hold both symmetric and asymmetric data about consumers.====In a two dimensional model with transport cost heterogeneity along different dimensions, Esteves (2009) investigates a scenario in which only one dimension is perfectly observed by both firms, whereas the second one is never known, showing that symmetric partial price discrimination rises profits. Here, firms perfectly identify consumers also along different dimensions, not necessarily the same one. Indeed, this article is closer to Liu and Shuai (2013). They study a two dimensional model in which firms can segment consumers in two groups along each dimension. They confirm the result of Esteves (2009), but additionally find that firms are worse-off with partial information on different dimensions. Here, instead, firms do not categorize consumers in groups. When firms observe a certain dimension, they get perfect information on it. Differently from Liu and Shuai (2013), firms are never worse-off under partial price discrimination. Moreover, this paper extends to two dimensional settings the current literature on data sales.====There is indeed a growing literature exploring the incentives of data brokers to sell one dimensional data to retailers. Some papers assume that, once information is collected into a dataset, the data broker can sell it only as a unique block. In a horizontally differentiated duopoly, Montes et al. (2018) show that an upstream seller has an incentive to sell data exclusively to one retailer. Kim et al. (2018) find exclusivity even when the downstream market is a triopoly, and Clavorà Braulin and Valletti (2016) provide a similar conclusion in a vertical differentiation duopoly.====Other works assume that the seller can partition the dataset prior to the sale stage. When a unit line can be partitioned into intervals of arbitrary sizes, Bounié et al. (2021) show that a data broker allows each firm to only identify its own high valuation consumers. My paper takes an intermediate approach: it adds a second dimension of information allowing the seller to split the dataset along different dimensions. Belleflamme et al. (2020) show that a broker always has an incentive to provide a tracking technology to both downstream firms, letting them identify consumers with asymmetric precision. This result echoes my equilibrium with downstream partial data differentiation. In their case the product is homogeneous whereas here there is two dimensional differentiation. Moreover, they focus on the precision of the tracking device or of the shared data, rather than the type of personal information held by the firms.====Data-driven vertical integration between a platform and one downstream firm is studied by Kim (2021) in a market populated by sellers endowed with asymmetric targeting technologies, resulting in exclusive data usage by the incumbent. Finally, Gu et al. (2021) focus on competitive upstream market structures, investigating the incentives of data brokers to merge their data sources, which is likely to happen when information is sub-additive rather than super-additive.====In what follows I firstly set up the model and the two extreme benchmark cases. In Section 3, I solve for the relevant games with partial privacy and I investigate the relationship between partial privacy and industry profits. In Section 4, I extend the analysis to exclusive data allocations in presence of full information. Finally, in Section 5, I characterize the optimal selling strategy of a monopolistic data broker. Moreover, I study firms’ incentives to autonomously collect consumer data.",The effects of personal information on competition: Consumer privacy and partial price discrimination,https://www.sciencedirect.com/science/article/pii/S016771872300005X,25 January 2023,2023,Research Article,25.0
"Sun Hailin,Yu Jun","School of Economics, Renmin University of China; Center for Digital Economy Research, Renmin University of China; Research Institute of State-owned Economy, Renmin University of China, Beijing, 100872, China,Institute for Social Governance and Development Research, Tsinghua University, 14F, #2 Shuangqing Building, 77 Shuangqing Road, Beijing, 100085, China,School of Economics, Shanghai University of Finance and Economics; Key Laboratory of Mathematical Economics (SUFE), Ministry of Education, 777 Guoding Road, Shanghai 200433, China","Received 4 August 2020, Revised 16 January 2023, Accepted 23 January 2023, Available online 25 January 2023, Version of Record 31 January 2023.",https://doi.org/10.1016/j.ijindorg.2023.102924,Cited by (0),"This paper examines how an online publisher utilizes its information about consumer preference to target advertising. In our model, two firms first bid for a prominent ad position in a publisher-organized position auction, and then compete on price in the subsequent product marketplace. We consider two dimensions of consumer heterogeneity. First, consumers are heterogeneous in product preference. Based on their tastes, some consumers prefer one product over the other, whereas other consumers may rank the products in an opposite order. Second, consumers differ in search preference, i.e., “nonshoppers” only consider the advertised product, while “shoppers” always search both firms’ products before buying. We show that targeted advertising based on product preference will mitigate price competition in product markets as well as competition in position auctions, the latter to the detriment of the publisher. In contrast, targeted advertising based on search preference always benefits the publisher, as the winning firm can charge monopoly prices to nonshoppers. We show that the publisher’s optimal choice is to utilize only the information about consumer search preference. We also explore the welfare implications of targeted advertising based on different types of consumer preference.","Search advertising, also known as a position auction, allows advertisers to submit bids to online publishers on specific keywords in order for their ads to appear prominently in the search results. Online publishers use a combination of submitted bids and ad relevance to rank the ads. Search advertising is considered as one of the most effective forms of online advertising because it is close to the user’s buying decision and matches it to the user’s stated information needs. Online publishers, such as search engines, social media and e-commerce marketplaces have succeeded in generating significant revenues from search ads. According to the financial reports of these companies, Google, Facebook, and Amazon generated advertising revenues of $209.49 billion, $115 billion, and $31.16 billion respectively in 2021, with paid search ads comprising the bulk of this revenue.====Given the success of this business model in practice, a range of related literature has theoretically investigated the bidding strategies of advertisers and the revenues of online publishers, as well as the impact of online advertising on consumer surplus and social welfare (Varian, 2007, Chen, He, 2011, Athey, Ellison, 2011, Xu, Chen, Whinston, 2011). Most studies have considered uniform search advertising, in which all consumers receive the same ad given by a position auction. However, with the rise of big data and artificial intelligence, online publishers have been able to collect and process unprecedented amounts of consumer-level data, such as consumers’ browsing behavior, their profiles on company sites, shopping habits and other basic demographic information, which online publishers can use to tailor ads to consumers’ interests and specific traits.==== Targeted advertising is important as it increases brand awareness and the engagement rate with consumers. For example, Google allows all advertisers to run interest-based ads on its display network (Bazilian, 2011). Advertisers on Facebook can use consumers’ demographic information, social activities on the platform, as well as social networks to target their potential customer base.==== Amazon’s sponsored ads can also be targeted to individual products, product categories or buyer interest.====This paper incorporates targeted advertising into the traditional position auction model and examines whether and how online publishers should use consumer data for targeted advertising. We also examine how targeted advertising changes firms’ bidding strategies in position auctions and their pricing strategies in product markets, and whether targeted advertising has a positive impact on welfare.====The existence of consumer heterogeneity is crucial for online publishers to develop targeted advertising. This paper considers two types of heterogeneity. First, consumers are heterogeneous in product preference. Based on their tastes, some consumers prefer one product over the other, whereas other consumers may rank the products in an opposite order. Second, consumers differ in search preference, i.e., some are experienced online shoppers, who prefer to surf and find the best deals (Childers et al., 2001), while others may be constrained by time or skills and therefore search only a limited number of advertisers.====In some cases, data on online activity can only support online publishers in deriving a consumer’s product preference. For example, an e-commerce marketplace can only track a user’s product purchase history, i.e., which products he/she has purchased, but not his/her browsing behavior, i.e., how many products he/she searched for before making a final purchase decision.==== Another example is that a consumer’s geographical location derived from his/her IP address is useful for inferring their product preferences, e.g. people living in Sichuan province in China generally prefer spicy food, while those living in Jiangsu province prefer sweet food. However, this type of data provides little information about consumers’ search preferences, as consumers in one province are unlikely to search more frequently than those in another province on a systematic basis. Thus, in this case, online publishers can target their ads based only on product preference (TP).====In other cases, online publishers can infer both product and search preferences. Cookies and user accounts profiles enable online publishers to track consumer browsing history,==== time spent on each clicked content, and search habits while shopping, allowing publishers to accurately infer a consumer’s search preference. In this case, online publishers can deliver targeted advertising based on product preference (TP), based on search preference (TS) or both (T2). Our study encompasses these two dimensions of consumer heterogeneity and discusses the effects of different targeting techniques.====In our model, we consider an online publisher, two firms ==== and ====, and a continuum of consumers with measure one. The online publisher allows the two firms to compete for a prominent ad position through a second-price position auction. The product of the firm that wins the prominent ad position will be advertised and thus first noticed by each consumer. After the position auction, firms compete on price in the subsequent product market. Consumers differ in their product preferences. A type ==== consumer prefers product ==== to product ==== in the sense that product ==== is more likely to suit her taste than product ====. We assume that firms are asymmetric, with firm ==== stronger than firm ====, i.e., the proportion of type ==== consumers is higher than the proportion of type ==== consumers. Consumers also differ in their search preferences. Similar to Varian (1980), we assume that shoppers always search for both products, whereas nonshoppers search only for the product that is prominently displayed.====Targeting techniques allow the online publisher to divide the whole market into different submarkets, based on consumers’ preference types. The online publisher can thus choose between uniform advertising (UA) and targeted advertising. Under UA, no targeting techniques are implemented and the prominent ad position is for the entire market. This means, the product of the firm that wins the position auction will appear prominently to all types of consumers. On the other hand, under targeted advertising, a position auction is organized in each submarket, which generates an advertised product (i.e. a prominent product) in that submarket. A firm may lose the position auction in one submarket, but win elsewhere. This means that the advertised products need not be the same for all the submarkets.====Existing literature on targeted advertising and targeted pricing focuses on the case where the online publisher can only identify consumers’ product preferences and thus choose between UA and TP. In our model, the publisher has data on two dimensions of consumer preferences, and can choose among four types of targeting techniques: UA, TP, TS and T2. To the best of our knowledge, this situation has never been studied in the literature.====In the benchmark model, we focus on the case where firms can charge targeted prices based on targeted advertising, i.e., where firms charge different prices in different submarkets. Our results show that different targeting techniques have qualitatively different effects on final equilibrium and welfare. Compared to UA, TP changes the advertisements that appear prominently to consumers of type ====, while TS does not change the advertisements to consumers. Under UA, the strong firm ==== appears prominently to all consumers. Under TP, each firm wins its strong market, i.e., firm ==== wins prominence to type ==== consumers, for ====. Under TS, the strong firm ==== still wins prominence toward nonshoppers, while prominence toward shoppers is irrelevant, as they always search for both products.====TP generates two different effects on the publisher’s auction revenue. On the one hand, the implementation of TP increases the asymmetry of the firms and thus reduces price competition in the product market, which increases firms’ incentives to bid for the prominent ad position. On the other hand, the increase in asymmetry also reduced competition between firms in the position auction. TP benefits the online publisher if and only if the former effect dominates, which occurs when there are a sufficient number of shoppers. Because TP reduces price competition between firms in the product market, it increases industry profits and reduces consumer surplus. In addition, TP displays consumers’ favorite products in their prominent advertising slots, which increases the likelihood of trade and overall social welfare.====The implementation of TS has a different effect on firms’ pricing strategies in the product market and their bidding strategies in the position auction. In the position auction, the prominent position facing shoppers generates no value to firms as shoppers always search for both products. However, the prominent position in the submarket of nonshoppers generates a higher value because the prominent firm can charge monopoly prices and earn monopoly profits given that nonshoppers only visit the prominent position. We show that the increase in auction revenue from the nonshopper submarket outweighs the loss from the shopper submarket, so that TS always benefits online publishers. In the product market, TS intensifies firms’ price competition for shoppers, but mitigates competition for nonshoppers. Intuitively, shoppers search more than nonshoppers, so firms have less monopoly power over shoppers. We show that the fall in the average price for shoppers outweights the rise in that for nonshoppers, and therefore TS always reduces industry profits and increases consumer surplus. Furthermore, TS does not change the product displayed in the prominent position for each consumer, and therefore does not affect the likelihood of trade or overall social welfare.====Our results show that, compared to T2, the publisher’s total auction revenue is higher under TS. This means that the publisher may not prefer to apply targeted advertising by using all the information available. Specifically, data on consumer product preference should never be used whenever data on consumer search preference is available. We compare T2 with TS by asking the question that whether the publisher should additionally conduct TP when TS has already been implemented. As mentioned before, the positive effect of applying TP is that price competition in the product market is moderated and therefore the valuation of the prominent position is increased for both firms. However, under TS, for the submarkets of shoppers, the position auctions always generate a zero revenue, regardless of whether TP is employed or not. Thus, the effect of additionally implementing TP lies only in the submarkets of nonshoppers. Note that TS has already reduced price competition in the submarkets of nonshoppers to the greatest extent – the prominent firms are able to charge monopoly prices in the product markets. Thus, with the presence of TS, the positive effect of TP is greatly weakened and therefore additionally implementing TP is detrimental to the online publisher.====We further examine the case where price discrimination is not allowed. Thus, consumers in all submarkets must receive the same price for each product. A somewhat surprising result is that, in the absence of price discrimination, TS has no effect on equilibrium and is arguably meaningless. Thus, TP is the only relevant targeting technique. The reason is that TS does not change the advertisements displayed to consumers and, therefore, the demand functions of the firms remain unchanged regardless of whether the publisher implements TS or not. Additionally, we show that, when price discrimination is prohibited, UA always generates higher auction payoffs to the publishers than TP does. This is because TP has a limited moderating effect on price competition in product markets in the absence of price discrimination. Our results show that in the absence of price discrimination, publishers do not implement targeted advertising in any form.====The rest of the paper is organized as follows. We review the relevant literature in Section 2 and present the formal model in Section 3. In Section 4, we address the market equilibrium for each targeting technique, based on which we derive firms’ bidding strategies in the position auctions. We then compare the online publisher’s auction revenues, equilibrium prices, industry profits and consumer surplus for different targeting techniques in Section 5. We examine the case where price discrimination is not permitted in Section 6, and discuss policy implications in Section 7. Section 8 provides extensions, and Section 9 concludes the paper. All omitted proofs are provided in the Appendix.",Competitive targeted online advertising,https://www.sciencedirect.com/science/article/pii/S0167718723000061,25 January 2023,2023,Research Article,26.0
Yin Nina,"China Center for Human Capital and Labor Market Research, Central University of Finance and Economics, China","Received 17 June 2021, Revised 23 December 2022, Accepted 28 December 2022, Available online 23 January 2023, Version of Record 21 February 2023.",https://doi.org/10.1016/j.ijindorg.2023.102922,Cited by (0)," policy and regulatory treatment of incremental innovation. To shed light on this problem, I focus on incremental innovations in selective serotonin reuptake inhibitor (SSRI) anti-depressant drugs. Based on individual-level prescription data, I conduct a structural estimation and found that the consumer surplus losses due to market exclusivity extensions far exceed the consumer surplus benefits from incremental innovation. Without considering innovation costs, the benefits to innovators from incremental innovations ranged from $660 million to $2.16 billion even in the absence of market exclusivity. Evidence indicates that the market exclusivity granted for incremental innovations in SSRIs resulted in a loss of social welfare of between $4.78 billion and $11.68 billion over the period 1996–2011. The result suggests that policy makers may need to revisit the provisions for granting market exclusivity to incremental innovations.","Many accuse the pharmaceutical industry of “ever-greening” or extending patent protection by introducing new products that are slight improvements over older drugs. An oft-cited example is the case of Prilosec vs. Nexium, regarding the best-selling prescription drug for heartburn.==== But opponents point to another counterexample, an incremental innovation with important improvements, Norvir, a heat-stable version of the antiretroviral drug.==== One may not have a problem with granting patents for radical innovations (the development of an entirely new molecule to treat a disease), as this allows innovators to recover their R&D costs from monopoly pricing and provides incentives for innovations that bring significant benefits to the public. However, for incremental innovations (small advances on existing drugs), this is controversial.==== One may argue that innovation itself is an inherently dynamic process in which one innovation builds on another and improvement necessarily builds on earlier innovations (Scotchmer, 1991). That is, innovation is a cumulative event, and progress can be both jumpy (radical innovation) and small (incremental innovation). We cannot refuse to provide incentives for incremental innovation because its value is unknown in advance. Does granting market exclusivity for incremental innovations truly enhance welfare, even after taking into account the losses of consumer surplus associated with granting market exclusivity? To shed light on this question, this paper estimates the welfare effects of incremental innovation and delayed generic entry due to granting market exclusivity in the context of selective serotonin reuptake inhibitor (SSRI) antidepressants.====“Incremental innovation” in pharmaceuticals, in contrast to radical innovation, involves improvements to existing drugs, i.e. the discovery of new therapeutic uses, new formulations, pediatric uses, or improvements in efficacy and safety. Like radical innovations, incremental innovations are granted patent and data exclusivity by national patent offices and drug regulatory agencies in order to encourage innovation and allow companies to recover R&D expenditures through market returns. In this paper, we define the market exclusivity specifically granted to incremental innovations as “additional market exclusivity”.==== While market exclusivity strengthens incentives to develop incremental innovations that may yield real clinical benefits, the costs associated with expanding monopolies – either through new patents or through data exclusivity– can be substantial: patients with limited budgets may not be able to purchase a treatment at all; other patients may have to reduce their consumption of other goods to be able to afford a patent-protected drug. A thorough review of such trade-off, by quantifying the value of incremental innovation and the associated potential loss of consumer surplus, is therefore essential to assess policies that balance static and dynamic efficiency.====To quantify the value of incremental innovation and the loss of consumer surplus from additional market exclusivity, this paper uses a conditional logit model for demand-side estimation and a Bertrand-Nash model for supply-side estimation to recover patient preferences for drug attributes (including price, indication, brand/generic dummy, age of drug, etc.) and marginal costs of the drugs. Based on the estimated demand-side and supply-side parameters, this paper conducts counterfactual analyses to explore the following questions: how would patent firms price in the face of early generic entry if incremental innovation is not granted market exclusivity; how would patients adjust their demand if incremental innovations that improve drug characteristics are not available; and therefore, the total welfare effect of granting market exclusivity to incremental innovations can be obtained by comparing the welfare change between the status quo (granting market exclusivity for incremental innovations) and the counterfactual situations (1. no award of market exclusivity & no provision of incremental innovation; 2. no award of market exclusivity but provision of incremental innovation).====Our results suggest that in the SSRI antidepressant market, total social welfare is worsened after granting additional market exclusivity to incremental innovations. Interestingly, it is still profitable for innovators to develop incremental innovations in the absence of market exclusivity incentives. Regardless of whether innovators are given additional market exclusivity, incremental innovation is always detrimental to generic competitors because the innovative improved product intensifies competition. The policy implication that emerges from our analysis is that granting additional market exclusivity to incremental innovations in the SSRI market may do more harm than good. The current patent policy and data exclusivity provisions warrant further study in the future. An appropriate policy regarding the length of patent and market exclusivity periods requires a comprehensive examination of all ATC classes of drugs in an empirical framework similar to the one presented here.====Based on individual-level data from the Medical Expenditure Panel Survey (MEPS, 1996–2011), this paper conducts individual-level conditional logit model estimation (Dunn, 2012). MEPS data detail individual purchase records, allowing us to take full advantage of the individual-level demand model to examine how individual characteristics (e.g., demographics, socioeconomic status, health status, and health insurance) and drug attributes (e.g., drug market price, out-of-pocket price per patient, quantity, etc.) jointly determine patient demand. In addition, as a demand estimation in the health care market, instead of adopting market list prices that might lead to an underestimation of price sensitivity, this paper estimates price elasticities with copay prices.====Lastly and most importantly, unlike Arcidiacono et al. (2013) and Taylor (2014) that use market-level data, the copay prices in our study were predicted from individual-level survey data, and thus exhibit individual variations and become identifiable at the first stage of the demand model. For this reason, the price endogeneity problem, which is prevalent in traditional BLP models, can be properly addressed in our study. This approach is similar to that of Gowrisankaran et al. (2014) and kills two birds with one stone: on the one hand, the use of copayments rather than market prices helps us to address the problem of underestimation of price elasticities; on the other hand, individual-level copayments serve to identify price coefficients with individual variations after product-year fixed effects resolve most sources of endogeneity.==== This approach can be widely applied in situations where health care markets are featured with individualized insurance coverage and where individual-level data are available.====This study contributes to policy discussions in the healthcare sector, pharmaceutical markets, and intellectual property regimes. In recent decades, many developed countries have been confronting rising health care expenditures (OECD, 2015), and the growth rates of pharmaceutical sector are even higher. Regulators have used various mechanisms to control pharmaceutical spending, including price controls (Pollard et al., 2004) and the promotion of generic drugs. The policy tool of market exclusivity used to reward innovation has also become one of the main focal points in policy discussions to curb the soaring growth in healthcare spending. Some studies have argued that this allows “Big Pharma” to reap huge monopoly profits from undeservedly lengthy exclusivity periods, leading to rising health care spending (Kesselheim et al., 2017). Others have suggested that the introduction of new drugs in fact improves the quality of treatment, leading to a quality-adjusted price decrease (Dunn, 2012), and that these drugs may have other benefits, such as reduced inpatient visits and lost workdays due to illness (Lichtenberg, 2001), thereby reducing total social costs. Our study integrates these two arguments and conducts a comprehensive quantitative analysis to estimate the loss of consumer surplus due to extended exclusivity and to evaluate the value derived from quality improvements in incremental innovations. Taking SSRI antidepressants as an example, our results suggest that granting an incremental innovation market exclusivity period contributes to the rising of healthcare expenditures.====Our paper also sheds light on the strand of literature concerning innovation policy and how it motivates innovators. As early as the 1960s, Arrow (1962) points out that a competitive system would fail to achieve optimal resource allocation for inventions due to three reasons (increasing returns, inappropriateness, and uncertainty). Thus, public efforts - including government intervention to foster innovation - were imperative. In a free market economy, patents are designed as an effective system to compensate firms for the cost of innovation and to promote research. Although there is evidence that patents are effective in stimulating national innovation in developed countries with higher levels of development, education, and economic freedom (Kyle, McGahan, 2012, Qian, 2007), other studies point out that patent enforcement incurs consumer surplus losses (Chaudhuri et al., 2006), impedes knowledge diffusion (Murray and Stern, 2007), and hinders cumulative innovation (Williams, 2013, Galasso, Schankerman, 2015). It has been suggested that fixed-term forms of patent protection leads to distortions in pharmaceutical innovation due to the fact that different cancer drugs vary in length of development (Budish et al., 2015).====Recognizing the limitations of the patent system, many researchers have proposed alternatives in encouraging innovation as a complement to the patent system (Kremer, 1998, Hollis, 2005, Lybecker, Freeman, 2007, Nathan, 2007). In the pharmaceutical industry, data exclusivity was first implemented in the United States in 1984.==== As a result, a study of the market exclusivity period suggests that the benefits of 6-month data exclusivity far outweigh the costs of conducting clinical trials, and suggests that some relatively inexpensive policies should be offered to replace the existing ones (Kesselheim et al., 2017). Li et al. (2007)’s study of pediatric exclusivity programs come to a similar conclusion.====It is beyond the scope of this article to fully assess the merits of these alternative mechanisms and patent regimes. Rather, our purpose is to assess the potential efficiency losses of existing policies (including patents) that are dedicated to stimulating incremental innovation. As Nordhaus (1969) concludes, the optimal patent policy should strike a balance between incentivizing innovation and reducing the loss of consumer surplus caused by monopoly pricing. Our paper suggests that uniform patent and exclusivity policies may not be as efficient in promoting incremental innovation as they are for radical innovation. Giving a minor incremental innovation the same patent protection period as that given to a new drug molecule may distort incentives. After all, achieving incremental advances requires less risk and cost. And at least for SSRI antidepressants, rewarding incremental innovation reduces social welfare.====This study has some limitations. First, due to the lack of detailed R&D data for each drug, the innovation cost of incremental innovation was not considered in our analysis, suggesting that we are estimating the upper bound on the welfare effect of introducing incremental innovation. Fortunately, it does not affect our main conclusion that the overall benefits of incentivizing incremental innovation are smaller than the costs associated with market exclusivity. A second limitation is that the analysis ignores the potential public benefits of negative information obtained from trials that attempt to discover new uses for existing drugs. Eisenberg (2005) points out that extensive clinical trial studies of existing drugs may uncover negative information. While it provides considerable social value in guiding drug use, this social value is captured by physicians, patients, and competitors, and on the contrary, drug developers may suffer significant losses from negative information.==== In this sense, excessive rewards for investigators’ successful studies (incremental innovations) can be treated as compensation for the losses caused by their failed studies. Otherwise, drug development companies would have no incentive to invest in R&D and provide information on clinical trials. While it is insightful and meaningful to assess this effect, however, unfortunately the current data do not support analysis from this perspective.====This paper is structured as following: Section 2 explains market exclusivity policies for pharmaceuticals and the antidepressant market. Section 3 describes the data and summary statistics. Section 4 details the empirical strategy. Section 5 presents the estimation results, and Section 6 concludes.","Pharmaceuticals, incremental innovation and market exclusivity",https://www.sciencedirect.com/science/article/pii/S0167718723000048,23 January 2023,2023,Research Article,27.0
Harris-Lagoudakis Katherine,"Iowa State University, Department of Economics, Heady Hall, 518 Farm House Lane, Ames, IA 50011, United States","Received 7 December 2021, Revised 26 December 2022, Accepted 27 December 2022, Available online 4 January 2023, Version of Record 15 January 2023.",https://doi.org/10.1016/j.ijindorg.2022.102918,Cited by (0),"This paper analyzes the effect of an online shopping channel on private label purchases, product exploration and price elasticities. Variation in the timing that an online shopping service was introduced is utilized as a source of exogenous variation in the decision to shop online. ==== estimates indicate a 0.3 to 1.0 (1.0 to 2.0) percent increase (reduction) in the proportion of private label (new) products purchased after the introduction of the online shopping service. Price elasticities are then estimated utilizing an Exact Affine Stone Index (EASI) demand model. Comparisons of in-store and multichannel price elasticities indicate that households are, on average, less price sensitive when shopping across both the in-store and online channels. Own-price (cross-price) elasticities are 1.07 (5.56) times larger in-store than they are in a multichannel setting. These findings suggest that retailers manipulate the online search platform and(or) the provision of substitutes to favor private label products, which have a higher margin. Additionally, these results suggest that retailers may find it profit maximizing to raise prices as consumer baskets become more sticky in the multichannel purchasing regime.","Due to increasing accessibility and the surge in demand for online shopping services as a result of the COVID-19 pandemic, 45 percent of consumers report shopping online for groceries more now than prior to the COVID-19 pandemic (Redman, 2021).==== Given the large growth rate, food manufacturers and industry experts are becoming more concerned with how the widespread adoption of online channels will alter the grocery retailing landscape. In particular, industry experts fear that online favorites lists and advertising algorithms based on past purchases will lead to a decline in new product exploration (Washington Post, 2020). These concerns contradict early digital economics research that has often thought of e-commerce as “frictionless” due to the ease of price comparison and the plethora of products available for purchase (Bakos, 1997, Brynjolfsson, Smith, 2000, Ellison, Fisher Ellison, 2005, Brown, Goolsbee, 2002). However, more recent work in the digital economics literature suggests that online search patterns can be manipulated in order to sustain higher margins and prices online (Goldfarb, Tucker, 2019, Ellison, Ellison, 2009).==== In further support of industry opinion, grocery products tend to be relatively inexpensive and grocery shopping is an activity that consumers are forced to engage in on a fairly regular basis. For these reasons, time saving features of the online purchasing environment (online favorites lists, targeted advertising, etc.) may outweigh the effects of sales features and price sorting options available in the online platform.====This paper evaluates the effect of the introduction of an online shopping channel, by a traditional brick-and-mortar grocery retailer, on the prevalence of private label (e.g. store brand) purchases and new product exploration. This paper also tests for structural differences in demand across the two purchasing regimes (exclusive in-store shopping and multichannel shopping). Changes to multichannel months are evaluated relative to in-store only months because unlike other products available for purchase online, it is common for digitally engaged grocery shoppers to continue to make purchases in the store (Campo and Breugelmans, 2015).==== Grocery scanner data, generated from the purchases of 34 thousand households, are utilized to estimate the effects of multichannel shopping. These data provide an attractive setting to study the effect of the online environment on multichannel demand for three reasons: first, the panel structure of the data allows for a within household comparison of purchases across the in-store and multichannel regimes; second, online and in-store purchases are fulfilled by the same retailer, alleviating concerns over differences in product selection and branding; third, the retailer of this study offers products for purchase online at the same prices as those found in the store.====This paper employs event study estimation strategies that utilize plausibly exogenous variation in the time the online grocery service became available, at different store locations, to evaluate the effects of multichannel shopping. This identification strategy produces intent to treat (ITT) estimates and relies upon the assumption that the time that the online service became available to a household is uncorrelated with a change in any other factors (household level dietary decisions, household time constraints etc.) that influence food demand. In later analyses, the availability of the online shopping service will be utilized as an instrumental variable for the decision to shop online; these analyses produce local average treatment effects (LATE) of online service use at the monthly level.====Whether or not households are more or less likely to engage in private label purchases and new product exploration in multichannel months compared to in-store only months is examined. Event study estimates indicate that the proportion of purchases allocated to private label products increases by 0.1 to 0.3 percentage points following the introduction of the online shopping service; these estimates represent a 0.3 to 1.0 percent increase over the pre-online service average, respectively. These findings suggest that retailers may manipulate the online platform and(or) the products it provides as substitutes for out of stock items to favor private label products, which have a higher margin relative to national brands.==== Event study estimates also indicate that the proportion of purchases allocated to new products declines by 0.5 and 1.0 percentage points three and six months following the introduction of the online shopping service, respectively. These estimates represent a 1 and 2 percent reduction over the pre-online service average.====A structural model of grocery demand that allows demand parameters to vary in multichannel months is then estimated. Thirty-three percent of the demand parameters are significantly different in months in which a household engages in multichannel shopping. Households are, on average, less price sensitive in months where they engage in multichannel shopping. The estimated own-price elasticities and cross-price elasticities are 1.07 and 5.56 times larger in-store than they are in multichannel months, respectively. Differences in price elasticities and product exploration across the two purchasing regimes suggest that firms may be able to increase revenues by increasing prices in the online shopping environment.====The empirical research most closely related to this study finds evidence that the convenience features of the online shopping experience outweigh the effect of price comparison features. Pozzi (2012) analyzes how online shopping lists and the convenience of being able to instantly generate your grocery basket with previous purchases influences decisions over brand choice within breakfast cereals when shopping online. While brand choice is the main focus of the paper, he also finds that households are generally less price sensitive when they shop online.==== Similarly, Chu et al. (2008) also study how purchasing patterns vary across trips made in-store and online with the same retailer. Chu et al. employ a discrete choice analysis across a number of different product categories and consistently find that households are more brand loyal, more size loyal and less price sensitive when they shop for groceries online.==== In a follow-up paper, the authors find that the magnitude of the effect of online shopping on brand loyalty, size loyalty and price sensitivity varies with household and product characteristics (Chu et al., 2010). Danaher et al. (2003) also study the effects of an online shopping channel on brand choice and find that brand loyalty is considerably higher for high market share brands.====There are also a number of studies that utilize reduced form analysis to document differences in purchases when households shop online. Pozzi (2013) employs a panel difference-in-differences estimation strategy to evaluate how online convenience influences purchases over products that are inconvenient to transport, namely, bulk products. He finds that when households use an online grocery service, they increase their monthly share of expenditure for bulk items in laundry detergent and soda by 30 and 80 percent, respectively.==== Utilizing a similar estimation strategy, (Pozzi, 2013) finds that households increase their monthly spending with the retailer upon introduction of an online shopping channel. Harris-Lagoudakis (2022), estimates the effect of shopping online on the composition and nutritional content of grocery purchases. Utilizing variation in the time that the online shopping service became available at different store locations, Harris-Lagoudakis (2022) finds that households allocate a smaller share of their grocery budget towards sugars/sweets/candies when shopping across online and in-store channels.====This paper extends the prior research in a number of ways. First, whether or not retailers position private label products favorably in the online shopping environment is evaluated. Prior research has shown that online search patterns can be manipulated in order to sustain higher margins and prices (Goldfarb, Tucker, 2019, Ellison, Ellison, 2009); furthermore, evidence of a systematic relationship between multichannel retailers and a reliance on private labels has also been found in prior work (Bonnet and Etcheverry, 2021). Retailers seeking higher margins might manipulate search patterns and(or) product substitutions to favor private label products. Second, I evaluate multichannel outcomes relative to in-store exclusive outcomes. Online purchasing data utilized in other studies and in this study indicate that multichannel shopping is quite commonplace (Campo and Breugelmans, 2015). Ninety-four percent of the monthly shopping occasions in which the online service is utilized at least once, indicate that the household conducted some shopping in the store and some shopping online. Furthermore, 43 percent of monthly spending is conducted via the online channel in a month in which a household utilizes the online shopping service at least once.==== For this reason, it is important to account for changes to in-store shopping trips in response to online shopping trips. Third, this paper estimates structural changes in demand that stem from the multichannel regime relative to the in-store exclusive regime.==== In contrast, the current literature has estimated structural differences between online and in-store demand.====The remainder of this paper is structured as follows: Section 2 discusses the online purchasing service and data. Section 3 presents evidence for decreased product exploration and increased private label purchases in multichannel shopping months. Section 4 discusses the grocery demand model and Section 5 presents the estimation strategy and results. Section 6 discusses and concludes.","The effect of online shopping channels on brand choice, product exploration and price elasticities",https://www.sciencedirect.com/science/article/pii/S0167718722000935,4 January 2023,2023,Research Article,28.0
"Bisceglia Michele,Piccolo Salvatore,Tarantino Emanuele","Toulouse School of Economics: 1, Esplanade de l'Université, Toulouse, 31080, France,Università di Bergamo: 2, via dei Caniana, Bergamo, 24127, Italy,Luiss: 32, Viale Romania, Roma, 00197, Italy,Compass Lexecon,CSEF,EIEF,CEPR","Received 22 October 2021, Revised 21 December 2022, Accepted 29 December 2022, Available online 4 January 2023, Version of Record 23 January 2023.",https://doi.org/10.1016/j.ijindorg.2023.102919,Cited by (0),.,"Merger control is a fundamental activity of Antitrust Authorities (AAs) all over the world. Between 2005 and 2014, the European Commission DG Competition reports 128 interventions in cartel and antitrust matters, while for the same period, it reports 184 cases in the area of merger enforcement alone (Eur, 2014).====The primary task of the merger review process is to identify the balance between the bright and dark sides of a merger, and thus avoid enforcement errors. This process is designed to limit the approval of mergers creating or strengthening dominant positions, which would significantly limit effective competition at consumers’ expense. However, mergers can also create efficiencies — e.g., cost synergies — whose downward pressure on prices may overcome the merger’s anticompetitive effects and make it desirable for consumers. The resulting trade-off between market power and efficiencies is central in the antitrust economics literature examining horizontal mergers (e.g., Motta, 2004, Whinston, 2007).====While there is a broad consensus about how the potential anticompetitive effects of a merger should be measured — e.g., through market definition analysis — assessing and certifying efficiencies is somewhat more critical. Efficiencies must be proven to be merger specific — i.e., likely to be accomplished only through the proposed merger and unlikely to be accomplished by the companies absent the merger. They must also be cognizable — i.e., they must be verifiable for likelihood and magnitude and must not result from an anticompetitive reduction of output. All this requires that the AA interviews the parties involved in a merger, examines the reports prepared by the proponent firms, and, in some cases, collects data and run simulation analyses itself before approving a merger. In the EU, for instance, it is a standard practice for economic experts from the European Commission, the notifying parties and third parties to participate actively in the merger review process (Botteman, 2006).====During this process, it is customary for merging firms to hire advisors whose task is to find and certify evidence that supports their case. Economists are increasingly employed in antitrust cases on both sides of the Atlantic. In Europe, the annual turnover of the main economic consultancy firms has steadily increased since the early 1990s, and is about 15% of the aggregate fees earned on antitrust cases, a proportion close to that in the U.S. (Neven, 2006).====Although it is common for firms to hire M&A advisors, the theoretical literature offers no guidance regarding how their presence impacts the investigation of the welfare effects of the merger, and thus the success of M&A.==== Understanding the impact of experts appears even more timely and critical in light of recent legal developments featuring increasing scrutiny of M&A advisory deals, including the circumstances in which fees are payable to advisors.==== For example, the decisions of the Delaware Chancery Court have emphasized the need for greater transparency on advisors’ potential conflicts of interest. The worry is that advisors may have perverse incentives to complete the transaction, possibly in opposition to the interests of shareholders. In this paper, we assess these concerns and examine how the expert’s contract and its disclosure affects the merger review process.====To this purpose, we study a static framework ==== Farrell and Shapiro (1990) in which two out of ==== symmetric firms, competing in a Cournot industry, propose a merger to an AA which adopts a consumer surplus standard, as is the case in both the U.S. and EU antitrust law.==== The basic trade-off is the standard one: the merger increases market power but it may also create efficiencies (Williamson, 1968).==== In the model, the proponent firms are uninformed about the (uncertain) efficiencies of the merger, but can hire an expert to gather information on their behalf.====Firms cannot verify the information reported by the expert to the AA — e.g., they lack the technical skills necessary to assess whether such evidence is informative or not (the AA, by contrast, owns these skills). This asymmetry of information creates a moral hazard problem that can be solved by offering the expert incentives tailored to the approval of the merger. The AA is also uninformed about the merger’s efficiency improvement but can run a costly internal investigation to learn its realized value. Hence, the model differs from Besanko and Spulber (1993) and Nocke and Whinston (2013) because firms are not privately informed about the merger characteristics ==== the AA. However, both can acquire this information by engaging in information gathering activities. Information acquisition is costly for both the AA and the expert, and is undertaken simultaneously by them. To make the problem interesting, we follow Besanko and Spulber (1993) in assuming that the merger harms consumer surplus in expected terms — i.e., behind the veil of ignorance, the AA always rejects it.====Within this framework, we consider a game with two alternative information regimes: one regime in which the expert’s contract is disclosed, the other in which it is not. This allows to analyze the expert’s role in the merger review process and the welfare effects of imposing the disclosure of his contract as part of firms’ due diligence.====Our results hinge on a somewhat natural coordination problem: the expert acquires information only if he expects the AA not to do so (vice-versa for the AA). Since gathering information about the merger’s efficiency improvement is costly for the expert and the AA, they both have an incentive to free-ride on each other. On the one hand, the AA would like the expert to gather information to base the decision regarding the merger on the evidence that he collects, while saving on its own information acquisition cost. On the other hand, the expert would like the AA to collect information to avoid paying the investigation cost while being rewarded if the merger is approved. In the region of parameters where this free-riding problem is relevant, contract disclosure may play a key role for consumer surplus.====Without disclosure, the game is ==== simultaneous. The coordination problem may lead parties to play mixed strategies in equilibrium. The merging firms randomize between offering or not to the expert an incentive compatible contract, which induces him to gather information. The AA randomizes between collecting information and remaining uninformed.====With disclosure, the game structure becomes sequential: first, the firms make their contract offer to the expert, and then AA and expert simultaneously decide whether to acquire information on the merger’s welfare harm. Simultaneity means that, even with disclosure, a mixed-strategy equilibrium may arise in the information acquisition game. In this equilibrium, the firms choose the advisor’s contract to maximize profits, whereas the expert and the AA randomize between collecting information and not.====Comparing the mixed strategy equilibria with and without disclosure, we find that contract disclosure makes consumers worse off. The intuition is rooted in how disclosure changes the equilibrium behavior of the agents in the game. When compelled to disclose the expert’s contract, the merging firms act as Stackelberg leader, thereby they optimally offer a smaller value of the bonus than the one that keeps the expert indifferent between gathering information or not in the equilibrium without disclosure. As a consequence, the AA’s probability of gathering information with disclosure will have to decrease to restore the indifference condition of the expert in the information acquisition stage.==== This implies that consumer welfare is lower with disclosure.====However, when the value of the information acquisition cost rises above a threshold, the proponent firms exploit their first-mover advantage ==== the AA by committing not to offer an incentive-compatible contract to the expert,==== thereby inducing the AA to acquire information. In this case, the probability that the AA takes an informed decision drops in the regime without disclosure, given that, due to the high information acquisition cost, both players gather information with low probability. Therefore, enforcing contract transparency benefits consumers when the cost of acquiring information is relatively large.====On top of the mixed-strategy equilibria discussed above, the information acquisition games with and without disclosure also admit pure-strategy equilibria. When including these pure-strategy equilibria in the comparison of the two disclosure regimes, the condition on the value of the information acquisition cost becomes a sufficient condition for disclosure to weakly benefit consumers. In other words, as long as the information acquisition cost is large enough, mandating disclosure is optimal from a consumer welfare standpoint independently of which equilibrium arises in the information acquisition games.====When analyzing the comparative statics of our results, we show that the region of parameters in which disclosure harms consumers expands as the market becomes less concentrated. As the number of firms in the industry increases, the investigation probability without disclosure rises above the one with disclosure. This happens because, without disclosure, both the AA and the firms acquire information with a probability that increases with the number of firms in the industry, whereas with disclosure it is only the AA to run the investigation.====Overall, although derived in a stylized setting, we can interpret these findings as providing a possibility result regarding how the disclosure of the expert’s contracts can harm consumer welfare. This result is robust when considering a biased expert who can credibly overstate the merger’s efficiencies (so that the AA may end up approving a consumer surplus decreasing merger), or when firms can offer him a performance-contingent contract (i.e., a bonus contingent on producing an informative report). Yet, our possibility result crucially hinges on the feature that the information gathering activities of expert and AA are imperfect substitutes. If, on the contrary, they are either perfect substitutes — i.e., their outcomes are perfectly correlated — or complement — i.e., both players need to find positive evidence for the merger to be approved — then contract disclosure unambiguously benefits consumers.====One may find the free-riding problem behind these results an unrealistic feature of our theoretical model. It can be objected that the AAs around the world have strong interests in carrying out rigorous investigations in all (potentially socially harmful) cases they face. In practice, however, a critical input shaping AAs’ investigation capacity is their budget (see, e.g., Neven, 2006). It directly affects the agency’s resources for enforcement, and constrains resource allocation. In 2008, the former Director General of the European Commission DG Competition (Philip Lowe) wrote that “DG Competition is understaffed when compared to other competition authorities” and that the agency should “concentrate its limited resources on specific priorities” (Lowe, 2008). Likewise, in 2013 Edith Ramirez (who at the time was the FTC chairwoman) stated that “in an effort to be most effective with limited resources, we pay particular attention to sectors where our action will provide the greatest benefit to consumers.”==== Posner (2000) claims that “States do not have the resources to do more than free ride on federal antitrust litigation”, and Feinberg and Husted (2013) provide related evidence. It is then reasonable that, as part of its constrained resource allocation, the AA chooses to investigate some cases and not others. To the extent that this choice is not observable by merging parties, the latter (and the expert they hire) must form conjectures on the AA’s investigation efforts, as they do in our mixed strategy equilibria.====The article proceeds as follows. After reviewing the related literature, we set up the model in Section 2. In Section 3, we first develop the analysis for the regimes with and without disclosure, and then study the effect of the expert’s contract disclosure on consumer surplus. Section 4 discusses some extensions of the model. Section 5 concludes with a discussion on the conditions under which disclosure affects consumer welfare. All proofs are in the Appendix. Additional material is contained in the Online Appendix.==== Inspired by Williamson (1968), the modern approach to horizontal mergers has been developed by Farrell and Shapiro (1990), who formalize the basic welfare trade-off arising from horizontal mergers in a model with Cournot competition and increasing marginal costs (see, e.g., also Mc Afee and Williams, 1992).==== These models assume complete information and a passive role for the AA.====Besanko and Spulber (1993) were the first to examine the role of the AA in a model with asymmetric information. They assume that the merging firms are better informed than the AA about the merger’s efficiency gains and show that, by committing to a conservative consumer surplus standard, the AA may increase aggregate surplus.==== A different form of asymmetric information is considered in Armstrong and Vickers (2010), who determine the optimal permission set in a model in which the AA can verify the characteristics of the received merger proposal, but does not know which other merger projects were available to the firms (see, e.g., also Lyons, 2003). Building on the same idea, Nocke and Whinston (2013) analyze the optimal merger approval policy in a model with a single acquirer who can make a merger proposal to one of several heterogeneous firms. They find that the AA optimally commits to a policy that imposes tougher standards on mergers involving firms with a larger pre-merger market share.====More closely related to our work, Sørgard (2009) studies a framework where the AA sets an activity level (how many mergers to investigate). He shows that an active merger control can lead to an adverse selection of proposed mergers, which in turn has implications for the possible decision errors made on investigated mergers. The optimal activity level trades off the possible mistakes from enforcement with the gains from deterring a potentially harmful merger.==== More recently, Dertwinkel-Kalt and Wey (2021) model a principal-agent problem between the legislator and the AA, where, as in our model, the latter engages in costly information gathering activities to ascertain the welfare effects of the proposed mergers. They show that allowing for remedial solutions dampens information acquisition incentives, and could therefore reduce consumer welfare.====None of these papers studies the role of experts and explicitly models their information acquisition process.==== By introducing these ingredients, our model delivers implications on the relationship between the accuracy of the merger evaluation process, the role of external advisors and the rules governing how much transparent their deals should be.",M&A advisory and the merger review process,https://www.sciencedirect.com/science/article/pii/S0167718723000012,4 January 2023,2023,Research Article,29.0
Lagerlöf Johan N.M.,"Department of Economics, University of Copenhagen, Øster Farimagsgade 5, Building 26, DK-1353 Copenhagen K, Denmark","Received 20 August 2020, Revised 19 December 2022, Accepted 27 December 2022, Available online 30 December 2022, Version of Record 17 January 2023.",https://doi.org/10.1016/j.ijindorg.2022.102917,Cited by (0),"This paper studies consumers’ incentives to hide their purchase histories when the seller’s prices depend on previous behavior. Through distinct channels, hiding both hinders and facilitates trade. Indeed, the social optimum involves hiding to some extent, yet not fully. Two opposing effects determine whether a consumer hides too much or too little: the first-period social gains are only partially internalized, and there is a private (socially irrelevant) second-period gain due to price differences. If the discount factor is large, the second effect dominates and there is socially excessive hiding. This result is reversed if the discount factor is small.","In the last couple of decades it has become increasingly common that consumers make retail purchases on the internet. While online shopping often is convenient for consumers, it also makes it relatively easy for sellers to keep track of individual customers’ purchasing decisions and thereby learn about their willingness to pay for the good. Using that knowledge, the sellers can charge personalized prices that leave certain consumers with a smaller surplus than otherwise. In a Washington Post article, Lowrey (2010) vividly describes this phenomenon and how it upsets consumers:====In the economics literature, the seller practice described in the quotation has been referred to as dynamic pricing, history-based pricing, or behavior-based price discrimination (BBPD). A seller who practices BBPD does not necessarily have to do this in the blunt manner suggested in the quotation, directly presenting different consumers with different price tags. Often more subtle approaches are available. For example, a seller can distribute a specific discount coupon only to certain consumers, thereby effectively offering a personalized price to them. Introductory offers that entitle new customers to pay a lower price than returning customers pay are another example of BBPD. Yet another way in which a seller can, in a more subtle way, practice BBPD is by so-called price steering: on her website (for example), a seller presents available options (say, more or less expensive versions of the product) differently to different customers (see, e.g., Hannak et al., 2014).====BBPD has been studied theoretically both in monopoly and oligopoly settings. One insight from this literature is that a firm’s opportunity to practice BBPD is not necessarily harmful to consumer welfare, as price discrimination tends to lead to more trade than otherwise and this can benefit also the consumers.==== However, in specific situations and if she can, a consumer clearly has an individual incentive to hide her purchase history from the seller. Indeed, if being a returning customer is interpreted as being a high-valuation customer, then you are better off pretending to be a new customer. In practice, there are several possibilities for consumers to hide their purchase histories by using various anonymizing technologies: they can refrain from joining loyalty programs or set their browsers to reject cookies; they can choose to end a newspaper subscription and then start a new one, instead of renewing the old subscription; or they “can use a variety of credit cards or more exotic anonymous payment technologies to make purchases anonymous or difficult to trace” (Acquisti and Varian, 2005, p. 367). These kinds of defense measures might come at a cost (a financial expense or nuisance) but they are certainly available.====The present paper is an attempt, within an equilibrium framework where a firm engages in BBPD, to study the incentives of consumers to hide their purchase histories with the help of anonymizing technologies. Among other things I ask whether, at the equilibrium and given other imperfections in the market, consumers use these technologies too little or too much. That is, from a social welfare perspective, does the market tend to generate too strong or too weak incentives for consumers to hide their purchase histories?====To help us think about whether (or under what circumstances) the incentives are likely to be too strong or too weak, refer to Fig. 1. This figure summarizes the results of a standard two-period monopoly BBPD model with a continuum of consumers, each having a valuation ==== (the same across the two periods and drawn from a uniform distribution on the unit interval). The firm has no production costs; hence, the social optimum involves all consumers purchasing the good. In period 1, the firm sets a single price ====, knowing only the distribution of valuations; the consumers then choose whether to purchase. In period 2, the firm can distinguish between buying and non-buying consumers and thus set two prices, ==== and ====. At the equilibrium, consumers with valuations above a cutoff ==== purchase in period 1. Moreover, the period 2 equilibrium price for the returning customers is higher than that of the new customers (====).==== Finally, the period 1 equilibrium price is strictly lower than the cutoff (====); this is because a consumer requires a positive first-period surplus in order to be willing to purchase in period 1 (for ====).====Given the above framework, consider now the possibility that a consumer has the opportunity to hide her purchase decision in period 1 (thus being eligible to purchase at the price ==== in period 2, also if being a returning customer). Suppose also that the hiding decision is made at an ex ante stage, prior to learning the valuation ====. Will her expected benefit from hiding be smaller or greater than society’s expected benefit (measured as total surplus)? By inspecting Fig. 1, we can identify three effects (or externalities) that determine whether a consumer with realized valuation ==== has too weak or too strong incentives, and these effects point in different directions:====The first effect, which creates an incentive to hide too little, matters for the consumer in the period in which she makes her hiding decision. The second and third effects, which create an incentive to hide too much, matter only in the following period. This suggests that, if the consumer cares sufficiently much about the future, then the second and third effects might dominate and the consumer thus invests too much in anonymizing technologies. In the model that I set up and study, I show that this is indeed the case. I also show that, if the consumer instead is sufficiently myopic (i.e., she assigns a small weight to her second-period payoff), then we can reverse this result: the consumer invests too little in anonymizing technologies.====The formal framework that I develop is close to the standard BBPD model described above, except that I add the opportunity for the consumers to hide their purchase histories. Thus, a monopoly firm produces and sells a nondurable good in each of two time periods. Each consumer’s valuation for the good is the same across the two periods and drawn from a uniform distribution. The valuation is initially the consumer’s private information; the firm knows only the distribution of valuations in the market. However, by observing the first-period consumption choice, the firm can make a noisy inference about a consumer’s valuation and then use this information when choosing the second-period price. In particular, a consumer’s choice to purchase the good in the first period suggests that her valuation is relatively high, which creates an incentive for the firm to raise that consumer’s second-period price. To protect herself from this, the consumer has the opportunity to hide her first-period purchase. This is modeled by letting the consumer choose a probability (i.e., any real number between zero and one) with which the information that she purchased the good is not available to the firm in the second period. Put differently, if the consumer chooses a particular hiding probability, then with that probability she will after her purchase look like a consumer who did not buy; with the complementary probability, she will indeed be identified by the firm as a consumer who purchased in period 1 and thus have to pay a higher price in period 2 (as in the standard setting). The choice of the hiding probability is made at an ex ante stage, before the consumer has learned her own valuation. This model feature captures the idea that the consumer adopts a long-term approach for dealing with privacy issues, for example, by choosing a setting on her computer that she sticks with through a large number of browsing sessions. I discuss this assumption at greater length in connection with the model description in Section 2. In the concluding section, I also briefly discuss the technical consequences of making the alternative assumption that the consumer’s hiding decision is made ==== she has learned her valuation.====In Section 3, I begin the analysis by studying a version of the model in which the hiding probability is given exogenously and the same for all consumers. I first solve for the equilibrium (for given parameter values, this is unique). As in the standard model without the opportunity to hide one’s purchase history, the equilibrium is characterized by a cutoff value of a consumer’s valuation—above which she purchases in the first period, and below which she does not. Given this model with an exogenous hiding probability, I take an initial look at social welfare. In particular, I show that the welfare-maximizing value of the hiding probability is strictly interior. That is, if a total-surplus maximizing social planner could choose the hiding probability, the consumers would hide their first-period purchases to at least some extent, yet not fully. The reason why a strictly positive degree of hiding is socially optimal is that hiding generates gains from trade in the first period. Moreover, the social cost of hiding in terms of hindering second-period price discrimination is small, provided that the degree of hiding is sufficiently small.====In Section 4, I endogenize the hiding probability. I confine the analysis to equilibria where all consumers choose the same probability. I characterize and show existence of such an equilibrium. I then turn to the question whether the equilibrium value of the hiding probability is larger or smaller than the value that maximizes total surplus in the market. I first show that for the case where the common discount factor equals one—so the weights assigned to the first- and second-period payoffs are the same—the equilibrium involves too much hiding. The intuition for this result can be understood in terms of the discussion in the beginning of this introduction. By hiding her purchase history, a consumer can in the second period buy the good at a lower price than otherwise; this individual gain does not enter total surplus, as it is a pure transfer (effect 2). Moreover, the act of hiding makes it harder for the seller to practice (trade-enhancing) price discrimination in the second period (effect 3). Both those effects suggest that the consumer hides too much. Another effect of hiding one’s purchase history is that it makes it possible to exploit first-period gains from trade to a greater extent; this suggests that the consumer hides too little, as she does not internalize the full benefit of the extra trade (effect 1). When the discount factor equals one, the weight on the second-period payoff is large enough for effects 2 and 3 to dominate.====A more general analysis, for the case where the discount factor is strictly below unity, is harder. Nevertheless, with the help of a numerical analysis, I can first show that the result discussed above—that the consumer hides too much—holds also for a range of discount factor values below one. Second, I provide examples where the discount factor is so low that the result above is reversed and, instead, the consumer hides too little. Intuitively, if the discount factor is sufficiently low, what matters for welfare is the first period and then the consumer does not internalize all the gains from trade that her hiding enables.====Section 5 studies an alternative model specification, in which the consumers learn whether their anonymization attempts have been successful only at a later stage. In such an environment, effect 1 discussed above cannot occur; thus, the possibility of undersupply of anonymization disappears. Section 6 discusses policy implications of the results, and Section 7 concludes. Appendix A shows that the main model used in the paper, with a continuous hiding choice, is equivalent to a certain alternative model with a binary hiding choice. Finally, Appendix B provides proofs of the results that are not proven in the main text.",Surfing incognito: Welfare effects of anonymous shopping,https://www.sciencedirect.com/science/article/pii/S0167718722000923,30 December 2022,2022,Research Article,30.0
"Abito Jose Miguel,Chen Cuicui","Ohio State University, 1945 N High St, Columbus, OH 43210, USA,State University of New York (SUNY) at Albany, Hudson 257A, Albany, New York, 12222, USA","Received 24 August 2021, Revised 22 December 2022, Accepted 23 December 2022, Available online 28 December 2022, Version of Record 13 January 2023.",https://doi.org/10.1016/j.ijindorg.2022.102915,Cited by (0),"Empirical work on strategic interactions is often subject to the critique that equilibrium selection assumptions drive the results. We develop a framework for partially identifying parameters of dynamic games without equilibrium selection assumptions. Our framework relies on incentive compatibility constraints that incorporate game theoretical results on equilibrium payoff sets to bound the unknown continuation payoffs. We apply this framework to identify cost parameters in three dynamic games where collusion is a potential outcome. The identified set demonstrates the ease of sustaining collusion with patient firms, in low demand and when monitoring is perfect, and can also be used to detect collusion.","Dynamic games provide an important theory to model long-run relationships that result in incentives and outcomes not otherwise captured by one-shot interactions. Progress in computation power and data availability has enabled the application of those theoretical insights to empirical work to answer important policy questions.==== Empirical researchers choose a solution concept for the dynamic game being studied and estimate parameters by matching theoretical predictions with data moments.====To obtain point estimates, however, additional modeling choices are made to focus on a particular equilibrium under the chosen solution concept. For example, to support collusion, researchers typically assume reversion to static Nash competition as the punishment for deviating from a collusive scheme. A natural question is how much those equilibrium selection assumptions drive the parameter estimates. Put differently, what can we learn about the parameters of dynamic games with no restrictions beyond the solution concept itself?====We develop a framework for partially identifying parameters of dynamic games from restrictions inherent in equilibrium concepts without equilibrium selection assumptions. Apart from identifying parameters that are immune to potential biases in equilibrium selection assumptions (as in, e.g., Tamer (2003)), our framework has the additional advantage of mobilizing a body of theoretical insights that have been under-utilized in empirical work. In particular, we demonstrate that the theoretical literature on equilibrium payoff sets (e.g., Fudenberg, Maskin, 1986, Stahl, 1991, Fudenberg, Levine, 1994, Hörner, Sugaya, Takahashi, Vieille, et al., 2011, Hörner, Takahashi, Vieille, et al., 2014, Abreu, Brooks, Sannikov, 2020) is valuable in identifying useful ranges of structural parameters in a rich class of dynamic games, including those with imperfect monitoring.====The key idea behind our identification framework is as follows. In dynamic games, the one-stage deviation principle allows us to use single-stage deviations to fully characterize firms’ incentives in choosing specific actions at a given history. Hence, for each action profile observed, the incentive compatibility constraints require that any alternative current-period action of each firm yield a discounted sum of expected payoffs no higher than what that firm gets with the observed action. The discounted sum of expected payoffs can be rewritten as a weighted sum of stage-game payoffs and continuation payoffs. Without assuming equilibrium selection, we cannot parameterize the continuation payoffs. However, we know that all continuation payoffs must come from the set of equilibrium payoffs corresponding to the chosen solution concept. Importantly, the equilibrium payoff set is invariant with the history of play or the particular equilibrium being played. We then leverage the theory literature in deriving or computing the equilibrium payoff set, as a function of structural parameters, to bound the possible continuation payoffs. The incentive compatibility constraints associated with the observed action profiles are now a system of inequalities that depend only on the structural parameters and therefore partially identify the latter.====We demonstrate our partial identification framework in three examples of dynamic games where collusion is a possible outcome. The goal is to identify cost parameters (along with discount factors) that rationalize observed behavior without imposing equilibrium selection assumptions such as firm conduct or collusive punishment strategies. Obtaining robust cost parameter estimates is important for antitrust because an inflated cost estimate makes collusive choices appear competitive and weakens the ability to detect collusion. The resulting identified set also can be used to simulate counterfactual experiments free of equilibrium selection assumptions.==== Our method of identifying cost parameters in potentially collusive settings complements existing methods that rely on various additional assumptions such as observed cost data (de Roos, de Roos, 2004), known competitive play from certain periods (Igami and Sugaya, 2022) or certain players (Miller et al., 2021), assumed firm conduct (Rosen, Eizenberg, Shilian), and a large number of instrumental variables and exclusion restrictions through the conduct parameter approach (Bresnahan, 1982, Nevo, 1998, Berry, Haile, 2014, Miller, Weinberg, 2017, Sullivan).====Our first example is a repeated quantity-setting duopoly under perfect monitoring. In each period, firms simultaneously choose a high or low production level. If jointly low production levels are observed, what does that imply for their (common) marginal cost if we impose subgame perfection? The set of Subgame Perfect Equilibria encompasses repeated play of Cournot competition, and when the discount factor is large enough, collusion with a grim-trigger strategy (assumed in virtually all structural empirical work on collusion) or any other conceivable strategy. In practice, we rarely know or observe what statistic of past deviations triggers cartel punishment, nor does this punishment have to take the extreme form of reversion to Cournot competition. To derive the equilibrium payoff set to bound the continuation payoffs, we use the Folk Theorem in Fudenberg and Maskin (1986) (as the discount factor goes to one) and exact payoff set characterization in Stahl (1991) (at any given discount factor). Our identification result shows that the lower bound on the marginal cost reduces as we increase the discount factor. This is consistent with the intuition that more patient firms can support collusion more easily. We also show that the identified set is useful in detecting collusion and deriving the minimum discount factor necessary to support collusion with any repeated-game strategy. We extend this example to include three actions, three firms, or both, and find that the identified set is smaller with more firms and fewer actions.====The second example is a stochastic version of the first example. In each period, firms observe a binary demand state and choose their production levels. This example is inspired by Rotemberg and Saloner (1986), who show that collusion is harder to sustain in periods of high demand. We use the numerical method in Abreu et al. (2020) to compute the expected equilibrium payoff set. Our results confirm that the observation of jointly low production, combined with subgame perfection, identifies a larger lower bound on the marginal cost in high demand states relative to low demand states, suggesting the difficulty of supporting collusion in high demand as Rotemberg and Saloner (1986). This result does not suggest that high demand necessarily implies competition. Indeed, we illustrate that if we falsely treat the high demand state as competitive, then the resulting marginal cost estimate is biased upwards and we can fail to detect collusion under low demand.====In our third example, we study a repeated research and development (R&D) investment game with imperfect public monitoring. While perfect information is a standard assumption in structural studies of collusion, there is a rich game theory literature on collusion under imperfect monitoring motivated by its realism (see Green and Porter (1984) Green and Porter (1984); Kandori (1992); Sannikov and Skrzypacz (2007); Aoyagi and Fréchette (2009) for imperfect public monitoring and Harrington, Skrzypacz, 2007, Harrington, Skrzypacz, 2011 for private monitoring). In our model, R&D induces a business-stealing effect (Nocke, 2007) so that firms may collude on not doing R&D. However, firms only observe whether the technology has a breakthrough, but not their competitor’s R&D investment. Imperfect monitoring thus makes collusion harder, because deviation to invest in R&D cannot be detected when there is no technology breakthrough. Indeed, using Fudenberg and Levine (1994) to derive the Perfect Public Equilibrium payoff set to bound the continuation payoffs, we find that the joint no R&D profile identifies a larger lower bound on the R&D cost under imperfect public monitoring than under perfect monitoring.====This paper builds on our previous work (Abito and Chen, 2021), which focuses on repeated games with perfect monitoring. This paper extends our prior work to stochastic games and games with imperfect monitoring. In fact, the range of games and information structure that our framework accommodates is only limited by the range for which the game theory literature can characterize the equilibrium payoff set of the appropriate solution concepts. For example, although we abstract away from unobserved heterogeneity in this paper, our framework can allow for it if the payoff set of equilibria with incomplete or private information can be characterized. Furthermore, we demonstrate how the framework can be useful to empirical work on collusion. It identifies structural parameters and helps detect collusion in a way that is free of equilibrium selection assumptions, quantifies the game theoretical insights on collusion (e.g., Rotemberg and Saloner, 1986), and is the first step towards enabling researchers to empirically study collusion with more realistic information structures. Similar to our prior work is Lee and Stewart (2016), who study the identification of payoffs in repeated games based on knowledge of the best-response correspondence.====The rest of this paper proceeds as follows. Section 2 presents the identification framework for a general dynamic game. We then apply the framework to a repeated quantity-setting duopoly in Section 3, a stochastic quantity-setting duopoly with demand states in Section 4, and a repeated R&D investment game with imperfect public monitoring in Section 5. We conclude in Section 6. The computer programs to replicate our numerical results in Sections 3.2, 3.4, 4.2, and 5.2 are available at ====.",A partial identification framework for dynamic games,https://www.sciencedirect.com/science/article/pii/S016771872200090X,28 December 2022,2022,Research Article,31.0
"Casacuberta Carlos,Gandelman Néstor","Universidad de la República, Uruguay,Universidad ORT Uruguay, Uruguay","Received 16 July 2021, Revised 16 December 2022, Accepted 24 December 2022, Available online 28 December 2022, Version of Record 20 January 2023.",https://doi.org/10.1016/j.ijindorg.2022.102916,Cited by (0),"In 2005, after a leftist coalition won the national election for the first time, Uruguay returned to sector-level wage bargaining councils with active government participation. We estimate product markups and wage markdowns using firm-level data for the period 2002–2016, and report decreasing wage markdowns and increasing -to a lesser extent- firm-level product markups. We find statistically significant impacts of minimum mandated wages on product markups and wage markdowns, and additional effects of unions on wage markdowns. The evidence suggests that firms operate in monopsonistic labor markets. Though their bargaining power in the labor market was reduced over time as a result of wage councils, firms were able to pass a sizable part of the increases in labor costs to consumers.","A broad discussion on measurement and changes in firm-level product markups is presently developing in the economics literature. This has important implications since market competition is the main driver of firm selection, productivity growth and welfare. It affects resource allocation between consumers and firms, and its evolution has profound macroeconomic consequences apart from the obvious effects on antitrust and tax policy design. The recent literature also emphasizes empirically disentangling firm-level product markup estimations from wage markdowns, ====, the ratio between price and marginal cost from the ratio of marginal labor revenue to wages. We contribute to this literature by considering the impact of sector-level wage negotiation on firm performance along such dimensions.====De Loecker and Warzynski (2012) and De Loecker et al. (2020) document a generalized rise in product markups in developed countries since the 1980s. Using data from the United States, they report that though product markups do not increase for most firms, there is a sharp increase for those in the upper tail of the markup distribution, which also gain market share. This generates an average product markup increase. They argue this leads to a declining labor share. Autor et al. (2017) discuss explanations for the fall in the labor share in the US economy in line with the findings of De Loecker and Warzynski (2012) about the impact of very large firms.====The magnitude and evolution of product markups in developing countries, and their relationship to the labor share and wage markdowns, have been less studied. We explore data for 2002–2016 from Uruguay, a small Latin American economy in which wage councils were set up to handle sector-level, centralized negotiations, with active government participation, and sector-level minimum wages increased substantially.====Methodologically, this paper is based on the Mertens (2022) extension of the De Loecker and Warzysnki (2012) approach.==== They propose a suitable alternative to obtain firm-level indicators of product markup over marginal cost. A set of recent papers (Mertens 2020, 2022; Yeh et al., 2022; Morlacco, 2019; Dobbelaere and Kiyota, 2018, among others) stress the importance of not considering wages as exogenous, but rather that firms have some degree of monopsonistic power in the labor market, hence facing an upward sloping labor supply schedule.====We contribute to the literature in several ways. First, we document the evolution of firm-level product markups and wage markdowns for a less-developed country. We go beyond manufacturing and include service sectors. The results show a different pattern than for many developed countries, including evidence of an increasing labor share of revenue. These trends are common to all firms and not exclusive to large ones.====Second, we distinguish between market power in output product markup and labor market wage markdown. We show that labor market wage markdown decreases are not compensated by increases of similar magnitude in firm's markups in their final goods markets. This suggests that although firms operated in monopsonistic labor markets, their overall market power was significantly eroded along the period.====Third, we show that changes in the institutional settings of wage negotiation ==== decreased wage markdowns and increased product markups. We address the main channel through which wage councils operate: setting minimum sectoral wages. The estimated elasticities imply that for every 1% increase in wage council mandated wages, firm wage markdown decreased by 0.10 to 0.15% while product markups increased by 0.10 to 0.14%. Thus, firms were able to pass a sizable portion of their increased labor costs to consumers. Given the change in mandated wages these elasticities account for the whole variation in product markups, but only for a quarter to a third of the variation in wage markdowns.====Fourth, we estimate the impact of union density on wage markdowns and find a statistically significant negative effect, with a wage markdowns elasticity to unionization of −0.08. Given the observed changes in union densities, this estimation implies that the direct effect is not economically meaningful and whatever effect unions produced on wage markdowns, it was mostly channeled through wage councils.","Wage councils, product markups and wage markdowns: Evidence from Uruguay",https://www.sciencedirect.com/science/article/pii/S0167718722000911,28 December 2022,2022,Research Article,32.0
"Langford Richard P.,Gillingham Kenneth","Managing Economist, Bates White Economic Consulting, 2001 K Street NW, North Building, Suite 500, Washington, DC 20036, United States,Yale University and NBER, 195 Prospect Street, New Haven, CT 06511, United States","Received 1 November 2020, Revised 30 November 2022, Accepted 7 December 2022, Available online 10 December 2022, Version of Record 24 December 2022.",https://doi.org/10.1016/j.ijindorg.2022.102904,Cited by (1),"This study models demand and supply in the market for new automobiles to estimate the economic benefits generated by the introduction of the hybrid electric vehicle. We estimate our model using all new vehicle registrations in California along with detailed demographic data. Our counterfactual analysis reveals that the addition of hybrids to the consumer choice set generates gains in social surplus that peak at roughly $1.2 billion annually in 2007, but environmental benefits make up 0.2% of the net benefits. We find that 31% of the social surplus in 2008 can be attributed directly to the hybrid vehicles independently from gains in fuel efficiency, with the remaining 69% attributed to the gains in fuel efficiency. We further show that all income groups benefit from the introduction of the hybrids, with the top 50% of the income distribution receiving the most.","Hybrid electric vehicles were first introduced to the United States in 2000 and by April 2016 over 4 million hybrid vehicles were sold, with a peak in market share of over 3% of the total light-duty vehicle market in 2013. By including an electric motor and regenerative braking, hybrids offer greater fuel economy than equivalent non-hybrid models, and have been an important contributor to firm compliance with Corporate Average Fuel Economy (CAFE) standards. Moreover, national policymakers took a keen interest in hybrids, incentivizing them with federal income tax deductions prior to 2006 and income tax credits that phased out with the number of hybrid vehicles sold for each manufacturer, before finally being eliminated in December 31, 2010. The core technologies in hybrids also laid the foundation for plug-in hybrid electric vehicles and electric vehicles, both of which have been subsidized with tax credits up to $7,500. The introduction of these technologies has been heralded by policymakers and reporters as a way to reduce carbon emissions, local air pollutant emissions, and dependency on imported oil. What has been less discussed, however, are the overall welfare effects of these new technologies.====In addition to any possible benefits from reduced environmental and energy security externalities, the welfare effects of the introduction of hybrid vehicles also include changes in consumer and producer surplus after accounting for the substitution patterns in new vehicle sales associated with the new equilibrium in the market. The welfare effects of the introduction of new technologies have long been of broad interest to economists (Bresnahan and Gordon, 1997), with notable work estimating the consumer benefits from many different new products, including cellular phones (Hausman, 1999), health care technology (Trajtenberg, 1989), computers (Bresnahan, 1986), and automobiles (Feenstra, 1988, Berry, Levinsohn, Pakes, 1993, Fershtman, Gandal, 1998, Petrin, 2002). This body of previous work has shown that the consumer surplus gains from new technologies can be quite large, a benefit of the Schumpeterian process of firms innovating to gain a first-mover advantage and transitory market power.====This study estimates a structural model of demand and supply in the automobile market to estimate the total consumer surplus effects of the introduction of hybrid vehicles. We use data on the complete set of new vehicle registrations from California over the period 2001–2008, along with household-level income and zip code-level demographics, to estimate substitution patterns in this market, which is the largest in the United States. The market is sufficiently large that automakers are even willing to develop technologies and trims primarily to serve this market.==== We perform a counterfactual analysis removing hybrid vehicles to compute a new equilibrium in the market and the welfare consequences of the introduction of the hybrid, and find that the addition of hybrids to the consumer choice set generates gains in social surplus that peak at roughly $1.2 billion annually in 2007.====The compensating variation is estimated at $35 (all values are in real 2017 dollars) for each new vehicle purchaser in 2001 and $409 in 2008, and is mostly driven by the success of the Toyota Prius. We find that compensating variation varies with income bracket, with the upper 50% benefiting more on average than the lower 50%. We also estimate an upper bound for the carbon dioxide emissions reduction benefits from the introduction of the hybrid, and find that these environmental benefits range between 0.03% and 0.4% of the total expected benefits. This result is striking, considering the attention that has been focused on the environmental benefits of hybrids. These findings demonstrate that the welfare gains from the introduction of the new product and the increased competition it engendered far outweigh the environmental benefits that received the most attention.====In addition to decomposing the benefits of hybrids into consumer, producer, and environmental benefits, we also separate the benefits from the hybrids’ gains in fuel efficiency from the benefits of the hybrid innovations themselves. We find that 31% of the social surplus in 2008 can be attributed directly to the hybrid vehicles independently from gains in fuel efficiency, with the remaining 69% attributed to the gains in fuel efficiency. Moreover, we find that 95% of consumer benefits in 2008 are attributed directly to hybrid innovations. These findings are in line with recent results, such as those in Sexton and Sexton (2014), showing that merely owning a hybrid generates utility for the consumer.====Our methodology combines elements of the estimation strategies from Petrin (2002) and Berry et al. (2004) to fully take advantage of our rich data. Just as in Berry et al. (1995), we posit a structural oligopoly model for the automobile market to estimate demand and supply, incorporating unobservable vehicle quality in the estimation to allow for highly flexible substitution patterns. We also estimate our demand model separately, confirming that our demand results–which are some of our most important results–are reasonably robust to the simultaneous model estimation. On the demand side, we add further moments based on demographic data, as in Berry et al. (2004) (“micro-BLP”), but in our case these moments are at the zip code level. Petrin (2002) uses a somewhat similar approach to examine the market benefits generated by the introduction of the minivan. Our estimation and counterfactual mirrors Petrin (2002) in that we also simulate the removal of the new product and calculate the consumer benefits for different demographic groups. Our work differs in using a much richer dataset and being a setting with substantial policy interest and environmental implications.====Several recent papers have examined different aspects of the introduction of the hybrid vehicle. Beresteanu and Li (2011) is perhaps the closest to our work methodologically; they use a modification of the Petrin (2002) estimation strategy, aggregate data on vehicle sales in 22 Metropolitan Statistical Areas (MSAs) from 2001 to 2006, and survey data from a cross-section (2001 NHTS) to examine the impact of policies to promote hybrids on the market equilibrium. Our study differs from Beresteanu and Li (2011) in multiple ways: the research question, exact methodology, data available, and results are all different. We bring to bear all new vehicle registrations in the state of California over an eight-year period and demographic information for approximately 2000 zip codes based on the location of vehicle registration, allowing for more precise micro-level moments in our estimation.====Our work also contributes to the small, but growing literature on hybrids. In addition to Beresteanu and Li (2011), Gallagher and Muehlegger (2010) and Chandra et al. (2010) also examine the effects of different policy instruments to subsidize hybrids. Gallagher and Muehlegger (2010) show that sales tax waivers are more effective than income tax credits. Chandra et al. (2010) finds that that a rebate policy in Canada induced 26% of the hybrids sold during that rebate period. Sallee (2011) estimates the incidence of tax credits for the Toyota Prius, finding that they were fully captured by consumers. Our work differentiates between the consumer and producer benefits from the introduction of the hybrid, and is the first to demonstrate that the benefits from the introduction of the new technology far exceed the environmental benefits, a result that may be surprising given the attention focused on the environmental benefits. We also demonstrate that through the re-equilibration of prices in the vehicle market, while higher-income households did benefit more from hybrids, the introduction of hybrids benefitted households across the entire income distribution.====The next section provides some brief background on hybrid vehicles and describes our data. Section 3 lays out the model and discusses our estimation approach. Section 4 presents the results. Section 5 provides counterfactual analysis that simulate the automobile market without hybrids, and the automobile market where hybrid cars are introduced but lack fuel efficiency. In each scenario, we measure the corresponding changes in consumer utility and market share. Using these changes, we estimate the compensating variation as the market benefit of hybrids, and the change in carbon emissions as the social benefit of hybrids, as well as the aggregate benefits. Section 6 concludes.",Quantifying the benefits of the introduction of the hybrid electric vehicle,https://www.sciencedirect.com/science/article/pii/S0167718722000790,10 December 2022,2022,Research Article,33.0
"Carnehl Christoph,Weiergraeber Stefan","Department of Economics and IGIER, Bocconi University, Via Roentgen 1, 20136 Milano, Italy.,Department of Economics, Indiana University, 100 S Woodlawn Ave, Bloomington, IN 47405, USA.","Received 2 July 2021, Revised 26 October 2022, Accepted 28 November 2022, Available online 9 December 2022, Version of Record 25 December 2022.",https://doi.org/10.1016/j.ijindorg.2022.102902,Cited by (0),"We estimate a structural model of procurement auctions with private and common value components and asymmetric bidders using detailed contract-level data on the German market for railway passenger services. Exploiting exogenous variation in the procurement design, we disentangle the asymmetries in private costs from asymmetries in information about the common value. While each asymmetry can rationalize a firm’s dominance, understanding its source is crucial for evaluating the auction design as welfare and revenue implications depend on the source of dominance. Our results indicate that the incumbent is slightly more cost-efficient and has substantially more precise information about the common value component. If the bidders’ strategic response to the common value asymmetry were eliminated, the average probability of selecting the efficient firm would increase by 43%-points.","A substantial part of economic activity is conducted via procurement mechanisms.==== In many countries and organizations, the procurement of goods and services above a certain threshold value must be awarded competitively. While the advantages of competitive procedures motivate many procurement regulations, concerns have been raised about their effectiveness in several industries. One of the most frequent problems is the existence of incumbency advantages that prevent entrants from submitting competitive bids.==== Understanding the sources of incumbency advantages and designing procurement mechanisms well is essential not only because of the size of these markets but also because buyers often have much flexibility in the procurement design. If used correctly, this flexibility can help to improve procurement outcomes.==== Even though incumbency advantages are widely acknowledged, the empirical literature analyzing its sources is scarce.====In this paper, we combine exogenous variation in the procurement design with a structural auction model to empirically study the sources of the dominance of the former state monopolist Deutsche Bahn (====) in the German market for short-haul railway passenger services (SRPS). More than 20 years after the market’s liberalization, DB Regio still operates the majority of the traffic volume (67.1% in 2016, Monopolkommission, 2019). The German market for SRPS, with its size of around EUR 8 billion in subsidies for 2016, is an important example of incumbency dominance that shares many features with similar markets in other countries. For example, in the 1990s, many European countries liberalized their telecommunications, retail electricity, and transportation services sectors. In all these markets, asymmetries between firms are prevalent because entrants that became active in the market relatively recently compete with established, often state-owned, incumbents. While the aim of the liberalization was a more efficient provision of publicly subsidized goods through increased competition, the experiences have been mixed.====For the German SRPS market Lalive and Schmutzler (2008) provide evidence using reduced-form regressions that DB has a significantly higher probability of winning auctions than its competitors. Industry experts regularly bring up two potential explanations for the incumbent’s dominance: a more efficient cost structure of DB, i.e., a private value advantage, and better information about future ticket revenues, i.e., an informational advantage about a common value.====We show theoretically—in an extension of the model of Goeree and Offerman (2003) to asymmetric firms—that both channels can rationalize a dominant firm. Therefore, to assess the efficiency of the market, disentangling the asymmetries in private and common value components is essential. Most importantly, an informational advantage of a firm creates an amplified winner’s curse for the less informed bidders, which induces them to bid overly cautiously. This can potentially have detrimental effects on both auction revenues and the selection of the cost-efficient bidder. To the best of our knowledge, we are the first to empirically disentangle these asymmetries, which allows us to determine the reason underlying the apparent incumbency advantage.====As the former state monopolist, DB has more experience in providing services and, as a publicly held firm, it may have advantages in financing expenditures compared to its rivals. In addition, ticket revenues from operating a train line constitute a common value component and DB might have an informational advantage regarding future demand. The reason is that ====, the sales and marketing division of the DB holding that owns DB Regio, manages ticket sales even on lines it is not operating itself. Therefore, it has access to comprehensive ticket revenue and passenger data. Entrants—and even procurement agencies—typically do not have access to this information.====Competition authorities have raised concerns about DB’s exclusive data access for many years; see, Monopolkommission (2009).==== Recently, this debate has been reinvigorated by a comprehensive discussion of DB’s data monopoly in Monopolkommission (2019).==== Ticket revenues typically cover about 40% of the total cost of a contract and are equal to approximately 67% of the winning bid on average (Rödl and Partner, 2014); therefore, the common value component is a substantial part of the value of a contract in our application.====For our estimation, we use a detailed contract-level data set on German SRPS procurement auctions, covering the period from 1995 to 2011. A key feature of our data is that we observe plausibly exogenous variation in the auction design, which enables us to disentangle the two asymmetries. While some local procurement agencies decide to have the train operating firms bear the revenue risk from ticket sales, other agencies decide to bear the risk themselves. If the ticket revenues remain with the agency (====), the auction is a standard asymmetric IPV auction. If the train operating company is the claimant of the ticket revenues (====), the auction is one with a private value (cost) and a common value (ticket revenues) component.====We use the gross auctions in a first step to recover the cost distributions of the firms using standard methods following Athey and Haile (2002). The exogeneity of the contract mode implies that lines auctioned with gross contracts are not systematically different from lines auctioned with net contracts; hence, we can extrapolate the cost distribution from gross to net auctions. This allows us to recover the distribution of asymmetric information in net auctions. Intuitively, conditional on the estimated cost distributions, any systematic differences in bidding behavior between DB and the other firms have to derive from differences in the information about common values. Assuming that the choice between net and gross contracts is exogenous may seem restrictive at first sight. Therefore, we provide extensive evidence based on industry reports, statistical tests, and reduced-form regressions in support of this key assumption in Section 2.2.====The results of our structural analysis reveal a systematic cost advantage of DB over its entrant rivals. Importantly though, it is not as large as one may initially expect given DB’s dominance in the market for SRPS. When comparing the cost distributions across bidder types, we find that DB’s cost distribution is dominated in a first-order stochastic dominance sense in only 23% of the auctions.==== The estimation of the informational advantage of DB over its competitors reveals that, indeed, in most auctions, DB has significantly more precise information about future ticket revenues. For example, our estimates imply that an entrant’s residual uncertainty, i.e., the variance of the unknown ticket revenues after having conditioned on the own revenue signal, is on average 4.3 times higher than that of the incumbent.====In summary, our results support the concerns of the German competition policy advisory council in Monopolkommission, 2009, Monopolkommission, 2019 that DB’s dominance is at least partially due to its informational advantage. This may call for regulatory interventions. For example, a relatively easy measure to increase efficiency could be to award more gross contracts, eliminating the auction’s common value component. We study this intervention in a counterfactual analysis. We find that if the net auctions in our sample were procured as gross auctions, the average ex-ante efficiency, i.e., the probability of selecting the cost-efficient bidder, increases drastically from 17% to 90%. While both the additional noise introduced by the revenue signal and asymmetries in the cost distributions already lead to considerable efficiency losses in net auctions, the most important source of inefficiency is the asymmetric precision of information about the common value component. Finally, our counterfactuals reveal that gross auctions tend to result in higher agency revenues as well.====Auction settings with private and common value asymmetries are pervasive. In principle, our model and estimation strategy can also be applied to other settings. The only requirement is that the data feature some variation in auction modes allowing the researcher to estimate the private value distribution from one sample and use this distribution to extrapolate private values on the sample with additional common value uncertainty. Other potential applications include oil drilling auctions, procurement auctions with subcontracting requirements, and auctions of objects with resale value. Oil drilling auctions are sometimes in the form of drainage lease auctions next to an existing tract and sometimes in the form of wildcat auctions.==== While the private value component is likely to be asymmetric across bidders in both settings due to the use of different technologies, in drainage lease auctions, one firm tends to have more precise information about its value than its rivals, while in wildcat auctions, information is plausibly symmetrically distributed (Hendricks and Porter, 1988).====Our work belongs to the literature on asymmetric first-price auctions. In a seminal paper, Maskin and Riley (2000a) show that stochastically weaker firms bid more aggressively and stronger firms win with higher profits. Goeree and Offerman (2003) provide a tractable framework to study auctions that involve both private and common value components, which we extend to accommodate bidder asymmetries.====De Silva et al. (2003) analyze data on highway procurement in Oklahoma with reduced-form regressions and find that entrants win with lower bids than incumbents. De Silva et al. (2009) study a natural experiment in which an information release policy makes uncertainty more symmetric. Both papers consider a setting with private and common values as in Goeree and Offerman (2003). However, their empirical strategy follows a reduced-form approach that does not allow them to distinguish the two asymmetries nor to disentangle which asymmetry induces the observed bidding pattern. In contrast, we explicitly model private and common value asymmetries and quantify each of the asymmetries with a structural empirical model.====For the estimation of our model, we rely on the extensive literature on the structural estimation of auctions; see Athey and Haile (2007) for an overview. Guerre et al. (2000) show how first-price IPV auctions can be nonparametrically identified and estimated based on winning bids only. However, identifying structural parameters in common value auctions is more complicated. Several papers have suggested different modeling assumptions and data requirements to obtain identification. Li et al. (2000) discuss identification and nonparametric estimation of conditionally independent private information auctions. Their arguments rely on a parametric restriction of the value conditional on winning. Février (2008) shows identification based on a parametric restriction of the private signal distribution. He (2015) obtains identification of a symmetric pure common value setting assuming, analogous to our common value model, that the common value is equal to the sum of common value signals. Somaini (2020) shows how common values can be identified in a structural model using observable variation in bidder-specific cost shifters.====Hendricks and Porter (1988) demonstrate that informational asymmetries across bidders have important implications for bidding behavior in offshore drainage lease auctions. Several papers following Hendricks and Porter (1988) have looked at auctions featuring asymmetries in the information about a common value. Li and Philips (2012) analyze the predictions of the theoretical asymmetric common value auction model in Engelbrecht-Wiggans et al. (1983) in a reduced-form analysis. They find evidence for private information of neighboring firms in drainage lease auctions. Hendricks et al. (1994) extend the framework in Engelbrecht-Wiggans et al. (1983) to allow for reservation prices of an informed seller and test the model’s predictions on equilibrium bid distributions. In particular, they focus on a pure common value setting with informed bidders and uninformed bidders who do not observe a private signal.====Hong and Shum (2002) estimate a model with both private and common value components and symmetric bidders using procurement data from New Jersey. Their focus is on estimating the relative importance of private and common values for specific types of auctions. In contrast, we have precise information about which parts of the contracts correspond to private and which to common value components. This additional information allows us to focus on quantifying each asymmetry.====Similar to our approach, Athey et al. (2011) compare different auction formats for timber auctions. They estimate a structural model to study entry and bidding behavior when firms are asymmetric in their private value distribution.====Our approach also shares some features with the growing literature on procurement auctions with multi-dimensional signals, see, for example, Luo and Takahashi (2019), who compare different auction formats for highway procurement in Florida and find that carefully considering which party bears the risk of cost overruns can significantly increase the efficiency of the market.====Finally, Hunold and Wolf (2013) study the German market for SRPS using a similar data set. In reduced-form regressions, they find that DB is more likely to win longer and bigger contracts. Moreover, DB has an advantage in net auctions indicating an informational advantage of DB. Lalive et al. (2021) analyze the respective benefits of auctions and negotiations in the context of our application. While they focus on the trade-off between competitive auctions and non-competitive negotiations with one firm, we focus on the specific contract design once the agency has decided to run a competitive procurement auction.",Bidder asymmetries in procurement auctions: Efficiency vs. information – Evidence from railway passenger services,https://www.sciencedirect.com/science/article/pii/S0167718722000777,9 December 2022,2022,Research Article,34.0
Adachi Takanori,"Graduate School of Management and Graduate School of Economics, Kyoto University, Japan","Received 6 December 2021, Revised 9 November 2022, Accepted 13 November 2022, Available online 28 November 2022, Version of Record 5 December 2022.",https://doi.org/10.1016/j.ijindorg.2022.102893,Cited by (0),", and ","This paper explores the welfare effects of third-degree price discrimination in oligopoly. Specifically, we consider a fairly general setting, and present sufficient conditions under which oligopolistic third-degree price discrimination increases or decreases Marshallian social welfare (i.e., the sum of consumer and producer surplus) when all discriminatory markets are served even in the absence of price discrimination. To do this task, we employ the ==== as a unifying methodology: a technique often used in public economics (Chetty, 2009, Kleven, 2021, Adachi, Fabinger, 2022) as well as macroeconomics (Barnichon and Mesters, 2022). Our analysis is mainly developed under firm symmetry; however, it can readily be extended to accommodate heterogeneous firms (see Online Appendix C). Moreover, our analysis permits a moderate degree of cost differences across separate markets.====Under third-degree price discrimination, consumers are segmented into separate markets and charged different unit prices in accordance with their identifiable characteristics (e.g., age, occupation, location, or time of purchase). In contrast, all consumers are charged the same price if third-degree price discrimination is not practiced (i.e., “uniform pricing”). Without loss of generality, the case of two markets can be considered to understand how price discrimination might change welfare in each market. If all firms are symmetric, the prevailing equilibrium price is common in either market whether price discrimination or uniform pricing is implemented. In this situation, if a discriminatory price becomes greater than the uniform price in one market, and the unit price decreases in the other market, the former market is traditionally called a “strong” market (====), and the latter a “weak” market (====) in the literature since Robinson (1933).==== More formally, this situation is expressed by ====, where ==== and ==== are the equilibrium prices under price discrimination in the strong and weak markets, respectively, and ==== is the equilibrium uniform price.==== Given such a price change, price discrimination increases output and social welfare in the weak market, but decreases them in the strong market. What are the overall effects of the price change?====In the analysis below, we follow Leontief (1940), Silberberg (1970), Schmalensee (1981), Holmes (1989), and Aguirre et al. (2010) to add the constraint ====, where ==== is interpreted as an artificial constraint on the profit maximization problem for oligopolistic firms under symmetry. Then, the regime change, which is discrete in its nature, is now measured by ==== and is continuously connected between ==== as uniform pricing and ==== as price discrimination in equilibrium. This formulation enables us to describe social welfare as a function of ====, ====, and characterize ==== in terms of economic concepts based on elasticity terms of market demand. In this way, whether social welfare improves or deteriorates by this global change of the regime can be determined. This methodology shares the central idea of the sufficient statistics approach where welfare consequences of policy changes are derived “in terms of estimable elasticities” (Kleven, 2021, p. 516). One benefit of focusing on sufficient statistics “rather than deep primitives” (Chetty, 2009, p. 452) in conducting welfare analysis is that one can focus on the deeper ==== that is “robust across a broad class of underlying models” (Kleven, 2021, p. 535) without a particular specification of market demand. If we instead start with a specific class of demand, it remains unclear to what extent the welfare analysis is valid under another class of market demand.====Our sufficient conditions for oligopolistic price discrimination to increase or decrease social welfare are provided by means of a cross-market comparison of the multiplications of three of the following economic concepts: (i) ====, which is the difference between price and marginal cost (====); (ii) ====, i.e., how the price responds to a small change in marginal cost (====); and (iii) ====, which measures the degree of market monopolization (====). These three sufficient statistics are determined by the following two first-order and two second-order elasticities: (a) the own price elasticity of the firm’s demand (====), (b) the cross price elasticity of the firm’s demand (====), (c) the curvature of the firm’s demand (====), and (d) the elasticity of the cross-price effect of the firm’s demand (====).====Specifically, this paper demonstrates that the product of all three concepts, ====, provides the sufficient condition for the change in welfare. As explained in Section 3.2, the product of conduct and pass-through evaluated at the discriminatory prices ====, ====, in Fig. 1(A) is interpreted as ====, measuring how output in each individual market changes in response to a marginal change in price. To evaluate a marginal change in welfare, profit margin ==== should be considered because it measures the welfare gain or loss that results from a marginal change in quantity under imperfect competition in which the price exceeds marginal cost. In this way, the welfare implications can be obtained by means of a cross-market comparison of the quantity change multiplied by the profit margin.====Existing literature on third-degree price discrimination has a centennial tradition, pioneered by Pigou (1920) and Robinson (1933), with their main focus on whether price discrimination increases or decreases social welfare (see Varian, 1989, Armstrong, 2006, Armstrong, 2008; and Stole, 2007 for comprehensive surveys of this literature). Among others, Schmalensee (1981) and Aguirre et al. (hereafter, ACV) (2010) study how ==== relate to output and welfare effects. Third-degree price discrimination necessarily entails allocative inefficiency because some consumers exist who have the same marginal utility but face different prices simply because they belong to different markets. Thus, for third-degree price discrimination to increase social welfare, it must sufficiently expand aggregate output to offset such misallocation across markets. Schmalensee (1981) shows that an increase in aggregate output is a necessary condition for third-degree price discrimination to increase social welfare—a conclusion that is generalized by Varian (1985) and Schwartz (1990)—and ACV (2010) identify a sufficient condition for price discrimination to raise social welfare: inverse demand in the weak market is more convex than that in the strong market at the discriminatory prices. Fig. 1(B) provides a graphical illustration of ACV’s (2010) argument: if uniform pricing is implemented instead, welfare loss in the weak market due to the output reduction that has arisen under price discrimination is sufficiently large (the right panel) as compared to the welfare gain in the strong market (the left panel), provided that the inverse demand in the weak market is sufficiently convex as compared to that in the strong market.====However, these studies are limited to ==== third-degree discrimination: to date, “there are ==== as to how discrimination impacts welfare” (Hendel and Nevo, 2013, p. 2723; emphasis added) when ==== competition is considered. For example, Holmes (1989) employs the same technique used by Schmalensee (1981) and ACV (2010) to examine the output effects of third-degree price discrimination in a symmetric oligopoly (see Section 3 for details). However, Holmes (1989) provides no welfare predictions (see also Dastidar, 2006).==== In this paper, we contribute to the literature by providing fairly general conditions regarding whether oligopolistic price discrimination increases or decreases social welfare.====Notably, our analysis does not necessitate the assumption of no cost differentials between discriminatory markets. In almost all theoretical studies on price discrimination, this assumption is made mainly to focus on demand differences. However, in many real-world cases of price discrimination, cost differentials are quite often observed, such as in the typical example of freight charges across regional markets with different transportation and storage costs (Phlips, 1983, pp. 5–7). In the narrowest definition of price discrimination, this might not be considered price discrimination because they can be regarded as distinct products. However, airlines can be arguably motivated to offer different types of seats because they seek to exploit heterogeneity among consumers. In light of this observation, this study permits a moderate amount of cost differentials to exist across discriminatory markets. Specifically, our analysis below needs not employ an explicit assumption regarding constant marginal costs in strong and weak market, ==== and ====, as long as the second-order conditions for profit maximization are satisfied and a sufficiently large discrepancy between ==== and ==== does not change the order of discriminatory prices from the one with no cost differentials.==== Fig. 1 also reflects this generalization: in (A), ==== and ==== are different, whereas in (B), marginal cost, ====, is common for both strong and weak markets.====In a closely related study, Chen et al. (2021) extend Chen and Schwartz’ (2015) analysis of monopoly to investigate the welfare effects of cost-based price discrimination (“differential pricing”) in oligopoly.==== In their setting, demand in market ==== with ==== symmetric firms is given by (using the notation of Chen et al. (2021)) ====, where ==== is the price vector in market ==== and ==== is the weight for market ==== satisfying ====. As such, market heterogeneity arises only from the supply side—firms’ marginal costs are different across markets—because own and cross elasticities are ==== that result from the common demand component, ====. Under this setting, Chen et al. (2021) are able to identify demand conditions to determine aggregate social welfare—expressed in terms of the weights ====—is concave or convex as a function of price: because the uniform price lies in between the discriminatory prices, social welfare is higher (corr. lower) under differential pricing if the welfare function is convex (corr. concave).====However, aggregate social welfare is no longer expressed in this simple manner once ==== is allowed. A typical situation when cost-based price discrimination can be at issue comes from the universal service requirement and fairness concerns (Okada, 2014, Geruso, 2017, DellaVigna, Gentzkow, 2019). In these cases, markets segmented by, e.g., geographical areas would differ in terms of price elasticities of market demand, and if so, the methodology of Chen et al. (2021) is no longer valid. In contrast, our analysis provides welfare implications more directly. As pointed out by Chen and Schwartz (2015, p. 103), our methodology “neither implies nor is implied” by the conditions in the analysis of Chen and Schwartz (2015) for monopoly and Chen et al. (2021) for oligopoly. In this sense, the analysis by Chen et al. (2021) and mine are not mutually exclusive but are complementary.====Our study is also in line with Mrázová and Neary (2017) who show the usefulness of ====—the relationship between demand elasticity and convexity which is not ascribed to a function or a correspondence—in comparative statics by suggesting the linkage between these first- and second-order elasticities and sufficient statistics such as markup and pass-through as shown in an empirical study by De Loecker et al. (2016).==== Mrázová and Neary (2017) point out that one of the advantages of working with the demand manifold instead of the demand function per se is that it is clearer to understand results from comparative statics and counterfactual experiments because demand elasticity and curvature are more closely related to them than demand primitives themselves.==== However, Mrázová and Neary (2017) mainly focus on perfect and monopolistic competition: when firm heterogeneity is taken into account, only cost/productivity heterogeneity à la Melitz (2003) is considered. In other words, ==== ==== ==== ==== ==== in Mrázová and Neary’s (2017) analysis because product differentiation in a strategic context is not taken into account. Therefore, Mrázová and Neary (2017) are only able to focus on two parameters, ==== and ====. While we do not make use of their method directly, we explicitly consider imperfect competition based on product differentiation: further research would be promising to investigate how Mrázová and Neary’s (2017) methodology can be more utilized for welfare analysis of imperfectly competitive behavior with the use of sufficient statistics.====Our methodology has the following policy implications. Admittedly, our welfare predictions are not “perfect” in that they are stated only as ==== conditions that justify the current regime: for example, the first part of Proposition 1 below provides one sufficient condition for when price discrimination is justified from a standpoint of social welfare. Hence, one may still miss some other parametric cases of market demand that can also support price discrimination simply because our sufficient condition does not hold. However, our results enable one to conclude that once our sufficient condition holds, a regime change that bans price discrimination==== decreases social welfare. In this sense, our sufficient conditions are “conservative” but “secure” in line with the “====” principle behind juridical decisions: it is important to prevent the “innocent” from being mistakenly judged as “guilty”.====The remainder of this paper is organized as follows. Section 2 presents our base model of oligopolistic pricing with symmetric firms and constant marginal costs. Then, we derive the sufficient statistics implications of welfare effects of price discrimination in Section 3. Subsequently, Section 4 provide parametric examples of three representative classes of market demand with product differentiation that are often employed in applies studies: linear, CES (constant elasticity of substitution), and multinomial logit with outside option. Section 5 concludes.==== Implications of aggregate output and consumer surplus are provided in Online Appendix B, and we argue in Online Appendix C that our methodology can be readily extended when firm heterogeneity is introduced.",A sufficient statistics approach for welfare analysis of oligopolistic third‐degree price discrimination,https://www.sciencedirect.com/science/article/pii/S0167718722000686,28 November 2022,2022,Research Article,35.0
"Cambini Carlo,Grinza Elena,Sabatino Lorien","Department of Management, Politecnico di Torino, Corso Duca degli Abruzzi 24 - 10129, Turin, Italy,Department of Economics, Social Studies, Applied Mathematics, and Statistics, University of Turin, Corso Unione Sovietica 218/bis - 10134, Turin, Italy,CEBRIG - Université Libre de Bruxelles; LABORatorio Riccardo Revelli, Belgium","Received 16 November 2021, Revised 16 November 2022, Accepted 18 November 2022, Available online 25 November 2022, Version of Record 5 December 2022.",https://doi.org/10.1016/j.ijindorg.2022.102901,Cited by (1),"We study the impact of ultra-fast broadband (UFB) infrastructures on the total factor productivity (TFP) and labor productivity of firms. We use unique balanced panel data for the 2013–2019 period on incorporated firms in Italy. Using the geographical location of the firms, we merge firm-level data with municipality-level information on the diffusion of UFB, which started in 2015. We derive consistent firm-level TFP estimates by adopting a version of the Ackerberg et al.’s (2015) method, which also accounts for firm fixed effects. We then assess the impact of UFB on productivity and deal with the endogeneity of UFB by exploiting the physical distance between each municipality and the closest optical packet backbone node. Our results show an overall positive impact of UFB on productivity. Services companies benefit the most from advanced broadband technologies, as do firms located in the North-West and South of Italy. We further decompose the impact of full-fiber networks from mixed copper-fiber connections and find that the former significantly contribute to enhancing firm productivity. Finally, by exploiting Labor Force Survey data, we provide suggestive evidence that productivity increases from UFB might be related to structural changes at the workforce level.","After one year of the COVID-19 pandemic and its shattering effects on our economies, on the 9th of March 2021, the European Union (EU) released a new plan to spur digitization and the deployment of digital infrastructures in order to “empower businesses and people in a human-centered, sustainable, and more prosperous digital future”. This plan, named “Digital Compass 2030”, is aimed at stimulating a substantial digital transformation of EU firms and public services.==== However, such a dramatic change in businesses would only be possible if all the firms and citizens participated in the digital transformation. The EU plan specifically considers the deployment of ultra-fast broadband (UFB) networks as a critical infrastructure to obtain such targets. These networks rely on optical fiber rather than copper wire in the “last mile”. Fiber-based networks are considered the most resilient, secure, and trustworthy infrastructures to support all the emergent digital technologies and they thus represent a key asset for the economic success and recovery of the EU. All across Europe - and particularly in Italy - Next Generation EU funds are largely being devoted to the deployment of UFB infrastructures.====The goal of this paper is to provide the first quantitative assessment of the impact of UFB investments on the productivity of firms. We focus on firm productivity, both labor and total factor productivity (TFP), as it is widely recognized as the ultimate engine of growth in today’s global economies (OECD, 2015). The deployment of UFB infrastructures can lead to several effects on economic activities and, in particular, on the productive performance of firms. Productivity may be spurred ==== more efficient business processes, the use of new digital technologies, and the acceleration of innovation enabled by UFB networks. For instance, UFB connections allow the transfer, analysis, and digital storage of larger amounts of better-quality data at a lower cost (Benassi et al., 2021). Moreover, UFB is a key enabling factor for flexible patterns of work, such as teleworking and smart-working, which may be associated with higher productivity (and labor force participation; Akerman et al., 2015). Italy represents an interesting field experiment to evaluate the effects of UFB connections. Starting from 2015, massive investments have been made in UFB infrastructures, mostly driven by the national incumbent telecommunication operator, TIM.==== Moreover, a new operator (OpenFiber) entered the market in 2017 and has significantly fostered UFB deployment.====We exploit a unique balanced panel data set for the 2013–2019 period, which collects municipality-level information on UFB access merged with firm-level data on Italian private-sector incorporated companies that were active in 2019. Our data are obtained from different sources. First, we have access to data from both the incumbent and the new operator, and we observe the staggered diffusion of UFB connections for each municipality. We also observe whether each municipality has access to basic fiber (FTTC) connections, or to the most advanced full-fiber (FTTH) networks.==== Second, we gather firm-level information from AIDA, a rich data set provided by the Bureau Van Dijk. This data set collects financial and other firm-level information on the Italian companies whose balance sheets are required to be filed with the chambers of commerce (i.e., incorporated companies). We thus merge UFB municipality-level data with firm-level data by using the operating center location of the firms. Third, we use representative data on Italian private-sector employees from the Labor Force Survey (LFS). Although we do not have firm-level data on the composition of the workforce, the LFS allows us to explore the relationship between UFB diffusion and relevant workforce indicators (e.g., based on skills and age) at detailed levels of aggregation. Finally, we retrieve municipality demographics, which we use as controls in the econometric analysis, from the Italian Statistical Office (ISTAT).====The empirical analysis is performed in two steps. The first one recovers measures of firm productivity. The TFP estimates of firms are obtained as the estimation residuals of sector-specific log-linearized value-added production functions. We pay attention to the consistent estimation of TFP by adopting a recent semi-parametric method based on Ackerberg et al. (2015) and Lee et al. (2019). In particular, our TFP indicators control for the simultaneity of production inputs and explicitly remove unobserved time-invariant firm heterogeneity. We instead compute labor productivity as the logarithm of value added over the number of employees. Labor productivity focuses on one input of the production process (i.e., labor), while TFP provides a commonly used indicator for the overall productive performance of a company (DeStefano, Kneller, Timmis, 2018, Devicienti, Grinza, Vannoni, 2018, Van Biesebroeck, 2007). In the second step of the empirical analysis, we investigate the impact of UFB access on the TFP and labor productivity of firms. Potential identification issues may arise from the non-randomness of the UFB diffusion process. Unobserved local shocks may affect both the UFB investment decisions and firm productivity, thereby biasing the ordinary least squares (OLS) estimates. To account for such endogeneity concerns, we use an instrumental variable (IV) approach that exploits plausibly exogenous variation in the distance between each municipality and its closest national optical packet backbone node (hereafter, we refer to it as “OPB node” or simply “node”; Cambini, Sabatino, Campante, Durante, Sobbrio, 2017, Miner, 2015). These nodes are upgraded facilities of the old telephone communication network constructed around the early 2000s, hence their location can be considered exogenous to the current productivity levels once we account for firm-level fixed effects and common time trends.====The results suggest an overall positive impact of UFB on firm productivity, by 2.9% for TFP and 3.9% for labor productivity. However, such an overall positive impact hides substantial heterogeneities across industry sectors and geographical locations. First, we find that only services companies obtain significant productivity gains from UFB connections. Along the same lines, only firms in the North-West and South of Italy are found to experience significant productivity increases from UFB connections. We then decompose the effect of FTTH from basic UFB connections. Although basic UFB infrastructures are found to be significant drivers of firm productivity, the results show an additional positive contribution of the most advanced FTTH networks. Finally, by using LFS data, we provide suggestive evidence that the diffusion of UFB is associated with substantial changes in the composition of the workforce of firms. In particular, we find that the UFB roll-out is related to significant decreases in the share of low-skilled workers employed by firms, as well as a significant increase in the share of young employees. These results are consistent with the existence of a substantial generational and skill-based digital divide (Akerman, Gaarder, Mogstad, 2015, Card, DiNardo, 2021, Goos, Manning, Salomons, 2014, OECD, 2018), whereby firms tend to reconfigure their workforce in favor of employees that are more able to exploit the advantages offered by the digital revolution.====These results are particularly important for the current policy debate around the European digital transformation. The recovery and resilience plans of most EU countries contain relevant public interventions to spread UFB connections. For instance, in 2021, the Italian government decided to invest as much as 3.8 billion euros in its national recovery plan to complete the roll-out of FTTH connections within the period 2022–2026. This study presents the first quantitative support for similar public policies. Our results show that such plans can have a substantial positive effect on firm productivity.====The remainder of the paper is organized as follows. Section 2 provides a review of the existing empirical studies on this topic and highlights the contributions of this paper to the extant literature. Section 3 discusses the data and the productivity measures used in the paper, and presents preliminary descriptive evidence. Section 4 describes the empirical setting and the adopted identification strategy. Section 5 presents and discusses the results. Finally, Section 6 concludes and highlights possible avenues for future research.",Ultra-fast broadband access and productivity: Evidence from Italian firms,https://www.sciencedirect.com/science/article/pii/S0167718722000765,25 November 2022,2022,Research Article,36.0
"Bourreau Marc,Manenti Fabio M.","Department of Economics and Social Sciences, Telecom Paris, Institut Polytechnique de Paris, France,CESifo, Munich, Germany,Dipartimento di Scienze Economiche ed Aziendali “M. Fanno”, Università di Padova, Padova, Italy","Received 8 July 2021, Revised 4 November 2022, Accepted 7 November 2022, Available online 15 November 2022, Version of Record 23 November 2022.",https://doi.org/10.1016/j.ijindorg.2022.102892,Cited by (0),"We develop a model of strategic geoblocking, where two competing multi-channel retailers, located in different countries, can decide to block access to their online store from foreign consumers. We characterize the equilibrium when firms decide unilaterally whether to introduce geoblocking restrictions. We show that geoblocking allows firms to soften competition, but at the cost of lower demand. A ban on geoblocking leads to lower prices, both offline and online. However, when firms can invest in increasing online demand, the ban may have adverse effects on investment and social welfare. We extend our analysis to account for price discrimination and investigate the role of shipping costs.","With the advent of the internet, new online sellers entered the retail markets and started to compete with traditional brick-and-mortar retailers (Brynjolfsson et al., 2009). In a response to this new competition, traditional retailers have invested in online stores alongside their brick-and-mortar stores to expand their sales (Pozzi, 2013). Today, most retailers are ====, selling both online and offline. However, for cost saving or strategic reasons, multi-channel retailers sometimes restrict online purchases from consumers in foreign countries, a practice known as ====.====The widespread adoption of geoblocking restrictions by retailers has been considered responsible for the low development of cross-border electronic commerce in the European Union (EU, 2016).==== According to Eurostat, in 2019, where 63% of European consumers purchased goods online, less than one third shopped from an online merchant based in another European country.==== Thus, the formation of a Digital Single Market, one of the priorities of the European Union based on the elimination of the barriers preventing the free movement of goods, persons, services, and capital within the EU, is still to come. In a response to this, in December 2018, the European Commission approved a new regulation banning geoblocking restrictions for the online sales of goods and services.====Duch-Brown and Martens, 2016 and Duch-Brown et al., 2020 evaluated the ban’s potential effect using data on consumer electronics products sold in EU countries from 2012 to 2015. Duch-Brown and Martens, 2016 estimate that a ban on geoblocking can generate a 0.7% increase in consumer surplus, mainly due to lower retail prices. Duch-Brown et al., 2020 find more modest gains in consumer surplus and welfare. Those gains accrue mainly to the broader variety of products available to the consumers with more comprehensive market integration, while the effect of the ban on prices is negligible.====Duch-Brown and Martens, 2016 and Duch-Brown et al., 2020 compare a situation where European markets are fully segmented (i.e., all retailers are assumed to have implemented geoblocking restrictions) to a situation where they are fully integrated after the ban on geoblocking. However, the European Commission’s sector inquiry showed that geoblocking was a ==== business decision of the retailers.==== In this paper, we investigate whether the industry-wide adoption of geoblocking restrictions is likely to arise without coordination between retailers.====Furthermore, these studies focus on the impact of the ban on product prices. They suggest that a ban on geoblocking can stimulate competition and increase static efficiency. However, some parties raised concerns about the potential distortions the ban may entail in terms of dynamic efficiency. For example, UEAPME, the association of crafts and SMEs in Europe, argued that the ban could impede the adoption of e-commerce by SMEs due to the increased competition in online markets.==== In this paper, we consider the impact of the ban on retailers’ efforts in enhancing online demand.====We develop a setting where two retailers, located in different countries, operate a traditional brick-and-mortar channel and an online channel. The retailers can decide to block access to their online store from foreign consumers. They then sell their products to offline and online shoppers, setting uniform prices across channels. We address three sets of questions. First, do geoblocking restrictions lead to higher prices? Second, when will firms adopt geoblocking restrictions? Third, what are the effects of a ban on geoblocking on consumer surplus and welfare?====We begin by showing that geoblocking restrictions lead to higher retail prices, both offline and online. When both retailers implement geoblocking restrictions, this is because each acts as a local monopolist in its home market. When one retailer introduces restrictions but not the other, the reason is subtler. The retailer that geoblocks access to its online store commits to be a soft competitor in the online retail market. The non-geoblocking retailer also becomes less aggressive because it has a monopoly over its online shoppers. These two effects reinforce each other, driving up prices.====Regarding our second question, we find that two distinct effects determine when a retailer wants to introduce geoblocking restrictions unilaterally: a ==== effect and a ==== effect. Intuitively, by blocking access to its online store, a retailer loses demand from foreign consumers and therefore, profits. However, geoblocking also represents a “puppy dog” strategy in Fudenberg and Tirole (1984)’s taxonomy. Opening the online store to foreign consumers makes a retailer appear “tough” in the competition for online shoppers. Since retail prices are strategic complements, the retailer should “under-invest” by geoblocking access to its online store to soften competition. When retailers offer sufficiently differentiated products, the competition softening effect is small relative to the demand loss effect, in which case retailers do not equally introduce geoblocking restrictions. Conversely, if retailers’ products are strong substitutes, the competition softening effect is the primary determinant of retailers’ decisions, and all of them adopt geoblocking restrictions. However, we find that asymmetric equilibria are also possible, where one retailer implements geoblocking restrictions but not the other. We show that a prisoner’s dilemma can also arise where retailers do not introduce geoblocking restrictions, whereas it would be profitable to do so from the industry point of view.====For our third question, we find that a ban leads to (weakly) lower retail prices, both online and offline, thus increasing consumer surplus. However, the magnitude of the decrease in prices depends on whether all the firms, only some of them, or none of them, would adopt geoblocking restrictions in the absence of the ban. As we have shown, all three situations may arise as an equilibrium outcome. By contrast, firms are hurt when geoblocking is banned. However, using a linear demand model with differentiated products, we show that the ban (weakly) increases the overall social welfare.====It is also important to account for the investments that firms can make to increase online demand. For example, firms can run marketing campaigns to raise consumer awareness of the existence of their online store. Using our linear demand model, we find that when geoblocking is banned, retailers react by reducing their investment. Under-investment in the online channel is another “puppy-dog” strategy that retailers can adopt to soften competition in the online retail market. From a policy perspective, the ban on geoblocking then involves a trade-off between lower prices and reduced investment in the online channel. We show that when a firm’s investment barely affects its demand in the foreign market (low investment spillovers), the ban on geoblocking leads to lower consumer surplus and social welfare. Conversely, if a firm’s investment increases online demand in the foreign market (high investment spillovers), the ban can in some cases lead to higher consumer surplus and social welfare.====In our baseline model, we consider that retailers charge uniform prices across their offline and online channels. This assumption is in line with the empirical evidence provided by Cavallo (2017), who shows that most multi-channel retailers charge the same price in their online and offline stores.====We investigate the robustness of our analysis when firms can charge different prices in their offline and online stores. We find that our main results carry through. When firms sell differentiated products, the demand loss effect is the main driving force for retailers, and they do not introduce geoblocking restrictions. Conversely, when products are strong substitutes, the magnitude of the competition softening effect is high, and retailers all block access to their online store from foreign shoppers.====As an extension, we also consider delivery costs for online purchases. We find that when firms can decide how much of these costs to pass through to their consumers in the shipping fee, they can implement a third-degree price discrimination scheme, charging different total prices to online and offline customers. In particular, this occurs when firms’ products are sufficiently differentiated. However, when products are strong substitutes, retailers offer free-shipping to online customers and implement non-discriminatory prices, as in the baseline model.====To sum up, we show that an industry-wide adoption of geoblocking restrictions may not always arise when retailers make unilateral decisions. Our results suggest that the evaluation of the impact of the ban on geoblocking should account for the degree of differentiation in the relevant markets, this practice being less likely to be adopted in markets with strong differentiation. Furthermore, our findings highlight a possible countervailing effect of the ban, with multi-channel retailers having the incentive to slow down their online development in response to the ban.==== Our paper contributes to the literature that explores the competition between online and offline retailers.==== To the best of our knowledge, the literature has not investigated multi-channel retailers’ incentives to block cross-border (online) sales, which represents our main contribution in this paper.====The existing literature has addressed a broad set of interesting questions, such as the strategic response of a traditional retailer facing the threat of entry of an online competitor (Liu, Gupta, Zhang, 2006, Dinlersoz, Pereira, 2007), the competition between pure brick-and-mortar retailers and online retailers (Loginova, 2009, Guo, Lai, 2017), or how an online marketplace can shape retail competition (Baye and Morgan, 2001). We differentiate from those earlier studies by considering the competition between multi-channel retailers, and study their incentives to introduce geoblocking restrictions. We then analyze the welfare impact of a ban on geoblocking.====Our paper also contributes to the literature that studies competition between firms that have loyal consumers (the offline market in our model) while at the same time competing for unattached consumers (the online market in our model). The literature on multi-sided platforms (Rochet, Tirole, 2003, Armstrong, 2006) shows that platforms can segment their market by offering differentiated matching services. Segmentation may arise endogenously due to self-selection of users on each side of the market because of network externalities (Ambrus and Argenziano, 2009), users’ preference for compatible matches (Gal-Or, Gal-Or, Penmetsa, 2019, Gal-Or, 2020), or competitive conditions in the product market (Karle et al., 2020). Segmentation may also be the result of specific strategies adopted by the platforms. For instance, the literature shows that the introduction of Most Favored Nation (MFN) clauses, which prevent sellers from offering their services at lower prices on alternative sales channels, may allow competing platforms to segment their market and relax competition.====Closest to our paper, Calzada et al. (2021) develop a model with two differentiated sellers (hotels) that decide whether to be listed on one or two intermediary platforms (online travel agencies). The authors show that segmentation is more likely to occur in equilibrium when the platforms impose MNF clauses, with sellers single-homing on different intermediaries.==== Similar to our findings, Calzada et al. (2021) show that segmentation arises in equilibrium when sellers are perceived as sufficiently substitute. Segmentation reduces the competitive pressure for sellers, even if it entails a lower demand. In our context, geoblocking represents such a segmenting tool, allowing sellers to relax competition at the cost of lower demand. Compared to Calzada et al. (2021), we consider a different market structure where sellers have their own (offline and online) distribution channels and do not need access to an intermediary platform. We also focus on a different policy question: the impact of a ban on geoblocking, affecting firms’ decisions concerning their distribution channels, rather than a ban on price restrictions such as MFN clauses.====The EU Regulation prohibiting geoblocking and other forms of geo-discrimination intends to promote the integration of digital markets and stimulate free electronic trade in Europe. With geoblocking, a firm gives up on selling its product abroad. In this view, our model also relates to the literature on international trade, which, since the seminal works of Brander and Krugman (1983), Brander and Spencer (1985), and Helpman and Krugman (1985), has studied the virtues and limits of free trade. More specifically, our article is related to the literature that studies antidumping in oligopolistic industries (see Blonigen, Prusa, 2003, Blonigen and Prusa, 2015 for a survey).====In this literature, the contribution most closely related to ours is Collie and Mai Le (2010). They consider a setting where a domestic and a foreign firm sell differentiated products in segmented markets. An antidumping regulation prevents the foreign firm from selling its product at a lower price abroad. The authors analyze the domestic firm’s decision to export in the foreign country. They show that the domestic firm may strategically choose not to export its product, giving the foreign firm a monopoly in the foreign market. Similar to our geoblocking scenario, this induces the foreign firm to raise the price in its market and, due to the antidumping restrictions, makes it less aggressive in the domestic market. We extend Collie and Mai Le (2010)’s study in several directions. In our setting, both firms decide whether to “export” their product via an online store, leading to strategic interactions in export decisions. In addition, we study the long-term effects of these decisions on firms’ investments and assess the role of shipping costs, a specific feature of electronic commerce.====We also contribute to the literature on partitioned prices. The literature has rationalized partitioned pricing in models with rational consumers (Ellison, 2005) and consumers suffering from behavioral biases (Gabaix and Laibson, 2006). We consider rational consumers who can anticipate their purchase’s total price when retailers charge a base price for the product and a shipping fee for online delivery. With rational consumers, retailers may use shipping fees to discriminate between online customers (Li and Dinlersoz, 2012). In our setting, shipping fees also allow retailers to price discriminate between online and offline shoppers. A more novel result is that when retailers’ products are strong substitutes, competition for online shoppers is intense, which leads firms to offer free-shipping to online consumers. Thus, our model provides another explanation for free-shipping, which is discussed in the literature, but merely seen as a promotional strategy to stimulate sales (see, e.g., Lewis, Singh, Fay, 2006, Frischmann, Hinz, Skiera, 2012, Chaoqun, Ngwe).====Finally, our analysis is reminiscent of the literature on strategic product lines in oligopolistic competition (Judd, 1983, Johnson, Myatt, 2003). This literature shows that multiproduct firms may expand or contract their product lines as a reaction to market entry. Similar to Johnson and Myatt (2003), in our framework a firm may expand its sales (by not geoblocking) or contract them (by geoblocking). Hence, geoblocking can be interpreted as a form of “product line pruning” where the firm discontinues its foreign market to soften competition from the rival firm.====The rest of the paper is organized as follows. We set up the model in Section 2. We then study the impact of geoblocking restrictions on prices in Section 3. In Section 4, we determine when geoblocking restrictions are adopted by retailers. In Section 5, we allow firms to invest in enhancing online demand. In Section 6, we consider price discrimination between the offline and online channels and investigate the role of shipping fees. In Section 7, we conclude.",Selling cross-border in online markets: The impact of the ban on geoblocking strategies,https://www.sciencedirect.com/science/article/pii/S0167718722000674,15 November 2022,2022,Research Article,37.0
"Gibbon Alexandra J.,Schain Jan Philip","Düsseldorf Institute for Competition Economics (DICE), Heinrich-Heine University Düsseldorf, Germany","Received 6 April 2022, Revised 29 September 2022, Accepted 21 November 2022, Available online 23 November 2022.",https://doi.org/10.1016/j.ijindorg.2022.102900,Cited by (0),"This paper analyses the impact of common ownership on markups and innovation and adds to the discussion of the recently observed patterns of a long term rise in market power. Using a panel of European manufacturing firms from 2005 to 2016, we structurally infer markups and construct a measure of common ownership. We use a propensity score reweighting estimator to eliminate biases due to observational characteristics and find an increase of firm markups ranging up to 3.3% on average in ","The recently observed pattern of a long term rise in market power accompanied by increasing industry concentration (De Loecker, Eeckhout, Unger, 2020, Autor, Dorn, Katz, Patterson, Van Reenen, et al., 2020, Syverson, 2019) has sparked interest and worries in the economic consequences and causes of this phenomenon. Simultaneously, the rapidly increasing prevalence of diversified institutional investors has changed industry concentration by creating ownership links between competing companies (Azar et al., 2018). Common ownership, defined as indirect corporate networks of at least two competing firms held by overlapping sets of institutional investors, is potentially one reason that we observe rising markups across many industries and countries.====Institutional investors held on average around 40% of Western European countries’ GDP in assets under management in 2018 (OECD, 2019), with common ownership emerging from a concentration of few but large investors within the same industry====. Investors owning larger shares of an industry are in a position to exert a certain degree of influence on directly competing companies. Economists (for example Azar et al., 2018) argue that in settings of common ownership, firms might no longer take strategic decisions independently. Aligned shareholder value maximisation incentives of firms provide room for possible anti-competitive behaviour regarding prices or innovation. Particular cases of interventions by common owners have gained attention from the media====. Apart from direct interventions, a reduction in performance-based managerial incentives by common owners constitutes a possible mechanism leading to anti-competitive outcomes (Antón et al., 2022).====Overlapping ownership structures affect the competitive landscape of firms in several dimensions. Early theoretical articles, (such as Reynolds, Snapp, 1986, Bresnahan, Salop, 1986, Salop, O’Brien, 2000) extend the classical concentration measure Herfindahl-Hirschman Index (HHI) to a modified Herfindahl-Hirschman Index (MHHI), taking into account ownership links at the industry level, and conclude that common ownership exerts an upward pressure on prices through rivals’ profit internalisation and may facilitate collusion (Gilo, Moshe, Spiegel, 2006, Shelegia, Spiegel, 2012). Firms compete less aggressively, as the negative effect on competing firms’ profit is partly taken into account through the common owners. Bayona et al. (2022) show that even monopoly profits can be replicated allowing for endogenous common ownership links. A relatively new theoretical paper (López and Vives, 2019) calls this the cartelisation effect of common ownership. In addition to these anti-competitive results, the authors find that common ownership can also have a beneficial effect. Depending on technological spillovers in a given industry, common ownership can spur innovation by increasing the marginal benefit of investment in research and development (R&D). They conclude that in markets with high technological spillovers, firms internalise the ==== decrease in competitors’ marginal costs caused by their own innovation through common ownership. Thus, this increases the marginal benefit of innovation which leads to higher innovation levels and possibly lower prices, also described by Shelegia and Spiegel (2022). On the contrary, Gutiérrez and Philippon (2017) show empirically that higher concentration and higher levels of common ownership tend to characterise industries with less investment in capital and R&D (i.e. industries with lower technological capacities). Also adding to the ambiguity, others find no significant effects or challenge the methodologies used for identification of anti-competitive effects (O’Brien, Waehrer, 2017, Rock, Rubinfeld, 2018, Patel, 2018, Lambert, Sykuta, 2019, Koch, Panayides, Thomas, Koch, Panayides, et al., 2021 and Lewellen and Lowry, 2021)====. More empirical research on the effects of common ownership is required, as the theoretical predictions on competition are ambiguous.====This article investigates the relationship of common ownership, markups, and innovation on a broad European manufacturing sample of large firms, as categorised by the European Commission, between 2005 and 2016. Using data from Bureau van Dijk’s Amadeus database and accounting for input endogeneity following Ackerberg et al. (2015), we estimate industry-specific production functions and calculate markups as in De Loecker and Warzynski (2012). Innovation activity by firms is measured by patents weighted with forward citations. Furthermore, we use the detailed ownership information available in Amadeus to construct the MHHI as used in other empirical studies (for example Azar et al., 2018), which we exploit as a measure of treatment intensity. We use a measure of technology spillovers by Bloom et al. (2013) at the three-digit NACE industry level as well as an industry classification of technological capacities by the European Commission (2019) to investigate in more detail how the effect of common ownership on markups and innovation varies along these dimensions, and to contribute to the further disambiguation of the effects of common ownership.====We use a propensity score reweighting estimator to control for biases due to observational characteristics. We define a binary treatment indicator taking the value one in markets in which at least one additional investment by an institutional investor creates common ownership links between competitors for the first time. This constitutes the first occurrence of common ownership in a given market. Control firms operate in markets that never experience common ownership. Commonly used approaches of exploiting institutional mergers (e.g. He, Huang, 2017, Azar, Schmalz, Tecu, 2018) and stock index inclusion of firms and rivals (e.g. Kennedy, O’Brien, Song, Waehrer, et al., 2017, Boller, Scott Morton, 2020) are less appropriate here, as we observe only a small number of firms listed in a stock index relative to the whole sample.====We find a positive effect of common ownership on firm markups that is significantly increasing with technological spillovers and ranges up to 6% in high spillover industries. The positive effect of common ownership on markups becomes stronger with increasing treatment intensity, relying on percentiles of the distribution of MHHI delta. Splitting the sample into four groups of increasing technological capacities (low, medium-low, medium-high, and high technology) according to the European Commission (2019), we find pronounced effects on markups in low-tech and high-tech industries.====Considering the impact on innovation activity, the emergence of common ownership has in fact a positive and statistically significant effect on citation-weighted patents in high spillover markets for inside firms, which are firms directly held by common investors. For these firms, the effect is increasing in spillovers and reaches up to 13% in high-spillover industries. By splitting up the sample with respect to technological capacities as defined by the European Commission (2019), we confirm the finding of a positive effect on inside firms that is increasing with technological capacities. For outside firms competing in the same market with commonly owned firms, we find only insignificant results for innovation activity. Both findings for markups and innovation activity are consistent with theoretical findings in López and Vives (2019). Our results are robust with respect to the measure of common ownership, regression and production function specifications, as well as a one-to-one propensity score matching approach combined with a difference-in-differences setup.====Although there are some empirical industry-specific studies that analyse anti-competitive effects of common ownership on prices==== in the airline and banking industry (Azar, Schmalz, Tecu, 2018, Azar, Raina, Schmalz, 2022), there is less work on a wider firm panel containing multiple industries. Backus et al. (2021) perform a calibration exercise on firms in the S&P500 index with initial markup estimates taken from De Loecker and Eeckhout (2021). In our analysis we abstract from general equilibrium effects as in Azar, Vives, 2021, Azar, Vives, 2021; Ederer and Pellegrino (2022). Kini et al. (2022) investigate the effect of common ownership on product differentiation of US listed companies. They also analyse firm markups and investment as outcome variables and find no average effect on markups, but a positive effect in industries characterised by high technological spillovers. For investments, they find an average positive effect that is more pronounced in high-spillover industries. The results on investments are consistent with our findings on innovation, but our results differ in terms of markups. Antón et al. (2021b) find positive correlations of common ownership in US firms with innovation activities and R&D expenditures, which are amplified differently in settings of either technological or product market spillovers. Kostovetsky and Manconi (2020) show increased intensity of patent citations among firms owned by overlapping institutional investors.====This article is substantially different from the existing literature and contributes in five main ways. First, we analyse a broad manufacturing sample in Europe that mostly consists of non-listed firms, whereas almost the entire empirical literature on common ownership is based on data sets of US listed firms and often focuses on specific industries. Second, we shed some light on the effect of common ownership on firms which are not directly commonly owned but which, in fact, compete in a market where there are common ownership links between rivals. This aspect has been largely neglected in the literature. Third, we offer a detailed analysis of industry characteristics regarding technological capacities and spillovers that drive the results of common ownership. Fourth, in direct comparison to Kini et al. (2022), this article focuses solely on citation-weighted patents as a more precise measure of innovation activity, as opposed to a wider range of investments as an outcome variable, consisting of capital expenditures, R&D expenditures, and acquisitions. This is advantageous, because innovation output is more important for welfare than innovation input, and the theoretical foundation given in López and Vives (2019) focuses on innovation spillovers only and may not be trivially extended to general investments in capital. Fifth, on a broader scale, our article also contributes to the rising market power discussion, as we find a pattern of rising markups in our sample for the top 10 percent of companies in the markup distribution.====The rest of this article proceeds as follows. Section 2 gives an overview of the data set and markup estimation. The identification strategy and the measure for common ownership are presented in Section 3. Results of the propensity reweighting estimator follow in Section 4 and robustness checks are reported in Section 5. Section 6 discusses the results and draws conclusions for future investigations and applications.","Rising markups, common ownership, and technological capacities",https://www.sciencedirect.com/science/article/pii/S0167718722000753,Available online 23 November 2022,2022,Research Article,41.0
"Jin Ginger Zhe,Leccese Mario,Wagman Liad","University of Maryland & NBER United States,University of Maryland United States,Illinois Institute of Technology United States","Received 29 October 2021, Revised 28 October 2022, Accepted 1 November 2022, Available online 12 November 2022.",https://doi.org/10.1016/j.ijindorg.2022.102891,Cited by (0),"Some argue that large platforms, such as Alphabet/Google, Amazon, Apple, Facebook and Microsoft (or GAFAM), are unusual in their number, pace and concentration of technology ====, with the potential to harm market competition. Using a unique taxonomy developed by S&P Global Market Intelligence, we conduct a descriptive study of GAFAM’s M&A activities, comparing them to those of other top acquirers from 2010 to 2020. We find: (i) GAFAM completed more tech acquisitions per firm than other groups of top acquirers, and acquired younger and more consumer-facing firms on average. (ii) The top 25 private equity firms outpaced GAFAM in tech acquisitions per firm since 2018. (iii) GAFAM acquisitions are less concentrated across tech categories than other top acquirer groups, due, in part, to an “acquire-adjacent-and-then-expand” strategy. (iv) Over time, more and more GAFAM and other top acquirers acquire in the same categories. (v) No evidence suggesting that a GAFAM acquisition in a category, compared to similar categories without GAFAM acquisitions, is correlated with a slowdown in the number of new acquirers acquiring in that category. Overall, we find that technology acquisitions do not shield GAFAM from potential competition that may arise from other GAFAM members or other firms that acquire in the same categories.","Recently, concerns have been raised about the mergers and acquisitions (M&A) activities of large incumbent firms, especially if the target firm is a small nascent competitor or produces complementary inputs. In theory, an acquirer could shut down innovations of the target firm (so-called “killer acquisitions,” Cunningham et al. (2021)), extend its existing market power to another market via acquisition and thereby discourage new innovative entrants in this market (creating a “kill zone,” Kamepalli et al. (2020)), or reduce competitor access to complementary assets (“raising rival’s costs,” Salop and Scheffman (1983); Ordover et al. (1990)). Regardless of the mechanism, these theories share the concern that acquisitions of innovative targets could curb innovation, preempt future competition, and reinforce the acquirer’s market position.====Broadly, these concerns could apply to M&A in all sectors, but the recent debates have highlighted them in technology. Much of the spotlight has been concentrated on the five leading technology platforms, namely Alphabet (Google), Apple, Facebook, Amazon, and Microsoft — known as GAFAM. Collectively, the five companies acquired hundreds of firms in recent years, raising the concern among government agencies and scholars that their M&A practices may be unusual in pace, volume and concentration. For example, scholars have argued that the nature of digital platforms — namely the indirect network effects between multiple sides of a platform — may have motivated incumbents like GAFAM to acquire nascent firms, especially if the target is an emerging competitor or collects complementary user data, allowing the incumbent to better monetize the data on the other sides of the platform (Motta and Peitz, 2021). It is also of potential concern that these motives would generate a systematic M&A pattern that is difficult to identify in each single acquisition, which has the potential to harm market competition in the long run (Scott Morton, Dinielli, 2020, Scott Morton, Dinielli, 2020).====In conjunction, antitrust enforcers around the globe have expressed interest in the topic. For instance, the UK’s Unlocking Digital Competition Report (March 2019) states that “Over the last 10 years, the 5 largest firms have made over 400 acquisitions globally” and that “this pace is not slowing, with close to 250 acquisitions in the last 5 years.”==== The US Congress issued a 2020 Majority Staff Report on Investigation of Competition in Digital Markets, similarly stating that GAFAM “acquired hundreds of companies just in the last ten years.”==== In September 2021, the US Federal Trade Commission released a staff report describing 600+ acquisitions completed by the five GAFAM firms between 2010 and 2019, including those not reported to the US antitrust agencies under the Hart-Scott-Rodino Act.====. Similar interest in GAFAM acquisitions was expressed in Australia====., France==== and the European Commission.====Recent legislative bills and proposals that relate to M&A activity have also been specifically targeted at GAFAM. For example, the proposed House of Representatives’ H.R. 3826, called the Platform Competition and Opportunity Act, would restrict “covered platforms” from acquiring competitors or potential competitors.==== Covered platforms are defined as those that (i) have at least 50 million U.S.-based monthly active users or at least 100,000 U.S.-based monthly active business users, (ii) have net annual sales or a market capitalization greater than $600 billion, and (iii) are critical trading partners for the sale or provision of any product or service offered on or directly related to the platform. Such criteria entail that only GAFAM firms, as of the time of the legislative proposal, would be impacted by the bill.====Outside the U.S., the European Union’s Digital Markets Act (DMA) provides certain presumptions for a company to be designated as “gatekeeper.” Those presumptions, in large part, relate to size: (i) an EU-wide turnover of at least EUR 7.5 billion in each of the last three financial years, or (ii) a market capitalization of at least EUR 75 billion in the last financial year, and (iii) at least 45 million monthly active end-users and at least 10,000 business users in the EU for at least one core platform service in the last financial year.==== With the exception of Verizon Wireless, through its acquisition of Yahoo, the five GAFAM firms were the only five firms that qualified as gatekeepers under the different iterations of and proposals for the DMA’s numerical thresholds for gatekeepers.==== Under the DMA, a gatekeeper must inform the EU Commission of any intended acquisition of a company that provides services in the digital sector, irrespective of whether the transaction triggers a merger filing requirement at the EU or Member State level. The EU Commission may then initiate merger control proceedings, even if the relevant filing thresholds are not met. Moreover, the EU Commission, should it determine that a gatekeeper engaged in systematic non-compliance, may altogether prohibit a gatekeeper firm from acquisitions in the digital sector for a period of time it deems proportionate to the non-compliance.====Despite the intense attention from regulators and policymakers, most of the supporting evidence focuses on the M&A history of GAFAM firms, ==== comparing them to other top acquirers or the overall trends of technology M&A. Such a comparison is difficult to make because the existing literature does not have a good method to classify acquired technology firms into detailed industry categories and subcategories. The commonly used North American Industry Classification System (NAICS), for instance, is limited, because even when using the most specific (6-digit) NAICS code, a large number of the acquired technology entities tend to fall under “Software Publishers” or “Internet Service Providers” (Werden and Froeb, 2018). As a result, researchers have so far adopted industry categorizations that are either ad-hoc, and/or specific to the relatively small sample of transactions under consideration (see, e.g., the 2019 LEAR report, Gautier and Lamesch (2021))====, or lack a taxonomy with a structured hierarchy (e.g., as in the 2020 House Majority Staff Report), making it difficult to cohesively evaluate whether acquisitions fall in adjacent or unrelated industries. As a result, the comparisons between GAFAM’s acquisitions relative to other leading technology acquirers have been limited.====In this paper, we aim to shed light on the pace, volume and concentration of GAFAM acquisitions relative to acquisitions by other top acquirers, including major technology companies and private equity firms. To do so, we use a dataset from S&P Global Market Intelligence that specifically tracks technology acquisitions and categorizes all associated firms into a hierarchical taxonomy. More specifically, besides GAFAM, we identify three groups of companies that have been or have the potential to be top acquirers of technology companies. The first group includes the 25 top-ranked companies that appear in Forbes’s 2019 ranking of Top 100 Digital Companies, excluding GAFAM. The second group includes the 25 largest private equity firms ranked by Private Equity International as of December, 2020. The third group includes another 25 firms that have the highest number of acquisitions in our S&P dataset but do not appear in any of the other groups of top acquirers. We put more emphasis on the firms ranked high on these lists, because the ongoing legislative and policy debates use firm size to define the thresholds of applicability. A test of whether such thresholds are correctly chosen naturally entails a comparison between firms around the size thresholds.====We show that, on the one hand, GAFAM has completed a greater number of tech acquisitions per firm than the other groups of top acquirers, and on average, GAFAM does acquire younger and more business-to-consumers (B2C) firms but they are not necessarily firms whose products are more data-intensive. On the other hand, on a per-firm basis, the top 25 private equity firms have outpaced GAFAM in tech acquisitions since 2018. Combining all acquisitions from 2010 to 2020, GAFAM acquisitions are significantly less concentrated across categories than any of the top acquirer groups we considered. This is achieved by GAFAM first acquiring targets in adjacent categories and then expanding around them.====While it may be argued that acquisitions by GAFAM firms, as the larger incumbents, could amount to claiming competitive turf in the target firms’ categories, and could consequently deter entry by potential competitors, our findings suggest otherwise. Specifically, we do not find evidence suggesting that GAFAM entry via acquisitions in categories, in comparison to similar categories in which GAFAM has not yet acquired, are correlated with any slowdown in the number of new acquirers acquiring in the same categories after the initial acquisitions by GAFAM. In addition, it has been argued that the largest platforms are increasingly encroaching on each other’s turfs.==== Our findings confirm that, over time, more members of GAFAM acquire in the same categories, and other top acquirers also acquire in the same categories as GAFAM.====Admittedly, the categories defined by the S&P taxonomy do not necessarily align with antitrust market definitions. That said, observing different acquirers entering the same business area via M&A still informs the debate about potential and/or nascent competition that ==== happen in antitrust markets in or related to that business area. Our findings suggest that potential competition in the same category could arise from different firms that acquire ventures in that category. Empirically, at least in the period and categories we study, technology acquisition does not shield GAFAM from potential competition that may arise from other GAFAM members or other firms acquiring in the same industry categories.====We also acknowledge that the aforementioned legislative efforts may be driven not only by GAFAM’s M&A activities, but also the nature of platform business, market power concerns, and antitrust histories of specific firms. Our descriptive evidence focuses on M&A only, and thus does not speak to other reasons that could justify the proposed legislative or policy changes. We aim for this paper to be one of the first data-driven efforts to aid policymakers in better understanding the similarities and differences between GAFAM and other acquirers, and to better target legislation to address problematic behavior.====The remainder of the paper is organized as follows. Section 2 reviews the related literature. Section 3 describes the S&P data, including the advantages of the S&P taxonomy over other classifications in the literature, and our definition of top acquirers. Section 4 provides a brief summary of M&A activities of GAFAM and other groups of top acquirers. Section 5 documents how GAFAM and other top acquirers differ in their concentration of M&A, and how they expand M&A activities over time. Section 6 documents the extent to which top acquirers acquire in the same categories, as well as the degree of acquisitions overlap within and across groups of top acquirers. Section 7 concludes with what this descriptive paper can and cannot say about competition, M&A and innovation, and directions for researchers to dive into other important questions in this area.",How Do Top Acquirers Compare in Technology Mergers? New Evidence from an SP Taxonomy,https://www.sciencedirect.com/science/article/pii/S0167718722000662,Available online 12 November 2022,2022,Research Article,42.0
"Azar José,Barriola Xabier","School of Economics and Business, IESE Business School, and CEPR, University of Navarra, Av. Pearson 21, Barcelona 08034, Spain,INSEAD, Boulevard de Constance, Fontainebleau 77305, France","Received 25 November 2021, Revised 5 September 2022, Accepted 25 October 2022, Available online 6 November 2022, Version of Record 12 November 2022.",https://doi.org/10.1016/j.ijindorg.2022.102890,Cited by (1),"We study how the MillerCoors joint venture affected craft brewers in the United States. We use scanner data to track the entry, assortment, and market share of artisanal and commercial brewers over a 4-year period after the merger. Using an instrumental variables strategy that uses only variation in concentration generated by the merger, we find that, in the average market, the merger led to an 11.59% increase in the number of craft brewers. The number of products per craft brewer did not increase. Most of the new entrants were small, and thus the market share of craft brewers experienced only a small (though statistically significant) increase. This entry of new firms may have been facilitated by the increase in prices by incumbent commercial producers following the merger.","The brewing industry's market share has been historically dominated by a handful of large firms (Lynn, 2012). These companies have carried out a series of mergers and acquisitions to solidify their presence across the globe (Howard, 2014). However, in the last 20 years, craft brewers have been gaining traction in an industry that has increased from 413 companies with 28,416 employees in 2000 to 4733 breweries with 86,668 employees in 2019, with the average number of employees per brewery going down from a maximum of 76 in 2002 to a minimum of 18 in 2019, as can be seen in Fig. 1. One of the main reasons behind this successful entrepreneurial movement is how craft brewers have differentiated themselves from commercial brewers by offering specialized products (Clemons et al., 2006).==== This unexpected revolution has been labeled as the “Strangest, Happiest Economic Story in America” (The Atlantic, 2018).====A recent example of consolidation in the brewing industry was the joint venture of SABMiller and Molson Coors, announced in October 2007. The companies argued that the merger would allow them to create efficiencies in transportation because they would be able to brew their beers in seven plants across the United States (Heyer et al., 2009).==== However, the deal raised concerns about the possibility of increased market power and price coordination because it entailed the second and the third largest brewer in the country. The merger was approved in June 2008, with MillerCoors starting its operations on July 1, 2008. In a retrospective study, Ashenfelter et al. (2015) found that the joint venture benefited markets that were close to the seven locations, with an average price decrease of 2%, but at the national level, the average price of the most popular beer brands increased by 2%. Another study, by Miller and Weinberg (2017), showed that the merger caused the prices of the most popular beer brands to increase by 6–8%. Although the increase in prices for the largest beer brands was bad for consumers, it may also have encouraged entry by new firms, because higher prices make entry more profitable (Caradonna et al., 2021). On the other hand, it could be argued that a larger firm has more power to strategically deter entry (Marino and Zábojník, 2006). It is therefore an empirical question of how a merger affects the entry of smaller firms into a concentrated market.====In this article, we address this question by studying the effect of the MillerCoors joint venture on the number and market share of craft brewers. We examine the situation using a detailed scanner data set that allows us to track the evolution of craft beers from 2008 until 2011 in a group of 1088 grocery stores that are located in 967 zip codes that cover 36 states. We use the MillerCoors joint venture as a shock to measure the impact that a sudden increase in concentration had on the presence and the market share of craft brewers.==== Building upon the recent studies on the brewing industry that tackle the effect of concentration on prices (Ashenfelter et al., 2013) and on advertising (Chandra and Weinberg, 2018), we find that there is a positive relationship between concentration and the number of craft brewers active in the market. In particular, we show that if there is an increase of 0.04 in the Herfindahl-Hirschman Index, which corresponds to the average increase in concentration after the merger, the number of craft brewers would increase by 11.59%. Moreover, we show that this increase in concentration would also create a jump in the craft brewers’ volume share of 0.04%.==== Thus, we show that the merger had a positive effect on the number and market share of craft brewers. Moreover, we show that the number of products offered by each craft brewer was not affected by the merger.====Therefore, entry by craft brewers was made easier by the consolidation of commercial brewers. In our case, and along the lines of Ashenfelter et al. (2015) and Miller and Weinberg (2017), we show that an increase of 0.04 in market concentration led to a 1.35% increase in the price of commercial beers and a reduction of 0.78% in their volume share. This price increase by commercial brewers may have contributed to facilitating entry by the small new firms since consumers facing higher prices for incumbent products might be willing to purchase products that are entering the market and provide a new experience (Clemons et al., 2006).====Our article contributes to the literature on merger retrospectives. In contrast to the existing literature, which has primarily focused on studying the effects that horizontal mergers have on pricing by the merging firms across different industries, for example in airlines (Borenstein, 1990), banks (Prager and Hannan, 1998), consumer electronics (Ashenfelter et al., 2013), grocery stores (Arcidiacono et al., 2020), carbonated soft drinks (Dubé, 2005), hospitals (Garmon, 2017), and health insurance providers (Dafny et al., 2012), our focus is on the effect that a merger between two large companies had on the response from smaller players. By focusing on this specific response, we are making a contribution to the literature by showing that mergers by large incumbent firms can create opportunities for smaller firms to enter the market and gain market share.====The rest of the article is organized as follows: We review the literature in Section 2. We explain the data and the methods in Section 3. We present our results in Section 4. Finally, we conclude in Section 5.",Did the MillerCoors joint venture strengthen the craft beer revolution?,https://www.sciencedirect.com/science/article/pii/S0167718722000650,6 November 2022,2022,Research Article,43.0
"Nurmi Satu,Vanhala Juuso,Virén Matti","Statistics Finland, FI-00022, Finland,Bank of Finland, P.O. Box 160, FI-00101 Helsinki, Finland","Received 5 November 2020, Revised 17 October 2022, Accepted 19 October 2022, Available online 27 October 2022, Version of Record 5 November 2022.",https://doi.org/10.1016/j.ijindorg.2022.102888,Cited by (1),"This paper analyses zombie firms in a dynamic setup where firm survival not only depends on its current returns, but the firm's exit decision is forward-looking. Building on a model of firm entry and exit and using firm-level data from Finland, we show that the expected future value and growth of a firm are key determinants in whether it is likely to recover from losses accumulated during a spell of weak performance. We find that including firm growth in zombie identification substantially reduces zombie incidence in the data as one third of low interest coverage ratio firms in a common zombie definition are growing companies. Moreover, over a half of exits from zombie status are recoveries to become healthy firms. A policy that may promote the survival of zombie firms is public subsidies. Our analysis does indeed support the notion that subsidized firms are less likely to die, but also their chances of recovery are higher.","Following the global financial crisis, poorly performing firms labelled as “zombies” have attracted considerable attention (e.g., Acharya et al., 2016; Adalet McGowan et al., 2018; Banerjee and Hoffman, 2018). The underlying idea is that a set of firms permanently fails to be profitable, and these zombies create a burden to other firms and the financial sector, as well as to the government which may also support these unviable firms by means of accommodative policies. Zombie firms are commonly identified by measures based on their ability to cover their debt service costs from current profits, possibly augmented by some other condition, over a rather narrow time frame (e.g., Adalet McGowan et al., 2018; Rodano and Sette, 2019; Acharya et al., 2019). Thus, the measures of zombie incidence do not well correspond to idea of permanent failures. Rather, the measures are consistent with a static model of firm behavior.====This paper analyses zombie firms in a dynamic setup where firm survival not only depends on its current returns, but the firm's exit decision is forward-looking. The exit decision is a key issue in zombie identification as the expected future value of a firm determines whether the firm is likely to recover from losses accumulated during a spell of weak performance. Building on a model of firm entry and exit and using register-based micro data from Finland (1999–2019), we challenge claims of a secular increase in zombie incidence driven by more persistent survival of non-viable firms (e.g., Adalet McGowan et al., 2018; Banerjee and Hoffman, 2018). Our application, which uses firm-level data on public subsidies to firms, suggests that subsidized firms are less likely to die, but also their chances of recovery are higher. The latter contributes to lowering zombie incidence.====To tease out the distinctions among firms labelled as zombies, we start by considering firm viability through the lens of a standard model of firm entry and exit (e.g., Hopenhayn, 1992; Syverson, 2011). Using the model as a guide, we augment the common interest coverage ratio (ICR) -based zombie definition (e.g., Adalet McGowan et al., 2018) with a forward-looking element that requires zombies to be non-growing firms. This separates true zombies from growing firms with temporarily weak earnings, which are misidentified as zombies in common definitions. We then analyse the demographics of zombie firms with firm-level data. Instead of merely examining the changes in the stock of zombies, we study the flows of firms into and out of zombie status that underlie the fluctuations in zombie incidence. To highlight the fact that being a zombie is often not a terminal state that ultimately leads to firm closure, we decompose the zombie exit margin into exits out of the market and exits back to health.====Our findings challenge widely accepted views of a secularly increasing and high zombie incidence, driven by more persistent survival of permanently unviable firms as presented in the recent literature and policy narratives (e.g., Adalet McGowan et al., 2018; Banerjee and Hoffman, 2018; Duval and Obstfeld, 2018; Bindseil and Schaaf, 2020;Demertzis and Viegi, 2021). First, identifying zombies using a forward-looking element, as guided by our model, suggests that zombie incidence is considerably lower than earlier studies indicate. We find that a third of firms that were labelled as zombies by the common interest coverage ratio (ICR) -based definition were in fact growing companies in our data. Indeed, zombie firms, as commonly defined in the literature, are often not truly distressed firms so much as growing companies with temporarily weak performance measures. Moreover, over half of the firms we observe exiting zombie status returned to health. This high recovery rate among growing firms suggests that viable firms may exhibit weak performance measures due to temporary factors such as restructuring or periods of heavy investment. Although occurring at a lower rate, recoveries of downsizing true zombies suggests that the presence of zombies may be a natural part of firm dynamics in which the flow of earnings relative to interest payments in a firm varies over time. As a response to their weak performance, these firms take downsizing measures to restore profitability and promote recovery. Moreover, changes in zombie incidence are driven by cyclical zombie entries, not a secular decline in zombie exits as popular narratives suggest.====In our policy application, we analyse the relation of government subsidies to zombie survival and recovery at the firm level. Our data includes firm-level information on public subsidies to firms, both at the aggregate subsidy level and disaggregated to various subsidy categories. Estimation results suggest that public subsidies to firms are related to the presence of zombies and their allocation across industries. However, the selection of firms that receive subsidies suggests that zombie status does not increase the likelihood that a firm will receive subsidies, rather the opposite. We take a closer look at aggregate and specific subsidies in relation to zombie exits. Our competing risks model suggests that receiving a subsidy, both at the aggregate subsidy level and for individual subsidies, is negatively related to firm death, but also the chances of recovery for a subsidized firm are higher. The former may contribute to longer zombie spells, but the latter to shortening them. Our brief analysis of low interest rates yields similar results. The estimation results thus suggest that the relationship between policy and zombie incidence is more complex than in the simple narratives that focus only on zombie deaths but do not acknowledge recoveries (Duval and Obstfeld, 2018,Bindseil and Schaaf, 2020,Demertzis and Viegi, 2021).====Our study contributes to the recent literature that has exhibited a renewed interest in zombie firms in the aftermath of the Great Recession (e.g., Adalet McGowan et al., 2018; Acharya et al., 2019; Borio, 2018). These studies are suggestive of a secular rise in zombie incidence across OECD economies, raising concerns of resource misallocation and congestion effects of zombies on healthy firms and having ultimately negative effects on growth and employment. Zombification is generally seen as an outcome of bank forbearance or low interest rates. While bank forbearance is well-studied, starting with the seminal works on zombies in Japan's stagnating economy during the 1990s (e.g., Hoshi, 2000, 2006; Caballero et al., 2008), public subsidies to firms, the third obvious candidate for supporting zombies, have received surprisingly little attention (e.g., Jiang et al., 2017).====We proceed as follows. Section 2 starts with an overview of zombies in the literature. In Section 3, we examine the viability of firms from a theoretical perspective to provide a coherent framework for identifying zombies. Section 4 describes the data and zombie incidence in the data. Section 5 provides an analysis of zombie demographics, i.e. entry and exit to zombie status and exit destinations. Section 6 studies the role of public subsidies for zombie demographics. Section 7 concludes.",Are zombies for real? Evidence from zombie dynamics,https://www.sciencedirect.com/science/article/pii/S0167718722000637,27 October 2022,2022,Research Article,44.0
"Alderighi Marco,Nicolini Marcella","SEP - University of the Aosta Valley, Italy,GREEN - Bocconi University, Italy,DEM - University of Pavia, Italy","Received 24 February 2021, Revised 10 October 2022, Accepted 18 October 2022, Available online 22 October 2022, Version of Record 1 November 2022.",https://doi.org/10.1016/j.ijindorg.2022.102886,Cited by (0),"We investigate the Italian fuel market after an asymmetric transparency rule the came into force allowing gas stations to conceal some price cuts on the governmental price comparison platform, hardly visited by customers but used by oil companies to monitor the market. As oil companies usually set retail prices, and gas stations are in charge of communication, the disclosure of price cuts induces rival wholesalers to behave less aggressively. Consequently, the prices of those gas stations which conceal information and those of their neighboring competitors are, on average, higher. Empirical findings support the conclusions of the strategic information disclosure argument, and they leave some room for complementary explanations, such as differences in costs or in profitability.","From the 2000s, regulatory agencies worldwide started requiring retailers to post their prices on online price comparison platforms, aiming at increasing transparency to consumers. While the literature has mostly focused on how these price communication rules affect competition and eventually lead to collusion (Byrne, de Roos, 2019, Luco, 2019, Cabral et al., 2021), we study the link between communication rules and strategic information disclosure on platforms. We show that, when a transparency rule leaves some margin of flexibility to retailers on the communication of prices, those retailers which act strategically will gain from concealing some price variations.====Since 2015, Italian law has established a transparency requirement rule, which mandates gas stations to publish retail fuel prices on a public online platform managed by the Ministry for Economic Development (MED):==== firms are required to communicate the current price of the day at least once a week and all price hikes within the day, while firms are free to choose whether to communicate or not when prices remain stable or reduce.====The platform always shows the latest available price. Thus, if a retailer does not communicate his price cuts, the price on the MED platform is higher than the price at the pump. In spite of its purpose, there is strong evidence that the MED platform was hardly used by consumers, and therefore it has not directly affected consumers’ search costs in the way described in Chandra and Tappata (2011). The platform has instead altered market competition, as the pricing departments of the oil companies have employed it to monitor retail prices of rival gas stations and to set local retail prices for their affiliates (AGCM, 2013, Byrne, de Roos, 2017).====Data coming from the MED platform show that about 83% of gas stations communicate price changes more often than is strictly required, and 17% of them communicate only when it is mandatory. We rationalize this outcome as a strategic practice of some retailers targeted at upstream competitors who monitor the platform by means of a cheap talk game (Crawford and Sobel, 1982): wholesalers are in charge of setting the price of affiliated retailers, while strategic retailers can decide to communicate or not the wholesalers’ decision to charge low prices. We show that the strategic retailers choose to conceal low prices in order to induce the rival wholesalers to set high prices for their affiliated retailers. In this way, the strategic retailers can gain consumers at the expense of their local market competitors.====In the empirical analysis, we use daily data on posted prices from the official Ministerial platform during the year 2015 to support two theoretical predictions. First, prices reported and charged by strategic retailers are, on average, higher than those reported by the other retailers. This result provides a first support that this type of strategic behavior is present. Second, prices reported by retailers located near a strategic one are higher than those reported by retailers who do not face strategic players. This piece of evidence suggests that strategic retailers can cheat the rival wholesalers and induce them to choose higher prices. We also show that empirical findings leave some room for complementary explanations, such as differences in costs or in profitability; however, the relative importance of these complementary explanations cannot be fully identified because of data limitations. Finally, we discuss our results under different market conditions, and we focus on the behavior of unbranded retailers, and on the learning we observe in our sample. Our main predictions are confirmed across these robustness checks.====The rest of the paper is organized as follows. Section 2 discusses the related literature. Section 3 provides a description of the Italian fuel market. An illustrative model is presented in Section 4. Methodology and data are introduced, respectively, in Sections 5 and 6. Section 7 illustrates the empirical results, Section 8 presents the robustness checks, and Section 9 concludes the paper.",Strategic information disclosure in vertical markets,https://www.sciencedirect.com/science/article/pii/S0167718722000625,22 October 2022,2022,Research Article,45.0
"Noel Michael D.,Qiang Hongjie","School of Art & Science, Texas Tech University, Lubbock, Texas, USA,School of Business, Central South University, Changsha, Hunan, People's Republic of China","Received 5 June 2021, Revised 30 August 2022, Accepted 1 September 2022, Available online 13 September 2022, Version of Record 19 September 2022.",https://doi.org/10.1016/j.ijindorg.2022.102877,Cited by (0),"There is a large literature on incomplete contracts, but one prominent type of incomplete contract has largely gone unnoticed. An “open price contract” is one in which a buyer commits to purchasing goods from a seller even though the price has not been agreed upon at the time of signing. Open price contracts generally give the seller the right to set prices ex post, and the buyer is then obligated to purchase from the seller at those prices. This gives rise to obvious incentives for short-run opportunistic pricing by the seller, but also ==== from long-run reputational consequences. In this article, we focus on a specific application where open price contracts are common and test for opportunistic ex-post pricing by sellers on locked-in buyers. We find that sellers’ long-run reputational incentives dominate opportunistic incentives, consistent with lessons from the relational contracts literature.","The theory of contracts has attracted much attention from economists over the past thirty years, and for good reason. Contracting is ubiquitous in our world and endogenously arises in response to dynamic inefficiencies that are likely to occur in their absence. Theorists have long studied optimal contract design in a variety of situations, including moral hazard (Shavell, 1979, Grossman, Hart, 1983), adverse selection (Baron, Myerson, 1982, Maskin, Riley, 1984), and uncertainty over future states of the world (Grossman, Hart, 1986, Hart, Moore, 1988, and Hart and Moore, 1990). This last field of research is called incomplete contracting, and stems from the idea that it can be difficult to forecast, and thus contract upon, all possible states of the world in advance. When an unexpected state of the world occurs and requires contract renegotiations, the potential for opportunistic behavior by one or both parties ex post can lead to hold-up problems and inefficiency ex ante. Assigning residual decision-making rights to one party or the other can improve the outcome (Aghion, Holden, 2011, Hart, 2017), but the assignment of rights also meaningfully affects the distribution of surplus. A large empirical literature has examined various aspects of incomplete contracts, such as contract design, efficiency, competitive effects, and others, in a variety of real world settings (Hoppe, Schmitz, 2010, Antrás, Staiger, 2012, and Bajari et al., 2014 and others).====In spite of the interest in incomplete contracts generally, one very prominent type of incomplete contract has received less attention in the literature. “Open price contracts” or “open price term” contracts are incomplete contracts in which a very fundamental component of the contract - namely the price - is not set in advance.==== In an open price contract, a buyer commits to purchasing products from a seller over a specified period of time before there is any agreement as to what the price will be. In most cases, the seller holds residual rights to set prices and change them from time to time as desired, and the buyer is then locked-in and obligated to purchase at the prices so set and so changed. It is essentially an upside-down ultimatum game in which the buyer decides whether to buy first, and the seller sets the prices second.====It may seem unwise for a buyer to commit to buying without knowing the price in advance, but these types of contracts are ubiquitous in the economy. Most industries that operate on a franchise model – from restaurants to clothing outlets to gasoline stations – utilize some form of open price term in their franchisor-franchisee contracts. They tend to be long term contracts in which franchisees pay a fee, secure a license to use the franchisor’s name, and agree to purchase most major inputs from the franchisor (or its approved vendors) over the life of the contract. Because it is difficult to negotiate the price of inputs for each future period of time and each possible realization of the world, open price terms are used and generally assign pricing rights to the seller. There is a large literature on franchising contracts in general, but studies tend to focus on other aspects of franchise contracts such as integration choice (Brickley, Dark, 1987, Lafontaine, 1992, Vita, 2000, Forbes, Lederman, 2009) or franchise fee structure (Bhattacharyya, Lafontaine, 1995, Lafontaine, Shaw, 1999, Lafontaine and Slade, 2007), while saying less about the open price terms contained in these contracts.====In this article, we examine the effects of open price contracts on surplus distribution. We test whether open price contracts do or do not give rise to opportunistic pricing by the party that has ex post control over prices (the seller) at the expense of the party that is locked-in (the buyer) and captive to the first party’s choices.====One might expect that the holder of residual pricing rights - in this case the seller - would set ex post prices in a way to maximize its own surplus under the contract. Locked-in buyers could in principle be slowly bankrupted under such a contract, but would not rationally enter into such a contract with this expectation. The seller’s concern for its own long run reputation and its ability to recruit new buyers in the future is the obvious counteracting force that can limit its short run opportunistic pricing incentives on these buyers.====In this way, our study is closely related to the relational contracts literature. Relational contracts are “informal agreements and unwritten codes of conduct” that affect how firms interact with another (Baker et al., 2002) – essentially, self-enforcing agreements “characterized by the ability of one party to terminate the contract with the other following the detection of an undesired action by the latter” (Kosova and Sertsios, 2018). While a buyer cannot terminate a contract in our context, as it is still a formal contract, a buyer can refuse to renew and alert other potential signees to what it believes are undesirable pricing actions on the part of the seller, thus harming the seller’s long run reputation and its ability to sign up new buyers (Klein, Leffler, 1981, MacLeod, 2007). We expect that reputational incentives and the long run value of a seller’s current and expected future relationships should be important in governing some notion of fair or consistent short run pricing on the part of the seller, at least in the eyes of the buyer.====A large theoretical literature has emerged examining relational contracts under a variety of situations and assumptions, including in the presence of asymmetric information (Levin, 2003, Halac, 2012) and involving disagreements among parties over whether an opportunistic behavior has actually taken place (Li and Matouschek, 2013).==== One interesting result from the asymmetric information models especially relevant for us is that the more-informed firm may take an action that is not in its short-run best interest if doing so helps avoid reputation-damaging perceptions of price opportunism from the less-informed firms it deals with. In our context, this means it is possible that a seller could potentially set prices inefficiently ==== to its locked-in buyers, even when price opportunistic incentives are present, and even when higher prices would be justified by cost increases, simply because it seeks to prevent less-informed buyers from misinterpreting higher prices as opportunistic behavior, causing reputational damage and the eventual loss of its long run relationships.====So to what extent do sellers engage in short run opportunistic pricing with buyers locked into open price contracts, and to what extent would buyers be naturally accepting of some degree of price opportunism on the part of sellers when entering into these contracts? Or do sellers not engage in opportunistic pricing at all? Given the buyers do not know sellers’ costs as well as sellers’ do, could sellers actually do the opposite of opportunistic pricing, setting prices especially low exactly when the incentive for opportunistic pricing is the highest? The latter would be a fascinating result if found to be true.====In this article, we explore these questions empirically. We contribute to the incomplete contracts literature and the relational contracts literature, by examining the balance between short-run price opportunistic incentives under an open price contract on one hand, and long-run reputational incentives on the other, using a clean example of a relational element within an otherwise formal contract, and using an empirical identification methodology that (as we will discuss) is unique to the literature.====We are looking for very specific contract-induced competitive price effects, so a macro-style cross-industry analysis would not be as helpful. Institutional detail would be lost and the cross-industry aggregation would obscure the effects of interest. So we examine price opportunism in the context of a specific application, one that will allow us to control for institutional detail, and one that will prove to be particularly amenable to a clean identification strategy. We select the wholesale motor fuels industry.====The wholesale fuels industry is characterized by open price contracts between franchisors and franchisees, which surround the purchase of the central industry input – fuel. The gas station dealer (franchisee, or buyer) agrees to purchase all of its fuel needs from the wholesaler (franchisor, or seller) over the life of the contract at prices that are only announced by the seller ex post and changed from time to time as the seller sees fit. There are several ways a seller could price opportunistically, but we are led to one particular manifestation of price opportunism that has been the source of numerous lawsuits on unfair pricing and contract interference. It is also one that is related to a phenomenon of much interest to economists in the gasoline literature - that wholesalers may pass through its cost increases more quickly and its cost decreases more slowly on locked-in buyers. Unlike the previous literature, we are not concerned about whether asymmetric passthrough exists – it is well known that it does. Rather, our interest is in whether any such effects will be substantially more pronounced on a wholesaler’s locked-in buyers who are subject to open price contracts than to a wholesaler’s unrestricted buyers who are not bound by such contracts. We examine other manifestations of price opportunism as well – as simple as a seller suddenly raising prices on a buyer after signing an open price contract – but the cost passthrough effect will prove to be the most interesting.====While we would expect the wholesale gasoline industry to be reasonably representative of other franchise industries that use open price contracts (and we know that asymmetric cost passthrough is also common across other industries (Peltzman, 2000), we focus on the wholesale gasoline industry because it offers two key methodological advantages.====First, unlike most other franchise industries, the wholesale gasoline industry operates under a dual-sales-channel paradigm, meaning that we can observe two types of buying arrangements at the same time – one in which wholesale fuel is sold under open price contracts, and one in which the physically identical wholesale fuel is sold on the unrestricted market without obligation. This makes it straightforward to compare a seller’s pricing practices to its locked-in buyers on one hand and its unrestricted buyers on the other. We do this in our first analysis.====The obvious limitation with this approach is that it is a straight cross-sectional comparison across buyer types, and buyers can self-select into these buyer types. While we do not expect the selection issue to create any meaningful bias in our specific context for several reasons discussed later, we recognize that the treatment (of entering into an open price contract) is not randomly assigned and may be correlated with unobservable buyer characteristics that themselves affect seller pricing. Essentially, it creates a potential omitted variables bias (the omitted variable being the selection process itself) that can impact our cross-buyer comparisons (Clougherty et al., 2016).====For this reason, we exploit a unique natural experiment in the industry that has the benefit of avoiding these types of issues entirely. With this natural experiment, we can compare sellers’ pricing behavior not across different (potentially self-selected) buyer types but within the same buyer type and, in fact, within the exact same buyer and even within the exact same contract, all at the exact same time. Since we have the same locked-in buyer on both sides of these comparisons (at the same time), we are able to perfectly hold all buyer unobservables fixed and our within-buyer comparisons cannot be infected by unobservable buyer characteristics that vary across buyer type.====We defer a detailed explanation of the natural experiment until later, as it requires some institutional background, but to preview, it is based on comparing seller pricing behavior to the same locked-in buyer at the same time under two different degrees of price opportunistic ====. The natural experiment surrounds a large and historic shock to the added cost of refining premium grade gasoline in the early 2010s (up and above that of regular grade), which forced sellers to fundamentally rethink how they set premium grade prices in the first place. The shock was large, unanticipated, and unprecedented. Importantly, it was substantially different in nature to the everyday crude-based cost shocks that regularly cause prices to rise and fall and that are familiar to all involved. Unlike everyday crude shocks, the premium shock moved sellers and buyers into largely uncharted territory with little historical precedent to rely on for what to do, and created a blank slate and therefore a unique opportunity for sellers to engage in opportunistic pricing if so desired.====Essentially, we have two fundamentally different types of shocks that correspond to two different levels of “surprise” (or variance) in the realized state of the world, with the premium shock corresponding to a greater “surprise” in the state of the world and thus a greater opportunity for a seller to exercise opportunistic pricing on its locked-in buyers. In our second analysis, we compare these two types of shocks and test whether greater “surprises” lead to greater degrees of opportunistic pricing on the same locked-in buyers, with all buyer unobservables fixed.====We need not ignore unrestricted buyers in this second analysis, however, as they can still be used as an additional layer of control. They are neither locked-in nor subject to opportunistic pricing under either type of shock as they are free to shop around as desired in the competitive market. Thus, our analysis takes the usual form of a natural experiment with two dimensions of control. But rather than the usual treatment/control group dimension and before/after time dimension, our first dimension is the market’s level of “surprise” following a cost shock, and the second dimension is the buyer’s level of “susceptibility” to that surprise. Such a natural experiment is unique in the literature and represents the second methodological advantage we gain by studying the wholesale gasoline industry.====The results of our two analyses agree. Our prior was that we would either find a high degree of price opportunism (if reputational constraints were weak) or a low degree of price opportunism (if reputational constraints were strong) but what we actually find is a ==== degree of price opportunism. Not only do sellers in our application ==== price more opportunistically when the opportunity arises, they actually price ==== opportunistically exactly when they have the greatest opportunity to. We think this is an especially interesting result. It is consistent with the relational contracts literature in which reputational effects and the value of long run relationships take center stage, and consistent in particular with asymmetric-information relational models where sellers have to be especially cautious of protecting their reputation against any real or perceived transgressions with current or potential locked-in buyers. Essentially, sellers are providing their locked-in buyers with a form of market price volatility insurance, that is neither written nor required by the open price contract, and that helps insulate them from market price volatility stemming from unusual events. The same insurance is not provided to unrestricted buyers that have no long term commitment with the seller.====Our study thus contributes to the empirical literature on relational contracts by exploring the trade-off between short run opportunism and long run reputational concerns but in a well identified way, something that has been difficult to do in the literature.==== Macchiavello and Morjaria (2021) point out that identification is a persistent challenge because 1) relational contracts are difficult to measure in the first place and 2) data is typically thin and most often limited to questionnaires or surveys. It is especially problematic for studies in developing nations where formal contract enforcement is weak and relational contracts are the norm (Gamage, Priyanath, 2019, Ajwang, 2020, Yang, Song, Zhang, Wang, 2020, Zhou, Yang, Zhuang, 2020, Macchiavello, Morjaria, 2021).====Fortunately, these problems do not arise here. We examine a clear relational element (the open price term) in an otherwise largely formal contract in the United States, with excellent and complete ex-post price data on how that relational aspect was resolved and to whose benefit. We have substantial variation in the degree of opportunistic incentives across buyers and grades, and can isolate the important “unwritten” element of the contract – the price of fuel – using a dual identification methodology that shows how sellers strike a balance between the opportunistic pricing incentive on one hand and reputational concerns on the other, in an unusually clean setting.====Ours is also the first study that we are aware of to look at open price contracts specifically. Given the ubiquitous nature of these contracts in the U.S., it is surprising how little has been done on open price contracts, and we hope that the current study will kickstart new research in this interesting area. In all, we contribute to four existing literatures – incomplete contracts, relational contracts, gasoline pricing and competition, and franchisor-franchisee relationships – plus what we hope will become a new literature on open price contracts.====The remainder of this study is organized as follows. Section 2 begins with the institutional background necessary to frame the identification strategy, and Section 3 describes the data. Section 4 outlines our methodology, and Sections 5 through 8 contain our main empirical results. Section 5 presents the first analysis and tests for price opportunism by comparing sellers’ pricing behavior to different buyer types. Section 6 presents the second analysis and tests for greater price opportunism following shocks corresponding to greater degrees of “surprise” in the state of the world. Section 7 takes an alternative approach and analyzes the potential for opportunistic pricing using dynamic and non-parametric methods. Section 8 is an ancillary analysis motivated by our main findings and investigates how the differential effects we find upstream impact competition downstream. Section 9 concludes.","Open price contracts, locked-in buyers, and opportunism",https://www.sciencedirect.com/science/article/pii/S0167718722000534,13 September 2022,2022,Research Article,46.0
"Clark Robert,Fabiilli Christopher,Lasio Laura","Queen’s University, Dunning Hall, 94 University Avenue, Kingston, Ontario K7L 3N6, Canada,Competition Bureau Canada, 1990 Brierwood Crt, Sudbury, Ontario, P3E 5S3, Canada,McGill University, CEPR, CIREQ CIRANO, 855 Sherbrooke St W, Montreal, Quebec, H3A 2T7, Canada","Received 15 February 2022, Revised 11 July 2022, Accepted 1 September 2022, Available online 12 September 2022, Version of Record 25 September 2022.",https://doi.org/10.1016/j.ijindorg.2022.102878,Cited by (0),"We study cartels that operated in the US generic drug industry, leveraging quarterly Medicaid data for the period 2011–2018 and a difference-in-differences approach comparing the evolution of prices of allegedly collusive drugs with a group of competitive control drugs. Our analysis highlights (i) the difficulty of establishing a suitable control group when collusion is pervasive, (ii) the importance of accounting for market structure changes when defining the control period, and (iii) the existence of across- and within-drug heterogeneity. We focus on six drug markets that that were part of the expanded initial complaint and where there was no entry in the same class during the collusive period, permitting a clean measure of the causal impact of collusion on prices. Our most conservative estimates suggest that collusion led to price increases of between 0 and 166% for each of the six drugs, and damages of between ==== and ==== million for the Medicaid market.","Over the past decade, the US generic drug industry has been a focus of attention as a result of large price spikes for many of its products. For example, in 2013–2014, albuterol sulphate, used to treat asthma and other breathing conditions, increased in price from $11 to $434 for a bottle of 100 2 mg tablets. Similarly, doxycycline hyclate, an antibiotic used to treat various infections, increased in price from $20 to $1849 for a bottle of 50 100 mg tablets, and glycopyrrolate, used to prevent irregular heartbeats during surgery, increased in price from $65 to $1277 for a box of 10 0.2 mg/L, 20 mL vials.==== The price spikes experienced in the market were in fact even more widespread than the anecdotal examples listed above and estimated to be on average 38% (Conti et al., 2018).====These price hikes ultimately led to increased antitrust oversight and in July 2014 Connecticut launched an investigation into generic drug pricing. The offices of some manufacturers were raided in September 2016 and the first charges were announced December 14th 2016 against the former CEO and former president of Heritage Pharmaceuticals. Both were said to have conspired to fix prices, rig bids and allocate customers for the antibiotic doxycycline hyclate and for the anti-diabetic medication glyburide. The next day, state attorneys general from 20 states filed a civil lawsuit against six pharmaceutical companies alleging they colluded to increase prices for the two afore-mentioned drugs. The former CEO and former president of Heritage pled guilty and agreed to cooperate in the antitrust probe. The complaint expanded in October 2017, increasing the number of manufacturers under investigation from 6 to 18 and the number of drugs from 2 to 15. On May 12th, 2019, a second complaint led by Attorney General William Tong increased the number of generic manufacturers to 20 and the number of drugs to over 100.====The objective of this paper is to quantify the causal impact of collusive behavior in the generic pharmaceutical industry.==== To do so we take advantage of quarterly prescription and reimbursement Medicaid state drug utilization data for the period 2011–2018 and information from the court documents. The latter are employed in particular to define the start and end dates of collusive activity for each drug. Then, using a difference-in-differences approach in which we compare the evolution of prices of allegedly collusive drugs to the evolution of prices for a group of competitive control drugs, we derive the ==== generated by the alleged collusive behavior. To quantify damages, we multiply the volumes sold during the collusive period by the overcharge.====Currently there are well over 100 generic products being investigated, but we concentrate our attention on six of these: (i) doxycycline monohydrate (henceforth doxy mono), (ii) meprobamate, (iii) nystatin, (iv) paromomycin, (v) theophylline, and (vi) verapamil. We focus on these six drugs for three reasons. First, they were part of the expanded first complaint. Second, in each case Heritage, whose CEO and president pled guilty, manufactured a product in the class. Lastly, there was no entry during the collusive period, allowing us to get a clean measure of the causal impact of collusion on prices.====Our analysis highlights three critical elements to consider for practitioners evaluating the impact of cartels using a reduced-form approach. The first is the difficulty of establishing a suitable control group when collusion is pervasive. Second is the importance of accounting for market structure changes when defining the control period. Finally, it is essential to consider the existence of across- and within-drug heterogeneity. Let us start with the first of these. An important and challenging part of our analysis, and of any attempt to evaluate the impact of a cartel on prices, is establishing an appropriate competitive control group against which to compare the evolution of prices for the products under suspicion. To do so, we combine the hierarchical structure of the World Health Organization’s Anatomical Therapeutic Chemical (ATC) classification with information from the complaints and identify drugs that are comparable to those under analysis, but that are not affected by the alleged cartel either via participation in it or by competitive effects. We select drugs that meet three criteria. First, they are off-patent oral drugs available throughout the period to Medicaid enrolees from similar therapeutic classes but are ==== direct competitors of the product under suspicion (i.e., they belong to a different market, defined at the level 4 ATC class). Second, they are not listed in the complaints as being a target of investigation. Third, they do not have any direct competitor involved in a cartel. This process results in between one and six control drugs for each of our treatment drugs.====Having established the control group for each drug, we must next determine the appropriate competitive control period in order to implement our difference-in-differences analysis. For the competitive control period we have three choices: (i) the period prior to the start of the cartel, (ii) the period after its collapse, or (iii) both. We elect to focus our attention on the period before the cartel gets off the ground for two reasons. As pointed out by Harrington (2004), if firms understand that antitrust authorities may use post-cartel prices in their determination of the overcharge, they may attempt to strategically maintain prices above competitive levels after the collapse in order to reduce the overcharge and the resulting damages. Furthermore, there are important market structure changes in the post-cartel period that would seriously affect the overcharge calculation. In particular, for three of the six drugs that we consider, the market structure changes are such that one firm is left in what amounts to a monopoly position following the cartel’s collapse. For meprobamate, one of the two manufacturers (Heritage) exited the market at the end of the cartel period, leaving the remaining firm (Dr. Reddy’s) to act as a de facto monopolist. Not surprisingly, prices did not collapse following the start of the investigation. The same is true of paromomycin, where Heritage became the only supplier after the exit of Sun. For theophylline, the two alleged colluders exited roughly a year after the collapse of the cartel, leaving recent entrant Alembic serve the entire market. Overall, we conclude that using the post-cartel period as part of the competitive control period would bias downward any damage estimate for these three drugs.====Taking these challenges into consideration, we estimate the overcharge for each of the six drugs. Our findings suggest that collusion had heterogeneous effects on prices both ==== and ==== the drugs we analyze, with some drugs and/or manufacturers experiencing significant increases in price and others no change at all, despite claims in the court documents of important effects. After controlling for time and product fixed effects, we estimate that the cartel caused prices for meprobamate to increase by ==== per defined daily doses. Meprobamate’s average price in the pre-cartel period is ==== and so this represents a 166% increase in price. Similarly, for nystatin we estimate a price increase of ====, which represents a 13% price increase. Finally, for theophylline we estimate a price increase of ====, which represents a 46% increase in price. We find no significant effect for doxy mono, paromomycin, and verapamil.====The finding of no significant overcharge in the case of doxy mono is unexpected, because, as we will see below, graphically it appears that there is a sharp increase in price during the period of alleged collusion. We investigate further what is going on by looking ==== drugs at the evolution of prices for different manufacturers. Our findings suggest that, in the case of doxy mono, two of the four manufacturers allegedly involved in the doxy mono cartel did not experience any price increase at all during the cartel period. Surprisingly, one of these was Heritage, whose CEO and president pled guilty. The other was Mylan, whose offices were raided, precipitating the investigation. The price increase was entirely driven by the two other manufacturers, Par and Lannett. Restricting attention just to these two manufacturers and looking at them separately reveals that the overcharge was ==== for Lannett (equivalent to an increase by 85% over the pre-cartel price) and ==== for Par, although for the latter the estimate is not significant.====We estimate damages of roughly $5 million overall, representing 43% of the $11.5 million in revenues earned during the collusive period for the four drugs for which we identify a significant overcharge. Although they do not appear to be particularly large, it is worth pointing out that they are estimated on the Medicaid population only and hence capture just a fraction of the increase in the costs borne by US patients: in the period under study, Medicaid represented roughly 10% of all prescription drug expenditures, with Medicare covering over a fourth of them and private insurance making up the rest. Also, Medicaid beneficiaries tend to use prescription drugs to a lesser extent than do Medicare Part D enrolees (Garthwaite et al., 2021). If we assume that, across insurance types, there were similar (i) overcharges, (ii) consumption patterns, and (iii) elasticities, then damage estimates for these six drugs for the whole US population would total $55 million for the collusive period. This amount is likely underestimated, as it ignores any potential spillover to other markets due to the strategic reaction of manufacturers of substitute products.====Interestingly, the alleged cartel is likely to have contributed to a large increase in ==== expenses on pharmaceuticals. Spending on outpatient drugs comprises 5% of total Medicaid benefits expenditure, and it has increased substantially over time, totalling around $30 billion in 2017. Generic drugs accounted for the vast majority of Medicaid prescriptions in 2014–2017. The fixed dollar copayment paid by Medicaid beneficiaries may have shielded them from bearing the price increases directly, which might partially explain why demand did not drop significantly as a result of the price hikes. As a result, most of the increased expenses were likely covered by Medicaid and only partially compensated by manufacturer rebates. Previous work has shown how Medicaid rules, including its rebates program, have effects on price levels in private insurance markets (Duggan, Scott-Morton, 2006, Feng et al., 2021), making our results relevant for understanding the impact of collusion on a larger scale. Patients covered by different insurance programs (Medicare Part D or private) who used these drugs during the cartel likely experienced significantly larger out-of-pocket costs.",Collusion in the US generic drug industry,https://www.sciencedirect.com/science/article/pii/S0167718722000546,12 September 2022,2022,Research Article,47.0
"Karakoç Gülen,Pagnozzi Marco,Piccolo Salvatore","University of Pavia and CefES, Italy,University of Naples Federico II and CSEF, Italy,University of Bergamo and CSEF, Italy and Compass Lexecon, USA","Received 12 February 2020, Revised 24 August 2022, Accepted 27 August 2022, Available online 31 August 2022, Version of Record 12 September 2022.",https://doi.org/10.1016/j.ijindorg.2022.102876,Cited by (1),"A manufacturer designs a long-term contract with a retailer who is privately informed about demand, and they face future competition by an entrant. When demand is correlated across periods, information about past sales affects firms’ behavior after entry. We analyze the incentives of the incumbent players to share this information with the entrant and show that the manufacturer and the retailer have contrasting preferences: when the retailer wants to disclose information, the manufacturer does not, and vice versa. Although transparency harms consumers and reduces total welfare, incumbent players jointly benefit from selling information to the entrant.","In vertically related markets, incumbents facing the threat of future entry may engage in anticompetitive practices to protect their market power — e.g., exclusive dealings, limit pricing, most-favored-nation clauses, and other forms of vertical restraints. Incumbents may also use information disclosure as a strategic tactic to discipline future competitors. However, although information sharing among rivals has been extensively studied in static oligopoly models (see, e.g., Vives, 2006, for a survey), little is known on firms’ incentives to share information in dynamic environments, where incumbents may strategically disclose or hide information to potential entrants. Even less is known about the interplay between these incentives and vertical contracting.====We analyze a dynamic vertical contracting environment in which a manufacturer interacts with an exclusive retailer for two periods. In the first period, the incumbent firms are monopolists, while in the second period they face competition by an integrated entrant. The incumbent firms may share information about their past sales with the entrant. In the baseline model we assume that firms compete ==== Cournot by selling a homogeneous product whose demand is uncertain and, in every period, is privately observed by the downstream players — i.e., by the retailer in both periods and by the entrant in the second period. The manufacturer designs a long-term contract to elicit the retailer’s private information, specifying the quantity that the retailer distributes and the transfer that it pays to the manufacturer in every period. Since demand is correlated over time, the retailer’s second-period production depends on its report about demand and on its production in the first period (that the manufacturer uses to update its beliefs about demand) — i.e., the optimal dynamic contract features memory (e.g., Baron, Besanko, 1984, Laffont, Tirole, 1996, Battaglini, 2005 among others).====Within this setting we examine the following questions. Do incumbents want to share their private information with the entrant? How is information sharing affected by vertical contracting? What is the price of information? What are the effects on final consumers?====We show that firms’ profits depend on the entrant’s information about first-period production. One may wonder why the entrant should care about this information, since it observes demand in the second period, when it chooses production. The reason is that, because the entrant does not observe demand in the first period, past production conveys information on the agency (organizational) costs of the incumbents and hence on the retailer’s production in the second period (i.e., its future competitive conduct), thus affecting firms’ choices and market competition post entry.==== In other words, the long-term interaction between the incumbent players creates a contractual link between periods, and the role of information sharing hinges on this link.====Our main result is that the manufacturer and the retailer have diverging incentives to share information about past production (or sales). When the downstream firm is willing to disclose this information to the entrant, the upstream firm does not want to do so, and====. The reason is that the manufacturer’s profit and the retailer’s information rent are affected by the entrant’s production in opposite ways. Moreover, the incentive to share information depends on the degree of demand uncertainty, which reflects the relevance of information asymmetry. Specifically, we find that when uncertainty about demand is small, the retailer wants to inform the entrant about its first-period production, while the manufacturer has no incentive to do so. The opposite incentives arise when uncertainty about demand is large: the manufacturer wants to disclose first-period production, while the retailer has no incentive to do so.====Information sharing has two effects on the incumbent firms’ payoffs. First, disclosing information decreases their joint profit. The reason is that, when the entrant knows that demand was low in the first period, it knows with certainty that its downstream rival will sell a low quantity in the second period (due to the distortion needed to minimize its intertemporal rent). Therefore the entrant becomes aggressive, thus reducing the equilibrium price and, in turn, the total profit that incumbent firms can share. The second effect of disclosing information has to do with the retailer’s information rent and can be decomposed as follows. On the one hand, facing an aggressive (informed) entrant in the low-demand state makes it less attractive for the retailer to misrepresent a high state as a low one — i.e., the manufacturer can use the entrant to ‘punish’ its retailer for misrepresenting demand. Hence, other things being equal, the manufacturer would like to face an informed entrant. On the other hand, however, when the entrant is very aggressive (i.e., because it is informed), it is more difficult for the manufacturer to control its retailer’s incentive to misreport demand through the distortion of output: a high distortion triggers a strong reaction by an informed entrant, which reduces the equilibrium price and, in turn, the upstream incumbent’s profit.====The degree of demand uncertainty affects these effects as follows. When uncertainty about demand is high, the retailer’s information rent is high, hence the second effect makes it worthwhile for the manufacturer to share information in order to decrease the retailer’s information rents, even though doing so reduces revenues. For the same reason, the retailer does not want to face an informed entrant. By contrast, when uncertainty about demand is low, facing an informed entrant is costly for the manufacturer because it cannot distort the quantity in the low-demand state too much since the informed entrant would respond aggressively. Of course, this effect benefits the retailer, who therefore prefers to share information and face lower distortions. Clearly, for intermediate demand uncertainty, the effect on revenues dominates the effects on rents for the manufacturer, so that both incumbent firms do not want to share information.====Interestingly, although the two incumbent firms never agree on whether to disclose information to the entrant at no cost, the entrant is always willing to pay a sufficiently high price to induce incumbents to jointly sell such information. The reason is that information sharing maximizes total firms’ profits in the market. Therefore, a market for information may arise in our environment.====The effects of information sharing on consumer surplus and total welfare depend on its impact on the overall efficiency of the industry. Sharing information reduces the incumbent’s production because it induces the entrant to increase production when the incumbent distorts it for rent extraction reasons. On balance, however, aggregate production is lower with information sharing because, holding constant the incumbent’s production, the entrant’s production decision is always efficient regardless of its information (since the entrant equalizes marginal revenue to marginal cost). In other words, although information sharing rebalances production between the incumbent and the entrant, it reduces market efficiency because it increases the retailer’s rent. As a result, consumer surplus and welfare are always lower with information sharing in our model and, contrary to what is commonly believed, a welfare maximizing policy should reduce transparency and forbid incumbents to disclose information to entrants.====Hence, in our environment, the emergence of a market for information always harms consumers, because it always induces incumbents to share information with entrants. Prohibiting a market for information, however, is not sufficient to protect consumers. In fact, our analysis suggests that welfare depends on the proper assignment among incumbents of property rights on information about past contractual arrangements and sales data, since this determines whether information is shared or not (in the absence of a market for information).====Our results are of interest for competition policy because they challenge the traditional conjecture that sharing ‘historical’ information is consumer-welfare neutral (as opposed to exchanges of current information) because it does not reveal future competitor strategies. The new channel analyzed in our paper suggests that, since long-term contracts feature memory, sharing historical sales data may indeed harm consumers, and more so when demand uncertainty is persistent. Interestingly, this contrasts with some regulatory trends intended to promote transparency and level paying field competition. Several national and international regulatory agencies are considering implementing new codes of conduct based on transparency.====Finally, in the online Appendix, we consider price competition with differentiated products and show that, in this case, transparency may actually increase consumer surplus and welfare. The reason is that prices are strategic complements and information sharing allows the entrant to react more accurately, thereby enabling the entrant to charge, on average, a lower price in equilibrium. Interestingly, in contrast to what happens under quantity competition, sharing information benefits consumers but does not always benefit the entrant because it reduces expected prices. On the whole, information sharing under price competition makes it more costly for the manufacturer to elicit information from its retailer, which lowers the entrant’s retail prices and tends to increase market efficiency.====The rest of the paper is organized as follows. We first motivate the empirical relevance of our analysis (Section 1.1) and discuss the existing literature (Section 1.2). Section 2 describes the baseline model and Section 3 discusses benchmarks without asymmetric information and without entry in the second period. Section 4 provides the equilibrium analysis with and without information sharing. In Section 5, we describe the incumbents’ incentives to share information and analyze a market for information. Welfare is discussed in Section 6. Finally, Section 7 concludes. All proofs are in the Appendix.",The value of transparency in dynamic contracting with entry,https://www.sciencedirect.com/science/article/pii/S0167718722000522,31 August 2022,2022,Research Article,48.0
Kong Yunmi,"Department of Economics, Rice University, 6100 Main St, Houston, Texas 77005, USA","Received 25 February 2021, Revised 9 August 2022, Accepted 25 August 2022, Available online 28 August 2022, Version of Record 14 September 2022.",https://doi.org/10.1016/j.ijindorg.2022.102875,Cited by (0),"A typical feature of English auctions and negotiated-price markets modeled as English auctions is that only the transaction price and identity of the winner are observed; auction entrants other than the winner are commonly not recorded in the data. Meanwhile, existing identification results for independent private values, including Athey and Haile (2002), require that the set of entrants be observed. This paper fills the gap by establishing nonparametric identification of asymmetric bidders’ value distributions when losing entrants are not observed, under a general class of entry models in which each potential entrant enters the auction with some bidder-specific probability.","English auctions are one of the most commonly observed auction formats, used to sell objects ranging from fish, timber, and tobacco to art, cars, and wine. In addition, markets with negotiated prices often resemble English auctions and are modeled as such in the economics literature;==== examples include Woodward and Hall (2012), Allen, Clark, Houde, 2014, Allen, Clark, Houde, 2019, and Cuesta and Sepúlveda (2019) on mortgage and consumer loan markets.====A typical feature of English auction and negotiated-price market data is that only the transaction price and identity of the winning bidder are observed. To establish identification from these observables, most papers reference Athey and Haile (2002). In their Theorem 2, Athey and Haile (2002) establish that value distributions of asymmetric bidders under the independent private values paradigm are nonparametrically identified from auction prices and winner identities alone. An assumption underlying this result, as stated in the paper, is that the “econometrician always observes the number of bidders” ====, so that the auction price can be modeled accurately as an ====:==== order statistic, i.e. the second best among ==== bidders’ valuations.==== In addition, if bidders are asymmetric, one must observe not only the number but “the set of bidders who participate in each auction”.====However, many candidate applications fail to satisfy this condition. In the example of a consumer loan, the econometrician may observe the final interest rate and identity of the lending bank as well as the set of all ==== banks in the consumer’s neighborhood (“potential entrants”) but will typically not observe the ==== banks from which the consumer actually solicited quotes (“entrants”). To assume ====–that consumers always solicit quotes from every bank in the neighborhood–would be a misspecification and bias estimates of banks’ value distributions. Due to their oral nature, even English auctions organized by federal and state governments often lack records of ====. This leaves the econometrician ignorant of the true order statistic that the observed auction price represents. Further complicating matters, the unobserved entrants may be different in every auction instance, may be stochastic, and may be determined by an endogenous entry process. As a result, there is a gap between the observed data and available identification theorems. Intuitively, an unobserved number of entrants poses a challenge to identification because it is difficult to distinguish whether a high auction price is caused by a large number of entrants or high value distributions.====This paper establishes nonparametric identification of asymmetric bidders’ value distributions when the number/set of entrants in an English auction are unobserved. The assumed observables are the auction price, identity of the winning bidder, and set of potential entrants. Requiring knowledge only of ==== entrants grants a significant advantage for empirical work because data on potential entrants are more readily available than data on entrants to each auction, as in the example above and as I elaborate further below. More precisely, I establish identification under a popular class of entry models in which each potential entrant enters the auction with some bidder-specific probability, which is unknown to the econometrician. I show that the entry probabilities are identified separately from the value distributions. The literature shows that knowledge of these entry probabilities often enables subsequent identification of entry model primitives, as I later illustrate. I emphasize, however, that the focus of this paper is on addressing the differential challenge that arises from the nature of English auction data, rather than on studying entry models per se,==== and the identification proof itself is agnostic about the specific entry model generating the entry probabilities. Discussion of applicability to specific entry models, including models of selective entry, follows in Section 3. As for the auction paradigm, this paper focuses on independent private values; for English auctions under the common value paradigm, Athey and Haile (2002) establish a strong negative result in which the model is not identified even when all bids are observed and the set of entrants is known.====Concretely, I consider an English auction with ==== potential entrants, in which each potential entrant ==== independently enters the auction with probability ====, and the resulting ==== entrants each draw a value from the distribution ==== and bid in the auction. I explain that, upon assigning appropriate placeholder outcomes to auctions with one or zero entrants, the distribution of auction prices and winner identities generated by this auction model is equivalent to that generated by a translated model in which all ==== potential entrants ‘enter’ the auction but now draw their value from an adjusted distribution given by ====. The atom of size ==== at the infimum of this distribution represents potential entrant ====’s probability of not entering the auction. The advantage of the translated model is that there are now ==== value draws from the adjusted distributions, where ==== is known, rather than ==== value draws from ====, where ==== is unknown. After reframing the English auction model in this manner, I apply a result from Nowik (1990), from the literature on competing risks, which shows identification of just such atomic distributions given observables that are equivalent to those available in English auctions. To clarify the distinction from the proof in Athey and Haile (2002), their Theorem 2 applies a result from Meilijson (1981), which is from the same literature as Nowik (1990) but assumes distributions are non-atomic and thus cannot be directly applied to the atomic distributions that arise as above upon translating auctions with probabilistic entry.====In addition to Athey and Haile (2002), closely related work includes Adams (2007) and Komarova (2013). Komarova (2013) provides an alternative to Meilijson (1981)’s proof and a more extensive discussion when it comes to identifying bidders’ value distributions in English auctions. A subsection of that paper considers the case of a stochastic number of entrants where the set of entrants is unobserved but the probability distribution of the set of entrants is known by the econometrician. The subsection is intended as an illustration and does not provide a general proof of identification. In the case of symmetric bidders, Adams (2007) proves identification when the observed auction price equals the ====:==== order statistic and the probability distribution of ==== is known by the econometrician or there is an instrument that shifts that distribution of ==== in an infinitely continuous manner.====When two or more ====:==== order statistics are observable per auction, such as on eBay, Song (2004) and Freyberger and Larsen (2022) identify the value distribution when neither ==== nor its distribution are known. There is also a literature studying identification in first-price auctions with missing information on bidders. Guerre and Luo (2022) prove identification from winning bids with unknown numbers of bidders by using the monotonic increase of the upper bound of first-price auction bids in the number of bidders. Meanwhile, An et al. (2010) and Shneyerov and Wong (2011) study first-price auctions where the number of observed entrants differs from the competition that bidders perceive and base their bidding strategies on.====My paper fills the need for an identification result in English auctions when the econometrician does not know the set of entrants nor the probability distribution thereof. This identification result does not require symmetry of the bidders, instruments, or observability of multiple bids per auction.",Identification of English auctions when losing entrants are not observed,https://www.sciencedirect.com/science/article/pii/S0167718722000510,28 August 2022,2022,Research Article,49.0
Christensen Finn,"Department of Economics, Towson University, 8000 York Rd., Towson, MD 21252 United States","Received 20 July 2020, Revised 28 July 2022, Accepted 2 August 2022, Available online 25 August 2022, Version of Record 11 September 2022.",https://doi.org/10.1016/j.ijindorg.2022.102873,Cited by (1),"The removal of Warner Music content from YouTube in the first three quarters of 2009 constitutes a plausible natural experiment to investigate the impact of streaming on live concert sales. This Warner-YouTube blackout had statistically and economically negative effects on Warner artists relative to non-Warner artists. Specifically, relative revenues and prices were lower and relative attendance was not higher. These effects were stronger among artists who recently had a song in the ==== Hot 100 and among those who were more frequently searched on YouTube just prior to the blackout. These findings suggest that the diffusion of streaming has stimulated the demand for live concerts. The evidence is also consistent with a differentiated Bertrand model of ticket pricing in which prices are strategic complements and prices and streaming penetration gives rise to increasing differences in the artist profit function. This suggests that concerts and streaming are complements in demand for a given artist, and that concerts by different artists are substitutes. More broadly, the paper is an example of how the results from the monotone comparative statics literature can be adapted for use with difference-in-differences estimation.","In terms of both relative and absolute revenue, live performance has become increasingly important in the music industry. As illustrated in Figure 1.1(a), between 1998 and 2018 real US recorded music revenues declined while real revenues from live performance in North America (NA) rose.==== And as illustrated in Figure 1.1(b), which is based on my calculations from Pollstar’s Top 200 NA Tours charts, the increase in live performance revenue was driven by both real prices (right axis) and ticket sales (left axis).====While live performance is important to the music industry as a whole, it is absolutely vital for artists as it generates the majority of their revenue (Krueger, 2019). Among the 2018 Billboard Top 50 Money Makers, an average of 80 percent of revenue was generated from touring, 8 percent from streaming, and the rest from sales and publishing. Only Drake and Taylor Swift made the list without touring.====The rise in live performance coincides with the digitization of music and the market penetration rate of streaming. From the launch of Napster in 1999, iTunes in 2003, Pandora in 2005, YouTube in 2006, Spotify in 2008 and its subsequent competitors, digital music and the streaming industry in particular have been growing rapidly. In broad terms, this paper attempts to understand whether and how the increased market penetration rate of streaming can explain the rise of the live music industry.====In particular, I exploit a licensing dispute between Warner Music and YouTube in 2009 as a natural quasi-experiment to identify the causal effect of recorded video streaming on live music revenue. As a result of this dispute, all Warner and subsidiary label content was removed from YouTube from January 2009 to October 2009. YouTube had resolved similar disputes with other labels, so this outcome was unexpected. At the time, record labels and YouTube had little stake in the live concert industry, so the dispute and resulting blackout were unrelated to events in the live music business. Thus, the blackout was a plausibly unanticipated exogenous shock to the live music industry.====Hiller (2016) uses the Warner-YouTube blackout to identify the causal effect of legal streaming on recorded music sales. He finds that the blackout increased the difference in album sales between Warner artists and non-Warner artists. In contrast, I find that the blackout decreased the difference in live concert revenue and prices, and weakly decreased the difference in quantity, especially among “hot” artists. Note that since Hiller (2016) finds that the blackout increased album sales, it is unlikely that the results in this paper could be interpreted as listeners “punishing” Warner artists.====The empirical analysis is based on a repeated cross-sectional data set compiled from Pollstar’s 2006-2012 Top 200 NA Tours charts, together with discography data from allmusic.com, YouTube search volume data from Google Trends, and historical weekly ==== Hot 100 charts.==== Compared to non-Warner artists, the removal of Warner content from YouTube in 2009 caused Warner artists to earn on average 18 percentage points less annual concert revenue. The difference in ticket prices decreased 11.8 percentage points. I do not pick up a statistically significant effect on annual total ticket sales, tickets per performance, or the number of performances.====These effects are amplified among “hot” artists, defined as those who had a ==== Hot 100 single in the year of or year before making the Top 200 NA Tours. For this group, revenue, price, and measures of quantity were negative and statistically significant. Moreover, artists who were more frequently searched for on YouTube just prior to the blackout had worse outcomes during the blackout.====I develop a theoretical model to help interpret these results. The model builds off of Krueger (2005) which argues that concerts were traditionally used to generate album sales, so tickets were priced below what would maximize concert profits alone. Digitization weakened the complementarity between concerts and album sales by allowing consumers to obtain recorded music without purchasing an album. Thus, ticket prices and revenue increased.====The monopoly model in this paper enriches the analysis by accounting for complementarity in both directions. Digitization, and streaming in particular, increases access to recorded music. This has two effects on ticket prices. First, it increases demand for concerts which likely puts upward pressure on price. But it also increases the return to lowering ticket prices because larger crowds return more streams. Ticket prices increase if the upward pressure from converting streamers into concert-goers is stronger than the downward pressure from converting concert-goers into more streams.==== Evidence from Papies and van Heerde (2017) is suggestive that the upward pressure dominates.====In a differentiated Bertrand duopoly setting in which concerts are substitutes, strategic effects may overturn the monopoly result. However, I show that ==== in prices increases unambiguously. In the present context, streaming rates decrease for Warner artists during the blackout, so the key prediction I test using a difference-in-differences (DD) strategy is that Warner artists’ price does not increase relative to non-Warner artists’ prices.====This approach is novel for two reasons. First, it is an example of how the insights from the monotone comparative statics literature can be adapted for reduced form empirical analysis (e.g. Topkis, 2011, Milgrom, Roberts, 1990, Vives, 1990, Vives, 1999). The predictions in this literature are not generally DD predictions and are thus not directly testable with the widely-used DD estimation strategy. But using concepts such as strategic complements, increasing differences, and decreasing differences I am able to make a DD prediction.====Second, the Bertrand model allows one to infer more from the data by employing contrapositive logic, specifically the identity “if ==== then ====” In this context, ==== is the set of assumptions of the model, and ==== is the unambiguous prediction that Warner’s relative price does not decrease as a result of the blackout. Since the data do not reject ====, the assumption set ==== is not refuted. The economic assumptions in ==== are that concert ticket prices are strategic complements between artists and that the artist profit function has increasing (nonincreasing) differences in own ticket prices and the own (rival’s) penetration rate of streaming. The former assumption is natural if concerts are substitutes between artists (Vives, 1999). The latter is natural if live and recorded music are complements in demand within artists and the streams-to-concerts path is stronger than the concerts-to-streams path. I emphasize that the empirical results do not prove that ==== is valid, but rather that ====={strategic complements ==== increasing/nonincreasing differences} as a model of concert pricing is not rejected by the data.====In the next section I provide a more detailed discussion of the mechanisms through which digitization could affect live music, as well as the contributions of this paper to that literature. The theoretical model is developed in Section 3. In Section 4 I discuss the data and provide summary and descriptive statistics. The estimation strategy is described in 5 Estimation Strategy, 6 Empirical Results reports the empirical results and the final section concludes. The appendix provides contextual background, a proof, and additional robustness analysis.",Streaming Stimulates the Live Concert Industry: Evidence from YouTube,https://www.sciencedirect.com/science/article/pii/S0167718722000492,25 August 2022,2022,Research Article,50.0
Esteves Rosa-Branca,"Department of Economics and NIPE, University of Minho, Portugal","Received 11 October 2021, Revised 12 August 2022, Accepted 15 August 2022, Available online 24 August 2022, Version of Record 17 September 2022.",https://doi.org/10.1016/j.ijindorg.2022.102874,Cited by (4),"This paper assesses the profit effects of Personalized Pricing (PP) in markets where businesses face consumers, who are heterogeneous in terms of preferences for firms/stores (horizontal dimension) and purchase quantity (vertical dimension). If firms’ data discloses only vertical information, firms can only employ group pricing. This is always a winning strategy. When data discloses horizontal and vertical information, perfect personalized pricing (PPP) becomes feasible. If data only discloses horizontal information, firms can only employ imperfect personalized pricing (IPP). By comparing uniform pricing (UP) with personalized pricing, we show that if the share of high demand customers in the market is greater than the share of low demand consumers, firms are always better off with no discrimination. More importantly, we show that if heterogeneity in purchase quantity is sufficiently high, then PP can indeed be a winning strategy for all symmetric discriminating firms. If heterogeneity in consumer value is high and the share of high demand consumers is sufficiently low, in comparison to UP, both firms are better off under IPP. For an intermediate share of high demand consumers, firms can get higher profits under PPP than under UP and IPP.","With the growth of Big Data, Big Analytics and self-learning price algorithms, online and brick-and-mortar sellers are experimenting with better ways to price discriminate. Some retailers are tracking consumers’ characteristics and behaviour (e.g. volume purchasing habits, consumers’ (real time) location, frequency of visits to an (offline/online) store and time spent on each visit, products viewed, consumer devices used, emotions, etc.). The aim is to estimate consumers’ willingness to pay for certain products or services and subsequently engage in personalized pricing, a form of price discrimination that involves charging different prices to consumers with different valuations.====In the era of Internet of Things, data collection for personalized offers is virtually infinite. This means that detecting personalized pricing can be a complex task, as the “technical possibilities for online personalization have become much more advanced and hard to capture/measure” EC (2018). In line with this, the reports on Personalized Pricing by the EC (2018) and OECD (2018) argue that although existing evidence is still limited, there is some data showing that personalized pricing is already occurring, at least to some extent.====A popular story published in the New York Times under the headline “How Companies Learn Your Secrets” discusses how and why Target tried to build a model to predict which shoppers were pregnant.==== The reason behind this strategy is that pregnant women are more likely to buy, for example, larger amounts of unscented lotion around the beginning of their second trimester, and later larger amounts of baby-related products (e.g. diapers, baby lotions and food, etc.). Target assigns every shopper a “unique code” that stores everything an individual buys. It also tracks a consumer’s geolocation and in store location, so it knows how far a consumer is from a Target store and from a competitor’ store, or in what aisle is the consumer Ezrachi and Stucke (2016). Based on each consumer’s purchase history, Target, like many other grocery stores, can also recognize single household (====) shoppers and multiple person household (====) shoppers. All this data feeds different personalized campaign promotions.====The Mckinsey (2019) report on “The future of personalization—and how to get ready for it” states that Macy’s, Starbucks, and Sephora are using GPS technology and company apps to trigger relevant in-app price offers when customers near a store.==== It also argues “... offline person-to-person experiences will be the next horizon for personalization. Physical spaces will be reconceived and digitized, and customer journeys will be supported far beyond a brand’s front door”. Moreover, “... the next level of in-store personalization is likely to include providing virtual and augmented reality experiences to all customers to help them experience products and services in different environments, such as trying hiking boots on a virtual mountain.” This kind of virtual experiences - certainly part of firms’ store differentiation strategies in the coming years - will play a key role in shoppers’ preferences for stores attributes, their willingness to pay and therefore in their buying decisions (see also “Personalizing the customer experience: Driving differentiation in retail” McKinsey, 2020).====In short, the formula behind firms’ price personalization campaigns might be explained by many variables such as volume purchasing habits, consumer preferences for store attributes, consumer preferences for product attributes, electronic devices used, consumers’ moods, health conditions, and other variables we cannot yet imagine.====This paper complements the extant literature on personalized pricing by assessing the profit effects of price personalization in homogeneous product markets where consumers are heterogeneous in terms of preferences for firms (stores) and purchase quantity. (Section 6 assesses the profit effects of (perfect) personalized pricing when firms sell differentiated products to heterogeneous demand consumers with different tastes for the firm’s products (full analysis in Esteves, 2022).) As discussed, store and consumers’ physical location is an important determinant of reported personalized pricing practices. Apart from location, offline stores may differ with regard to digitized facilities, augmented reality experiences, personalized recommendations, atmosphere, layout, empathy, etc. Online (and app) stores can differ, for instance, in terms of augmented reality experiences, (personalized) website design, payment methods, delivery options, availability of chat, trust, personalized recommendations (based for instance on interests, mood, voice, health conditions).====A fundamental, but not a new question, which is of interest to practitioners, economics and marketing scholars alike is the following: ====Although in monopolistic markets, we can offer a quick ‘yes’ to this question, the complexity of oligopolistic markets suggests that in this case the answer to the question is not that easy. As stated by Zhang (2009) on a revision on targeted/personalized pricing, the simple answer to a question like this is ‘it depends’. This is, of course, the easy part of the answer. The difficult part is to figure out what it depends on.====This work aligns closely to the theoretical literature on personalized pricing (e.g. Thisse, Vives, 1988, Matsumura, Matsushima, 2015). Many researchers have already looked at the profit effects of PP in markets with ==== consumers where heterogeneity in consumer preferences is due to: (i) tastes for differentiated products (e.g. Shaffer, Zhang, 1995, Matsumura, Matsushima, 2015), (ii) brand loyalty (e.g. Shaffer and Zhang, 2000; Shaffer, Zhang, 2002, Chen, Choe, Matsushima, 2020) and (iii) geographical location (e.g. Thisse, Vives, 1988, Matsumura, Matsushima, 2015). A common finding in unit demand models is that personalized pricing based on data about consumer preferences (horizontal information) makes ==== firms worse off. As pointed out by Corts (1998, p. 321) in markets exhibiting best-response asymmetry, “Competitive price discrimination may intensify competition by giving firms more weapons with which to wage their war.” When symmetric firms all have access to the required data to employ personalized pricing, they can target each other’s customers with great accuracy and efficiency, and consequently, each individual customer is a market to be contested. For that reason, the intensity of price competition due to PP reduces all prices to the detriment of firms’ profits.====Back in 2017, The Economist published a story titled, “The world’s most valuable resource is no longer oil, but data”.==== No one ignores that data is an important input of firms’ targeted pricing and/or advertising strategies. However, why would companies be so eager to collect data for pricing if by doing so they could be a prisoners’ dilemma kind of situation? Of course, because, under some market conditions, data-based pricing is employed by firms as a winning strategy.====In light of this, some studies highlight that the conclusion that PP is bad for profits is not inevitable. The rationale for the positive effect of PP on profits may lie on firms’ asymmetry (e.g. Shaffer, Zhang, 2002, Ghose, Huang, 2009, Matsumura, Matsushima, 2015),==== delivered pricing and price sensitive demand (e.g. Esteves and Shuai, 2022),==== multi-dimensional product differentiation (e.g. Esteves, 2009),==== imperfect targetability (e.g. Chen et al., 2001),==== imperfect informed consumers (e.g. Esteves, Resende, 2016, Esteves, Resende, 2019) and price-product personalization (e.g. Laussel and Resende, 2022).====While the economics and marketing literature have generated important theoretical insights regarding personalized pricing in oligopolistic markets, there still are a number of unanswered questions. To my knowledge, little is known about the profit implications of quoting personalized prices in ==== markets where some customers purchase more units than others. In light of this, the aim of this paper is to investigate under what conditions “yes” can arise as the answer to the previous question in contexts where symmetric firms face a proportion of ====igh and ====ow demand customers with horizontal preferences for them.====In comparison to previous studies, modeling customer heterogeneity in terms of consumer preferences and purchase quantity allows firms to gain a new dimension of information. In addition to data revealing “====” (horizontal information), businesses’ data can also reveal information pertaining to “==== ” (vertical information).====Fudenberg and Villas-Boas (2012) note that more information in symmetric markets where firms price discriminate according to consumer preferences data will lead to more intense competition between firms at the expense of profits. An additional contribution of this paper is to investigate under what conditions are price-discriminating firms better off with less or more information about consumers. In other words, are firms better off with access to data disclosing vertical or horizontal information or both?====With this goal in mind, I build a game theoretical model with two firms, ==== and ==== selling a homogeneous good in store A and B, respectively, to a unit mass of consumers. A’s store is located at point 0 and B’s store is located at 1. Following Shin and Sudhir (2010), I adapt the Hotelling model to introduce customer heterogeneity in demand: a ==== igh)-type consumer purchases ==== units of the good and a ====ow)-type consumer purchases only one unit, ====.==== The proportion of ==== and ==== type consumers in the market is, respectively, ==== and ==== with ==== Both ==== and ==== type consumers’ preferences for stores (denoted ====) are uniformly distributed along the interval ==== A consumer of type ==== is at ‘distance’ ==== from store ==== and ==== from store B. Thus, ==== represents the consumer preference for store ==== over store ====I compare the no information/uniform pricing benchmark, with three different data-driven price discrimination schemes. Most often personalized pricing is associated to first-degree or “perfect” price discrimination. I take this view in this paper too. In the extreme case where firms’ data discloses perfect information about the two dimensions of consumer heterogeneity - demand types and individual preferences - firms are able to employ a Perfect Personalized Pricing (henceforth, PPP) scheme. Obviously, I do not exclude (perhaps) more realistic pricing practices where firms’ data does not reveal full information about the two dimensions. In light of this, I consider the case where firms’ data reveals information about consumer preferences but not about demand heterogeneity. Because each consumer demand type is not revealed, firms can only employ an Imperfect Personalized Pricing (henceforth, IPP) strategy. Likewise, when data available is more limited, it is also possible that data-based pricing discriminates groups instead of individuals, thus resulting in group pricing (henceforth, GP). I also allow for this possibility, when each firm’s user data discloses information about consumer demand heterogeneity but not about individual preferences.====The paper highlights that whether price discrimination based on consumer data is good or bad for profits depends on the type of information disclosed by firms’ data and the market conditions. I show that symmetric firms might become better off with information about consumers. If firms’ data is limited and discloses only vertical information (demand heterogeneity), then, in comparison to no discrimination, group pricing is always a winning strategy. The benefit of this price discrimination scheme is higher in markets characterized by high demand heterogeneity and a low share of H-type consumers. Furthermore, firms are always strictly better off under group pricing than under imperfect or perfect personalized pricing.====The comparison between uniform and personalized pricing (perfect or imperfect) suggests that when heterogeneity in purchase quantity is low and/or the share of H-type consumers is higher than the share of L-type consumers, firms are always worse off with perfect/imperfect personalized pricing than with uniform pricing. This result is consistent with existing theoretical models.====More importantly, I add to the literature a model where under some market conditions personalized pricing might indeed be a winning strategy for symmetric firms. This is specially the case in markets where (i) the heterogeneity in purchase quantity is sufficiently high and (ii) the proportion of H-type consumers is sufficiently low. The intuition behind this result is as follows. Under no discrimination, firms charge the same price to L and H type consumers. Demand is more price elastic when H consumers demand more units. An increase in price reduces more the utility of a H-type consumer who demands more units than the utility of a unit demand consumer. The higher utility reduction implies that High demand consumers are more price elastic than Low demand consumers. For this reason, under UP, an increase in H-type consumers’ demand, intensifies competition and reduces uniform profits. When ==== is not too high, profits fall more the smaller is the proportion of H-type consumers in the market.==== Under these market conditions==== and ==== sufficiently high - firms compete fiercely under uniform pricing. Therefore, in markets with these features, access to consumer data for personalized pricing relaxes price competition to the benefit of profits.====Summing up, in oligopolistic markets characterized by a high enough heterogeneity in consumer value and a higher share of L-type consumers, PPP may be the winning strategy if the share of H-type consumers is intermediate; IPP is the winning strategy when the share of H-type (L-type) consumers is sufficiently small (high). Hence, I highlight that a relatively small share of H type consumers, purchasing a sufficient high number of units, might allow firms to compete with personalized prices in a profitable way. In contrast, in markets in which there are more H-type than L-type consumers, or with low heterogeneity in consumer demand, firms would be better off if data for PP were not available (or if PP were not allowed). The extension of the model to a product differentiation setting (Section 6) highlights that PPP can still be profit enhancing. This is true if the heterogeneity in demand compared to H-type consumers’ disutility cost incurred when consuming ==== units is sufficiently high and the share of unit demand consumers is high enough.====The rest of the paper is organized as follows. Section 2 presents the model. The benchmark case of uniform pricing is discussed in Section 3. Section 4 presents the equilibrium analysis for the three different price discrimination schemes. Section 5 discusses the price and profit effects of different price discrimination strategies. Section 6 assesses the profit effects of PPP under the product differentiation interpretation of the model. Final remarks appear in Section 7. All the proofs are relegated to the Appendix.","Can personalized pricing be a winning strategy in oligopolistic markets with heterogeneous demand customers? Yes, it can",https://www.sciencedirect.com/science/article/pii/S0167718722000509,24 August 2022,2022,Research Article,51.0
"Holler Emanuel,Rickert Dennis","WHU - Otto Beisheim School of Management, Germany,MINES Paris, PSL University, France,Düsseldorf Institute for Competition Economics (DICE), Heinrich Heine University, Germany","Received 17 March 2020, Revised 10 July 2022, Accepted 11 July 2022, Available online 16 July 2022, Version of Record 11 August 2022.",https://doi.org/10.1016/j.ijindorg.2022.102871,Cited by (1),"We study empirically the price effects of upstream cartels that sell through downstream retailers to final consumers. We focus on a German coffee producer cartel that colluded under two different regimes: (i) involving wholesale prices in 2003 and (ii) with additional resale price maintenance from 2005 to 2008. We find that collusive wholesale prices affected coffee consumers, but only in the short run. We quantify an average overcharge of 3% for the collusive regime without resale price maintenance in 2003. The cartel, however, broke down a few months after the announcement. The collusive overcharge increased to 14% after the introduction of resale price maintenance, and the cartel lasted for a few years. Our primary result is, therefore, that resale price maintenance facilitated better organization of the cartel, as evidenced by the higher overcharges and longer cartel duration. Our secondary result is that loss-leading incentives of retailers during high-demand peaks destabilized the upstream coffee cartel.","Upstream collusion on wholesale prices with additional resale price maintenance are a striking feature of many recent producer cartels.==== This pattern was first revealed in the German coffee cartel and then confirmed by investigations of other markets in Germany, such as sweets, pet food, beer, beauty and personal hygiene, baby food, and baby cosmetics.==== Similar vertically-related cartels have been uncovered in the markets of liquid fertilizer, toys, endives (France); alcoholic and non-alcoholic beverages in Austria; pepper bells, onions (Netherlands); watermelons (Hungary); and dairy products (UK).==== In other cases, upstream cartels struggled to maintain collusive agreements absent resale price maintenance clauses, for instance, during the Italian and Spanish pasta cartels (Crede, 2019).==== In our empirical study, we analyze the incentives of (i) upstream cartels to implement resale price maintenance and (ii) retailers to accept such collusive agreements.====Upstream cartels that sell their products through retailers to final consumers face a severe organization problem: How to maximize cartel profits ==== convince retailers to accept collusive contracts? The problem is as follows. Most retailers cannot fully pass-on collusive wholesale prices because they would loose a significant share of demand to competitors. Consequently, retail markups decrease given the higher wholesale prices and incomplete pass-through. The upstream cartel instead benefits by earning collusive markups, while the demand loss is lower than it would be without the incomplete pass-on of retailers: retailers shield consumers from higher price increases by absorbing part of the collusive wholesale prices in their markups. But why do we observe cases in which upstream cartels use resale price maintenance? Resale price maintenance implies higher retail prices and, therefore, lower demand, which leads to lower profits for the upstream cartel compared to a scenario without resale price maintenance. One reason is that retailers may refuse collusive contracts that reduce their markups and prefer to sell other products, such as private labels. Thus, the upstream cartel has to compensate the retailers for forgone profits if they want to collude successfully. Economic theory shows that resale price maintenance can solve this organization problem by maximizing channel profits and paying retailers some share of the additional profits. Whether or not cartels implement and sustain resale price maintenance depends on several factors, such as bargaining power and retail competition, which previous empirical studies have not analyzed in detail.We aim to fill this gap.====We are the first to provide narrative and empirical evidence consistent with the rationale that resale price maintenance allows better organization of upstream cartels by aligning incentives of the channel members. We focus on the German upstream coffee cartel (2003–2008) that coordinated several wholesale price increases with and without resale price maintenance. In March 2003, the upstream coffee cartel agreed on collusive wholesale prices but did not impose resale price maintenance. This cartel broke down five months after its implementation. In December 2004, the upstream cartel convinced all major retailers to accept resale price maintenance clauses, which lasted until 2008, when the Federal Cartel Office uncovered the cartel. Another notable pattern occurred at high-demand peaks during national holidays, when the cartel was subject to frictions. Retailers depend on coffee as their main loss leader: by deviating from the collusive agreement with low coffee prices, they were able attract consumers into their stores and stimulate sales of more profitable goods in the shopping basket.====Our first research question is to test empirically the theoretical predictions of Hunold and Muthers (2021). Their model shows that strong retailers with attractive outside options may refuse collusive linear pricing contracts. Resale price maintenance, in contrast, allows upstream cartels to set monopoly retail prices and adjust wholesale prices such that retailers make sufficient profits when selling the collusive products. We find that the cartel was more effective under the collusive regime of resale price maintenance (14% overcharge and 3-year duration) than under the regime of linear wholesale prices (3% overcharge and 5-month duration). By finding that resale price maintenance increases collusive overcharge and cartel duration, we substantiate the prediction of Hunold and Muthers (2021) that resale price maintenance indeed stabilizes upstream cartels.====Our second research question is to study the cartel frictions that arise when the collusive product is a loss leader in the sense of Chen and Rey (2012). In particular, we analyze if retailers still accept resale price maintenance in the coffee category (which was used as a loss-leading product before the cartel) when retail demand increases during national holidays. Intuitively, higher demand at national holidays has two countervailing effects on coffee cartel prices and cartel stability. On the one hand, collusive profits in the coffee category increase if more coffee units are sold at a higher price, which stabilizes the cartel. On the other hand, a retail price reduction for coffee may attract consumers and increase the profits generated by the whole shopping basket. If the additional profits from other products (e.g., chocolate) outweigh the coffee markup reduction, this provides incentives for retailers to reduce coffee prices, which destabilizes the cartel. Our results show that the coffee cartel could not sustain collusive overcharges at high-demand peaks and that loss-leading motives of retailers caused these temporary cartel breakdowns. Thus, we identify loss-leading motives as a destabilizing factor for upstream cartel stability.====We employ a unique and extensive coffee data set from 2003 through 2009 covering periods before, during and after the collusion. Our data are a combination of two sources. First, we use home-scan data that contains, ====, information on coffee purchases, coffee prices, and overall retail expenditures from a representative household panel. Second, we integrate facts from two court decisions in 2014 and 2018 made by the Regional Court of Higher Appeal (Oberlandesgericht, OLG henceforth). These court documents report detailed stylized facts on the coffee cartel agreements and the internal working of the cartel. Combining the stylized facts from the documents with consumer panel data enables us to characterize the anatomy of upstream cartels in much more detail than previous studies.====We use the standard difference-in-differences methodology to infer cartel overcharges relative to the price developments of a cartel outsider, similar to the literature on merger evaluation (see, e.g., Ashenfelter, Hosken, 2010, Ashenfelter, Hosken, Weinberg, 2013). We posit that the coffee products of a German hard discounter qualify as a valid control group for two reasons. First, we find that the prices of the hard discounter and the cartel members follow parallel trends in the long run. This is mainly due to the fact that all coffee products contain Arabica beans as a common cost ingredient. Second, we provide anecdotal evidence—e.g., statements from business insiders and testimonies in court documents—and formal tests demonstrating that the hard discounter does not react to the coffee cartel during collusive periods. However, we acknowledge a potential drawback of this control group: consumer demand (e.g., during national holiday weeks) may have different short-run effects on the cartel members and the hard discounter. To address this concern, we (i) conduct a before-during-after analysis and (ii) use retail private labels as an alternative control group. Because private labels are sold in the same store as the cartel brands (which is not the case for the hard discounter), they better proxy price trends of the cartel members around holiday demand. This comes at the cost that we more likely expect price responses of private labels to cartel formation, which would lead to an underestimation of long-run cartel effects. Nonetheless, we find that our results are robust with respect to both sensitivity tests.====Our two main results are likely not restrained to the German coffee cartel. Our first result is that resale price maintenance facilitates better organization of upstream cartels, and successful collusion is typically associated with higher consumer prices. A similar mechanism can be expected in other multi-billion end-consumer markets where upstream cartels used resale price maintenance (cf. the aforementioned cases in Europe and in the UK). Price is the main competitive variable in such markets, while services, as in Mathewson and Winter (1984) and Hunold and Muthers (2017), play a minor role. Therefore, we contribute to the intense discussion on how resale price maintenance affects price competition. Our result that resale price maintenance increases cartel overcharges supports the anti-competitive nature of price floors and strict resale price maintenance for markets like ours, where price is the main competitive ingredient.====Our second result is that retailers in the coffee market broke up the producer cartel at the foreseeable demand peaks of national holidays. We posit that similar results can be expected in other grocery markets when the cartelized product is a loss leader. The UK Competition Commission, 2000, Competition Commission, 2008 notes that all main retailers use loss leaders to attract customers at certain times of the year (i.e., holidays and other events, such as the World Cup). Common loss leaders are staple goods that consumers purchase regularly, such as milk and dairy, sugar, alcohol, bread, and bakery products (Chen and Rey, 2012). However, retailers select the loss-leading product according to local preferences for daily goods. German retailers use coffee, washing powder, shampoo, diapers, body and skincare, and oral hygiene products (OLG, 2018, §72). Yogurt seems to be the main loss leader in France (Florez-Acosta and Herrera-Araujo, 2020), while hypermarkets in Chinese Taipei used products, such as, tissue paper and rice.==== Our indicate that cartels tend to be less stable for loss leaders compared to regular products. In such a context, cartel stability depends on (i) the relative importance of the loss leader and the remaining shopping basket and (ii) the persistence and magnitude of cyclical or seasonal demand patterns.==== The cartel literature focuses mostly on horizontal collusion (for overviews, see, e.g., Levenstein, Suslow, 2006, Connor, Bolotova, 2006). Our paper relates to studies analyzing upstream cartels selling through downstream retailers. Verboven and van Dijk (2009) and Basso and Ross (2010), for instance, analyze the incentives of non-colluding retailers to pass on the collusive input prices to final consumers. Another literature strand focuses on “Hub&Spoke” arrangements (Harrington, Clark, Horstmann, Houde).==== In this type of vertical collusion, retailers (“hubs”) facilitate collusion between the upstream cartel members (“spokes”), for instance, by enabling information exchanges. Instead, we study consumer price effects when retailers explicitly join the collusive agreements by accepting resale price maintenance.====Our study is related to the cartel stability of vertical collusion. One literature strand analyzes how side payments, from upstream cartels to retailers, make collusion more sustainable (Schinkel, Tuinstra, Rüggeberg, 2008, Gilo, Yehezkel, 2020, Gu, Yao, Zhou, Bai, 2019). We instead study how the introduction of resale price maintenance by an upstream cartel affects cartel stability and cartel effectiveness. One rare study in this vein is by Jullien and Rey (2007), who extend Green and Porter (1984)’s model of collusion with private demand information to a vertically-related structure. Jullien and Rey (2007) find that uniform retail prices eliminate information asymmetry for the cartel, which makes collusion more sustainable because deviation is easier to detect. However, demand is observable and predictable in mature grocery retailing. Therefore, our study is more related to Hunold and Muthers (2021), who show that resale price maintenance helps upstream cartels overcome an inherent cartel instability by aligning the incentives of upstream cartels and retailers. Our empirical results substantiate their predictions that resale price maintenance helps to (i) mitigate cartel instability and (ii) establish higher overcharges.====Our work also relates to the literature on multi-category pricing incentives for retailers by identifying loss-leading as a destabilizing factor for upstream collusion. Hunold and Muthers (2021) study the stability of upstream collusion in a situation of substitute products and exogenous constant outside options for retailers. Thus, retailers make the same profit in all scenarios because of the binding contract-acceptance constraint in each equilibrium. This is an adequate description of our market during regular sales periods. However, grocery retail demand usually increases substantially around national public holidays (Chevalier et al., 2003), which creates observable, predictable demand cycles. A pattern of observable but uncertain (or unpredictable) high-demand peaks is analyzed in the seminal work of Rotemberg and Saloner (1986). They find that tacit collusion is harder to sustain during demand booms because the incentive to defect becomes greater. Therefore, firms temporarily cheat during booms and move back to collusion. We provide empirical support for a similar mechanism during observable and certain high-demand states: retailers temporarily lower the coffee prices during national holidays and then return to the collusive outcome.====The remaining part of our study is structured as follows. In Section 2, we describe our data, the German coffee market, the cartel members, the collusive timeline and agreements, and the pricing trends. In Section 3, we describe our empirical strategy, the validity of our control group, and the main difference-in-differences estimation. Section 4 describes our main results for collusive overcharges and presents several robustness- and plausibility checks. Section 5 concludes the study.",How resale price maintenance and loss leading affect upstream cartel stability: Anatomy of a coffee cartel,https://www.sciencedirect.com/science/article/pii/S0167718722000479,16 July 2022,2022,Research Article,52.0
"Creane Anthony,Manduchi Agostino","Department of Economics, University of Kentucky, Lexington, KY 4506, USA,Jönköping University Business School, Jönköping, Sweden","Received 10 May 2020, Revised 28 March 2022, Accepted 1 June 2022, Available online 9 June 2022, Version of Record 26 June 2022.",https://doi.org/10.1016/j.ijindorg.2022.102860,Cited by (1)," also shows that a pure-strategy, symmetric equilibrium typically does not exist in the latter model if the number of sellers is large, a fact which accounts for the different conclusions drawn in the two papers.","Advertising is ubiquitous in print, airwaves and digital media with over a half-trillion dollars being spent on advertising each year (====). Given the large expenditures on advertising and its effect on consumption decisions, determining the welfare effects of these expenditures has been a focus of economists, policymakers, and the public. While there are many concerns regarding advertising, Telser (1964), Nelson (1970) and others have argued that its informational role could lead to a net social benefit. Seminal work introduced the modeling to investigate how informative (truthful) advertising benefits society both through ==== - consumers learn of the existence of the market - and through ==== - consumers learn a product’s characteristics relative to rival products (Butters, 1977, Grossman, Shapiro, 1984).====Although society and firms incur the same resource cost from advertising, the benefits accruing to them can differ even with truthful advertising. On the one hand, the market prices and the possibility of spillovers generally do not allow firms to capture the full benefits resulting from their advertising activities, both in terms of demand creation==== and in terms of product-matching. On the other hand, firms do not factor the lost profits that their activity entails for their rivals into their maximization problem. Their advertising decisions thus may also include a ====. The net balance between the positive demand and matching externalities and the business-stealing effect may result either in insufficient or in excessive advertising, depending on the setting considered (Bagwell, 2007).====Grossman and Shapiro (1984) argue that their pure strategy symmetric equilibrium (====) in a version of the Salop (1979) model in which firms must advertise their products features socially excessive advertising for a given number of firms, and thus provides a counterexample to Telser (1964), Nelson (1970) and others. Due to the complexity of the model, the key part of their analysis relies on an approximation which is most accurate in scenarios featuring a large number of active sellers (Grossman and Shapiro, 1984, p. 80, Fn. 10): that virtually all consumers receive at least one advertisement. The demand creation effect thus plays a negligible role and the excessive advertising result hinges on the fact that the business-stealing effect dominates the matching effect. However, Christou and Vettas (2008, p. 109, Fn. 29) present numerical examples showing that the Grossman and Shapiro (1984) excessive advertising result “is not generally correct” in cases with a small number of firms, and note that Tirole (1988) had mooted this possibility and demonstrated it in a Hotelling (1929) model.====A second issue in Grossman and Shapiro (1984) is that the lack of quasi-concavity of the sellers’ profit functions poses a threat to equilibrium existence. Christou and Vettas (2008) show through examples that the pricing game of Grossman and Shapiro (1984) may not have a pure strategy-equilibrium, as each firm may deviate from the candidate equilibrium price to a higher price that is only accepted by the consumers who receive no competing offers.==== A related problem stems from the possibility of deviations to ==== - low prices which would make a seller’s product preferred by all consumers to the products of the seller’s closer competitors offered at the candidate equilibrium price. The latter problem is qualitatively similar to the problem that rules out pure strategy equilibria in the Hotelling (1929) model (d’Aspremont, Gabszewicz, Thisse, 1979, Vickrey, 1964). Creane (2020) shows that if supercompetitive prices are accounted for, a necessary condition for existence is that each firm reaches at least half the population, which implies that most of the examples presented in Grossman and Shapiro (1984) are not equilibria.====In this paper, we return to the question of informative advertising in an oligopoly, with an approach that limits the relevance of discontinuities and yields precise existence conditions. Our approach also allows us to take into account the demand creation effect of advertising and to investigate the welfare properties and the comparative statics of the model without resorting to approximations. Following Grossman and Shapiro (1984), we base our framework on Salop (1979). However, we take the Grossman and Shapiro (1984) implicit assumption of a large number of firms to the limit and consider a continuum of firms, each one of which sells a finite number of units of the respective product. This assumption mirrors the corresponding assumption on consumers can also be found in Butters (1977). ==== the assumption would not appear to change the underlying market mechanisms beyond the characterization of potential competition, and indeed our PSSE price and profits expressions have the same comparative statics as Grossman and Shapiro (1984). However, in our setting, a price drop never induces a discontinuous increase in demand. Moreover, we can focus on the effect entry has on advertising without conflating entry’s effect on better matching (as Stahl (1994, Fn. 1) noted) because, with a continuum, entry solely increases the measure of the set of active firms.====Our main finding is that ==== (Proposition 4). By contrast, Grossman and Shapiro (1984, Section 4) - whose main focus is on scenarios with a large number of sellers, in which essentially all consumers receive at least one advertisement==== - find that advertising levels for each firm are always excessive. The basic reason for the different results is the larger value of the candidate equilibrium price in Grossman and Shapiro (1984) as compared to ours, which translates into larger profit margins and a stronger incentive to advertise. However, such a price may ==== be an actual equilibrium price of the Grossman and Shapiro (1984) model. We show in fact that for ==== total measure of contacts established through the sellers’ advertisements, the choice of the candidate equilibrium price is dominated by the choice of supercompetitive prices if the number of firms is sufficiently large.==== Deviations to supercompetitive prices create substantial analytical complications and are not considered in the analysis of Grossman and Shapiro (1984, Fn. 8).====By contrast, our continuum of sellers, uniformly distributed over a continuum of possible locations, allows us to seamlessly factor supercompetitive prices into the demand faced by each seller. We can thus account for the greater elasticity of demand induced by such prices, which in turn results in an equilibrium price that is ==== than the candidate equilibrium price of Grossman and Shapiro (1984) in comparable situations. The low price translates into a relatively low return to advertising and thus into excessively low advertising levels, from the social point of view.====Our analysis highlights a pattern that emerges in the modeling of Grossman and Shapiro (1984). If each firm reaches the entire market (====), the Grossman and Shapiro (1984) price (====) is equal to the Salop (1979) price, and there is no profitable deviation. However, as the fraction of the market each firm reaches (====) decreases, the price ==== increases as informational frictions mitigate the intensity of competition. The reduction in price necessary to set the supercompetitive price - whose absolute value is constant for a given number of sellers - thereby becomes a smaller ==== of the initial price. Thus, even if the percentage demand increase from the price cut becomes smaller, the profit from a deviation increases. Our analysis, which focuses on changes in the number of sellers delivering an arbitrary total volume of advertisements, shows that this effect ultimately makes the undercutting prices more profitable than the candidate equilibrium price when ==== is sufficiently large.====The Grossman and Shapiro (1984) candidate equilibrium is nevertheless an equilibrium if the number of buyers contacted by each seller is sufficiently large, given the number of active sellers - a scenario that can require a very large total volume of advertising even for small numbers of active sellers, as we show in Section 6. In cases of this type, virtually all buyers receive at least one offer, and many buyers are very likely to receive offers from sellers who are both (i) close to her location and (ii) close between them.====The business-stealing effect thus overcomes the demand-creation effect, and advertising is over-provided, in equilibrium. Excessive advertising in the examples in Grossman and Shapiro (1984) can thus be related to the excessive entry result of Salop (1979), whereby demand creation is ruled out by the assumption that the buyers are fully informed about the sellers’ offers and the entry of new firms only results in match improvement, in expectation.====We also consider the effect of product differentiation on social returns, an effect not examined in Grossman and Shapiro (1984). First, we find that greater product differentiation can either increase or decrease the optimal level of advertising, depending on how strong the demand creation effect is, showing the importance of the inclusion of the demand creation effect. If roughly at least one-sixth of the consumers receive no advertisements, (so on the margin there is demand creation), then an increase in product differentiation decreases the optimal level of advertising. This is because when a consumer receives only one ad, then the greater the transportation cost is, the less their expected net surplus. In contrast, since Grossman and Shapiro (1984) eliminate the demand creation effect, there an increase in transportation cost cannot decrease the optimal level of advertising. Comparing the social and private returns, Christou and Vettas (2008) provide examples with a fixed number of firms in which advertising can be either insufficient or excessive from a social point of view, depending on whether product differentiation is weak or strong. In our model, we show that as product differentiation increases, the private value increases relative to the social value because the PSSE price increases.====We then extend our model and endogenize entry, showing that our result of socially insufficient advertising does have a counterpart in an equilibrium number of active sellers that is socially too small. This result can again be traced back to the low equilibrium price of our model, whose effect on the sellers’ advertising decisions is now partly absorbed by the relatively small number of active sellers. The result is also at odds with the corresponding result of Grossman and Shapiro (1984), who conclude that equilibrium features excessive entry. Again, the Grossman and Shapiro (1984) equilibrium does hold in scenarios with a small number of sellers with a large number of buyers contacted by each seller.",Informative advertising in monopolistically competitive markets,https://www.sciencedirect.com/science/article/pii/S0167718722000364,9 June 2022,2022,Research Article,53.0
"Spiegel Yossi,Toivanen Otto","Coller School of Management, Tel Aviv University, Israel, CEPR, and ZEW,Aalto University School of Business, Helsinki GSE, Finland, KU Leuven, and CEPR","Received 15 January 2022, Revised 4 May 2022, Accepted 22 May 2022, Available online 28 May 2022, Version of Record 29 September 2022.",https://doi.org/10.1016/j.ijindorg.2022.102859,Cited by (0),"Using data from five annual conferences of the European Association for Research in Industrial Economics (EARIE), we shed light on the role of academic conferences in disseminating research results and on research in IO. Among other things, we find that (i) there are disagreements between members of the scientific committee when they evaluate the same paper in almost half of the cases, though large disagreements are present in only ==== of the cases; (ii) between ==== of the submitted papers remain unpublished years after the conference and those that are published, take on average over 3 years to get published; (iii) presentation at the conference is associated with a higher likelihood of publishing in an IO journal, although only ","The dissemination of academic research is crucial for scientific progress. One of the key mechanisms to facilitate this process is academic conferences. Indeed, there are many academic conferences today. For instance, EconBiz, which is a comprehensive academic search portal for journals, working papers, and conferences in business studies and economics, lists 911 academic conference in economics for 2019 (the last year before the COVID-19 crisis).==== Sarabipour et al. (2021) report that the academic conferences industry is worth “tens of billions (US $) worldwide” and the total cost for a participant of attending large national and international scientific meetings is “in the thousands of dollars, equivalent to one or more months of graduate and postdoctoral researcher net salary worldwide.” Apart from money, academic conferences are also time consuming, which adds to the cost of participation. Given their importance for the dissemination of knowledge, and their costs, it is clearly important to better understand how academic conferences function.====In this paper, we shed light on this question using data from the annual conference of European Association for Research in Industrial Economics (EARIE). As far as we know, the conference is representative of field-specific large annual conferences in economics, and is one of the two largest and well-respected annual conferences in Industrial Organization.==== Our data covers all papers submitted to the EARIE conferences in 2010, and 2012-2015 (the data for the 2011 conference was sadly lost). Using this data, we examine how many papers were submitted and what their characteristics are in terms of type (theoretical, empirical, or experimental) and authorship, the factors that affect the likelihood that a submitted paper will be published and how long it takes to publish it, and the determinants of the number of citations it gets conditional on being published. Our analysis informs us not only about the functioning of academic conferences in general, but also provides insights about research in Industrial Organization (IO) more specifically.====Overall we have data on 2,261 submissions. For each submission, we observe the evaluation of the paper by the scientific committee of the conference; whether the paper was accepted or rejected and whether an accepted paper was presented or withdrawn by the authors; whether, where, and when the paper was eventually published; and conditional on publication, how many citations it has received.====Our dataset has several advantages relative to datasets that were used in the literature to study academic conferences and the publication process in economics. First, all papers in our data were submitted to the EARIE conference, so by revealed preference, their authors viewed them as related to IO broadly defined. This facilitates the comparison of papers to one another. By contrast, most existing research on the topic (which we discuss below) is based on data from general-audience conferences and journals and therefore covers much more diverse set of papers. Second, we observe papers at an earlier stage in their life cycle than most other studies, which rely on data on published papers. We can therefore assess the time it takes to publish research in economics. Third, our data also includes papers that were not yet published or may never be published, which allows us to study the determinants of publications.====A special feature of our data is that each paper submitted to the EARIE conference was reviewed by two members of the Scientific Committee (SC), which includes close to 100 senior researchers from all areas of Industrial Organization. Each reviewer gave papers that he/she reviewed one of seven possible grades: A (“Definite accept”), B+ (“Accept”), B (“Maybe accept”), B- (“Borderline”), C (“Probable reject”), D (“Reject”), and F (“Definite reject”).==== The reviewers’ grades give us a measure of paper quality, which we use as a control in order to study how other factors, such as acceptance and presentation at the conference, whether the paper is theoretical, empirical, or experimental, and whether it is single or co-authored, affect the eventual publication of the paper and the number of citations it has received if published. This type of quality measure is rarely available for researchers, which makes it hard to disentangle the effects of various characteristics on publication and citations from the effect of unobserved quality.====We begin the analysis by documenting the submissions and reviewers’ grades. Of the 2,261 submissions in our data, ==== are theoretical, ==== are empirical, ==== combine theory and empirics, ==== involve experiments, and a small number involve either policy, case studies, or surveys. The distribution of presented papers is similar, indicating that the SC was not biased towards one type of work or another.==== In terms of authorship, ==== of the submissions are single authored, although this fraction is higher for theoretical papers than for empirical papers (==== vs. ====), but is much lower for experimental papers (only ==== are single authored).====We then turn to the grades that submissions received from the SC members. Not surprisingly, papers with higher grades were more likely to be accepted. Comparing the grades that two different reviewers gave the same paper reveals that disagreements between reviewers are common: in ==== of the cases, one reviewer was positive (grades A or B+) and the other neutral (grades B or B-) or negative (grades C, D, and F), or one reviewer was neutral and the other was negative. Large disagreements, however, with one positive reviewer and the other negative, occurred in only ==== of the cases. These results are broadly consistent with Welch (2014) and suggest that evaluations of research output are affected in large part by the reviewers’ idiosyncratic tastes, though large disagreements are not common.====Almost half of the submissions in our data were still unpublished by July 2021. Those that were published - 1,171 papers in all - appeared in 306 different outlets, mostly peer-reviewed journals, which vary a great deal in terms of quality and discipline. Although the most popular journals for papers that were accepted for the conference are in essence IO journals (====, and ====), only ==== of all publications appeared in IO journals. The rest were published in general audience or field journals in economics or outside economics,==== and a few were published in books. The wide variety of publication outlets reflects the fact that the conference is quite broad and open to work which is not at the core of IO. Moreover, a little over ==== of the publications are in one of 28 highly-ranked journals, which we define as “high-quality” journals. These journals include high profile journals in economics, including the top-five journals, but also journals in other fields.====When looking at the probability of publishing papers, we find that controlling for quality through the reviewers’ grades, empirical and experimental papers are ==== percentage points (p.p.) more likely to be published than theoretical papers or papers that combine theory and empirics, and co-authored papers are almost 29 p.p. more likely to be published than single-authored papers. Presentation at the conference, however, is associated with a higher probability of publishing in an IO journal but not in “high-quality” journals.====There is concern in the economics profession that it takes too long to publish papers in economics (see, e.g., Yohe (1980); Ellison (2002) and Conley et al. (2013)). Although we do not know when papers were first written (we only observe them when they are submitted to the conference) our data shows that indeed there is a long lag between submission to the conference and eventual publication (we will refer to this gap as “publication lag”). The average publication lag in our data is over 3 years, and ==== of all published papers in our data are published 5 years or more after the conference. Regression analysis reveals that on average, after controlling for paper quality, presented papers take ==== longer and withdrawn papers take ==== longer to publish than rejected papers, and theoretical papers take around ==== longer to publish than empirical papers. Authorship and reviewers’ grades do not have a statistically significant effect on publications lags.====Finally, we examine the citations that published papers receive. First, we find that withdrawn papers receive more citations than rejected ones, but presented papers do not. This result may arise because some withdrawn papers may have also been accepted to other conferences that take place at the same time as the EARIE conference,==== or because authors of higher quality papers may be busier. Second, controlling for acceptance and presentation at the conference, authorship, publication year, and paper quality, we find that relative to empirical papers, theoretical papers receive ==== fewer citations, experimental papers receive ==== fewer citations, and papers that combine theory and empirics receive ==== fewer citations. Third, single-authored papers, which as mentioned above, are less likely to be published and have longer publication lags when published than co-authored papers, also receive around ==== fewer citations than co-authored papers with similar characteristics in terms of acceptance and presentation at the conference, paper type, publication year, and paper quality. Fourth, papers published in journals for which we observe the relevant field, and are published outside economics receive substantially more citations than publications in economics with similar characteristics in terms of paper type, authorship, and paper quality. The difference in citations is ==== for papers in entrepreneurship, ==== for papers in innovation studies, ==== for papers in finance, and ==== for papers in OR and management. Importantly, the cross-field comparison is based on papers as that were submitted to the EARIE conference, so by revealed preference, their authors believe that they are all related to IO broadly defined, and are therefore comparable. Moreover, the differences in citations arise after controlling for papers’ characteristics (presentation at the conference, paper type, authorship, and paper quality), and therefore are likely to reflect different norms of citation across fields rather than intrinsic differences between papers.====Our paper contributes to the literature on the production and dissemination of scientific research, and in particular to the literature that studies the relationship between conference participation and publication outcomes. Gorodnichenko et al. (2021) study data on over 4,000 papers presented at three large conferences in economics (the American Economic Association (AEA) meetings, the European Economic Association (EEA) meetings, and the Royal Economic Society (RES) meetings) over the 2006–2012 period and 60,000 non-presented papers by the authors of presented papers. They find that conference participation is associated with a higher probability of publishing in high-quality journals (but not in other journals),==== more citations, and more abstract views (especially in the month of the conference or the following month). They also find that presented papers take almost 6 months longer to get published. However, they do not observe whether the non-presented papers in their data were submitted to the conferences and were rejected or withdrawn or were never submitted. Moreover, they do not observe reviewers’ grades and cannot control for paper quality when comparing presented and non-presented papers or papers across conferences and fields. By contrast, we compare papers that were submitted to the same conference and were either presented, withdrawn, or rejected, and we examine the determinants of publications and citations after controlling for paper quality.====de Leon and McQuillin (2020) study data on all 29,142 papers included in 2009-2012 on the programs of the two largest conferences in Political Science: the annual meetings of the American Political Science Association (APSA) and the Midwest Political Science Association (MPSA). They find that the cancellation of the 2012 APSA meeting due to Hurricane Isaac led to fewer citations and fewer SSRN downloads of papers that were included on the program of the cancelled meeting.==== Campos et al. (2018) use the same data and setting, and find that the cancellation of the 2012 APSA meeting has also led to a ==== decrease in the likelihood of subsequently co-authoring with another conference participant, especially from a different academic institution. They also find that the conference-initiated collaborations lead to better publication outcomes. Both papers however do not observe all submitted papers and therefore cannot compare accepted and rejected papers from the same conference. Moreover, they do not observe reviewers’ grades and cannot control for paper quality and hence can assess only indirectly whether the adverse effect of the cancellation of the 2012 APSA meeting arises because papers could not be “advertised” or because their authors did not benefit from useful feedback.==== In addition, they do not observe the eventual publication of papers.====There is a large literature on the publication process in economics. Unlike our paper, most of these studies observe papers only after they have been published. For instance, Ellison (2002); Azar (2007); Hamermesh (2013); Card and DellaVigna (2013), and Angrist, Azoulay, Ellison, Hill, Lu, 2017, Angrist, Azoulay, Ellison, Hill, Lu, 2020 document trends in publication in economics journals, but since they only observe published papers, they cannot tell how many and which papers are rejected, how long it takes papers to get published, and what are the determinants of eventual publication. Moreover, most existing studies do not observe the reviewers’ grades and cannot control for paper quality like we can. For instance, papers that study the relationship between authorship on citations (e.g., Card and DellaVigna (2013); Kuld and O’Hagan (2018), and Hsu and Huang (2011)) cannot tell if co-authored paper receive more citations because they benefit from researchers’ collaboration and are therefore better, or because they have more authors who can draw attention to the paper. Our results suggest that it is the latter effect, as we show that co-authored papers get more citations than single-authored papers with similar reviewers’ grades. Likewise, when we compare citations across fields, we show that publications in innovation studies, entrepreneurship, finance, and OR and management receive more citations than publications in economics with similar reviewers’ grades.====Other than Welch (2014), we are aware of only one other paper - Card and DellaVigna (2020) - which has access to referees’ recommendations.==== They study how editors at four leading economics journals decide which papers to publish and show that referees’ recommendations are strong predictors of eventual citations.==== However, they do not examine many of the issues that we examine, such as the determinants of publication lags, the effects of authorship and paper type on publication and citations, and cross-field differences in citations.====The rest of the paper proceeds as follows: In the next section, we describe the EARIE conference in detail. We then discuss our data collection in Section 3. In Section 4, we examine the data on paper submission and on the grading of the submitted papers by the SC members. In Section 5 we examine what happened to the submitted papers and in particular if, where, and when, they were published. Then in Section 6 we examine how many citations published papers have received by the end of November 2021. We conclude in Section 7. In the Appendix we provide some further analysis.",From conference submission to publication and citations: Evidence from the EARIE conference,https://www.sciencedirect.com/science/article/pii/S0167718722000352,28 May 2022,2022,Research Article,54.0
Dewatripont Mathias,"Université libre de Bruxelles (I3h, Solvay Brussels School and ECARES)","Received 24 January 2022, Revised 6 May 2022, Accepted 22 May 2022, Available online 25 May 2022, Version of Record 29 September 2022.",https://doi.org/10.1016/j.ijindorg.2022.102858,Cited by (0),"This paper analyzes the subsequent steps of the covid vaccine experience in the European Union. It stresses the features of the US innovation ecosystem which were responsible for its success. It argues that the European Union did reasonably well in procuring vaccines and logistically organizing their delivery but that vaccine hesitancy proved to be a key constraint. The paper discusses European countries’ vaccination strategies and their plusses and minusses. Finally, it draws some lessons for dealing with the tradeoff between drug and vaccine innovation and ====, an issue whose importance is bound to grow with promising but costly scientific advances.","This paper draws lessons from the covid vaccine experience for the European Union. In order to do so, it reviews the subsequent steps a successful vaccination campaign requires: (i) the development of safe and efficient vaccines; (ii) sufficient high-quality-vaccine production capacity (ideally at ‘socially acceptable’ prices); and (iii) a vaccination strategy that is logistically effective and manages to convince the population to get vaccinated.====The emergence of new efficient vaccines in record time has been a great success of public and private international cooperation. However, credit should go here to the US ‘Operation Warp Speed’, and Europe should draw important lessons from this episode.====As for vaccination, it is fair to say that authorities in most EU countries have been up to the logistical job, but that vaccine hesitancy has been a key challenge. Different countries have performed quite differently, so lessons can be drawn about the various strategies, which should be informed by rigorous empirical studies. A general lesson though is the need for proper communication: in many countries, it was unfortunate that the ‘right not to be vaccinated’ was initially put forward assertively without questioning its potential inconsistency with the need to achieve herd immunity, which was simultaneously asserted.====Finally, the paper draws some lessons from this saga in terms of improving the tradeoff between innovation and affordability, a challenge which is growing with the emergence of new, costly therapies thanks to the progress of science. In particular, it suggests taking advantage of the new role the EU Commission has taken as representative of the 27 member states for price negotiations with pharmaceutical companies.====The paper focuses on Europe and therefore does not discuss the (very relevant) question of insufficient access to vaccines in lower-middle-income and especially low-income countries.==== This is of course a key issue for which adequate levels of technology transfer, global production capacity and funding from rich countries is crucially needed.",Which policies for vaccine innovation and delivery in Europe?,https://www.sciencedirect.com/science/article/pii/S0167718722000340,25 May 2022,2022,Research Article,55.0
Kyle Margaret K.,"MINES Paris and PSL Research University, 60 Boulevard Saint Michel, Paris 75006, France","Received 31 December 2021, Revised 20 April 2022, Accepted 29 April 2022, Available online 14 May 2022, Version of Record 29 September 2022.",https://doi.org/10.1016/j.ijindorg.2022.102850,Cited by (1),"This article provides an overview of the performance of innovation policies in the pharmaceutical sector. It discusses the conditions under which various policies are mostly likely to result in socially valuable R&D. Interactions between different domestic policies, as well as the importance of considering strategic interactions between governments, are also highlighted.","The Covid pandemic has brought increased attention to the pharmaceutical sector, and in particular to the development and distribution of vaccines and treatments. Health care’s increasing share of OECD economies and the growth in health care spending driven by patented pharmaceutical treatments has long been a concern to policymakers.====Much of the increased life expectancy realized during the 20th and 21st centuries can be credited to pharmaceutical interventions. Pharmaceuticals are easily transported and adopted around the world, and usually simple to deliver to patients: that is, the diffusion of innovation is broader and faster than is the case for many other medical breakthroughs, such as surgical treatments. Production costs, while higher for biologics and vaccines than for small molecule drugs, are generally low. The burden of infectious diseases has fallen dramatically. In the last 20 years, for example, a recent study in the United Kingdom found that the introduction of new vaccines for human papillomavirus “has successfully almost eliminated cervical cancer” (Falcaro, 2021); the World Health Assembly has targeted the elimination of viral hepatitis due to the availability of oral direct-acting antiviral treatments (World Health Organization, 2016); and CFTR modulators are expected to have dramatic effects on life expectancy for those with cystic fibrosis (Balfour-Lynn and King, 2020). Pharmaceuticals are used to manage HIV, diabetes, and many chronic cardiovascular conditions. The rapid development of vaccines for Covid is credited with preventing the deaths of almost half a million lives for people over 60 in Europe alone, less than one year after their introduction.====However, in addition to concerns about inequitable access to pharmaceutical innovations and their affordability, policymakers worry that current innovation policies provide insufficient incentives to develop, e.g., novel antibiotics or treatments for diseases that primarily affect less developed countries. Some argue that pharmaceutical firms produce too many “me-too” drugs, offering little clinical benefit over existing treatments. These concerns and others have led various stakeholders to call for changes to innovation policy. One prominent recent example is the proposed patent waiver for Covid vaccines, now being considered by the World Trade Organization. Others have gone further, arguing that research and development should be “delinked” from prices (Love, 2011), or that pharmaceutical production should be nationalized (Mazzucato et al., 2020). Others push for expansion of existing policies, such as more public financial support for medical research. US President Biden’s proposed budget for 2022 includes an increase of 10% and 25% on basic and applied research, respectively, for the Department of Health and Human Services.====. The US Trade Representative has maintained its position on the importance of intellectual property (IP), and the implementation and enforcement of IP laws by trading partners.====In this paper, I provide an overview of the success and failures of innovation policy in pharmaceuticals. I focus on the underlying conditions necessary for different innovation policies to be effective at directing innovative effort towards targets where the social return is high.","Incentives for pharmaceutical innovation: What’s working, what’s lacking",https://www.sciencedirect.com/science/article/pii/S0167718722000261,14 May 2022,2022,Research Article,56.0
"Chen Jiawei,Jiang Lai,Syed Shah Saad Andalib","University of California, Irvine, United States","Received 26 March 2020, Revised 27 April 2022, Accepted 28 April 2022, Available online 11 May 2022, Version of Record 14 June 2022.",https://doi.org/10.1016/j.ijindorg.2022.102848,Cited by (0),"We develop an empirical model of consumer usage and price uncertainty under a three-part tariff plan to study the effects of an enacted “bill shock” agreement in mobile telecommunication markets, which requires mobile network operators to inform consumers when they use up the monthly allowance of their mobile phone plan. Using a rich billing dataset, we estimate an ","A sudden and unexpectedly high bill for a subscription that shocks the consumer upon receiving it is known as a “bill shock.” In mobile telecommunication markets, a bill shock is a sudden and unexpected increase in a mobile phone user’s monthly bill that is not caused by a change in service plans. While the issue of bill shock is most prominent in mobile telecommunication markets, consumers have also faced bill shocks in other scenarios, including credit card bills, rental bills, energy bills, and medical bills, causing regulatory concerns (see, for example, Freking and Alonso-Zaldivar (2019) and Hausman (2019)). In many cases, firms design the contractual terms of their products and services in a way to exploit consumers’ imprecise usage forecasts and extract surplus at consumers’ expense (Grubb, 2015b).====In April 2013, an agreement between the Federal Communications Commission (FCC) and mobile network operators committed operators to alert consumers when they approach and exceed the voice, text, and data allowances included in their mobile phone plans. This agreement was reached as the response to a proposed bill shock regulation, which would require mobile network operators to inform consumers when they use up the monthly allowance of their mobile phone price plan (U.S. mobile network operators charge consumers a three-part tariff: a fixed monthly fee, a monthly allowance of free calling minutes, and an overage price per minute). The objective of the bill shock regulation was to reduce consumers’ uncertainty regarding the marginal price they were paying for the next unit of consumption so that they would not be shocked by the bill they would receive at the end of the billing cycle. Under the three-part tariff pricing structure, the source of consumer marginal price uncertainty comes from consumers’ usage uncertainty: they cannot keep perfect track of their usage, and so they do not know for sure whether their actual usage is below or above the monthly allowance (the marginal price changes drastically at the point of monthly allowance).====To better understand the effects of the bill shock regulation, in this paper we develop an empirical model of consumer usage and price uncertainty under the three-part tariff plan. We use this model to predict how mobile phone companies would adjust their pricing decisions if the bill shock regulation is implemented to eliminate uncertainty in consumer usage and price.====We present an empirical industry model in which consumers have price uncertainty when they make their calling decision on their mobile phones. This price uncertainty occurs because consumers are unsure of their exact usage relative to the number of free minutes (allowance) included in the plan. We model consumer price uncertainty by including a perception error (actual usage/perceived usage) in consumers’ consumption decisions. We assume that the perception error has a mean of 1 and follows a log-normal distribution (this assumption is supported by results from a field study, reported below). With the perception error, consumers cannot keep track of their exact usage; instead, they recall their past usage with error. The presence of the perception error can be interpreted as limited consumer attention to keeping track of the exact usage.====The industry model has three stages. First, mobile network operators decide on the pricing structures of mobile phone plans; second, consumers decide whether to use mobile phones and, if so, which plan to subscribe to; and third, consumers make consumption decisions conditional on their chosen plan. The model is estimated using a rich billing dataset. We jointly estimate the consumers’ preference for usage and subscription to mobile phone services. We then back out the mobile network operators’ marginal cost using the demand estimates and the optimal pricing condition. Given these estimates, we simulate the price and quantity changes in the counterfactual scenario in which the regulation is implemented.====A crucial step in this estimation is identifying consumer price uncertainty. Our identification strategy is based on the lack of bunching at the point where the marginal price changes discontinuously: under the assumption that the distribution of consumer preference for calling is smooth, if consumers were aware of their exact usage, a mass point of consumers would use exactly their monthly allowance of free minutes; such bunching does not appear in the data, and this is informative about the degree of consumer price uncertainty.====In the counterfactual analysis, we study the effects of the bill shock regulation which requires cell phone companies to alert consumers when their usage hit the allowance. We first allow consumers to readjust their subscription and consumption decisions assuming no price adjustments from the firms. We then allow mobile network operators to readjust their prices in response to the bill shock regulation and, after finding the new price equilibrium, measure how consumer surplus and firm profits would be affected.====Assuming no price adjustments, we find that the elimination of usage and price uncertainty leads to a higher number of total calling minutes and a lower number of total overage minutes, with a large increase in the number of subscribing households using exactly their allowances. Consumers benefit from more calling minutes and lower overage payments, and monthly total consumer surplus increases by $53 million. Monthly total operators profit, on the other hand, decreases by $117 million mainly due to lower overage payments and higher costs associated with the larger number of calling minutes. Overall, monthly total surplus decreases by $64 million.====When price adjustments are incorporated into the analysis, we find that all major operators decrease the allowances, the fixed fees, and the overage prices in response to the regulation. Following the elimination of consumers’ usage and price uncertainty and the operators’ adoption of new pricing structures, the number of households who subscribe to mobile phone plans increases by 19.5% from 41 million to 49 million. The total number of monthly calling minutes remains largely unchanged, while the total number of monthly overage minutes increases as the new pricing structures (which feature increased per-minute prices for free minutes and reduced overage prices) induce more consumers to knowingly and optimally incur overage calling minutes.====Following the regulation and the resulting pricing changes, consumers incur lower fixed fee payments but higher overage payments. In addition, due to diminishing marginal utility of calling minutes for individual households, the fact that roughly the same number of calling minutes are now spread over a larger number of households results in an increase in the total utility from those calling minutes. The combined effect of those factors is an increase of $132 million in monthly total consumer surplus. The operators also benefit from the regulation and the resulting pricing changes, with monthly total operators profit increasing by $175 million. An increase in mobile penetration explains the joint increase in firm profits and consumer surplus. Overall, monthly total surplus increases by $307 million, which contrasts with the total surplus loss in the case without price adjustments and highlights the importance of accounting for firms’ strategic responses when evaluating the effects of a regulation.====Theoretical work by Grubb (2015a) shows that the welfare effects of bill shock regulation are ambiguous. The seminal empirical work by Grubb and Osborne (2015) predicts that the regulation would lower average consumer welfare by about $33 per year. The panel nature of data in Grubb and Osborne (2015) allows them to address consumers’ beliefs and learning: similar to Grubb (2009), consumers have biased beliefs, and furthermore consumer calling thresholds are less sensitive to the overage rate if individuals are overconfident, as their overconfidence leads them to underestimate the likelihood of an overage. In comparison, consumers have rational expectations in our model (the cross-sectional nature of our data prevents us from estimating consumers’ beliefs), and consumers would increase overage calling as a result of a reduction in overage prices in our counterfactual simulations. In addition, the data used in Grubb and Osborne (2015) pertains to a specific type of consumer—university students who were enrolled with a single mobile network operator, whereas the data used in this paper is nationally representative and covers all carriers, which allows us to assess the effects of the regulation on a broader population. We therefore view our paper as complementary to Grubb and Osborne (2015), and together these papers allow a more comprehensive understanding of the important yet nuanced effects of the bill shock regulation.====Our paper also relates to studies on optimal three-part tariff plans. Fibich et al. (2017) consider a theoretical model in which a service provider sells to risk-neutral and rational consumers. They characterize the optimal three-part tariff plan under general conditions, focusing on the monopoly case and abstracting from competition among service providers. Baek and Brueckner (2015) consider a theoretical model of three-part tariffs in the presence of heterogeneous users. They find that the monopoly outcome yields under-consumption of the service by both high- and low-demand consumers, while the duopoly outcome under Bertrand competition is efficient. Our empirical model takes into account the strategic interactions among the major mobile-phone carriers when computing the new price equilibrium following the bill shock regulation. By incorporating the strategic interactions among the firms when computing the new equilibrium, our counterfactual results give a more realistic picture of what would happen if the regulation comes into effect.====In this paper, we apply our empirical framework to mobile telecommunication markets in the U.S., but our approach and findings have useful implications for other countries and industries as well, in light of similar bill shock regulations in those settings. For example, Roaming Regulation in EU requires operators to send free text messages to their travelling customers to notify them about roaming charges, in order to protect consumers from bill shock arising from those charges (McGowan, 2018). Likewise, U.K. Retail Banking Market Investigation Order 2017 requires that banks send customers a text alert before charging them for entering into an un-arranged overdraft (Feikert-Ahalt, 2019). The results in the present paper can serve as one benchmark for future studies in this area, and our empirical framework can be modified to fit other settings for investigating how firms’ strategies will respond to such regulations and what the implications are for consumer welfare and industry profit.====The remainder of the paper is organized as follows. Section 2 presents the intuition of the model using diagrams. Section 3 proposes an empirical industry model with consumer usage and price uncertainty. Section 4 describes the data used for the estimation of the model. Section 5 discusses the identification and estimation results for the parameters in the model. Section 6 discusses the effects of bill shock regulation via counterfactual simulations. Section 7 concludes.",An empirical model of the effects of “bill shock” regulation in mobile telecommunication markets,https://www.sciencedirect.com/science/article/pii/S0167718722000248,11 May 2022,2022,Research Article,57.0
Miao Chun-Hui,"Department of Economics, University of South Carolina, Columbia, SC 29208, USA","Received 14 December 2019, Revised 22 April 2022, Accepted 25 April 2022, Available online 5 May 2022, Version of Record 16 May 2022.",https://doi.org/10.1016/j.ijindorg.2022.102847,Cited by (1),"Firms often sell a basic good as well as an ancillary one. When selling through platforms, they pay a fee for intermediated sales. The fee structure affects the pricing of the ancillary good. Under ad valorem fees, sellers have an incentive to shift revenue to the less taxed good. A lower fee on the ancillary good may thus increase its price. As a result, removing Apple’s App Store’s payment restrictions for in-app purchases can potentially harm consumers and lower welfare. Our analysis also shows that efficient consumption of the ancillary good is achieved under a fee or ==== that increases with the elasticity of demand for the good.","In many markets, firms sell ancillary goods (consumable, baggage allowance, breakfast) that complement the consumption of basic goods (equipment, flight ticket, hotel room). The pricing of ancillary goods has been the subject of many antitrust lawsuits,==== as well as a substantial body of literature. However, few studies in the existing literature have considered the subject in markets where firms pay a fee for intermediated sales. This paper contributes to the literature by examining how the fee paid to platforms (or intermediaries more generally) affects the pricing of ancillary goods.====We first consider markets in which a basic good is sold via intermediaries, but an ancillary good is sold directly by firms. The best example is perhaps the travel industry, where air flights, hotel rooms, and rental cars can be booked via online travel agencies, but incidental charges are collected on-site and paid directly to the service providers. When sellers pay an ad valorem fee on the sales of the basic good, lowering the basic good price while raising the ancillary good price allows a seller to keep more of its revenue. This revenue shifting effect raises the prices of the ancillary good, generating a pattern commonly known as “razor and blades” pricing.==== As a result, sellers charge supracompetitive prices in the ancillary good despite their lack of market power in the basic good. Further, the price markup of the ancillary good increases with its inverse elasticity of demand.====Although it is quite common for firms to sell ancillary goods directly but basic goods indirectly via intermediaries, there are notable exceptions. For example, Apple’s App Store requires that hosted apps use Apple’s payment system for any in-app purchases (“IAPs”). To study related issues, the paper then turns to markets where an intermediary charges ad valorem fees on both goods. It shows that a fee on the ancillary good has two competing effects on its price. On the one hand, the fee raises the effective cost of the good; on the other hand, it weakens the revenue-shifting incentive. If the second effect dominates, a higher fee can lead to a lower price for the ancillary good. Hence, it may be optimal to tax the ancillary good more heavily even though it has a more elastic demand. This result contrasts with the “inverse-elasticity” rule for the taxation of independent goods, ==== the Ramsey rule. We find that efficient consumption of the ancillary good is achieved under a fee or tax that increases with the elasticity of demand for the good. When applied to digital marketplaces such as the App Store, our analysis suggests that removing its payment restrictions for IAPs can potentially lead to higher prices, increasing developers’ profits but decreasing social welfare.====The paper is organized as follows. In the remainder of this section, we provide some background information on the Apple case and review related literature. Section 2 introduces the model and considers markets in which a basic good is sold via intermediaries, but an ancillary good is sold directly by firms. Section 3 considers markets in which both goods are sold via intermediaries. Section 4 concludes. The appendix contains proofs omitted from the main text. An Online Appendix examines the case of a flat fee under price parity clauses.",The pricing of ancillary goods when selling on a platform,https://www.sciencedirect.com/science/article/pii/S0167718722000236,5 May 2022,2022,Research Article,58.0
"de Haas Samuel,Herold Daniel,Schäfer Jan Thomas","Chair for Industrial Organization, Regulation and Antitrust, Department of Economics, Justus-Liebig-University Giessen, Licher Strasse 62, Giessen 35394, Germany","Received 23 December 2020, Revised 11 February 2022, Accepted 10 April 2022, Available online 29 April 2022, Version of Record 16 May 2022.",https://doi.org/10.1016/j.ijindorg.2022.102844,Cited by (1)," prices after the acquisition. Our findings are consistent with the interpretation that Flixbus pursued a strategy of pre-emption prior to the acquisition: Flixbus offered a high number of bus rides to decrease residual demand. After the acquisition, when the threat of entry was eliminated, Flixbus decreased the supply of trips per route and day. We would like to thank two anonymous referees and Frank Verboven for helpful comments and suggestions. We would also like to thank Niklas Dürr, Georg Götz, Justus Haucap, Paul Heidhues, Sven Heim, Michael Hellwig, Henry Thecat, Peter Winker and the participants of the following conferences: EARIE Annual Conference 2018, VfS Annual Conference 2018, MaCCI Annual Conference 2019 and CRESSE Conference 2019 for fruitful discussions on earlier versions of this article.","The concept of entry deterrence and entry barriers has been analyzed by economists at least since Bain (1949). More thorough theoretical analyses using game theoretic concepts were then established by, for instance, Spence (1977), Dixit, 1980, Dixit, 1982, Kreps and Wilson (1982), Schmalensee (1982) and Judd (1985). Although there now exists a large body of theoretical literature====, empirical analyses are scarce in this area. Empirical evidence is provided for specific industries such as automobiles (Barroso, Giarratana, 2013, Moreno, Terwiesch, 2016), pharmaceuticals (Bergman, Rudholm, 2003, Ellison, Ellison, 2011, Regan, 2008, Morton, 2000), hospitals (Dafny, 2005), airlines (Goolsbee and Syverson, 2008), hard drives (King and Tucci, 2002), casinos (Cookson, 2017), telecommunication (Weiman and Levin, 1994) and titanium dioxide (Hall, 1990). Cross-industry and overview studies are, for instance, presented by Bronnenberg et al., 2009, Bunch and Smiley (1992) or Bayus and Putsis (1999).====In this paper, we try to enrich the empirical knowledge on entry deterrence upon investigating a takeover in the interurban bus industry in Germany. In particular, we investigate a product proliferation or pre-emption strategy in the sense of Schmalensee (1978), which means that the incumbent renders market entry unprofitable by committing to offer a large variety of products (Wilson, 1992). We interpret different rides on a given route as different variants of a good, i.e., a trip from Munich to Hamburg at 8 a.m. is no perfect substitute to the same trip at 4 p.m.. This gives the incumbent firm the opportunity to deter entry by offering trips at multiple times throughout the day, i.e., offering trips with a high frequency.====Various European countries have recently opened up competition in the markets for interurban bus passenger transport. One example is Germany, where the industry was deregulated in 2013.==== After a period of market consolidation, by the end of 2016 Flixbus had a market share of more than 70% and the company announced the takeover of Postbus. Although Postbus was the largest remaining competitor of Flixbus, the operator was not active on all routes served by Flixbus. The merger was not investigated by the German antitrust authority because the turnover thresholds were not met.==== The date of the takeover was November 1, 2016. At the same time, BerlinLinienBus, a subsidiary of Deutsche Bahn, left the market due to low profitability. As a consequence, Flixbus’ market share rose to over 90% in the aftermath of the takeover.====We study the strategic behaviour of Flixbus prior to and after the takeover of Postbus. Our data set contains route-level data on prices and on the number of trips per route and day (frequencies) for about 6,000 routes in Germany for a period between September and December 2016 and the same period in 2017.====We find that after the takeover of Postbus ==== average prices increased and the average number of trips per route and day decreased. This is, however, mainly driven by the fact that Postbus was a low-price carrier, i.e., the prices were significantly lower than average. Controlling for this effect by using a fixed-effects panel regression with dummy-variables for the operator Flixbus and the takeover allows us to isolate Flixbus’ behavior before and after the takeover. We identify a mild decrease in Flixbus’ average prices of ==== and a drastic, highly significant decrease of ==== in Flixbus’ average daily supply of trips per route.====While these findings are inconsistent with standard merger analysis (e.g., Farrell and Shapiro 1990), based on which one would expect that Flixbus increases prices due to the gains in market power, they can be explained by pre-emption in the context of a differentiated goods (Salop) model (Schmalensee, 1978, Eaton, Lipsey, 1979). That is, prior to the takeover Flixbus used its first-mover advantage to render market entry unprofitable on routes not served by Postbus by offering a high number of trips per day. By using this strategy, Flixbus was able to charge relatively high prices on those routes because the consumers’ inconvenience costs were low. Given that the interurban bus market is rather small, Postbus was thus not able to establish a sufficiently large customer base to run a profitable business and, at some point, left the market. Additionally, entry of an entirely new firm was unlikely at least in the short run due to the necessary investments in marketing (establishing a customer base), a fleet and the permissions to serve the respective routes. After the acquisition of Postbus it was thus no longer necessary to follow a strategy of pre-emption so that Flixbus reduced the supply of trips per route and day on those routes where Postbus was not active before the acquisition. We also identify a statistically significant decrease in average prices of less than ====. Even though the magnitude of the effect on prices is relatively small, it could be explained by an attempt to dampen a decrease in demand associated with the reduction of trips per day. Given that Postbus’ business was basically shut down after the takeover, it is unlikely that the effect can be explained by merger efficiencies.====Our contribution to the literature is two-fold. First, we provide empirical evidence for the strategic use of pre-emption in a sense of product proliferation as in Schmalensee (1978). Although there are some empirical studies analyzing capacity pre-emption (Dafny, 2005, Ellison, Ellison, 2011) the closest related empirical study is Goolsbee and Syverson (2008).==== They empirically investigate the incumbent firms’ reaction to a rise in the threat of entry in the passenger airline industry in the US. They find that when the probability of entry increases incumbents decrease prices, which leads to higher passenger traffic. However, Goolsbee and Syverson (2008) cannot differentiate between specific flights, which our data allows us to do. The case we investigate is particularly interesting because the incumbent firm did not increase prices after the threat of entry was eliminated. We find that the high price level during the phase of entry deterrence was a result of the preemptive product proliferation strategy and the resulting large number of trips (variety) offered by the incumbent. Second, in the context of merger analysis, our study provides an example where constant or even slightly decreasing price levels after takeovers or mergers may actually be a sign of market power, especially in differentiated goods markets such as the German interurban bus industry. Decreasing prices alone are thus not necessarily an indication of welfare gains of a merger.====This article is structured as follows. In Section 2, we provide a brief overview of the German interurban bus industry. In Section 3, we present a theoretical model that is used to derive predictions for our empirical analysis. We present the data, descriptive statistics and empirical findings in Section 4. Section 5 concludes. Robustness checks and proofs can be found in the Appendix.",Entry deterrence due to brand proliferation: Empirical evidence from the German interurban bus industry,https://www.sciencedirect.com/science/article/pii/S0167718722000200,29 April 2022,2022,Research Article,59.0
"Abito Jose Miguel,Knittel Christopher R.,Metaxoglou Konstantinos,Trindade André","Ohio State University, United States,Shultz Professor of Applied Economics, MIT and NBER, United States,Carleton University, Canada,FGV EPGE, Brazil and Amazon, United States,MIT Sloan School of Management and NBER, United States","Received 19 May 2020, Revised 2 November 2021, Accepted 11 April 2022, Available online 26 April 2022, Version of Record 13 May 2022.",https://doi.org/10.1016/j.ijindorg.2022.102843,Cited by (0),"We examine the inefficiency of uncoordinated environmental regulation of CO==== emissions from electricity generation for a large regional U.S. wholesale electricity market that spans multiple states. We estimate a dynamic structural model of production and investment to compare the social welfare of two counterfactual regulatory scenarios. In both scenarios, emissions targets set by the regulator are met via endogenously determined CO==== prices in markets aiming to correct the externality. In the first scenario, the CO==== prices are state-specific. In the second scenario, there is a regional CO==== price. According to our social welfare estimates, the inefficiency of uncoordinated regulation is mitigated substantially because of the firms’ participation in an integrated product market in two main ways. First, firms reallocate output from states with high CO==== prices to states with low prices. Second, this reallocation spurs investment in cleaner capacity that is exempt from CO==== regulation. Our finding regarding the mitigation and, potentially, elimination of the inefficiency is robust to alternative models of optimal investment behavior.","Product markets typically fail in the presence of externalities. Since the work of Coase, economists have known that one solution to an externality problem is to establish property rights and let agents negotiate. In the presence of heterogeneous agents, market-based mechanisms (henceforth, MBMs) have also been shown to provide cost-effective solutions. These MBMs are designed around a price that corrects the externality efficiently.====In the case of pollution, several MBMs, including markets for emissions permits, have been proposed by policy makers. A market for permits is an efficient way to correct the negative externality accounting for heterogeneous marginal abatement costs. In the textbook setting, a single price-oriented MBM that corrects the externality, such as a market for permits, maximizes the gains from trade.==== In practice, organizing a single MBM often requires coordination across multiple jurisdictions, which is generally difficult due to differences in their preferences and priorities. At best, jurisdictions implement separate and often uncoordinated MBMs. For example, 27 jurisdictions had implemented or were scheduled to implement a form of a trading system for carbon emissions in 2019. Most of these trading systems were organized at the provincial or city level, and they were not linked to each other.==== In the U.S., attempts to address greenhouse gas emissions at the federal level have largely failed. Unless new legislation is passed, future attempts will most likely be at the state level, which will result in uncoordinated MBMs.====The goal of the paper is to measure empirically the outcomes of uncoordinated MBMs and compare them to those of a single MBM regulating emissions in order to quantify potential inefficiencies using social welfare as our metric. Our key insight is that firms invest more to increase their production capacities with uncoordinated MBMs, which mitigates, and potentially eliminates, the inefficiencies resulting from the lack of coordination of regulatory actions. In particular, given that the lack of coordination leads to spatial dispersion in prices aiming to correct the externality, there are locations where it is cheaper to emit. All else equal, profit-maximizing firms reallocate production, along with emissions, towards these locations until it is no longer profitable to do so or capacity constraints in the low-priced locations prevent them from doing so. These constraints spur investment in new capacity that is cleaner and is exempt from CO==== regulation. Because of the dispersion in CO==== prices, the incentive to invest in new capacity is stronger in the case of uncoordinated MBMs.====The regulation of CO==== emissions from electric power plants is our empirical setting. The electricity sector is a major source of CO==== emissions, just behind transportation, and has been the direct target of efforts to curb emissions. We focus on power plants in the Pennsylvania–New Jersey–Maryland (PJM) Interconnection, which is the world’s largest wholesale electricity market spanning multiple U.S. states. PJM is an ideal setting to study inefficiencies with uncoordinated regulation of markets for CO==== emissions because there is considerable heterogeneity in the plants’ fuel mix and emissions resulting in substantial heterogeneity in the stringency of emissions regulation across states. In the presence of such heterogeneity, a regional CO==== market allows gains from trade.====Using rich plant-level data, we set up and estimate a dynamic structural model of production and investment preserving the heterogeneity both within and across firms. We simulate the model and estimate social welfare for two counterfactual scenarios of CO==== regulation. In both scenarios, emissions targets set by the regulator are met via CO==== prices, namely, the shadow prices of the emissions constraints. We use the CO==== emissions targets from the Clean Power Plan (CPP) proposed by the Obama Administration in August 2015. Although the CPP was repealed by the Trump Administration, which proposed the Affordable Clean Energy (ACE) Rule in August 2018 to replace the CPP, it is the most recent example of comprehensive CO==== emissions regulations by the U.S. Environmental Protection Agency under the Clean Air Act. In the first scenario, there are separate (state-specific) CO==== markets that correct the negative externality. In the second scenario, there is a single regional (PJM-wide) CO==== market. In the first scenario, each state is required to keep CO==== emissions below its specified target. In the second scenario, regional compliance requires the sum of CO==== emissions across states to be below the sum of the states’ targets. Hence, in the first scenario, the market participants pay CO==== prices that depend on the plants’ location. In contrast, the market participants in the second scenario pay a single CO==== price regardless of the plants’ location.====Given the heterogeneity in plant characteristics and regulatory stringency across states, state-specific CO==== prices will most likely differ. In this case, the marginal abatement costs of plants are not equal. Based on insights from the trade literature (e.g. Samuelson, 1948, Mundell, 1957), if CO==== emissions are treated as a factor of production, inefficiencies from the lack of a single CO==== market may not be large if firms participate in an integrated product market and spatial reallocation of production (hence, emissions) is possible. In our setting, although power plants are located in different states and are subject to different CO==== prices, they supply electricity to an integrated product market (the PJM wholesale market). All else equal, profit-maximizing firms move production from plants in states with high CO==== prices to states with low CO==== prices. In fact, the larger the spatial heterogeneity in CO==== prices, the stronger the incentive to reallocate production. In the absence of any frictions in output reallocation and CO==== price adjustment, a firm continues to reallocate output until there is no difference in CO==== prices. However, frictions that impede reallocation and sustain the inefficiency due to separate CO==== markets are possible. One class of important frictions that we focus on are capacity constraints.====To understand the role of capacity constraints, we first examine the difference in social welfare between a single and separate CO==== markets assuming investment in capacity is exogenous. We refer to this difference in social welfare as ==== inefficiency. Next, we investigate whether optimal investment is different in the two regulatory scenarios. The comparison of social welfare in this case takes into account potential differences in the incentives to invest, and, therefore, differences in capacity levels. The difference in social welfare between the two scenarios that accounts for optimal investment in capacity is a measure of what we refer to as ==== inefficiency.====Our results show that the static inefficiency, namely the reduction in social welfare due to separate CO==== markets is small, and it is completely eliminated for sufficiently large capacity levels. In this latter case, the capacity constraints no longer bind and, hence, frictionless production reallocation is possible. As a result, the social welfare with separate CO==== markets is qualitatively similar to the social welfare with a single CO==== market.====Because the size of the inefficiency depends on the level of capacity, we examine the firms’ level of investment in capacity for each regulatory scenario. When we allow for optimal investment, which is exempt from CO==== regulation, the capacity with separate CO==== markets exceeds its counterpart with a single CO==== market. Given that investment in new capacity facilitates reallocation of production when capacity constraints bind, there is an additional benefit from investment in new capacity, which is increasing in the dispersion of CO==== prices. This additional incentive to invest is not present in a single CO==== market because there is no dispersion in CO==== prices. In our simulations, the additional investment with separate CO==== markets is sufficiently large to cause the generation cost to fall below its level with a single CO==== market, eliminating the dynamic inefficiency. Moreover, and depending on how we model optimal investment behavior, the decline in the generation cost with separate CO==== markets may exceed the increase in investment cost. As a result, the cost—generation plus investment— may actually be lower with separate CO==== markets.==== Our paper is related to the literature on incomplete regulation. Incomplete regulation occurs when there is no uniform adoption of regulations across jurisdictions, hence, exempting a subset of polluting sources from regulation. As such, separate and uncoordinated markets to correct the externality can be a direct consequence of incomplete regulation. The literature has focused on the important problem of emissions leakage whereby firms relocate production (and emissions) to unregulated jurisdictions, which reduces the efficacy of the regulation (see, e.g., Fowlie, 2009, Fowlie, Reguant, Ryan, 2016). A similar form of leakage occurs when firms face overlapping state and federal regulations in only a subset of states and state regulations are stricter than federal ones (Goulder, Jacobsen, van Benthem, 2012, Goulder, Stavins).====We contribute to the literature by illustrating the importance of thinking about adjustments that may not happen in the short-run and showing how ignoring these adjustments may overstate problems with incomplete regulation.==== Although the short-run inefficiency with uncoordinated regulation can be substantial, what drives the inefficiency—different state-specific CO==== prices in our case—can actually encourage more investment. More investment in turn mitigates the inefficiency due to lack of coordination. In fact, if there are distortions that weaken the incentives to invest (e.g. strategic capacity withholding and lax environmental regulations), the long-run social welfare may end up being higher.====The paper is also germane to the literature that investigates the interaction between environmental regulation and other forms of regulation and market structure.==== Our work is most related to Ryan (2012) and Fowlie et al. (2016) who build a Markov Perfect Nash Equilibrium (MPNE) framework and use a two-stage estimation method based on Bajari et al. (2007) to study the effects of environmental regulation in an oligopoly setting.====In terms of estimation, we follow Ryan (2012) and Fowlie et al. (2016), and adapt their methods to the electricity industry and our institutional setting. One important departure from their work is that we only estimate investment costs because we compute production costs directly from data on plant-level heat rates, emission rates for various pollutants, and other operations-and-maintenance related costs (e.g., Mansur, 2007, Bushnell, Mansur, Saravia, 2008, Gowrisankaran, Reynolds, Samano, 2016). Furthermore, our approach for the counterfactual simulations differs significantly from Ryan (2012) and Fowlie et al. (2016) in two ways. First, the computation of the stage-game equilibrium is more involved because we find the set of prices that simultaneously clear a large number of (CO==== and the electricity) markets. Second, given the stage-game equilibrium payoffs for each point in the state space, we solve for the MPNE by combining an Upwind Gauss-Seidel approach (Judd, 1998) with a state-wise Nash Equilibrium (NE) approach. This hybrid approach is less prone to convergence issues because finding the MPNE reduces to sequentially solving the NE of normal-form games in the state space.====Finally, our paper contributes to the empirical literature on electricity markets. Most of the literature has focused on firms exercising market power through strategic bidding and withholding of existing capacity—see Green and Newbery (1992) and Wolfram (1998) for early contributions, and more recently, Borenstein et al. (2002); Hortacsu and Puller (2008); Mansur (2007), and Bushnell et al. (2008). In contrast to these papers, we model strategic investment in new capacity, which has only received limited attention (e.g. Bushnell and Ishii, 2007).==== The remainder of the paper is organized as follows. Section 2 gives some background for our empirical setting. We present our model in Section 3, followed by a discussion of estimation and results in Section 4. Section 5 contains our counterfactual analysis. We finally conclude in Section 6. We relegate some additional material to the online Appendix.",The role of output reallocation and investment in coordinating environmental markets,https://www.sciencedirect.com/science/article/pii/S0167718722000194,26 April 2022,2022,Research Article,60.0
"Choe Chongwoo,Matsushima Noriaki,Tremblay Mark J.","Department of Economics, Monash University, Australia,Institute of Social and Economic Research, Osaka University, 6-1 Mihogaoka, Ibaraki, Osaka 567-0047, Japan,Farmer School of Business, Miami University, USA","Received 22 November 2021, Revised 16 April 2022, Accepted 21 April 2022, Available online 26 April 2022, Version of Record 6 May 2022.",https://doi.org/10.1016/j.ijindorg.2022.102846,Cited by (5),"We study a two-period model of behavior-based price discrimination where firms can agree to share customer information before the first-period competition begins, and the information can be used for personalized pricing in the second-period competition. We show that information sharing is individually rational for firms as it softens upfront competition when information is gathered, consumers are worse off as a result, but total surplus can increase thanks to the improved quality of matching between firms and consumers. These findings are robust to firm asymmetries and varying discount factors for consumers and firms.","Over the past several decades, the remarkable advances in digital technologies have allowed firms access to a vast amount of consumer data at the granular level. The availability of such data in conjunction with powerful machine-learning tools can be a potential source of competitive advantage.==== While there are many ways firms can gather such data, one important channel in various service industries such as banking and finance, retail, and travel has been customers’ past purchase histories.==== Such information can provide firms with the opportunities for behavior-based, or history-dependent, price discrimination (Chen, 2005, Fudenberg, Villas-Boas, 2006). In the crudest form, firms can exercise third-degree price discrimination with two market segments, existing and new customers (Chen, 1997, Fudenberg, Tirole, 2000). As the quality of information improves, the segmentation of existing customers can be further refined (Liu and Serfes, 2004), leading to personalized pricing in the limit. While personalized pricing may be in limited use in practice, it is becoming more prevalent in some industries thanks to the availability of big data and finer-grained analysis, and has been drawing attention from policy circles.====As data collection and usage often occur among competing firms, opportunities to share consumer data exist. For example, the airline and tourism industry relies on code-sharing to exchange data across firms. Firms intending to share data can also utilize a third party for which participants voluntarily provide their data, which is then aggregated.==== Information sharing also exists in the banking industry in the form of open banking, where participating banks can share and leverage customer data to promote the development of new apps and services.==== While it seems intuitive that information sharing may increase competition (Kim, Choi, 2010, Chen, Choe, Cong, Matsushima, 2022), its full effect on firm behavior is more subtle. In particular, the expectation of intensified competition due to information sharing will affect firms’ incentives to invest in and gather customer information in the first place.====This paper studies a dynamic model of behavior-based price discrimination to address how the possibility of information sharing affects competition both at the stage of information gathering and at the stage when information is shared for common use. Information sharing does not have bite if competiting firms have the same information set. This is the case in models of behavior-based price discrimination such as Fudenberg and Tirole (2000) where firms compete in third-degree price discrimination. Thus our model builds on Choe et al. (2018) where competition is in personalized pricing, but extends it in two important directions. First, we allow firms to commit to information sharing before they compete in price. The ensuing stages are then as follows: in the first period, firms compete à la Hotelling, at the end of which each firm acquires full information on all of its own customers; in the second period, they compete using a mix of personalized and uniform prices. If they agreed on information sharing, then both firms use personalized prices for all consumers based on the same, full information; otherwise, each firm sets personalized prices only for its own previous customers and a uniform price for the rest. Second, unlike most existing studies on behavior-based personalized pricing that assume firms with symmetric costs, we consider firms that may differ in their production costs.====Our main findings can be summarized as follows. First, information sharing is an equilibrium outcome in that each firm’s discounted sum of profits is larger under information sharing than that in all equilibria without information sharing. The main driver of beneficial information sharing is softened competition in the first period. Although information sharing intensifies competition in the second period by allowing both firms to use personalized pricing for all consumers, the effect is relatively small since, even without information sharing, firms use personalized pricing for their own first-period customers. In sum, the benefits from softened competition in the first period outweigh the costs of intensified competition in the second period. Second, as a robustness check, we consider the case where firms may have different marginal costs of production. We show that the cost asymmetry does not have any effect on our main conclusion; it only determines which firm becomes more aggressive in the first period. However, the cost asymmetry introduces a discontinuity in the set of equilibria in the subgame without information sharing. When the cost difference is small, there continue to exist two asymmetric equilibria as in Choe et al. (2018). But the equilibrium is unique when the cost difference is above a certain threshold. In this equilibrium, the less efficient firm chooses aggressive pricing in the first period and, as a result, there is one-way customer poaching by the more efficient firm in the second period. These results hold for any discount factors for firms and consumers.====While firms are better off with information sharing, consumers are worse off in that the discounted sum of consumer surpluses is smaller when firms share information. The intuition is as follows. Information sharing affects consumer surplus through two channels: prices and the quality of matching between firms and consumers, as reflected in the transportation cost in our Hotelling model. Information sharing increases the quality of matching with less customer poaching in the second period than without information sharing, which benefits consumers. The flip side is that information sharing creates a negative effect for consumers by softening competition in the first period. Given that firms benefit from information sharing, we expect the negative effect to dominate the positive quality-of-matching effect. At the same time, because prices reflect only the division of surplus between firms and consumers, information sharing can increase total surplus thanks to the positive quality-of-matching effect.====We also analyze several further extensions of our model. First, when firms share customer information, competition in third-degree price discrimination leads to larger profits and consumer surplus, but smaller total surplus than when competition is in personalized pricing. Second, we consider the case where the market is only partially covered in the first period. Because the competition-softening benefits of information sharing in the first period are reduced in this case, we find that firms choose not to share information. Third, when consumers’ preferences change over time so that customer information gathered in the first period is imperfectly correlated with that in the second period, information sharing continues to be an equilibrium outcome. It is because sharing information with imperfect preference correlation makes competition in the second period less intense but competition in the first period more intense than when preferences are perfectly correlated. Fourth, when firms make information sharing decisions at the end of the first period, we show that they choose not to share information. The cases with partial market coverage and alternative timing of information sharing highlight the main mechanism at work in our model. Namely, the pre-commitment to sharing information before information gathering and the large benefits from softening up-front competition are crucial for our results. It is because the main benefits from information sharing materialize when the agreement to share information softens competition at the stage when firms gather customer information.====Our work makes contributions to the literature in several important ways. First, we enrich the existing literature on behavior-based price discrimination by incorporating firms’ decisions on information sharing. Second, the existing literature on information sharing reviewed in the next section shows that customer information sharing can be an equilibrium outcome only if there are sufficient asymmetries in brand loyalties, product differentiation, or consumer preferences. In contrast, we show that information sharing emerges under a robust space of parameters if firms can commit to information sharing before gathering information. This may be taken as an explanation for the prevalence of institutions such as database co-ops or open banking. Third, we extend the literature by allowing firm asymmetries and show how the set of equilibria depends on cost asymmetry. In particular, we provide a precise condition on when multiple equilibria collapse to a unique equilibrium and fully characterize these equilibria. Finally, our welfare analysis shows a clear trade-off between consumer surplus and total surplus that results from information sharing. This can shed light on possible regulations that govern customer information sharing among competing firms. Of course, this is subject to a caveat that our focus is on the use of customer information for pricing purposes only and, therefore, we do not consider other effects of information sharing.====The rest of the paper is organized as follows. The next section provides a review of the related literature. Section 3 presents our baseline model, which is analyzed in Section 4. We provide additional discussions and extensions to the model in Section 5 and conclude the paper in Section 6. All deferred proofs are contained in Appendix.",Behavior-based personalized pricing: When firms can share customer information,https://www.sciencedirect.com/science/article/pii/S0167718722000224,26 April 2022,2022,Research Article,61.0
"Aridor Guy,Gonçalves Duarte","Department of Economics, Columbia University, USA,Department of Economics, University College London, UK","Received 6 April 2020, Revised 14 April 2022, Accepted 21 April 2022, Available online 26 April 2022, Version of Record 19 May 2022.",https://doi.org/10.1016/j.ijindorg.2022.102845,Cited by (5),"We study a model of strategic interaction between a producer and a platform that employs a recommendation system, following an information design approach. Upon entry into the production market, the platform biases recommendations to credibly steer consumers towards its own goods. Despite the increased upstream competition, platform entry and self-preferencing can decrease ==== and result in foreclosure of the independent producer. We then consider the natural policy remedy of separating recommendation and production or imposing unbiased recommendations and find it leads to welfare gains if the platform’s revenue potential is large enough, but to significant welfare losses when it is not. The ambiguity of such a policy’s welfare implications and the dependence on the industry’s returns highlights the importance of targeted restrictions on platform self-preferencing.","An increasing number of online platforms deploy recommender systems to assist consumers with purchase decisions by providing information on available goods, which determinedly impacts consumer choice. These systems facilitate information acquisition on product quality by consumers in environments where there are thousands or even millions of alternatives available. Existing experimental literature supplies causal evidence of these systems’ immense power in steering demand, with market shares being significantly affected even by recommendation systems that supply simple information to consumers (Salganik et al., 2006). Moreover, anecdotal evidence reflects the huge influence the information provided by these systems has on consumption choices individuals make: recommendations are said to account for 75% of consumed content on Netflix and 35% of page views on Amazon (MacKenzie et al., 2013).====However, online platforms increasingly not only deploy recommender systems, but also produce and make available their own goods alongside other firms’, with unclear implications to consumer welfare. Major platforms and technological leaders in the development and deployment of recommender systems — such as Amazon, Netflix, and Spotify — all now develop their own goods that are then made available on their platforms: Amazon produces more than 22,000 goods that are available on the firm’s platform (Davis, 2020), Netflix hosts more than 2300 “Netflix Originals” titles (Netflix, 2021), and Spotify is now investing in producing its own audio content (Binder, 2020).====One possible consequence of this dual role of the platforms as both a recommender system and a producer is that platforms may systematically bias their search and recommendation systems towards their own goods. Indeed, not only is there a substantial amount of popular press coverage suggesting this,==== there are also a number of recent legislative initiatives that appear motivated by potential abuse of this dual role, both in trying to prevent platforms in this dual role from biasing their recommendation systems in favor of their own goods and in proposing the separation of the roles of recommender and producer.==== While the goal of regulators is to increase consumer welfare in such markets, it is unclear whether these regulatory initiatives will ultimately harm or hurt consumers once equilibrium effects are taken into account.====In this paper we study the welfare consequences of a platform acting as both a producer and a recommender and consider the specific role played by the deployment of a recommender system. We set up a stylized model of a pay-for-access platform where producers make investment decisions about the quality of their goods and revenue is split according to each goods’ market share. Our main focus is in contrasting resulting consumer welfare, investments, and market shares across three different scenarios: (i) ====, where only a good by an independent firm is available; (ii) ====, where the platform can both produce a good and design recommendations, and both the platform’s good and the independent firm’s are available; and (iii) ====, where we modify the dual role case by imposing an exogenous policy that requires that recommendation be unbiased, or truthful and neutral.====Unlike other papers that study the consequences of platform steering (e.g. Hagiu and Jullien 2011; de Corniére and Taylor 2019; Teh and Wright 2020), we model the platform’s recommendation as providing information on good quality to consumers as opposed to directly influencing the search order or choosing the consumed good for a fraction of consumers. Producers have access to stochastic investment technology that affects the likelihood that goods are of high versus low quality. Consumers’ prior beliefs on good quality stem from the observed investments, and they update their beliefs on each good’s quality based on the recommendation policy of the platform. This allows us to build credibility of recommendation directly into our model, where the platform’s ability to steer the behavior of rational and Bayesian consumers is naturally limited and depends on the design of its recommendations.====In our model, the revenue generated by the platform is based on pay-for-access and producers are compensated according to their (expected) consumption share. We believe this captures the fundamental elements that platforms in this dual role face (e.g. Spotify, Netflix), and constitutes one of the primary drivers in motivating the platforms to bias recommendations towards their own goods. Additionally, as a reduced-form proxy for the extent to which the independent firm is reliant on the platform for revenues, we allow the independent firm to have access to alternative sources of revenue, and characterize their impact on equilibrium investment decisions and consumer welfare. For instance, in the movie industry, Follows (2016) reports that box office revenues are an essential source of revenue for movie producers, while Lynch (2018) highlights that in the music industry, recording artists made most of their revenues from touring despite the widespread usage of online streaming services.==== We show that the degree to which the independent firm depends on the platform as its main source of revenue is decisive for whether the platform’s dual role has a positive or a negative effect on consumer welfare.====The platform’s entry into the upstream market affects consumer welfare both via investments in good quality, as well as through the platform’s recommendation policy, which provides consumers with information on realized good quality.====Although limited by the need for recommendations to be credible in order for consumers to choose the recommended good, the platform is able to bias recommendations towards its own good so as to reappropriate the gains from information provision to increase its market share. Hence, the platform’s dual role leads to biased recommendations that induce a significant pressure on the independent firm’s equilibrium investment decisions: for the independent firm to appropriate a share of platform demand it need not only to invest more than the platform, but it also needs the realized quality of its good to be strictly higher than the platform’s.====While it would be natural to expect consumer welfare to increase with the platform’s entry — as product variety increases and as more information cannot harm consumers — the prospect of biased recommendation and its resulting equilibrium effect on investment levels leads to an ambiguous effect on consumer welfare. When the independent firm is heavily dependent on the platform and recommendation bias favoring the platform is most significant, the platform’s entry results in lower investment levels and consumer welfare. However, if the alternative sources of revenue are sufficiently large, the platform’s entry increases consumer welfare.====This result is driven by two opposing forces which influence the marginal incentives of the independent firm to invest in quality, compared to when the independent firm is the sole producer. The first is a ==== channel, which expresses the ability of the firm to expand the revenue it obtains from the platform by increasing its investment level. This channel is depressed by platform entry and its use of biased recommendations as these significantly decrease the independent firm’s market share and, therefore, the independent firm receives but a fraction of its marginal impact on total platform revenue. The second is a new ==== channel, whereby additional investment by the independent producer allows it to increase its market share. The platform uses its recommendation policy to appropriate a share of the demand, but its ability to bias is limited by the difference in investments in good quality. Then, larger investment by the firm naturally limits the platform recommendation bias, which then expands the firm’s market share. These two opposing forces generate a threshold effect for the investment levels of the independent firm in terms of its reliance on the platform as its main source of revenue: When the independent firm overly relies on revenue obtained from the platform, the platform’s dual role effectively depresses its incentives to invest in quality, resulting in lower consumer welfare; if instead it has access to other sources of revenue that are significant enough, the independent firm invests more strongly than when it is the sole producer, driving up consumer welfare.====One important consequence of the platform’s dual role and the resulting recommendation bias is the possibility of foreclosure of the independent firm. In equilibrium, this occurs when the independent firm’s alternative revenue sources are small relative to the platform size, in which case the platform finds it profitable to become the product-quality leader. Then, due to the ability to bias recommendations towards its own good, it completely drives demand away from its competition, capturing the entire demand on the platform. Although this is a sharp prediction, it echoes recent trends in video streaming markets, where platforms’ original content quality — as indicated by awards received — has notably risen while their own content has simultaneously dominated platform viewership. When the independent firm’s revenue sources are large enough compared to the platform’s revenue potential, the platform becomes a product-quality follower. Even then, the platform still partially forecloses the independent firm by biasing recommendations in favor of its own goods, enabling it to achieve a higher market share and profit than otherwise.====We explore a natural policy remedy: ensuring that the platform cannot simultaneously provide recommendation services and produce goods; or, equivalently, a policy that prevents the platform from providing biased recommendations towards its own goods.==== This analysis further provides insight into how much of the distortion in consumer welfare and investment decisions is due to the platform’s ability to bias recommendations relative to entry and information provision alone. Although it would be reasonable to expect an unambiguous improvement in consumer welfare — since more informative recommendations should only help consumers relative to biased recommendations — we find that this policy can actually harm consumers under certain conditions.====There are two observations that lead to this result, which stem from the fact that the unbiased recommendation policy induces downward equilibrium adjustments in investment decisions that can outweigh the welfare gains arising from unbiased recommendation. The first is that the ==== channel is more responsive to investments by the quality follower, and less to investments by the quality leader. This is because, under the dual role, the platform’s use of recommendations to expand its demand share leads to fully dissipating any informational gains to consumers, and revenue is completely determined by the investments of the quality leader. Unbiased recommendations maximize informational gains and expand overall revenue, but lead to the aforementioned effect on how investments by the platform and the firm affect total revenue at the margin. The second is that imposing unbiased recommendations results in a depressed ==== channel relative to the dual role for both the independent firm and the platform. Thus, imposing a fairer competition for demand share between the platform and the independent firm by requiring unbiased recommendations has the unexpected negative effect of reducing pressure to invest in quality by both parties.====Combined, these two observations result in an ambiguous welfare effect of imposing unbiased recommendations. Relative to the dual role, unbiased recommendations improve consumer welfare when the platform’s market size is comparable to the firm’s alternative market size. However, when the platform’s market size is large enough or when the independent firm relies mostly on alternative revenue sources, stripping the platform from its power to bias recommendations in its favor overwhelmingly depresses incentives to investment and entails a loss in consumer welfare. Further, the magnitude of welfare effects of such a policy depends crucially on the industry’s structure and the relative weight of different revenue sources.====Our results illustrate how platforms entering the upstream production market benefit from biasing recommendations. However, the resulting distortion from this entry does not necessarily harm consumers, as it may spur other producers to invest more aggressively in good quality to counter not only increased product competition, but especially recommendation bias. An important element to consider is how dependent the other producers are on revenues from the platform. Only when this dependence is significant will policies targeting bias in recommender systems or separating recommendation and production altogether have a positive effect on consumer welfare. While platforms and online retailers have other means at their disposal to distort consumption choices that are not explicitly addressed in this paper, our model suggest caution when considering policy interventions: the bias in recommender system may be inducing independent firms to produce higher quality goods than what they otherwise would. Indeed, our results provide a rationale for the stipulations of the ==== and the ==== that end the dual role only for dominant, “gatekeeper” firms. In such cases the welfare effect for consumers is likely to be positive, whereas for smaller platforms it will have an ambiguous or lesser effect.",Recommenders’ originals: The welfare effects of the dual role of platforms as producers and recommender systems,https://www.sciencedirect.com/science/article/pii/S0167718722000212,26 April 2022,2022,Research Article,62.0
"Bryan Kevin A.,Lemus Jorge,Marshall Guillermo","Rotman School of Management, University of Toronto, Canada,University of Illinois at Urbana-Champaign, United States,Sauder School of Business, University of British Columbia, Canada","Received 21 October 2021, Revised 21 March 2022, Accepted 25 March 2022, Available online 2 April 2022, Version of Record 9 April 2022.",https://doi.org/10.1016/j.ijindorg.2022.102841,Cited by (2),"We propose a model to show that when innovation in a given field becomes more lucrative, its ==== can be distorted even though its ==== rises. Higher payoffs attract innovators, making the R&D supply side more competitive. This competition endogenously shifts effort toward less promising but quicker-to-invent projects. We empirically quantify the magnitude of this distortion, in the context of pharmaceutical innovation during the Covid-19 pandemic. In the social planner solution, 74 percent more firms would have worked on vaccines and 17 percent more on novel compounds. Policy remedies include advance purchase commitments based on ex-ante value, targeted research subsidies, and antitrust exemptions for joint research ventures.","What happens when innovation in a particular field becomes more valuable or less costly? For instance, a demand shock can increase payoffs, as with pharmaceutical innovation during a pandemic. Policy changes like stronger patents, innovation prizes, and research subsidies can make innovation more profitable. A complementary invention may facilitate the development of new ideas, such as machine-learning breakthroughs for drug discovery. The first-order effect of any of these changes in a particular field is to increase the ==== of innovation.====These exogenous payoff shifters change more than just the rate of R&D, however. While higher payoffs mean firms are willing to pay the fixed cost of setting up a research program, firm entry alters market structure, which endogenously affects the ==== of invention. That is, the particular research projects firms pursue are a function of the nature of competition. As R&D competition becomes more intense, firms shift effort away from long-run, high-value projects, and toward short-run, less-valuable partial substitutes. Why? Lower-quality, quick-to-discover inventions decrease the ex-post marginal value of high-quality inventions that are partial substitutes. This externality is ignored when firms choose R&D portfolios. In equilibrium, the industry becomes more likely to “race” toward projects that can be completed quickly.====We will first formalize the idea that exogenous shifts in underlying payoffs distort the direction of innovation relative to the socially optimal one, using a simple theoretical model of invention choice with endogenous market structure. We then empirically implement this model to estimate the magnitude of this directional distortion.====Our analysis exploits the extensive and detailed documentation on pharmaceutical development during the Covid-19 pandemic. We use proprietary data on firm characteristics and project choice of entrants during the first six months of the pandemic (i.e., ==== the introduction of major government subsidies and other interventions).==== This setting is ideal for four reasons. First, there is well-documented data on hundreds of entrants working on Covid-19 projects. Second, there are well-defined choices of direction such as “vaccine or therapeutic,” with vaccines widely believed ex-ante to be more difficult to develop and more socially valuable (see, e.g., Xue and Ouellette, 2020). Third, there is standardized data on each firm’s prior research capabilities. Fourth, we know exactly when the crisis started, and we observe a shock to its severity in early March 2020 when the epidemic globalized.====In the social planner solution, our estimates show that 74 percent more firms would have worked on vaccines and 17 percent more on novel compounds between January and June 2020. Of the 74 percent gap, 10.2 percent remains even if we assume pharmaceutical firms earned the full social surplus of their inventions: strategic racing rather than just underappropriation slowed vaccine development.====Our results principally build on two strands of the innovation literature, one theoretical and one empirical. On the theoretical side, the potential for strategic racing either in terms of the quantity of R&D (Loury (1979); Reinganum (1982)) or its direction (Bryan and Lemus (2017); Hopenhayn and Squintani (2021)) has long been noted. This literature broadly shares the idea that innovators do not account for how their research effort affects the probability of success by other firms. We extend this insight by endogenizing market structure, showing that since the extent of strategic racing is increasing in R&D market fragmentation, “hot” technological areas that attract many entrants are more at risk of directional distortion.====There is a nascent empirical industrial organization literature studying the impact of market structure on innovation outcomes (Goettler and Gordon (2011), Igami and Uetake (2020)). This literature, however, does not speak to the direction of innovation. In non-structural work on invention direction, Moser (2005) suggests that inventors in countries without strong patent protection shift effort toward inventions which can be protected by secrecy, such as Swiss watches. Acemoglu et al. (2012) shows that factor scarcity and inventor subsidies affect the direction of invention in the context of climate change policy. Hanlon (2015) does the same when studying textile innovation at the time of the Civil War. These studies demonstrate that R&D direction shifts to areas where inventor rewards are higher and reacts to changes in factor prices and technological substitutability. However, to our knowledge there are no other empirical studies on distorted invention direction based on endogenous market structure, structural or otherwise.====Finally, there has been a great deal of research using the Covid-19 pandemic as a case for innovation policy. This literature is vast; however, of particular note are Gross and Sampat (2021) on how Covid-19 policy compares, favorably or otherwise, to World War II “crisis” innovation, Agarwal and Gaule (2021) showing that the quantity of scientific research can in some cases be quite elastic, without crowd-out, when sufficient public spending is directed toward a particular goal, and Kremer et al. (2022) on incentivizing vaccine production with advanced market commitments.",R&D competition and the direction of innovation,https://www.sciencedirect.com/science/article/pii/S0167718722000170,2 April 2022,2022,Research Article,63.0
"Nagaoka Sadao,Yamauchi Isamu","Research Institute of Economy, Trade and Industry and Tokyo Keizai University,Research Institute of Economy, Trade and Industry and Meiji University","Received 9 February 2021, Revised 21 January 2022, Accepted 11 February 2022, Available online 10 March 2022, Version of Record 20 March 2022.",https://doi.org/10.1016/j.ijindorg.2022.102839,Cited by (0),"Using an exogenous policy change in Japan that accelerated ==== examination, we examined how information constraints in a patent office affect patent examination quality in terms of both type I errors (wrong grants) and type II errors (wrong rejections). We found that accelerated examination increased both types of errors, but the increase in the number of wrong grants dominated that in the number of wrong rejections. Furthermore, the effect was stronger for technology sectors wherein the information constraints on examiners were stronger (short technology cycles and early examination requests) before the policy change. Our main results held when using the examination and appeal panel data of the Japan Patent Office (JPO) as well as for the matched sample of the JPO and European Patent Office examinations. In conclusion, a better information gathering system is important for improving the quality of patent examination by patent offices.","The quality of patent examination is the cornerstone of an efficient patent system for promoting innovation (Federal Trade Commission, 2003; Jaffe and Lerner, 2004; National Research Council, 2004; US Government Accountability Office, 2013, 2016a, 2016b). The existence of patents with uncertain validity can result in a situation wherein a “weak” patent leads to strong market power (Farrell and Shapiro, 2008; Lemley and Shapiro, 2005). Ambiguous patents make inadvertent infringement more likely and litigation more frequent (Bessen and Meurer, 2008). Low quality of patent examination can result in a higher number of patents with fewer inventive steps than required by patentability standards, which could, in turn, reduce the incentive for high-quality inventions====.====This study analyzes the relationship between the amount of information available to patent examiners and their patent examination quality. The longer the gap between the application and examination dates, the larger the stock of useful information that becomes available to the examiner for identifying relevant prior art and understanding the technology development up to the focal application. In other words, the stock of information available to the examiner at the time of examination is likely to be a key determinant of examination quality and increases over time (P, Rockett, K., 2016). However, few studies directly investigate this relationship.====Information constraints regarding the identification of the most relevant (or the closest) prior art are likely to cause wrong grants, because the examiner must prove the lack of novelty or inventiveness in order to reject an application. In addition, imperfect understanding of the technological trajectory from prior art to the focal patent application can increase the likelihood of wrong rejection due to a hindsight bias (Mandel, 2006)====. Thus, unlike earlier studies, we examine the consequences of information constraints for both type I errors (wrong grants) and type II errors (wrong rejection), controlling for examination standards and resources.====To establish causality, this study utilizes a policy reform in Japan that reduced the period for which applicants could request an examination from seven years to three years. This policy change was aimed at reducing uncertainty due to pending patent applications. However, this change may have simultaneously affected the quality of examination adversely by tightening examiners’ information constraints. We investigate how this accelerated initiation of the examination increased the grant rate and the rate of challenged or eventually reversed refusals. We also assess whether the effects were stronger in sectors with more severe information constraints on examiners (short technology cycles and early examination requests) even before the policy change.====We face two important identification challenges, because the policy change was likely to have affected examination quality through mechanisms other than increased information constraints. First, the change increased examiners’ workloads, as discussed below, which could have reduced their examination quality. However, the policy change facilitated the comparison of the examination outcomes for the applications filed before and after the policy reform for similarly timed examinations. As the applications filed before the policy change had longer allowable periods for examination requests compared with those filed after the policy change, many of the before-change applications were examined alongside those filed after the change. Furthermore, as applications are randomly assigned to examiners, there is little need to control for the potential differences between examiners with respect to the applications filed before and after the policy reform, because randomization averages the effects of such differences on examination outcomes.====A second confounding factor is that the application quality differs between the treatment and control groups due to increased examination requests after the policy change. We adopt three measures to address this factor. First, we introduce extensive covariates controlling for application quality. Second, we use a “restricted” sample, excluding the bottom 50% of the distribution, because increased examination requests would be likely to affect the bottom part of the treatment group most heavily. Third, to perform a robustness check, we examine a matched sample (twin sample for the same inventions) of the Japan Patent Office (JPO) and the European Patent Office (EPO) examinations, which fully controls for invention quality.====The findings show that the accelerated timing of examination can result in more type I and type II errors, and the effect on type I errors is stronger in technology sectors that had greater information constraints before the policy change. The study suggests the importance of strengthening the patent office's information gathering system and examination capability in complementing policymaking on accelerated patent examination.====The rest of the paper is organized as follows Section 2. provides a brief review of the related literature Section 3. provides the institutional background and hypotheses for our econometric analysis Section 4. covers the data Section 5. presents the estimation framework and results Section 6. concludes the study.",Information constraints and examination quality in patent offices: The effect of initiation lags,https://www.sciencedirect.com/science/article/pii/S0167718722000157,10 March 2022,2022,Research Article,64.0
Sinitsyn Maxim,"Northwestern University, 2211 Campus Drive, Evanston, IL 60208, United States","Received 17 December 2020, Revised 8 February 2022, Accepted 28 February 2022, Available online 3 March 2022, Version of Record 15 March 2022.",https://doi.org/10.1016/j.ijindorg.2022.102838,Cited by (0),I introduce and analyze a framework of price leadership that incorporates an important feature of a modern retail ,"In this paper, I propose a novel framework for modeling price promotions. One of the firms—a leader—commits to a promotion calendar, which specifies in advance the frequency and depth of future price promotions. Another firm—a follower—responds optimally to this probability distribution over prices. A crucial feature of this framework is that the follower’s response is based on the entire price distribution rather than each specific price realization.====One example of such interaction is the competition between a supermarket and a convenience store. These stores represent a large portion of the US economy. Supermarket sales exceeded $680 billion in 2019 (Conway, 2020) and convenience stores had in-store sales (excluding fuel) of $252 billion (NACS, 2020).==== Supermarkets and convenience stores employ different types of pricing strategies. Major supermarkets frequently offer temporary price reductions or sales. They remain an important tool in the retail industry with 67% of all orders sold using a price markdown in 2016 (Action, 2017). Supermarkets do not choose their prices weekly, but instead collaborate with manufacturers on a trade promotion calendar—a schedule of planned prices and promotional activities up to a year in advance. After the schedule is set, the timing and depth of price discounts are not easy to change, especially for larger retailers (Anderson, Malin, Nakamura, Simester, Steinsson, 2017, Sinitsyn, 2017, Anderson, Fox, 2019). Thus, when modeling the promotion strategy of supermarkets, it is important to incorporate the fact that the prices are not set contemporaneously, but instead follow sticky plans.====In order to learn more about the pricing strategies of convenience stores, I conducted several interviews with their owners (including both an independently run store and a franchisee of a large national chain). I found out that in general, convenience stores do not have long-term price commitments and have the flexibility to set their prices after observing the supermarket’s price. In fact, one of the owners described that they would personally visit the neighboring supermarket to examine the prices in key categories. However, the purpose of these visits was to get a general sense of the price levels without an intention to undercut specific prices. In practice, convenience stores do not match the weekly fluctuations of supermarket prices. There are several reasons for why this might be the case.====First, there are menu costs associated with frequent price changes, which a convenience store might not want to bear. Second, the promotions work best when they are accompanied by other elements of marketing mix, such as advertising. A convenience store is likely to lack resources for expensive advertising campaigns. Finally, using the framework developed in this paper, it is possible to show that even if it were feasible for the convenience store to match the supermarket’s price movements, it could strategically choose not do so and maintain a single price in order to soften the pricing strategy of the supermarket.====Taken together, a supermarket chooses a promotion calendar that specifies the depth and frequency of planned discounts. A convenience store responds to this calendar but not to specific prices. I interpret this by modeling the supermarket as committing to a probability distribution over prices, and the convenience store responding to the chosen distribution rather than a realized price. Thus, the appropriate model for this industry dynamic is a sequential game with the supermarket choosing its (potentially mixed) strategy first, and the convenience store choosing its price second. All consumers buy from the store that offers the lowest price. Formally, the model is a Stackelberg-Bertrand game with homogeneous consumers where the leader chooses a mixed strategy and the follower responds to the distribution of prices rather than to the realization of this strategy. A similar environment was recently studied by Xu and Ligett (2018) who examined first-price auctions with a leader publicly committing to a mixed bidding strategy.====Consider an example where the consumers’ reservation price is 1 and both stores’ costs are 0. Assume, further, that if both firms charge identical prices, the follower captures the whole market.==== I will maintain this assumption throughout the paper. If the leader uses a single price, this price is matched by the follower, and the leader earns zero profit. The leader can, however, create a promotion calendar in which reservation price 1 is charged in half of the weeks and sale price ==== is charged in the remaining weeks. Then, the follower’s optimal response is to match the regular price of the leader by charging 1. It results in an expected profit of ==== per week. The follower cannot increase its profit by matching the leader’s sale price as that would also generate a profit of ====. By using this promotion calendar, the leader sells to all consumers when it conducts a sale. Thus, it earns a profit of ====. By using more sale prices, the leader can increase its profit further. If it charges the regular price in ==== of the weeks, a sale price of ==== in ==== of the weeks, and a sale price of ==== in the remaining ==== of the weeks, the follower’s best response is again to match the regular price. The leader earns a profit of ====.==== In the limit, as I show in the paper, if the leader can use infinitely many sale prices, it will charge the reservation price with probability ====, use a continuous probability distribution over sale prices, and earn an expected profit of ====. The follower also earns an expected profit of ====. Note that despite selling to homogeneous consumers, both firms earn a positive profit in this framework.====The optimal mixed strategy of the leader is set to force the follower to charge the highest possible price. The leader achieves this by using a carrot-and-stick strategy. The carrot part of the strategy is to charge the reservation price with a relatively large probability, making it attractive for the follower to match this price. Then, any sale by the leader captures all consumers, so it benefits from charging higher sale prices. At the same time, it needs to prevent the follower from undercutting its sale prices, so the stick part of the strategy is to place sufficient probability on low sale prices. These theoretical results are consistent with the actual pricing patterns of supermarkets and convenience stores. Convenience stores are known to have higher average prices (Broda, Leibtag, Weinstein, 2009, Caspi, Pelletier, Harnack, Erickson, Lenk, Laska, 2017) while offering less frequent promotions than supermarkets (Walters, 1991, Powell, Sumanyika, Isgor, Rimkus, Zenk, Chaloupka, 2016).====These considerations can manifest themselves in other pricing situations.==== One possibility is a cross-sectional rather than intertemporal interpretation of the model. Consider a leader who operates a (unit mass) set of local retailers in some region and chooses a distribution of prices across them. A follower operates a national chain and is restricted to offer a single price in all of its stores. Assume that each location in this region is served by one local retailer outlet and one store from the national chain. If we allow the leader to choose its prices first, the follower is able to observe the distribution of prices across the leader’s stores, i.e., the leader’s mixed strategy, but it has to respond with a single price. A similar pricing strategy in a different setting was obtained in Dobson and Waterson (2005), who showed that a chain store might commit to use uniform pricing in several markets differentiated by level of competition. While this involves charging lower prices in uncontested markets, such stores benefit from softened price competition in the markets where they face rivals.====Another example is Amazon, which is known for its frequent price changes.==== A conventional explanation of this phenomenon is that Amazon is involved in dynamic pricing, adjusting its prices in response to the constantly changing competitive environment (The Economist, 2016). The results in my paper show that Amazon could be using frequent price fluctuations to enforce discipline among other merchants and prevent them from competing on low prices.====As I discuss in the paper, the main features of the equilibrium are unchanged if the follower is able to respond to a specific price of the leader with some probability ==== or if it has to pay a cost to inspect the leader’s price. In the former case, the strategies of the firms remain unchanged. In the latter case, if the cost of inspection is small enough, the leader prevents such inspections by increasing the probability of charging the regular price. A separate extension of the basic framework incorporates the follower having a higher production cost. For the follower, this decreases the profitability of sale prices relative to the reservation price. The leader then is able to increase its sale prices and to decrease the probability it places on the regular price. Another extension considers the possibility of both stores having loyal consumers. I find that their presence does not fundamentally change the nature of strategies. The leader always prefers to use a promotion calendar instead of charging a single price. If the follower has more loyal consumers than its rival, it also prefers that the leader uses a promotion calendar. Thus, both firms benefit when promotion calendars are used. This result highlights additional aspects that convenience stores should consider if they plan to invest in developing more flexible pricing strategies. The ability to match weekly fluctuations in supermarket prices would render supermarket promotions ineffective. The supermarket then will abandon promotion calendars in favor of a single low price, which would lead to smaller profits for all firms.",Price leadership with promotions,https://www.sciencedirect.com/science/article/pii/S0167718722000145,3 March 2022,2022,Research Article,65.0
Petrikaitė Vaiva,Vilnius University and CEPR,"Received 19 May 2020, Revised 14 December 2021, Accepted 3 February 2022, Available online 27 February 2022, Version of Record 8 March 2022.",https://doi.org/10.1016/j.ijindorg.2022.102828,Cited by (0),"This paper studies a ==== with horizontally differentiated products and costly sequential consumer search. Prices are posted and search costs must be paid to observe the match values of goods. Consumers choose both in what order to inspect the products and whether to inspect the goods before purchasing. When the expected match value is relatively high, consumers always consider the cheaper product first. However if the expected match value is low, then consumers may check the more expensive product first. The fact that a consumer may buy a product without inspection softens price competition. As a result, the comparative statics of prices and surplus division with respect to search costs is different from that of a sequential search model with observable prices.","Although consumers acknowledge the benefits of search and product comparison when shopping,==== they often skip product inspection before making purchase decisions.==== Such consumer behaviour influences firms’ pricing strategies and the distribution of surplus. This paper shows that the possibility for consumers to skip product inspection before buying changes the comparative statics results of prices and surplus division with respect to search costs compared to the sequential search setting of Choi et al. (2018). This finding is important when taking economic policy decisions related to the transparency in online markets.====I analyse a duopoly with horizontally differentiated products, publicly observable prices and symmetric single-product firms. In this market, a consumer must engage in a costly sequential search process to find the match values of goods. Different from a conventional market setting with sequential search, the consumer can buy a product without observing the match value of the good. In that case, the consumer does not pay any search costs. Thus, the consumer must decide on whether to inspect a product before buying it next to the decisions on the optimal search order and search termination. The optimal search order depends on the difference between prices, the distribution of match values and search costs. Specifically, consumers search the cheaper product first in two cases: (i) the expected match value is high, (ii) the expected match value is low and either the difference between prices is sufficiently big or both prices are high. Meanwhile if the expected match value and both prices are small, then consumers inspect the more expensive product first. The latter deviation from the optimal search order of Weitzman (1979) happens because consumers want to avoid very negative ex post utility.====The expected price varies non-monotonically in search costs. This is related to how one specific price threshold depends on search costs. This threshold is the highest price that a consumer agrees to pay for a product without inspecting it. If product inspection is cheap, then this threshold price is very low, and no firm charges a price less than this threshold in symmetric equilibrium. As a result, consumers never buy goods without inspecting them, and the expected price is decreasing in search costs, for similar reasons as in Haan et al. (2018) and Choi et al. (2018). If search is very costly, then the threshold price is large and firms charge prices less than the threshold price with a positive probability. This probability is increasing in search costs. When a consumer buys a good without inspecting it, then the prominent position in the search order is not so (or even less) appealing. This softens price competition, and the expected price is increasing in search costs.====Consumer surplus depends on how well purchased products meet consumers’ preferences, what prices the buyers pay and how much they spend on search. The effect of search costs on consumer surplus via prices is dominant. The expected paid (minimum) price is quasi-convex in search costs. As a result, consumer surplus is quasi-concave in search costs. How welfare varies with the cost of search depends on the matching quality between consumers and purchased products, and the total paid search costs. The effect of the total search costs is the driving factor. Because the total paid search costs are quasi-concave in the cost per search, welfare is quasi-convex in search costs.",Escaping search when buying,https://www.sciencedirect.com/science/article/pii/S0167718722000042,27 February 2022,2022,Research Article,66.0
"Aguiar Luis,Gagnepain Philippe","Department of Business Administration, University of Zurich, Plattenstrasse 14, Zürich, Switzerland,Paris School of Economics-Université Paris 1, 48 bd Jourdan, Paris, France","Received 6 July 2020, Revised 8 February 2022, Accepted 14 February 2022, Available online 19 February 2022, Version of Record 14 March 2022.",https://doi.org/10.1016/j.ijindorg.2022.102830,Cited by (2),"We attempt to identify and measure knowledge spillovers in the French urban transport sector, which is strongly regulated and where a few large industrial groups are in charge of operating several urban networks. We build and estimate a structural cost model where the service is regulated by a local government and is provided by a single operator. Knowledge spillovers are directly linked to the know-how of a specific group, but they also depend on the incentive power of the regulatory contract which shapes the effort of the local managers. Exerting an effort in a specific network allows a cost reduction in this network, but it also benefits other networks that are members of the same group. We find that diversity of knowledge across operators of the same group improves absorptive capacity and increases the flow of spillovers. Simulation exercises provide evidence of significant reductions in total operating costs following the enlargement of industrial groups.","Knowledge spillovers are usually seen as a process in which firms obtain new knowledge from external sources. They are interpreted as externalities in a competition game where the agents are unable to fully appropriate all benefits from their own R&D activities. Knowledge spillovers are key ingredients of firms’ productivity and economic growth (Jaffe, 1986, Romer, 1990, Grossman, Helpman, 1991). The economic literature has discussed to a large extent issues related to R&D and the production of knowledge spillovers. Important challenges have been the measurement of spillovers between firms, the identification of the factors that influence their generation, whether firms are able to fully take advantage of incoming spillovers and appropriate their proprietary knowledge and whether knowledge externalities are geographically localized (see for instance Bernstein, Nadiri, 1989, Jaffe, Trajtenberg, Henderson, 1993, Audretsch, Feldman, 1996, Cassiman, Veugelers, 2002, Bloom, Schankerman, Van Reenen, 2013).====In this paper, we propose to focus on the issue of the identification of spillovers in the particular case of the regulation of firms with incentive contracts (Laffont and Martimort, 2002). There are two important features in our model: First, the incentive power of each regulatory contract directly shapes the operator’s R&D intensity which is re-interpreted as the cost-reducing effort activity in our setting. Second, spillovers here are measured within the industrial groups that provide public services in several urban areas simultaneously. Thus, spillovers are directly linked to the know-how of a specific group, but they also depend on the decisions taken by a local manager.====We apply our framework to the French urban transport industry, which is particularly well fitted for our purpose. In each urban network of significant size, a local authority regulates and monitors public transport services while a single firm (the operator) is in charge of the operation within a specific regulatory framework. The latter takes the form of a written contract that defines the payment and cost-reimbursement rules between the parties. Fixed-price contracts are implemented in a very large majority of urban areas. Under a fixed-price regime, the operator receives subsidies to cover ex-ante (expected) operating deficits, and is thus provided with powerful incentives to reduce operating costs.====A distinguishing feature of this industry is that about eighty percent of the local operators are owned by three large industrial groups. The transport services provided in different urban networks by operators belonging to the same industrial group are therefore, in essence, provided by the same productive unit. This peculiarity suggests that these operators may benefit from the exchange of information and feedback on experience across many networks operated in different localities with different characteristics. In other words, the economic activity involved in one specific network may affect the economic activity in other networks operated by the same company. In the specific context of the urban transport industry, we expect these spillovers or externalities to take place when a cost-reducing activity developed by one of the operators reaches other productive units of the group. Hence, all networks operated by the same group benefit from the cost-reducing efforts provided by all the members of the group. To reflect the fact that these spillovers could be related to a large array of know-how generated by the operator, ranging from technological to organizational, we will refer to them as knowledge spillovers throughout the text.====In order to be able to exploit incoming spillovers, operators need to work on their absorptive capacity. The latter depends on the ability of the operator to identify the value of new information and assimilate it, which potentially entails basic skills, similar languages, or scientific or technological capabilities. We therefore build and estimate a structural cost regulation model that accounts for the fact that R&D expenditures (the effort of the operator) and absorptive capacity are directly related and allow the production of knowledge spillovers. In each given city, the operator is one of the three large industrial groups present in France or it can be an independent local entity. In both cases, the operations are run by a local manager who decides upon the effort level to be exerted to reduce the operating costs of the local transportation activity. The operator maximizes its own profit and determines the optimal effort level. The latter depends on the local incentives, but it is also affected by all the other effort activities exerted by the other operators of the same group. The econometric task consists then in recovering the parameters of a static model of cost regulation, and testing for the relevance of knowledge spillovers. Our results suggest that the flow of knowledge spillovers across the members of the same group are significant and increase with the size of the group, and they allow transport operators to obtain significant cost reductions. Moreover, operators’ activities that present larger differences in characteristics relative to their group benefit to a larger extent from the efforts provided by other operators of their group. Thus, while a minimum degree of overlap of knowledge across operators is necessary for internal communication, there are also benefits to diversity of knowledge and organizational structures across networks.====Our work shares features with different strands of the empirical literature. On the one hand, our paper contributes to the recent empirical literature on incentive regulatory policies. Gagnepain and Ivaldi (2002 and 2017), and Gagnepain et al. (2013) focus on the same type of data and show that fixed-price contracts lead to a significant decrease of the operating costs of the local operators in France. Our model builds on a similar framework and assumes moreover that the technology of each local operator is not independent of the decisions of the other managers that belong to the same industrial group. Thus, following Holmstrom (1982), we argue here that group incentives matter, although there is no monitoring from the headquarter in our framework. From that perspective, our paper is one of the first to take into account knowledge spillovers in a regulation context.====On the other hand, our paper also relates to the empirical literature on R&D knowledge spillovers with the difference that it focuses on spillovers within operators rather than across operators.==== Most related to our paper is the work of Klette (1996), which uses data on Norwegian manufacturing firms and analyzes the interaction between firm performance and R&D expenditures. The author evaluates R&D at the line-of-business level within each firm and also identifies firms that belong to the same interlocking group of firms, i.e., the set that includes a parent company and all subsidiaries in which the parent company owns a majority share of equity. The paper sheds light on significant spillover effects across different lines of business (e.g. chemicals or metal products) within a firm but also reveals significant spillovers for activities within a line of business that are carried out by different firms within the same group. Szulanski (1996) analyzes firms’ ability to diffuse best practices internally. The paper suggests that the main barriers to internal knowledge transfer are knowledge-related factors such as the recipient’s lack of absorptive capacity. In our framework, we identify the absorptive capacity for each industrial group in our industry and find evidence of diffusion of knowledge spillovers across transport operators linked to the same group. In particular, we are able to construct indices that relate to the structural differences between a given network and the remaining networks from the same group. Another related example is Darr et al. (1995), which analyzes knowledge transfers acquired through learning-by-doing in service organizations. The authors focus on pizza stores owned by different franchisees and find evidence of knowledge transfer across stores owned by the same franchisee but not across stores owned by different franchisees. A related issue highlighted by the literature is the importance of free-riding among the different franchisees of a given chain (see for example Brickley, 1999, and Lafontaine, Slade, 1997, Lafontaine, Slade, 2007). A franchisee has incentives to free-ride on the tradename of the franchisor given that her effort is private while the benefits will accrue to all the members of the chain. This closely relates to our model where each local operator privately pays the cost of its effort which will be beneficial to all members of the same industrial group.====The organization of the paper is as follows: Section 2 describes the regulation of urban transportation in France in more detail and discusses the assumptions that are maintained throughout the paper. Section 3 presents our model of cost regulation which encompasses the main features of urban transportation and the environment in which operators make their decisions. Section 4 then develops a formal specification of the cost function to be estimated. Section 5 is devoted to the construction of the variables and the presentation of our results. Section 6 evaluates the cost gains of adding operators to a group. Section 7 provides a summary and some concluding remarks.","Absorptive capacity, knowledge spillovers and incentive contracts",https://www.sciencedirect.com/science/article/pii/S0167718722000066,19 February 2022,2022,Research Article,67.0
Marcoux Mathieu,"Département de sciences économiques, Université de Montréal, CIREQ and CIRANO, C.P. 6128, succ. Centre-Ville, Montréal, QC, H3C 3J7, Canada","Received 21 July 2020, Revised 4 February 2022, Accepted 7 February 2022, Available online 16 February 2022, Version of Record 25 February 2022.",https://doi.org/10.1016/j.ijindorg.2022.102829,Cited by (1),I estimate a game of mobile network investment between national incumbents and a new entrant to shed light on the limited success of competition enhancing policies in Canada. I recover player-specific unobserved heterogeneity from bids for spectrum licences to address the unavailability of regressors required to identify incumbents’ responses to the new entrant’s decisions. I find that incumbents benefitting from important economies of density is a plausible explanation for policies’ limitations. I then evaluate the equilibrium effect of subsidizing the new entrant’s transceivers and find that this counterfactual policy increases its investment while only slightly modifying incumbents’.,"The state of competition in mobile telecommunications has been open to debate in many developed countries. On the one hand, considerable investment required to operate a mobile network may discourage new firms from competing against incumbents. On the other hand, industry regulators often advocate intensified competition to avoid having to intervene and prevent incumbents from exercising market power. Unfortunately, competition enhancing policies often fail to sustainably increase the number of providers operating in the industry. In the past, there has been several examples of smaller mobile service providers merging together or being bought by larger providers.==== A recent high profile example of reduction in the number of players is the merger between T-Mobile and Sprint, respectively the third and fourth largest mobile providers in the United States. Naturally, competition-related issues tend to be more concerning in less densely populated areas which typically attract fewer operators and less investment. Strategic interactions between operators are especially relevant in this setting due to concerns that encouraging investment from new entrants may discourage investment made by incumbents.====In this article, I shed light on limitations of the Canadian government’s attempts to increase competition in an industry dominated by three national incumbents. In Canada, as in many other countries, spectrum licences required to develop a mobile network are allocated through auctions. In hopes to encourage new firms entering the industry, the government prohibits national incumbents from bidding on a fraction of licences set aside for small and regional players. During the 2008 Advanced Wireless Services 1 (AWS1-2008) auction, a handful of new providers took advantage of this set-aside policy and built a mobile network. However, some of these firms failed to use the spectrum they were allocated and eventually resold it to other providers, including incumbents. Furthermore, some of these providers went out of business and were bought by incumbents.====In order to understand these limitations and to evaluate the potential of alternative policies, I estimate an empirical game in which providers, more precisely three incumbents and a new entrant, decide how many transceivers (i.e., cell antennae) to install in different geographical markets. Focusing on relatively less densely populated markets, I find that there are important economies of density in transceivers’ locations. Because it is building a brand new network, the new entrant cannot benefit from such economies of density to the same extent as incumbents. This asymmetry may explain the limitations mentioned above and motivates the study of two counterfactual policies. First, I quantify the role stocks of transceivers play on the operators’ probabilities of investing in their network by comparing the estimated equilibrium with the counterfactual where all operators have stocks equal to zero. This drastic change in the economic environment reduces the number of operators investing in their network with non-negligible probabilities to one or two firms. Interestingly, the new entrant is one of these remaining firms, at least in some parts of the country. This result suggests that an effort to somewhat compensate the new entrant’s lack of economies of density may substantially help it to install more transceivers. In a second counterfactual experiment, I evaluate the equilibrium effects of subsidizing the new entrant’s first few transceivers. My results suggest that this policy encourages the new provider to install more transceivers, while generating only minor distortions in incumbents’ decisions. In some cases, I even find incumbents to be slightly more likely to install transceivers in response to the increase in the new entrant’s probability of entering the market.====An important challenge in the current empirical application is the difficulty to identify the effect of the new entrant’s decisions on incumbents’ payoffs. Being able to convincingly quantify such strategic interactions in network investment is key to properly assess the equilibrium effects of any counterfactual policies designed to encourage the new entrant’s investment. Intuitively, one may hope to recover the effect of strategic interactions on players’ payoffs if there exists an exogenous source of variation that does not affect their own payoffs, but shifts their competitors’ decisions. In the same spirit as in other econometric models of simultaneous equations, many identification arguments available in the literature exploit exogenous variation in some player-specific observable regressors that are excluded from competitors’ payoffs (e.g., Pesendorfer, Schmidt-Dengler, 2003, Pesendorfer, Schmidt-Dengler, 2003, Bajari, Hong, Krainer, Nekipelov, 2010, Bajari, Hong, Krainer, Nekipelov, 2010). In the current setting, variation in stocks of transceivers previously accumulated would be a suitable candidate. Unfortunately, such variation does not apply to the new entrant because, by definition, its stocks of transceivers are exactly zero across the country, therefore jeopardizing the identification of incumbents’ responses to the new entrant’s decisions.====Part of the current article’s contribution is to illustrate how recovering player-specific unobserved heterogeneity from predetermined outcomes helps in identifying strategic interactions between competitors. Bids for spectrum licences are used to recover unobservables which provide variation required for the identification of the incumbents’ response to the new entrant’s decisions. The intuition for this identification argument is as follows. Some information known to all players potentially affects network investment, but is unobservable to the econometrician. For examples, providers may have better knowledge about the population’s demand for mobile services in a given market or specific constraints related to the installation of transceivers in a given location. Moreover, information related to agreements between providers (e.g., network sharing, tower sharing, domestic roaming, etc.), transceivers’ technology and spectrum compatibility is not observed by the econometrician. Given that these unobservables affect providers’ payoffs in the mobile network investment game, they should also impact providers’ bids for licences beyond observable information. In short, player-specific unobserved heterogeneity across markets can explain why a player would bid differently on two licences covering separate markets that are similar according to their observable characteristics. The unobservables recovered from bids for licences should therefore capture some player-specific unobserved heterogeneity relevant for network investment. In my estimation results, I find that unobservables recovered from bids have a significant impact on the new entrant’s payoffs and that including the recovered unobservables improves the precision of strategic interaction estimates. I also find that the recovered unobservables are capturing variation in one of the incumbents’ payoffs that is consistent with known network sharing agreements.====The analysis focuses on operators’ decisions to install transceivers in relatively small and less densely populated markets. The justification for restricting one’s attention to these units of analysis is twofold. First, mobile network operators rarely all install transceivers in these markets, preferring to provide coverage in some areas through agreements with competitors. It follows that smaller markets are a more interesting laboratory to study strategic interactions in network investment. Second, by focusing on smaller markets and controlling for some characteristics of surrounding larger markets that are dropped from the analysis, one bypasses the need to model potential spillovers. In other words, in this context, the independent isolated markets assumption often maintained in the empirical games literature is more likely to be satisfied.====It is worth mentioning that competition in mobile telecommunications is a topic that has drawn considerable attention in economics.==== Several articles have studied Federal Communications Commission auctions in the United States focusing, among other topics, on collusion (e.g., Cramton, Schwartz, 2000, Cramton, Schwartz, 2000, Cramton, Schwartz, 2002) and the efficiency of the system in place (e.g., Fox, Bajari, 2013, Xiao and Yuan). Effects of changes in market structure on industry outcomes have also been investigated. For instance, Bajari et al. (2008) estimate consumers’ valuation of larger coverage areas resulting from firms consolidation. Seim and Viard (2011) examine how new entries affect retail pricing and product availability.====These articles are far from being an exhaustive list of all empirical work done on competition in mobile telecommunications. However, to the best of my knowledge, despite the fact that transceivers’ locations are important determinants of competition in this industry, very few papers explicitly model strategic behaviour in such decisions through a game of market entry as the current effort does. Some recent contributions must be mentioned. Lin et al. (2022) use a game of market entry to study the deployment of 4G-LTE by the four main network operators in the United States and simulate a merger between two of these firms. They also consider the problem of unobserved heterogeneity and they propose a control function approach to deal with the endogeneity problem generated by these unobservables. Granja (2021) proposes a dynamic game of entry and technology upgrade to study how coverage requirements affected the roll-out of 3G in Brazil. Such coverage requirements do accelerate the introduction of technology, but they considerably reduce network operators’ profits. His counterfactual analysis suggests different subsidies that deliver higher aggregate profits without slowing down roll-out.====It should be noted that data on transceivers have been used in other empirical studies. In particular, Sun (2016) uses information on transceivers’ locations in Connecticut to assess the effect of signal quality on providers’ market shares. Transceivers’ data are also used to control for product quality in Bourreau et al. (2021)’s study of tacit collusion in France. Björkegren (2019) uses data on towers’ locations in Rwanda to measure network coverage and its effect on the adoption of mobile telephones. Elliott et al. (2021) combine an economic model of competition in prices and infrastructure investment with an engineering model. In their model, demand and network quality are determined simultaneously. Information on the location of transceivers is used to assess network quality and infrastructure costs.====This paper also contributes to the empirical literature studying the effect of market structure on operators’ investment in their mobile network. Using a panel of 33 OECD countries, Genakos et al. (2018) find that mergers between network operators tend to increase investment per firm, without introducing significant changes in the industry’s total investment. In front of these results, the authors encourage further research based on specific case studies. In one of his counterfactual experiments, Björkegren, Forthcoming shows that introducing an additional mobile network operator in Rwanda could have increased the incumbent’s incentive to invest in rural areas. In that case, more competition seems to trigger more investment. Once again, while this list is not exhaustive, it is worth emphasizing the importance of quantifying strategic interactions to better understand the effect of market structure on operators’ investment, as it is done in the current paper.====Finally, the current article contributes to a recent and growing literature which advocates allowing for a flexible information structure when estimating empirical games. While many models can be classified as games of either complete or incomplete information,==== recent empirical applications have shown that rigid assumptions on players’ information could lead to significantly different results (see applications in Grieco, 2014, Magnolfi, Roncoroni, 2019). I argue that, in the current empirical application, both common-knowledge (complete information) and private (incomplete information) unobservables should be taken into account. On one hand, because some of the players are established incumbents operating in the Canadian mobile telecommunications since the1990====s, providers may know more about each others’ payoffs than I can account for using observable variables. On the other hand, since I am studying the entry of a new provider in this industry, the model should allow for some part of competitors’ payoffs being privately observed.====Grieco (2014) and Aguirregabiria and Mira (2019) allow for both private and common-knowledge unobservable information as well as multiple equilibria being realized in the data. Magnolfi and Roncoroni (2019) consider an alternative equilibrium concept which also allows for a flexible information structure and multiple equilibria. In all these articles, point identification results require observing player-specific regressors and, therefore, do not directly apply here. To the best of my knowledge, the current article is the first to propose exploiting the identifying power associated with variation in player-specific unobservable heterogeneity in the estimation of an empirical game.====The rest of the paper proceeds as follows. Section 2 briefly describes the role of transceivers and relevant features of the Canadian mobile telecommunications industry. In Section 3, I present a model of mobile network investment. Transceivers’ locations are studied via an empirical game in which three incumbent network operators and a new entrant decide how many transceivers they want to install in several isolated geographical markets. I also propose a simple reduced-form model of operators’ bids for spectrum licences required to install transceivers. This reduced-form model is used to define residuals which are treated as common-knowledge unobserved heterogeneity in the game of transceivers’ locations. The data are described in Section 4. Section 5 discusses the identification issue faced in the empirical analysis, i.e. the absence of player-specific regressors for the new entrant, and the proposed solution leveraging variation in common-knowledge unobserved heterogeneity. Section 6 reports estimation results highlighting important economies of density in transceivers’ location decisions. Two counterfactual experiments are in Section 7: the equilibrium effects of shutting down incumbents’ stocks of transceivers and subsidizing the first few transceivers installed by the new entrant. Section 8 concludes.",Strategic interactions in mobile network investment with a new entrant and unobserved heterogeneity,https://www.sciencedirect.com/science/article/pii/S0167718722000054,16 February 2022,2022,Research Article,68.0
"Creane Anthony,Jeitschko Thomas D.,Sim Kyoungbo","Department of Economics, University of Kentucky, USA,Department of Economics, Michigan State University, USA,Department of Industry and Market Policies, Korea Development Institute, South Korea","Received 4 October 2020, Revised 24 January 2022, Accepted 30 January 2022, Available online 5 February 2022, Version of Record 14 February 2022.",https://doi.org/10.1016/j.ijindorg.2022.102826,Cited by (0),"Asymmetric information is a classic example of market failure that undermines the efficiency associated with perfectly competitive market outcomes, as goods or services are not always allocated to those who value them the most. Credible certification that substantiates unobservable characteristics of products that consumers value is a potential solution to such market failure. We examine the welfare effects of certification in markets in which asymmetric information induces a misallocation of goods, and compare the market equilibrium when the certification technology becomes available with the equilibrium without certification. We find that despite certification improving allocative efficiency, overall welfare may decrease when such certification is either only imperfectly accurate or costly to the firm (but not necessarily to society). Most of these findings are tied to the subtle interplay of consumer and producer decisions of self-selecting across two markets: certified and non-certified markets, as the self-selection has welfare implications in both markets.","Asymmetric information is a classic example of market failure that undermines the efficiency associated with perfectly competitive market outcomes—the “lemons” market (Akerlof, 1970). This issue has recently regained more detailed scrutiny with the rise of products labeled as “organic,” “fair trade,” “local,” or “sustainable,” etc. In markets for such products, the concept of product quality in consumer preferences has extended to include the process of production and distribution. When such properties are claimed by firms, along with specific corporate philosophies or policies (e.g., the issuance of green bonds to finance production, or placing a firm’s reserves or pension funds in low-carbon investments), it is very hard or impossible for consumers to ascertain the desired attribute on their own. As a result, without a means of conveying the relevant information to consumers, there is a potential for an allocative inefficiency in these markets: the higher quality goods are not always allocated to those who have greater value for higher quality.====Due to the aforementioned credence good nature,==== quality assurance mechanisms, such as offering warranties (e.g., see Spence, 1977) and building up a reputation (e.g., see Klein and Leffler, 1981; and Shapiro, 1983), are not expected to function well to address the information problem in those markets.==== Accordingly, credible (third-party) certification is often considered the only potential solution for overcoming informational asymmetries. Also, while “quality” in the traditional, lemon’s sense may be hard to measure due to the subjective nature of tastes, properties of the production process or firm policies, as in the examples above, are more objective. Although the degree to which such claims are backed up by formal certification or regulated by law varies, numerous certification schemes are in use in many marketplaces.====Considering the popularity of certification and its apparent attractiveness in overcoming the problem of information asymmetries, it is worth formally studying how the introduction and use of certification affects social welfare. To this end, we consider markets in which goods have unobservable characteristics that consumers value and analyze the market equilibrium when there is a technology available that credibly verifies the relevant attributes of products. One effect the certification technology has is to potentially create two markets (market segmentation)—one with certified products and the other without—while without certification there is only one pooled market. As a result, one has to consider both the firm’s decision as to whether to seek certification, as well as the consumer’s choice as to which market to patronize.====While it is readily apparent that costless and perfectly accurate certification completely overcomes the information asymmetries and therefore increases social welfare, our main result is that if certification is not free to producers or if it is not flawless, then the introduction of certification may reduce social welfare. In particular, if producers incur even a small cost in the certification process (as a fixed cost to the seller of the good, but not necessarily as a cost to society====) even when the certification technology is perfectly accurate—and can therefore resolve informational asymmetries in the market—overall welfare may decrease due to the possibility of certification. Similarly, even when the certification technology is costless and free to producers, welfare may decrease when such certification is possibly prone to error, i.e., is imperfect—either due to technical limitations, or due to misleading claims, as is the case with “greenwashing” or creation of a “green sheen.”====It is true that the certification process admits better information and therefore increases efficiency by reallocating high quality goods to consumers with relatively higher valuations. However, there is a subtle interplay of consumer and producer decisions of self-selecting across two markets—the certified and the non-certified market—that has welfare implications across both markets. With average quality, prices, and quantities traded in both markets determining total welfare, this interplay can result in a net loss of total welfare. For illustrative purposes, consider a setting in which some high quality producers can become certified. In so doing, the market is segmented into a certified and a non-certified market. Welfare generated in the former is a gain to society, as consumers who value quality particularly highly move into that market and have their demand met by high quality producers. However, there are three effects that adversely affect welfare in the non-certified market. First, as these firms’ superior quality is certified and they exit the non-certified market, supply in the non-certified market is reduced (shifts left). Second, as high-value consumers exit the non-certified market to patronize the market for certified goods, demand decreases (shifts left) in the non-certified market. Third, due to exit of high quality from the non-certified market, average quality in the non-certified market is reduced, which further reduces (shifts down) demand in the market. So clearly there is a welfare loss in the non-certified market. Indeed the market for non-certified goods may collapse completely (the classic lemon’s problem). When certification is perfect and costless, the first-best allocation emerges, but when there is even a small cost, or a small imperfection in the certification technology, then neither market is assured to attain the first best and the interplay between the two markets determine total welfare. That is, even if certification is either costless to society or perfect (but not both), certification can reduce welfare.====To further illustrate the subtlety of this interplay across markets, suppose that there is a segmented market, but that the certification technology can make both small Type-I errors and small Type-II errors (i.e., wrongfully deny certification to high quality, or wrongly certify a low quality good). Improving information by reducing the number of Type-II errors (false positives) implicates welfare in both markets: Increasing average quality in the certified market due to weeding out of low quality in that market leads to an increased willingness to pay by existing consumers and leads to more high-value consumers self-selecting into that market, both of which increases demand; but it also decreases supply in the certified goods market as the erroneously certified goods are weeded out. Simultaneously, it increases supply in the non-certified market, while also reducing average quality there—which also reduces demand, both of which put downward pressure on prices which may drive high quality producers to shut down. This can result in welfare losses in the non-certified market that exceed the gains in the certified market (Example 2 in the paper), so despite having better information in the market when eliminating Type-II errors, overall welfare can decrease.====To make these interactions clearer, we begin by initially assuming that certification may come at a fee and that there are Type-I errors (a failure to appropriately certify good quality), but there are no Type-II errors—that is, poor quality is never mis-certified as high quality. For this case, we show combinations of certification costs and imperfections in the certification technology that lead to a reduction in welfare compared to a market without any certification technology. We further demonstrate that this finding holds even if certification is costless (but imperfect) or perfect (but entails a cost to firms, albeit not necessarily a cost to society).====We show that the findings concerning welfare effects are robust and are reinforced when extending the model to allow for Type-II errors. While it might not be surprising that the addition of the Type-II error reinforces results tied to welfare reductions, we also demonstrate the possibility that welfare can be increased with the introduction of Type-II errors, relative to the case where there are only Type-I errors. (That is, the admission of better information by reducing or eliminating the Type-II error decreases welfare.) In particular, low quality being mis-certified leads to a shifting of low quality into the market for certified goods. This increases supply in that market, and also lowers demand of existing consumers as average quality deteriorates—both of these effects put downward pressure on the price of certified goods. However, average quality rises in the non-certified market due to the exit of some (mis-labeled) low quality from the non-certified into the certified market. This leads to a reduction of supply in the non-certified market, while also increasing the demand of those patronizing that market—both of which lead to upward pricing pressure on the price of non-certified goods. In addition to these primary effects, which all have welfare implications, the net welfare effect is determined by the new equilibrium as consumers self-select which market to patronize upon the shifting of average quality, prices and quantities across the two segmented markets. All of these shifts have inframarginal welfare effects of varying magnitudes and signs, so that the net effect can be either positive or negative.====Our analysis contributes to the literature on certification (or labeling), or more broadly, quality disclosure. There are two strands of the certification literature. The first strand focuses on the role of strategic certification intermediaries. Some studies aim to investigate how competition between certifiers affects market outcomes including optimal pricing schemes, the amount of information transmitted to consumers or quality provision (e.g., Albano, Lizzeri, 2001, Fischer, Lyon, 2014, Hvide, 2009; and Lizzeri, 1999). Some other papers (e.g., Benabou and Laroque, 1992) explore whether a reputation concern can mitigate the incentive problems of certification intermediaries; and Mahenc (2017) explores the honesty of certifiers. Some of these papers have something in common with ours in that they show a potential source of inefficiency associated with certification. However, while they emphasize the role of strategic certifiers as a source of inefficiency, we are more interested in sellers’ incentives for getting certified and their incidence on markets, taking certification environments as given.====The other strand of this literature focuses on a seller’s incentive for quality disclosure. Since the “unraveling result”==== was presented by Grossman (1981) and Milgrom (1981), many subsequent studies have explored the subject of when and why unraveling fails to hold (e.g., see De, Nabar, 1991, Guo, Zhao, 2009, Grubb, 2011, Hagenbach, Koessler, Perez-Richet, 2014, Hotz, Xiao, 2013, Shavell, 1994; and Viscusi, 1978).==== Closely related to such work, several authors also studied whether mandatory disclosure laws enhance or hinder efficiency, when voluntary disclosure fails to occur (e.g., see Bar-Isaac, Caruana, Cuñat, 2012, Board, 2009, Gavazza, Lizzeri, 2007, Harbaugh, Maxwell, Roussillon, 2011, Hermalin, Katz, 2006;==== Jovanovic, 1982; and Matthews and Postlewaite, 1985). Though these papers share several features with ours, the most important difference in ours is that we compare the situations with and without certification, rather than comparing a situation in which sellers voluntarily choose to get certified with another situation in which sellers must get certified.====Several studies focus on the adverse welfare effects of certification.==== The authors of these studies find that welfare may decrease with the availability of certification mostly in the context of Eco-labeling, as we do in this paper. Among these studies, Baltzer (2012), Bonroy and Constantatos (2008) and Zago and Pick (2004) differ from ours in that they examine other underlying mechanisms that lead to diminished welfare compared to the one we study, namely that certification can exacerbate an adverse selection problem.====The most closely related papers to ours are Mason and Sterbenz (1994) and Mason (2011). The former shows that forced testing can be welfare-reducing, while the latter shows that even without forced testing, certification can be welfare-reducing due to the cost of certification. This is in contrast to our finding, where welfare losses stem from the equilibrium interaction between consumer and firm decisions across different markets, potentially exacerbating allocative inefficiencies and adverse selection. To put it differently, if certification costs were not a welfare cost (e.g., they are just a transfer between the firm and the certifier), then the welfare loss disappears in Mason (2011), whereas it remains in ours.====An additional finding in Mason (2011) is that increased cost of certification can increase welfare, provided that in equilibrium all high quality firms seek certification, but the increase in costs dissuades some low quality firms from pursuing certification (in the hopes of a Type-II error occurring), that is, a decrease in Type-II errors is necessarily welfare enhancing in Mason (2011). In contrast, we show that a reduction in Type-II errors can actually reduce welfare, as it pushes low quality into the non-certified market which causes a welfare loss in the non-certified market that may exceed the welfare gain obtained in the certified market through higher average quality (through the same mechanism that creates the previous welfare loss). Indeed, our examination of downward sloping demand and of the interconnectedness of certified and non-certified markets and how these tie into firm and consumer self-selection decisions is unique.====Section 2 contains the base model, and we compare two benchmarks: the full information and no information cases. We derive a certification equilibrium and conduct a welfare analysis in Sections 3 and 4 for the base model. Section 5 extends the analysis to include Type-II errors and demonstrates the robustness of the results, but also establishes further cases where worse information can lead to higher welfare. Finally, Section 6 concludes. All omitted proofs are in the appendix, where we also include further results related to the main analysis.",Welfare effects of product certification under latent adverse selection,https://www.sciencedirect.com/science/article/pii/S0167718722000029,5 February 2022,2022,Research Article,69.0
"Alcalde Pilar,Vial Bernardita","Universidad de los Andes, Facultad de Ciencias Económicas y Empresariales, Av. San Carlos de Apoquindo 2200, Las Condes, Santiago, Chile,Pontificia Universidad Católica de Chile, Instituto de Economía, Vicuña Mackenna 4860, Macul, Santiago, Chile","Received 31 January 2020, Revised 17 January 2022, Accepted 2 February 2022, Available online 4 February 2022, Version of Record 11 March 2022.",https://doi.org/10.1016/j.ijindorg.2022.102827,Cited by (0),"An annuity is an insurance policy designed to protect the annuitant from longevity risk. However, retirees are willing to reduce annuity payments by choosing guaranteed periods or firms with better risk ratings, suggesting they are also concerned with the provider’s default risk and bequeathing their heirs if they die early. We use a hedonic price model to estimate the demand for retirement products and the expected cost of providing annuities for companies. Then we analyze market outcomes under counterfactual scenarios that sequentially restrict consumers’ choice sets, forcing retirees to select quotes that maximize payments. We show that such restrictions could affect the pension system in other dimensions beyond annuity payouts—linking to a broader literature on the unintended consequences of regulating tariffs. We find that firms have low margins and slightly increase their payouts under restricted scenarios. The annuity share decreases, and replacement rates increase, but the restrictions lower consumer utility. They also generate a substantial deterioration in the rating distribution of selected firms.","The design and performance of pension systems have increasingly become central concerns in public policy debates worldwide. The pension systems’ performance is usually evaluated using replacement rates– the ratio of the pension to pre-retirement earnings. Among all retirement products, one of the most relevant is lifetime annuities, an insurance policy designed to protect the annuitant from longevity risk. However, the evidence suggests that annuitants also consider other risks involved, thus focusing on monthly payments and replacement rates provides an incomplete assessment of the benefits received upon retirement. Two distinct patterns emerge at retirees’ choices over ten years in Chile. First, a significant proportion of retirees select firms with better risk ratings, which offer lower payouts, suggesting a concern about the provider’s default risk. Second, most retirees are willing to reduce their payouts by contracting guaranteed periods, protecting their heirs in case of early death. The share of retirees willing to reduce monthly payments to favor other product attributes varies among retirees who have different characteristics and use different intermediaries. Those patterns, however, can hardly be explained by supply-side behavior: we show that prices do not vary by intermediary type on average, and the differences in prices indicate that retirees choose these attributes even though they are more expensive.====We use a hedonic price model to describe customers’ choices and estimate the demand for retirement products and their attributes with individual-level data from the Chilean pension market. We then use these results to estimate the expected cost of providing annuities for insurance firms, using the firm strategies. Many authors claim that bequest motives and life-expectancy heterogeneity play a central role in explaining the ==== (see, for instance, Bernheim, 1991, Lockwood, 2012, Finkelstein, Poterba, 2004 among others). Our results show that these same factors play a role in explaining the demand for annuity attributes for those individuals who decide to annuitize some of their wealth.====Most retirees in the dataset select an annuity with a guaranteed period. While an annuity smoothes the annuitant’s consumption while she is alive, protecting her from longevity risk, a guarantee smoothes the consumption of her beneficiaries within a specified period, protecting them from the risk of her early death. In this sense, a guarantee is a particular way of bequeathing, different from the usual noncontingent inheritance. This attribute increases the firm’s cost and reduces the pension payment for the customer, but retirees still value them. Willingness to pay for guarantees is significantly lower for customers with higher survival probabilities: younger annuitants, women, and annuitants with large savings funds, who also represent a higher cost to the firm.==== Survivor benefits play a similar role to guarantees, but they pay only a fraction of the annuity payout and only for legal beneficiaries. Even though survivor benefits are mandated by law, we observe some variation in the data that suggests that guaranteed periods and survivor benefits are indeed substitutes. Husbands were not considered legal beneficiaries for female married annuitants until 2008. We find that willingness to pay for guaranteed periods decreased when this survivor benefit increased.====Additionally, many retirees select a provider that does not offer the highest monthly payment. Our analysis assumes that consumers’ choices reveal their preferences for attributes, including the firm’s default probability as the critical determinant of quality in this market.==== If the rating measures insolvency risk,==== even risk-neutral customers would be willing to pay a premium for better-rated firms: a lower default probability increases the expected value of the annuity.==== We find that better-rated firms have higher expected costs than firms with worse ratings, but they also can charge higher prices, and consumers buy their products more frequently even though they are more expensive. These firms also enjoy a cost advantage in the long run: expected costs increase with guarantees, but proportionately less for firms with better ratings. We also find that willingness to pay for better ratings increases with retirement age and life expectancy: it is significantly higher for women and annuitants with large savings funds.====To analyze how the possibility of substituting monthly payments for other costly attributes affects the outcomes of the retirement market, we perform three related counterfactual exercises. We either restrict product attributes by only allowing quotes with no guarantee, permit only the quote with the largest payout within each category, or both. The counterfactual simulations show that firms that use sales agents or have good ratings increase quoted payments slightly on average, while those without sales agents or good ratings tend to decrease them. The annuity share decreases slightly with each additional restriction on the demand side, and accepted payouts increase 2.8% on average. However, the restrictions would lower consumer utility. They would also generate a substantial deterioration in the rating distribution of selected firms. We predict the market share of the highest-rated firms (AA+) would drop from 26.2% to 16.4%, while the market share of the lowest-rated companies (A+ or lower) would increase from 6.2% to 15.7%, mainly because better-rated firms have higher costs on average.====The previous discussion suggests that forcing retirees to select the quotes that maximize monthly payments could affect the pension system in other dimensions beyond replacement rates. On the one hand, restricting the choice of product attributes reduces the demand for annuities: some customers may be willing to purchase annuities only if those attributes are available. On the other hand, the existence of a premium for better-rated firms may provide incentives for firms to make cautious investment decisions in the expectation of creating a reputation for solvency and increasing prices in the future. Decreasing the premium may block the reputation mechanism, inducing firms to change their risk-taking and pricing strategies.==== These results are related to a broader literature that studies the unintended consequences of regulating tariffs on other variables valued by customers (see Genakos and Valletti, 2011 for instance).====Another feature of the market worth analyzing is the role of intermediaries. Individuals may choose an intermediary —either a sales agent from an insurance company or an independent adviser— before requesting quotes; this choice is unknown to firms. Using an intermediary is costly, and it may indicate a different degree of financial and tax literacy or a different attitude towards financial and longevity risks. On the other hand, selecting a sales agent from an insurer may indicate a higher preference for that brand. Finally, intermediaries’ advice could also affect the valuation of attributes. Intermediaries’ incentives and the role of their advice on consumer choices have been studied in many matching markets, including health and financial markets and especially in the insurance and mortgage industries (see Inderst and Ottaviani, 2012a for a good survey), the residential market (see Bar-Isaac and Gavazza, 2015), as well as the retirement market (see Mitchell, Smetters, 2013, Bhattacharya, Illanes, Padi, 2020, Alcalde, Vial, 2020).====We find that retirees using sales agents show a higher willingness to pay for a better risk rating and a lower responsiveness to price differentials. Retirees without intermediaries show less preference for annuities and a lower willingness to pay for guarantees. Both results may be explained by a composition effect and the influence of intermediaries’ advice. Recommendations from sales agents probably highlight the attributes of their company, and both types of intermediaries are likely to emphasize attributes that make annuities more attractive to retirees, as they get higher fees when the retiree chooses an annuity.==== The results of the counterfactuals simulations are in line with this hypothesis: when guarantees are not available, the annuity share significantly decreases for customers using intermediaries. In addition, we show as an extension that banning the use of sales agents would produce a substantial reduction in the annuity market share in the short term, as the fraction of retirees that enter directly and select a programmed withdrawal would increase sharply.====Illanes and Padi (2019) shows that the Chilean Social Security system provides a counterexample for the annuity puzzle widely discussed in the literature. Our results for the counterfactual simulations suggest that allowing annuity policies to include some form of bequeathing —through guaranteed periods, for example— helps to increase the demand for annuities, and intermediaries play a role in promoting those attributes and increasing the annuitization rate.====The paper is organized as follows. Section 2 introduces the main characteristics of the annuity market in Chile and the data available. Section 3 develops the demand and supply models and describes its specification, while Section 4 shows the estimation results of the estimation and analyzes the main patterns found in the data. Section 5 reviews how the possibility of substituting monthly payouts for other annuity attributes affects replacement rates. Section 6 contains the conclusions.","Implicit trade‐offs in replacement rates: Consumer preferences for firms, intermediaries and annuity attributes",https://www.sciencedirect.com/science/article/pii/S0167718722000030,4 February 2022,2022,Research Article,70.0
Doi Naoshi,"Otaru University of Commerce, 3-5-21 Midori, Otaru, Hokkaido, 047-8501, Japan","Received 23 December 2020, Revised 8 January 2022, Accepted 12 January 2022, Available online 24 January 2022, Version of Record 5 February 2022.",https://doi.org/10.1016/j.ijindorg.2022.102825,Cited by (2)," sales for each consumer type, though such data do not have to be observed at the product-level. This data requirement implies that the method mainly captures observed heterogeneity.","Random coefficients logit (RCL) models are widely used in demand estimation because they allow a flexible substitution pattern among products with a tractable number of parameters to be estimated. Berry et al. (1995) (hereafter BLP) established a method to estimate RCL models with aggregate data, that is, product-level (not consumer-level) data. The BLP method and its modified versions are widely applied.====In general, estimation of RCL models with aggregate data is not an easy task. The BLP method estimates parameters using a generalized method of moments (GMM) estimator. The moment conditions are formed by interacting a structural error term, which is typically interpreted as the utility from unobserved product characteristics, with a set of instruments. Obtaining values of the error term for each observation, which are necessary to calculate the sample counterpart of the moment conditions, consumes considerable time and effort. In the BLP method, the values are numerically solved for to equate the model prediction of market shares to the data using a contraction mapping approach. This numerical process is required each time the GMM objective function is evaluated. That is, the “outer loop” (searching for the parameter values that minimize the GMM objective function) contains the “inner loop” (searching for the error term values that equate the market shares predicted by the model and those in the data). This nested structure of loops results in a long calculation time.====This paper proposes a new method for estimating RCL models without the inner loop. In the method, values of the structural error term are analytically obtained without the contraction mapping. The estimation thus becomes a simple GMM without the inner loop and requires a much shorter computation time than the BLP method.====The proposed method is applicable when (a) consumers are assumed to be segmented into a finite number of types (i.e., random coefficients are drawn from a discrete distribution) and (b) data on sales aggregated by consumer type are available. This data requirement implies that the method mainly captures observed heterogeneity, which is based on, for example, demographics and consumer categories. For example, in the airline industry, some studies report that a demand model that assumes two distinct types of passengers fits the data well (e.g., Escobari and Hernandez, 2018). The two types are often interpreted as business and leisure passengers. The method proposed in this paper can be employed when one estimates an RCL model that allows the two types of passengers to have different tastes (i.e., coefficients) for product attributes. The data needed for the method are, in addition to the data commonly used for the estimation of RCL models (i.e., prices, shares, and instruments for prices), the ==== number of business passengers and that of leisure passengers. Note that data on the number of passengers of each type are not required to be observed at the product-level; data aggregated across products are sufficient.====Situations satisfying (a) and (b) are not rare. Condition (a) is the assumption of “discrete-type” random coefficients, that is, a vector of coefficients representing a consumer’s taste takes one of a finite number (e.g., two) of possible values.==== Discrete-type RCL models are a parsimonious way to capture the correlation of tastes for different product attributes, as discussed in Berry and Jia (2010), and are widely used to approximate demand structures of various markets, including airlines (Berry, Carnall, Spiller, 1996, Berry, Jia, 2010, Ciliberto, Williams, 2014), ketchup (Besanko et al., 2003), video games (Nair, 2007), prescription drugs (Iizuka, 2007), carbonated soft drinks (Eizenberg and Salvo, 2015), and retail gasoline (MacKay and Remer, 2019).====Furthermore, the additional data requirement of condition (b) is not demanding. What is required is the ==== sales for each consumer type; the data do not have to be observed at the product-level. Although the usual product-level sales data employed in estimations of RCL models do not necessarily contain information on the types of purchasers, another source may provide the information on the ratio of each consumer type among ==== sales. In some cases, such information can be obtained from public surveys. For example, the Consumer Expenditure Survey of the United States provides information on the demographics (e.g., income level) of purchasers of new automobiles (Petrin, 2002). If a discrete-type RCL model of automobile demand defines consumer types based on the demographics, then the survey data are sufficient to apply the proposed method. This paper provides an illustrative application with a dataset for the Japanese airline industry.====If total sales by consumer type are available, then there is another option. Adding moment conditions involving types of purchasers is expected to increase the efficiency of the estimation, as Petrin (2002) demonstrates for the estimation using the BLP contraction mapping.==== Additional information on purchasers’ characteristics is therefore worth obtaining, even when the BLP method is employed. The contribution of this study is that it shows that if such data are available, then the estimation procedure can become much easier and less time-consuming than the BLP method.====A limitation of the proposed method is that consumer types should be according to observed heterogeneity (e.g., business versus leisure travelers in the airline example; income classes in the automobile example) because the method requires type-level data. The literature has stressed that even after observed heterogeneity is controlled for, unobserved heterogeneity may still play a substantial role in reproducing substitution patterns found in the data (Berry et al., 2004a). This paper provides Monte Carlo exercises to demonstrate how much bias is introduced by using the discrete-type approach when the true model has unobserved heterogeneity.====Extension to random coefficients nested logit (RCNL) models enables unobserved heterogeneity regarding product group dummy variables to be captured.==== This paper shows that the proposed method can be applied to RCNL models. For models in which all products are in one group and the outside option is in another group, the structural error term can be analytically calculated using the same dataset as in the RCL case, which includes total sales by ====. However, if products are divided into more than one group, total sales by ==== are also required to analytically calculate the structural error term.==== Many studies have investigated the methodological issues in estimating RCL models using aggregate data.==== The literature recognizes that simulation error in numerical integration to approximate market shares predicted by an RCL model affects the asymptotic properties of the estimator (Berry et al., 2004b). Brunner et al. (2017) show that numerical instabilities of the estimator are mainly driven by the simulation error and conclude that with accurate approximation, the estimator is well behaved.====Discrete-type RCL models have the useful property that market shares can be analytically calculated without numerical integration, as discussed in, for example, Berry and Jia (2010). Accordingly, the problem of simulation error can be completely resolved without additional computational burden. Note, however, that the inner loop to obtain values of the structural error term is still needed even for discrete-type RCL models in general.====For discrete-type RCL models, Kalouptsidi (2012) propose a method that modifies the inner loop. In the method, a fixed-point algorithm is applied based not on the market shares of products, as in the BLP method, but on the shares of consumer types. Accordingly, the dimension of the nonlinear system in the inner loop changes from the number of products to the number of consumer types, which is smaller in most cases. Kalouptsidi (2012) shows via Monte Carlo experiments that the method is faster than the BLP algorithm. However, it maintains the nested loop structure.====This paper contributes to the literature by proposing an alternative estimation method for discrete-type RCL models. In the method, since values of the error term can be analytically obtained, the inner loop is no longer required. Therefore, the computation time is drastically reduced, as shown by Monte Carlo experiments.==== The second section introduces a discrete-type RCL model. Section 3 explains the proposed method. After Monte Carlo experiments are presented in Sections 4 and 5 provides an illustrative application using actual data of the Japanese airline industry. Section 6 concludes the paper.",A simple method to estimate discrete-type random coefficients logit models,https://www.sciencedirect.com/science/article/pii/S0167718722000017,24 January 2022,2022,Research Article,71.0
García-Vega María,"School of Economics, University of Nottingham, University Park, Nottingham NG7 2RD, United Kingdom","Received 16 November 2020, Revised 29 September 2021, Accepted 27 December 2021, Available online 10 January 2022, Version of Record 18 January 2022.",https://doi.org/10.1016/j.ijindorg.2021.102819,Cited by (0),"In this paper, I examine whether the Great Recession of the late 2000s (henceforth abbreviated as GR) had an effect on the organization of R&D in young versus older firms. Using a difference-in-difference approach and propensity score reweighting estimator, for a panel of more than 12,000 Spanish firms from 2005 to 2014, I compare young firms with older firms before and after the GR. I show that young firms implemented three key compositional changes in their R&D policies during the GR as compared to older firms: a) they reduced their R&D employment by firing medium and low-skilled R&D workers; b) they hired high-skilled R&D workers; and c) they increased their capital investments for R&D. These changes in R&D policies suggest that, during the GR, young firms substituted medium and low-skilled R&D workers by high-skilled workers and machines. These effects are mediated by the firms’ financial health.","R&D is a fundamental driver of productivity, firm dynamics, market reallocations and, ultimately, economic growth. Investments in R&D depend on economic conditions, which is why recessions are crucial events. As Schumpeter (1923/1939) argues “…there is more brain in business at large during recession than there is during prosperity” (p.143). Therefore, studying the organizational changes that firms undertake in their R&D policies during recessions – the main topic of this paper – is important for understanding job creation, growth potential and aggregate fluctuations.====The economic literature shows that during recessions labour is often relocated and firms tend to reorganize their production and innovation methods (Davis and Haltiwanger 1992; Aghion and Saint-Paul 1998; Manso et al. forthcoming). Recent literature suggests that one way to reduce costs in recessions is through automation by substituting medium-skilled workers with machines and simultaneously hiring a small number of high-skilled workers that complement these machines (Brynjolfsson and McAfee 2011; Autor 2015; Morin 2016).==== Moreover, during recessions, firms can hire from a larger pool of high-skilled workers (Bewley 1999; Mueller 2017).====The process of skill-upgrading and automation or mechanization is particularly important for the R&D department and for young firms.==== Human capital contributes crucially to firms’ technological activities. Moreover, young firms are highly adaptable to the hiring environment (Geurts and Van Biesebroeck 2016). One of the reasons is that young firms have lower labour adjustment costs than old firms because severance payments typically rise with tenure and the average worker tenure is small in young firms.====The main contribution of this paper is to provide evidence of the mechanization and skill-upgrading of R&D for young firms during the Great Recession of the late 2000s (henceforth abbreviated as GR).==== I compare young firms with older firms before and after the Great Recession in a difference-in-difference framework and use a propensity score reweighting estimator. I examine whether the GR influenced the organization of R&D in young versus older firms. I find that young firms adjusted their R&D employment during the GR. I show that young firms implemented three key compositional changes in their R&D policies during the GR as compared to older firms: a) they reduced their R&D employment by firing medium and low-skilled R&D workers; b) they hired high-skilled R&D workers; and c) they increased their capital investments for R&D. These changes in R&D policies suggest that, during the GR, young firms substituted medium and low-skilled R&D workers by high-skilled workers and machines. These effects are mediated by the firms’ financial health.====I use a panel dataset of more than 12,000 Spanish firms from 2005 to 2014. This dataset is uniquely suitable for my purposes because it contains detailed information of the R&D inputs of the firm as well as firm-level financial information. Thereby, I can measure, at the firm level, changes in the composition of the R&D labour force and R&D investments during the GR. Moreover, in Spain, as in many European countries, firing costs rise with tenure,==== which makes young firms in the sample more likely to adjust their labour force than older firms because of their lower firing costs. My results contribute to the understanding of the changes in the internal organization of firms’ R&D during an economic cycle. In particular, I show that the GR particularly affected the reorganization of the R&D of young firms.====The GR was an unexpected shock (Aghion et al., 2021). Most economies experienced financial restrictions, large declines in their domestic demand, and drops in employment at all educational levels (Farber 2015). There is evidence that R&D activities are time-dependant and procyclical (Beneito et al., 2015; Fabrizio and Tsolmon 2014) and that the distress of the banking sector during the GR negatively affected firm innovation (Spatareanu et al., 2019). However, some of the research that is conducted during recessions seems to be more radical than during non-recession times. For example, Manso et al. (forthcoming) show that innovative activities are explorative during recessions and exploitative during booms. Lebdi and Hussinger (2016) study the pattern of innovations of startups founded at different times of the business cycle. Their results suggest that startups founded during the crisis introduce fewer product innovations but of higher novelty compared to the products introduced by startups founded during non-crisis periods. These results are consistent with my finding that startups skill-upgraded their R&D labour force during the GR. A major difference with respect to previous studies on R&D during the business cycle is that I document the changes in R&D inputs during the GR and find that there is mechanization in R&D. My results also suggest that young firms, with similar characteristics to older firms before the GR, become more efficient in the way they conduct their R&D during the crisis as compared to older firms.====This paper also contributes to the literature on empirical studies of the performance of young firms. Sedláček and Sterk (2017) analyse the potential of young firms over the business cycle. They find that firms founded during periods of economic growth grow larger than firms founded during recessions. Piergiovanni (2010) finds that young firms, during their first six years, grow faster than older firms. Haltiwanger et al. (2013) show that, conditional on survival, young firms grow faster than older firms and they are also more volatile. In this line, García-Quevedo et al. (2014) find that young firms are more erratic in their R&D policies than older firms. As Coad et al. (2016) show for a sample of Spanish firms, a possible reason is that young firms invest more in riskier R&D activities than older firms.==== Compared to these papers my paper identifies the R&D policies that young firms undertake during the GR that might contribute to the differences in performance between younger and older firms.====The rest of the paper is organized as follows. In Section 2, I discuss the relevant theory. In Section 3, I describe the dataset and the main variables. In Section 4, I explain the econometric specification. Section 5 presents the baseline results, as well as robustness checks. Section 6 shows additional empirical evidence, investigating heterogeneous effects and the effects of the GR to further innovation inputs, outputs, and firm economic variables. Finally, in Section 7, I summarize the results and conclude.",R&D restructuring during the Great Recession and young firms,https://www.sciencedirect.com/science/article/pii/S0167718721001119,10 January 2022,2022,Research Article,72.0
"Denicolò Vincenzo,Zanchettin Piercarlo","University of Bologna and CEPR, Italy,University of Nottingham, United Kingdom","Received 13 October 2020, Revised 5 August 2021, Accepted 6 December 2021, Available online 1 January 2022, Version of Record 10 January 2022.",https://doi.org/10.1016/j.ijindorg.2021.102811,Cited by (0),We analyze patent protection with sequential and complementary innovation. We argue that in these cases the classic Nordhaus trade-off between innovation and static monopoly distortions is different from the case of isolated innovations. We parametrize the degree of innovation sequentiality and complementarity and show that the optimal level of patent protection increases with both. We also address the issue of the optimal division of profit among different innovators.,"In the modern economy, innovative products or processes often involve different patents, each protecting a separate piece of innovative knowledge. In some cases, basic discoveries open the way to subsequent improvements and applications, so innovation is ====. In others, the distinct innovative components can be invented independently of each other, so innovation is ====. In all of these cases, however, the aggregate value of the inventions is greater than the sum of their individual values. As a consequence, models of isolated, independent innovations are no longer applicable; technology is more “complex.”====Many scholars have argued that when the technology becomes more complex,==== the social costs of patent protection increase. One reason for this is that the fragmentation of intellectual property may create problems of coordination among the patent holders.==== Furthermore, it has been argued that patents may impede the sharing of intermediate technological knowledge among innovative firms.==== In the light of these issues, the conventional wisdom is that for complex technologies, patent protection should be weaker. Without questioning the relevance of these problems, this paper highlights a countervailing effect, which is of first-order magnitude but seems to have gone unnoticed so far. This effect implies that, all else equal, sequentiality and complementarity demand stronger and not weaker patent protection.====The new effect relates to the classic trade-off between innovation and monopoly deadweight losses. Since Nordhaus (1969), this trade-off has been at the centre of the analysis of optimal patent design for isolated innovations. The economic literature on sequential and complementary innovation, on the other hand, has mainly focused on other issues, such as the externalities across innovations and the division of the profit among the different innovators. Yet, sequentiality and complementarity may affect the Nordhaus trade-off and change its optimal resolution. That is precisely the focus of the present paper.====The effect uncovered here can be intuitively explained as follows. With isolated innovations, the optimal level of patent protection is an increasing function of the elasticity of the supply of inventions. This elasticity is the percentage increase in the probability of success ==== associated with a one percent increase in R&D expenditure ====, ====. In simple models, the optimal level of protection is, in fact, directly proportional to the elasticity. Now consider the case of multiple, related innovations. For example, think of two innovations, 1 and 2, that are strictly complementary, meaning that the stand-alone value of each is nil but the aggregate value of both is positive. In this case, research is effectively successful only if both innovations are achieved, so the relevant probability of success (assuming statistical independence) is ====. As a consequence, the relevant elasticity is now the ==== of the individual elasticities, ====. This ==== effect is the ultimate reason why the level of patent protection should be higher than in the single innovation case. That is, in a nutshell, the message of this paper. The rest of the paper formalizes the above argument and extends it in various ways.====The compound-elasticity effect has been overlooked by the vast literature on sequential and complementary innovation because, as noted, that literature has almost invariably focused on issues other than the Nordhaus trade-off. In particular, the literature on sequential innovation has concentrated on the analysis of ==== patent protection, i.e., the protection granted to the first inventor against the second one. This literature has either taken as given the level of ==== protection, i.e., the protection against imitators, or conflated the two forms of protection in a single policy variable.==== Similarly, the literature on complementary innovations has focused on the division of profit taking the overall level of protection as given.====The only previous paper that distinctly addresses forward and backward protection is the classic article by Green and Scotchmer (1995) on sequential innovation. However, Green and Scotchmer drastically simplify the Nordhaus trade-off assuming that innovations can be achieved with probability one by sinking a fixed R&D investment. Since patents are distortionary, this implies that the optimal level of backward protection is simply the one that allows innovators to just cover their R&D costs. With two-stage innovation, this principle applies separately to the two innovators. However, Green and Scotchmer further assume that the policymaker cannot control the division of profit finely. As a result, when both inventors’ costs are covered, at least one of them will inevitably obtain a positive rent. This wasteful over-remuneration, which inevitably arises with sequential innovations, implies that backward protection should be stronger than with isolated innovations.====This paper arrives at the same conclusion as Green and Scotchmer but for different reasons. To highlight the differences, we abstract from the waste-of-profit effect assuming that the division of profit can be fine tuned. On the other hand, we posit a smooth relationship between R&D investment and innovation. This assumption is necessary for a proper analysis of the Nordhaus trade-off, as Green and Scotchmer (1995) recognize in their discussion of endogenous R&D investment (p. 31).====The rest of the paper proceeds as follows. In Section 2, we present and analyze our baseline model of sequential innovation. We parametrize the degree of sequentiality and provide conditions under which both backward and forward protection should be strengthened as the degree of sequentiality increases. In Section 3, we show that our conclusions are robust to several extensions of the baseline model. Section 4 presents similar results for the case of complementary innovations. Section 5 summarizes and offers some final remarks. Proofs are collected in the Appendix.",Patent protection for complex technologies,https://www.sciencedirect.com/science/article/pii/S016771872100103X,1 January 2022,2022,Research Article,73.0
Fagart Thomas,"Department of Economics, Centre for Competition Law and Economics, Stellenbosch University, Private Bag X1, Matieland, 7602, South Africa","Received 2 October 2018, Revised 20 November 2021, Accepted 4 December 2021, Available online 16 December 2021, Version of Record 6 January 2022.",https://doi.org/10.1016/j.ijindorg.2021.102810,Cited by (0),"In the usual theory of collusion, the possibility of collusion increases with the discount factor. The firm’s incentive to deviate is short term: the firm increases its immediate profit but reduces its future profits due to how competitors will react. This paper shows that this result is no longer valid when firms require investment to increase their production and that investment is, at least partially, irreversible. In that case, a firm deviating from a collusive agreement increases its capacity of production, preempting its competitor, and may obtain a dominant position on the market. A higher discount rate makes preemption more profitable, and thus collusion harder to sustain.","In many industries, a firm’s production is limited by its infrastructure, i.e. the amount of factories or equipment that it owns. This production capacity can evolve over time, as firms invest in new infrastructure. The cost of building a new capacity is usually sunk and investments are thus (at least partially) irreversible. Capacity limitation and irreversible investment impact competition, and therefore collusion.====This was the case for the steel cartel operating in the US between 2005 and 2007.==== ArcelorMittal, U.S. Steel, Nucor, Gerdau, Commercial Metals and Steel Dynamics coordinated to reduce the market capacity by closing blast furnaces, leading to a significant increase in prices. Capacities also played a key role in the Lysine cartel operating from 1992 to 1995. When Archer-Daniels-Midland Company decided to cheat the collusive agreement, it first made significant investments in a new unit of production, doubling its capacity, thereby becoming the major player in the lysine market, even long after the cartel ended.====These examples emphasize that investment decisions can be one of the strategic variables used to collude, whether during the implementation of collusion as in the steel cartel, or whether during a deviation as in the Lysine cartel.==== This paper studies the theoretical implications of the utilization of capacity investment as a strategic variable for cartel implementation. It shows the existence of long run incentives for deviation during periods of collusion. To deviate from a collusive equilibrium, a firm has to invest in new units of capacity in order to increase its production. In doing so, the deviating firm commits to its new capacity due to the irreversibility of investment. This reduces the competitor’s incentive to invest in order to punish the deviating firm. When its capacity is large enough, it even prevents the competitor from implementing any punishment. A firm may thus increase its long run profit by deviating from the collusive equilibrium. This contrasts with the standard theory of collusion, whereby a deviation’s only effect is to increase a firm’s short run profits, while reducing long run profits due to punishment.====More precisely, this paper studies a dynamic Cournot duopoly with irreversible investment in capacity. At each period of time, the firms decide on their production level, which is limited by their capacity, and on their next period capacity through buying some assets. Investments are therefore irreversible, as the firms can only increase their capacity. The investment price is linear, and the firms are assumed to always produce at full capacity. These assumptions of irreversible investment and of full capacity production make the reasoning intuitive. Appendix B relaxes them.====To study collusion, this paper focuses on a particular kind of strategy, i.e. the grim-trigger strategy for status quo. A firm following this strategy keeps its initial capacity as long as its competitor also keeps its own initial level of capacity. When one of the firms deviates, the other responds by investing according to the non-cooperative behavior. Due to the commitment effect of capacity investment, the punishing firm invests less than the Cournot outcome. This is the long run effect of the deviation: the deviating firm can gain an advantage in terms of capacity during the deviation, which persists for the rest of the game. In that sense, the deviating firm preempts its competitor: its building of capacity reduces its future competitor’s building of capacity. The usual short run effect is also present: there is a period, between the implementation of the deviation and the implementation of the punishment, when the deviating firm is the only firm to have a capacity larger than the collusive capacities. Both short run and long run effects have to be considered in order to determine a firm’s incentive to deviate.====To isolate the long run effect, Section 3.2 focuses on the limit case in which the period of time shrinks to zero. In such a case, there is a parallel between the decision of the Stackelberg leader and the optimal deviation. The punishing firm decides on its capacity taking into account the capacity already installed by the deviating firm. Collusion is feasible only when the Stackelberg profit is under half of the monopoly profit. A low discount factor is thus harmful to collusion, as more patient firms are more likely to invest, and then prefer to become the Stackelberg leader rather than obtaining half of the monopoly profit. When the period of time is long enough, the short run effect may compensate for the long run one and the impact of the discount factor on collusion will depend on which effect dominates.====In the usual theory of collusion, the relationship between the discount factor and the sustainability of the collusion is increasing, leading to the existence of a critical discount factor, after which collusion may be sustained. The impact of a market feature (multimarket contacts, cost asymmetry, number of firms, entry barrier, demand growth or market transparency) is then analyzed based on the impact it has on this critical discount factor. This methodology is used to make practical recommendations to competition authorities.====When investments are (at least partially) irreversible and may be used as strategic devices, the non-monotonic relationship between the discount factor and the sustainability of the collusion implies the existence of not one, but two critical discount factors. Collusion is feasible when the discount factor falls between these two critical values. Even though one may expect the market factors to influence the lower discount factor as usual, as it is mainly driven by the short run incentive for deviation, nothing is known about the impact of these market factors on the largest critical discount factor, which is driven by the long- run incentive for deviation. The theory of collusion is then incomplete in the case of markets with partially irreversible investments in which investment is used as a strategic device.====Section 2 presents the model and the non-cooperative equilibrium under the assumptions of full capacity production and total irreversibility of investment. Section 3 focuses on the collusive equilibrium for status quo, characterizing the equilibrium when the time period shrinks to zero. Section 4 concludes. Appendix A shows how the long run and short run incentives for collusion cohabitate outside the limit case. Appendix B generalizes the results under the assumption of partial irreversibility and of production under capacity. The proofs are given in Appendix C.",Collusion in capacity under irreversible investment,https://www.sciencedirect.com/science/article/pii/S0167718721001028,16 December 2021,2021,Research Article,74.0
"Hindriks Jean,Serse Valerio","UCLouvain, LIDAM/CORE, Economics School of Louvain, Voie du Roman Pays 34, Louvain-la-Neuve, Belgium,Compass Lexecon, Brussels and UCLouvain, LIDAM/CORE, Belgium","Received 6 May 2020, Revised 17 November 2021, Accepted 2 December 2021, Available online 10 December 2021, Version of Record 21 December 2021.",https://doi.org/10.1016/j.ijindorg.2021.102809,Cited by (1),"In April 2014, the Belgian government reduced the VAT rate on the electricity price from 21% to 6% to support low-income families. In September 2015, the tax cut was repealed, and the VAT rate was reinstated to 21% in the context of a change of government. This paper investigates the impact of such temporary and (plausibly) exogenous VAT reform on the Belgian electricity market. We study the pass-through of the VAT reform to electricity prices and the effect of this (exogenous) price change on electricity demand. We estimate the VAT pass-through on residential electricity prices by a ==== method, using business electricity prices (not subject to VAT) as a control group. Our findings reveal that both the tax cut and the tax hike were entirely shifted to the electricity price (100% pass-through). To assess the impact of the VAT change on demand, we perform a counterfactual demand analysis of the electricity flowing monthly over the grid at the network operator level. Exploiting VAT and non-VAT related price variations, our results show a price elasticity of residential demand for electricity between -0.09 and -0.17. Interestingly, we also find that demand reacted quickly and symmetrically to the VAT cut and the subsequent VAT hike.","In order to promote competitiveness and employment, the Belgian Federal Government decided to reduce the Value-Added Tax (VAT) rate for the supply of electricity to household consumers from 21% to 6% from 1 April 2014 (Law of 15 May 2014 implementing the Pact for Competitiveness, Employment and Recovery) .==== As of 1 September 2015, the federal government decided to reverse this VAT reduction early and charged 21% VAT again.====This paper investigates the impact of such (plausibly exogenous) VAT reform on the Belgian electricity price and demand. We also provide novel evidence on the possible (a)symmetry in both the VAT pass-through rates and the demand elasticity using the two tax change instances. Indeed, the VAT reforms involved first a tax decrease to 6%, followed by a tax increase to 21% within 17 months. This provides the opportunity to test for symmetry in the price and demand responses to the tax cut and tax reversion over a limited time interval under otherwise similar electricity market conditions.====The quality of the tax incidence analysis depends on the plausible exogeneity of the tax reforms. In a Royal Decree of 21 March 2014, it was decided to cut the VAT rate on electricity supply to residential customers from 21% to 6% conditional on evaluating this tax reform by September 2015. The VAT cut was part of the general “competitiveness and employment pact” of the ruling government. The VAT cut was intended to benefit the low-income households and reduce inflation, which given the Belgian automatic indexation, translates into wage costs. The inflation rate was reduced by 0.4 pp, indexation was postponed by four months, and the net budgetary cost of the reform was estimated at 536 million euros in 2015 (BFP, 2015).====Then the newly elected government proposed in July 2015 a new “tax shift” plan. This plan intended to shift the tax base partly from labor to consumption as part of a strategy to reduce wage costs and promote employment. The tax shift plan involved the VAT rate increase on electricity from 6% to 21% in September 2015. Although both VAT reforms were claimed to be motivated by the same concern for labor cost and employment, they were probably driven by political reasons (see also Benzarti et al., 2020; Fuest et al., 2018; Castanheira et al., 2012 who showed that political factor is the main driver of tax reforms). Therefore, the two successive VAT reforms can be seen as plausible exogenous shocks driven by political considerations from two different ruling governments, which were not linked to economic conditions in the electricity market. Also, the reality of the 2015 “tax shift” is that the lower income tax does not offset the extra consumption taxes in the household budget constraint.====We estimate the VAT pass-through to the residential electricity price by employing a ==== method. We use as a control group the electricity price for professional consumers (firms) to assess the price incidence of the VAT changes for residential users (households). VAT is not taken into account for professional consumers, as it is recoverable for them. The assumption is that the residential electricity price would have followed the same trend as the professional electricity price without the VAT changes. Our sample with prices for the consumer and firm segments displays parallel trends both before (January 2013-March 2014) and after (October 2015-December 2016) the VAT changes (see Fig. 3).====The results show that the VAT cut was entirely shifted to the residential electricity price. This result is in line with recent studies showing that the cost of emission permits in Europe is almost entirely shifted to electricity prices (Fabra and Reguant, 2014; Hintermann, 2016). However, there is a lack of evidence about the pass-through of VAT to retail electricity prices. The earlier empirical work has mostly focused on the incidence of excise taxes rather than sales taxes and VAT for a wide variety of markets.==== Nevertheless, in the EU, the VAT is the largest source of tax revenue (on average 30% of total tax revenue), so it matters to understand its price incidence. There is also currently a debate on using VAT as part of the recovery plan for Europe.====Some empirical studies estimate the pass-through of sales/VAT taxes in other markets. Doyle and Samphantharak (2008) and Marion and Muehlegger (2011) studied the incidence of sales taxes on fuel in the United States, with a pass-through close to unity and symmetric response to a temporary moratorium on a 5% gasoline tax increase. Carbonnier (2007) showed that two large VAT cuts in France (on housing repairing services and new car sales) were partly shifted into consumer prices. Similarly, Kosonen (2015) found that the pass-through rate of the VAT cut on hairdressing services in Finland was 50%. Benedek et al. (2020) studied various tax reforms in the Eurozone and showed that the standard VAT rate was usually entirely shifted, whereas the reduced VAT rates were partly shifted. In contrast, we find that the VAT cut to 6% was fully shifted to electricity prices.====This literature typically identifies the VAT pass-through by looking at consumer price changes before and after the VAT reform while controlling for the price evolution in other markets.==== However, finding a reliable control group for VAT changes is rather difficult. This is because the VAT affects the whole market to which it is applied. Furthermore, even if the VAT change affects only one sector, there is a risk of a cross-price effect on the closest sectors producing substitute products. We address these concerns by adopting a difference-in-differences approach that uses business prices as a counterfactual scenario. Business prices are not subject to VAT but have the same cost components as residential prices. Moreover, these two tariffs are not substitutable since households cannot switch to business prices. Without controlling for business prices, we would have reached the misleading conclusion that the VAT cut was (slightly) under-shifted to consumer prices.====We also test for possible asymmetries in the VAT incidence since we observe both a VAT cut and an increase of the same size within a limited time interval (17 months). To our knowledge, there are few similar “symmetric tax reforms” in Europe with two (significant) VAT changes of the same size but in opposite directions. There is the Finland VAT cut on hairdressing of 14 percentage points in January 2007 and the subsequent VAT increase of 14 percentage points in January 2012. Using Beauty salons as a control group, Benzarti et al. (2020) showed that the pass-through rate for VAT increase is double that of the VAT cut. Another symmetric VAT reform (of smaller magnitude but more comprehensive) was in Hungary, with the standard VAT cut of 5 percentage points in January 2006 (for all commodities excluding diesel and gasoline) and a subsequent VAT increase of 5 percentage points in July 2009. Using neighbouring countries as a control group, Benzati et al. (2020) found evidence of (persistent) asymmetric pass-through rates with a much stronger price incidence of VAT increases. They also provided strong evidence of asymmetric pass-through rates for other commodities (except for the communication, household furnishing, equipment, and maintenance sectors)==== from other European “asymmetric” VAT reforms, where the VAT cuts and hikes were of unequal magnitudes.====The analysis of Benzati et al. (2020) suggests that asymmetric VAT incidence is widespread and not specific to some commodities, challenging standard (static) tax incidence theory (Hindriks and Myles, 2013). However, it is always possible that the asymmetric pass-through rate is either due to the asymmetric VAT changes or to different economic conditions underlying the VAT cut and increase.==== Our electricity market analysis does not find any asymmetry in tax shifting between the VAT cut to 6% and the subsequent VAT hike to 21%. Our difference-in-differences analysis suggests that the VAT changes were entirely shifted to the consumer price. This result is certainly expected for the regulated component of the electricity bill (transmission and distribution costs), but it is not evident for the other unregulated components (energy cost and providers fees).====Several factors can help explain the perfect shifting of the VAT rate on electricity price. First, the retail electricity market is quite competitive. Under perfect competition, complete shifting is a likely scenario even though under-shifting is also possible. Another factor is the salience of the reform. The cut was widely advertised in the media. This suggests that the tax reform was under scrutiny, with public pressure to shift the VAT cut entirely to the consumers.==== Lastly, many electricity contracts are based on publicly available indexation formulas (variable contracts) and are closely monitored by the regulator. Since the VAT rate is directly applied to these formulas, providers had to modify them to not shift the VAT cut entirely to consumers. This could have been either too costly or difficult to justify to both consumers and the regulator.====Another contribution of this work is to estimate the impact of the VAT changes on residential demand for electricity. The firm segment may not be a reliable control group for the demand analysis because the professional demand for electricity does not necessarily follow the same dynamic patterns as the residential demand.==== We then perform a counterfactual analysis based on predicted residential demand (using observed covariates such as sunlight, temperature, solar production, and monthly-distributor fixed effects). We study the monthly demand for electricity at the network distributor level, controlling for changes in other determinants of energy use during the period. We find that the VAT cut from 21% to 6% generated a 2% increase in electricity demand.====Exploiting different sources of price variation (VAT and non-VAT), we find a price elasticity of residential demand for electricity between −0.09 and −0.17. This range aligns with recent estimates based on quasi-experimental price variations (see, for instance, Ito, 2014; Deryugina et al., 2020).==== We also provide evidence about the symmetry of the demand response to the price cut and the price hike using the two tax changes. Demand responded to the same extent (albeit in a different direction) to a price cut and a price hike of the same magnitude. This finding has important implications for price-based climate policies and energy conservation. It indicates that VAT hikes are entirely shifted to consumers and can be effective in reducing energy use.==== In contrast, a reduction in the VAT rate is likely to reduce electricity prices, thus increasing electricity consumption.====Lastly, we show that consumers responded quickly to the VAT reform, increasing their demand one month after the VAT cut. This evidence contrasts with the previous literature, suggesting that consumers react slowly to changes in electricity prices.==== The VAT reform was announced early in the media, and the pass-through was perfect, making the reform more salient. Such salience of the tax reform may, in turn, induce consumers to react more quickly to price changes (Chetty et al., 2009; Ito et al., 2018). Combining the flat electricity rates with the salience of the VAT reform makes it simple for the consumer to understand the actual change in the electricity price. The short-run demand elasticity may also be explained by the impact of economic incentives on energy-efficient use.==== Moreover, we find that the increase in electricity demand is concentrated in sunnier and warmer periods when energy is less needed for heating and home lighting.==== This seasonal variation in demand elasticity can indicate that residential electricity is essential in winter rather than summer.====The structure of the paper is as follows. Section 2 describes the institutional details of the Belgian electricity market. Section 3 presents the data and summary statistics. Section 4 estimates the VAT pass-through rate for the two tax changes and tests the pass-through symmetry using a difference-in-differences approach. Section 5 estimates the demand effect (without a control group) using the two price change instances. We also estimate the demand elasticity and its symmetry between price cut and hike. Section 6 concludes.",The incidence of VAT reforms in electricity markets: Evidence from Belgium,https://www.sciencedirect.com/science/article/pii/S0167718721001016,10 December 2021,2021,Research Article,75.0
Mertens Matthias,"Halle Institute for Economic Research (IWH) and the Competitiveness Research Network (CompNet), Kleine Märkerstraße 8, 06108 Halle (Saale), Germany.","Received 1 June 2021, Revised 12 October 2021, Accepted 29 November 2021, Available online 7 December 2021, Version of Record 10 January 2022.",https://doi.org/10.1016/j.ijindorg.2021.102808,Cited by (9),"I derive a micro-founded framework showing how rising firm market power on product and labor markets and falling aggregate labor output elasticities provide three competing explanations for falling labor shares. I apply my framework to 20 years of German manufacturing sector micro data containing firm-specific price information to study these three distinct drivers of declining labor shares. I document a severe increase in firms’ labor market power, whereas firms’ product market power stayed comparably low. Changes in firm market power and a falling aggregate labor output elasticity each account for one half of the decline in labor's share."," a worldwide decline in labor's share in economic output (Karabarbounis and Neiman, 2013; Dao et al., 2017). This not only has severe distributional consequences, but also raises doubts on widely applied Cobb-Douglas production models relying on constant output elasticities of input factors. Not least, the decline in labor shares poses questions about the meaning of work and the future role of people in the economic activities of our society. Therefore, it is unsurprising that a substantial body of literature debates the causes behind the global decline of wage shares.====Traditionally, research explains falling labor shares through changes in firms’ mode of production that reduce the importance of labor to firms. These changes are often seen as a result of biased technological change (Acemoglu, 2003; Oberfield and Raval, 2021), declining relative capital prices (Karabarbounis and Neiman, 2013), or globalization, which facilitates the offshoring of domestic production activities (Harrison, 2005; Elsby et al., 2013). Other work highlights the erosion of labor market institutions (Blanchard and Giavazzi, 2003) and discusses the role of measurement error in explaining declining labor shares (Koh et al., 2020). Most recently, the literature discusses whether rising product markups and firm concentration contributed to falling labor shares (De Loecker and Eeckhout, 2020; De Loecker et al., 2020, henceforth DLEU, Autor et al., 2020).====Yet, despite substantial work, the quantitative importance of the individual channels causing falling labor shares remains not well understood. This study provides such a quantification using a micro-econometric production side framework nesting most of the currently debated mechanisms behind declining labor shares into a parsimonious framework. My framework offers three competing explanations for a fall of labor's share: i) a fall in firms’ labor output elasticities, capturing changes in firms’ mode of production associated with a decreasing importance of labor to firms, ii) an increase in firms’ product market power, or iii) an increase in firms’ labor market power. The latter two explanations refer to an increase in market distortions and reflect inefficient scenarios.==== In contrast, a decrease in labor's output elasticity causes a fall in the wage share even within a competitive environment. In this case, a fall in labor's share results from an aggregate output maximizing reallocation of factor shares.====To study these three distinct drivers of changes in labor's share, I use 20 years of micro-data on German manufacturing sectors firms starting in 1995. This dataset contains information on firms’ product quantities and prices, making the data perfectly suited for my study as it allows me to measure ==== price variation, which is crucial for calculating unbiased measures of market power parameters and output elasticities (De Loecker et al., 2016, henceforth DLGKP)). In most studies, such information is not accessible.====From applying my framework, I provide two novel insights. First, by decomposing firm market power into product and labor market power, I show that, coinciding with the fall of labor's share, there is a high and rising degree of aggregate firm ==== market power, whereas aggregate ==== market power, although moderately increasing, stays low. This constitutes important evidence for the debate on rising firm market power and its role for falling labor shares put forward by De Loecker and Eeckhout (2020) and DLEU, who document a severe increase in firm market power throughout the world (particularly in the US) that coincides with the fall of labor shares.==== Most of this debate abstracts from labor market imperfections.==== As consequence, variation in labor's share that does not result from a changing technological importance of labor is by design attributed to ==== market power. My findings of slightly increasing product market power and strongly rising labor market power challenge this conclusion and point to a key role for labor market power in explaining the documented fall of labor's share.====Furthermore, by assuming competitive labor markets, the existing literature cannot determine whether rising firm market power results from product or labor markets. Clarifying this, as I do, is important because policies targeting output market power differ from policies targeting labor market power. And indeed, my estimates show that labor market power is the more important source of (growing) firm market power in the German manufacturing sector. Whereas I cannot infer on the much-discussed rise of market power in the US, my findings demonstrate that incorporating labor market power into the analysis is crucial for understanding rising firm market power and its macroeconomic implications.====Second, I use my framework to quantify the contribution of market power and changing production processes to the declining wage share. This assessment is informative on the quantitative importance of market power and changing modes of production in explaining a fall of labor's share.==== I answer this question using a simple thought experiment: If a declining labor share results from efficient changes in firms’ production processes, the aggregate labor output elasticity decreases in concordance with labor's share. If the labor share, however, falls due to an increase in firms’ product or labor market power, one observes a wedge between the wage share and the aggregate labor output elasticity. Applying this idea, I find that half of the decline in Germany's manufacturing sector labor share between 1995 and 2014 is explained by a decrease in the output elasticity of labor. The other half is accounted for by increasing firm market power. Given my estimates of product and labor market power, I infer that most of the contribution of market power results from growing labor market power. Notably, the falling aggregate labor output elasticity results from declining labor output elasticities within most industries and not from reallocation processes between industries. This supports studies causing doubts on production models featuring time-constant output elasticities of production factors, as most applied Cobb-Douglas specifications (Chirinko et al., 2011; Raval, 2019).====In addition to the mentioned literature, my study relates to a substantial body of work on changes in labor's share dating back at least to Kaldor (1955–56, 1957), who established the stability of the labor share as one of his famous stylized facts for economic growth. Already in 1958 (Solow, 1958), published a “skeptical note” on the presumed constancy of factor shares. In earlier work, Keynes (1939) called the factor share stability “a bit of a miracle”.==== Whereas much of the work in this field focuses on macroeconomic frameworks, my article belongs to a young literature using micro-data to analyze movements of labor's share (e.g. DLEU; Autor et al., 2020; Kehrig and Vincent, 2021; Oberfield and Raval, 2021). The main advantage of using micro-data is that it allows to study the distribution of variables across firms. This is important for understanding the causes of changes in labor's share, labor's output elasticity, and aggregate product and labor market power, which, as I highlight, is central for deriving policy implications.====Finally, my work relates to a fast-growing literature emphasizing labor market power as an alternative source of firm market power and which recently raised concerns that labor market power creates substantial welfare losses that are comparable to those from product market power (e.g. Naidu et al., 2018; Berger et al., 2019; Azar et al., 2020).==== My article contributes to this research strand by providing insights on how reallocation processes between firms contributed to a severe increase in aggregate labor market power over the past decades in Germany. Furthermore, by linking a direct and firm-specific measure of labor market power to movements of labor's share, my study offers novel insights on the macroeconomic relevance of (growing) firm labor market power.====The remainder proceeds as follows: Section 2 describes the data. Section 3 derives my framework from which I infer on the mechanisms behind declining labor shares. Section 4 presents descriptive evidence, conducts decomposition exercises, and calculates the contribution of rising market power and changing modes of production to the fall of the labor share. Section 5 provides additional discussion and robustness tests. Section 6 concludes.",Micro-mechanisms behind declining labor shares: Rising market power and changing modes of production,https://www.sciencedirect.com/science/article/pii/S0167718721001004,7 December 2021,2021,Research Article,76.0
"Belleflamme Paul,Peitz Martin,Toulemonde Eric","CORE/LIDAM, Université catholique de Louvain, 34 Voie du Roman Pays, Louvain la Neuve B-1348, Belgium,CESifo, Germany,Department of Economics and MaCCI, University of Mannheim, Mannheim D-68131, Germany,CEPR, UK, CESifo and ZEW, Germany,Department of Economics and DEFIPP-CERPE, University of Namur, 8 Rempart de la Vierge, Namur B-5000, Belgium,IZA, Germany","Received 7 January 2021, Revised 1 October 2021, Accepted 21 November 2021, Available online 26 November 2021, Version of Record 3 January 2022.",https://doi.org/10.1016/j.ijindorg.2021.102807,Cited by (1),"We introduce asymmetries across platforms in the linear model of competing two-sided platforms with singlehoming on both sides and fully characterize the price equilibrium. We identify market environments in which one platform has a larger market share on both sides while obtaining a lower profit than the other platform. This is compatible with higher price-cost margins on one or both sides, noting that in the latter case one margin must be negative. Our finding raises further doubts on using market shares as a measure of market power in platform markets.","The definition and measurement of market power are central issues in the theory of industrial organization. It is customary to define market power as a firm’s ability to profitably set a price above marginal cost. As a consequence, the absolute price-cost margin (or, more common, the Lerner index as the ratio of price-cost margin over price) gives a ==== measure of market power. On the other hand, market share and profit can be seen as ==== measures of market power. In industries that can be represented by standard oligopoly models of price competition with horizontally differentiated products (e.g., the asymmetric Hotelling duopoly and the logit demand oligopoly), these measures can be used interchangeably as there is a positive association between them: Within an industry, a larger firm also has a larger absolute price-cost margin and higher profit (see Remark 1 in Section 3). Therefore, in such industries, market power is clearly defined and the various indicators proposed to measure it can be used interchangeably.====In industries with two-sided platforms, defining and measuring market power proves more problematic. Two-sided platforms offer products or services to two distinct groups of users and are thus active on two markets. The presence of cross-group network effects creates an interrelation between the prices that platforms set on these two markets. This interrelation may also drive platforms to decrease the price on one side – possibly even below the marginal cost – to increase revenues on the other side. Defining market power as the ability to raise price above marginal cost is therefore inappropriate in these industries: First, several prices – and not just one – need to be jointly considered; second, ==== price ==== marginal cost may increase profitability. Besides this problem surrounding the ==== of market power, which has already been amply discussed in the literature,==== we argue in this paper that industries with two-sided platforms also raise serious issues regarding the ==== of market power of different firms within an industry.====The main problem is that the three measures of market power – price-cost margins, market shares, and profit – do not always go hand-in-hand in industries with two-sided platforms. A platform with higher profits than its rival can be seen as reflecting a high degree of “platform market power” considering the two markets in conjunction. We show that ====. Moreover, in such cases, the price-cost margins of the larger platform must be higher on at least one side – when one margin is negative, margins may be higher on both sides. It follows that market shares and also price-cost margins cannot be taken as adequate measures of platform market power if this is associated with high profitability.====To establish these results, we reconsider a workhorse model in the two-sided platform literature, namely the two-sided singlehoming duopoly model with access fees proposed by Armstrong (2006). Market shares on each side correspond to the fraction of users that join a platform and platforms incur a cost for each active user.==== In this model, platforms simultaneously choose first their access fees on both sides and, next, users on each side decide simultaneously which platform to join. Contrary to Armstrong (2006), we do not postulate symmetry but we allow platforms to differ in all relevant parameters (costs, stand-alone benefits for each user group and cross-group network effects enjoyed by the two user groups). In spite of the large number of parameters, exploiting the linearity in this model makes it possible to reduce the parameter space and to fully characterize the subgame-perfect Nash equilibrium of the two-stage game.====Within this richer framework, we show that under some conditions, there are equilibria such that one platform with a “competitive advantage” (that is, lower marginal cost, larger stand-alone benefits, or larger cross-group network effects) on at least one side has a larger market share on both sides but obtains a lower profit than the rival platform. The presence of cross-group network effects (on at least one side) is necessary for this result, as well as different price-sensitivities across the two sides.==== This result sharply contrasts with the finding mentioned before that, in standard oligopoly, the more profitable firm must be the larger firm, must enjoy higher price-cost margins, and must have an advantage of some sort. Introducing direct network effects in a Hotelling setting cannot generate the tension between size and profit either: The firm with the higher market share is necessarily the more-profitable firm.==== This shows that our result is specific to competition between two-sided platforms: Market share can be a particularly poor measure of market power or success in markets with two-sided platforms. In particular, we show that the tension between market shares and profit can arise when both firms decide not to subsidize any user (that is, they set prices above marginal cost).====We are not aware of previous work addressing the relationship between market shares, price-cost margins and profits in markets with two-sided platforms. In most of the literature, it is assumed that platforms are symmetric (giving rise to symmetric market shares and profits at equilibrium); this includes Rochet and Tirole (Rochet, Tirole, 2003, Rochet, Tirole, 2006) and Hagiu (2006) among others, as well as, in a model with Hotelling demand, Armstrong (2006) and Armstrong and Wright (2007),==== and, in an oligopoly model of two-sided singlehoming with more general demand, Tan and Zhou (2021). While some papers allow for asymmetric two-sided platforms (e.g., Viecens, 2006, Ambrus, Argenziano, 2009, Njoroge, Ozdaglar, Stier-Moses, Weintraub, 2009, Njoroge, Ozdaglar, Stier-Moses, Weintraub, 2010, Gold, 2010, Lin, Li, Whinston, 2011, Ponce, 2012, Gabszewicz, Wauthy, 2014, Belleflamme, Toulemonde, 2018, Jullien, Pavan, 2019, Anderson, Peitz, 2020), these papers did not investigate the link between market shares and price-cost margins or profit. The only paper – to the best of our knowledge – that assesses the link between market shares and profits is Sato (2021), which replicates our main finding under logit demand – that is, if a platform has larger market shares than its rival, this does not necessarily imply that it makes higher profits.====On the empirical side, perhaps closest is Argentesi and Filistrucchi (2007) who assess the market power of Italian newspapers by estimating a structural oligopoly model of competing platforms under the assumption that both sides – readers and advertisers – singlehome and that readers are not affected by the level of advertising. In particular, they find that readers are subsidized.====In Section 2, we develop the model. In Section 3, we characterize the equilibrium. In Section 4, we provide conditions under which, in equilibrium, one platform attracts more users on both sides but obtains lower profit than the other platform and characterize possible outcomes with respect to market shares, price-cost margins, and profit. In Section 5, we discuss the policy implications of our findings. We conclude in Section 6. Several proofs and the derivations of the numerical examples are relegated to the Appendix.",The tension between market shares and profit under platform competition,https://www.sciencedirect.com/science/article/pii/S0167718721000990,26 November 2021,2021,Research Article,77.0
"Liu An-Hsiang,Siebert Ralph B.","Beijing Normal University Hong Kong Baptist University, United International College, 2000 Jintong Road, Tangjiawa, Zhuhai, Guangdong, China,Purdue University, Department of Economics, Krannert School of Management, 403 West State Street, West Lafayette, IN 47907-2056, United States,CESifo, Poschingerstr. 5, Munich 81679, Germany","Received 3 September 2020, Revised 18 October 2021, Accepted 4 November 2021, Available online 17 November 2021, Version of Record 5 December 2021.",https://doi.org/10.1016/j.ijindorg.2021.102797,Cited by (1),"We focus on the estimation of market entry costs that are declining over time and evaluate their impact on competition and market performance. We employ a dynamic ==== model in which firms make entry, exit, and production decisions in the presence of declining entry costs and learning by doing effects. Focusing on the static random access memory ====, we show that entry costs decline drastically by approximately 2 percent or $40 million per quarter. We conduct a simulation exercise in which a social planner can protect an incumbent from subsequent entrants for different lengths of time. Based on declining entry costs over time, our results show that entry regulation can increase producer and total surplus since regulation can prevent firms from entering too early at overly high entry costs. If own learning and ","A long-standing topic of interest among economists and policy makers has been the relationship between market structure and market performance. A well-established fact in the economics literature is that free entry can result in an “excessive” number of firms entering the market (see Berry, Waldfogel, 1999, Dixit, Stiglitz, 1977, Mankiw, Whinston, 1986; Perry, 1984; Pesendorfer, 2005, Spence, 1976a, Spence, 1976b; von Weizsaecker, 1980, among others). Most empirical studies consider markets that are characterized by constant entry costs over time. In contrast, our study concentrates on technology markets, where entry costs are declining over time, as explained by production machinery becoming more inexpensive and the existence of secondary machinery markets. Beyond declining entry costs, technology markets are frequently characterized by declining production costs caused by learning-by-doing effects. In the presence of learning by doing, firms learn from their production experience and achieve future savings in production costs.====Our study evaluates the impact of declining entry costs and declining production costs caused by learning by doing on market performance and economic welfare. As our research question, we focus on whether a social planner has opportunities to improve economic welfare while engaging in entry regulation such as optimizing the timing of entry. This welfare evaluation is especially useful for regulatory authorities since entry policies are a common instrument in technology markets (more details are provided later). Furthermore, our study contributes to the literature on entry by estimating declining entry costs over time when entry costs are unobserved.====Declining entry costs over time have strong implications on firms’ optimal timing to enter markets since firms have an option value to delay entry. This implies the following trade-off: An early entry requires a high entry cost, but it provides the opportunity to collect profits for a longer time period due to prolonged market presence. An early entry also provides an opportunity to earn higher discounted profits due to low competition in early periods. While firms consider only their private benefits from entering, a social planner considers that firms may enter too early, which results in excessive entry costs incurred at early periods. Furthermore, social inefficiencies of entry can arise from business-stealing effects since an entrant imposes a negative externality while “stealing” output from incumbent firms (see Mankiw, Whinston, 1986, Siebert, 2015). Therefore, entry is often more desirable to the entrant than it is to society such that private and social incentives are misaligned. A social planner has the opportunity to improve economic welfare by imposing entry regulations that prevent firms from entering too early and paying overly high entry costs. The entry regulation, however, comes at an economic cost, as it reduces competition and increases prices.====This study also considers the effects of declining production costs caused by learning by doing on market structure and performance. It has been shown that learning by doing provides incentives for firms to enter the market early, as it allows firms to achieve a head start in production, move down the learning curve quickly, and achieve a comparative cost advantage versus their competitors (see Cabral, 1997; Dasgupta, Stiglitz, 1988, Fudenberg, Tirole, 1983, Ghemawat, Spence, 1985; Scherer, 1980; Spence, 1981). Studies have also shown that the learning curve can erect entry barriers for potential later entrants (see Scherer, 1980; Spence, 1981). In the presence of learning by doing, a social planner might consider protecting the first entrant from subsequent entrants, which allows the incumbent to move down the learning curve quickly and achieve high cost efficiencies (see Dasgupta and Stiglitz, 1988). In contrast, entry protection reduces competition and increases markups. The welfare assessment becomes more complex once we consider learning spillovers—that is, once firms achieve production cost savings by learning from their competitors (see Irwin and Klenow, 1994). Spillovers can substantially attenuate the barriers to entry erected by proprietary learning and provide further incentives for firms to enter (see Ghemawat and Spence, 1985; Spence, 1981).====Overall, the impact of entry protection on social welfare is ambiguous and ultimately becomes an empirical question. Our study focuses on static random access memory (SRAM) chips, which belong to the semiconductor family. The semiconductor industry is often cited as a “strategic” industry, and it has frequently been the target of competition and antitrust policies (see Irwin, Klenow, 1994, Siebert, 2010). The SRAM market is well suited for our research purposes for several reasons. First, the SRAM industry is a high-technology market characterized by high entry costs that decline drastically over time.==== Second, the SRAM industry is characterized by free entry.==== Third, the SRAM industry is characterized by successive entry where firms continue to enter the market many years after the product generation has been launched. Fourth, learning by doing is a well-known feature in memory chip industries such as the SRAM industry. Finally, the SRAM industry is characterized by fairly homogeneous chips within generations, which keeps the analysis tractable.====We employ a dynamic oligopoly model characterized by firms’ entry, exit, and production decisions. Our empirical analysis puts special attention on the estimation of unobserved declining entry and production costs. In determining the optimal time to enter, firms compare the time-specific entry cost with the discounted net profits while forming expectations about future market conditions. Our estimation procedure builds upon the literature of dynamic games.==== We adopt a two-stage algorithm and estimate firms’ entry, exit, and production policies in the first stage. In the second stage, we perform forward simulations, calculate the discounted continuation values, and estimate the marginal cost and entry cost parameters.====Our estimations return reasonable results for firms’ policy functions, as well as demand, marginal cost, and entry cost parameters. For example, we find that entry costs decline by approximately 60 percent throughout the evaluation period, which corresponds to entry cost reductions of $40 million per quarter. Our estimation results also return substantial learning-by-doing and spillover effects.====To show the economic impact of declining entry costs and learning effects, we perform several simulations with an emphasis on dynamic efficiency gains. We begin with a social planner who compares the welfare effects of entry regulations with free entry (see also Ferrari, Verboven, 2010, Schaumans, Verboven, 2008). We consider entry regulations of different lengths of time and evaluate their impact on entry, exit, production, prices, and welfare. The social planner can protect the first entrant from subsequent entry for different lengths of time to prevent excessive entry during early periods and to prevent firms from paying excessive entry costs.====Our simulation results show that a longer-lasting entry protection duration for the incumbent firm monotonically reduces consumer surplus compared to the free entry case. Regarding the impact of the entry regulation on producer surplus, we find that producer surplus declines for short protection periods, begins to increase after six quarters, and eventually is greater than the producer surplus under free entry due to the prevention of excessive entry cost spendings from early entry. The entry regulation can generate gains in producer surplus that even dominate the losses in consumer surplus such that entry regulation can be total welfare enhancing for sufficiently long protection periods. Our welfare simulation results highlight that declining entry costs over time can cause inefficiencies due to too many firms entering too early and paying an associated entry cost that is too high from a social welfare perspective. A sufficiently long duration of entry protection increases entry cost savings, which overcompensate the consumer welfare losses from entry regulation. Therefore, beyond the existing insight that entry regulation can serve as a mechanism to avoid excessive entry, our study provides additional insights, that is, entry regulation can prevent excessive “early” entry and reduce the associated excessive entry costs.====A further simulation exercise assumes constant entry costs over time and evaluates the impact of the same entry protection regulation lasting for different lengths of time. Our results show that entry protection again reduces consumer welfare. In contrast to the results with time-variant entry costs, however, producer and total surplus monotonically decline in the length of entry protection. The simulation result is different when entry costs vary over time since constant entry costs over time do not provide opportunities for entry cost savings when delaying subsequent entry, that is, firms pays the same entry cost independent of the entry time. Hence, our study shows that entry regulation under time-variant entry cost can be welfare improving due to postponing entry and generating entry cost savings.====We conduct further simulations in which we allow for time-variant entry costs but eliminate own learning and learning via spillovers. Our results show that entry regulation is not as harmful to consumer surplus and less harmful to other firms that faced delayed entry. We find that entry protection can improve producer and total welfare already for short lengths of entry protection.====Similar to Blonigen et al. (2017), we also consider a simulation in which a social planner can make entry more or less costly. The social planner can charge a tax or provide a subsidy, both of which are commonly applied regulatory instruments in technology markets (see Das, Roberts, Tybout, 2007, Dasgupta, Stiglitz, 1988; Krugman, 1984); further details are provided later). In our case, upstream manufacturers of production machinery are charged a tax or receive a subsidy, which changes the cost of purchasing production machinery (the entry cost) for SRAM producers. The changes of entry costs have welfare implications since they affect the SRAM producers’ optimal timing to enter the market, as well as their production and exit decisions.====The results show that taxes (subsidies) decrease (increase) consumer surplus due to a smaller (larger) number of entering firms. A tax (subsidy) increases (reduces) total welfare, as it attenuates (exacerbates) the problem of spending an excessive amount on early entry. Similar to entry regulation, taxes reduce consumer welfare, but they can serve as a total welfare-improving instrument (with declining entry costs) to prevent an excessive number of firms from entering too early at overly high entry costs.====The remainder of the paper is organized as follows: Section 2 provides a literature review. Section 3 describes the institutional features of the semiconductor industry and presents summary statistics. Section 4 introduces our dynamic oligopoly model, and Section 5 describes the econometric model. In Section 6, we discuss the estimation results and present welfare simulation results based on our dynamic model. We conclude in Section 7.",The competitive effects of declining entry costs over time: Evidence from the static random access memory market,https://www.sciencedirect.com/science/article/pii/S0167718721000898,17 November 2021,2021,Research Article,78.0
Landry Peter,"University of Toronto, Mississauga, Canada,Rotman School of Management, University of Toronto, St. George, 105 St George St, Toronto, ON M5S 3E6, Canada","Received 15 February 2020, Revised 1 November 2021, Accepted 9 November 2021, Available online 15 November 2021, Version of Record 11 December 2021.",https://doi.org/10.1016/j.ijindorg.2021.102798,Cited by (1),"This paper theoretically investigates the pricing and advertising decisions of a monopolist that sells to consumers who, in each period, ==== consider the (cognitively costly) decision to buy its product. In the proposed model, consumers can be compelled by advertising to consider the buying decision, while the likelihood of considering the decision also depends on past buying decisions. A product is then said to be “insistent” if ","A consumer will not buy a product if the thought of buying doesn’t even cross their mind. So what does it take for a consumer to consider a buying decision? One possibility is a firm’s marketing activities — an advertisement, for instance, could give a consumer the idea to buy the advertised product. Another possibility is one’s memory of past buying decisions. For example, a consumer who routinely buys (or routinely thinks about buying) a product may be more likely to ‘remember’ the buying decision without being prompted than if they lacked such a habit.====This paper models a dynamic market in which a monopolist sells a non-durable product to forward-looking consumers, each of whom ==== consider the decision to buy the product in a given period, while incurring a fixed ‘decision cost’ if and when they do.==== Moreover, whether or not a consumer considers the decision in a given period is endogenously determined (though not directly chosen) in two ways. The first form of such “endogenous consideration” is ==== whereby a consumer’s likelihood of considering the decision depends on their past decision-making — namely, whether they considered buying in the previous period, and if so, whether they decided to buy.==== Here, a consumer’s likelihood of considering the decision is assumed to be higher if they considered the decision in the previous period (all else equal), reflecting the idea that one’s memory of a recently-considered decision makes it more salient.====We then say that a product is ==== if a consumer is more likely to consider buying the product after considering but ==== buying it than after buying it (while a product is ==== if the reverse is true). This idea of an insistent product stems from recent theoretical work (Landry, 2019) and may be understood as a product for which a consumer experiences the attention processes associated with “craving.”==== Namely, a product that is craved would naturally be insistent in this sense, as cravings tend to linger — while continuing to draw attention to the craved activity — if they are not satisfied.==== In the model, it follows that consumers have an extra incentive to buy an insistent product (or to ==== buy a non-insistent product) because doing so reduces the likelihood of incurring the decision cost in the next period — or put differently, buying an insistent product may actually “help” the consumer get their mind off the decision.====As the second form of endogenous consideration, advertising can compel consumers to consider the buying decision.==== For the purposes of this paper, the formal representation of such ==== is meant to be very simple. Namely, in each period the firm either advertises to all consumers or does not advertise at all. In turn, an advertisement always serves the same function — it ensures that all consumers consider the decision in that particular period — without having any other direct effect besides this immediate effect on consideration. As a result, any future effect of advertising or any variation in the effectiveness of advertising must arise indirectly through the interaction of advertising-induced consideration with history-dependent consideration.====In this vein, the first (of three) main contributions of this paper is that it offers a potential micro-grounded mechanism — based on consumers’ optimal buying decisions — for several well-known advertising concepts (listed in the top portion of Table 1). For instance, the model predicts greater returns to advertising newer products and diminishing returns when repeating an advertisement.==== Endogenous consideration also provides a common basis for ==== and ==== effects (e.g. Givon and Horsky, 1990) in that an advertisement can increase future sales both ==== and ==== (respectively) an increase in present sales. In addition, the model offers a potential rationale for the common use of ==== strategies (e.g. Little, 1979, Dube, Hitsch, Manchanda, 2005) as it predicts that it is optimal for the firm to advertise periodically in ‘spurts’.====While detailed discussions of other models will be deferred to the main text, it is worth noting that the advertising concepts listed in Table 1 are addressed by other models too. In a smiliar vein, the present work is not meant to suggest that the real-world manifestations of these concepts are explained entirely by “endogenous consideration”; instead, endogenous consideration represents a potentially important contributor to these effects. Also of note, these effects are typically captured as built-in assumptions or emerge as properties of advertising ==== that directly model the effect of advertising on aggregate sales. The present treatment thus complements these more macro-level approaches by explicitly modeling advertising effects as arising through the individual buying decisions of forward-looking consumers.====As its second main contribution, this paper shows how endogenous consideration can motivate the use of various pricing strategies (listed in the bottom portion of Table 1), with new implications regarding the coordination of pricing and advertising decisions.==== When selling an insistent product, the model predicts that it is optimal to use ==== over each advertising cycle, setting a relatively high initial price then gradually lowering it until advertising resumes. Furthermore, an insistent product can be sold at a profit even if it is “worthless” (in the sense of having zero consumption value). For a non-insistent product, it is instead optimal to use ==== over each advertising cycle, setting a relatively low initial price then gradually raising it. It can also be optimal to occasionally offer free samples (equivalently, setting price equal to zero) or to delay selling a non-insistent product as part of an ‘anticipation-building’ strategy.====The final main contribution of this paper comes from formalizing a novel distinction between insistent and non-insistent products — defined in terms of consumer decision-making processes with endogenous consideration — while exploring its implications for pricing and advertising decisions. The analysis highlights the potential strategic relevance of this distinction, as the model suggests many differences for marketing insistent and non-insistent products.====The rest of this paper proceeds as follows. Section 2 presents the model. Section 3 derives the firm’s optimal pricing strategy for a given advertising schedule and addresses some special cases of interest. Section 4 considers the full equilibrium, in which advertising decisions are endogenous (in addition to pricing and buying decisions). Section 5 concludes.","Pricing, advertising, and endogenous consideration of an “insistent” product",https://www.sciencedirect.com/science/article/pii/S0167718721000904,15 November 2021,2021,Research Article,79.0
"Mossay Pascal,Shin Jong Kook,Smrkolj Grega","Department of Economics, Kyungpook National University, Korea,The Department of Economics and Statistics, Korea University Sejong Campus, Korea,Newcastle University Business School, 5 Barrack Road, Newcastle upon Tyne, NE1 4SE, United Kingdom","Received 19 August 2020, Revised 9 November 2021, Accepted 10 November 2021, Available online 14 November 2021, Version of Record 27 November 2021.",https://doi.org/10.1016/j.ijindorg.2021.102799,Cited by (0),"To explore the relationship between spatial location and quality differentiation, we build a dataset of over 30,000 restaurants rated by TripAdvisor, across large UK cities. We document several new stylized facts. Top-rated restaurants tend to be spatially more concentrated around the city center than bottom-rated ones. Whereas top-rated restaurants tend to locate close to other top restaurants, bottom-rated restaurants tend to locate away from each other and closer to top-rated ones. Our theoretical model can explain the main features of observed spatial patterns. Consistently with the predictions of our model, our ==== finds that an increase in the population density in the city center is associated with a decrease in the spatial dispersion of both top and bottom restaurants, this reduction being larger in magnitude for top ones. Also, a larger quality difference between top and bottom restaurants increases both the absolute and relative dispersion of top restaurants.","In the United Kingdom, the food service industry generated a total turnover of over GBP 72 billion and employed over 1 million people in 2018. Moreover, eating out has become so popular that the revenue of restaurants and other food services constitutes the largest revenue share in the leisure sector.==== The trend is similar in other western countries. For instance, in the United States, where the restaurant industry generates over USD 800 billion in sales, restaurants have become so widespread that the US National Restaurant Association reports that “nearly 6 in 10 adults have worked in the restaurant industry at some point during their lives”.====Restaurants have become something much more than simply a place to eat and are now part of modern life, thus constituting an important component of urban amenities. Nowadays, to stay competitive, restaurants do their best to provide customers with a unique dining experience; many of them now offer a wide range of food and beverages, from coffee and cocktails to salads and healthy eating options. Also, a respected online reputation has become an essential business asset for restaurant owners. Not only do a majority of consumers consult online reviews====, but also more than one in three consumers would generally not eat in a restaurant rated below 4 stars on online review sites such as TripAdvisor or Google.==== Among the various sources of differentiation for restaurants, location plays an important role. According to the US National Restaurant Association, 56% of consumers would choose a restaurant within a walking distance over another.====Given the intrinsic importance of location and quality, the restaurant industry is a natural choice for studying the relation between these two differentiation factors affecting competition. In this study, we explore whether any systematic differences between the locations of top- and bottom-rated restaurants arise. In other words, we study spatial competition among quality-differentiated firms, where quality is that perceived by consumers. To do this, we build a unique, hand-collected dataset, which maps over 30,000 restaurants listed and rated on TripAdvisor’s online review site, across cities in England and Wales. We find that top-rated restaurants tend to be spatially more concentrated than bottom-rated ones and locate closer to the city center. The estimates of Ripley’s K functions show that top-rated restaurants tend to locate closer to other top-rated restaurants, whereas bottom-rated ones tend to locate away from each other and closer to top-rated ones.====The spatial clustering of companies has attracted considerable interest in both theory (e.g., Krugman, 1991, Porter, 2000) and empirical research (Marcon, Puech, 2003, Duranton, Overman, 2005, and numerous others). Of particular interest is the question about which parameters determine the formation of clusters. Gordon and McCann (2000), for instance, compare the advantages and disadvantages of geographical proximity as perceived by business leaders in different sectors and conclude that the clustering of economic activity brings about several advantages that usually outweigh by far the disadvantages of increased competition. We contribute to this extensive literature with a novel study on the potential relation between quality differentiation and the spatial clustering of firms (restaurants) in cities.====The empirical literature on restaurants typically focuses on the impact of competition (e.g., an increase in the number of firms) on prices and on the relation between prices and quality. For instance, De Silva et al. (2016) show that competition does not decrease but rather increases restaurants’ prices. Due to economies of spatial clustering, restaurants benefit from positive externalities in denser, well-served restaurant areas, which attracts more consumers. They also show a positive relationship between the prices charged by restaurants and their quality.====In cities, population density affects the variety of available products. Schiff (2015) shows that a higher population density increases the diversity of cuisines and the range of restaurant quality levels in cities. In high-density areas, consumption benefits are large because consumers can visit more restaurants they prefer. Couture (2016) estimates the average household’s willingness to pay to move to a denser area. Both these studies reemphasize the role of cities as centers of consumption (see Glaeser, Kolko, Saiz, 2001, Glaeser, 2012).====Our analysis focuses neither on the relationship between quality and price nor on quality competition. Rather, we contribute to the literature by exploring the relationship between restaurant quality and spatial patterns that we observe in the data (e.g., the absolute and relative spatial dispersions of top and bottom restaurants). We also propose a competition model between restaurants that can explain the main features of observed spatial patterns. Our model builds on imperfect competition à la Hotelling (Hotelling, 1929). Restaurants compete in terms of price and location for consumers distributed along a line segment. Locations are chosen in the first stage of the game and prices are set in the second one. Unlike in the traditional Hotelling model, consumers here have idiosyncratic tastes about restaurants. This means that they visit any restaurant with a positive probability. Moreover, firms are vertically differentiated by the quality of the good they serve to consumers. There are 2 high- and 2 low-quality restaurants. Importantly, qualities are assumed to be exogenous (i.e., related to long-term decisions). As is the case in the standard Hotelling duopoly model, the subgame perfect equilibrium in price and location is solved by backward induction. We develop a numerical algorithm that relies on a state-of-the-art nonlinear solver to find the multiple equilibria of the model, both symmetric and asymmetric ones.====When consumer taste heterogeneity is relatively low, travelling costs play an important role in buying decisions. The city center attracts top restaurants, as it provides them with the best access to the customer base. Top restaurants are able to drive bottom restaurants away from the center through quality competition. The latter ones then disperse around top restaurants, primarily serving customers living outside the city center. In contrast, when consumer taste heterogeneity is relatively high, buying decisions appear as random to restaurants. Consequently, the city center offers relatively little advantage in terms of access to the customer base. As competition is fiercer between top restaurants, they disperse more than bottom ones.====We also consider non-uniform spatial densities of consumers. For a two-firm model, Anderson et al. (1997) showed that tight density functions constitute a clustering force leading to lower prices. Considering quality-differentiated firms, we show that the magnitude of the quality difference between top and bottom restaurants qualifies this result. Although a higher concentration of consumers in the center attracts both types of restaurants, a larger quality difference increases further the competition between top restaurants, inducing them to move apart. The relative magnitude of these two effects determines which type of restaurants locates closer to the center.====We derive a number of testable predictions from our model about the spatial dispersion of top and bottom restaurants, and the effect of quality. We build empirical specifications and run regressions to test these hypotheses. We are able to validate most of them. In particular, we find that an increase in the population density in the city center reduces the spatial dispersion of both top and bottom restaurants, this reduction being larger in magnitude for top ones. A larger quality difference between top and bottom restaurants increases both the absolute and relative dispersion of top restaurants.====The rest of this paper is organized as follows: In Section 2, we describe our data, explore the spatial patterns of restaurants, and establish several new stylized facts. In Section 3, we present and study our model. We characterize equilibria in terms of parameters and derive testable implications. In Section 4, we empirically test our model’s predictions and outline those to be considered for future work. Section 5 concludes. Appendices contain the list of cities in the sample as well as various robustness checks.",Quality Differentiation and Spatial Clustering among Restaurants,https://www.sciencedirect.com/science/article/pii/S0167718721000916,14 November 2021,2021,Research Article,80.0
"Cosnita-Langlais Andreea,Tropeano Jean-Philippe","EconomiX, UPL, Univ. Paris Nanterre, CNRS F92000, France,Université de Paris 1 and Paris School of Economics, France","Received 20 March 2020, Revised 1 November 2021, Accepted 2 November 2021, Available online 14 November 2021, Version of Record 4 December 2021.",https://doi.org/10.1016/j.ijindorg.2021.102795,Cited by (0),"This paper examines the impact of commitment decisions on the efficiency of antitrust enforcement. We discuss the optimal use of commitments considering past rulings as a source of knowledge to better assess future similar antitrust cases. Our framework combines two key effects: the deterrence of the anticompetitive behavior by the different enforcement regimes, and the dynamic perspective through litigation as a source of learning. We show that if the level of penalty is high enough, the antitrust authorities undervalue the dynamic informational benefit of litigation and tend to over-use commitments.","Currently, two options are available for most competition agencies to address an antitrust violation from unilateral anticompetitive conduct: either rule on an infringement through formal litigation, or instead reach a negotiated settlement with the firm, also called commitments decisions (Europe) or consent decrees (US).====These two enforcement tools lead to different outcomes. Litigation results in a formal prohibition of the practice together with the payment of a fine if the firm is eventually convicted. But litigation requires a robust theory of harm based on strong evidence. The alternative is to reach a settlement and fix the anticompetitive effect of the practice by negotiating commitments with the firm. Each procedure involves costs and benefits. The key social benefit of a commitments procedure is an earlier restoration of market competition for lower administrative costs. In contrast, the main benefits from a prohibition decision are the deterrence through monetary sanctions and the creation of precedents that may improve future enforcement. Indeed, when using commitments, the competition agency does not formally identify an infringement, and the firms admit no wrongdoing,==== which impedes the creation of precedents. Following Landes and Posner (1976, p. 250 and 251), legal precedents may be viewed as an investment to increase the stock of knowledge useful for the assessment of future cases: competition agencies and courts of justice learn from litigating. Several recent antitrust cases epitomize the implications of the lack of learning from commitment decisions. After filing a case against the major US studios and Sky UK for geoblocking agreements in 2015,==== the EC finally closed it in 2019 after all investigated parties had offered commitments. However, the main legal question raised by the case (copyright infringement and parallel trade within the common European market) was left unresolved. The latest decision of the European Court of Justice in this case emphasizes that a commitments decision only requires a preliminary assessment of the practice, whereas establishing the existence of an infringement must follow from a thorough examination.==== Formal rulings need a solid theory of harm, that may be used in future similar circumstances. After many years where the EC dealt with the abusive royalties in standard essential patent cases by means of commitments, a formal decision was eventually reached in the Motorola case that clarified the law and set a precedent.==== The European Commission openly acknowledged this potential for learning from formal rulings: it recently fined Google € 2.42 billion for breaching EU antitrust rules and stated that “Google has abused its market dominance as a search engine by giving an illegal advantage to another Google product, its comparison shopping service. [... ]Today’s decision is a precedent which establishes the framework for the assessment of the legality of this type of conduct”.==== Surprisingly, albeit planning to enforce prohibitions whenever there is significant need for “deterrence, punishment and legal precedent”,==== the European Commission issued only very few prohibition decisions in novel areas of intervention for which legal guidance is much needed.==== Since the entering into force on May 1 2004 of the Council Regulation No 1/2003 making room for commitments, the European Commission has heavily relied on such decisions to deal with antitrust violations: more than 60% of the antitrust cases (excluding cartels) did not formally sanction a violation, and more than 70% of the abuse-of-dominance cases were resolved with commitments (Mariniello, 2014).====In this paper we discuss the optimal use of antitrust commitments. Our primary goal is to understand how a forward-looking and benevolent competition authority (CA henceforth) may balance the present and future costs and benefits (saving of litigation costs vs lower deterrence and weaker learning process) of using commitments rather than pronouncing a formal prohibition decision.====For this purpose we develop a theoretical model to contrast two enforcement regimes: “the strict enforcement” where the antitrust authority cannot propose commitments to firms in order to fix the alleged anticompetitive behavior, but instead commits to always litigate the case, and the “flexible enforcement” which allows the CA to choose between proposing commitments and going to trial. Our broad objective is to identify the social cost and benefit associated with each enforcement regime. To do so we consider alternatively two cases: first a purely static analysis, to grasp the basic trade-off between litigation and commitments. Then we turn to a dynamic setting, where present rulings afford better knowledge on future similar cases: the CA learns to correctly determine case facts in future cases by litigating present ones.====We start with the purely static analysis and show that the flexible enforcement leads the CA to propose commitments too often. Being able to choose between commitments and litigation leads the CA to neglect the higher deterrence effect of litigation, and instead focus only on the saving of trial costs following a commitments offer. We show that the ability of the CA to assess accurately cases before possible litigation has an ambiguous impact on this static bias towards commitments. Indeed, if anticompetitive practices are quite accurately detected by the CA, then commitments are efficient in saving on litigation costs. However, the accurate detection of anticompetitive practices also makes litigation a valuable enforcement tool by imposing fines and thus efficiently deterring the anticompetitive practices. In the end, if the deterring effect of litigation is high enough, a better assessment of cases reinforces the efficiency of the strict enforcement.====We then go on to examine a dynamic setting where the procedural choice is made at each period, and for which we assume that present litigation increases the accuracy of future detection. The question we address is to what extent the systematic static bias in favor of commitments may cause a dynamic inefficiency by distorting the CA’s assessment of the future situations. We show that the static bias is either neutral or leads the CA to undervalue the future informational benefit from present litigation. Again, the deterrence achieved by the CA’s detection activity is critical for this outcome: as explained before, a better future detection accuracy tends to make the litigation procedure optimal if deterrence is strong. This implies that in this case, the future CA’s decision to favor the commitments is a source of an even stronger such bias at the present period. The policy implication of our results is that the flexible enforcement fosters excessive incentives to use the commitments procedure when the antitrust enforcement produces substantial deterrence of anticompetitive practices. This is true in a static context, but taking into account the dynamic learning effect of litigation reinforces this result. Instead, a weaker deterrence, due to lower fines in case of conviction for instance, will lower the opportunity cost of using the commitments procedure.====The paper unfolds as follows: next we review the related literature and our contribution to it. Then we present our model and examine first the static choice between litigation and commitments, before looking into this trade-off in a dynamic setting.",Learning by litigating: An application to antitrust commitments,https://www.sciencedirect.com/science/article/pii/S0167718721000874,14 November 2021,2021,Research Article,81.0
Drouard Joeffrey,"Univ Rennes, CNRS, CREM - UMR6211, Rennes F-35000, France","Received 15 April 2018, Revised 23 September 2021, Accepted 2 November 2021, Available online 13 November 2021, Version of Record 20 December 2021.",https://doi.org/10.1016/j.ijindorg.2021.102794,Cited by (0),"We study how the presence of locked-in customers in a downstream market affects the distribution choice of an upstream content provider. Two asymmetric distributors compete in a mature market and the content provider sells its rights using lump-sum fees. A higher number of locked-in customers reduces the need to resort to exclusivity to relax downstream competition. The content provider therefore sells its rights to both distributors when there is a sufficiently-high proportion of locked-in customers. We show that an exclusive affiliation with the smaller rather than the larger distributor facilitates distributors’ rent-extraction, in particular for low-quality content. When there are few locked-in customers, the content provider sells low-quality content to the smaller distributor and high-quality content to the larger distributor. Our results suggest that competition authorities should cautiously evaluate the effects of lower ","Digital-content distributors need to acquire premium content as a way of increasing the value of their services. They in particular would like to acquire this content on an exclusive basis in order to strengthen their market position. Distributors can be thought of as pay-TV, telecom or over-the-top providers that compete for sports and movie rights or premium channels. Apart from the distributors’ need to sign exclusive deals in order to differentiate their product from that of their rivals, these markets are also characterized by switching costs that can produce “consumer lock-in”. These costs can be represented in monetary terms but also include time, effort and psychological hurdles. This lock-in effect partly explains why switching rates are relatively low in these industries.====The size of the customer base and the extent of consumer lock-in are drivers of profitability in content-distribution markets (Chen and Hitt, 2006). However, with the exception of Weeds (2016), the existing theoretical work analyzing the determinants of content exclusivity make no reference to switching costs. We here analyze the content-distribution strategies employed in markets with locked-in customers. We consider a mature market with two asymmetric downstream distributors and one upstream content provider. The upstream provider sells the rights for its content using lump-sum fees, and can be exclusive to one distributor or multi-home (Stage I). Distributors then compete on price (Stage II). While some consumers are fully locked-in due to prohibitively-high switching costs, others can switch supplier at no cost.====The allocation of content across distributors that emerges from Stage I is the one that maximizes the sum of distributors’ profits in Stage II. The effect of Stage-I exclusive distribution on these profits is as follows. On the one hand, some customers will be denied access to the content if the content provider affiliates exclusively with one distributor. Consumers’ average willingness-to-pay for distributors’ services is therefore greater when the content provider multi-homes rather than affiliating exclusively with one distributor. On the other hand, exclusivity allows for vertical differentiation among distributors, which can facilitate distributors’ rent-extraction. Whether content is exclusive to one distributor or present on both distributors depends fundamentally on the proportion of locked-in customers. A higher percentage of locked-in customers increases the market power of each distributor, and thus reduces the need to resort to exclusive distribution to avoid the dissipation of surplus in favor of customers. The content provider therefore multi-homes when there is a sufficiently-high proportion of locked-in customers; for intermediate values of the percentage of locked-in customers, the content provider multi-homes if content quality is not too high and otherwise affiliates exclusively with the larger distributor.====When few customers are locked-in, the content provider sells its rights on an exclusive basis. Since it can supply the content on a larger scale, the distributor with the larger customer base may more easily take advantage of the content than its smaller rival. However, this does not always follow. As the fraction of locked-in customers falls, the demand served by an ==== distributor becomes less contingent on whether it has a large or small customer base; moreover, competition among distributors is fiercer if the content provider affiliates exclusively with the larger rather than the smaller distributor. Thus, the initial disadvantage of a smaller customer base can be overcome if there is a sufficiently-low proportion of locked-in customers. We in particular show that the content provider affiliates exclusively with the smaller distributor if the mass of locked-in customers and content quality are not too high.====At the end of the game, distributors are left with their individual ==== Stage-II profit regardless of the content-distribution outcome. The value of a large customer base is therefore equal to the Stage-II profit of a ==== distributor minus the Stage-II profit of a ==== distributor. With a larger customer base, a distributor can guarantee itself a higher minimum profit and thus limit the harmful effects of competition against a rival of higher quality. It follows that the value of a large customer base is positive and greater than if there were no content provider.====Our results bear two interesting policy implications. They first suggest that competition authorities may face a trade-off between increasing consumer mobility to bring about tougher price competition in the downstream market and a less-restrictive policy on switching costs in order to promote non-exclusive distribution. By encouraging exclusive distribution, a lower number of locked-in customers may reduce consumer welfare. Second, they underline that an initially-small distributor can overcome its larger competitor by acquiring exclusive content.====The remainder of the paper is organized as follows. Section 2 presents the relevant literature. Section 3 introduces the model. Section 4 characterizes the equilibrium pricing and content-distribution strategies and determines the value of a large customer base. Section 5 discusses the relationship between welfare and the number of locked-in customers. Section 6 provides extensions to and robustness checks of the baseline model. The detailed analysis and proofs of this section are set out in the Online Supplementary Material. Section 7 concludes.",Content-distribution strategies in markets with locked-in customers,https://www.sciencedirect.com/science/article/pii/S0167718721000862,13 November 2021,2021,Research Article,82.0
"Bohland Moritz,Schwenen Sebastian","Technical University of Munich, School of Management, Germany,Technical University of Munich, School of Management, German Institute for Economic Research (DIW Berlin), and Mannheim Institute for Sustainable Energy Studies, Germany","Received 15 March 2020, Revised 20 October 2021, Accepted 27 October 2021, Available online 9 November 2021, Version of Record 2 December 2021.",https://doi.org/10.1016/j.ijindorg.2021.102792,Cited by (4),"We show how policies to support clean technologies change price competition and market outcomes. We present evidence from electricity markets, where regulators have implemented different policies to subsidize clean energy. Building on a multi-unit auction model, we show that currently applied subsidy designs either foster or attenuate competition. Contract-based output subsidies decrease firms’ mark-ups. In contrast, market-based designs that subsidize clean output via a regulatory premium on the market price lead to higher mark-ups. We confirm this finding empirically using auction data from the Spanish power market. Our empirical results show that the design choice for renewable subsidies significantly impacts pricing behavior of firms and policy costs for consumers.","Governments around the globe are adopting policies to limit climate change. The standard policy instruments include carbon prices, R&D grants, and subsidies for the deployment of “clean” technologies (e.g., Goulder, Parry, Williams Iii, Burtraw, 1999, Acemoglu, Aghion, Bursztyn, Hemous, 2012). While previous research has focused on the market impact of carbon prices and R&D support (e.g., Johnstone, Haščič, Popp, 2010, Fabra, Reguant, 2014, Aghion, Dechezleprêtre, Hemous, Martin, Van Reenen, 2016, Calel, Dechezlepretre, 2016), relatively less is known on how subsidies for clean technology change market outcomes and welfare. In this paper, we address this question by studying the impact of subsidies for clean energy on market outcomes in the power sector.====Support policies for clean technologies are omnipresent in power markets, where many governments have rolled out large-scale programs to subsidize renewable energy.==== To better understand the impact of subsidies on market outcomes, we contribute by offering a model of pricing behavior under different policy designs. By exploiting detailed firm level data from the Spanish power market, we also investigate welfare effects empirically and find that the design of support mechanisms significantly affects market prices, rents, and, as such, overall policy costs.====Our model formalizes pricing decisions by firms that produce with “clean”, i.e., low-carbon and “dirty” carbon-intensive inputs. The regulator implements a mechanism that establishes an output subsidy for the clean technology. Motivated by existing real-world mechanisms in electricity markets, we investigate two standard designs to reward this subsidy. First, subsidies may come as a linear tariff per unit produced from clean assets. Alternatively, subsidies are implemented via a regulatory premium that clean production earns on top of the market price. In the former mechanism, clean production is rewarded independently of the market price and yields profits equal to its output times the per-unit subsidy. In essence, this mechanism constitutes a forward contract for producing with clean technologies between producers and the regulator. We thus refer to this mechanism as contract-based “tariff”.==== In the alternative mechanism, profits from clean production depend on the equilibrium market price and are topped up by the regulatory premium. We thus refer to this mechanism as market-based “premium”. In 2017, these two mechanisms were employed by more than 80 countries worldwide (IRENA et al., 2018). Our model investigates pricing behavior under these two regimes and allows for both perfect and imperfect competition.====We find that under perfect competition, the design of the support mechanism is irrelevant. However, when firms are able to charge mark-ups, the design of the support mechanism affects market prices and rents. In particular, our results highlight the critical role of market size effects on pricing incentives. Contract-based tariffs decrease market size, as only conventional, carbon-intensive capacity is sold on the market. Consequently, firms that own conventional capacity merely face demand left unsatisfied from clean production. When firms charge strategic mark-ups, they hence face a smaller market, resulting in lower equilibrium market prices. In contrast, when the support mechanism rewards clean production by a premium on top of the market price, the market size remains large. In this case, profits for both clean and conventional technologies are a function of the equilibrium market price, and firms have ample incentives to charge higher mark-ups.====We empirically test this prediction on pricing strategies under different support designs. Next to detailed bidding data from Spanish electricity wholesale auctions, we exploit an institutional change in the support design. In Spain, both the tariff mechanism and the premium mechanism have been applied, where the latter successively replaced the former during the years 2004 and 2005 (Batlle et al., 2012). We conjecture that, following the transition to the premium mechanism, we observe higher equilibrium mark-ups and thus market prices.====We investigate this effect using hourly observations on price-quantity decisions by Spanish power producers. Our empirical findings show that the mark-up significantly increases under the premium mechanism as compared to the tariff mechanism. The magnitude of this effect is economically significant. Counterfactual calculations show that during our period of observation the market-based premium design increased firms’ markups on average by about 5%. The policy hence has been costly to consumers who had to pay for the regulatory premium and in addition lost rents due to higher mark-ups on the electricity wholesale market.====We also document this effect when focusing on the two largest firms in the market, as especially larger firms with high shares of clean production increase their mark-ups. In addition, we illustrate how the policy change impacts market concentration. Specifically, we show that mark-ups increase parallel to a decrease in market concentration and rapid entry of new firms. As such, measuring the competitive benefits of different support designs by market concentration or firm entry can be misleading.====Our paper contributes to the vast literature on policy designs to mitigate climate change. One strand within this literature has focused on carbon pricing and its effect on electricity prices (e.g., Fabra and Reguant, 2014), as well as its effects on investment in clean energy (e.g., Calel and Dechezlepretre, 2016). A second strand, to which we more closely relate, examines subsidies for clean energy provision. Reguant (2019) investigates the interaction between carbon taxes, feed-in tariffs, and renewable portfolio standards in California, and shows trade-offs between efficiency and distributional concerns. Dressler (2016), Acemoglu et al. (2017), and von der Fehr and Ropenus (2017) analyze the market impact of renewable support mechanisms and their costs to consumers theoretically. They propose oligopoly models to analyze pricing decisions when firms hold a portfolio of conventional and subsidized wind and solar capacity. Also Ritz (2016) provides a theoretical study on the impact of renewable subsidies, and examines the equilibrium interaction between renewables competition and forward contracting. Focusing on the Texas power market, Cullen (2013) evaluates both costs and benefits of renewable support empirically and estimates that the value of emission offsets from wind power outweighed its subsidies. Furthermore, Gowrisankaran et al. (2016) develop a method to quantify the economic value of subsidized solar energy and highlight the social costs of intermittent renewable production. Indeed, while the existing empirical literature has thus far mostly evaluated the costs and benefits of clean energy subsidies, their design and consequences for firms’ pricing decisions have not been empirically investigated yet. One exception is a recent study by Imelda and Fabra (2021) who, like us, document that fixed tariffs mitigate market power. Using a dominant firm model, they find that reduced incentives for arbitrage can counteract this effect. We consider large incumbent firms and explore market power effects of different support schemes within the supply function framework, that allows to explicitly model the uncertainty in market-wide renewable output. Our empirical analysis moreover shows that market power effects persisted despite significant market entry of small renewable producers.====Our paper also adds to the literature on strategic pricing in multi-unit auctions for electricity (e.g., Green, Newbery, 1992, von der Fehr, Harbord, 1993, Wolfram, 1998, Fabra, von der Fehr, Harbord, 2006, Reguant, 2014). More specifically, our model draws from the share auction framework in Wilson (1979) and multi-unit auction models that explore bidding strategies in power markets (e.g., Hortaçsu and Puller, 2008). We rely on the modeling approach in Hortaçsu and Puller (2008) as it aids us in tailoring our model to renewable energy provision in power markets. We amend the model by adding different support mechanisms for clean generation. The mechanism through which our model demonstrates the effect of subsidies on equilibrium market prices is similar to the one outlined in the literature on forward markets (e.g., Allaz, Vila, 1993, Wolak, 2003, Bushnell, Mansur, Saravia, 2008, van Koten, Ortmann, 2013, van Eijkel, Kuper, Moraga-González, 2016, Ito, Reguant, 2016). Equivalent to forward contracts, clean generation that is rewarded by a contract-based tariff reduces spot demand and thus prices. With a premium, this effect vanishes and prices increase.====The remainder of this article is organized as follows. Section two presents the regulatory environment and data. Section three outlines a model of bidding behavior in multi-unit auctions in electricity markets. The model incorporates the two standard mechanisms of renewable support and closely guides our empirical investigation. In section three, we illustrate our empirical strategy and discuss different econometric specifications. Section four presents our empirical findings. Section five concludes.",Renewable support and strategic pricing in electricity markets,https://www.sciencedirect.com/science/article/pii/S0167718721000849,9 November 2021,2021,Research Article,83.0
Harrington Joseph E.,"Department of Business Economics & Public Policy, The Wharton School, University of Pennsylvania, United States","Received 24 May 2021, Revised 26 October 2021, Accepted 31 October 2021, Available online 3 November 2021, Version of Record 20 July 2022.",https://doi.org/10.1016/j.ijindorg.2021.102793,Cited by (3),"Competitors privately sharing ==== is universally prohibited under antitrust/competition law. In contrast, there is no common well-accepted treatment of competitors privately sharing ====. This paper is the first to show that a private exchange of prices can result in higher prices for consumers. Conditions relevant to determining when such an information exchange is anticompetitive are identified.","The canonical collusive practice involves private communication among competitors that results in them agreeing on the prices they will charge buyers. Contrary to that description, there are a number of episodes for which firms communicated about prices different from those that buyers would pay. Some cartels privately communicated list prices - as in high fructose corn syrup,==== urethane,==== and cement==== - and others communicated surcharges (or component prices) - as in air freight,==== air passenger,==== and railroads====. While list prices and surcharges may ultimately affect final (or transaction) prices, they are distinct from those prices. A second departure from that canonical collusive practice is that cartel members’ communications may not involve agreeing on price but instead only comprise an exchange of prices. This paper explores the possible anticompetitive harm when both of those elements are present: firms privately exchange non-transaction prices such as list prices and surcharges.====Competition law is unequivocal in prohibiting firms from agreeing on prices, whether they are final prices, list prices, or surcharges. It is unlawful by the per se rule in the United States (under Section 1 of the Sherman Act) and by object in the European Union (under Article 101 of the Treaty of the Functioning of the European Union).==== There is not, however, a common position with respect to firms sharing prices. In the U.S., the exchange of prices - and even an agreement among firms to exchange prices - is not outright prohibited:====The sharing of prices is evaluated under the rule of reason. In the market for corrugated containers, firms were found guilty because the sharing of prices was shown to have an effect on prices.==== In the EU, the exchange of prices comes under Article 101 as a concerted practice. To establish a concerted practice,====The European Commission has taken the position that “mere attendance at a meeting where an undertaking discloses its confidential pricing plans to its competitors is likely to be caught by Article 101(1).”==== This view has been put into their guidelines which state: “information exchange can constitute a concerted practice if it reduces strategic uncertainty... because it reduces the independence of competitors’ conduct and diminishes their incentives to compete.”====The lack of a common treatment of the sharing of prices is at least partly due to the absence of a well-established theory of harm. This missing theory of harm is exemplified by the EU’s vague claim that the exchange of prices is potentially harmful because it “reduces strategic uncertainty.” However, there are no theoretical or empirical analyses showing that less “strategic uncertainty” among firms results in higher prices for consumers. In the case of the U.S., an agreement to share prices is not per se illegal because courts claim there can be procompetitive benefits, though there are no economic studies showing those benefits. We then find judicial views on both sides of the Atlantic are unsubstantiated which calls for a rigorous economic analysis to deliver clarity.====The absence of a theory of harm is also problematic for the enforcement of competition law. For jurisdictions with the rule of reason (or by effect), understanding when a private exchange of prices is anticompetitive is essential to determining when such an exchange should be prohibited. For jurisdictions with the per se rule (or by object), a conviction on liability still leaves private litigants having to establish harm in order to collect damages.==== Furthermore, the lack of an economic argument showing consumer harm from such an information exchange could result in a future court reversing a per se prohibition.==== For all of these reasons, competition policy would be on more solid and stable ground if it was better understood when and how such information exchanges are anticompetitive.====These issues and challenges are exemplified by the recent trucks cartel case in the EU.==== It was documented that executives of truck manufacturers regularly met and shared gross list prices. Absent the information exchange, gross list prices were only distributed internally and, in particular, were not shared with customers. Gross list prices were part of the pricing process that eventually determined the prices charged to dealers and final purchasers. While the European Commission ruled that the truck manufacturers were in violation of Article 101, private litigation is currently seeking to determine whether there was harm to final purchasers and that could depend on what information was actually conveyed at those meetings. Claims range from executives ==== gross list prices to ==== gross list prices to ==== on gross list prices.====While it is commonly recognized that there is harm if it can be shown they ==== on prices, that is not the case when all that can be shown is that they ==== prices.====This research project addresses two questions. First, is it harmful to consumers when firms privately share their prices? Second, if it is harmful, should the sharing of prices be subject to the per se rule (by object) or rule of reason (by effect)? This paper addresses the first question by developing a theory of harm associated with the private sharing of prices and delivers conditions when such an information exchange raises prices. Thus, the first question is answered in the affirmative. The second question will be addressed in a companion policy paper where the following per se prohibition is proposed: ====In order to be able to draw general policy conclusions, it is important that the theory of harm is robust and relies on some general factors. Towards that end, a parsimonious model is developed with two distinctive features that I believe are ubiquitous and compelling. The first feature is that firms (or, more specifically, their executives) are not sharing transaction prices. They are instead sharing prices that may ultimately affect the prices that consumers pay but are not necessarily the final prices that are put before consumers. In the trucks case, manufacturers shared list prices, and list prices would be expected to affect dealer prices which would then affect the prices paid by final purchasers. If airlines shared fuel surcharges, one would expect those surcharges to affect final passenger prices delivered by the airline’s pricing algorithm. And if executives of retail chains shared posted prices, the prices faced by consumers could be different through the offering of discount coupons or rebates. The critical property is that the prices shared may influence the transaction price but, at the time of the information exchange, those transaction prices are not yet determined. The second feature is that the executives who are sharing prices may be able to influence transaction prices but do not have full control over them. This feature differs from the standard economic model of the firm, which has a single decision-maker choosing prices, but is consistent with actual practice. Generally, prices are the result of a process within a firm involving various employees with different responsibilities. As this second feature is crucial to the theory of harm, Section 3 reviews some of the evidence supporting it.====The way those two features are encompassed in the model of this paper is stylized but the insight it yields is intuitive and robust (in the sense that it only relies on some general assumptions). The sharing of prices by firms is captured with a two-stage structure. Firms first choose prices and then, after sharing those prices, can change the price at some cost. To relate this structure to the trucks case, suppose an executive chooses a list price. In the absence of sharing list prices, a list price would lead to some final prices for purchasers according to the firm’s internal pricing process. When executives of truck manufacturers share their list prices, knowledge of other manufacturers’ list prices may induce an executive to intervene in the internal process by which the list price affects the final purchaser prices. However, such intervention is costly to the executive as other employees must be convinced of the price change or be incentivized to make it. That cost to changing price captures the limited ability of the executive to influence final prices. Though the structure is stylized, in that it subsumes the internal pricing process through an adjustment cost for price, it has the appealing feature that it is not dependent on a particular modeling of that internal pricing process.====The main finding is that a private information exchange of prices by competitors is harmful to consumers when the cost of adjusting price is neither too small nor too large which can be interpreted as the colluding executives having some but not full control over the final price. The intuition is as follows. When executives privately share prices, they are given the opportunity to effectively change the prices they will be offering to consumers before consumers have an opportunity to transact. However, this sharing of prices would be of no consequence when it is near costless to change prices. In that case, the information exchange is basically cheap talk so each executive would simply set the final price to maximize profits regardless of what was learned about rivals’ prices from the information exchange.==== Sharing of prices would also have no effect when it is very costly to change prices. For example, suppose executives share list prices and, after sharing them, each executive is either unable to change its list price or finds it very difficult to intervene in the process determining the discounts provided off of the list price. Again, the information exchange would have no effect. However, when the executive has some limited control - as reflected in a moderately-valued adjustment cost to price - sharing prices is anticompetitive. When they have an agreement to share prices, an executive will set a supracompetitive price because the executive knows that, should it be learned that other firms have set lower prices, they will have the opportunity and means to respond by lowering price. This anticipation that other firms would respond to a low price incentivizes executives to select and share supracompetitive prices.====As will become clear, the theory is an adaptation of a well-understood strategic pricing mechanism exemplified by most-favored-customer clauses. From a purely theoretical perspective, the paper’s contribution is modest. From the practical perspective of understanding firm conduct (why do firms privately exchange prices?) and providing guidance for courts (how should it rule regarding an information exchange?), the paper’s contribution is more significant.====Section 2 discusses related research. Section 3 offers some evidence regarding company pricing processes which motivates the model. (A reader who would like to skip that material can read the final paragraph of Section 3 and determine if they accept the conclusions from the case studies.) Section 4 provides the simplest structure for establishing that a private information exchange of prices can be harmful to consumers. Section 5 shows that the conclusions from that simple model are general as the anticompetitive effect of sharing prices holds for the canonical price game with differentiated products. For the case when firms share list prices, Section 6 provides a more explicit model which links list prices to the prices paid by consumers. A discussion of the practical application of the theory of harm is offered in Section 7, and Section 8 concludes.",The Anticompetitiveness of a Private Information Exchange of Prices,https://www.sciencedirect.com/science/article/pii/S0167718721000850,3 November 2021,2021,Research Article,84.0
"Melo Carolina,Moita Rodrigo,Sunao Stefanie","Insper Institute of Education and Research (INSPER), Rua Quatá 300, São Paulo, SP, 04546-042, Brazil,University of São Paulo (USP), Faculdade de Economia, Administração e Contabilidade da Universidade de São Paulo (FEA-USP), Avenida Professor Luciano Gualberto 908, São Paulo, SP, 05508-010, Brazil","Received 7 December 2020, Revised 27 October 2021, Accepted 4 November 2021, Available online 7 November 2021, Version of Record 18 November 2021.",https://doi.org/10.1016/j.ijindorg.2021.102796,Cited by (0),"In this paper, we study the connection between pass-through and market power in the Brazilian Liquefied Petroleum Gas (LPG) industry. We use a state tax shock and apply a difference-in-differences strategy to estimate pass-through – at different levels of the supply chain – and an instrumented difference-in-differences strategy to estimate demand, and then feed a theoretical model to make inferences about a conduct parameter – that measures market power. We find an incomplete pass-through at the distribution level and for the supply chain as a whole, and a more-than-complete one at the retail level. Furthermore, we estimate price-elasticity of demand to be greater than one. When we feed a theoretical model of pass-through under ==== with these estimates, we obtain a high conduct parameter at the whole supply chain level and an even higher one at the retail level alone. These results show that considering only the whole supply chain pass-through may lead to hasty conclusions about market power. Besides contributing to the empirical literature that connects pass-through with market power, we contribute to on-going national discussions regarding competitiveness in the LPG industry.","The concept of pass-through==== is very important in economics and its study embraces a wide range of topics in industrial organization. Theory shows that pass-through is determined not only by supply and demand, but also by market power (Bulow, Pfleiderer, 1983, Seade, Delipalla, Keen, 1992, Anderson, de Palma, Keide, 2001, Weyl, Fabinger, 2013). Many empirical studies, therefore, have sought to make a connection between pass-through and the existence of market power, so to elucidate potential welfare implications. In the energy sector, pass-through receives special attention (Fell, 2010, Fabra, Reguant, 2014, Stolper, 2016, Duso, Szucs, 2017, Genakos, Pagliero, 2019) due to the market power that producers and distributors usually hold and manifest through markups over marginal costs, leading to welfare decreases (Borenstein et al., 1997).====In this paper, we study the connection between pass-through and market power in the Brazilian Liquefied Petroleum Gas (LPG) market. We estimate pass-through at different levels of the supply chain – including the distribution and retail levels, and the supply chain as a whole –, estimate demand, and calibrate a theoretical model to make inferences regarding a conduct parameter – an elasticity-adjusted Lerner index that measures market power – at both the retail and the whole supply chain levels.====In Brazil, the interest in the LPG market is due to the enormous reach and use of this type of gas as fuel for cooking, which is present in 100% of the country’s municipalities and used by over 90% of households. Its price represents a substantial share of the monthly spending of the lowest income population (Sindigas, Sindigas). Therefore, cost changes along the supply chain of LPG directly affect the welfare of most Brazilian families, especially the lowest income ones.====The Brazilian LPG industry comprises production, distribution, and retail activities. The distribution price faced by retailers is made up of the producer’s price, social contributions and social security related taxes, a state tax, and the distributor’s gross margin; and the retail price paid by consumers is the sum of the distribution price with the retailer’s gross margin (Petrobras, 2019). At the production level, there is one big player that holds monopoly power; at the distribution level, the market is concentrated in the hands of five big players; and, at the retail level, the market is made up of numerous players. Distributors are allowed to sell LPG either to final consumers directly or to retailers, who then reach consumers (Araujo-Jr., 2009). Given this industry structure, the National Petroleum Agency (ANP) and the National Competition Regulator (CADE) have faced cases of coordination among distributors to impose retail prices and retaliate retailers that oppose their conditions. Even though several cases have been investigated, no sound evidence against competition in the retail market has been documented to date.====In January 2017, the Brazilian state of Minas Gerais suffered a sudden shock in the LPG state tax. This shock is the basis of our empirical strategies used to estimate pass-through and demand. To estimate pass-through, we use a Difference-in-Differences strategy, in which we compare municipalities located in Minas Gerais to municipalities located in bordering federative units, before ==== after the tax shock. In order to estimate demand, we use an Instrumented Difference-in-Differences strategy, in which exogeneity from the tax shock is used to isolate the “good” variation in price.====Empirical results show that pass-through in the distribution segment and the industry as a whole is incomplete, estimated at about 0.61 and 0.76, respectively. Pass-through in the retail segment, however, is more-than-complete, estimated at about 1.24. As for demand estimation results, we find price-elasticity estimates to range from 1.06 to 1.14, using our preferred log-log specification. And, when we allow for a more flexible functional form, using a quadratic specification, we find evidence of a convex demand curvature. Results for the tax shock effect on prices are robust to several checks.====When we feed the theoretical model of pass-through under imperfect competition with these estimates, we find that – for a supply elasticity ranging from 0.1 to 0.5 – the conduct parameter of the supply chain as a whole ranges from 0.61 to 1.02 and the retail conduct parameter ranges from 0.78 to 1.07. Moreover, in both cases, the conduct parameter decreases as supply elasticity increases. If we take supply elasticity to the extreme – that is, if we consider an infinitely elastic supply –, the conduct parameter of the whole supply chain approaches zero, but the retail conduct parameter remains higher – at about 0.2. Therefore, we find evidence of market power in the retail segment for any degree of supply elasticity. Furthermore, results show that if we only consider the whole supply chain pass-through, we underestimate the conduct parameter in the retail market.====The theoretical literature that entails both pass-through and market power started being built long ago. Jenkin (1872) defined pass-through under perfect competition and showed that, in this case, pass-through depends on the relative elasticity of demand and supply. Bulow and Pfleiderer (1983) and Seade (1985) theoretically addressed pass-through in the monopoly case. These authors made the connection between the elasticity of marginal surplus and the curvature of demand, and showed how it affects pass-through when there is no competition in the market. Delipalla and Keen (1992) and Anderson et al. (2001) advanced the work on pass-through under imperfect competition for specific models, adding a constant conduct parameter to the equation. More recently, Weyl and Fabinger (2013) generalized pass-through under imperfect competition for any symmetric model of complete information, allowing the conduct parameter to vary with quantity.====The empirical literature that connects pass-through and market power comprises two types of studies, one that uses structural model approaches and another that uses reduced form approaches. As discussed in Duso and Szucs (2017), studies that use structural models validate that markup adjustments can determine incomplete pass-through and focus on disentangling the mechanisms involved in this relationship – see, for instance, Bettendorf and Verboven (2000), Goldberg and Verboven (2001), Devereux and Engel (2002), Nakamura and Zerom (2010), Hellerstein and Villas-Boas (2010), Bonnet et al. (2013). In studies that use reduced form approaches, researchers have been engaged in documenting how pass-through changes with different degrees of competition and (to some extent) explaining their pass-through estimates with market power arguments (Stolper, 2016, Duso, Szucs, 2017, Cabral, Geruso, Mahoney, 2018, Francois, Manchin, 2019, Genakos, Pagliero, 2019). Many of these studies have focused either on the supply chain of an industry as a whole or on a specific segment of the chain. We emphasize the importance of looking at different segments of the supply chain, so that the overall chain pass-through and conduct parameter do not mask what happens at a specific level.====More related to our work, Pless and Benthem (2019) estimate both pass-through (to the final consumer) and demand to make inferences regarding market power in the Californian solar energy industry. The authors aim at presenting an empirical test for their theoretical argument that a more-than-complete pass-through, combined with a sufficiently convex demand, is enough to say that a market is imperfectly competitive – if certain uncommon conditions can be ruled out. Although they use empirical estimates along with theory to say something about market power, they do not calibrate a theoretical model to calculate the conduct parameter – as we do in this paper.====Our paper is unique for we estimate pass-through along the supply chain to make inferences about a conduct parameter at both the whole supply chain and the retail levels. Moreover, we provide an additional empirical test for what Pless and Benthem (2019) argue in their paper, by showing that over-shifting in the retail segment and a convex demand imply the existence of market power for any level of supply elasticity. Thereupon, we believe to be contributing to both the applied literature that connects pass-through and market power through reduced form approaches and the on-going policy discussions regarding the competitiveness of the Brazilian LPG retail market in which ANP and CADE have been involved.====The rest of the paper is organized as follows. In Section 2, we provide background information on the Brazilian LPG market. In Section 3, we provide the theoretical setup, including the model that we calibrate and how we go about calibration. In Section 4, we present our data and the state tax shock experienced by Minas Gerais. In Sections 5 and 6, we present our empirical analyses of pass-through and demand, respectively. Section 7 is dedicated to the calibration of the theoretical model and consequent results. Finally, in Section 8, we present a brief discussion about our results and conclusions.",Passing through the supply chain: Implications for market power,https://www.sciencedirect.com/science/article/pii/S0167718721000886,7 November 2021,2021,Research Article,91.0
"Martimort David,Pouyet Jérôme,Trégouët Thomas","Paris School of Economics (EHESS), 48 boulevard Jourdan, 75014 Paris, France,CY Cergy Paris Université, CNRS, THEMA and ESSEC Business School, Avenue Bernard Hirsch, BP 50105, 95021 Cergy, France,CY Cergy Paris Université, CNRS, THEMA, Université de Cergy-Pontoise, 33 Boulevard du Port, Cergy Pontoise Cedex 95011, France","Received 19 May 2020, Revised 15 October 2021, Accepted 18 October 2021, Available online 4 November 2021, Version of Record 15 November 2021.",https://doi.org/10.1016/j.ijindorg.2021.102791,Cited by (0),"An incumbent seller contracts with a buyer under the threat of entry. The contract stipulates a price and a penalty for breach if the buyer later switches to the entrant. Sellers are heterogenous in terms of the gross surplus they provide to the buyer. The buyer is privately informed on her valuation for the incumbent’s service. Asymmetric information makes the incumbent favor entry as it helps screening buyers. When the entrant has some bargaining power vis-à-vis the buyer and keeps a share of the gains from entry, the incumbent instead wants to reduce entry. The compounding effect of these two forces may lead to either excessive entry or foreclosure, and possibly to a fixed rebate for exclusivity which is afforded to all buyers.","Exclusive clauses are prevalent in platform markets. In its recent anti-monopoly guidelines for platform economy, China’s competition regulator especially targeted exclusive agreements, such as the so called ==== clause under which a platform prevents a merchant from selling on multiple platforms.==== As of 2018, the Indian regulator banned exclusive contracts between platforms and merchants.==== In 2019, the EU Commission concluded that ==== Last, an investigation launched by the French Competition Authority unveiled that brands were tied in various ways to the dominant operator in the flash-sales market.==== These examples all point in the same direction: Exclusivity provisions are practices that are widely spread in the digital economy====In this article, we revisit the rationale for exclusivity clauses in a context, particularly relevant for the digital economy, where a dominant firm deals with a large number of heterogenous buyers and faces the threat of entry by a competing seller. We shall argue that exclusivity provisions have then a screening purpose that tends to alleviate their foreclosure effect.====Our model is directly inspired from Aghion and Bolton (1987)’s seminal work on contracts as a barrier to entry. We adapt their framework to economic contexts that prevail in the digital economy. An incumbent firm (or platform) provides one unit of service (or digital service) to a buyer (a merchant) but faces the threat of entry by a rival who might be more attractive. The incumbent firm can commit ex ante to a contract with the buyer. This contract stipulates a price for the seller’s service as well as a penalty for breach (or early termination fee) if the buyer decides to switch later on to the entrant. The difference between the price and the penalty will often be referred to as a rebate for exclusivity. The penalty for breach acts as a rent-shifting tool for the seller to appropriate the potential gains of entry.====In Aghion and Bolton (1987), the incumbent and the entrant differ in their costs of providing the service and the buyer’s valuation for this service is common knowledge. We depart from this framework in some important ways. First, the incumbent and the entrant no longer differ in terms of their costs of providing the service but with respect to the buyer’s value of the service that those sellers may generate. In the digital economy, (marginal) costs can be considered as being null and platform sellers are differentiated in terms of the various pools of customers that they give access to their merchants. Second, the buyer might be privately informed on her valuation for the incumbent’s service. An alternative interpretation is that there is a continuum of heterogeneous buyers who differ in terms of their valuations for that service. Again, this assumption appears quite realistic in the case of large dominant platforms which remain by and large ignorant of merchants’ market opportunities.====In this context, our first contribution is to analyze the impact of asymmetric information on the capability of the incumbent seller to erect barrier to entry through contract design, stressing the role that exclusivity provisions may serve. To isolate this impact from other forces, we first assume that the buyer has all the bargaining power in her relationship with the entrant; an assumption that is later relaxed. In that polar scenario, the buyer can reap all gains from entry if she buys from the entrant. If, additionally, there is complete information between the incumbent and the buyer, there are no reasons for the incumbent to distort inefficiently entry since the incumbent-buyer coalition appropriates all the entry gains. The rebate for exclusivity is thus zero and there is neither inefficient foreclosure nor inefficient entry. Those entry gains end up being pocketed by the incumbent through an adequate choice of the penalty for breach.====This efficiency result no longer holds when the incumbent is uninformed about the buyer’s valuation for its service, since in this scenario, he can no longer tailor the penalty for breach to the foregone surplus. Under asymmetric information, screening the different types of buyers now becomes an issue. In our context, and even if buyers have unit demand, screening remains feasible because buyers with higher valuations for the incumbent’s service have less incentives to switch to the entrant. Therefore, the probability of letting entry occur can be used as a screening device by playing on the penalty for breach. Under asymmetric information, some information rent must be left to buyers to induce information revelation on surplus. To limit those rents, the incumbent increases the rebate above its full information level. So doing discourages buyers from underreporting their valuation, for this would imply to forego large gains from trade with the incumbent. Increasing the rebate also means decreasing the penalty for breach relative to the price for the incumbent’s service; which facilitates entry. Therefore, asymmetric information makes the incumbent softer vis-à-vis the entrant and leads to socially excessive entry.====Our second contribution is to study the role of the buyer’s bargaining power vis-à-vis the entrant. When the buyer does not capture all the gains associated to entry and leave the entrant with some share of those, the incumbent is willing to inefficiently distort entry so as to reduce the share of the rent that cannot be ultimately pocketed back.==== The incumbent should now set a negative rebate for exclusivity; which has a foreclosure effect on the entrant. This result revisits in our specific context the so called ====, as highlighted first by Aghion and Bolton (1987).====While asymmetric information calls for positive rebates for exclusivity as seen above, the fact that the buyer might not have all the bargaining power vis-à-vis the entrant instead requires to set negative rebates. There is thus a significant tension between the two polar objectives of the contract, namely extracting information rent from the buyer and extracting bargaining rent from the entrant. When both asymmetric information and limited bargaining power on the buyer’s side are present altogether, the optimal rebate trades off these two effects. The tension between those two objectives can be so strong that the optimal incentive compatible contract entails a fixed rebate and all buyers, whatever their types, are bunched on the same inflexible allocation.====Our article belongs to the literature that analyzes the strategic role of contracts in market contexts. The so called Chicago School of Antitrust (Posner, 1976, Bork, 1978) defended the view that an upstream incumbent seller does not want to lock down its buyer in an exclusive relationship that inefficiently deters the entry of a more efficient competitor. In an influential article, Aghion and Bolton (1987) showed that contracts can erect entry barrier when there is uncertainty on the efficiency gains of entry and when the incumbent cannot fully appropriate these gains. With respect to their analysis, we keep the assumption that the buyer has a unit demand but add two key ingredients. First, sellers differ not in the cost of supplying the buyer but in the surpluses they provide to the buyer. Second, there is asymmetric information about the buyer’s valuation for the incumbent’s service. Rebates for exclusivity have thus a screening purpose.====Several analyses have extended Aghion and Bolton (1987)’s framework towards various directions. Marx, Shaffer, 1999, Marx, Shaffer, 2004 consider a multi-unit demand buyer and analyze how contractual restrictions may limit the incumbent seller’s ability to shift back the rent from entry. Choné, Linnemer, 2015, Choné, Linnemer, 2016 and Martimort et al. (2017) further add asymmetric information (on the buyer’s preferences and on the entry cost) and the possibility that the incumbent seller is a dominant firm (in the sense that it is an essential trading partner for a fraction of the buyer’s demand). These articles study the exclusionary properties of nonlinear price-quantity schedules, depending on whether the price charged by the dominant firm depends on the quantity purchased from the rival firm or not. Ide et al. (2016) examine how the surplus between the dominant seller and the buyer is shared when ex ante lump-sum transfers are ruled out. They show that some ex ante commitment is required to generate inefficient foreclosure and is thus subject to a possibility of renegotiation or, conversely, that in the absence of commitment, exclusion becomes impossible. Our model endows the incubent with such an ex ante commitment power.====Still considering asymmetric information about the buyer’s preferences, Calzolari and Denicolò (2013) investigate market-share discounts and exclusive contracts in a symmetric duopoly setting with multi-unit demand. Calzolari and Denicolò (2015) relax the symmetry assumption by assuming that one firm has a competitive advantage (either on cost or on quality) over its rivals and is thus dominant. In these papers, exclusivity plays the role of a screening device whose effects come in addition to standard screening distortions in a multi-unit demand context. Our model differs from Calzolari and Denicolò (2015) on several grounds. First, the buyer has limited bargaining power over the entrant. Second, the buyer’s valuations for the incumbent’s and the entrant’s services are independent random variables. Third, only one unit is traded with precludes the use of quantity as a screening device. Yet, exclusivity plays also a key role as a screening tool in our context as well. However, we somehow confirm their earlier result that inefficient foreclosure or socially excessive entry may both arise in equilibrium depending on the sharing of the bargaining power between the buyer and the entrant.====Finally, another strand of the literature, less related to our contribution, investigates how mis-coordination among several independent buyers (Rasmusen, Ramseyer, Wiley, 1991, Innes, Sexton, 1994, Segal, Whinston, 2000, Spector, 2011, Chen, Shaffer, 2014, Chen, Shaffer, 2019, Bedre-Defolie, Biglaiser, 2017) or the presence of several competing buyers (Fumagalli, Motta, 2006, Simpson, Wickelgren, 2007, Abito, Wright, 2008, Asker, Bar-Isaac, 2014, Wright, 2009) might also facilitate vertical foreclosure.====Section 2 presents the model and analyzes a benchmark where no contract is ever signed. Section 5.1 considers the symmetric information scenario with contract signing and shows that entry is socially efficient when the buyer has all bargaining power vis-à-vis the entrant. Section 5.2 shows that asymmetric information between the incumbent and the buyer leads to an excessive level of entry. Section 5 studies the situation in which the entrant has some bargaining power vis-à-vis the buyer. There, we show that, first, entry may instead be restricted and second, that inflexible contracts can emerge as a response to the conflicting objectives of extracting information rent from the buyer and bargaining rent from the entrant. All proofs are relegated to an Appendix.",Contracts as a barrier to entry: Impact of Buyer’s asymmetric information and bargaining power,https://www.sciencedirect.com/science/article/pii/S0167718721000837,4 November 2021,2021,Research Article,92.0
"Argentesi Elena,Buccirossi Paolo,Cervone Roberto,Duso Tomaso,Marrazzo Alessia","University of Bologna, Department of Economics, Piazza Scaravilli 2, Bologna 40126, Italy,Lear, Via di Monserrato 48, Rome 00186, Italy,Ofcom, Riverside House, 2a Southwark Bridge Road, London SE1 9HA, United Kingdom,Deutsches Institut für Wirtschaftsforschung (DIW Berlin), Technische Universität (TU) Berlin, Berlin Centre for Consumer Policies (BCCP), CEPR, and CESIfo. Mohrenstr. 58, Berlin 10117, Germany,Lear, Via di Monserrato 48, Rome 00186, Italy","Received 6 January 2020, Revised 23 September 2021, Accepted 1 October 2021, Available online 16 October 2021, Version of Record 28 October 2021.",https://doi.org/10.1016/j.ijindorg.2021.102789,Cited by (1),"We study a merger between two Dutch supermarket chains to assess its effect on the depth as well as composition of assortment. We adopt a difference-in-differences strategy that exploits local variation in pre-merger competitive conditions and thus in the merger outcomes. To define our control group, we account for selection on observables through a matching procedure. We observe that, after the merger, the assortment of the merging parties converges in markets where they are not directly competing one with the other. Instead, the merging parties reposition their assortment to avoid cannibalization in the areas where they directly competed before the merger. While the target’s stores reduce the depth of their assortment when in direct competition with the acquirer’s, the latter increase their assortment. This suggests that variety is a strategic variable in retail chains’ response to changes in local competition.","In retail markets, firms can use various strategic choice variables, in addition to prices, to respond to local competitive conditions. Indeed, the 2010 revision of the US Horizontal Merger Guidelines emphasizes the importance of non-price dimensions of competition. However, the evaluation of such effects and their interaction with price effects is not straightforward and it remains a controversial issue in merger control (OECD, 2013). In particular, mergers’ effects on variety are ambiguous, as they may lead firms to spread similar products apart, to withdraw duplicative products, or to crowd products together to preempt entry” (Berry and Waldfogel, 2001, p. 1009).====We examine the impact of mergers on non-price strategies in grocery retail. This sector represents an ideal setting to study the role of assortment choices and product positioning for competition, especially in geographically localized markets. Assortment is an important tool to respond to changes in local competitive conditions, more so when retail chains have nationally uniform prices, which is documented by DellaVigna and Gentzkow (2019). Moreover, non-price attributes are an important determinant of consumers’ preference and customer satisfaction.====We analyze the occurrence of a national merger that, in addition to potential implications for competition at the national level, was expected to have a heterogenous impact on local markets due to the differences in pre-merger local market competitive conditions. The merger between two large Dutch supermarket chains – Jumbo and C1000 – was conditionally approved by the Dutch competition authority – Autoriteit Consument & Markt (ACM) – in 2012, subject to divestitures in some local markets. This merger is well suited to assess the competitive impact at the local level as, in some local areas, the merging parties were in direct competition pre-merger (overlap areas) while in other areas they were not (non-overlap areas).====We adopt a Difference-in-Differences (DiD) strategy that compares overlap and non-overlap areas to causally identify the effect of the merger at the local level. Our identification strategy relies on the proposition that the competitive effects of the merger are likely to be stronger in the former areas than in the latter ones, as, other things equal, only in overlap areas did the intensity of competition change because of the merger. By matching overlap and non-overlap areas with a propensity score procedure that is based on observable characteristics, we account for differences in demand and supply conditions across treated and non-treated areas. While this identification strategy based on local variation cannot identify the potential impact of the merger at the national level, it is appropriate for analyzing non-price dimensions of competitions such as assortment decisions, which are often made at the local level (Quan and Williams, 2018).====Focusing on a relatively small and homogeneous market, such as the Dutch grocery retailing, has the advantage of allowing the use of granular data on the location of stores and on the characteristics of local areas. We use a database that entails quarterly information on average prices as well as the number of products – variety – for 122 product categories sold in a sample of 124 stores of the merging parties and their main competitors located in different areas scattered across the Netherlands for the 2010–2013 period. These categories almost completely cover the space of grocery products offered in the country during the sample period. As commonly done in the literature on retail markets, we define variety as the depth of assortment, i.e. the number of stock keeping units (SKUs) sold in each product category (Ren et al., 2011).====We first provide evidence that variety decisions – thus also average category prices – appear to be made at the local level, while pricing decisions for individual products are not. We show that, following the merger, the merging parties' assortment seems to converge in non-overlap areas where the parties are not directly competing one we the other. This is consistent with the fact that, at the national level, the merged entity chooses to have a more uniform assortment after the merger in order to rationalize supply costs. However, in overlap areas, the merger led to an average increase in product variety as well as in average category prices if compared to counterfactual non-overlap areas, which would suggest a move toward a larger and more expensive assortment.====These average effects are however the result of two opposing forces. In overlap areas, the acquirer – Jumbo, the high-variety chain – substantially raised its assortment as well as its average category prices with respect to counterfactual areas. On the opposite, C1000 decreased its assortment while leaving the average category prices unchanged, which implies a move toward a smaller, but not cheaper, assortment. These results are driven by those stores that were not re-branded. Instead, C1000 stores that took the Jumbo insignia followed its pattern by substantially increasing their variety. Finally, the significant, yet much smaller, increase in the competitors’ variety coupled with no significant change in their average category prices, signals that they also strategically reacted to the merging parties’ repositioning, though by less.====We further qualify these findings through an event study analysis, which allows us to better compare the time evolution of variety and average category prices between treated and control areas. Differences in variety between overlap and non-overlap areas are small and follow a steady trend both for Jumbo and C1000 pre-merger. Instead, after the merger, variety in overlap areas substantially diverges from variety in non-overlap areas for both merging chains. In Jumbo’s stores, variety is significantly higher in overlap than in non-overlap areas after the merger. Instead, C1000 stores in overlap areas substantially reduced the depth of their assortment with respect to the counterfactual non-overlap stores. Moreover, it appears that these changes take a few quarters to materialize and then remain stable until the end of the sample period.====Thus, exploiting the heterogeneity of the pre-merger local competitive conditions and looking behind average effects allows us understanding the strategic effect of the merger at the local level. It led to a softening of competition through the repositioning of the assortment’s depth and composition in areas where the two different insignias were still competing for customers. This is consistent with theoretical findings that merging parties move away from each other in the product space to avoid cannibalization following a merger (Gandhi, Froeb, Tschantz, Werden, 2008, Mazzeo, 2003).====The paper is structured as follows. In Section 2, we summarize the relevant literature. In Section 3, we provide information on the Dutch grocery market and the merger under consideration. Section 4 describes the data. We present our econometric model in Section 5 and the empirical results in Section 6. Section 7 presents additional results and Section 8 concludes.",The effect of mergers on variety in grocery retailing,https://www.sciencedirect.com/science/article/pii/S0167718721000825,16 October 2021,2021,Research Article,93.0
Sim Kyoungbo,"Korea Development Institute, 263 Namsejong-ro, Sejong-si 30149, Korea","Received 7 October 2020, Revised 23 September 2021, Accepted 23 September 2021, Available online 8 October 2021, Version of Record 10 November 2021.",https://doi.org/10.1016/j.ijindorg.2021.102788,Cited by (0),"This paper studies how an inventor protects a “complex” innovation that involves multiple complementary components. Each component can be protected through either patent or secrecy protection, so that the entire innovation may be protected through a patent-secret combination. Potential entrants might acquire these components either through costly imitation or licensing. I find that, first, secrecy is optimal when the patent length is relatively short; otherwise, a patent-secret combination is optimal; second, the inventor is over-rewarded compared to an inventor with an innovation that is equivalent except that it involves only a single-component so that the entire innovation can be protected through either patent or secrecy protection; and, third, a policy that precludes the use of a patent-secret combination enhances allocative efficiency ex-post but may stifle R&D incentives ex-ante.","Firms protect their innovations not only through patents but also through secrecy.==== These two protection modes help firms appropriate returns from their innovations through different mechanisms. Patents grant patent holders the legal right to exclude others from using protected innovations for a fixed period of time but they also require full information disclosure. In contrast, trade secrets can be protected for an unlimited period of time and do not require any information disclosure but are vulnerable to independent reverse engineering.====In the intellectual property rights (IPR) literature, patents initially received far more attention than trade secrets. Subsequently, several authors highlighted trade secrets as an important appropriability method (e.g., Levin et al., 1987; and Cohen et al., 2000) and analyzed the secrecy mechanism as a viable alternative to patents (e.g., Friedman et al., 1991). However, most of this work does not consider the potential for the combined use of patents and trade secrets, which is the focus of this paper.====Filling this gap in the literature is important because IPRs are all about determining how much society should reward the inventor. Without considering the possibility that patents and trade secrets can be combined for a single innovation, a carefully chosen IPR policy may unintentionally over-reward the inventor. To this end, this paper considers a situation in which an inventor has a “complex” innovation that involves multiple complementary components. The inventor can protect each component through either a patent or secrecy; i.e., the innovation may be protected through the combined use of patents and trade secrets. In such a setting, I ask the following questions: Under which condition does the inventor opt for a patent-secret combination? Would the inventor of a complex innovation earn more than an inventor of a “simple” innovation when the complementary use of patents and trade secrets is feasible? (Here and hereafter a “simple innovation” refers to an innovation that is the same as its corresponding complex innovation in all respects, for examples aggregate imitation costs, profit levels, and so on, except that it involves only a single-component so that the entire innovation can be protected through either patent or secrecy protection.) Is restricting the inventor’s choice set to the binary one, either patenting or relying on secrecy, socially desirable?====Substantial anecdotal evidence suggests that patents and trade secrets are combined in many settings. Premarin, a Wyeth product (this company merged with Pfizer in 2009), has been the most popular hormone-therapy drug for menopausal women since 1942. In the early 1940s, Wyeth filed two patents to protect this innovation. However, the company effectively extended the duration of its exclusive position in the industry beyond the length of these patents. The reason for this success was that no rivals could uncover its closely secured trade secrets, more specifically the so-called Brandon Process: a secret chemical process for extracting conjugated estrogen from the urine of pregnant mares (Jorda, 2007; and Orly, 2013).====Jorda (2007) also attributes General Electric’s success story in industrial (or synthetic) diamonds to the use of patents in conjunction with trade secrets. GE patented part of its technology for making diamonds and, therefore, the disclosed information was publicly available to rival companies after its patents expired. However, as GE also closely kept the remaining part secret, it could maintain a dominant position in that market. Pan (2010) reports another example in Coskata, an Illinois-based energy company; the firm applied for patents to protect technology on a bio-reactor while keeping secret the identity of the micro-organism that was being fed into the bio-reactor. In addition, there are many other examples that show similar strategies in the food industry (see the introduction in Belleflamme and Bloch, 2013) and in the organic chemical industry in the 19th century (Arora, 1997). In fact, some patent attorneys advise their customers to use this type of strategy.====This paper first examines the best protection mode for an inventor who is facing potential imitators after having discovered a complex innovation. To this end, it is important to determine the inventor’s ability to appropriate returns from their innovation under each protection mode: the patent only mode, the secrecy only mode and the optimal patent-secret combination. My framework extends Henry and Ponce’s (2011) work to the context of complex innovations. Because these authors show a novel mechanism through which the inventor can use secrecy protection to delay the entry of imitators, building on their model allows one to endogenize the imitator’s entry decisions under a patent-secret combination and, thereby, develop a clearer and more detailed understanding of the equilibrium outcomes.====The first key finding of the paper is that secrecy only protection is optimal when the patent length is relatively short; otherwise, it is optimal to combine patents and trade secrets. To understand this result, it should be first noted that the patent only method is dominated by any patent-secret combination because the inventor can effectively extend the duration of their monopoly beyond the length of patents (====) by combining the two protection mechanisms; due to the perfect complementarity of the components, patenting components makes it infeasible for imitators to enter the market during the patent period, and keeping the remaining components secrecy delays entry after ====. Moreover, the optimal patent-secret combination requires patenting only the component with the minimal imitation cost because the duration of the prolonged monopoly is determined by the aggregate imitation costs of all of the components that are kept under secrecy. The optimal patent-secret combination is more advantageous than the secrecy only method in the sense that it takes advantage of the patent system, but at the same time it is less advantageous in the sense that it requires the inventor to disclose a part of the innovation, thereby increasing the rates of entry. Therefore, the optimal protection mode has a binary feature: depending on ====, choosing either the secrecy only method or the optimal patent-secret combination.====Next, this paper compares the optimal decision and the payoff to the inventor of a complex innovation (hereafter, the “complex inventor”) with that to another inventor of a simple innovation (hereafter, the “simple inventor”). ==== (i.e., with identical profitability and imitation costs), the complex inventor is more inclined to patent because they can take advantage of the patent system and only permit partial disclosure of the whole innovation. Moreover, the complex inventor is (weakly) over-rewarded compared to the simple inventor. When ==== is relatively short so that both inventors rely only on secrecy, they earn the same expected payoff regardless of whether the innovation can be fragmented, but if ==== falls into the region in which the complex inventor opts for the optimal patent-secret combination, this inventor earns strictly more than the simple inventor.====In addition to the positive analysis of ex-post innovation problem, this paper briefly demonstrates ex-ante and ex-post policy implications of imposing a “strict disclosure requirement” (hereafter, an SDR), under which the patent office turns down all patent applications that disclose only part of the innovation in compliance and, thus, the inventor cannot use a patent-secret combination. Consequently, when an SDR is imposed, the complex inventor’s payoff schedule converts to one that is exactly the same as that of the simple inventor. Whereas an SDR always enhances ex-post welfare, the ex-ante welfare implication is ambiguous. This is because the availability of the combined use of patents and trade secrets might boost the inventor’s ex-ante investment incentive.====: There is a large body of literature that views patents and trade secrets as mutually exclusive alternatives in a variety of contexts. Anton and Yao (2004) study the inventor’s choice between patenting and relying on secrecy, focusing on signaling (or strategic disclosure) aspects. Kultti et al. (2007) and Kwon (2012) consider a similar problem in a patent race model in which two or more inventors compete to develop the same innovation simultaneously, and they make some normative arguments regarding socially desirable patent policies. Denicolo and Franzoni (2004) focus on whether granting a prior user right to the first inventor (i.e., the subsequent inventor is able to patent and exclude the first inventor when the first inventor had resorted to trade secrets) increases social welfare. Erkal (2005) and Bhattacharya and Guriev (2006) consider a cumulative innovation framework. More specifically, Erkal (2005) considers two consecutive R&D races between two firms in which the winner of the first race has an option to protect their innovation either by patenting or through secrecy, and compares several patent policy regimes. Bhattacharya and Guriev (2006) build a model in which the upstream inventor has two potential licensees in the downstream market. They show which of the two protection modes is more efficient from the inventor and the social planner’s points of view. All of these papers offer important insights into understanding distinctions between patents and trade secrets as substitutes for each other but they neglect the possibility of the combined use of those two mechanisms.====Although there is a nascent strand of literature that explores the complementary use of patents and trade secrets, research papers on this topic are still sparse. Exceptions include Belleflamme and Bloch (2013), Ottoz and Cugno (2008) and Ottoz and Cugno (2011). The first two papers show that the complementary use of patents and trade secrets may provide optimal protection in different frameworks. However, my approach differs from theirs in two respects. First, the entry times of potential imitators are endogenously determined through strategic interactions among players, which implies that the strength of each protection mode is also endogenously determined. This approach is advantageous in that, for example, it allows one to get a clearer understanding of the mechanism through which a patent-secret combination can extend the monopoly period beyond the term of the patent. Second, I examine how the availability of a patent-secret combination affects welfare ex-ante as well as ex-post and when it has a negative impact on social welfare. Ottoz and Cugno (2011) touch on welfare issues that are associated with a patent-secret combination. However, their focus is on studying under which conditions a broad scope of trade secret law is desirable in terms of (ex-ante) social welfare, assuming that an innovation is protected by a specific patent-secret combination.====The remainder of the paper is organized as follows. In Section 2, the baseline model and assumptions are introduced. Section 3 provides the main analysis, including calculations of the inventor’s expected payoffs under each protection mode, derivations of the inventor’s optimal decision and a comparison of their payoff to the payoff of the simple inventor. In Section 4, I briefly discuss some of the modeling assumptions and policy implications regarding the SDR. Section 5 contains two possible extensions and checks the robustness of the main results, and the final section presents the conclusions.",Optimal use of patents and trade secrets for complex innovations,https://www.sciencedirect.com/science/article/pii/S0167718721000813,8 October 2021,2021,Research Article,94.0
"Morita Hodaka,Nguyen Xuan","Institute of Economic Research, Hitotsubashi University, Tokyo, Japan,Department of Economics, Faculty of Business and Law, Deakin University, Geelong, VIC, Australia","Received 11 March 2020, Revised 29 May 2021, Accepted 21 September 2021, Available online 4 October 2021, Version of Record 24 October 2021.",https://doi.org/10.1016/j.ijindorg.2021.102787,Cited by (6),"When Northern firms undertake FDI in the South, their superior technology spills over to Southern firms and enables Southern firms to enhance their product quality. This paper explores ","Foreign direct investment (FDI) induces technology spillovers, which often enhance local firms’ quality standards. That is, if a foreign firm builds its manufacturing plant in a less developed country, local competitors can enhance their product quality by learning from the foreign firm’s performance, or by employing workers from the foreign firm.==== This phenomenon is referred to as ==== throughout the paper. For example, the Chinese automaker Chery Automobile hired a number of engineers from the Nissan-Dongfeng joint venture, which was established upon Nissan’s FDI in China. The resulting technology spillovers through these engineers significantly improved Chery’s car quality (Luo, 2005). Similarly, the investment of U.S. software firms in Bangalore since 1984 has created technological and information externalities to Indian software firms. Consequently, this has enabled local firms to produce software meeting international standards (Patibandla, Kapur, Petersen, 2000, Pack, Saggi, 2006). In Appendix B, we present some real-world examples of quality-enhancing technology spillovers drawn from the Chinese automobile industry, the Chinese garment industry, and the Bangladesh garment industry.====Anticipating the potential benefits of technology spillovers, Southern governments often induce FDI in industries in which local firms need to learn advanced technologies and know-how from foreign firms. In the case of the Chinese automobile industry, the Chinese government imposed high tariffs on imports of foreign cars to induce foreign automakers to undertake FDI in China.==== In a similar move, the Indian government promoted FDI in the software sector by enforcing the Copyright Act. This strengthened intellectual property rights (IPR) protection for both local and foreign firms in India.==== These types of policies have proved to be successful in attracting FDI to industries in which local firms need to learn technologies from foreign firms.====Given the important interconnections among FDI, technology spillovers, and IPR protection, several papers have explored these interconnections for cost-reducing technology spillovers. However, to the best of our knowledge, no previous papers have analyzed them in the context of quality-enhancing technology spillovers. We aim to fill this gap in the literature. We explore a model that incorporates quality-enhancing technology spillovers in an international duopoly model of vertical product differentiation, in which we focus on market-seeking FDI, that is, FDI with the motive of serving particular markets through local production and distribution (UNCTAD, Nachum, Zaheer, 2005).====A Northern firm (firm ====) and a Southern firm (firm ====) compete in the Southern market, where firm ==== chooses either home production or FDI in the South. We demonstrate that when firm ==== undertakes FDI, it may reduce the quality level for its product to reduce the amount of technology that spills over to firm ====. Consequently, the equilibrium level of firm ====’s product quality can be lower under FDI than under home production.====We call the phenomenon in which firm ==== chooses a lower quality level under FDI than under home production firm ====’s ====, which happens under a broad range of parameterizations in our model. This is a new finding in the analyses of international oligopoly models, and is consistent with real-world observations. Maskus (2004) finds, in interviews with public officials, university scholars, and enterprise managers conducted in China in 1998 and 2001, that foreign companies may undertake defensive actions in the presence of weak IPR protection. In the interviews, nearly all managers of foreign enterprises indicated that in the past they transferred technologies that are at least five years behind global standards in the expectation that these technologies would be lost to local competition.====Mansfield and Romeo (1980) present their survey results of sixty-five cases involving technology transfer from U.S. firms to their foreign affiliates. They show that, on average, the U.S. firms transferred technologies that were 10 years old to developing countries. Nevertheless, firms in the host countries had benefitted from technology “leaked out” from U.S. firms’ affiliates. Mansfield and Romeo point out that technology spillovers from multinational firms to local firms often enhance local firms’ technological capabilities.====In our model, the Southern market consists of two types of consumers: high-valuation and low-valuation types. Each firm, ==== and ====, chooses its product quality, where the production cost is increasing in quality level. Firm ==== has a superior technology in the sense that it does not have a binding constraint on its choice of product quality, whereas firm ==== cannot choose the profit-maximizing quality level due to its technological limitation. Firm ==== is located in the South, while firm ==== can be located in the North (home production) or in the South (FDI). By undertaking FDI, firm ==== can avoid tariffs.==== However, a fraction of firm ====’s technology spills over to firm ==== under FDI, and the technology spillovers increase the highest possible level of quality firm ==== can choose for its product.====Firm ==== undertakes FDI when the rate of quality-enhancing technology spillovers, denoted by ====, is relatively small, and the Southern government can induce firm ====’s FDI by lowering ==== (strengthening IPR protection). We find that the Southern government can increase Southern welfare by doing so, but the induced FDI can reduce Southern consumer surplus under a range of parameterizations. This result is in contrast to the standard result that arises from an international Cournot duopoly model of cost-reducing technology spillovers, in which technology spillovers necessarily increase Southern consumer surplus by intensifying competition between Northern and Southern firms. Our finding, which arises from our focus on the quality-enhancing aspect of technology spillovers, suggests that when Southern governments formulate IPR protection and trade policy, they should carefully assess the impact of quality-enhancing technology spillovers accompanied by FDI.====As detailed in the next section, a number of papers have studied models of technology spillovers in North-South trade contexts in a partial equilibrium setting, in which spillovers reduce production costs. As the spillover rate increases, a higher fraction of Northern firms’ technology spills over to Southern firms but Northern firms have lower incentives to invest in their R&D activities. Similar trade-offs have also been studied under general equilibrium frameworks in which Northern firms innovate and Southern firms imitate. Thoenig and Verdier (2003), for example, explore a quality ladder model and show that North-South trade integration leads Northern firms to follow a defensive production strategy to avoid technology imitation by Southern firms. Specifically, Northern firms choose a technology that involves a greater share of tacit knowledge and non-codified know-how, such as the one that uses more skilled workers than non-skilled workers. Hence, in Thoenig and Verdier’s (2003) model, technology spillovers induce skill-biased innovation.====We contribute to the literature by exploring quality-enhancing technology spillovers under the vertical product differentiation model. The Northern firm can choose a suitable level of technology (product quality) without incurring innovation costs to adopt in its Southern affiliate. This modeling choice is consistent with the reality: when multinational firms set up their production facilities in developing countries, they often choose a certain technology from a set of available technologies that have already been developed (Mansfield and Romeo (1980); Maskus (2004); among others).====In our analysis, the trade-off regarding the spillover rate arises not from lower R&D incentives but from the Northern firm’s endogenous product quality choice upon FDI. An increase in the spillover rate increases the fraction of the Northern firm’s technology that spills over to the Southern firm, but may induce the Northern firm to reduce the level of its product quality. We show that this new trade-off is important when we consider welfare consequences and economic impacts of technology spillovers in North-South trade contexts.====The rest of the paper is organized as follows. Section 2 discusses our contributions to the literature. Section 3 presents our model and its equilibrium characterization. In Section 4, we first report the welfare implications of the different modes of expansion (HP versus FDI) with exogenous tariff and spillovers rates. We then examine effects of endogenous spillovers rates with the policy implications and effects of endogenous tariff rates. In Section 5, we first compare the effects of quality-enhancing technology spillovers in our model to the effects of cost-reducing technology spillovers in an international Cournot duopoly model. We then consider an extension of our model in which firm ==== can directly imitate (free-ride) partially the quality of firm ==== when it undertakes FDI, and also consider an extension of our model in which the Southern government can impose a requirement on firm N, if it seeks to conduct FDI, to partner with a domestic firm to form a joint venture. Section 6 offers concluding remarks.",FDI and quality-enhancing technology spillovers,https://www.sciencedirect.com/science/article/pii/S0167718721000801,4 October 2021,2021,Research Article,95.0
"Martin Simon,Shelegia Sandro","Düsseldorf Institute for Competition Economics (DICE), University of Düsseldorf, Germany,Universitat Pompeu Fabra, Barcelona School of Economics, and CEPR","Received 15 October 2019, Revised 7 May 2021, Accepted 31 August 2021, Available online 15 September 2021, Version of Record 25 September 2021.",https://doi.org/10.1016/j.ijindorg.2021.102775,Cited by (1),"We consider a signaling model capturing the introductory and the mature phase of a product. Information concerning product quality is transmitted between consumers through reviews, which partially depend on the expectations consumers had prior to their purchase. When future sales are sufficiently important, a novel tension arises: High-quality types may want to underpromise and overdeliver by imitating low types in order to get a better review. We show the existence of a Pareto-improving separating equilibrium. Both more informative reviews and price transparency can lead to higher prices. Our analysis reveals a new rationale for loss-leadership.","For many consumers it has become natural to gather information about products online prior to purchasing. The reviews of previous consumers are an especially important part of the available information. There is growing empirical evidence on the relationship between consumer reviews, demand, and revenue (Anderson, Magruder, 2012, Chevalier and Mayzlin, 2006, Dellarocas et al., 2007, Jin, Lee, Luca, et al., 2018, Liu, 2006, Luca, 2016, Reinstein, Snyder, 2005, Zhu, Zhang, 2010; see De Maeyer, 2012, and Magnani, 2020, for detailed surveys).====Therefore, it is increasingly the case that firms’ pricing and marketing choices need to take into account how consumers write reviews. There are many ways in which firms can influence reviews for their products or services. These practices are poorly understood theoretically. This is one of the first papers to model how consumers write reviews and how firms respond to the review creation process with their pricing strategies.====We start with the premise that consumer reviews depend on two main ingredients. The first is the ==== of a product, as is usually the case in the theoretical literature on the topic (see e.g., Jullien and Park, 2014). Additionally, reviews also convey how the product fares relative to the consumers’ ====, which may depend on the price paid and other marketing-related activities. For instance, Luca and Reshef (2020) show that a restaurant’s price increase of 1% decreases average ratings by 3–5%. Hui et al., 2021 find that consumers are more likely to create reviews when their expectations were not fulfilled. The incorporation of expectations in the review creation process is also motivated by anecdotal evidence that expectations matter for reviews. Many “5-star” reviews on websites such as Amazon.com do not try to say that this is the best product one could have purchased for the need, but that it is very good for the price (and thus relative to the prior expectation). Similarly, otherwise well-functioning products frequently get 1-star reviews not because they are bad, but simply because they fall short of very high expectations commanded by a high price.====Starting with this novel view of review creation, we model a monopolist who introduces an experience good. Quality to consumers consists of two parts ==== and ====. The first part ==== is known to the firm, whereas the extra part ==== is not. Initially, neither is known to consumers. The monopolist’s pricing choice influences the quality expectations of the consumers and hence the introductory-period demand. Introductory-period consumers create a review after consumption. Naturally, a good review is more likely if the true quality is high, but less likely if prior expectations are high since consumers are less likely to be satisfied with the product. Post-introductory period consumers observe the review, but do not observe the introductory period prices, and update their beliefs about product quality accordingly.====Our main objects of interest are introductory+period prices conditional on the firm-known quality component ====. We derive a separating equilibrium which perfectly reveals the product quality ==== to introductory-period consumers. This equilibrium is supported by the prospect of favorable reviews, which are beneficial to firms and can be shaped by the expectations the firm induces in the first period. As long as there is some degree of uncertainty concerning the extra quality component ==== in the mature phase, our reviews have this property. Hence, the presence of expectations-depending reviews and a post-introductory period creates endogenous incentives to obtain a good review in the introductory period.====The monopolist faces a novel trade-off in the choice of the introductory-period price. Charging a higher price increases introductory-period profit but also leads to higher expectations. When the product inevitably falls short of these elevated expectations, early consumers are likely to respond with a bad review, which reduces post-introductory period profits. This tension is common to many signaling models, but in our model there is an additional incentive to underpromise and overdeliver since reviews also alter the profitability of downward deviations. When a high-quality firm deviates to below its equilibrium price, it suffers introductory-period loss. But, by inducing lower expectations upon which it is bound to overdeliver, it also increases the probability of a good review and thus higher profits in the future. Our separating equilibrium guarantees that deviations in either direction are not profitable. Equilibrium prices are increasing in quality and depend on the price premium for a good review, which in turn depends on review informativeness. Importantly, we find that reviews that depend on expectations lead to a Pareto improvement.====Furthermore, we offer a novel explanation for loss leadership (Farrell and Klemperer, 2007, Lal and Matutes, 1994): Introductory prices can be below marginal cost for low-quality types, particularly when the mature phase is relatively important. High types are tempted to cut their price in the first period in order to induce a good review and capitalize in the mature phase, which can only be mitigated by below-cost pricing by low types. The low types are willing to suffer first-period losses because deviation to higher (positive) prices jeopardizes good reviews and thus mature-phase profits.====The separating equilibrium of the introductory phase has several interesting features. Equilibrium prices are non-monotone in the mature-phase market size ====. For an initially low ====, increasing ==== results in higher first-period prices, whereas the opposite is true when ==== is already high. The reason is the following. When ==== is low, imitation of low types is deterred by high types charging below-monopoly prices. The lowest quality type does not distort prices at all. When the market size ==== increases, low types’ deviations are less profitable since there is stronger punishment through bad reviews. This allows high types to charge higher prices. Exactly the opposite applies when ==== is initially high. In that case price setting is primarily constrained by the incentives of high types to underpromise and overdeliver. There is no distortion for the highest quality type and low-quality types charge below-monopoly prices in order to deter deviations by high types. Thus, prices are maximized for an intermediate level of ====, where all types charge the full information monopoly price.====Our framework lends itself naturally to exploring the relationship between platform design and prices. In particular, we first study the effect of review informativeness on introductory-period prices. This can be influenced by review aggregation platforms by asking consumers specifically about certain features when creating a review. Again, we find a non-monotone relationship. When informativeness increases from initially low levels, introductory prices increase. The disciplinary force of bad reviews is relatively weak, so high types need to charge relatively low prices in order to deter low types from imitation. As informativeness increases, reviews increasingly substitute as an imitation deterrent, allowing high types to charge higher prices. Conversely, increasing informativeness from already high levels leads to lower introductory prices because, in that case, a good review is very profitable for the firm. Hence, imitation of high types needs to be deterred through the competitive pricing of low types, and more so the more informative the reviews are. Thus, for platforms that make profits proportional to firms’ profits, there is an inherent barrier to the maximal amount of information these platforms want to allow to be revealed.====The second platform design aspect we study concerns the effect of price transparency, i.e., allowing second-period consumers to observe first-period prices on top of the review. We find that this may be detrimental to consumers and may lead to higher prices, in particular when the mass of second-period consumers ==== is high. In that case, price setting is constrained by high types who may want to underpromise and overdeliver. On a more transparent platform where also first-period prices are transmitted, there is less temptation to deviate downwards, allowing low types to charge higher prices.====In the next subsection, we show how our paper relates to the existing literature. In Section 2 we describe our main model, which we analyze in Section 3. We study implications for platform design in Section 4. We consider a number of extensions in Section 5, where we relax our baseline assumption that all quality types have the same marginal costs and show that qualitative results of the model are unaltered. We also establish the robustness of our main results to richer review information structures and show that the incentive to underpromise and overdeliver persists in multiple-periods settings. Finally, we conclude in Section 6.",Underpromise and overdeliver? - Online product reviews and firm pricing,https://www.sciencedirect.com/science/article/pii/S0167718721000680,15 September 2021,2021,Research Article,96.0
"Herz Benedikt,Mejer Malwina","European Commission, Brussels 1049, Belgium","Received 25 November 2020, Revised 7 July 2021, Accepted 24 August 2021, Available online 14 September 2021, Version of Record 13 October 2021.",https://doi.org/10.1016/j.ijindorg.2021.102776,Cited by (1),"The design right is a widely used but poorly understood intellectual property right that allows the protection of products’ aesthetics and outer appearances. We study the influence of design protection on price by exploiting cross-country differences in the scope of protection in the European automotive spare parts market: In some countries, repair parts are exempted from design protection, while in others they are not. Based on detailed price data, our difference-in-differences estimates imply that design protection increases prices by about 5–8%, with large differences between carmakers. We then link our findings to the literature on deviations from the ====. We document large cross-country price deviations for identical spare parts and provide evidence that a part of these price deviations can be explained by the lack of harmonization of design protection in combination with carmakers’ pricing-to-market strategies.","In 2018, Apple and Samsung finally decided to settle their seven-year dispute over Apple's allegations that Samsung “slavishly” copied designs related to the iPhone and iPad (====, 2012; Nellis, 2018). This and other high-stakes court cases show that product form and design have become increasingly important for the development of new products and that companies are willing to invest substantial resources into defending related intellectual property (IP).==== It is therefore surprising that – while patents, copyrights, and trademarks have been extensively studied in the economic literature – the industrial design right has for the most part escaped the attention of economists.====The industrial design right (hereafter, “design right”) is an IP right that protects the appearance of a product with the aim of promoting esthetic innovation and product differentiation. The owner can prevent third parties from making, selling, or importing articles bearing the protected design for commercial purposes.==== Many industries ranging from automotive through clothing, footwear, and sports goods to furniture rely on design protection.==== Such protection is especially important for electronic devices, including smartphones, with Samsung, Apple, LG, and Philips ranking among the top ten applicants in Europe and the United States.==== The number of design right applications is on the rise, reaching over 1.02 million filings globally in 2018 (WIPO, 2019).====Despite its ubiquity, the design right is not harmonized across jurisdictions: While WTO members agreed on some minimum requirements under the TRIPS agreement, there is no consensus on the scope of protection.==== Because of its implications for the economically important automotive spare parts market, an especially contentious question is whether protecting the design of a component part separately from the product into which it is embedded should be possible.====To inform the debate, it is important to understand the effect of design protection on price. In defining the optimal scope of protection, policy makers are facing a trade-off: The broader the scope, the more right owners can increase the prices of protected goods, which might increase profits and therefore incentivize innovation. However, higher prices might also reduce consumer surplus and prevent the creation of follow-on products and innovation.====A strong price effect would also imply that the lack of harmonization might have broader implications. Variation in the scope of design protection might translate into cross-country price dispersion for identical goods and might therefore offer (a partial) explanation for deviations from the law of one price (LOOP) for identical products, a phenomenon that has been widely documented in the economic literature.====Despite the centrality of the question, no empirical evidence exists on the influence of design protection on price. One potential reason for this lack of evidence is that the identification of causal effects is complicated because ownership of the respective right is endogenous to pricing decisions.==== For example, products with greater commercial appeal are more likely to be protected by a design title, but are also likely to be offered at a higher price. In addition, to control for unobservable factors, in an ideal setting one would like to compare prices of identical goods with and without design protection. However, this is difficult given the lack of experimental variation.====In this article, we address these challenges to identification by taking advantage of cross-country differences in the scope of design protection in Europe. We focus on the European market for (visible) automotive spare parts. As of 2018, in 18 European Union (EU) member states, car manufacturers can protect ==== spare parts using design rights and therefore prohibit the production and import of identical parts by independent manufacturers. In these countries, customers must therefore purchase visible repair parts exclusively from original manufacturers or their suppliers. In the remaining ten EU member states, national design law specifically excludes visible spare parts from this protection via a so-called “repair clause,” enabling competition from independent manufacturers. We use a regression model to compare the prices of identical spare parts between countries with and without a repair clause.====Our research is based on a novel data set that contains the pre-tax prices of 12 types of spare parts for 60 car models from 2001 to 2016 in 16 EU member states plus Norway and Switzerland. While the last two are not EU members, they nevertheless participate in the EU internal market.==== A key feature of our data is that spare part prices are listed by car model. This allows us to make cross-country price comparisons between exactly defined products. We can, for example, compare the price of a windscreen for a ==== between Germany and the UK in 2016. To address the potential concerns of omitted variable bias, we also provide difference-in-differences estimates exploiting the fact that the radiator, a component part inside the vehicle, is not a ==== spare part and therefore not subject to the repair clause, regardless of the jurisdiction.====We show that design protection increases the prices of visible spare parts on average by about 5–8%, depending on the empirical specification. Based on these estimates, a back-of-the-envelope calculation suggests that an EU-wide repair clause would save EU consumers between 450 and 720 million Euros annually on the purchase of visible automotive spare parts alone. While we find no differences across vehicle sizes, we find that estimated price effects vary substantially between carmakers, suggesting differences in the degree to which manufacturers exploit design protection in their pricing strategies. We discuss this finding in particular in the context of the 2018 press reports that revealed that several major carmakers used an algorithmic pricing software (Partneo) to identify the maximum price that consumers would be willing to pay for a spare part. Strikingly, we find the strongest effect of design protection on pricing for the car manufacturers in our sample that are known to have used the pricing software.====Our findings have important implications for the literature that studies deviations from the LOOP. We show that prices of automotive spare parts differ very substantially within the EU internal market. In 2016, the headlamp for a ==== was 30% more expensive in France than in the UK. The windscreen for a ==== was 42% more expensive in Germany compared to Spain. This complements research by others that documents large price dispersions for identical goods in the highly integrated EU internal market where formal trade barriers were abolished a long time ago and many countries share the same currency.====We argue that design protection contributes to cross-country price dispersion. Whether such a link exists is ex-ante unclear and therefore an empirical question. In countries where visible spare parts are design protected (i.e., where no repair clause exists) there is no competition from independent spare parts manufacturers that might “arbitrage away” cross-country price differences. Carmakers can therefore make better use of pricing-to-market strategies, for example, by conditioning spare part prices on purchasing power, fuel taxes, or climate conditions. However, the effect of design protection on price dispersion also depends on whether design protection of visible parts is predominantly permitted in countries with relatively low or high price levels.====We provide evidence that design protection indeed increases cross-country price dispersion. Our evidence is based on two empirical tests. First, as described above, our findings suggest that carmakers take into account design protection in their pricing strategies to varying degrees. A testable implication is therefore that price dispersion should be largest for those carmakers that make most use of such strategies. We show that this in indeed the case in our data and that this relation only holds when price dispersion is calculated in the sample of countries where visible spare parts can be design protected (i.e., where independent spare part manufacturers cannot compete). Second, based on a difference-in-differences estimation approach, we find that the price dispersion of identical visible spare parts is relatively larger across the set of countries that permit design protection for visible parts compared to countries that do not permit protection. Strikingly, no such difference is apparent when considering the radiator, a spare part not subject to the repair clause, regardless of the jurisdiction. Both tests suggest quantitatively important effects.====This article makes several important contributions to the economic literature. A large literature in economics studies IP rights and their implications on market structure and price. The effects of patent protection are well understood, especially in the context of generic entry after patent expiry in the pharmaceutical industry. A consistent finding is that the loss of exclusivity leads prices drop by 40–50% (e.g., Scherer, 2000; Castanheira et al., 2019; European Commission, 2019). For copyright, Li et al. (2018) exploit a differential increase in the copyright length of books by dead authors in Britain in 1814. They find a substantial effect on price, probably because of publishers’ improved ability to perform intertemporal price discrimination. Reimers (2019) identifies a positive effect of copyright on prices by exploiting an abrupt change in copyright protection in the year 1923.==== We complement this literature by documenting a substantial price effect for the design right. As we argue in more detail below, our finding is especially important since the optimal scope of design protection currently attracts substantial policy interest. Our article also differs methodologically from existing studies because we show ==== price effects for ====.====Second, we complement a small literature that, based on survey evidence, finds that design rights only play a minor role in appropriating returns from innovations. For example, Blind et al., (2006) documents that patenting German firms regard design rights as the least important protection instrument. Arundel (2001), Moultrie and Livesey (2014), and Lim et al. (2014) report similar findings.==== By contrast, the price effects that we find indicate that – at least in the automotive industry – design rights can play a substantial role in appropriating returns from innovations.====Third, we contribute to the large literature on deviations from the LOOP. It has been shown that international borders and even regional borders have a surprisingly strong effect on price dispersion (Engel and Rogers, 1996; Ceglowski, 2003). Price differentials have been attributed to differences in distribution costs (Crucini and Shintani, 2008), differences in currencies (Cavallo et al., 2014), and to strategic pricing and varying mark-ups (Haskel and Wolf, 2001; Simonovska, 2015).==== Price differentials in the European car market are especially well documented: Large price differences persist despite the EU heavily promoting the integration of the market (Verboven, 1996; Goldberg and Verboven, 2004, 2005). In recent research, Dvir and Strasser (2018) find that active pricing-to-market strategies (e.g., based on differences in climate or fuel taxes) might explain some of the differences.====We provide further empirical evidence of large and persistent price differences for homogenous, narrowly defined products in the highly integrated EU market. Unlike the existing literature, we identify a specific regulatory difference that affects competition in markets and therefore causes price dispersion. Our results therefore lead to the general conclusion that small regulatory differences across markets can contribute to quantitatively important and persistent deviations from the LOOP. As noted by Goldberg and Verboven (2005), in-depth analyses of particular markets can therefore greatly help – and might even be indispensable – to improve our understanding of what factors can explain the sustained price dispersion of homogeneous products in integrated markets.====Fourth, we are the first to show clear empirical evidence of a link between the price dispersion of homogeneous products and the scope of IP right protection. Our article thus connects the literature on price differences for homogenous products with a literature that studies the fragmentation of IP rights systems in Europe. Examples are van Pottelsberghe and Mejer (2010), who document the costs of the fragmented European patent system, as well as Herz and Mejer (2019) and Beukel et al. (2017), who study the effects of the partial harmonization of the EU system for trademarks and designs, respectively. While the existing literature mostly focuses on the effect of fragmentation on the administrative costs for applicants, we provide evidence of the effect of the fragmentation of IP rights systems on product market outcomes.====Finally, this article has important policy implications, because it contributes to the contentious debate on whether to exempt spare parts from design protection.==== In the EU, during the last three decades, the European Commission made three legislative attempts to harmonize this issue, but without success.==== On the national level, France and Germany have recently been working on legislative proposals to introduce a repair clause for visible spare parts into their national design laws. Similar initiatives are underway outside of the EU, for example, in the United States and Brazil.====The remainder of this article is structured as follows. Section 2 briefly describes the institutional background of design protection and the repair clause in the EU with a special emphasis on the automotive aftermarket. In Section 3, we present the data. In Section 4, we show that design protection leads to higher prices for visible spare parts and that car manufacturers differ in their pricing strategies. In Section 5, we document that substantial deviations from the LOOP exist for identical spare parts and provide evidence that part of these deviations are driven by the lack of harmonization of design protection in combination with manufacturers’ strategic pricing. In Section 6, we conclude and discuss policy implications.",The effect of design protection on price and price dispersion: Evidence from automotive spare parts,https://www.sciencedirect.com/science/article/pii/S0167718721000692,14 September 2021,2021,Research Article,97.0
"Motta Massimo,Tarantino Emanuele","ICREA-Universitat Pompeu Fabra and Barcelona School of Economics;, Ramon Trias-Fargas 25-27, 08005 Barcelona, Spain,LUISS Department of Economics and Finance, Viale Romania 24, 00124 Rome, Italy","Received 8 April 2020, Revised 29 July 2021, Accepted 31 July 2021, Available online 2 August 2021, Version of Record 9 September 2021.",https://doi.org/10.1016/j.ijindorg.2021.102774,Cited by (12),"Motivated by a number of high-profile antitrust cases, we study ==== when firms offer differentiated products and compete in prices and investments. Since the net effect of the merger is a priori ambiguous, we use aggregative ==== to sign it: we find that absent efficiency gains, the merger always reduces total investments and ====. We also prove that there exist classes of models for which the results obtained with cost-reducing investments are equivalent to those with quality-enhancing investments.","In a series of recent high-profile mergers in the mobile telephony industry in the EU,==== the telecom industry urged the European Commission (which had jurisdiction on these mergers) to take into account that the mergers would have led to higher investments.==== Mobile Network Operators (MNOs) have made two main arguments in support of this claim. The first is related to existence of scale economies of various nature, and as such it is not conceptually controversial (but it would need to be empirically verified). The second argues that a merger favors investments because industry consolidation would give firms stronger incentives to invest. This argument in particular has resonated with politicians and heads of government, and has been widely discussed in the press.==== Whether mergers encourage or not investment and innovation is an issue which goes well beyond the telecom industry. Antitrust agencies all over the world, for instance, recognize the importance of assessing the dynamic effects of a merger and the possibility that it may reduce innovation and product variety.====This paper studies the competitive effects of horizontal mergers in a context where firms compete both in prices and in investment levels. To our knowledge, and quite surprisingly, despite the intense policy debate, there has been little work (that we shall refer to below) on this issue. Of course, there exists a wide literature on the related issue of the effects of competition in general on investments and on innovations.==== However, this literature analyses what happens to investments when some proxy variable for competition intensifies or relaxes symmetrically for all firms, whereas we explicitly study the effect of a merger, which is an inherently asymmetric change: two firms combine their assets whereas the competitive environment (for instance the toughness of competition or the extent of product differentiation) is otherwise the same. Apart from capturing the nature of a merger, our model also allows to uncover the different effects that a merger has on insiders and outsiders, as well as on its overall competitive impact (what is the effect on consumers?), a question which is less relevant in a literature that focuses on how investment and R&D effort react to a symmetric shock to competition (consumers typically benefit/suffer as competition intensifies/softens).====As is standard, in the benchmark case - that is, absent the merger -, firms sell one product. The merger will create a new multi-product firm which owns two product varieties, thereby breaking the symmetry in the industry. To model investments, in our base model we follow the literature on process innovation (among others, Dasgupta, Stiglitz, 1980, Spence, 1984, Bester, Petrakis, 1993, Qiu, 1997, López, Vives, 2016), and assume that (====) firms compete simultaneously on investments and in the product market,==== and (====) investments reduce the cost of production. We then extend this framework to separately consider (====) a sequential model in which firms first set investments and then prices, and (====) product innovation, i.e., a setup where investments increase product quality.====Our analysis suggests that, under no (or, by continuity, weak enough) efficiency savings, a merger will reduce aggregate investments and harm consumers. Interestingly, this net effect will be the result of the decrease in investment and rise in prices on the side of the merging parties (the insiders), and the increase in investments, with prices which may either increase or decrease on the side of the outsiders to the merger. These outcomes confirm the result that a merger harms consumers unless there are sufficiently strong efficiency gains in a multi-dimensional setting where firms decide on investments and prices.====Let us now be more specific about what we do in the paper. In the model with simultaneous price and cost-reducing investment decisions, absent efficiency gains, the merger results in the insiders raising their prices and reducing their investments. This is ultimately due to the margin-expansion effect of the merger: the merged entity internalizes that a price decrease in one of its products will reduce the demand of the other product it sells, and this determines an upward pressure in prices relative to the benchmark where all firms are independent. In turn, higher prices will lead to a lower quantity sold by the insiders, and a lower marginal revenue from investing for the insiders, whose investments will therefore decrease.====In standard models of mergers with price-setting firms, constant marginal costs, and where investments are not considered, outsiders’ prices also increase due to strategic complementarity. In our model with dynamic efficiencies, however, this property does not necessarily hold, which makes the analysis of the effects of the merger far less straightforward. The key point is that, when the insiders increase their prices, this will tend to increase outsiders’ prices as well. But since outsiders’ prices increase less, their demand tends to increase (their market share will rise for sure), and this will increase their investment levels. That is, two different effects are at work: one which tends to increase outsiders’ prices, and the other, through lower costs, which tends to decrease them. At equilibrium, outsiders’ prices may either increase or decrease, and indeed we shall show that either outcome may arise according to the assumptions made on consumer demand.====The fact that one cannot be sure any longer of the effect of the merger on outsiders’ prices also implies that one is not able to establish the effect of the merger on consumers in general. Not only insiders’ prices go up and outsiders’ prices may go down, but also, given that products are differentiated, one cannot readily sum up the effects of possibly different signs to find the aggregate ‘net effect’. Then we ask: under which conditions can we establish the impact of the merger? To answer this question, we first show that one can reduce the dimensionality of the problem by restricting attention to one decision variable (prices) rather than two (prices and investments). In turn, this allows us to make use of recent developments in oligopoly theory, and reformulate the model using aggregative game theory - which is possible whenever the payoff of a player depends on its own action and an additively separable aggregate of all players’ actions (Selten, 1970).====By doing this, we establish that - absent efficiency gains - the merger has a negative impact on consumer surplus.==== Specifically, this holds for the demand functions that satisfy the Independence of Irrelevant Alternatives (IIA), property, like the CES and the logit demand models, which are commonly used for merger simulations. We also show it holds in standard parametric product differentiation models - such as the Shubik-Levitan and the Salop circle models - which do not satisfy the IIA property. We then find a sufficient condition for which the merger decreases total investments. It is satisfied by logit and Shubik-Levitan demand systems.====We extend our analysis in two directions. While in the main model we follow the literature and look at simultaneous (or unobservable) investments, in our first extension we consider a model of sequential investments in which firms first invest, their choice is observed by all, and then choose prices. The (well-known) presence of strategic effects inherent to sequential settings makes it difficult to establish propositions of general validity about the effects of the merger. Moreover, an aggregative game theory formulation is not generally possible for the sequential game. Nonetheless, the analysis of parametric models confirms the qualitative results found for the simultaneous moves case: the merger harms consumers; it increases prices and decreases investments of the insiders; it increases investments of the outsiders; and it may either decrease or increase outsiders’ prices.====Next, we study the effects of a merger in a product-innovation model where firms undertake quality-enhancing investments. Within the general setting, the merger gives rise to a trade-off. On top of the margin-expansion effect that arises with cost-reducing investments, product innovation also gives rise to a demand-expansion effect (see Jullien and Lefouili, 2018). More specifically, on the one hand, the merged entity will internalize the fact that increasing the quality of one product will reduce attractiveness (and profits) of its other product, and this reduces its incentive to invest; on the other, by raising prices the merger will increase the marginal profitability of investments. Again, the question is whether we can find natural conditions such that the results of the analysis are unambiguous. To this end, we prove that there exist two broad classes of models where quality-enhancing investments are equivalent to cost-reducing ones, and which therefore give rise to the same results as discussed above. Importantly, these models can accommodate standard demand functions like logit, CES and Shubik-Levitan, on top of popular vertical product differentiation models.====Let us now mention the relationship between our paper and related branches of the literature.==== After writing this paper, others have studied the relationship between mergers and investments.==== Federico et al. (2018) compute numerically the total effect of a four-to-three merger in an industry characterised by linear demand and stochastic innovations.==== Bourreau and Jullien (2018) propose a product-innovation model in which, by expanding consumer demand, a merger to monopoly can be procompetitive. Unlike our paper, none of these study a general demand framework allowing for differentiated products with both cost-reducing and quality-increasing investments (and possibly asymmetric demand).==== For a survey on this recent literature, see Jullien and Lefouili (2018).====After reducing the dimensionality of the problem, our base model might also be interpreted as one where differentiated firms compete in prices and have decreasing marginal costs. The standard reference for models of mergers under price competition is Deneckere and Davidson (1985). However, they assume constant marginal costs, so when (absent efficiency gains) insiders raise prices, outsiders will increase prices too, unambiguously resulting in lower consumer surplus. As explained above, with decreasing marginal costs the overall effect of an increase in the merging firms’ prices is of ambiguous sign in principle. By relying on the aggregative game theory we can show that the net effect of the merger is anticompetitive, even though buyers of outsiders’ products may be better off. We are not aware of other models of mergers under decreasing marginal cost functions. Farrell and Shapiro (1990) propose a Cournot model with homogeneous goods and ==== marginal costs, and also establish that, absent efficiencies, mergers are anticompetitive.====As for the empirical literature on the effects of mergers on investments, it is also quite scant, and does not offer clear insights on what are the likely effects of the merger.==== Of course, there is also a large empirical literature on how competition impacts upon innovations, investments and productivity,==== but again a merger is not tantamount to a general shift in the competitive pressure in a sector.====The paper continues thus. Section 2 studies the effects of the merger within a simultaneous moves model with cost-reducing investments. In Section 3, we extend the analysis by considering a sequential moves game and quality-enhancing investments. Section 4 concludes.","The effect of horizontal mergers, when firms compete in prices and investments",https://www.sciencedirect.com/science/article/pii/S0167718721000679,2 August 2021,2021,Research Article,98.0
"Benndorf Volker,Odenkirchen Johannes","Goethe-Universität Frankfurt, Theodor-W.-Adorno-Platz 4, Frankfurt 60323, Germany,Düsseldorf Institute for Competition Economics (DICE), Heinrich-Heine-Universität Düsseldorf, Universitätsstr. 1, Düsseldorf 40225, Germany","Received 10 September 2020, Revised 15 July 2021, Accepted 16 July 2021, Available online 17 July 2021, Version of Record 25 July 2021.",https://doi.org/10.1016/j.ijindorg.2021.102773,Cited by (4),"We examine coordinated and unilateral effects of horizontal partial cross-ownership (PCO) in a laboratory experiment. We consider homogeneous Bertrand markets where firms have symmetric, non-controlling shares of each other, and conduct the experiment with both stranger and partner matching. The partner data (repeated game) confirm the prediction that firms are more (tacitly) collusive with PCO than without. In the stranger data (one-shot game), average prices are increasing with higher degrees of PCO. This is inconsistent with rather extreme Nash predictions for this setup. We show that in a Quantal Response Equilibrium firms’ incentives to compete are reduced with passive PCO. QRE predictions explain the data from the stranger treatment well.","Minority shareholdings have recently received much attention by scholars, competition authorities, and organizations. For example, Posner et al. (2017) argue that the U.S. should introduce a public enforcement policy of the Clayton Act to mitigate anti-competitive effects of minority shareholdings. At the same time, the authors also warn that regulation needs to minimize the resulting disruptions to equity markets. In Europe, the European Commission released two papers exploring a stricter regulation of non-controlling shareholdings, and it has expressed concerns about the anti-competitive effects of such links in previous merger cases.==== However, the responsible commissioner also voiced reservations concerning novel legislation as it might hinder companies’ business interests.====Both cases suggest that it is not clear whether the anti-competitive effects of non-controlling investments are strong enough to warrant novel legislation. The point at issue is that novel regulation of minority shareholdings needs to consider both positive and negative effects of non-controlling investments. The upsides of minority shareholdings were the subject of a recent report by the OECD (2017). They include efficiency gains, diversification of risks, reinforcement of business relationships, and access to new markets and technologies. The downside of non-controlling investments is that there may be anti-competitive effects. So far, such effects have been addressed in theoretical and empirical studies.====The present article provides experimental evidence of the anti-competitive effects of minority shareholdings between direct competitors. We use a simple version of a model by Gilo et al. (2006) and derive theoretical predictions for the one-shot base game as well as the infinitely-repeated game. In the experiments, we consider several degrees of ==== (PCO) in both environments. The one-shot predictions are tested using stranger matching, and a partner-matching design with indefinitely many periods tackles the repeated game. This enables us to make ceteris paribus comparisons and clearly distinguish between coordinated and unilateral effects. To our best knowledge, we are the first to experimentally examine the impact of horizontal (passive) partial cross-ownership in oligopolistic markets.====We find strong evidence that partial cross-ownership may induce substantial coordinated and unilateral anti-competitive effects. Average selling prices are positively correlated with the degree of partial cross-ownership in the stranger treatment and in the partner treatment. In the partner treatment, the price increases are primarily driven by collusive behavior. In the stranger treatment, the price increases are attenuated because collusive behavior does not play a role. However, the price levels observed in the stranger design are higher the higher the degree of cross-ownership. This finding is inconsistent with the Nash predictions for the one-shot game, but is well predicted by Quantal Response Equilibrium (McKelvey and Palfrey, 1995).====We conclude that PCO reduces the incentives to compete and favors unilateral effects, which result in higher average prices. Tacit collusion, facilitated by partial cross-ownership, can further increase these unilateral effects but is not necessary for higher prices in this setup. Hence, the experimental data adds to the evidence on the negative repercussions of passive minority shareholdings for consumers and its importance for competition policy.====The remainder of this paper is structured as follows. Section 2 discusses some practical aspects of minority shareholdings and gives more background information. Section 3 summarizes the related literature. In Section 4, we first present the model our experiment is based on, and then discuss the theoretical predictions. Section 5 gives an overview of our experimental design and procedures. The results are shown in Section 6. In Section 7, we provide a power analysis, and Section 8 concludes.",An experiment on partial cross-ownership in oligopolistic markets,https://www.sciencedirect.com/science/article/pii/S0167718721000667,17 July 2021,2021,Research Article,99.0
"Aguiar Luis,Waldfogel Joel,Waldfogel Sarah","University of Zurich, Switzerland,Carlson School of Management, Department of Economics, University of Minnesota and NBER, USA,University of Wisconsin, USA","Received 20 December 2019, Revised 28 June 2021, Accepted 6 July 2021, Available online 7 July 2021, Version of Record 20 July 2021.",https://doi.org/10.1016/j.ijindorg.2021.102765,Cited by (8),"Platforms are growing increasingly powerful, raising questions about whether their power might be exercised with bias. While bias is inherently difficult to measure, we identify a context within the music ","In the past few decades, many markets have become dominated by platforms that enjoy increasing returns, giving them power and the possibility of abusing it. Prominent examples include Google, whose search algorithm is alleged to favor properties in which it has a financial interest (Edelman, 2011) and Amazon, which may use its internal data to disadvantage its suppliers (Zhu and Liu, 2018). The music industry, too, is now dominated by a small number of platforms, chiefly Spotify and Apple Music, whose power has invited scrutiny about possible bias.====At the same time, digitization has brought an explosion of new creative production; and large libraries of new and old songs are now directly available to consumers. For example, Spotify has over 40 million songs and adds about 40,000 per week. In this environment, platforms such as Spotify and Apple Music – through the songs they choose to promote – have substantial power to determine which songs and artists succeed. The main mechanism for promotion on these platforms is “playlists,” which are both informative lists of songs as well as utilities for listening to these songs. Playlists can have many followers, and placement on influential lists has large impacts on artist and song success (Aguiar and Waldfogel, forthcoming).====Both theoretical considerations, as well as perspectives of industry participants, provide reasons for concern that such power might be exercised with bias (Hagiu, Jullien, 2011, Bourreau and Gaudin, 2018). First, some upstream suppliers – the “major” record labels – have ownership stakes in Spotify, which could give Spotify a reason to provide more advantageous promotion of their products. In parallel, representatives of independent record labels have voiced concern about bias against their products (Dredge, 2016, Dredge, 2015). Industry participants have also raised concerns about platform roles in the music industry success of women. Although it is not clear a priori why platforms would have a financial incentive to engage in gender bias, there is reason for concern, particularly in the wake of the #metoo movement.====Assessing these claims requires a method for measuring platform bias, which is inherently difficult. One could test for bias simply if one knew which factors legitimately affected the way in which platforms should rank products. Conditional on these factors, one could then ask whether different suppliers received differential treatment from a platform (for example, search Google rankings). In our context, one could ask whether different songs – by label type or artist gender – received different rankings on playlists. For example, if two songs had the same expected commercial promise, but the major-label song received a better ranking on the playlist, then one could infer bias in favor of major label music. One might be tempted to implement this by regressing songs’ playlist ranks on characteristics reflecting commercial promise as well as label type, interpreting the label type coefficient as bias. But it is difficult to rule out the possibility that apparent bias reflects correlation with relevant but unobserved factors. So we regard this “conditioning on observables” (COO) approach as inherently unpromising, although we implement a version for comparison in Section 5.3.====In the past few decades, researchers have developed alternative, ==== approaches for measuring bias. Rather than asking whether different groups are treated differently after controlling for available factors, these tests compare the ex post performance of different groups assessed to be of equal promise. This approach has two information requirements. First, one needs a measure of the decision maker’s ex ante assessment of promise. Second, one needs measures of ex post performance. To use a traditional offline example, if a hiring firm gives applicants a score, then one can compare, say, the on-the-job performance of hired men and women. If women outperform men who had received the same score at hiring, then the firm’s scoring procedure is biased against women. Researchers have applied this approach to measuring bias in policing, bail setting, hiring, and journal publication, among other contexts (Ayres, Waldfogel, 1994, Smart, Waldfogel, 1996, Hellerstein, Neumark, Troske, 1999, Knowles, Persico, Todd, 2001, Arnold, Dobbie, Yang, 2018, Card, DellaVigna, Funk, Iriberri, 2020).====The way that Spotify operates some of its playlists satisfies the information requirements for outcome-based bias testing, which surmounts the challenge inherent in the COO approach. First, we can observe curators’ ex ante assessments of songs’ commercial promise: Spotify maintains human-curated “New Music Friday” lists in each of the countries in which it operates.==== Each week, Spotify curators rank songs on each country’s list; and songs ranked higher on these lists ultimately stream more based, in part, on curators’ ex ante assessments of songs’ commercial promise (Aguiar and Waldfogel, forthcoming). Second, we can also observe ex post streaming: Spotify discloses daily streams for the top 200 songs by country. These two ingredients allow bias tests that compare eventual success, relative to ex ante curator assessments. Our contribution is the development of an implementable approach to measuring platform bias, illustrated with measurement of possible platform bias in Spotify’s New Music playlists.====While our analysis focuses on a subset of the many playlists available at Spotify, we note that the New Music Friday lists are of particular relevance. They have arguably become “an important part of launching any new song” and the exposure they offer is often seen as “career-changing,” also by improving the likelihood of appearing on other important Spotify playlists (Aswad, 2020). Because the New Music Friday lists serve as an important entry point for new products and artists, their potential biases may affect not only which existing products succeed, but also the type of products entering the market in the future.====Using data on 5736 songs assigned to 14,747 top 20 entries on weekly New Music lists in 26 countries during 2017, we find the following. First, songs with better New Music ranks have more ex post streaming success on the platform, which provides evidence – to a first approximation – that curators rank songs to maximize eventual streams of songs on the playlists. Second, conditional on New Music ranks, songs on independent record labels stream less than songs on major record labels. We find a similar result by gender, although the gender differential in ex post streaming arises only for songs that curators rank outside the top 10. Hence, despite concerns about bias against independent-labels and women, these results indicate that independent music, and music by women, receive better ranks than their eventual on-platform streaming performance seems to warrant.==== Independent music receives an average boost of about 2 ranks, while music by women receives an average boost of 1.4 ranks. Third, the degree of bias implied by the outcome-based tests differs systematically from the amount implied by a conditioning-on-observables regression of New Music ranks on song types and available predictors of commercial promise, reinforcing our concern about the adequacy of conditioning on observables. Fourth, our bias results do not appear to be driven by differential promotion by label type or artist gender. Finally, we estimate the relative weights that curators apply to independent-label music and music by women, relative to their respective complements: Curators behave as if they maximized weighted streams, where the weights are 40% higher for independent-label music, and 10% higher for music by women. We conclude that Spotify New Music playlist rankings use the platform’s power to promote music from independent record labels and, to a lesser extent, women; and we suggest that – with access to data – our approach is applicable to other platform contexts.====The paper proceeds in five sections after the introduction. Section 2 provides background on playlists and possible reasons for concern about bias. We also place this work in context of the relevant existing literatures. Section 3 presents a simple model of the curator’s ranking decisions in which curators maximize the weighted sums of streams for different types of music, and unequal weights reflect bias. Section 4 describes the data, and Section 5 presents results, including both direct evidence on the existence of bias and measures of the welfare weights curators attach to independent vs major music and music by women vs men. Section 6 concludes.",Playlisting favorites: Measuring platform bias in the music industry,https://www.sciencedirect.com/science/article/pii/S0167718721000588,7 July 2021,2021,Research Article,100.0
"Balzer Benjamin,Schneider Johannes","Economics Discipline Group, University of Technology Sydney, Australia,Department of Eonomics, Universidad Carlos 3 de Madrid, Spain","Received 22 October 2020, Revised 23 June 2021, Accepted 30 June 2021, Available online 3 July 2021, Version of Record 17 July 2021.",https://doi.org/10.1016/j.ijindorg.2021.102764,Cited by (2),"We study coordination among competitors in the shadow of a market mechanism. Our main example is standard setting: either firms coordinate through a standard-setting organization (SSO), or a market solution—a standards war—emerges. A firms veto to participate in the SSO triggers a standards war. Participation constraints are demanding, and the optimal SSO can involve on-path vetoes. We show that vetoes are effectively deterred if firms can (partially) release their ==== to the public. We discuss several business practices that can serve as a signaling device to provide that information and to effectively ensure coordination.","The road to the standard for high-resolution home-video discs (DVDs) was not straight. After a first attempt to coordinate on a standard (in 1994) failed, two rival technologies were developed: the MultiMedia Compact Disc (MMCD) by Sony and Philips and the Super Density (SD) disc by Toshiba and Time Warner. Although the movie industry was pushing for a unified standard, no cooperation was in sight and a standards war seemed inevitable.====Why did the two camps at first refuse to cooperate? One explanation is that they had private information about their prospects in a standards war. As a result, both may have been optimistic that their technology would prevail in the market and thus refused to concede. Alternatively, and more strikingly, both parties may have been optimistic despite holding a common prior.====Mutual optimism occurs if both parties receive a positive signal about their own capabilities in a standards war. But they are less optimistic about their competitor’s expected capabilities. When evaluating their strategic options, both parties put little weight on the possibility of facing a strong competitor. Mutual optimism implies that each party expects a favorable outcome. Yet a standard-setting organization (SSO) can at most grant a favorable outcome to one of them. So a strong firm has an incentive to refuse to participate in an SSO.====In the case of the DVD standard, both camps were optimistic. The MMCD camp was convinced that without the patents it held on the compact disc (CD), no successful implementation of a video disc was possible. The SD camp, meanwhile, was convinced that its dual-layer technology advanced far beyond anything based on CD technology (see Taylor, 2006, ch. 2).====According to industry observers, a group of technicians from the leading computer companies (known as the technical working group, or TWG) played a major role in persuading both camps to form the DVD Consortium (later the DVD Forum). In June 1995 Sony executive Norio Ohga shared his view that a standards war was unavoidable (see Taylor, 2006, ch.2). As the camps prepared for a standards war, the TWG announced that it was going to analyze the two camps’ proposals.==== Shortly thereafter, both camps announced that they would work together. And in fact they finally united in the DVD Consortium.====Why did the announcement by the TWG induce the camps to cooperate? The message contained no information about whom the TWG would side with. Instead, two other aspects of it were important. First, the announcement was credible. As Toshiba executive Koji Hase later put it, TWG chairman Alan Bell “is fair, hes very fair. He did not side with Toshiba [or] Sony for that matter. He tried to be as fair as possible.”==== Second, the announcement implied that the TWG was going to release the outcome of its evaluation at a later date (see Wolpin, 2007).====The TWG was committed to releasing information about the technologies if the parties could not find a solution on their own. That commitment overcame the coordination failure that resulted from mutual optimism. The TWG did not interfere with the formal rules governing how the firms would compete in the market if they could not agree on a standard (the market solution). Instead, it influenced the structure of expected information. Moreover, it did so by announcing its plans to send a signal rather than actually sending a signal.====Most industries operating in two-sided markets coordinate on a de facto standard eventually. A standard is a platform that governs firms’ interaction. If a standard is not imposed by a regulator, there are, broadly speaking, two ways industries can set their standard. Firms can cooperate via an SSO, or market forces can determine the outcome. In the former case, the SSO implements the standard. In the latter case, the standard emerges as the outcome of a standards war.====In this paper, we model the market solution as a game of incomplete information. An SSO is the alternative to the market. It determines the standard outside the market. An SSO can only be established if firms agree to form one.====We address the question: is there a simple and cost-effective way to foster coordination? Our answer is yes, provided firms have access to a certain signaling device. The most important feature of that signaling device is that it can conceal information for some time before releasing it. We construct the optimal signaling device. It has two realizations per firm. We refer to it as ====.====Informational punishment has a variety of desirable features: (i) in equilibrium revealing private information is only a threat (that is, the threat is executed with probability 0); (ii) informational punishment has no effect on firms’ incentive constraints conditional on acceptance; (iii) a decentralized implementation is possible (that is, firms can design informational punishments themselves); and (iv) the punishment does not require enforcement by a third party but rather results from the complying firms’ competitive market behavior.====Our stylized model of the standard-setting process is inspired by the DVD case. Two firms are on the verge of a standards war. A third party collects information from firms and promises to release (some of) the information if coordination fails. We construct the optimal information-revelation device and show how it facilitates coordination on a standard.====Informational punishment can be carried out (i) through the SSO itself or (ii) by each firm individually. We discuss several business strategies for informational punishment: product pre-announcements, information leakage, and the provision of beta versions. Finally, we address the role of the signal sender’s commitment. We show that the sender has no incentives for opportunistic behavior.====The main motivation of this paper is to explain the formation of SSOs. However, our findings apply to any setting in which parties coordinate in the shadow of a market solution. Examples include environmental agreements, coordination among legislators, strikes, litigation, and R&D alliances.==== Our analysis shows that a third-party’s announcement that it will send a signal about one firm’s cost structure can persuade another firm to join the SSO. The announcement relaxes participation constraints and thereby increases the set of implementable SSOs.====Informational punishment works because a signal realization about firm ==== has two effects. The first effect is direct and distributional. It changes the other firms’ perception of firm ====’s cost structure. The second effect is indirect and behavioral. A firm’s continuation strategy is a function of its information set. Obtaining more information alters the firm’s continuation strategy. Via equilibrium reasoning, we can see that the firm’s change in behavior alters the behavior of its competitors.====Bayesian updating implies that the distribution of posteriors averages to the prior distribution. The same does not hold for expected payoffs. The reason is that firms expect to adjust their behavior after each realization. Their continuation payoffs depend on the adjusted strategy profile. Expected payoffs are nonlinear in the information structure and hence expected payoffs before the signal’s realization are not equivalent to expected payoffs if there is no signal at all.====Informational punishment exploits the behavioral channel. It decreases firms’ outside options and thereby relaxes participation constraints. Under informational punishment, firms commit to releasing some of their private information if another firm vetoes the SSO. Releasing information influences the action choices of all firms in the market—those participating and those vetoing. The threat of information release persuades firms to participate in the SSO.====Informational punishment has a set of additional attractive features. First, it separates the signaling effect of a veto from firms’ participation decisions. Second, it has no direct effect on either the (expected) outcome of the SSO or the incentive constraints because informational punishment operates off the equilibrium path. That is, the threat of informational punishment relaxes parties’ participation constraints. Third, informational punishment does not need to be executed on the equilibrium path. In sum, informational punishment enlarges the set of implementable SSOs. Yet it does not rely on a third-part ability to enforce actions or on noncredible threats.==== In line with Simcoe (2012), we assume that standard setting is a process in which industry consensus overcomes default market forces. While his focus is on bargaining under complete information, we use incomplete information as the predominant friction. Ganglmair and Tarantino (2014) also use an incomplete-information framework to model bargaining in R&D settings. While they study cases in which private information threatens to reverse an agreement, we study cases in which private information threatens the initial agreement. In a similar vein, our study complements Spulber (2018), who addresses the voting procedure inside organizations and how that interacts with the underlying market structure. Our model addresses an earlier stage. We are interested in whether firms decide to join an agreement and how they can convince others that coordination is better than the market.====In line with Farrell and Saloner (1985), we view a standards war as a contest between competing standards. Instead of engaging in a standards war, firms can coordinate on an SSO that governs their patent rights.====We follow Farrell and Simcoe (2012) and assume that firms can choose from a set of SSOs to avoid the costly market mechanism. In line with them, we assume that firms hold private information about their own patents. However, unlike them, we are not primarily interested in whether the ==== standard arises. We are agnostic about the standard’s quality and instead focus on the ==== (Lerner and Tirole, 2015) of an SSO.====Dequiedt (2007) emphasizes the importance of nontrivial participation constraints in a model of collusion in auctions. Like us, he studies how participation constraints restrict the set of implementable outcomes. The crucial difference is that in our model, firms ==== commit to following recommendations of a third party if coordination fails. Instead, if a firm vetoes the SSO, the SSO becomes void and firms compete in the market. ==== firms select their individual best response in the market and he SSO itself influences these responses only indirectly through the information structure it implies ==== firms have observed who vetoed the SSO.====In our baseline model, we assume that the signaling device is offered by an impartial third party. That third party can commit to a certain device ==== eliciting firms’ information. Under this assumption, concerns about informational opportunism such as those in Dequiedt and Martimort (2015) do not apply directly.====Informational punishment applies the tools of Bayesian persuasion (see the literature following Kamenica and Gentzkow, 2011)—in particular, convexification (Aumann and Maschler, 1995). Our problem differs in the collection of information. Information has to be elicited from the firms. That is, the signaling device has to satisfy incentive constraints. Moreover, in the persuasion literature, a designer actively persuades firms to take a certain action. Informational punishment works through a more subtle channel. A ==== persuades firms to participate in the proposed SSO by threatening to release information if the firms do not participate. Thus, on the equilibrium path, information is never provided. Instead, the threat alone convexifies outside options. As a consequence, the availability of a communication channel by itself fosters coordination, and persuasion occurs without the need for the signal to realize.====Correia-da Silva (2017) and Gerardi and Myerson (2007) offer an alternative mechanism to induce participation. The main difference from our model is that in their models firms can verify neither a veto nor an acceptance decision. They propose a trembling device to relax participation constraints. The trembling device triggers a spurious veto ====. The existence of ==== vetoes eliminates the signaling value of an ==== veto, as firms cannot credibly signal that they caused the observed failure to coordinate. In our setup, trembling devices are ineffective since it is publicly observable which firm vetoed the mechanism. Instead, we propose informational punishment to get firms to participate. Unlike trembling devices, informational punishment has no influence on the SSO itself.====The fact that full participation need not be optimal even in rich mechanism spaces is documented in Celik and Peters (2011). In an extension, we show that if the mechanism space is rich enough, an SSO exists that is optimal ==== ensures full participation.====Our research also connects to our own work on information spillovers in mechanism design (Balzer and Schneider, 2019, Balzer and Schneider, 2021). In Balzer and Schneider, 2019, we derive a general framework to design mechanisms with information spillovers in arbitration problems. There the choice of the mechanism affects the information structure and action choices ==== the mechanism is used. In this paper, by contrast, we are interested in how information revelation affects decisions ==== an exogenously given SSO arises. Balzer and Schneider, 2019 ignore this consideration by assuming a fixed outside option for each type.====Balzer and Schneider, 2021 address alternative dispute resolution in legal disputes. There, too, we consider a mechanism with a game as an outside option—namely, litigation. However, the litigation model in Balzer and Schneider, 2021 implies that utility functions are convex in the information structure. Convexity makes informational punishment superfluous in that setting: initial participation in the mechanism is optimal even absent informational punishment. Information is relevant only ==== the mechanism. Indeed, informational punishment has no function in Balzer and Schneider, 2019, Balzer and Schneider, 2021.",Persuading to participate: Coordination on a standard,https://www.sciencedirect.com/science/article/pii/S0167718721000576,3 July 2021,2021,Research Article,101.0
Lewis Matthew S.,"John E. Walker Department of Economics, Clemson University, 228 Sirrine Hall, Box 341309, Clemson, SC 29634-1309, USA","Received 8 May 2020, Revised 6 May 2021, Accepted 8 June 2021, Available online 17 June 2021, Version of Record 13 August 2021.",https://doi.org/10.1016/j.ijindorg.2021.102761,Cited by (3),"Empirical studies exploring the relationship between competition and price discrimination don’t generally consider the role of product differentiation or the asymmetric adoption of discrimination across firms. Using a customized empirical approach to examine the use of Saturday-night stayover discounts in the U.S. airline ====, I show that discounts are used more often when facing competitors that offer differentiated products but less often when competing with firms that don’t use discounts. Legacy carriers rarely use discounts when competing with Southwest or other low-cost carriers, but the presence of competing legacy carriers sometimes enhances the use of discounts.","Theoretical models of both second-degree price discrimination (Stole, 1995, Dai, Liu, Serfes, 2014) and third-degree price discrimination (Holmes, 1989, Stole, 2007) have shown that an increase in competition can lead to an increase or a decrease in the amount of discrimination. Empirical studies have also extensively examined this relationship across a variety of markets and settings—sometimes finding that discrimination increases with competition (as in Stavins (2001), Busse and Rysman (2005), Borzekowski et al. (2009), and Seim and Viard (2011)) and other times finding that competition reduces discrimination (Gaggero, Piga, 2011, Gerardi and Shapiro, 2009, Lin, Wang, 2015).====While the theoretical literature offers many valuable insights, it has primarily focused on symmetric equilibria arising from competition between symmetric (or symmetrically differentiated) firms. Similarly, existing empirical studies have generally examined how the (symmetric) discriminatory pricing strategies of all firms in the market are affected by the presence of additional competitors, but have not carefully investigated differences in discriminatory pricing or the heterogeneous effects of competition across firms. In practice, however, competing firms often face different costs, sell different products, and utilize different discriminatory pricing strategies. Of the few theoretical papers that consider discrimination with firm and consumer heterogeneity (Dogan, Haruvy, Rao, 2010, Shulman, Geng, 2013, Lin, 2017), most find that equilibria do exist in which one firm discriminates and the other firm does not. In this paper I explore the importance of such asymmetries through an empirical investigation of Saturday-night stayover discounts, which are frequently used in the U.S. domestic airline market by legacy carriers (American, United, and Delta) but not by low-cost carriers (Southwest, JetBlue, etc.).====As in the broader literature, empirical studies of discrimination in airline markets have focused largely on the symmetric effects of competition. In addition, the data limitations faced by many of these studies make it difficult to control for unobserved cost differences when measuring the extent of price discrimination or identify the relationship between competition and discrimination. Borenstein and Rose (1994), Gerardi and Shapiro (2009), Dai et al. (2014), and others rely on price data from the Airline Origin and Destination Survey (DB1B) produced by the U.S. Bureau of Transportation Statistics. These data include a 10% sample of all tickets sold, but only report the quarter of travel, making it impossible to control for underlying cost differences between flights. As a result, these studies generally relate the degree of dispersion in prices (charged by a carrier on a route in a quarter) to the level of concentration on the route, but are unable to clearly separate price discrimination from cost-based price variation. Gaggero and Piga (2011) use more-detailed data from routes between the UK and Ireland, but adopt a similar approach to examine ticket price dispersion.====A second set of papers, including Stavins (2001), Giaume and Guillou (2004), Hernandez and Wiggins (2014), and He (2016), use itinerary-specific fare quote data to examine how concentration on the route affects the price differences associated with specific ticket restrictions (e.g., advance purchase or Saturday-night stayover restrictions) or fare classes.==== Observable market and flight characteristics are used to control for cost-based factors that contribute to price differences across itineraries. However, given the unpredictable nature of flight-specific demand, substantial cost variation is likely to occur even across flights with the same observable characteristics. As a result, the findings of such studies may still be biased if unexplained cost differences are correlated with the price differences airlines attach to ticket restrictions on specific flights.====In this study I overcome these issues by using a customized data collection strategy to compare fares quoted to passengers staying over Saturday night with fares quoted to passengers not staying over Saturday night for travel on the ====.==== Since the opportunity cost at a particular time of selling a seat on a particular flight is the same regardless of who purchases the seat, this comparison controls for all the unobserved factors affecting costs. Any remaining price differences can, therefore, be entirely attributed to price discrimination. A similar approach is used by Escobari and Jindapoon (2014) to examine non-refundable ticket discounts, Lewis (2020) to identify directional price discrimination, and by Escobari et al. (2018) to study round trip discounts.====The findings establish several interesting new empirical facts that enhance the current understanding of airline price discrimination. First, legacy carriers are roughly three to five times more likely to use Saturday-night stayover discounts on routes where they do not compete with Southwest. The presence of other low-cost carriers (like JetBlue) have a similar though slightly smaller effect, but competition from ultra-low-cost carriers (like Spirit or Frontier) have little impact on legacy carrier discounting behavior. Second, Saturday-night stayover discounts are larger and more frequent when the competitors offer a different type of service (nonstop vs one-stop) than when they offer the same service type as the legacy carrier. Third, the presence of additional legacy carriers on a route does not reduce the use of Saturday-night stayover discounts as much as competition from Southwest or LCC carriers, and in some cases competition from other legacy carriers increases the likelihood of discrimination.====The results also contribute new evidence to the broader literature on price discrimination with asymmetric firms. While theoretical models with asymmetric firms often have equilibria in which one or both firms price discriminate, airlines rarely discriminate on routes where legacy and low-cost carriers are both present. Moreover, in terms of the use of Saturday-night stayover discounts, the number of competitors on the route may not be as important as whether those competitors also utilize Saturday-night stayover discounts. This can explain why discounts are used less when competing against Southwest or other LCC carriers (that never offer Saturday-night stayover discounts), but are sometimes used more frequently when competing against legacy carriers (that may also be using discounts). Finally, the results demonstrate that differentiation alters the effect of competition on discrimination. Legacy carriers’ use of Saturday-night stayover discount is most strongly influenced by Southwest, whose characteristics most-closely resemble the large legacy carriers, and is least impacted by ultra-low-cost carriers, who offer a very different type of product to customers. Similarly, there is significant quality differentiation between nonstop vs one-stop flights, and the use of discrimination is more strongly influenced by competition from carriers offering similar flights (of the same service type) than from those offering more differentiated flights (of a different service type).====With regard to some types of competition, my findings correspond with the conclusions of Gerardi and Shapiro (2009) and Gaggero and Piga (2011) that additional competition (among similar firms) tends to reduce the degree of price discrimination. However, they also complement those of Chandra and Lederman (2018) and Dai et al. (2014) suggesting a more nuanced relationship in which competition can sometimes have the opposite effect. Moreover, my analysis contributes a new source of evidence to the literature, leveraging unique data and a new empirical approach that isolates discrimination by eliminating cost-based price differences. By more extensively incorporating product heterogeneity into the empirical analysis the results also help to clarify the conditions under which competition reduces or enhances discriminatory activity in this market.",Identifying airline price discrimination and the effect of competition,https://www.sciencedirect.com/science/article/pii/S0167718721000540,17 June 2021,2021,Research Article,102.0
"Vasserman Shoshana,Watt Mitchell","Stanford Graduate School of Business, SIEPR and NBER, USA,Stanford University, USA","Received 31 December 2020, Revised 28 May 2021, Accepted 31 May 2021, Available online 11 June 2021, Version of Record 26 November 2021.",https://doi.org/10.1016/j.ijindorg.2021.102758,Cited by (5),"Auctions are inherently risky: bidders face uncertainty about their prospects of winning and payments, while sellers are unsure about revenue and chances of a successful sale. Auction rules influence the allocation of risk among agents and the behavior of risk-averse bidders, leading to a breakdown of payoff and revenue equivalence and a heightened significance of auction design decisions by sellers. In this paper, we review the literature on risk aversion in auctions, with an emphasis on what can be learned about auction design from theoretical modeling and empirical studies. We survey theoretical results relating to the behavior of risk-averse agents in auctions, the comparison of standard auction formats in the presence of risk aversion and implications for auction design. We discuss standard and more recent approaches to identifying risk preferences in empirical studies and evidence for the significance of risk aversion in auction applications. Finally, we identify areas where existing evidence is relatively scant and ask what questions empirical research might ask given the theory and where further theoretical research may be beneficial given existing empirical results.","Auctions guide decisions in many economically significant domains, including government procurement, spectrum allocation, electricity generation, online advertising and financial markets. Market designers choose rules for the auction to promote their objectives. In the classic risk-neutral independent private values model, most of these decisions are moot: the revenue and payoff equivalence theorems imply that the only real decision to be made by an auctioneer is the choice of a reserve price. In the optimal auction, characterized by Myerson (1981), there is no role for entry fees, randomized payment or allocation rules, buyout prices, subsidies to losing bidders and a variety of other non-standard auction designs that are observed in practice.====In order to reconcile the use of these non-standard auction design tools, the literature has noted that bidding behavior is often inconsistent with the predictions of the classic model.==== In this case, the equivalence of different auction designs may no longer hold. Risk aversion—a central example of a force that extends beyond the classic model—has been identified as a relevant factor in economic decision-making in many choice domains (Holt and Laury, 2014). In auctions, risk-averse agents respond to the uncertainty inherent in the allocation and payment rules, leading to consequential differences in behavior compared to their risk-neutral counterparts. This generates a richer role for auction designers, who may gain by accounting for—and potentially exploiting—this risk-avoiding behavior.====In this survey, we discuss the theoretical literature that analyzes and guides the design of auctions with risk-averse agents, as well as the empirical literature that tests for evidence of risk aversion and estimates the primitives governing risk-averse bidding behavior. A relevant question for empirical study is the extent to which risk aversion substantively affects behavior in real-world auctions. We begin in Section 2 by summarizing different types of empirical evidence that have been offered to argue that risk aversion is a relevant factor in auction settings. Drawing on examples of auctions conducted by governments, on online platforms and in the lab, these studies show that observed bids do not conform to the predictions of a risk-neutral model and are more consistent with some model of risk aversion.====We begin Section 3 by describing a benchmark model of risk-averse bidding in standard auctions. With this, we both fix notation and discuss the classic result, formalized==== by Holt (1980), that the revenue of a first-price auction with risk-averse bidders exceeds that of a second-price auction. We survey several papers showing that this ranking is robust to a variety of additional modeling considerations, including endogenous entry and heterogeneity of risk preferences, although not all. We describe revenue-maximizing auctions for risk-averse bidders (Matthews, 1983, Maskin, Riley, 1984), which are strikingly different from classic auction formats, including the risk-neutral optimal auction (Myerson, 1981). Because this mechanism is not observed in practice, we discuss implications of risk aversion for the design of classic auctions, including the choice of reserve prices and entry fees, and a variety of non-standard auction designs that may be rationalized by risk aversion. Finally, we describe a number of further extensions to the model which may be relevant, including interdependent values, ==== uncertainty, related behavioral biases and approximate mechanism design.====In Section 4, we discuss empirical studies of risk aversion in auctions, beginning with the econometric strategies that have been developed to identify the primitives governing bidding behavior observed in risk-averse bidders. Many design decisions, such as the choice of a reserve price or entry fee/subsidy, require knowledge of the distribution of bidders’ values and the extent of their risk aversion. A classic non-identification result by Guerre et al. (2009) shows that—in contrast to risk-neutral settings—empirical bid distributions alone are generally insufficient for identifying both value distributions and utility functions of risk-averse bidders. Successful identification strategies thus exploit additional variation from parallel auction formats, exogenous participation, endogenous entry decisions and related strategic actions made by auction participants, such as the portfolio allocation decisions inherent in scaling auctions. We also discuss what has been learned about auction design from the application of these identification strategies to real-world data.====We do not emphasize in this survey the substantial experimental literature on risk aversion in auctions. Much of this literature, including many papers in the excellent surveys of Kagel (1997) and Kagel and Levin (2016), has exploited laboratory studies of auctions to illuminate the risk preferences of human decision makers, rather than for the purpose of informing auction design. Because we are more interested in the implications of risk aversion for the design of auctions, we only mention the key experimental studies that have guided the development of the theoretical and empirical literature on auctions.====We conclude by summarizing the key lessons learned from the theoretical and empirical literature on risk aversion in auctions and discuss what we see as key open questions in the field. Table 1 presents this summary in condensed form. Because many theoretical results in the risk-averse setting depend on the extent of risk aversion and relationships between risk aversion and value distributions, we believe there could be benefits from further empirical studies to understand when and why risk aversion is relevant to auction design. Empirical analysis could also direct theoretical research towards areas where better modeling would be valuable, such as the question of whether simple mechanisms that require less knowledge of the bidder type distribution may be able to approximate optimal results.",Risk aversion and auction design: Theoretical and empirical evidence,https://www.sciencedirect.com/science/article/pii/S0167718721000515,11 June 2021,2021,Research Article,103.0
Albert Jason,"Economists Incorporated, Washington, District of Columbia, United States","Received 5 September 2019, Revised 6 March 2021, Accepted 2 June 2021, Available online 9 June 2021, Version of Record 1 July 2021.",https://doi.org/10.1016/j.ijindorg.2021.102759,Cited by (0),"This paper studies a dynamic model of a fee-for-service healthcare system in which healthcare providers attract patients by prescribing antibiotics. Using antibiotics limits antibiotic-treatable infections, but fosters the growth of antibiotic-resistant infections. The paper demonstrates a ‘Goldilocks’ effect from provider competition. A perfectly competitive market for providers over-prescribes antibiotics because providers do not bear the cost of antibiotic-resistant infections. A patient monopolist under-prescribes antibiotics in order to increase the level of treatable infection. This is because while infection is a ‘bad’ for society, infection is a ‘good’ for a provider of antibiotics under a fee-for-service regime. Due to more moderate antibiotic use, oligopolistic competition can be the optimal decentralized market structure. The paper then demonstrates how the model can be used for policy analysis.","Antibiotics are a special economic good. Using antibiotics causes both a positive externality by limiting the spread of antibiotic-treatable infections and a negative externality by fostering the growth of antibiotic-resistant infections (Layton and Brown, 1996). While antibiotic use has greatly tempered the level of treatable infections, resistant infections have become a burdensome public health problem, costing the United States an estimated twenty billion dollars and twenty three thousand lives annually (CDC, Roberts, Hota, Ahmad, Scott, Foster, Abbasi, Schabowski, Kampe, Ciavarella, Supino, Naples, Cordell, Levy, Weinstein, 2009). The Centers for Disease Control and Prevention states that the rise in “antimicrobial resistance is one of our most serious health threats” (CDC, 2013). Understanding how economic incentives affect antibiotic use is a problem of first-order importance.====This paper develops a model of a fee-for-service healthcare system in which healthcare providers attract patients by prescribing antibiotics. This is a natural interaction to model, as patients frequently request and receive antibiotics from their providers, even when use is medically inappropriate (Bauchner, Pelton, Klein, 1999, Stivers, 2005). The extent of such misuse by providers appears to be severe: in the United States, patients with a sore throat are prescribed antibiotics about seventy percent of the time, even though antibiotics are only effective in about ten percent of cases (Barnett and Linder, 2014). Further, outpatient antibiotic use, which is the focus of the model, significantly contributes to the community spread of antibiotic resistant infections (Seppälä et al., 1995).====The model is used to study a number of questions. First, how do the dynamics of antibiotic use generated by the decentralized decisions of providers and patients compare to the socially optimal dynamics? Relatedly, what is the optimal market structure of healthcare providers? There is significant variation in the market concentration of providers across counties in the United States (Schneider et al., 2008). More competitive markets for providers use antibiotics more frequently than less competitive markets (Bennet et al., 2015; Fogelberg (2013)). However, given the competing effects of antibiotic use on treatable and resistant infection, the optimal market structure of providers is an open question in the economics literature on antibiotic resistance. Finally, what role can public policy play in correcting the potential misuse of antibiotics by providers?====To study these questions, the paper embeds an economic model of the provider-patient relationship into a dynamic epidemiological model of infection and antibiotic resistance. The economic model describes how provider and patient behavior determine antibiotic use and the epidemiological model describes how infection evolves in response to antibiotic use.====The epidemiological model features an antibiotic-treatable infection and an antibiotic-resistant infection. While infection is a very broad term, the model focuses specifically on ==== diseases. The two infections compete for resources in the ecosystem. When a patient with the treatable infection takes the antibiotic, the patient raises his probability of recovery. The patient is then less likely to infect other healthy agents. However, by removing a source of competition for the resistant infection, the patient’s use of the antibiotic allows the resistant infection to grow and spread at a faster rate in the future.====Figs. 1 and 2 show the evolution of Penicillin-treatable and Penicillin-resistant invasive ==== infections in Baltimore City, Maryland and the Nashville-Davidson region of Tennessee from 2003 to 2009.====Baltimore City, featured on the top, displays patterns typical from antibiotic use: a decrease in the rate of treatable infections and an increase in the rate of resistant infections. Nashville-Davidson, on the other hand, displays the opposite pattern: an increase in the rate of treatable infections and a decrease in the rate of resistant infections. Interestingly, in 2001, Nashville-Davidson started a campaign to reduce antibiotic use.==== The graphs highlight competition between the treatable and resistant infections for resources: one infection grows at the expense of the other. It is this competition between the two infections, the winner of which is determined by the frequency of antibiotic use, that the epidemiological model describes.====The economic model consists of a matching process between providers and patients. Providers choose how frequently to prescribe antibiotics for their patients and patients choose whether to see a provider and which provider to see. Patients gain utility from a provider’s prescription behavior, the effectiveness of the antibiotic, and an idiosyncratic component related to the quality of their match with a provider. Patients choose whether or not to pay a fixed fee to see their most preferred provider, ignoring the external effects of their antibiotic use on infection and resistance. Providers choose a prescription rate to maximize profits, given the behavior of other providers and patients, internalizing their own effect on the future levels of infection. Provider and patient behavior together determine the aggregate amount of antibiotic use, which is then embedded into the epidemiological model.====When the market is competitive, providers have a negligible effect on the evolution of disease and therefore no incentive to conserve the treatable infection. This incentivizes providers to perpetually ==== antibiotics in order to attract as many patients in the current period as possible. While high antibiotic use is initially a boon for society as the level of treatable infection quickly falls, the resistant infection grows at a quicker rate than is socially optimal, eventually causing a loss that more than offsets the initial gain.====At the other end of the competitive spectrum, a patient monopolist ==== antibiotics relative to the planner. This increases profit along two margins for the monopolist. First, the monopolist maintains higher antibiotic effectiveness, which increases the value of the monopolist’s product and, in the long run, induces more patients to pay the fee. Second, the monopolist can maintain a perpetually higher total level of infection than the social planner desires, which results in a larger pool of potential fee-payers. The paper finds conditions for the monopolist to maintain a steady-state characterized by an undesirably high level of infection.====Increasing the level of competition from monopoly to duopoly generates a steady-state with a lower level of infection. Competition for patients induces the duopolists to prescribe antibiotics more frequently than the monopolist, leading to a steady-state that yields strictly lower treatable infection and higher welfare than the monopolist’s steady-state. However, as the market structure becomes increasingly competitive, antibiotics eventually become over-prescribed. Using numerical analysis, the paper shows that social welfare as a function of the number of providers can take on an inverted U-shape (illustrated in Section 4.4). Social welfare initially increases as the market becomes more competitive from monopoly, peaks at some level of oligopolistic competition, and then decreases as the market becomes perfectly competitive. Oligopolistic competition can thus be the optimal market structure.====However, even the optimal decentralized market does not use antibiotics optimally. There is also a sense in which the market does not come ‘close’ to optimal use. The paper finds that the optimal antibiotics policy can be characterized by cycling between periods of high and low antibiotic use, whereas the decentralized market settles into a steady state.====This raises interest in policy solutions. The paper demonstrates how the model can be used to evaluate the welfare effects of several different public policies aimed at mitigating provider misuse of antibiotics. The paper first computes the optimal static antibiotics tax for different market structures and and finds that it is negative (a subsidy) at low levels of competition and becomes positive at high levels of competition. This result highlights the need for policy to be sensitive to the underlying market structure for providers. While the static tax improves on the market outcome, it cannot induce cycling behavior. The paper then demonstrates that the optimal dynamic tax can induce cycling, and while it still cannot perfectly implement the planner’s solution, it can substantially improve on the market outcome. The paper also computes the value of a diagnostic procedure that differentiates between the treatable and resistant infections, which can be used to determine whether financing such an innovation is efficient.====The paper makes two contributions to the economics literature on antibiotic resistance (which is discussed in detail in Section 2). First, the paper develops a novel model of imperfectly competitive provision of antibiotics. The paper demonstrates new results about the optimality of oligopolistic competition and generalizes results about monopoly and perfect competition previously discussed in the literature. Second, the paper provides a framework for policy analysis.====The rest of the paper will proceed as follows: Section 2 discusses the related literature. Section 3 presents the epidemiological model of infection and antibiotic resistance and the economic model of provider competition. Section 4 studies the market allocation of antibiotics. Section 5 studies the effect of public policy. Section 6 concludes. For ease of exposition, all derivations can be found in the appendices.",Strategic dynamics of antibiotic use and the evolution of antibiotic-resistant infections,https://www.sciencedirect.com/science/article/pii/S0167718721000527,9 June 2021,2021,Research Article,104.0
"Bet Germán,Cui Shana,Sappington David E.M.","Department of Economics, University of Florida, P.O. Box 117140, Gainesville, FL 32611, USA","Received 23 July 2019, Revised 21 March 2021, Accepted 20 May 2021, Available online 2 June 2021, Version of Record 24 June 2021.",https://doi.org/10.1016/j.ijindorg.2021.102756,Cited by (0),"Upstream collusion that increases the price of an input can harm an independent downstream producer (D). We ask whether this harm is more or less pronounced when D’s downstream rival is a vertically integrated producer. We find that such vertical integration increases D’s loss from collusion when D is not a particularly strong competitor. However, when D is a sufficiently strong competitor, vertical integration can reduce D’s loss from collusion when price competition prevails downstream.","Just as the threat of government prosecution can help to deter illegal collusion by producers of key inputs, so can the threat of private litigation by input purchasers.==== Basso and Ross (2010, p. 896) note “the trend... toward continued and even growing support for private actions” to recover losses from collusion.==== The European Commission (EC)’s ==== proposes “specific measures that would ensure ... all victims of infringements of EC competition law have access to effective redress mechanisms” (EC, 2008, §1.2).====The incentives for private litigation against colluding input suppliers depend in part on accepted principles regarding the harms caused by collusion. A potential plaintiff, particularly a risk-averse plaintiff, may be reluctant to sue for actual harm suffered if courts do not routinely recognize the type of harm in question and measure the magnitude of the harm with substantial accuracy.==== Considerable effort has been devoted to identifying potential sources of harm from collusion by input suppliers, as explained further below. However, the literature does not provide systematic guidance on how vertical integration affects the harm that collusion imposes on input purchasers that compete in imperfectly competitive downstream markets. This guidance is important because colluding upstream suppliers often operate in downstream markets also.====To illustrate, the nation’s sugar refiners colluded in the 1970s to set the price at which they sold refined sugar to candy manufacturers.==== Some of the candy manufacturers were vertically-integrated with sugar refiners (e.g., subsidiaries of Borden) whereas other manufacturers were unaffiliated producers. Suppliers of TFT panels were also accused of colluding in the late 1990====s and early 2000====s to raise the prices at which they sold the panels to manufacturers of many products, including cellular telephones.==== The accused panel suppliers included Samsung and Toshiba, companies that also manufactured cellular telephones and competed against firms like Motorola and Nokia that did not manufacture TFT panels.====’====The primary purpose of this research is to determine whether the damage that collusion by upstream input suppliers imposes on an unaffiliated input purchaser is more pronounced when a rival downstream producer is vertically integrated with one of the upstream suppliers.==== Our formal analysis focuses on the interaction between two upstream suppliers (U1 and U2) and two downstream producers (D1 and D2) of differentiated retail products. We allow for the possibility that U1 and D1 might be vertically integrated. We find that such vertical integration increases the loss that upstream collusion between U1 and U2 imposes on the unaffiliated downstream producer (D2) when D1 is a relatively strong competitor. However, vertical integration can reduce D2’s loss from collusion under downstream price competition when D2 is a sufficiently strong competitor.====D2’s loss from collusion is always larger under vertical integration (VI) than under vertical separation (VS) in the presence of downstream quantity competition.==== This conclusion reflects two effects that arise under VI. First, U1 and U2 value D1’s downstream profit and so are inclined to charge D2 a relatively high input price in order to enhance D1’s profit. Second, the relatively low input price (====) that D1 perceives under VI induces it to act aggressively in its competition with D2.====These two effects also ensure that D2’s loss from collusion is larger under VI than under VS in the presence of downstream price competition as long as D2’s competitive strength (====) is no greater than D1’s competitive strength (====).==== However, D2’s loss from collusion can be smaller under VI than under VS in the presence of downstream price competition when ====. Downstream competition is relatively intense under price competition, particularly under VI where D1 perceives a relatively low input price (====). The intense downstream competition leads U1 and U2 to focus on securing upstream profit from input sales to D2 rather than enhancing D1’s downstream profit when D1 is a relatively weak competitor. U1 and U2 ensure substantial sales of the input to D2 under VI by charging D2 a lower input price under VI than under VS. The lower input price under VI ensures that when ==== is sufficiently pronounced, D2’s loss from collusion is smaller under VI than under VS in the presence of downstream price competition.====The literature observes that, in addition to the “direct” harm collusion by input suppliers can impose on input purchasers, such collusion can impose “indirect” harm on other parties. For example, retail customers can be harmed by the price increases retail producers implement in response to the elevated input prices they face due to collusion by input suppliers (e.g. Kosicki, Cahill, 2006, Han, Schinkel, Tuinstra, 2009, Basso, Ross, 2010, Boone, Müller, 2012, Brander, Ross, 2017).==== Retail customers of substitute products can also be harmed when the prices of these products rise in response to the higher prices charged by customers of the colluding upstream suppliers (e.g., Inderst et al., 2014).====We abstract from these indirect harms that collusion can impose. Instead, like Verboven and Van Dijk (2009) (VV), we focus on the losses that upstream collusion by input suppliers imposes directly on input purchasers. VV identify three effects of upstream price collusion on the profit of retail firms that purchase the input: higher input costs, increased revenue due to higher equilibrium retail prices, and reduced revenue due to foregone sales caused by higher retail prices.==== The authors observe that the magnitudes of these effects can vary if a downstream producer is vertically integrated. However, their study is not designed to provide systematic guidance on how vertical integration affects losses from collusion.====We offer a first step in providing such guidance. We do so in a streamlined setting where, when the upstream suppliers collude, they do so perfectly and costlessly, both in the presence of vertical integration and in its absence. Thus, we do not analyze whether vertical integration facilitates or hinders collusion.==== Rather, we investigate how vertical integration affects the losses that collusion among input suppliers imposes on input purchasers. Thus, our analysis might be viewed as an inquiry into whether standard calculations of losses from collusion that do not consider the prevailing vertical industry structure tend to understate or overstate the losses that actually arise when some colluding input suppliers are vertically integrated.====The analysis proceeds as follows. Section 2 describes the key elements of our model. Section 3 characterizes outcomes in the absence of collusion. Sections 4 and 5 characterize equilibrium outcomes and losses from upstream collusion in the presence of downstream quantity competition and downstream price competition, respectively. Section 6 discusses extensions of our analysis and provides concluding observations.",The impact of vertical integration on losses from collusion,https://www.sciencedirect.com/science/article/pii/S0167718721000497,2 June 2021,2021,Research Article,105.0
"Charpin Ariane,Piechucka Joanna","Economic Advisory, Deloitte France,DIW Berlin, Germany","Received 7 January 2020, Revised 1 June 2021, Accepted 1 June 2021, Available online 9 June 2021, Version of Record 27 June 2021.",https://doi.org/10.1016/j.ijindorg.2021.102760,Cited by (1),Many ,"The intensive merger activity observed since the 1990s along with speculation about the effectiveness of competition enforcement are fueling a growing interest in retrospective analyses of mergers. While there are a substantial number of studies estimating the price effects of large and/or controversial mergers, there exists little evidence on the effects of mergers on cost efficiencies. At the same time, efficiency gains are often one of the main arguments of merging parties in front of competition authorities and constitute, in theory, a central aspect to the economic motivation behind mergers. They constitute the primary justification as for why the mergers of competitors may benefit consumers. Not surprisingly, one of the ==== of the report of Ormosi et al. (2015) providing a review of merger decisions in the European Union is that, “====.”====The objective of this study is to assess whether a merger between two major transport groups gave rise to efficiency gains. We do this by studying the effects on the operating costs of local urban public transport operators of a large merger that took place between two of the three leading urban transport groups in France. In 2009, Transdev selected Veolia Transport to merge. This merger was approved by the French Competition Authority in 2010 with remedies and the deal closed in 2011. The new entity, which faced one main competitor and three smaller competitors in the market, had a market share of approximately 40%. While the French Competition Authority mostly focused on potential anticompetitive effects of the merger, the main concern being the reduction in the number of competitors in competitive tenders, the notifying parties argued that the transaction would generate merger specific efficiency gains. These were claimed to be linked, on the one hand, to the achievement of cost savings in the operation of the networks and, on the other hand, to the possibility of providing a more extensive service offering thanks to the pooling of experience between the parties.====In France, the urban public transport sector is regulated by local authorities (cities or groups of cities) that are in charge of its organization at the local level. In the majority of networks, this task is delegated to a private/public-private operator. Most operators belong to major transport groups. To select a transport operator, the local authority is required to launch a competitive tender procedure in which it specifies the characteristics of the service to be provided (network length, ticket fares, etc.). One of the main choice criterion in the tenders is the level of subsidies asked by the operator to operate the service. Hence, efficiency gains can be a powerful means for operators to succeed in tenders. At the same time, efficiency gains, if reflected in lower subsidies paid to transport operators, reduce taxpayer burden. This is of interest in a sector that is highly subsidized, with commercial revenues covering less than 30% of the cost of operating the service (see GART, 2015).====We study whether efficiency gains actually materialized by performing an ==== evaluation of the merger focusing on its effect on operating costs of local operators belonging to the merging transport groups. Our analysis of efficiency gains is based on a unique and detailed database that provides information on the characteristics of urban transport networks, as well as detailed costs of urban transport operators in France over the 2006–2014 period, both before and after the merger. This data is further complemented by a database of competitive tenders for the choice of transport operators taking place between 2004 and 2014. In particular, it contains the identities of the incumbent operator, of the winner of the tender, as well as of other operators submitting offers in the competitive procedure. This database enables us to construct various control groups, using information on the intensity of competition in networks and introduce a descriptive analysis of the evolution of the level of competition in the industry.====We employ a difference-in-differences methodology to study the potential efficiency gains resulting from the merger by comparing the evolutions of operating costs of networks operated by the merged companies with those of networks operated by competing operators. Finding a suitable sample and control group to estimate the causal effects of mergers can be a great challenge in many industries (see, for instance, the discussion made by Nevo and Whinston (2010)). The characteristics of the local public transport industry do seem to be a good field for applying the methodology, as we can easily exploit variations in the conditions across local networks.====In our empirical execution, we focus on a sample of networks, where contracts were signed prior to the merger and spanned post merger. This allows us to address a number of potential identification concerns.==== Firstly, it alleviates a potential worry that a switch of operator in a tender organized post the merger may be endogenous. Secondly, it addresses potential spillover effects. Spillover effects may be an issue only if competitors of the merging parties expected the merging party to become more aggressive in competitive tenders following the realization of merger efficiency gains. They could have reacted strategically by improving their bids in competitive tenders and subsequently decreasing operating costs. We believe that the reaction of competitors to the merger, it any, is likely to concern only contracts signed after the merger. Finally, it rules out the possibility that the merger actually gave rise to efficiencies, but these were offset by a reduction of competition through the eliminations of a competitor.====We further consider two main control groups in order to address the possibility that networks operated by competitors of the merging parties have reacted to the merger. The first control group comprises all the networks managed by the three main competitors of Veolia Transport and Transdev, namely CarPostal, Keolis, and RATP Développement. The second control group is more restrictive and comprises only networks where there seems to be no competition from Veolia Transport or Transdev during public tenders. This control group is composed only of networks in which neither Veolia Transport nor Transdev did not submit a bid in tenders for the operation of the transport service throughout the period of our analysis.====To identify more precisely the impact of the merger on the costs of transport operators, we further explore heterogeneity in the cost effects by exploiting the richness of our data. Firstly, we consider the decomposition of operating costs, by considering separately labor and material costs. Secondly, we examine whether the effect of the merger differs between the networks operated by the respective merging groups, Veolia Transport and Transdev. Finally, we also consider the effects of the merger, depending on whether the merging parties were direct local competitors prior to the merger, defined as having networks in the same or neighboring counties (fr. ====). In addition, we undertake a number of robustness tests and consider the use of alternative outcome variables.====Under no specification are we able to reject the hypothesis of non-significant efficiency gains for the merging parties. We complement our main study with an analysis of bidding patterns. Our descriptive analysis suggests a decrease in both the average number of tender participants and the replacement rate. Therefore, even if efficiency gains would have materialized following the merger, they would most likely not be credible enough to counterbalance its potential anticompetitive effects. We further support our finding with anecdotal evidence. Specifically, Veolia Transport was said to have made its merger decision too quickly, resulting not just in a poorly prepared merger, but also employees and clients reluctant to the merger due to differences in cultures between the two groups. This emphasises the role played by the context of the transaction (culture, choice of the target, perception by clients and employees, operational preparation of the merger, etc.) in the lack of materialization of efficiency gains, a question not given much weight neither by economists in their models nor by competition authorities in their analysis of potential effects of mergers.====The ability to generalize our results and comment on the European horizontal merger policy is clearly limited. Hence, we cannot conclude from our results that a change of market structure in the transport sector cannot lead to efficiency gains. However, we contribute to a growing number of case studies undertaken by economists that can help determine whether horizontal merger policy is being properly enforced. Most such studies concentrate on impacts of horizontal mergers on prices. A large volume of empirical research directly estimates the effects of large and/or controversial mergers by employing a difference-in-differences methodology. In particular, many studies exploit the growing availability of data and features of the retailing sector to assess large mergers in Europe (e.g. Aguzzoni, Argentesi, Buccirossi, Ciari, Duso, Tognoni, Vitale, 2014, Aguzzoni, Argentesi, Ciari, Duso, Tognoni, 2016, Allain, Chambolle, Turolla, Villas-Boas, 2017) and in the U.S. (e.g. Ashenfelter, Hosken, 2010, Ashenfelter, Hosken, Weinberg, 2013, Ashenfelter, Hosken, Weinberg, 2015). Earlier work applying a similar methodology focused on mergers in major industries, such as airline markets (Borenstein, 1990, Kim, Singal, 1993), banking (Focarelli and Panetta, 2003) and petroleum (Hastings, 2004, Gilbert, Hastings, 2005). Further, a small literature studies the actual effects of mergers with the aim of assessing the validity of predictions of merger simulation models (see, for instance, Peters, 2006, Weinberg, 2011, Weinberg, Hosken, 2013, Friberg, Romahn, 2015, Björnerstedt, Verboven, 2016).====Despite the growing number of studies analyzing the price effect of mergers in a variety of industries, little work exists on the ==== evaluation of merger-specific efficiency gains. To our knowledge, there exist few studies covering a limited number of sectors that evaluate cost efficiencies from mergers directly. Brito et al. (2013) evaluates the impact of mergers in non-life insurance markets in Portugal through their effect on exercising of market power via coordinated effects and firms’ internal efficiency. Its analysis relies on the specification and estimation of a structural model including preferences, technology, and a market equilibrium condition. It shows that following the mergers, there is no evidence of either an increase in market power through coordinated behavior or changes in cost efficiency levels. Kwoka and Pollitt (2010) study the impact of the merger wave that took place in the U.S. electricity industry by analyzing its impact on operating and total costs in electricity distribution. It employs data envelopment analysis to assess efficiency effects of mergers and concludes, on the basis of its results, that electricity mergers are not consistent with improved cost performance. Dranove and Lindrooth (2003) examines hospital consolidation in the U.S. by focusing on its effect on hospital costs. Cost function estimates of hospitals undergoing consolidation are compared to ==== hospitals chosen based on propensity scores. The empirical strategy is based on the assessment of whether the cost functions of actual mergers are lower than those of pseudo-mergers. It shows that mergers consisting of consolidation of financial reporting and licenses generated cost savings in the 2–4 years following the mergers. Clearly, more retrospective studies are needed to help evaluate the actual effects of mergers on efficiency gains.====The paper is organized as follows. Section 2 presents the French urban public transport industry and describes the merger under study. Section 3 presents our empirical strategy and Section 4 our main empirical analysis, findings, and robustness checks. Section 5 discusses the results and Section 6 concludes.",Merger efficiency gains: Evidence from a large transport merger in france,https://www.sciencedirect.com/science/article/pii/S0167718721000539,9 June 2021,2021,Research Article,106.0
"van Heuvelen Gerrit Hugo,Bettendorf Leon,Meijerink Gerdien","CPB Netherlands Bureau for Economic Policy Analysis, CPB, P.O. Box 80510, The Hague, 2508 GM, The Netherlands","Received 12 February 2020, Revised 10 June 2021, Accepted 10 June 2021, Available online 17 June 2021, Version of Record 11 July 2021.",https://doi.org/10.1016/j.ijindorg.2021.102762,Cited by (2),"We follow the production function approach to assess markups, which requires the estimation of the output elasticity of a flexible input. In the basic setup we estimate a structural value added production function, using temporary contract hours as flexible input. We find rather stable markups in the Netherlands in the period 2006–2016. We show that extending the flexible input incorrectly with fixed contract hours results in an increasing markup. Findings are robust to an alternative setup, in which a gross output production function is specified and materials are used as flexible input. Implications for applied work and policy are discussed.","An expanding body of research finds a sharp increase in the aggregate markups in the US and Europe (including the Netherlands), which is driven by firms located at the top of the markup distribution (i.e. De Loecker, Eeckhout, Unger, 2020, De Loecker, Fuss, Biesebroeck, 2018, Calligaris, Criscuolo, Marcolin, 2018, Diez, Fan, Villegas-Sanchez, 2018, Eggertsson, Robbins, Wold, 2018). Other studies find that markups in the US and Europe have increased only moderately or even remained more or less stable (i.e. Traina, 2018, De Loecker, Fuss, Biesebroeck, 2018, Hall, 2018, Cavalleri, Eliet, Mc Adam, Petroulakis, Soares, Vansteenkiste, 2019, Weche, Wambach, 2018). These differing results have triggered a discussion on methodology (see e.g. Basu, 2019, Flynn, Gandhi, Traina, 2019). A key issue in the production function approach, used in many markup studies, is the choice of the flexible input (an input that is more or less free of adjustment costs), as it determines the trend of the markup (Traina, 2018, Karabarbounis, Neiman, 2018).====The main aim of this paper is to show that the choice of the flexible input is crucial, and may explain at least some of the discrepancies in the findings in the recent literature. We illustrate this with the case of the Netherlands, which has a dual labour market with a large share of flexible work arrangements and temporary labour contracts. This duality enables us to use only the temporary labour hours for the flexible input. However, all inputs in the production function should be subject to adjustment frictions, as the variation caused by these frictions is the source of the identification of output elasticities (see Ackerberg et al., 2015). The identification strategy will fail with inputs that have no adjustment frictions, such as temporary labour hours (Gandhi, Navarro, Rivers, 2020, Flynn, Gandhi, Traina, 2019). We address this identification problem by applying the solution proposed in De Loecker and Scott (2016) and Gandhi et al. (2020) and estimate the production function with wages, a serially correlated input price that differs across firms, in the control function. In addition to estimating a Cobb Douglas (CD) function, we report results for the more flexible Translog production function.====Our results show that when we take the duality of the Dutch labour market into account and use as flexible input the labour of temporary workers in hours, we obtain stable markups in the Netherlands for the period 2006–2016. We find that firms located at the upper end of the markup distribution increase their markups. However, this increase is far below the magnitude found in other papers and is not driven by large firms. We demonstrate that extending the flexible input incorrectly with fixed contract hours results in an increasing markup. This underscores the need to take into account the institutional features of a country in measuring markups. Second, we find that alternative setups produce fairly similar results as our preferred setup. Our preferred setup includes as flexible input temporary labour and its output elasticity results from estimating a structural value added production function. In an alternative setup we combine a gross output function and materials as flexible input.====Instead of measuring labour in persons or costs, we construct a series consisting of working hours. According to Basu (2019), estimating markups using the production function approach is best executed using a single, distinct measure of physical input as flexible input. In the Netherlands, temporary contract workers can be hired or fired more easily; their input will thus adjust more quickly to changes in output and hence is a better flexible input. The OECD publishes indicators of the strictness of regulation on dismissals for temporary and regular contracts (OECD, 2020b). The latest indicator (for 2013) shows a large gap for the Netherlands between regular and temporary contracts (2.82 versus 0.94), indicating that it is easier to hire and fire workers on a temporary contract than a regular contract. By contrast, France has stricter regulation for temporary contracts (3.63) than regular contracts (2.38). The US has one of the lowest restrictions on firing employees on regular contracts (1.10) and temporary contracts (0.25).====The share of temporary contract employees in the Netherlands is one of the highest in the OECD. With a share of 20.8% in 2016 it is almost twice as high as the OECD average of 11.8% (OECD, 2020a). We see in Fig. 1 that employees with a temporary contract leave the workforce during downturns and enter during upswings of the Dutch economy, thus reflecting the lower hiring and firing costs of temporary contracts. In comparison, fixed contract labour hours are less affected by the business cycle.",Markups in a dual labour market: The case of the Netherlands,https://www.sciencedirect.com/science/article/pii/S0167718721000552,17 June 2021,2021,Research Article,107.0
"Gerlach Heiko,Nguyen Lan","School of Economics, University of Queensland, St Lucia QLD 4069, Australia,Department of Economics, Tulane University, New Orleans, USA","Received 12 October 2018, Revised 20 April 2021, Accepted 8 May 2021, Available online 9 June 2021, Version of Record 28 June 2021.",https://doi.org/10.1016/j.ijindorg.2021.102757,Cited by (0),". We also discuss the choice between simultaneous and staggered price increases with an exogenous antitrust detection function, the allocation of price leadership with cost asymmetry, and the effect of product differentiation on price staggering.","Over the last two decades antitrust authorities around the world have stepped up their efforts to prosecute hard-core cartels. Records from recent cartel cases show that companies are well aware of the increased threat. Strategies to avoid detection from buyers and antitrust authorities have become part of the organization of cartels. A particularly well documented practice of cartels to avoid the appearance of collusion is price staggering, the orchestration of sequential rather than simultaneous price increases.==== Consider, for example, the ==== cartel prosecuted by the European Commission (EC):====Similarly, the ==== cartel used sequential price increases as they “...could be passed off, if challenged, as the result of price leadership in an oligopolistic market.”==== However, staggering a price increase is costly as the price leader risks losing sales before the follower raises the price. This was a clear concern in the ==== cartel:====Flexsys’ concerns were later attested as it lost significant volumes and key customers.====In this paper, we develop a theoretical model to study the optimal organization of staggered pricing in cartels. In our benchmark model, we consider a Bertrand duopoly with homogenous goods when firms compete in prices with an infinite horizon. In response to the cartel’s perceived risk of being investigated at the end of a simultaneous price move, the firms are interested in raising the industry price from its (exogenously given) current level towards a target price. A staggered price increase involves the cartel specifying a leader who raises the price to the target level in period ====. In period ====, the other cartel member follows suit and raises its price to the same level. In our benchmark model we show that, for intermediate discount factors, a single staggered price increase can be implemented if and only if the target price is neither too small nor too large relative to the follower price. Large price increases give the follower a strong incentive to ==== by just undercutting the target price. By contrast, small price increases are not enough to prevent the leader from ==== and attract current period sales. Hence, a sustainable staggered price increase is of intermediate size to balances these deviation incentives.====Following some well-documented cartel cases, we then explore a situation where the cartel is scheduling two consecutive staggered price increases. We show that whether a cartel chooses the same leader in each increase or alternating price leadership depends on the initial price level relative to the final target price. A low current industry price requires a larger first increase and gives the first follower a stronger incentive to deviate upwards. In order to sustain the cartel, it is then optimal to again allocate this firm the follower role (and the associated higher profits) in the second price increase. Vice versa, if the current price is relatively high, the first leader is more tempted to deviate downwards and is thus assigned the follower role in the second increase to stabilize the cartel. We also show that two price increases are sufficient to take the cartel from any starting price to the monopoly price.====Furthermore, we introduce an explicit antitrust detection function to analyze the cartel’s optimal choice between simultaneous and staggered price increases. We demonstrate that staggered pricing is easier to sustain than a simultaneous price move when the planned price increase is of intermediate size. We also extend our analysis to cartel with asymmetric firms and differentiated products. When cartel members face asymmetric cost of production, the cartel increases the sustainability of a price move if it assigns the leader role to the small, high-cost firm while the low-cost firm follows. The effect of product differentiation on the sustainability of staggered price increases is ambiguous. A move from the competitive to the monopoly price level is harder to sustain relative to a situation with homogenous products if and only if the degree of product differentiation is intermediate.====The literature on cartels is usually concerned with sustaining collusion at a given level such as the monopoly price.==== Very little attention has been given to the gradual process of reaching that target price level which seems to be a major impediment to cartel formation in practice. Harrington (2005) characterizes the optimal cartel price path to its steady-state level in the presence of an exogenous buyer detection process and (accumulating) damage awards. The steady-state cartel price is strictly below the monopoly level when penalties include (price dependent) damage awards. When penalties are price independent, the cartel reaches the monopoly price in the long run.==== Harrington (2004) extends the analysis of this framework by considering lower discount factors in which the cartel stability constraint is binding on the optimal price path. In this case, numerical analysis shows that the optimal cartel price might be first increasing and then decreasing. In both papers cartel members charge the same price and raise their prices simultaneously. We consider sequential price increases when buyers and antitrust authorities are suspicious of simultaneous price movements of cartel members.====Our paper also relates to the long-standing discussion of price leadership in industries. Going back to the early work by Stigler (1947) and Markham (1951), three (overlapping) forms of price leadership are typically distinguished. Dominant firm price leadership occurs when one large producer sets its price and a smaller competitor or a competitive fringe follows as price takers. Deneckere and Kovenock (1992) show that when two capacity-constrained firms compete in prices, the larger firm emerges as the leader in equilibrium. The reason is that the small firm stands to lose more from being undercut and sets a lower price as leader. By contrast, the large firm can provide a price umbrella which allows the small firm to undercut and sell its entire capacity.==== Competitive, barometric price leadership refers to situations where changes in prices reflect market conditions and asymmetrically informed firms.==== Finally, collusive price leadership occurs when the process of price changes is intended to coordinate prices at the collusive level. For example, Byrne and de Roos (2019) document in an empirical study of the retail gasoline market that dominant firms used price leadership to create a focal price point at the top of the petrol cycle. By contrast, in our analysis, firms are not concerned about learning the optimal collusive price level but they stagger price increases to avoid antitrust detection.====More recently, Ishibashi (2008) and Mouraviev and Rey (2011) use a framework that endogenizes the timing of firms’ strategy choice in a repeated game. In each period there is an extended stage game with action commitment in which the firm who wants to lead can commit to its price choice. The waiting firm can observe the choice of the leader and select its price before demand is realized. Mouraviev and Rey (2011) show that price leadership can drastically increase the sustainability of price collusion. A deviation of the leader can be immediately punished and the follower can be assigned a higher market share to prevent undercutting. Ishibashi (2008) demonstrates that the firm with the larger capacity emerges as price leader in order to demonstrate its commitment not to deviate. Finally, Marshall et al. (2008) develop a theoretical model of price announcements in a duopoly and use data from the Vitamins cartel to determine the existence of explicit collusion based on the pattern of price announcements. Our paper differs from this literature in two aspects. We do not require explicit price commitment as it arises endogenously in our model. More importantly, we consider situations where the price leader loses demand to the follower during a staggered price increase. This introduces the possibility of upward and downward price deviations which the optimal organization of a staggered price increase has to overcome.====The remainder of the paper is organized as follows. In the next section, we set up and analyze the benchmark model with one staggered price increase. In Section 3, we derive the optimal organization of a cartel that implements two staggered price increases. In the following section we introduce an explicit antitrust detection function. Sections 5 and 6 consider cartels with cost asymmetry and product differentiation, respectively. The last section concludes. All proofs are relegated to the Appendix.",Price staggering in cartels,https://www.sciencedirect.com/science/article/pii/S0167718721000503,9 June 2021,2021,Research Article,108.0
"Cooke Dudley,Fernandes Ana P.,Ferreira Priscila","University of Exeter, England, United Kingdom,University of Minho, NIPE, Portugal","Received 21 July 2020, Revised 14 June 2021, Accepted 16 June 2021, Available online 24 June 2021, Version of Record 8 July 2021.",https://doi.org/10.1016/j.ijindorg.2021.102763,Cited by (1),"This paper identifies a causal link between changes in product market competition, firm reorganization and within-firm wage inequality. We exploit a unique episode of comprehensive firm entry deregulation as a quasi-natural experiment and use exceptionally detailed linked employer-employee data for the universe of private sector firms and workers in Portugal. Following deregulation affected firms flatten their hierarchies: the number of layers is reduced and managers’ span of control increased. Dropping a hierarchy layer is accompanied by a significant reduction in wage inequality within the firm, by 8% for the average pay ratio between the top and the bottom layer and 4.4% for the 90-50 percentile wage ratio, showing that there are real changes arising from firm reorganization. Overall wage dispersion, measured by the ==== of hourly pay, is also reduced. We discuss mechanisms and interpretations for these changes.","The production of any good requires time, knowledge, and collaboration between individuals and teams. Recent theories have focussed on organizations taking the form of “knowledge hierarchies”, where production workers deal with routine tasks and experts specialize on giving directions to solve more complex problems.==== The organizational choice for a firm is to determine the structure of the hierarchy, given by the number of layers of increasing knowledge and the span of control of experts. Firms tend to change the organization of production in response to shocks, such as deregulation and international trade. Increased competition and uncertainty may induce firms to significantly change their hierarchical structure. The restructuring is then expected to affect wage inequality within firms, as knowledge requirements change across the hierarchy.====Recent research has emphasized the role of hierarchies and organizational practises in explaining firm growth and productivity, and the distribution of wages (e.g., Garicano and Rossi-Hansberg (2006); Caliendo and Rossi-Hansberg (2012); Caliendo et al. (2015)). However, there is still very limited evidence on what causes firm reorganization, and particularly how competition shocks affect within-firm wage inequality as firms adjust by re-organizing production.====This paper studies how increased domestic product market competition induces firms to change their internal organization and how this change affects wage inequality. We investigate the effect of firm entry deregulation on the structure of a firm’s hierarchy, particularly the number of layers and the average span of control of managers. We also study how these changes affect the distribution of wages within the firm and wage inequality. An important contribution of our paper is to identify a causal link between changes in competition in the domestic product market and firms’ organizational change and wage inequality. To do so, we exploit a unique episode of comprehensive firm entry deregulation across industries as a quasi-natural experiment, and use exceptionally detailed employer-employee linked data for the universe of private sector firms and all of their workers.====Our main findings are that increased domestic competition reduces firm-level sales and leads firms to flatten their hierarchies: they reduce the number of layers and increase managers’ spans of control. Reorganization is accompanied by a reduction in within-firm wage inequality. These results are consistent with knowledge-based and incentive-based hierarchy theories (e.g., Caliendo and Rossi-Hansberg (2012); and Chen (2017)), which predict that the optimal number of layers increases with production scale. Differences in pay across layers are modelled as a function of knowledge or monitoring effort, respectively, and reducing the number of hierarchical layers affects the distribution of pay and inequality within the firm as skill requirements and incentives change for all workers. At the worker-level, we find that the pay and career transitions of individual workers are also affected, showing that there are real changes arising from firm reorganization.====To identify the causal effect of increasing domestic competition on organizational change and inequality, we exploit an exogenous change in entry barriers. We use the “On the Spot Firm” (OTSF) program, implemented in Portugal from 2005 to simplify business registration, as a natural experiment. The program created government offices (‘one-stop shops’) where entrepreneurs can register a new firm in a single visit, while prior to the reform it took 78 days on average, and the requirement to complete numerous procedures and forms, involving visits to several different public offices. The fees were also reduced from 13.5 to 3% of GDP per capita in the “On the Spot Firm” offices (Bank, Bank). The reform was implemented in different municipalities over time randomly, and by the end of 2009, 164 municipalities had a one-stop shop, as shown in Fig. 1. The initiative was hugely successful, resulting in a significant increase in firm entry. Portugal is now among the countries where starting-up a business is fastest in the world.====We use the roll out of the “On the Spot Firm” program, the cross municipality-time variation in adoption, to cleanly identify the effect of increased competition on firms’ corporate hierarchies and pay structure. To study firm’s internal organization and wages, we use comprehensive employer-employee linked data, which tracks each firm and each employee over time. The data has unusually rich and detailed information on workers’ characteristics, such as gender, age, education, skill level, occupation, experience, type of contract of employment, hours of work and earnings. We measure changes in organization using hierarchical occupations to define four layers of increasing knowledge and responsibility in the firm, following recent literature (e.g., Caliendo et al. (2015)). The data also has information on the firms’ industry, location, employment, number of establishments, sales volume, and legal and ownership structures.====To motivate the subsequent empirical analysis of the causal effect of the reform on firm organization and inequality, we start by documenting a positive and significant relationship between the number of hierarchy layers and sales in our data and between the number of layers and within-firm pay inequality, consistent with the theory. Specifically, lower sales are associated with flatter hierarchies and firms with less layers have lower wage inequality, measured by the pay gap between the top and the bottom layer, a measure closely linked to the theory, and by the 90-10 and 90-50 percentile wage gaps, to measure changes in the distribution of pay. Wage variation within firms across layers accounts for 47% of within-firm wage inequality in our sample. Therefore, variation in wages across layers is important to understand the distribution of wages within firms.==== We also show that the “On the Spot Firm” program significantly increased firm entry within industries and municipalities and, as new firms enter the market, firm sales, output and employment of affected incumbents are reduced. In line with the theory, the negative shock to production scale following the reform is expected to induce firms to reorganize by dropping layers and in turn reduce within-firm inequality.====We find that firms changed the structure of the hierarchy in response to the reform. In particular, our estimates show that the depth of the hierarchy, measured by the number of layers, is significantly reduced and the span of control of top managers increased after the “On the Spot Firm”. The effects are largest for firms with three and four layers prior to the reform. The data that we use allow us to obtain estimates that control for observed firm characteristics, as well as for unobserved firm specific heterogeneity. The fact that adoption of the reform varied across municipalities and time allows us to control for municipality-specific and time-specific effects in our specifications. Our finding that firms responded to the negative shock to production scale following the reform by dropping layers is consistent with theoretical results.====We also find that wage inequality decreased after the reform within affected firms that reorganize by delayering, whilst the effect is statistically insignificant for firms that did not change the number of layers, consistent with a reduction in inequality arising through firm reorganization. Specifically, the reduction in hierarchical layers is accompanied by a 8% drop in the top-bottom-layer pay ratio within firms, relative to the sample mean. We also estimate a 6% reduction in the 90-10 percentile pay ratio and a 4.4% decrease in the 90-50 wage gap within the affected firms that reorganize. The standard deviation of log pay, a measure of overall dispersion, also decreased. We also find that the top-to-bottom layer knowledge gap, measured by formal education (schooling) or labor market experience is reduced within firms. These results are consistent with knowledge-based and incentive-based hierarchy theories, where reducing the number of layers in the hierarchy affects the wage distribution and inequality as the distribution of knowledge and incentives change across the organization. The effects are driven by workers that stay in the same firm after the reform, and are not arising from changes in workforce composition.====Finally, we assess individual-level growth of pay and career transitions after the reform, to paint a more detailed picture of firms’ changes in organizational and wage structures in response to the policy change. We find that workers in all layers prior to the reform are more likely to exit the affected firms that reorganize and that stayers in the firm are less likely to be demoted or promoted, with the exception of workers in the bottom layer, which have a higher probability of promotion, within the firm or across firms. The flattening of firms’ hierarchies induced by the reform can therefore have lasting consequences on the career paths of individual workers. We also find that the growth rate of pay increases for workers in the bottom layer, consistent with the reduction in inequality through reorganization.====Our results are related to Guadalupe and Wulf (2010), who show that competition from international trade liberalization, following the Canada-US free trade agreement, leads to flatter firm hierarchies. Our analysis focusses on increased domestic competition, from an exogenous shock to entry barriers. As such, we provide independent evidence of the importance of greater product market competition for firm reorganization. Additionally, our detailed employer-employee data allows us to go further in investigating the real effects of organizational change following a competition shock, particularly the distribution of wages within the firm as well as individual pay and career advancement. In that respect, our paper is related to recent research by Friedrich (2020), who finds that a negative trade shock leads Danish firms to delayer, and dropping a hierarchy layer significantly reduces inequality throughout the organization. We present new evidence of a causal effect of increased domestic firm entry following deregulation on firm organizational restructuring and reduction in within-firm wage inequality.====Our paper contributes to the empirical literature in several respects. Studies that identify a causal effect of competition on firm reorganization and within-firm wage inequality are very scarce. While previous work has focussed mostly on trade shocks, we exploit an economy-wide episode of firm entry deregulation across industries. As such, our natural experiment allows us to obtain results that can be interpreted more generally than for exporting firms, whose organizational and compensation structures may be marked by idiosyncrasies. Moreover, the role of entry barriers is of considerable importance as countries seek to increase competition through deregulation. Evidence of how deregulation can affect firm hierarchies and wage inequality more broadly can have future policy implications for other countries. The staggered nature of the implementation of the OTSF reform over time across municipalities also offers a unique natural experiment to cleanly identify the effects.====Our paper also contributes to the literature that has used theories of knowledge-based hierarchies to understand economic phenomena such as firm productivity, wage inequality, and the gains from trade liberalization (e.g., Caliendo and Rossi-Hansberg (2012)). Caliendo et al. (2015) show that reorganization within French firms, through changes in hierarchical layers of workers, is important to understand how firms grow and contract and the evolution of wages and employment in each layer, as predicted by the theory. In a recent paper, Caliendo et al. (2020) show that Portuguese firms that reorganize by adding a management layer increase quantity-based productivity and decrease revenue-based productivity. A related literature focusses on incentive-based hierarchies, where supervisors, in higher layers, incentivize workers to exert more effort by increased monitoring of subordinates (see Chen (2017); Chen and Suen (2019)).====Previous studies investigate the effect of product market and entry regulation on labor market outcomes. Notably, Bertrand and Kramarz (2002) show that entry regulation in the retail sector increased retailer concentration and slowed down employment growth in France. Blanchard and Giavazzi (2003) develop a macroeconomic model to study distribution effects of product and labor market deregulation; the entry of new firms reduces mark-ups thus increasing the probability of unemployment for workers employed in incumbent firms, even as overall unemployment falls. Fernandes, Ferreira, Winters, 2014, Fernandes, Ferreira, Winters, 2018 find that the same reform analyzed in this paper increased the returns to skills and executive compensation at the worker-level, as demand for those workers increases with firm entry. While those studies estimate the effects at the individual-level, conditional on worker skills, and do not condition on changes in hierarchy, this paper studies within-firm pay inequality arising through reorganization - for firms that flatten their hierarchy following the reform - and considering changes in workers’ skills.====Finally, our paper contributes more generally to a literature on within-firm wage inequality. Mueller et al. (2017) show that firms with higher pay inequality between top- and bottom-level jobs have higher valuations and stronger operating performance. Song et al. (2019) document the rise in earnings inequality between workers in the US and show that the rise in within-firm inequality occurred mostly within large firms. We show that deregulation contributes to reduce within-firm inequality through reorganization.====The rest of the paper is organized as follows. The next section discusses the conceptual framework that guides the empirical analysis. Section 3 describes the data used and presents descriptive statistics. Section 4 documents stylized facts about the relationship between firm scale, organization and wage inequality. In Section 5 we describe the “On the Spot Firm” quasi-natural experiment and outline the empirical strategy. Section 6 presents and discusses the results on the effect of the policy change on firm creation, production scale, firm hierarchical structure and wage inequality within the firm. The last section concludes.","Entry deregulation, firm organization, and wage inequality",https://www.sciencedirect.com/science/article/pii/S0167718721000564,24 June 2021,2021,Research Article,109.0
"Bennato Anna Rita,Davies Stephen,Mariuzzo Franco,Ormosi Peter","Loughborough University, Epinal Way, Loughborough LE11 3TU, United Kingdom,School of Economics and Centre for Competition Policy, University of East Anglia, Norwich Research Park, Norwich, NR4 7TJ, United Kingdom,Centre for Competition Policy, University of East Anglia, Norwich Research Park, Norwich, NR4 7TJ, United Kingdom","Received 13 October 2020, Revised 13 May 2021, Accepted 19 May 2021, Available online 29 May 2021, Version of Record 15 June 2021.",https://doi.org/10.1016/j.ijindorg.2021.102755,Cited by (0),This paper is a retrospective evaluation of how innovation changed following ,"This paper examines how the consolidation in the worldwide hard disk drive (HDD) market affected innovation. This involved three mergers in 2011/12 (Seagate/Samsung, Western Digital (WD)/Hitachi, and Toshiba/Hitachi’s 3.5-inch production), which reduced the number of competitors from 5 to 3 firms. The first two were agreed with the competition authorities subject to remedies, one of which involved a divestment of certain assets to Toshiba, and we count the acquisition of these divested assets as the third merger.====The paper contributes to two different strands of literature. The first is the policy-driven literature on retrospective evaluations of the competitive impact of mergers. Although that literature is voluminous, it almost entirely focuses on just the price effects of mergers: two studies, Ashenfelter et al. (2014) and Kwoka (2014) identified and reviewed more than 60 such case studies, and Mariuzzo and Ormosi (2019) further disaggregated these retrospectives and reviewed over 600 market-level price-impact estimates. In stark contrast, however, we find much less on how mergers affected innovation. This paper addresses that gap. Second, it adds to the wider literature on the relationship between competition and innovation. Here, the seminal works by Arrow (1962) and Schumpeter (1942) have generated an enormous body of subsequent research on whether competition stimulates or discourages innovation, but few writers have examined the impact of mergers. As Gilbert notes in a contemporary comprehensive review: “Only a few empirical studies apply sophisticated statistical techniques to uncover the effects of mergers on R&D and innovation” (Gilbert, 2020, p.6). Furthermore, amongst the few that there have been, most have provided only aggregate, and sometimes rough, evidence, summarising the average effect across large samples of markets. Remarkably, few have focussed on specific markets, and again this paper hopes to address this gap with an in-depth case study.====This particular case itself is of interest from a variety of perspectives. In policy terms, a reduction from 5 to 3 in any market is sufficiently concentrating to alert the interest of competition authorities: a common policy question is whether 3 is sufficient to maintain competition? Analytically, it provides an unusual chance to assess comparatively three mergers occurring more or less simultaneously, within the same market. On the other hand, it poses problems when seeking for a viable comparator. Geographically, it is a worldwide market and therefore spatially unique; in terms of product space, while there are other industries in the same broad sector of data storage, they are at different stages of their product cycle, which makes supply-side comparability difficult. In this respect, the case is representative of many others in a world of globalised markets.====We attempt to contribute to both academic methodology and merger policy debates. Methodologically, we depict ’innovation as multifaceted, involving measures of inputs (R&D spending and patent activity) and ’outputs of innovations (the number of new models and the quality-adjusted unit price of HDD capacity). For this purpose, the paper assembles a unique dataset of four different measures of innovation in a single case study. This is a more holistic approach than is usual in the previous empirical literature, and the greater breadth takes us back closer to some of the typologies to be found in the founding writers in the technology literature - Schumpeter’s distinctions between invention and innovation, and process versus product innovation. It also allows us to distinguish conceptually between the effects of the merger on the amount of R&D (say) and the productivity of that R&D.====To make causal inferences from this data we use a matrix competition method, which generates the counterfactual scenario (the no-treatment outcome for the treated units) from the observed elements of the matrix of control outcomes corresponding to untreated units/periods. Matrix completion methods relax on the parallel trend assumption, which would be required in a difference-in-difference setting and which is something we cannot assume away on at least some of our outcome measures.====The paper provides significant evidence of increased R&D but falling patent activity following the mergers. We argue that the fall in patent numbers is most likely a reflection of the removal of duplication and defensive patenting. Regarding the innovation outputs, there is a heterogeneous response across firms, with Seagate and Toshiba, increasing, but Western Digital reducing the number of their new models. To examine how innovation input drives innovation output, we bring our measures together in a fixed effect model, to show that the merger contributed to improved R&D productivity for Seagate. On the other hand, for Western Digital there is no evidence that the increased R&D contributed to increased innovative outputs - they significantly reduced the range of marketed new products. In the case of Seagate, we interpret this as a sign of important synergies between Seagate and Samsung, which were substantiated by the merger. We believe that the heterogeneous innovation response to the mergers could be a sign of the difference in the intensity of merger interventions, as Western Digital was effectively held separate from Hitachi for years after the merger, whereas Seagate was objected to much less severe remedies.====As a retrospective merger study, we hope the case is of interest in its own right - enabling us to assess the competition authorities’ decisions on whether or not to permit the mergers, albeit subject to remedies. As just mentioned, these mergers combined to reduce the number of participants from 5 to just 3 in a strategically important hi-tech industry - in such a case, it is important to scrutinise the decisions of competition authorities. More generally, and looking forward, innovation-focused ex-post evaluations are likely to become ever more important in the competition policy debate on information-intensive markets. Hopefully, this paper offers some insights into some of the conceptual and methodological which will confront similar future case studies.",Mergers and innovation: Evidence from the hard disk drive market,https://www.sciencedirect.com/science/article/pii/S0167718721000485,29 May 2021,2021,Research Article,110.0
"Barbosa Klenio,Boyer Pierre C.","SKEMA Business School – Université Côte d’Azur, 60 Rue Fedor Dostoievski, Sophia Antipolis 06902, France,CREST, École polytechnique, Institut Polytechnique de Paris, 5 avenue Henry Le Chatelier, Palaiseau 91120, France","Received 17 June 2020, Revised 21 April 2021, Accepted 16 May 2021, Available online 27 May 2021, Version of Record 11 June 2021.",https://doi.org/10.1016/j.ijindorg.2021.102754,Cited by (1),"We study the long-run impact of procurement discrimination on market structure and future competition in ==== where learning-by-doing makes incumbent firms more efficient over time. We consider a sequential procurement design problem in which local and global firms compete for public good provision. Both firms benefit from learning-by-doing if they provide the public good in the previous period, but global firms only may be able to transfer learning-by-doing from different markets. We show that the optimal procurement has to be biased in favor of the local firm even when all firms are symmetric with respect to their initial cost distribution.","Discriminatory programs that favor local and small firms in government procurement are common in several countries. In the US, for instance, the Small Business Act explicitly requires that the federal government grants a significant portion of the annual procurement contracts and sales to small businesses.==== Under the Buy-American Act, the United States Government offers a 6 percent bid preference to domestic suppliers. The European and Japanese governments do not explicitly state the formulae by which local and small bids are to be compared with foreign bids. Instead, these governments achieve favoritism by more covert methods: allowing a short time for the submission of bids, applying residence requirements on bidders, and defining technical requirements in such a way that it is difficult or impossible for foreign firms to comply (see, e.g., McAfee and McMillan (1989); Coviello and Mariniello (2014)).====A significant literature in economics contains findings that support the desirability of these programs. Several papers have shown that bid preferences programs in favor of weak (local and small) firms may lower government procurement cost (e.g. Myerson (1981); McAfee and McMillan (1989)) and achieve distributional goals (e.g. Branco (1994); Vagstad (1995); Naegelen and Mougeot (1998)). Nevertheless, these works are static and usually assume that local suppliers are weak firms. Indeed, procurement is a repeated game, and current awarding policies may affect the future market structure of industries in which suppliers’ efficiency is endogenously determined. Previous studies have analyzed repeated games of procurement and the dynamic effects of discrimination and bid preference programs (Lewis, Yildirim, 2002, Iossa, Rey, 2014, Arve, 2014, Cisternas, Figueroa, 2015). Although drawing important insights on the trade-offs in dynamic procurement, these works do not examine how discrimination policies in procurement affect market structure and future competition.====In this paper, we characterize the optimal dynamic procurement design when firms are symmetric with respect to their initial production cost, but learning-by-doing makes incumbent firms more efficient over time and some firms have synergies across markets.==== Our dynamic environment allows us to account for the long-run impact of procurement discrimination on firms’ efficiency and market structure.====To investigate the dynamic aspects of discrimination in procurement, we study a two-period model of procurement design in which two types of firms compete for public good provision: local and global firms. In each period, the public authority chooses the procurement mechanism that maximizes the social welfare net of the cost of public funds. Firms privately know their own cost for producing the public good in the correspondent period. Firms are symmetric with respect to their initial first-period cost distribution, and both firm types benefit from learning-by-doing, in the form of expected cost reduction over time, if they provide the public good in the previous period. Contrary to the local firm, the global firm may have access to learning-by-doing even when it is not the incumbent in the local market. Indeed, global firms have synergy advantages and learning-by-doing may be transferable from external markets – the unique difference between local and global firms that we consider.====Due to learning-by-doing, local and global firms may have high or low expected production costs in the last period of the game. This potential asymmetry between firms, combined with the assumption that firms are privately informed about their cost, implies that the authority faces the well-known static trade-off between rent extraction and efficiency in the second period (see, e.g. Myerson (1981); McAfee and McMillan (1989). In order to induce truthful revelation of production cost, the public authority has to give an extra rent to firms. The effective cost of the provision by a firm is the virtual production cost: the sum of firm’s actual cost plus the informational rent. When the authority knows that firms have asymmetric costs’ distribution, it is optimal to bias the awarding rule in favor of the firm with the highest expected cost in order to reduce the informational rent of the firm with the lowest expected one. As a result, the public authority may not select the firm with the lowest cost (i.e. the most efficient one) if it has a too high informational rent.====In the first period, firms have symmetric costs’ distribution. Therefore, there is no need to bias the procurement from the static trade-off perspective. However, biasing the procurement may still be optimal. Indeed, the authority faces a dynamic trade-off between low monetary transfer today and high monetary transfer tomorrow, which has not been addressed in previous studies. On one hand, a selection of a global firm with low cost in the first period implies low monetary transfer in this period. On the other hand, it implies high expected monetary transfer in the second period: the local firm without previous benefit from learning-by-doing will certainly have higher expected cost in the second period. In contrast, the global firm may still be a competitor with low expected cost in the second-period even if it is not the incumbent firm in the local market. These effects go in opposite directions. We show the conditions on the cost distribution such that it is optimal for the public authority to discriminate in favor of the local firm in the first period. These conditions hold when costs are distributed according to Uniform, Pareto, Log-Normal and Exponential distributions. This bias in favor of the local firm increases the chances of creating a pool of firms with low expected cost in the future, enhancing future competition and reducing expected transfer. Accordingly, a first-period bias is optimal even if part of the competitive advantage of an incumbent firm will be corrected by a bias in the second-period procurement. This implies that the local firm may be optimally selected, even though it has higher first-period production cost than the global one.====In a nutshell, our paper shows that the benevolent authority has to bias the mechanism in every period in favor of the least efficient firm. This bias fosters competition and reduces transfers to the provider. Our main result suggests a bias favoring a local firm, when a local firm is more likely to be the least efficient one from the dynamic perspective.====To understand the main factors that determine these results, we consider the optimal procurement mechanism when the public authority can make long-term commitment as a benchmark. In this setting, the public authority commits to a two-period contract at the beginning of the first period after the firms are privately informed of the first-period costs. The optimal procurement mechanism with commitment dictates that the public provision shall be awarded for two consecutive periods to the firm with the lowest first-period cost. Hence, when the public authority can commit to a long-term contract there is no handicapping in first period. This result suggests that the first-period handicapping of the local firm, prescribed by the optimal sequential mechanism in paragraph above, is driven by the public authority’s lack of commitment.====In the basic model we assume that the local firm can produce the public good only in the local city, whereas the global firm can produce in several markets (local market and elsewhere). Although this distinction between global and local firms allows us to draw conclusions on the optimal discriminatory procurement when the nature of the competitors is taken as given, it abstracts from the fact that discriminatory mechanisms can affect local firm’s decision to compete locally or globally. To examine the model when the nature of competitors is endogenously determined, we consider an extension in which the local firm can decide at an ex ante stage to be an exclusive local competitor or a global one. Following the trade literature, we consider that the local firm acting globally can obtain profit from an outside market, but it has an imperfect perception about the external market profitability (De Loecker, 2007, Greenaway, Kneller, 2007, Albornoz, Pardo, Corcos, Ornelas, 2012, Eaton, Eslava, Jinkins, Krizan, Tybout, 2015, Atkin, Khandelwal, Osman, 2017). The results of this extension show that it is socially optimal that the local firm decides to be a global competitor, regardless the local firm’s external profitability perception. We also find that the first-period discrimination in favor of an exclusive local firm can induce the local firm to serve only the local market. These results imply that the local-favored first-period auction leads to a social welfare distortion on the nature of the local firm. Note that, if the public authority could commit to not favor the exclusive local firm in the first-period auction, then the local firm will never choose to be an exclusive local competitor (and the ex ante social optimal is achieved). Nevertheless, commitment to non-favoritism is a non-credible threat: when the public authority is called upon ex interim to design the first-period mechanism, it will optimally bias the first-period mechanism in favor of an exclusive local player. These findings reveal that lack of commitment to non-discrimination in procurement is a fundamental driving force behind the results of this paper.====Our results are interesting for a variety of reasons. First, recent empirical works study the effect of discriminatory programs that favor potentially weak bidders in several industries. For instance, Marion (2007) and Krasnokutskaya and Seim (2011) study the effect of small business bid subsidies in California highway procurement auctions, and Athey et al. (2013) analyze set-asides of forest service timber auctions for small businesses in the US.==== These studies find that discriminatory programs have negative effects on public budgeting, increasing procurement costs and reducing government revenue. In particular, the pool of efficient firms participating to the procurement contest seems to be determinant for procurement costs. Even though all these studies only take into account the instantaneous and short-run effects of those programs, they suggest that the long-run impact of discrimination on market structure and future competition are keys aspects and should be considered when evaluating those policies. Our paper provides a theoretical framework to understand the dynamic implications of discriminatory programs.====Second, in industries with learning-by-doing and synergies across markets, the use of a repeated symmetric mechanism rather than the optimal discriminatory mechanism characterized in this paper has important economic consequences. In particular, in a repeated first-price sealed-bid auction without discrimination, the public authority pays higher expected monetary transfer for good suppliers, and the global firms earn higher rents. Intuitively, global firms have lower expected production cost than local ones due to learning-by-doing and synergies across markets. With level-playing-field bidding, the latter would impose little competition pressure on the former, who could be get away with bidding relatively high.====The remainder of the paper is organized as follows. Section 2 reviews the related literature. Section 3 describes our model. Section 4 characterizes the benchmark of optimal procurement mechanism when the public authority can commit to a long-term contract. Section 5 presents the optimal dynamic procurement mechanism when the public authority offers sequential short-term mechanisms. By solving the model by backward induction, we first characterize the optimal second-period procurement mechanism, and then the optimal first-period mechanism. Section 6 examines how this optimal discriminatory procurement affects the local firm’s decision to become global. Section 7 discusses the robustness of the results to alternative modeling setups. The last section contains concluding remarks. The proofs of the propositions and lemmas can be found in the Appendix. A supplementary material contains complementary results and numerical exercises.",Discrimination in Dynamic Procurement Design with Learning-by-doing,https://www.sciencedirect.com/science/article/pii/S0167718721000473,27 May 2021,2021,Research Article,111.0
"Böheim René,Hackl Franz,Hölzl-Leitner Michael","Johannes Kepler University Linz, Wifo Wien; IZA, Bonn, CESifo Munich, Germany,Johannes Kepler University Linz, Germany","Received 1 October 2019, Revised 25 February 2021, Accepted 4 May 2021, Available online 25 May 2021, Version of Record 8 June 2021.",https://doi.org/10.1016/j.ijindorg.2021.102743,Cited by (3),"We analyze price dispersion using panel data from a large price comparison site. We use past pricing behavior to instrument for potential endogeneity of firms to select into certain product markets. Our main result is that greater price adjustment costs result in greater price dispersion even in e-commerce markets where price adjustment cost are thought to be negligible. Although the impact of price adjustment costs on price dispersion became weaker over time, the causal effect of price adjustment costs on price dispersion is still present at the end of our observation period. Our results are robust to many alternative empirical specifications. We also control for a range of alternative explanations of price dispersion, such as search cost, service differentiation, obfuscation, vertical restraints, and market structure.","Economists frequently focus on price dispersion (PD) because it “reflects the ignorance of the market” (Stigler, 1961). In efficient markets, PD should only be a disequilibrium phenomenon that vanishes over time, and in perfect markets, we expect a PD of zero. For this reason, PD can be taken as an indicator for the efficiency of online markets or e-commerce. Online price comparison sites allow consumers to compare prices easily which should increase price competition and, ultimately, lead to lower PD. We find that PD did not disappear in an online market, but increased substantially over time. We analyze determinants of PD using data from the dominant price comparison site in Austria.====Direct menu costs, such as writing new price tags, may not play a substantial role in e-commerce, as firms can change prices with a few mouse clicks, and technology ensures the synchronization of prices on all platforms on which the firm operates. Some components of price adjustment costs (PAC), such as the managerial costs of deciding if, and by how much, to change prices, could be an important reason why firms ask for different prices. PAC arises from the costs of gathering information on competitors’ prices and consumers’ reactions to price changes, and the costs of deciding on the extent and timing of price changes.==== For instance, Kauffman and Lee (2010) conclude that PAC are one reason for price dispersion (PD), even in the presence of the most advanced information technology. Gorodnichenko et al. (2018) show that even in e-commerce markets, PD does not vanish over time and it is important to identify the reasons for PD in online markets. We show that PAC are, among others, one reason for diverse prices in e-commerce.====We analyze the effect of firms’ PAC on PD using panel data from the dominant price comparison site in Austria, geizhals.at (“miser” or “skinflint”), and cover the universe of Austrian firms with an online shop. The price comparison site lists firms’ offers and the price, the product descriptions, availability, shipping costs, pay formalities, and so on. In that way, the search engine substantially reduces the search costs for consumers.==== We analyze 241,606 products listed between July 1, 2006 and December 31, 2012.====We do not observe PAC directly. We proxy PAC by the firms’ price setting behavior using two different sets of indicators. First, following Nakamura and Steinsson (2008) or Alvarez et al. (2011), we use the number and the size of price changes. We expect that firms that change their prices less often will have a greater PAC than firms that change their prices more often. In addition, firms that change their prices less often might in consequence change their prices by higher amounts than firms that change their prices more often. Second, following Fisher and Konieczny (2000), we consider the synchronicity of price changes for multi-product firms. Firms that change the prices of their goods at the same time (or by the same amount) might have greater PAC than firms that change the prices of their goods more discerningly.====E-commerce is an appropriate field to study PD and PAC. Compared to offline markets, we expect PD in online markets to be small as e-commerce offers virtual markets for homogeneous products with many sellers, where Bertrand competition should drive PD to zero (e.g., Ellison and Ellison, 2009). Price comparison websites keep search costs low and should intensify competition, resulting in small price differences. The use of IT to observe and change prices for products that are offered online keep the information cost and physical costs of changing prices low. Since the penetration of IT in this market is high, one might expect PAC to be small.====However, our results indicate that the greater PAC of the firms in an online market, the greater is the PD. Our results are based on a causal analysis in which we control for firms’ endogenous selection into markets according to the market’s PD. We use an instrumental variables approach in which we use the firm’s listing decision for predecessor goods in the same product category as an instrument for its current price setting behavior.====We obtain robust results that suggest that there is indeed a causal link from firms’ heterogeneous price adjustment costs to PD. The managerial costs of price adjustments (i.e., the costs of information gathering, decision making, and communication) are one reason for PD, even in online markets where firms have very low physical PAC. We find evidence for this causal link even if we control for other possible causes for PD, such as consumers’ search costs, obfuscation or vertical constraints. Our results show that prices are less flexible than the theoretical expectation of Jevons (1871)’ “Law of one Price” predicts. Amongst other explanations, PAC create a substantial price rigidity, even in such competitive markets as e-commerce, where we would not expect PAC—and other reasons—to play an important role in the determination of PD. However, our results demonstrate that over time, the influence of PAC on PD weakened. Technological advances in price setting, such as intelligent price setting algorithms, could contribute to more efficient markets in the future.====Gorodnichenko et al. (2018) also study determinants of price dispersion using data from a price comparison website with different product categories. They analyze aspects of price changes such as size, synchronicity, frequency of sales, and the impact of product replacement on price changes. They show that price stickiness, but also search costs and price discrimination, contribute to observed price dispersion in online markets. In contrast to their work, our empirical approach which uses panel data and an instrumental variables approach establishes the causal effect of PAC on PD more clearly. Moreover, our data provide us with a rich set of variables to control for market characteristics that could cause PD.====We contribute to the literature in several ways. First, we provide and test several proxies for PAC in e-commerce (including evidence on synchronized price changes) and find empirical evidence that higher PAC lead to more PD. In the absence of directly observed PAC, we find that proxies that combine duration between price changes and the (relative) extent of price changes, or of changing many products simultaneously by similar relative price changes, perform relatively better than one-dimensional measures.====Second, we show that firms select into markets based on the markets’ PD. For this reason, we suggest an instrumental variable strategy to solve for the endogeneity bias arising from this selection. With this approach, we show that firms’ PAC have a causal effect on markets’ PD.====Third, we use a comprehensive set of control variables that control for most of the alternative theoretical explanations for PD. Our estimation results indicate that, among others, obfuscation, vertical constraints, search costs, product availability, service differentiation, and degree of competition also contribute to PD in online markets.",The impact of price adjustment costs on price dispersion in e-commerce,https://www.sciencedirect.com/science/article/pii/S0167718721000369,25 May 2021,2021,Research Article,112.0
"Gil Ricard,Kim Myongjin","Smith School of Business of Queen's University,Economics Department at the University of Oklahoma","Received 15 July 2019, Revised 8 April 2021, Accepted 21 April 2021, Available online 2 May 2021, Version of Record 15 May 2021.",https://doi.org/10.1016/j.ijindorg.2021.102742,Cited by (6),"This paper studies the impact of competition on quality provision in the US airline ==== exploiting a novel source of exogenous variation in competition. While mergers among market incumbents may stifle competition, a ==== may increase the probability of entry if the merging airlines were not operating prior to ==== in the market but each of them had presence at different route endpoints. We find non-merging incumbent airlines increase their flight frequency upon entry threat and accommodate entry of the newly merged airline by lowering flight frequency upon entry. While non-merging incumbents reduced arrival delays only upon entry of the newly merged airline, we find that incumbents decrease their cancelation rates and departure delays both upon merger announcement and entry of the newly merged airline. Our evidence suggests an increase in competition may increase consumer surplus, because non-merging incumbents increase quality and convenience, while keeping their prices unchanged.","Although the economics and management literatures have documented the effect of competition on price for years, non-price competition remains relatively understudied despite its prominent role in firms’ competitive strategy and antitrust policy considerations.==== Because non-price competition can take different shapes and affect consumers in different ways, understanding its impact on other firms and ultimately consumers’ well-being is particularly important. Firms may compete on location or hours in service, which are observable and unambiguous in consumer preferences. Alternatively, firms may compete on quality, which may affect consumers differently through vertical and horizontal differentiation.====Because quality provision is costly and it has an uncertain impact on consumer demand, firms may react to increases in competition by either increasing or decreasing quality, depending on how sensitive consumers are to quality relative to prices. On the one hand, Bar-Isaac (2005) shows how competition may decrease quality.==== For example, Prince and Simon (2015) find that more competition between LCC airlines reduce on-time performance. Becker and Milbourn (2011) find an increase in competition among credit-rating agencies may indeed lower quality and reliability because reputational incentives are weakened, and Bennett et al. (2013) show emission-test facilities are more lenient when they face more competition and fear losing customers that failed their test. On the other hand, Horner (2002) shows how competition may increase quality through reputation-building behavior. Mazzeo (2003) shows that flights in more competitive routes are less likely to be delayed, and Seamans (2012) find that private incumbent cable providers respond to potential entry from public firms by providing faster upgrades. Ultimately, whether competition increases (or decreases) quality is an empirical question that depends on the context and industry-specific strategic incentives.====Moreover, understanding the impact of competition on all margins and strategic decisions of the firm is important for various reasons. Whereas an increase in competition may likely decrease prices and therefore unambiguously increase welfare and decrease profits, the impact on quality and other strategic variables offer ambiguous overall effects on firm profits, consumer surplus, and, most importantly, total welfare.==== Therefore, policy makers and government agencies attempting to regulate entry and competition in any industry should understand the consequences of their policies on both qualitative and quantitative dimensions, and not only on those that are easily observable and quantifiable.====Even though defining quality is always subject to debate and discussion, a few industries provide consensus on how to quantify quality across products and firms. For example, the letter grading system in the hospitality industry allows consumers to distinguish more hygienic restaurants from less so (Jin and Leslie, 2003). In the healthcare industry, patient recovery time is viewed as a good measure of quality (Cutler et al., 2014). Similarly, the airline industry is an optimal setting to investigate the impact of competition on quality provision. This industry presents many dimensions of quality (leg-room, food, on-flight media), but we focus here on flight frequency and delays as standard measures of quality used in the airline industry. On the one hand, consumers value flight frequency because it allows them to be more flexible regarding their travel schedule (Forbes and Lederman, 2013; Berry and Jia, 2010). On the other hand, consumers also value reliability and therefore are likely to discount airlines with frequent delays and cancelations (Forbes, 2008; Prince and Simon, 2015 and 2017; Gonzalez et al., 2017). We contribute to this literature by simultaneously investigating the impact of competition in the US airline industry on quality through both travel flexibility and reliability with a novel source of exogenous variation in competition. We measure the former through flight frequency, and the latter through cancelations, arrival, and departure delays.====For this purpose, we exploit a plausible and novel source of exogenous variation in competition in the US airline industry. As documented in the literature (Ito and Lee, 2003; Richard, 2003), over the last 30 years, the US airline industry has seen dramatic changes to its structure due to a significant degree of consolidation through major airlines' mergers (most recently, Delta-Northwest, United-Continental, and American Airlines-US Airways). The existing literature focuses on the fact that mergers decrease the degree of competition in markets when merging firms are both incumbents, namely, because the number of competitors decreases and market power concentration increases (Kim and Singhal, 1993; Prince and Simon, 2017; Gonzalez et al., 2017; Chen and Gayle, 2019). Our paper highlights a distinct feature of mergers in the airline industry whereby mergers may increase the degree of competition through entry in markets that the merging airlines did not operate prior to merger. Following the spirit of Goolsbee and Syverson (2008) and defining markets as non-directional routes, we use the merger between two airlines that did not fly a route, but each had a presence at a different endpoint of the route as plausible exogenous variation in entry threat and entry probability into a market post-merger. We aim to identify changes in behavior of the non-merging incumbent airlines before and after merger announcement, and before and after the newly merged airline enters the route. Take as an illustrative example the merger between US Airways (US) and America West Airlines (HP). Prior to the merger, US Airways operated in Philadelphia airport (PHL) but not in Knoxville airport (TYS), while America West Airlines operated in TYS but not in PHL. Meanwhile, Delta was a monopoly on the PHL-TYS route. After the announcement of the US-HP merger, the probability of entry by the newly merged US Airways increased in the PHL-TYS route, and it became a threat to the only incumbent carrier, Delta, in the route (Goolsbee and Syverson, 2008). According to our empirical strategy, in this particular market we only evaluate the behavior of the incumbent, Delta. This empirical strategy is our main differentiating factor with other papers in the literature examining the effect of competition on market outcomes.====Our data combine DB1B market and ticket data (flight-level and ticket-level data), T100 flight-characteristics data (seats, number of flights, average number of flights, distance measure, etc.), and OTP (on-time-performance data including cancelation, departure delay, arrival delay) between 1993 and 2013. In the end, our final data set provides evidence of behavior for seven major airlines (American, Continental, Delta, Northwest, TWA, United and US Air) in 9,681 markets during 84 quarters spread along 21 years for a total of 417,168 airline/market/quarter/year data points. Our final sample of 688 routes allow us to explore the impact of four mergers between major carriers (American-TWA, US Air-America West, Delta-Northwest, and United-Continental) with entry threats by the merged airlines. Note that we focus on these seven major airlines because their networks were already developed; therefore, changes in the number of flights and OTP are most likely to be the result of a strategic response to changes in their competitive environment, instead of resulting from strategic network expansion.====The findings in this paper contribute to two distinct streams of literature in economics and industrial organization. On the one hand, we contribute to the literature examining the impact of entry threat and incumbent behavior. We are able to investigate whether the actions of incumbents differ during entry threat period and entry, clearly differentiating competitive reaction to entry versus entry deterrence behavior. While most of this literature has focused on price and capacity, we investigate entry-deterring behavior using quality provision. On the other hand, we contribute to the literature examining the impact of entry and competition on quality provision. To the best of our knowledge, ours is the first paper examining the non-merging incumbents’ response to a “potential entry threat” and entry by merging airlines in a route where the merging airlines were not previously operating.====Our findings are as follows. First, we show that incumbent non-merging airlines increase their number of flights in routes not flown by merging airlines before the merger, and they do so before and after the merger announcement. This result contrasts the findings in Goolsbee and Syverson (2008) when they examine changes in flight frequency due to Southwest entry. Second, we find that incumbent airlines decrease flight frequency upon entry of the newly merged airline. This finding is consistent with the incumbents engaging in entry-deterrence strategies prior to entry. If entry occurs, incumbents decrease their number of flights to accommodate entry of the newly merged airlines. Our data shows that entry does not occur in 158 cases out of 688 routes with potential entry threat. This sets an upper bound of 23% of occasions in which entry deterrence may have been successful. Third, we also find that incumbent non-merging airlines reduced their arrival delays in response to the actual entry of merging airlines but not in response of an entry threat. Fourth and last, we show that incumbents reduced their departure delays and their number of cancelations upon both entry threat and entry.====Parallelly, we show that there were no price changes by incumbents before and after merger announcement and entry of the newly merged airlines. This suggests that increases in flight frequency and improvements in OTP are likely to increase directly consumer surplus. Because we lack information on the cost of increasing quality provision by incumbents, we are unable to make conjectures regarding changes in total welfare due to potential entry threat and threat of newly merged airlines.====The paper is structured as follows. We discuss the relevant literature and our contribution in further detail in section 2. Section 3 describes our empirical methodology and our data. In section 4, we present our results. Section 5 investigates result heterogeneity and robustness checks. We conclude in section 6.",Does competition increase quality? Evidence from the US airline industry,https://www.sciencedirect.com/science/article/pii/S0167718721000357,2 May 2021,2021,Research Article,113.0
"Chen Jiafeng,Kominers Scott Duke","Harvard Business School and Department of Economics, Harvard University, United States","Received 14 December 2020, Revised 5 April 2021, Accepted 6 April 2021, Available online 27 April 2021, Version of Record 26 November 2021.",https://doi.org/10.1016/j.ijindorg.2021.102737,Cited by (2),"We investigate a market thickness–market power tradeoff in an auction setting with endogenous entry. We find that charging admission fees can sometimes dominate the benefit of recruiting additional bidders, even though the fees themselves implicitly reduce competition at the auction stage. We also highlight that admission fees and reserve prices are different instruments in a setting with uncertainty over entry costs, and that optimal mechanisms in such settings may be more complex than simply setting a reserve price. Our results provide a counterpoint to the broad intuition of Bulow and Klemperer (1996) that market thickness often takes precedence over market power in auction design.","Many auctions in the real world impose sizable admission fees: OneCause, an online fundraising platform for non-profit organizations, advises sellers to charge for admission to their auctions; in 2015, these fees totalled over $34.7 million in ticketing proceeds, averaging $55,000 per client.==== Meanwhile, Barrett-Jackson, a car auction company, charges a $250–$400 registration fee, which certainly exceeds the cost of administrative logistics.==== Sotheby’s, an art auction house, occasionally hosts sales that require paid tickets.==== Potential bidders in the aforementioned auctions must pay upfront—they tend not to know the exact characteristics of the objects being sold until after paying the admission fees.====Admission fees clearly raise revenue, and they are sometimes the optimal revenue instrument—allowing the auctioneer to extract the full social surplus (Engelbrecht-Wiggans, 1993). However, admission fees by nature restrict entry. A common intuition in auction market design suggests that auctioneers want to encourage as much participation as possible, in order to capture the gains from bidder competition. Moreover, the benefit of increasing participation is known to outweigh the benefit of exercising market power through optimal reserve prices (Bulow and Klemperer, 1996). Thus, in auction design we often have a strong intuition that the preference for ====—increasing the number of auction participants—should take precedence over any preference for ====—designing auction mechanisms specifically favorable to the seller. Using an admission fee as a revenue instrument seems on its face hard to square with that intuition, as admission fees exercise market power precisely by ==== market thickness.====In this paper, we clarify the role of market thickness by observing that if entry is endogenous, then market power may be preferable to market thickness, even when entry is costly. The main reason is that in the endogenous entry framework, the scope of the seller’s market power expands—and, consequently, admission fees can be much more effective at raising revenue than reserve prices. Indeed, setting admission fees enables auctioneers to extract revenue ==== potential bidders learn their full valuations and acquire private information that affects their willingness to pay; in other words, the admission fee allows the seller to contract with bidders ex ante (McAfee and McMillan, 1987a). When the valuation distribution has a long upper tail, fees are then able to extract far more revenue than instruments like reserve prices that screen participation once bidders’ values are known.====We work with an independent, symmetric private values setting with endogenous entry and uncertainty over bidders’ private entry costs. We find that auctioneers may prefer a second-price auction with few potential bidders ==== to one with more potential bidders and no admission fee. We enumerate a few extreme cases, in which the optimal admission fees are almost prohibitive. We also provide a limited characterization of optimal mechanisms in the endogenous entry setting, extending the McAfee and McMillan (1987b) framework to a stochastic entry cost model.====Our contribution is threefold. First, we investigate the market thickness–market power tradeoff in an endogenous entry setting—a thought experiment that closely resembles that of Bulow and Klemperer (1996); we show that accounting for entry can tip the balance in favor of market power over market thickness. Second, we observe that in some extreme cases of the model, the optimal admission fees may be extremely high—indeed, so high as to reduce the auction to a private sale. Third, we develop a framework in which the seller has uncertainty over the bidders’ entry costs, generalizing previous work by McAfee and McMillan (1987b), Engelbrecht-Wiggans (1993), and Moreno and Wooders (2011).====We view the third point as a crucial complement to the first and second. The auctions with entry literature, as summarized in Section 2, has simple examples in which market power dominates market thickness but does not explicitly compare to Bulow and Klemperer (1996), as we do in this paper. For example, assuming no cost uncertainty and the ability to set bidder-specific admission fees, Engelbrecht-Wiggans (1993) shows that the seller can extract the full social surplus, and it is of course possible that full social surplus for a smaller set of bidders is more than expected revenue for a larger one. However, assuming no uncertainty tilts the scale far in favor of market power: with certainty over the bidder costs—even without the ability to set bidder-specific fees—the optimal admission fee could be extremely high, as we show in Section 6. Moreover, assuming no uncertainty also masks the full complexity of the optimal auction design problem that we explore in Section 7, where uncertainty over bidders’ entry costs makes reserve prices and admission fees distinct and complementary instruments for increasing revenue.====The remainder of this paper is organized as follows. Section 2 summarizes the related literature. Section 3 presents a simple example that illustrates our main results. Section 4 introduces our model and equilibrium concept. Section 5 presents our main results, showing sufficient conditions under which auctioneers prefer smaller auctions with admission fees to larger auctions with open entry. Section 6 shows that if bidders’ value-discovery costs are known to the seller—a special case of the main model—then there may be pathological circumstances in which the optimal admission fee is very high. Section 7 briefly discusses optimal mechanisms under endogenous entry. Section 8 concludes the paper.",Auctioneers sometimes prefer entry fees to extra bidders,https://www.sciencedirect.com/science/article/pii/S0167718721000308,27 April 2021,2021,Research Article,114.0
"Carrillo Juan D.,Tan Guofu","Department of Economics, University of Southern California, Los Angeles, CA 90089, USA","Received 25 January 2015, Revised 12 February 2021, Accepted 15 April 2021, Available online 24 April 2021, Version of Record 14 May 2021.",https://doi.org/10.1016/j.ijindorg.2021.102741,Cited by (6),"We characterize the pricing structure in a model of platform competition in which two firms offer horizontally differentiated platforms and two sets of complementors offer products that are exclusive to each platform, respectively. We highlight the presence of indirect network effects: platforms and complementors benefit from the quality and number of firms in their group and suffer from the quality and number of firms in the rival’s group through their effects on prices and market share. We then determine the incentives of platforms to subsidize the independent complementors in an equilibrium. We further analyze the incentives of each platform to form a strategic alliance with complementors through contractual exclusivity or technological compatibility, or to integrate with the complementors. Finally, we discuss the welfare consequences of these strategies.","In the early 2000s, Sony and Toshiba released a new generation of technologically differentiated, non-compatible DVD formats: Blu-ray and HD-DVD. This contest was widely viewed as an updated version of the VCR battle delivered between Betamax and VHS in the 1980s. As in the previous war, the market eventually tipped although this time with a victory for Sony. According to some industry experts, a major reason for Blu-ray’s success stemmed from the ability of the Sony group to secure the exclusive participation of a large fraction of complementary firms (complementors) such as movie studios (Sony, 20th Century Fox, Walt Disney, and Warner Brothers) and major retail distributors including Wal-Mart Stores. Just as in the VCR fight, firm alliances and network effects were at least as decisive as consumer preferences and technological differentiation between the respective groups. Similar battles have been renewed more recently between streaming platforms both in music and television. The videogame industry provides another interesting example of the importance of complementors in platform competition. When Microsoft, Sony or Nintendo launch a new generation of their videogame consoles, they typically provide also new games designed exclusively for their console. These games exploit the comparative advantage of their platform (portability, visual graphics, exclusive characters, etc.).====These multi-billion dollar Information Technology industries share important similarities with each other and also with the computer and smartphone industries. First, there is competition between horizontally and vertically differentiated, possibly non-compatible, platforms: high definition DVD players (Blu-ray and HD-DVD), movie streaming sites (Netflix, Hulu, Amazon), music streaming sites (Spotify, Pandora, Apple music), videogame consoles (XBox, Playstation and Switch), computer operating systems (Windows, macOS and Linux), and smartphone operating systems (Android and iOS). Second, the platforms themselves provide only a limited utility to consumers. In fact, a platform is mostly a means to enjoy some complementary products (movies, videogames, software, apps), making the availability of complementors an essential component in the consumers’ purchase decision. As an immediate implication, platform providers are likely to produce some of these complementary products as well. More importantly, platforms will fiercely compete to attract the most important and successful independent complementors (movie distributors, game developers, software programmers) present in the market. Third, the profit of each platform and complementor depends on the prices of all other market players via their effects on the relative attractiveness of each platform.====In this paper, we propose a theoretical model of platform competition that captures the main ingredients of the markets described above. The basic elements of our model are the following. There are three types of players: (i) two platforms that offer vertically and horizontally differentiated products; (ii) an oligopolistic market of complementors that produces goods for either platform; and (iii) a continuum of consumers with different preferences over platforms and complementary products. Consumers enjoy both the platform and the complementary goods associated with the platform. However, platforms are essential in the sense that complementary goods can only be enjoyed through them. Also, there is a multi-party pricing structure: each platform charges positive or negative per-unit fees (royalties or subsidies) to the complementors in its group, and both platforms and all the complementors set prices simultaneously and non-cooperatively to consumers. Finally, platforms may or may not own some complementors.====Our setting combines some elements of previous analysis from two strands of the industrial organization literature. First, as in the literature on hardware-software complementarity and network externalities, players in our setting enjoy indirect network benefits: A platform becomes more attractive to consumers as the number of its complementors increases (see Economides, Salop, 1992, Church, Gandal, 1993, Church, Gandal, 2005, Economides, Katsamakas, 2006, Farrell, Klemperer, Armstrong, Porter, 2008 and the references therein). In our model, however, platform competition results in a richer structure of interactions: the number of complementors in each platform affects pricing and therefore profits of both platforms and all complementors, and it also affects the equilibrium utility of consumers. We also consider the direct interactions between the platform and its complementors through royalty payments, contractual exclusivity or technological compatibility. Second, as in the fast developing literature on two-sided markets, the value of a platform for one side of the market increases with the number of players in the other side of the market that adhere to it (for instance, see Rochet, Tirole, 2002, Rochet, Tirole, 2003, Rochet, Tirole, 2006, Rochet, Tirole, 2011, Caillaud, Jullien, 2003, Armstrong, 2006, Hagiu, 2006, Armstrong, Wright, 2007, Choi, 2010, Weyl, 2010, Jullien, 2011, White, Weyl, Hagiu, Wright, 2015, Hagiu, Jullien, 2014, Jullien, Pavan, 2019, Tan, Zhou, 2021) for theoretical analyses, and Rysman (2004), Lee (2013), Chao and Derdenger (2013), Derdenger (2014) for empirical studies). Because of this positive externality, optimal pricing by platforms involves cross-subsidization. Our model, however, departs from this literature in some important respects. On the one hand, we introduce oligopolistic competition in one side of the market and direct payments between players at both ends. This feature changes the nature of competition and the determination of the price equilibrium. On the other hand, to focus our analysis, we assume that the fraction of players that adhere to each platform in the complementors’ side is fixed and exogenously given (e.g., due to technological considerations). We argue that our setting captures better the market structure of the industries described above and is perhaps less compelling for analyzing other examples of two-sided markets (e.g., internet purchases, dating markets and the market for credit cards). More importantly, our model allows us to study new issues such as the optimal three-way pricing (and its comparative statics), the incentives of platforms to form strategic alliances through technological compatibility or exclusive contracts, and the consequences of these business arrangements for the welfare of consumers.====Our analysis and the main conclusions can be summarized as follows. We first characterize the equilibrium when both platforms and all complementors simultaneously set their prices to consumers. We present the conditions for an interior equilibrium (where platforms share the market) and corner equilibria (where the market is entirely captured by one platform). We show that even in a tipping market, the presence of the second platform affects the price and profits of the firms in the dominant group. We also show that all firms in a group benefit from an increase in the platform quality and the number of complementors that adhere to it. At the same time, they suffer from an increase in the platform quality and number of complementors in the rival group. This is simply because a platform is more desirable to consumers the greater the value (quantity and quality) of the firms in the group relative to the value of the firms in the other group. Higher value here implies the possibility to charge higher prices while keeping consumers’ loyalty.====The next step is to introduce the possibility of transfers between platforms and complementors prior to the pricing game between firms and consumers. Because a platform is an essential component of a group, we assume that platforms choose a royalty or subsidy per-unit of output sold by complementors to consumers. From the viewpoint of the complementors, this extra pricing instrument only modifies the marginal cost of production. From the viewpoint of the platforms, it introduces a new trade-off: Charging royalties increases direct revenues; providing subsidies decreases the prices that complementors charge to consumers, thereby making the group more attractive. When the two groups have the same number of complementors, we provide conditions on the demand for complementary products under which subsidies arise in equilibrium. In particular, we find that when the demand is weakly cost-amplifying (i.e., the pass-through function is no less than 1) both platforms provide subsidies to their complementors. The intuition is as follows. Despite that a royalty generates positive revenues per-unit, it leads to excessively high prices for complementary products, thereby reducing too much the demand for these products. On the balance, both platforms have incentives to subsidize their complementors. When the demand is cost-absorbing (i.e., the pass-through function is strictly less than 1), numerical calculations suggest that subsidy is also likely to occur in equilibrium.====We then analyze what happens when a platform owns some or all the complementors in the group. As in two-part tariffs, a platform finds it optimal to price its own complementary goods at marginal cost to avoid quantity distortions and gain from the sale of the platform at higher prices. Because this ownership structure increases the equilibrium market share of the platform, it benefits the independent complementors in the group and hurts all the firms (platform and complementors) in the rival group. We also show that consumers are better-off if platforms own all complementors than if complementors are independent because the decrease in complementors’ price offsets the possible increase in platform’s price. Finally, we argue that a platform that can make its own complementary goods available to the customers of the rival platform (Word for Apple computers or Microsoft games for Playstation) faces a trade-off between revenues generated directly by extra sales and loss of market share. We show that it is always optimal to put the complementary goods for sale, but at a price that exceeds the monopoly level.====We further study the incentives of firms to reach business agreements. We show that each platform gains from making its technology compatible with complementors in the other group, as it expands the attractiveness of the platform to consumers. Each platform also gains by signing an exclusive agreement with complementors, as it reduces the attractiveness of the rival platform to consumers. Which outcome prevails depends on the profits of complementors under the different arrangements, the relative bargaining power of firms, and the legal and technological constraints on the feasible set of agreements.====The paper is organized as follows. In Section 2, we introduce the basic model. In Section 3, we derive the consumer demand for the competing platforms and complementors, characterize the price equilibrium, and determine the properties of the equilibrium prices and profits. In Section 4, we analyze the effect of introducing direct transfers between platforms and complementors. In Section 5, we determine the equilibrium prices and the implications when platforms own some of the complementors. In Section 6, we study contractual exclusivity and discuss its welfare implications. In Section 7, we briefly discuss some extensions of the model.",Platform competition with complementary products,https://www.sciencedirect.com/science/article/pii/S0167718721000345,24 April 2021,2021,Research Article,115.0
"Garrett Daniel,Gomes Renato,Maestri Lucas","Toulouse School of Economics, University of Toulouse Capitole, France,Toulouse School of Economics, CNRS, University of Toulouse Capitole, France,FGV EPGE Escola Brasileira de Economia e Finanças, Brazil","Received 22 December 2020, Revised 26 March 2021, Accepted 27 March 2021, Available online 20 April 2021, Version of Record 26 November 2021.",https://doi.org/10.1016/j.ijindorg.2021.102735,Cited by (0),"We study competition by firms that simultaneously post (potentially nonlinear) tariffs to consumers who are privately informed about their tastes. Market power stems from informational frictions, in that consumers are heterogeneously informed about firms’ offers. In the absence of regulation, all firms offer quantity discounts. As a result, relative to Bertrand pricing, ==== benefits disproportionately more consumers whose willingness to pay is high, rather than low. Regulation imposing linear pricing hurts the former but benefits the latter consumers. While consumer surplus increases, firms’ profits decrease, enough to drive down utilitarian welfare. By contrast, improvements in market transparency increase utilitarian welfare, and achieve similar gains on consumer surplus as imposing linear pricing, although with limited distributive impact. On normative grounds, our analysis suggests that banning price discrimination is warranted only if its distributive benefits have a weight on the societal objective.","Nonlinear pricing (or second-degree price discrimination, in the terminology of Pigou, 1920) is the practice whereby firms price each unit purchased of a good differently, leading to quantity discounts or premia. This practice is widespread in many industries, such as electricity, gas, telecommunications, and advertising, and it is also common across a range of consumer goods. Its distributive consequences, that is, its effects across different types of consumers, have long intrigued economists. As Dupuit (1849) wrote:====Although formulated in terms of the quality/comfort of train rides, the reasoning of Dupuit equally applies to settings where firms offer quantity discounts, restricting the purchase of the poor (low willingness to pay), who face high marginal prices, but being magnanimous with the rich (high willingness to pay), who are given discounts for bulky purchases.====More recently, the negative distributive effects of price discrimination came to the fore of the regulatory debate. Concerns for equity motivated, for instance, the regulator to ban “declining-block pricing” (i.e., quantity discounts) for electricity in California (see Borenstein, 2012).==== Similar considerations have led to the imposition of social tariffs for gas and electricity in several European countries, requiring the utility companies to give discounts to the poor (those who have low consumption), effectively weakening (or entirely undoing) the quantity discounts originally in place.==== At the heart of these policies lies the belief that (unrestricted) nonlinear pricing leverages firms’ market power and magnifies inequality across consumers (which is a salient issue in the debate on the regulation of utilities).==== In other settings, concerns about the undesirable affects of overconsumption can also play a role, as for instance in the Alcohol etc. (Scotland) Act 2010 which seeks to mitigate bulk purchases of alcohol by banning quantity discounts in certain Scottish outlets.====While relevant for policy, little is known about the effects of nonlinear pricing, vis-á-vis tariff regulation, in oligopolistic settings. Part of the difficulty lies in having analytically tractable models of second-degree price discrimination under imperfect competition. This question has been taken up recently by Garrett et al. (2019), for settings where imperfect competition arises due to imperfect observability of offers by consumers (equivalently, due to “limited consideration sets”). In this paper, we build on the equilibrium analysis of our earlier article to understand the welfare implications of nonlinear pricing, contrasting it to the situation where firms are constrained to offer linear tariffs, an important benchmark in light of the regulatory concerns discussed above.====We develop our analysis in the context of the continuum-of-types setting of Garrett et al. (2019), focusing on the situation where types are distributed uniformly on the unit interval. Section 2 describes this model, where firms oligopolistically compete for consumers by simultaneously posting tariffs (which are either unconstrained, or linear in the quantity purchased, depending on the regulatory regime). Consumers’ private types index their preferences, which can be understood in terms of different marginal utility curves. As noted above, firms’ market power stems from informational frictions, in that consumers are heterogeneously (and partially) informed about the offers available in the market.====Section 3 derives the equilibrium outcome under laissez-faire, where no restrictions are imposed on the firms’ price schedules. As consumers face informational frictions, firms in equilibrium randomize over tariff schedules (that is, the equilibrium is in mixed strategies). Equilibrium tariffs are ordered, in that a more generous tariff charges a lower price for any given quantity. In equilibrium, firms are indifferent among all tariffs in the support of the mixed strategy, as more generous tariffs attract a larger demand while generating lower profits per sale.====Because the equilibrium marginal tariff schedules are weakly above cost (with equality only at the largest purchased quantity), consumers purchase inefficiently low quantities of the good (except those with the highest willingness to pay). More generous tariffs also exhibit a lower marginal tariff schedule (which describes the price of each marginal unit of the good). Distortions therefore decrease with the generosity of the tariff, as consumers get closer to their efficient purchasing level. Relatedly, a larger set of consumers is served under more generous tariffs, increasing efficiency.====A crucial feature of the equilibrium is that marginal tariff schedules are decreasing in quantity, that is, firms offer ====. This feature has striking distributive consequences. To analyze these, we construct a benchmark by fixing informational frictions (in particular, allowing consumers with bad luck to be knowledgeable of no firm), while assuming that firms post the Bertrand tariff (selling each quantity of the good at cost). In a plausible scenario,==== consumers with the highest willingness to pay obtain in equilibrium around ==== of their benchmark payoff, while those consumers with willingness to pay in the 20th percentile of the taste distribution obtain around ==== of their benchmark payoff (as they buy few units of the good, facing high marginal tariffs). This finding echoes the comments of Dupuit, in that price discrimination induces firms to be “==== while being (perhaps with some exaggeration) ====In light of the rather unequal effects of price discrimination across consumers, we analyze in Section 4 the equilibrium outcome were the regulator to impose linear pricing, therefore preventing quantity discounts. Similarly to laissez-faire, firms in equilibrium randomize over tariffs (now linear). Low per-unit prices attract more customers but generate lower profits per sale, rendering firms indifferent across all equilibrium tariffs.====The uniformity of marginal prices implies that all consumers (including those with high willingness to pay) purchase an inefficiently low quantity. This allocative distortion is accompanied, however, with greater inclusiveness, in that firms in equilibrium serve a larger range of consumers.====Section 5 compares the outcomes under laissez-faire and linear pricing. Intuitively, the inability to price discriminate pushes firms to compete less intensively for the “rich,” who obtain (probabilistically) lower payoffs. Being unable to generate (as much) profits from the “rich,” firms under linear pricing compete more fiercely for the “poor,” who (probabilistically) benefit from this policy.====In the same plausible scenario alluded to above, consumers with the highest willingness to pay obtain in the linear-pricing equilibrium around ==== of their benchmark payoff, while those consumers with willingness to pay in the 20th percentile of the taste distribution now obtain around ==== of their benchmark payoff (in comparison to only ==== under laissez-faire). Accordingly, linear pricing provides significant gains to the latter consumers, while only modestly hurting those consumers with the highest willingness to pay.====Aggregating over types, consumer surplus slightly increases (by ==== in the aforementioned scenario) due to linear pricing. Firms however suffer when exposed to this regulation. Namely, their profits fall by around ====. This is enough to generate a small reduction in utilitarian welfare (by about ==== in the scenario of Footnote 5).====To place these welfare effects in a broader context, we compare them to variations in informational frictions, which drive firms’ market power in our setting.==== In the same plausible scenario as above, a slight increase in the average size of consumers’ sample, from 1.5 to 1.6, is enough to generate the same gains in aggregate consumer surplus. This comparison suggests that market transparency is more consequential for aggregate consumer surplus than regulation imposing linear pricing.====All in all, improvements in market transparency, although increasing utilitarian welfare and aggregate consumer surplus, have relatively limited distributive effects.==== For instance, in the aforementioned scenario, the average sample size of consumers would have to increase from 1.5 to 1.9 to match the gains from linear pricing enjoyed by consumers the 20th percentile of the taste distribution. Although generating distortions (absent under laissez-faire) in the consumption of buyers with the highest willingness to pay, our analysis indicates that linear pricing is an effective redistributive tool. This accords with the use of social tariffs (which renders the marginal tariffs closer to uniform) in utility regulation, where distributive concerns play an important role in the policy debate.",Oligopoly under incomplete information: On the welfare effects of price discrimination,https://www.sciencedirect.com/science/article/pii/S016771872100028X,20 April 2021,2021,Research Article,116.0
Vickers John,"Department of Economics and All Souls College, University of Oxford, United Kingdom","Received 29 December 2020, Revised 12 April 2021, Accepted 12 April 2021, Available online 20 April 2021, Version of Record 26 November 2021.",https://doi.org/10.1016/j.ijindorg.2021.102739,Cited by (3),"This lecture provides a selective discussion of some issues in the economics of competition for imperfect consumers. Limited consumer awareness – about what deals are on offer, or about the consequences of purchasing decisions – is a central feature of many markets. Consumer imperfection can give rise to market power, but can also co-exist with strong rivalry between firms, and then is more for ‘consumer policy’ than competition policy. Two recent court cases illustrate some of the issues involved. The lecture then reviews a line of theoretical work on competition for imperfect consumers. The analysis highlights some policy tradeoffs, and a key theme is how patterns of consumer awareness matter for patterns of competition among firms.","Much of the theory of industrial organization concerns imperfect competition for perfectly informed customers. In this lecture, however, I want to talk about competition for consumers that are imperfect, for example in their awareness of what deals are on offer, or about the consequences of deals they enter into. Given costs of information acquisition, there is nothing irrational in being imperfect in that sense. Anyway consumer imperfection is a primary fact about how many markets work. It is manifested in the widespread consequence that some consumers get much worse deals than others for essentially the same product in the same market.====This can happen in at least three ways. The first is ==== different firms charging different (uniform) prices for the same product. Second, there is ==== – the same firm charging different prices to different consumers, a kind of intra-firm price dispersion. Third, there is ====, whereby naive or unlucky consumers incur high follow-on charges that the savvy or lucky avoid. In short, and despite the internet, we are a long way from the law of one price in many consumer markets.====Consumer imperfection can give rise to market power, and hence questions for competition policy. For example, weak customer response was a central feature of the findings of the Competition and Markets Authority, 2016a report on UK energy markets. Although the supply side was moderately, rather than highly, concentrated – with six large firms plus challengers – the CMA concluded that “some suppliers have a position of unilateral market power, arising from the extent of customer lack of engagement in the market, and that these suppliers have the ability to exploit such a position, for example through price discrimination”. The disengaged retail customers on poor deals tended to be lower-income households, which appears to be a widespread phenomenon as documented by Byrne and Martin (2021).==== Following the CMA report, price caps were introduced for consumers on pre-payment meters and for those on default tariffs with ‘standard variable’ rates, which disengaged consumers typically paid for their energy supplies. This example directly exhibits the issues of price dispersion and price discrimination mentioned above. And it relates to the issue of add-on pricing if ‘loyal’ customers on default tariffs get substantially worse deals than those who shop around.====Consumer imperfection can co-exist with otherwise effective competition. For example, in markets with add-on prices or charges that arise only in certain contingencies, which some consumers might not have foreseen, a pattern of bargains (for savvy consumers) and ripoffs (for the naive ones) can emerge quite consistently with vigorous rivalry. We are then not within the realm of competition policy, at least as normally understood, but consumer policy. Some especially important examples of these issues come from consumer financial markets, the subject of Campbell’s (2016) Ely lecture to the American Economic Association – “an area where competitive markets may deliver substandard outcomes that can be improved through intervention”.====When I was at the UK’s Office of Fair Trading==== 15-20 years ago, with responsibilities spanning both competition and consumer policy – or rather their union – it was striking how economists were ubiquitous in competition policy but rarely to be seen in consumer policy, despite both dealing with markets, and sometimes the very same market. That contrast was a theme of a lecture published as Vickers (2004), but a lot has happened since – in our economic understanding, in market developments (especially online), and to some extent in policy.==== This is a large topic and a survey of those developments would go far beyond the scope of this paper.==== Instead, the main aim of this paper is to give a selective account of a line of theoretical work with Mark Armstrong, some of it currently in progress, on competition for imperfect consumers. That work mostly addresses the first two issues above – price dispersion and price discrimination – in moderately competitive markets.====But I will begin with the issue of add-on charges, and with law more than economics, by drawing attention to two recent cases in consumer and contract law decided by the UK’s Supreme Court. I do this to suggest that consumer and contract law cases, which are much less familiar to industrial economists than antitrust and merger cases, perhaps deserve more attention from us. They can have considerable economic significance for how markets work, and they may be of interest to economists in their own right.",Competition for imperfect consumers,https://www.sciencedirect.com/science/article/pii/S0167718721000321,20 April 2021,2021,Research Article,117.0
"Fazel-Zarandi Mohammad M.,Horstmann Ignatius,Mathewson Frank","Sloan School of Management, Massachusetts Institute of Technology USA,Department of Economics and Rotman School of Management University of Toronto Canada","Received 9 June 2019, Revised 11 April 2021, Accepted 12 April 2021, Available online 20 April 2021, Version of Record 12 May 2021.",https://doi.org/10.1016/j.ijindorg.2021.102740,Cited by (0),We typically assume that exit of competitors from an ,"In competition circles, a widely held view is that a reduction in the number of oligopoly producers, market demand constant, must increase profitability of those that remain. The logic is straightforward: With fewer competitors, the market power of those that remain is enhanced and this enhanced market power translates to increased profits. This logic is flawless as long as the reduction in the number of producers has no impact on the prices of the inputs they utilize. Such will surely be the case for input markets in which the downstream producers represent a negligible fraction of total input demand. However, not all supply chains have this feature. The US auto industry is an example. In this case, the Big Three producers represent a significant fraction of total demand for auto parts. Changes in the structure of the auto industry will surely have repercussions for the auto parts industry. Indeed, Goolsbee and Krueger (2015) note that, during the US financial crisis and ensuing discussions of a bailout for the US auto industry, there was agreement in the White House that “it was essential to rescue GM to prevent an uncontrolled bankruptcy and the failure of countless suppliers, with potentially systemic effects that could sink the entire auto industry”. One can readily think of examples of other industries in which the downstream is made up of a small number of large producers utilizing specialized inputs manufactured by upstream producers: the aerospace industry, shipbuilding, defense systems, trains and light rail vehicles to name but a few.====In these situations, one can only understand the impact of exit on downstream firm profitability by understanding how this exit impacts prices throughout the supply chain. If exit in the downstream industry drives up input prices, then it is no longer obvious that the remaining firms are more profitable. It is this issue that we explore in this paper. As a first pass at the problem, we present a quite stylized model of a supply chain. In the model, there is a fixed number of competing final goods producers – the downstream firms – manufacturing a homogeneous good. Each final goods producer requires a single (aggregate) input provided by an imperfectly competitive, upstream market. The input is identical across upstream manufacturers and is produced under increasing returns to scale. There is also free entry into this market. Pricing in the supply chain is determined by a sequence of Cournot markets, with the final goods producers acting as price-takers in the input market.====Within this stylized supply-chain setting, we show that a reduction in the number of downstream competitors results in an increase in the price of the input provided by the upstream market. This price increase arises from the combination of scale economies and free-entry in the input market. All else equal, this rise in the input price increases costs and reduces profits of each of the remaining producers. Depending on the supply-chain environment, this profit reduction can more than offset any increased profitability directly induced by the exit. In particular, we show that, when (i) mark-ups in the upstream market are large – there are large scale economies in the upstream market – and (ii) mark-ups over input costs in the downstream market are small, then exit of one or more producers from the downstream market reduces profits of the remaining firms. The reason is that, with significant scale economies in the upstream market, the reduction in input demand because of the exit of a downstream producer results in a large increase in the input price; with a small mark-up, a reduction in competition leads only to a small increase in profits. On net, profits of the remaining firms decline.====The simplicity of our initial supply-chain model has the benefit of allowing us to present our key insights in a straightforward fashion. However, the model’s very simplicity also leaves it unclear just how applicable our results are to more general settings. In subsequent sections of the paper, we explore this question in some detail, utilizing a variety of different supply-chain models. We find that, for sequential pricing models with constant marginal cost, the crucial element is that exit from the downstream market induces exit from the upstream market. If the number of upstream producers is ==== by the reduction in the number of downstream producers, then the remaining downstream firms unambiguously gain from exit. It is only when downstream exit leads to upstream producer exit that input prices rise. We also show in this context that, even in more general demand settings, whether downstream profits decline with exit depends on the size of the downstream mark-up relative to the upstream mark-up: large upstream mark-ups and small downstream mark-ups tend to result in a decline in downstream profits with exit. This result holds both in the case of single and multiproduct producers and whether firms exit entirely from the downstream market or merely reduce the number of products they offer in their product line (rationalize their product line). Finally, we examine the impact of exit when input prices are negotiated rather than set in a sequential pricing fashion. In particular, we provide conditions on the supply-chain environment under which downstream exit reduces profits of the remaining firms when input prices are negotiated in a “Nash-in-Nash” fashion (Collard-Wexler et al. (2019)). We show that downstream exit must enhance the outside option of upstream firms relative to that of the remaining downstream firms if exit is to reduce profitability of the downstream firms. We argue that it is this insight that unifies the negotiation and sequential pricing approaches.====In addition to being informative purely on the issue of whether exit improves the profitability of remaining competitors, we think this analysis can also be valuable for understanding industry dynamics at different stages of the industry life-cycle. Specifically, when relatively new industries are in the shake-out phase (prior to reaching industry maturity), our analysis suggests that exit can have impacts along the supply-chain that result in higher input prices that magnify the extent of the shake-out. Similarly, in the expansion phase of an industry, the induced entry into upstream supply markets can result in lower input prices, thereby magnifying the expected profitability of entry. Our results may also explain an observation by Klepper (1979) that, in certain industries, firms that survive the shake-out phase are more likely to have integrated into their supply chain than those that do not survive. In this case, the integrated firms can avoid the rise in market input prices that reduce profitability of the non-survivors.====This is not the first paper to investigate issues of entry, exit and scale economies in a supply-chain setting and their impacts on competition and prices. Chen (2005) considers how vertical disintegration can lead to benefits, not just for the formerly integrated producer, but also its rivals when scale economies are present at the upstream stage. While our paper investigates a very different issue, much as in Chen, scale economies upstream play a key role in determining the benefits / costs to downstream firms of competitor entry and exit. Chen and Riordan (2007) also study supply-chain competition. They examine how the use of exclusive supply contracts by downstream firms can improve the profitability of both upstream and downstream producers. In this case, if all downstream producers agree to use the same supplier exclusively, they can eliminate all other upstream competitors. The exclusive supplier becomes a common agent for the downstream firms and, with non-linear contracting, can raise profits for all of the downstream firms as well as for itself. Rather than investigating how the elimination of upstream competitors can benefit all, our paper investigates how the elimination of downstream competitors can damage the remaining downstream firms, a very different message. Finally, although not a supply-chain setting, our paper draws on insights from Horstmann and Markusen (1986). They show that, in a final goods market with scale economies and free entry, tariffs eliminate foreign competition but induce entry by domestic competitors in a way that drives all firms up their average cost curves. This inefficient entry leads to higher prices and reduced welfare. In an analogous fashion, the exit of downstream competitors in our paper drives input suppliers up their average cost curves. This leads to higher input prices and (possibly) reduced profits for the remaining downstream firms.====The remainder of the paper is organized as follows. Section 2 provides the model and equilibrium outcomes. Section 3 analyses the impact of downstream firms exiting the market entirely. Section 4 considers various generalizations of the basic supply-chain model and analyses the generality of our key results. A final section contains concluding remarks. Proofs of the principal results appear in a Mathematical Appendix",Can Firms Benefit From Competition?,https://www.sciencedirect.com/science/article/pii/S0167718721000333,20 April 2021,2021,Research Article,118.0
"Bisceglia Michele,Padilla Jorge,Piccolo Salvatore","Toulouse School of Economics, France,University of Bergamo, Italy,Compass Lexecon, USA,CSEF, Italy","Received 25 September 2020, Revised 29 March 2021, Accepted 3 April 2021, Available online 15 April 2021, Version of Record 5 May 2021.",https://doi.org/10.1016/j.ijindorg.2021.102738,Cited by (4),"We study the competitive and welfare effects of wholesale price-parity agreements. These contracts prevent a monopolist, who sells its product to final consumers both directly and indirectly through alternative distribution channels, to charge different input (wholesale) prices to competing intermediaries (e.g., platforms). In a multi-channel and multi-layered ","The existing IO literature has broadly demonstrated that retail price-parity agreements — i.e., vertical contracts that restrict retail price competition for a given product or service across alternative distribution channels — produce anti-competitive effects (see, e.g., Edelman, Wright, 2015, Boik, Corts, 2016, Johansen, Vergé, 2017, among others). Inspired by recent antitrust cases involving the hotel booking industry, these models consider games where competing sellers reach final consumers both through their own direct distribution channels as well as via third party, indirect (intermediated) distribution channels (e.g., platforms).==== They compare unfettered competition with competition under wide and narrow retail price-parity and conclude that these agreements exacerbate double marginalization by softening downstream competition and, ultimately, hurt consumers.==== Essentially, an intermediary setting high commissions (or fees) to sellers for the distribution of their products or services will not lose market share, since sellers cannot offer more favourable prices through alternative distribution channels, including the direct (non intermediated) distribution channel. The platform can charge high fees, knowing that those fees will be spread across all transactions, irrespective of the distribution channel consumers use, and that consumers will not find lower-cost alternatives elsewhere.====The literature, however, has devoted less attention to wholesale (or input) price-parity agreements. These are provisions requiring a seller to distribute the same product across all indirect distribution channels at the same wholesale price and are typically used in multi-layered vertical industries, where specialized retailers connect with sellers via competing intermediaries (platforms). The airline ticket distribution industry is a shining example where these provisions are enforced, and has recently attracted considerable policy and regulatory attention on both sides of the Atlantic (see Section 2).==== Other examples of industries where wholesale price-parity agreements are likely to be used are emerging business-to-business (B2B) platforms, featuring the same multi-layered structure. These platforms are ubiquitous in e-commerce (e.g., Amazon, Alibaba, TradeWheel, DHGate, and ECVV) and are likely, in the future, to attract similar policy and regulatory concerns (see, e.g., Byungjoon, Choudhary, Mukhopadhyay, 2007, Lucking-Reiley, Spulber, 2001, Kalvenes, Basu, 2006, for an account of these industries).====In industries with such a structure, sellers can enter into bilateral agreements with intermediaries to constrain the input prices charged through their distribution channels, while leaving the retail prices offered downstream unconstrained. Hence, sellers will mark up the commissions charged by the intermediaries when setting wholesale prices to the downstream retailers. In turn, intermediaries will negotiate fees above their marginal costs, and downstream retailers will also mark up the sellers’ wholesale prices. Retail prices will thus reflect two mark-ups.====In this paper we explore this new territory and examine the competitive and welfare effects of wholesale price-parity agreements. We build a three-layered agency model in which a monopolistic seller (e.g., airline) distributes its products both directly, through its own distribution channel, and indirectly, through two intermediaries (platforms, such as GDSs). Each intermediary is accessed by a retailer (e.g., travel agent or OTA), which in turn relies on the IT infrastructure provided by the intermediary to buy the seller’s product on behalf of final consumers. Each intermediary negotiates a per-unit commission with the monopolist for each unit of product purchased through its channel. The monopolist charges retailers an input or access price that they must pay for each unit of product purchased.==== Retailers set final prices (paid by final consumers) in competition between themselves and with the monopolist’s direct distribution channel. We assume that distribution channels are horizontally differentiated and that consumers exhibit a preference for variety regarding distribution channels.====Our model shows that the monopolist and the intermediaries do not necessarily have aligned incentives concerning the introduction of a wholesale price-parity agreement. Specifically, while wholesale price-parity always hurts the monopolist, it may benefit intermediaries when competition between the direct and the indirect distribution channels is sufficiently intense. Moreover, input price restrictions may improve consumer surplus, absent efficiencies, even though they still lead to an increase in intermediaries’ commissions.====The key observation is that wholesale price-parity can reduce the monopolist’s incentives to set high access prices when distributing through intermediaries. By mandating identical input prices, wholesale price-parity ==== introduces a common cost shock component downstream, which retailers pass on to consumers. By contrast, without wholesale price-parity, the monopolist may discriminate retailers with different input prices (equivalent to idiosyncratic cost shocks) that are still passed on to consumers, but to a lesser extent than a common cost shock because (under passive beliefs) retailers are more afraid of increasing prices. Hence, under wholesale price-parity the monopolist has a lower incentive to increase input prices knowing that these higher prices will be passed on to a greater extent through the whole supply chain creating excessive marginalization. In this sense, one could interpret wholesale price-parity as a device to reduce multiple marginalization.====On the normative side, we find that the effect of wholesale price-parity agreements on consumer surplus is ambiguous and depends on the degree of competition in the industry. Wholesale price-parity increases consumer welfare when the products provided through different distribution channels are not too differentiated. In this case, parity of wholesale prices in the indirect distribution channels makes consumers purchasing through retailers better off insofar as it mitigates double marginalization. In addition, it also makes consumers in the direct distribution channel better off, since the price in the direct channel is lower with than without wholesale price-parity, because the monopolist has to compete more aggressively to divert business towards that channel when the provision is in place. This result suggests that prohibitions on input price discrimination across distribution channels might be justified in industries featuring a strong degree of product substitutability between direct and indirect distribution channels, as well as a multi-layered structure as that postulated in our model, whereas prohibitions on retail price discrimination are much harder to justify.====By contrast, when products are sufficiently differentiated, wholesale price-parity hurts consumers since the extent of double marginalization does not depend on the degree of competition in the downstream market — i.e., imposing input price restrictions has little commitment value, thereby the larger intermediaries’ commissions charged when parity is in place translate into larger wholesale and retail prices in the indirect channels.====Notably, whenever the parity provision benefits intermediaries, it also benefits consumers, but not the other way around. This suggests that, when wholesale price-parity clauses are imposed in a market which features strong intermediaries (in terms of their bargaining power), consumers may well benefit from these clauses, which calls for great caution regarding bans or restrictions.====Finally, we show that, while the intermediaries may persuade retailers to accept the provision through appropriate side payments, persuading the monopolist is harder — i.e., even if the joint profit of the intermediaries and the retailers is higher with than without the provision, the total industry profit and the joint profit of the monopolist and the intermediaries may well drop when the provision is introduced. Hence, under these circumstances, the only way to protect consumer surplus is to allocate more bargaining power to the intermediaries over the adoption of the parity provision.====These results are rather general and hold under alternative assumptions on the contractual architecture. In the extensions, we show that wholesale price-parity has the same qualitative effects on consumer surplus when access prices are public, when intermediaries offer two-part tariffs and under ad-valorem rather than per-unit commissions. We also show that wholesale price-parity is unambiguously pro-competitive under a wholesale distribution model and anti-competitive when retailers compete by setting quantities.====Summing up, our analysis suggests that, even in the absence of efficiencies, evaluating the competitive and welfare effects of wholesale price-restrictions requires a case-by-case analysis. The model puts forward some new empirical predictions that we hope future research will investigate more in depth.====We organize the remainder of the paper as follows. After reviewing the related literature, in Section 2, we provide some useful background to assess the policy relevance of the airline ticket distribution industry and the coherence of our assumptions with business practice. In Section 3, we set up the model, characterize the equilibrium with and without wholesale price-parity and develop a competitive and welfare analysis. In Section 4, we argue that our conclusions are robust to a number of extensions. Section 5 concludes. Proofs of the main results are in the Appendix. Additional material is contained in the online Appendix.==== The key contribution of our paper to the existing literature is to clarify in a formal model that the economics of retail price-parity and wholesale price-parity agreements can be very different. The extant literature on parity agreements only looks at a simplified version of our game — i.e., two-layer B2C industries. Specifically, Boik and Corts (2016) consider a monopolist facing two competing platforms, which first simultaneously choose whether to impose a price-parity agreement, then set per-unit commissions. After observing these choices, the monopolist sets final prices on both platforms. Within this framework, a price-parity clause is unambiguously anti-competitive since it raises platforms’ commissions and retail prices. Similar results are found by Johnson (2017), who allows for competition in the upstream market. The anti-competitive nature of price-parity provisions is challenged by Johansen and Vergé (2017), who consider endogenous platform participation and the presence of direct sales channels in addition to upstream competition (two ingredients that are also present in our model). In their setting, if upstream competition is fierce enough, consumers benefit from the introduction of a price parity clause provided that sellers can delist from platforms charging excessively high commissions. The mechanism through which parity agreements benefit consumers in our model is different from the argument put forward by Johansen and Vergé (2017), since we consider a monopolistic seller who never finds it optimal to delist from a platform.====Other contributions (Ronayne, Taylor, 2018, Calzada, Manna, Mantovani, 2019, Shen, Wright, 2019, Wang, Wright, 2020) suggest that platforms use these clauses to avoid ==== — i.e., consumers using the platform to learn of products, but then buying through the firms’ direct sales channel if they find a lower price.==== Edelman and Wright (2015), instead, assume that, as a result of costly investments, platforms are able to provide benefits to buyers,==== and they show that a price-parity clause leads to inflated retail prices, excessive adoption of the platforms’ services, over-investment in benefits to buyers, and ultimately a reduction in consumer surplus.====Starting from the consideration that price-parity clauses may indeed be needed to prevent showrooming, and in addition there is no clear evidence that a ban of these provisions produces tangible results,==== Gomes and Mantovani (2020) investigate a natural alternative to restricting price parity: namely, how to optimally cap platforms’ commissions. They find that the optimal cap reflects the Pigouvian precept according to which a platform should not charge fees greater than the externality that its presence generates on other market participants.====In all these models, the pro-competitive effect of price-parity agreements is driven by the presence of efficiencies, which are instead absent in our model. We are not aware of any paper dealing with wholesale price-parity despite the importance of such clauses (see Section 2).====Indeed, motivated by laws banning input price discrimination, such as the Robinson-Patman Act in the US and Article 102(c) TFUE, the welfare effects of banning retail versus input price discrimination have been widely analysed in several contributions (e.g., Katz, 1987, De Graba, 1990, O’Brien, Shaffer, 1994, Yoshida, 2000, Inderst, Shaffer, 2009, Miklós-Thal, Shaffer). However, in contrast to us, these models focus on two-layered industries organized with a wholesale (or resale) business model.====Notably, in such industries, in the presence of an upstream monopolist who has all the bargaining power ==== symmetric downstream retailers, the effects of wholesale price-parity coincide with those of disclosing distribution contracts. In this respect, the previous literature has found that, while under two-part tariffs contract disclosure harms consumers (e.g., Hart, Tirole, 1990, Rey, Tirole, 2007), the opposite holds under linear contracts and price competition (Gaudin, 2019).====Lastly, as for the comparison between the wholesale and the agency model, Foros et al. (2017) show that, even if platforms’ commission rates remain the same across the two business models (for exogenous reasons), the use of price parity clauses may facilitate the adoption of the agency model which, in turn, may involve lower consumer prices. Our analysis shows that, like also in Johnson (2017), the agency model increases consumer surplus and platforms’ profits also when contracts and parity provisions are endogenously chosen within each business model (see Section 4.9).",When prohibiting wholesale price-parity agreements may harm consumers,https://www.sciencedirect.com/science/article/pii/S016771872100031X,15 April 2021,2021,Research Article,119.0
Fabra Natalia,"EnergyEcoLab, Universidad Carlos III de Madrid and CEPR, Spain","Received 31 December 2020, Revised 21 March 2021, Accepted 23 March 2021, Available online 15 April 2021, Version of Record 26 November 2021.",https://doi.org/10.1016/j.ijindorg.2021.102734,Cited by (4),"Addressing climate change requires full ==== of our economies. Whether this objective is achieved at least cost for society hinges on good policy design. In turn, this calls for a thorough understanding of firms’ and consumers’ incentives in the presence of asymmetric information, the determinants of strategic interaction, and the impact of market design and market structure on the intensity of competition. Industrial Economics thus has much to contribute towards a successful Energy Transition, while benefiting from the exciting research opportunities it brings. In this paper, I survey some of the recent developments in this area. My focus is on the power sector, and in particular, on the regulatory and market design challenges triggered by the expansion of intermittent renewables with almost zero marginal costs. I conclude with some questions that merit further research.","Energy is embodied in all goods and services. If such energy is generated through the combustion of fossil fuels it generates carbon emissions, which are the leading cause of climate change. The mounting evidence in this regard (IPCC, 2018) has prompted governments around the globe to put in place policies aimed at achieving the Energy Transition, i.e., pathways to fully decarbonize their economies.==== This process will require heavy investments in low carbon assets and energy efficiency in order to substitute fossil-fuels with renewables while reducing the energy needs. Likewise, new market rules and regulatory arrangements will be needed to make this process happen.====One can analyze the Energy Transition from a myriad of perspectives. In this paper, my focus will be on the Energy Transition in the power sector, and I will approach it from an Industrial Economics perspective. Relative to other approaches, Industrial Economics recognizes the relevance of incentives in the presence of asymmetric information, the determinants of strategic interaction, and the impact of market design and market structure on the intensity of competition (Tirole, 1988, Laffont, Tirole, 1993). “====,” as Wilson (2002) has described them, which explains why Industrial Economics has played a prominent role in the recent literature analyzing the options for decarbonizing the power sector. In this paper I survey some of such contributions.==== Among all the sectors involved in the Energy Transition, the power sector will bear a significant burden of the overall emissions reductions. Given its unique ability to transform renewable energy into electricity, it can contribute to decarbonizing other polluting sectors in which electricity is used as a substitute for fossil fuels (notably, transportation through the deployment of electric vehicles and hydrogen cars; or buildings through the deployment of electric heat pumps). Technology breakthroughs are making this easier. Over the last decade, there have been massive reductions in the costs of wind and solar investments, which have now become competitive ==== the conventional energy sources (coal, gas and nuclear).==== These trends imply that the costs of generating power in a 90% carbon-free market will be 10% lower than in a 55% carbon-free market (Goldman School of Public Policy, 2020).====In several countries, renewables already represent more than one third of total electricity generation, and they are expected to become the main source for power generation by 2030.==== However, the rapid expansion of intermittent renewables with almost zero marginal costs is bringing new challenges for the performance of electricity markets under their current design. As Joskow (2019) puts it: ==== Industrial Economics can greatly contribute to addressing these fundamental questions by identifying the regulatory and market-based solutions that minimize the costs of decarbonizing the power sector, which is a necessary condition for decarbonizing the whole economy.====In this paper I classify the main challenges faced by current and future electricity markets in three blocks. First, the understanding of how competition takes place in renewable-dominated electricity markets is a prerequisite for designing market rules that provide the correct incentives for firms to invest in renewables and to price them efficiently. Investments in renewables are costly, as they involve large capital upfront costs. However, once in place, renewables have almost zero marginal costs of production. For this reason, an increase in renewable generation shifts the aggregate supply function to the right, leading to a price-depressing effect, as illustrated in Fig. 1.====The scale of this effect depends on several factors, including the energy mix and the market structure. Both factors will determine the intensity of competition among renewables and the change in the conventional energy sources’ decisions in response to the expansion of renewables (Acemoglu, Kakhbod, Ozdaglar, 2017, Fabra, Llobet, 2019, Bushnell, Novan). In Section 2 of this paper, I survey some of the main contributions that shed light on these issues.====Second, the analyses of competition in wholesale electricity markets with large shares of renewable energy lead to another important conclusion: the average market price captured by renewables will fall below their average costs. The reason is that, at times when the supply of renewables exceeds total demand, the short-run electricity market prices will converge towards the (almost zero) marginal costs of renewables.==== To provide long-term signals that support future investments, it will be important to rely on mechanisms, other than short-run electricity markets, to determine payments for renewables without distorting the choice of locations and their efficient use once the investments have taken place. Not surprisingly, auctions for renewable investments are rapidly expanding worldwide (IRENA, 2019). Design features of such auctions - including, among others, the product that is auctioned off (e.g., either energy or capacity), the winning projects’ price exposure, or the set of technologies that are allowed to compete - are key determinants of the auctions’ success. In Section 3, I discuss the implications of such choices through the lens of recent contributions in this area.====And third, because renewable energies are not always available, their expansion has been traditionally associated with the need to build back-up capacity to avoid disruptions in energy supply. However, more efficient solutions to cope with renewables’ intermittency may come from either the demand side (e.g., by using dynamic prices to induce a shift in consumption towards hours with high renewables’ production), or from the supply side (e.g., through storage). Both can help mitigate the need to maintain excess capacity. In particular, demand response mechanisms can induce load shifting from high-price periods (i.e., those with high demand/low renewables) to low-price periods (low demand/high renewables). Storage technologies can also help address renewables’ intermittency by allowing storage during periods with abundant renewables’ generation, and release during periods when renewables are scarce. However, some pending issues merit further research. On the one hand, even though there are strong theoretical arguments in favour of dynamic pricing (Borenstein, 2002, Borenstein, Holland, 2005), the experimental literature has shown that active demand response is not likely in the absence of enabling technologies (Harding and Sexton, 2017), a result that has recently been confirmed empirically (Fabra et al., 2021). Regarding storage, not only do their costs remain high, but market power can also create distortions for the efficient operation and investment in storage facilities (Andrés-Cerezo and Fabra, 2020). On the other hand, both options are very complementary with the renewables’ expansion, as renewables will likely enlarge cost and price volatility which will in turn enhance the social and private benefits of demand response and storage. In Section 4, I review these issues with the aim of identifying policy options to enhance the effectiveness of demand response and storage as complements for the renewables’ expansion.",The energy transition: An industrial economics perspective,https://www.sciencedirect.com/science/article/pii/S0167718721000278,15 April 2021,2021,Research Article,120.0
"Ahundjanov Behzod B.,Noel Michael D.","Bucknell University, United States,Texas Tech University, United States","Received 19 February 2020, Revised 18 December 2020, Accepted 12 March 2021, Available online 30 March 2021, Version of Record 20 April 2021.",https://doi.org/10.1016/j.ijindorg.2021.102733,Cited by (1)," and 2) an environmentally-targeted “carbon levy”. While similar on the cost side, the taxes were very different in name and transparency on the benefit side. Results show that benefit-side transparency can matter – responses were lower and incidence higher for the more transparent carbon levy than with the less transparent excise tax.","Legislators around the world have a long history of attaching names to individual taxes with specific purposes, such as Canada’s “Fair Share Health Care Levy” or China’s “Environmental Protection Tax”. The simple act of naming a tax and transparently conveying its purpose can make the tax more palatable to taxpayers, provided the purpose is deemed worthy. This stands in contrast to general tax increases which, while servicing many of the same types of programs and public services, are a bit of a black box and can be met with more opposition. When George H.W. Bush famously promised “Read my lips, no new taxes” in the 1988 presidential campaign, but later signed a budget bill that included general tax increases, public reaction was swift and negative - irrespective of what social programs the new taxes would ultimately be used for. That broken promise played an important role in Bush’s failed re-election bid four years later.====One type of tax with a specific name and transparent purpose attached to it, and one that has received a great deal of recent attention, is the so-called “carbon tax” or “carbon levy”. Carbon levies, commonly imposed on sales of gasoline and other products derived from fossil fuels, are designed to reduce carbon emissions while simultaneously funding research and development of more environmentally friendly sources of renewable energy. As of the time of writing, the United States is one of the few industrialized countries without a carbon levy, but fifteen U.S. states in the West and Northeast were actively considering one. In January 2019, dozens of prominent economists published a letter in the Wall Street Journal urgently calling for the introduction of a federal carbon levy.==== A small but growing literature has emerged to evaluate the effects of carbon levies on carbon emissions, and in most cases, carbon levies are found to be effective at reducing carbon emissions (e.g. Rivers and Schaufele, 2015). Effects can work through short run substitution away from more expensive fossil fuels and through long run substitution towards more cost-effective alternative energy sources whose development has been funded by the levy.====In this study, we look at a different aspect of carbon levies. We are not interested in estimating the effects of a carbon levy per se, but rather in estimating the effects of ==== a carbon levy and transparently conveying its potential benefits to consumers. We want to know if consumers will respond differently to a tax when that tax is given a specific name and attached to a transparent set of benefits that (many) consumers value, in contrast to a generic tax increase of a similar size, which funds general revenues and does not provide as transparent a set of benefits. In other words, we examine how transparency on the benefits side affects the incidence of a tax.====Our prior is that transparency of benefits should not matter to the taxpayer in terms of market activities since, from a financial perspective, a dollar is worth a dollar. But it is well known that consumers often act in a more inelastic way and pay a premium for products that embody perceived benefits to society during production, e.g. more environmentally friendly, union made, humane, or animal-friendly. To the extent that consumers are more accepting of a carbon levy whose stated purpose is more agreeable to them and whose expected benefits are more transparently communicated, those consumers may respond in a less negative, and more inelastic way.====The idea has been discussed and debated in policy circles, but establishing a link between the transparency of benefits of a tax and the incidence of the tax has been difficult due to an identification problem. Basically, a tax generally either has a meaningful name that conveys a transparent purpose and an expected set of benefits, or it does not. The challenge is to find a comparable benchmark tax, one that is roughly equal in size and equal in transparency on the cost side, but different in transparency on the benefit side. In practice, the researcher may need to compare the named tax with a tax on a different product, or in a different state, or in a distant time period, or on products purchased by different kinds of consumers.====Fortunately, and largely unique to the literature, we solve this identification problem by exploiting a fortuitous natural experiment in the retail gasoline industry in the province of Alberta, Canada. The provincial government there implemented two new taxes on the sale of gasoline, one in April 2015 and another in January 2017, twenty-one months apart, of roughly equal sizes and equal transparency on the cost side, but with very different names and very different degrees of transparency on the benefits side.====The first, imposed in April 2015, was a simple increase in the usual excise tax on gasoline of 4.0 cents per liter (or approximately 11.4 U.S. cents per gallon). The increase was significant (on gasoline prices of about a dollar per liter) and was used to supplement general provincial revenues which in turn contributed to a variety of general government services, such as administration, health care, education, infrastructure, and regulation. While the tax would ultimately fund important services, there was no special name or specific purpose attached to the tax, and the link between it and the direct benefits that it was expected to provide was less transparent to consumers.====In contrast, the second tax, imposed in January 2017, came with a special name, a transparent purpose, and a clearly-stated set of expected benefits. The so-called “carbon levy” was implemented as a tax to help the environment, reduce greenhouse gas emissions, invest in renewable energy projects and energy efficiency programs, and promote other environment conservation projects (Government of Alberta, 2018). The tax was similar to the excise tax increase on the cost side, at 4.49 cents per liter (or approximately 12.8 U.S. cents per gallon), and like the excise tax increase, was transparent in terms of its existence. But unlike the excise tax increase, the carbon levy offered transparency on the benefits side as well, with readily available information on which environmental programs would be supported and in what dollar amounts. While the carbon levy was by no means only positively received, as it is still a tax, the added benefit-side transparency enabled a vibrant public discussion about the value of the benefits it would provide.====The two Alberta taxes – similar in cost-side transparency but different in benefit-side transparency – present a unique opportunity to examine the relationship between benefit-side transparency and consumer incidence, holding all else equal. Using panel data on wholesale and retail gasoline prices, along with tax information, for both regular and premium grade gasoline for Western Canadian cities over a five year period, we estimate the consumer incidence of the two taxes, or as it is commonly called in the gasoline literature, the degree of tax passthrough. Cities in the province of Alberta serve the treatment group, first receiving the excise tax treatment in April 2015 and then the carbon levy treatment in January 2017. As it turns out, these were also the only two tax increases on gasoline anywhere in Western Canada over our sample period, meaning that other cities in Western Canada can serve as an additional layer of control for what would have happened to gasoline prices but for the two taxes. Other cities in Western Canada are an almost ideal control because they are supplied from the same deposits of crude oil, and are served by the same pipelines and refineries.====There are several important questions. We first examine the extent of passthrough of tax changes into gasoline prices generally and test whether they are “complete” or “incomplete”. Passthrough on the excise tax is complete if retail prices in Alberta cities increase by 4.0 cents per liter after April 1, 2015 (when the excise tax increased from 9 to 13 cents per liter), relative to the control cities. Passthrough on the carbon levy is complete if retail prices in Alberta cities increased an additional 4.49 cents per liter after January 1, 2017 (when the carbon levy was introduced). Passthrough of a tax change is incomplete if the corresponding price change is less than the tax change itself. The question of whether passthrough is complete or incomplete has been a long-standing question in the gasoline literature (e.g. Marion, Muehlegger, 2011, Silvia, Taylor, 2014) and we contribute to that literature here.====Second, we test whether the degree of passthrough, or incidence, of a tax depends on the naming and transparency of benefits of the tax. To the extent that consumers are more accepting of a carbon levy with a transparent set of benefits (viewing the price increase net of social benefit to be smaller), relative to an equivalently-sized excise tax increase without the same benefit-side transparency, we would expect the degree of passthrough to be higher on the carbon levy and the incidence of the carbon levy to fall more heavily on consumers. If not, the degree of passthrough should be relatively more uniform across the two taxes. This is a testable proposition.====This question of whether consumers respond, not just to gasoline price increases generally, but differently to different sources of gasoline price increases, has received little attention in the gasoline literature. Two notable exceptions, and two studies closely related to this one, are Chouinard and Perloff (2004), which examine differential consumer responses to changes in federal versus state taxes, and Li et al. (2014), which examine differential consumer responses to excise taxes versus tax-exclusive cost changes. Both find that cost components in gasoline do matter. Our study contributes to this small but important behavioral literature.====To preview results, we find meaningful differences in passthrough and incidence between the carbon levy on one hand and a general excise tax increase on the other. The degree of passthrough is higher and largely complete for the carbon levy in relatively short order, whereas it is smaller and less than complete for the general excise tax increase. The results suggest that consumers more accepting of a tax with a specific name and a transparent set of benefits will respond less negatively to the tax. The policy implication is straightforward - better connecting the taxes that are paid to the benefits they are expected to provide can make those taxes more palatable to consumers and reduce negative reactions.====One novel feature of our study is that we estimate effects not only for regular grade gasoline but for premium grade gasoline as well. Few gasoline-related studies include premium grade gasoline in their analysis, but in framing their conclusions about “gasoline” generally, implicitly assume that results on regular grade gasoline carry over to higher octane grades as well. Our study shows that this assumption can be premature. We find some interesting differences in relative incidence across the two grades of gasoline, with important distributional implications.====The remainder of this paper proceeds as follows. Section 2 provides background information and review of relevant literature, Section 3 discusses the data and our empirical strategy. Section 4 presents the results, and Section 5 provides some concluding remarks.",What’s in a name? The incidence of gasoline excise taxes versus gasoline carbon levies,https://www.sciencedirect.com/science/article/pii/S0167718721000266,30 March 2021,2021,Research Article,121.0
"Tan Hongru,Wright Julian","School of Economics, Sichuan University, No.24 South Section 1, Yihuan Road, Chengdu 610065, China,Department of Economics, National University of Singapore, 117570, Singapore","Received 15 December 2020, Revised 6 March 2021, Accepted 9 March 2021, Available online 24 March 2021, Version of Record 26 November 2021.",https://doi.org/10.1016/j.ijindorg.2021.102732,Cited by (1),"We consider the reasons why a monopoly multi-sided platform may price differently from a social planner. The existing literature has focused only on the classical market power distortion and a distortion in the spirit of Spence. We show two additional distortions appear in the presence of cross-group network effects, which we call the displacement distortion and the scale distortion. We show conditions under which the displacement distortion exactly offsets the Spence distortion, and provide an example in which the total of these different distortions results in monopoly prices per user that are lower than the social planner’s on both sides. Our results have implications for regulatory policy, which we briefly discuss.","The striking success of large multi-sided platforms like those run by Alibaba, Amazon, Apple, Facebook, Google, Microsoft, and Tencent is no doubt driven in good part by the positive network effects these platforms enjoy. But this has also generated intense regulatory interest, which has mainly focused on the potential abuse of their market power via practices such as restrictive platform access, bundling to leverage market power, exclusive deals, self preferencing, and price parity clauses. Various policymakers and commentators have also called out the high prices charged to the seller-side of some of these multi-sided platforms (e.g. appstores, credit card platforms, Facebook and Google’s ads, and hotel booking platforms).====In this paper we take a step back and ask a more basic question—what are the pricing distortions implied by a monopoly two-sided platform that enjoys network effects. The common understanding is that aside from a classical market-power distortion in which firms restrict output to increase price above cost, there is an additional distortion due to profit maximization, which has been referred to as the Spence distortion (Weyl, 2010, Levin, Tan, Wright, 2018, Veiga, 2018, Katz, 2019). A monopoly platform distorts in the spirit of (Spence, 1975) by internalizing only the externality new users impose on marginal users on the other side, whereas a social planner would internalize the externality imposed on average users on the other side.==== The Spence distortion can provide a new explanation for why prices may sometimes be too high (or too low) on one or other side of a multi-sided platform. Specifically, the argument goes that after adjusting for the classical market-power distortion, a monopoly platform will set its price too high on one side compared to the planner’s socially optimal price on that side when on the other side the interaction benefit of average users is greater than the interaction benefit of marginal users. This is because when setting its price, the monopoly platform does not internalize the effect on the higher interaction benefits of the inframarginal users on the other side.====In an earlier comment on Weyl’s paper (Tan and Wright, 2018), we noted that Weyl’s comparison of the monopolist’s and planner’s price was incorrect since it was made by directly comparing first-order conditions that should have been evaluated at different outcomes. We noted the existence of other distortions that could add to or lessen the Spence distortion when comparing the two prices. In this paper, by providing a comparison of pricing outcomes directly, we characterize these additional distortions.====The first additional distortion accounts for the fact that when there is heterogeneity in users’ interaction benefits, the marginal agents under the profit-maximizing prices do not have the same interaction benefits as under the welfare-maximizing prices. We call this the ====, and it normally works in the opposite direction of the Spence distortion. It captures that the marginal user’s interaction benefit on each side is itself distorted because of monopoly pricing. Thus, if the monopoly platform sets a high price on side ==== the marginal user on side ==== must have a high interaction benefit, which means when pricing on side ==== the externality the monopolist takes into account is distorted upwards compared to the interaction benefit of the marginal side ==== users at the socially optimal price. Other things equal, this will lead the platform to set too low a price on side ====. Thus, when the interaction benefits of average users exceeds that of marginal users, so the Spence distortion is positive, this displacement distortion will be negative. For instance, if at the monopoly price the marginal user on side ==== has an interaction benefit equal to the average users’ interaction benefit based on the social planner’s price, then the displacement effect will exactly offset the Spence distortion on side ====. In general, the net effect of the Spence and displacement distortions can therefore go in either direction.====In the simplest case with heterogeneity only in interaction benefits, which is when the Spence distortion is at its largest, we show the displacement distortion is always in the opposite direction to the Spence distortion on at least one side of the market. Moreover, in the case interaction benefits on each side are distributed according to the generalized Pareto distribution with log-concave demand (which includes the case quasi demands are linear), we show that the displacement distortion always exactly offsets the Spence distortion on both sides. In such a case, monopoly pricing in multi-sided platforms only involves the classical market-power distortion on each side and nothing else.====This result has implications for the claimed excessive merchant fees in the debit and credit card industry. Monopoly debit and credit card payment platforms are typically modelled as facing cardholders and merchants that vary only in their interaction benefits.==== With heterogeneity only in interaction benefits, one might think that Spence distortions are key to understanding price distortions in payment cards. Weyl (2010) suggests this is the case for AmEx, noting (p.1642) “Given its limited ability to price discriminate, AmEx fails to fully internalize the preferences of loyal users, putting too little effort into attracting merchants and charging them a higher price than would be socially optimal.” Our result suggests otherwise. It shows that in such a setting, the Spence distortion will tend to be offset by the displacement distortion. Instead, other explanations may be more relevant. Excessive merchant fees for card payments have been explained in Rochet and Tirole (2002) and Wright (2012) by merchant internalization due to price coherence, by Wright (2004) due to asymmetries in pass-through rates, and in Bedre-Defolie and Calvano (2013) by an asymmetry in price discrimination possibilities due to the fact consumers decide which payment method to use for each transaction whereas merchants make an all or nothing decision whether to accept the card.====An even more extreme result is obtained when we compare a monopoly platform’s prices and Ramsey prices (i.e. prices that maximize total welfare subject to the platform breaking even) in the setting with only heterogeneity in interaction benefits. In this case the Spence distortion does not arise at all, and only the classical markup and the displacement distortion remain. Because cost must be recovered in the Ramsey outcome, there is no scope to take into account the Spence distortion.====When heterogeneity is purely in membership benefits, neither the Spence distortion nor the displacement distortion arises, reflecting that there is no difference between marginal and average interaction benefits. This may suggest that only a classical distortion remains. However, even though interaction benefits are constant, an additional distortion can still arise in the tariff charged per participant, which we call the ====. It reflects that the profit a monopolist can extract from side ==== from interactions with a participant on side ==== depends on the number of participants on side ==== which can be different in the monopolist’s solution outcome compared to the social planner’s solution. This means the incentive a monopoly platform has to increase the tariff on side ==== can be excessive or insufficient (depending upon the relative magnitude of the interaction cost and the constant interaction benefit on side ====). We show that this distortion in tariffs can be as important as (or more important than) the classical market-power distortion.====We also consider the case with heterogeneity in both interactions benefits and membership benefits using Weyl’s Scale-Income model. Based on our decomposition of the distortions between privately and socially optimal tariffs, all four possible distortions can arise—classical, Spence, displacement and scale. Our analysis suggests there is no particular reason why Spence distortions should be the main focus of policymakers in the case of monopoly pricing by a multi-sided platform. Indeed, we provide a numerical example to show that it is possible that the regulator does better reducing prices opposite the side with the smaller Spence distortion and allowing them to increase on the side opposite that with the larger Spence distortion. Thus, we question (Weyl, 2010)’s advice that “... the novel element in two-sided markets is that regulators should focus most on reducing price opposite a side with a large Spence distortion” (p. 1666).====The rest of the paper proceeds as follows. In Section 2 we present the model. In Section 3 we provide a general analysis allowing for both interaction and membership heterogeneity. In Section 4 we then get more specific results by applying this analysis to the case with only one dimension of heterogeneity, which is the standard setting in most models of multi-sided platforms. Section 5 concludes.",Pricing distortions in multi-sided platforms,https://www.sciencedirect.com/science/article/pii/S0167718721000254,24 March 2021,2021,Research Article,122.0
"Bouckaert Jan,Van Moer Geert","Department of Economics, University of Antwerp, Oxera, and ECARES, Université Libre de Bruxelles. Prinsstraat 13, B-2000 Antwerp, Belgium,Department of Finance and Economics, Hanken School of Economics and Helsinki Graduate School of Economics. P.O. Box 479, 00101 Helsinki, Finland","Received 8 October 2019, Revised 9 February 2021, Accepted 1 March 2021, Available online 10 March 2021, Version of Record 21 March 2021.",https://doi.org/10.1016/j.ijindorg.2021.102727,Cited by (0),"This paper investigates joint bidding when firms have incentives to sign subcontracts with each other after competing in the bidding stage. A bidding consortium affects the horizontal subcontracting market and, through backward induction, alters firms’ bids. Our findings challenge the current legal practice that consortia without efficiencies must pass the “no-solo-bidding test”, requiring that its members could not bid stand-alone. Our framework predicts that the formation of a temporary consortium, which has the feature that it dissolves after submitting a losing bid, benefits the procurer. The winning bid is more competitive with a temporary as compared to a structural consortium.","Joint bidding is common in procurement and refers to the practice where two or more firms submit a single bid. Also, winning and losing bidders often form contractor-subcontractor relations after the contract has been awarded in the bidding stage. Both practices can reduce costs by efficiently reallocating production across firms, for example when firms are subject to idiosyncratic cost shocks or capacity constraints. Accordingly, joint bidding and subcontracting are widespread in a diverse set of industries. For example, Hendricks and Porter (1992) report on joint bidding for outer continental shelf oil drilling leases in the U.S., an industry where subcontracting occurs (Hendricks et al., 2003 and Haile et al., 2010). Corwin and Schultz (2005) analyze the practice of loan syndication between banks to reduce the cost of issuing capital. Further, in military aircraft procurement, competing bidders often team up due to heterogeneous specializations (Miller, 2005). Marion (2015) studies subcontracting between competing firms in the California highway construction industry.====Competition authorities and courts rely mainly on the following two criteria to assess joint bidding. First, bidding consortia without efficiencies are only permitted if it is infeasible for their members to participate in the procurement on a stand-alone basis. They must pass what we refer to as the “no-solo-bidding test”. The reasoning is that, when failing the test, the joint bidding arrangement reduces competition by lowering the number of bidders. This argument has already been used in 1975 by the US Congress to prohibit joint bidding arrangements between large oil companies for offshore oil leases (Hendricks and Porter, 1992). In Europe, the no-solo-bidding test has been prominently applied in several recent court cases. For example, joint bidding for patient transportation contracts was regarded by the Norwegian Supreme Court as a restriction of competition by object since parties could have bid solo.==== As a second criterion, when failing the no-solo-bidding test, sufficient offsetting efficiencies are required, for example through organizational integration. In Italy, the competition authority recently accepted efficiency arguments to permit joint bidding by two competing pharmaceutical companies that could have bid solo.====In this paper we highlight that joint bidding arrangements between competitors, although reducing the number of bidders, also reduce the demand for contracting. We develop a model which explicitly accounts for the effect of joint bidding on the terms of trade in the subcontracting market. The findings from the model challenge in several respects the standards against which joint bidding arrangements are currently assessed.====We capture the strategic interaction between firms by modeling a two-stage game, detailed in Section 2. In the first stage, firms submit bids and the contract is awarded to the lowest bidder. Nature then independently draws a zero or high cost for each individual firm. Stage two is the horizontal subcontracting market. There are incentives to form contractor-subcontractor relations when the winner draws a high cost and there is at least one losing firm drawing a zero cost. The distribution of surplus between the contractor and the subcontractor(s) is modelled in reduced form representation. By doing so, our model covers a range of subcontracting market conditions.====There exists a wide variety of bidding consortia in practice. To study their competitive effect, we argue that a key dimension along which consortia differ is whether they are temporary or structural in nature. Temporary consortia jointly bid and, when selected as winner, jointly contract. However, when losing, cooperation between its members breaks down. The decisions to act as subcontractor are then made solo by the original entities. Temporary consortia thus capture cooperation agreements that are ==== and dissolve if another firm submits the lowest bid. In contrast, a consortium is structural when it involves an agreement or joint venture which is longer-lasting and, for example, concerns multiple tenders. A structural consortium thus crucially differs from a temporary consortium because it continues to operate jointly regardless of the outcome of the bidding stage (joint contracting ==== joint subcontracting). Table 1 summarizes their distinction.====Empirical evidence suggests that both types of consortia are relevant, as illustrated by the following examples. In auctions for U.S. offshore oil and gas leases, “firms who bid jointly in one area of a sale will not necessarily do so in other areas or other sales” (Haile et al., 2010, p. 391). In this industry, for example, Exxon bids jointly only 13% of the time, reflecting ==== cooperation, whereas the consortia Kerr/Marathon/Felmont and LaLand/Hess/Cabot bid jointly over 90% of the time (Hendricks and Porter, 1992, p. 507). Another example of long-lasting cooperation is the Norwegian joint venture established by Ski Taxi and Follo Taxi, entrusted with submitting joint bids over the course of multiple years (Sanchez-Graells, 2018). Gugler et al. (2020), in their study of the Austrian construction sector, report that about two thirds of bidding consortia are formed only once and that other bidding consortia bid jointly up to thirty times.====A temporary consortium in our model corresponds to a “non-full-function joint venture” and would in Europe be assessed under TFEU 101 and the Guidelines on horizontal cooperation agreements. A structural consortium, in contrast, would be treated as a full-function joint venture and accordingly fall under the merger regulation. In the US, the standards used to determine whether joint bidding violates the antitrust laws are described in the Antitrust guidelines for collaborations among competitors (Federal Trade Commission, 2000, §3.37). The consortium, however, would be treated as a merger if appropriate when “the collaboration does not terminate within a sufficiently limited period by its own specific and express terms.” (Federal Trade Commission, 2000, §1.3).====Sections 3 and 4 present an analysis of solo bidding and joint bidding through a temporary consortium, respectively. We find that the formation of a temporary consortium between firms failing the no-solo-bidding test changes the terms of trade in the subcontracting market, to the benefit of the procurer. Specifically, joint contracting from losing rivals results in two distinct pro-competitive effects. First, it reduces the cost at which the consortium can contract inputs, thereby allowing it to submit a more competitive bid. Second, the consortium's increased contracting power reduces subcontracting profits for outsiders. The outsiders thus forego a smaller amount of profits when winning, leading them to compete more fiercely as well. Our findings cast doubt on the economic rationale behind the no-solo-bidding test as a basis for assessing joint bidding in procurement industries with subcontracting.====In our model, a temporary consortium reallocates production across its members. This approach captures joint bidding agreements between competitors that do not integrate their operations. In the U.S., these joint bidding arrangements are regarded as anti-competitive and therefore challenged as per se illegal Federal Trade Commission (2000). In a similar spirit, in the EU, a recent court case has judged such an arrangement as anti-competitive.==== Our analysis yields that, even without integration of operations, a temporary consortium alters the terms of trade in the subcontracting market and benefits the procurer.====In Section 5, we consider a structural consortium. As with a temporary consortium, a structural consortium efficiently reallocates production and does not create additional synergies. We thus model the structural consortium with the presumption of no synergies. This presumption follows from current merger policy under which structural consortia are assessed. Moreover, it allows for a clean comparison between the two consortia types.====A structural consortium continues to operate jointly even if it loses in the bidding stage. There are two resulting anti-competitive effects. First, outsiders pay more to contract inputs, which weakens their ability to submit a competitive bid. Second, increased subcontracting power by the consortium makes it more attractive for the consortium to subcontract, and thereby reduces the consortium's incentives to submit a competitive bid. These two additional effects distinguish the structural consortium from the temporary consortium. We conclude that structural consortia should be assessed with more scrutiny than temporary consortia.",Joint bidding and horizontal subcontracting,https://www.sciencedirect.com/science/article/pii/S0167718721000205,10 March 2021,2021,Research Article,123.0
"Ivaldi Marc,Zhang Jiekai","Toulouse School of Economics, France,Hanken School of Economics and Helsinki Graduate School of Economics, Finland","Received 21 December 2020, Revised 1 March 2021, Accepted 1 March 2021, Available online 9 March 2021, Version of Record 26 November 2021.",https://doi.org/10.1016/j.ijindorg.2021.102729,Cited by (3),"The empirical analysis of media platforms economics has often neglected the multi-homing behaviour of advertisers. Assuming away the cross-substitutability and/or complementarity between the advertising slots of different platforms could damage the quality and the robustness of counterfactual analysis. To evaluate the consequence of such an abstraction, we compare the simulation results of hypothetical platform ==== when the demand on the advertising side is derived from a Translog cost model which allows for multi-homing, and when it is approximated by using a simple log-linear inverse demand model that ignores the differentiation among media platforms’ advertising slots. Ignoring the existence of substitutes or complements on the advertising side would result in overpredicting the losses of the viewers’ surplus and in underpredicting the gains in platforms’ revenues.","Recent studies on equilibrium models of advertising-financed media platforms based on the economics of two-sided markets have contributed to the understanding of the theory of harm raised by mergers in platform industries and the measurement of its effects. Simulating the outcomes of a change in platform ownership given the estimated preferences and costs relies crucially on the precise estimation of the demand patterns of viewers and advertisers.====This paper comments on a practical challenge in the simulation of media platform mergers that arises from the multi-homing behaviour of advertisers. Since the latter decides on how many advertising minutes to buy and often combine the advertising slots of several media platforms to reach the desired amount of audience, their choice of platforms is not discrete. Therefore, we cannot rely on a discrete choice model to estimate the substitutability or complementarity between advertising platforms. As a matter of fact, this issue has been neglected in the literature. Often the demand model of advertisers has been approximated a log-linear model that relates the logarithm of advert prices of a given platform to the logarithm of its own advert quantities and audience. While it is easy to implement, this approach assumes away the substitutability and/or complementarity between the advertising slots of different platforms. Not only such an abstraction is not realistic, but it certainly could deteriorate the quality and the robustness of predictions based on this model, like the outcomes of merger simulation.====Ivaldi and Zhang (2020) (IZ2020 in what follows) propose an alternative framework in which the advertisers’ decision program consists in minimizing the total advertising costs to reach a given level of audience. This framework can be implemented by approximating the true cost function of advertisers by a Translog cost function, which allows to explicitly estimate the own- and cross- price elasticities of demand for different advertising platforms. The Translog cost function has two nice features: First, it does not impose any substitution patterns between advertising platforms, which is consistent with the multi-homing behaviour of advertisers. Second, it can be easily estimated even with a large number of competing platforms. Nevertheless, this approach complexifies the procedure to simulate a change in policy such as a platform merger as it requires to solve sequentially two systems of equations. Indeed, a media platform, which raises revenues from selling time slots for advertisements, faces a trade-off between the preferences of viewers and the objectives of advertisers: The advertisers wish to reach a large audience while the viewers could be negatively affected by the amount of advertising. Such a media platform must internalize the network externalities between the viewers and the advertisers in order to maximize its profit. This feature should be taken into account in the merger simulation procedure. It implies in practice to find the profit maximizing level of advertising prices as a function of the corresponding demand for advertising. A simplified model for advertisers’ demand ignoring the cross-elasticities of demand on the advertising side allows one to derive directly a closed form solution for a channel’s advertising price as a function of its amount of advertisement. However, a more comprehensive model such as the proposed Translog cost function requires to solve a non-linear system of equations in order to obtain the demand for advertising slots of one channel as a function of the advertising prices of all channels of the market.====In this paper, we assess the relevance of such a complex simulation model. Using the same data as in IZ2020, we simulate the outcomes of two hypothetical platform mergers (between two substitutable advertising sales houses (ASHs) and between two complementary ones), when we approximates the demand on the advertising side, first by using the the Translog model proposed in IZ2020, and then by using a simple log-linear inverse demand model. Our results, comparing the simulated merger effects using the two alternative demand models for advertisers, suggest that one always over-predict the increase in total amount of advertising (accordingly, under-predict the increase in average advertising prices) when one neglects the substitutability or complementarity between the merging ASHs. In other words, ignoring the existence of substitutes or complements on the advertising side during the simulation of a platform merger would result in over-predicting the loss of the viewers’ surplus and in under-predicting the gain in platforms’ revenues. The magnitude of such discrepancies in the simulated merger effects is more important when it concerns a merger between two substitutable ASHs.",Simulating media platform mergers,https://www.sciencedirect.com/science/article/pii/S0167718721000229,9 March 2021,2021,Research Article,124.0
"Andres Maximilian,Bruttel Lisa,Friedrichsen Jana","Universität Potsdam, Am Neuen Palais 10, 14469 Potsdam, Germany,WZB, Reichpietschufer 50, Berlin 10785, Germany,Humboldt-Universität zu Berlin, Unter den Linden 6, 10099 Berlin, Germany,DIW Berlin, Mohrenstr. 58, 10117 Berlin, Germany","Received 3 July 2020, Revised 25 February 2021, Accepted 27 February 2021, Available online 4 March 2021, Version of Record 19 March 2021.",https://doi.org/10.1016/j.ijindorg.2021.102728,Cited by (13),"The experimental literature on antitrust enforcement provides robust evidence that communication plays an important role for the formation and stability of cartels. We extend these studies through a design that distinguishes between innocuous communication and communication about a cartel, sanctioning only the latter. To this aim, we introduce a participant in the role of the competition authority, who is properly incentivized to judge the communication content and price setting behavior of the firms. Using this novel design, we revisit the question whether a leniency rule successfully destabilizes cartels. In contrast to existing experimental studies, we find that a leniency rule does not affect cartelization. We discuss potential explanations for this contrasting result.","The formation and stability of cartels heavily relies on communication between cartel members. For example, Harrington Jr (2006) reports empirical data that cartel members typically meet on a monthly or at least quarterly basis, Awaya and Krishna (2016) show in a formal model that such frequent monitoring indeed leads to higher prices, and Fonseca, Normann, 2012, Fonseca, Normann, 2014 find that communication facilitates cartel formation in experimental markets. Similarly, other experiments support the idea that communication facilitates coordination (see, e.g., Cooper, DeJong, Forsythe, Ross, 1992, Gomez-Martinez, Onderstal, Sonnemans, 2016). While the aforementioned studies emphasize that communication in cartels serves as a means of information exchange, coordination, and monitoring,==== communication also contains a lot of content that is not directly related to cartel formation.==== Such innocuous communication may not directly target cartelization, but may nevertheless facilitate it in more subtle ways. For example, it seems plausible that informal chat and regular meetings create an atmosphere of “knowing each other,” thereby reducing strategic uncertainty, which may facilitate and stabilize cartels.==== If this is indeed the case, firms may use innocuous communication as an alternative to explicit price coordination and monitoring because a competition authority cannot rely on innocuous communication as sufficient evidence for a violation of competition law.====In this paper, we focus on the role of such innocuous communication on the effect of a leniency rule on cartelization. By implementing leniency rules, competition authorities can grant cartel members fine reductions if they deliver information that helps the authority to uncover the cartel. We argue that the possibility to engage in legally innocuous communication is likely to mediate the desired deterring effect of a leniency rule. This is because a firm that reports the cartel to the competition authority is only exempted from paying fines if it delivers sufficiently hard evidence for the cartel to be sanctioned. Such hard evidence typically comes in the form of some protocols of agreements about concerted practices of the firms. If no such explicit agreements exist—because communication was sufficiently innocuous—the leniency rule might lose its bite against cartels.====Authorities claim that leniency rules successfully impede cartelization in the long run (OECD, 2002). However, perverse effects of the leniency rule are possible because it has an option value that reduces the expected future fine (see Motta and Polo, 2003) and because reporting may be used to threaten potential deviating firms. Further, Emons (2020) argues theoretically that a leniency rule may not be effective if firms choose their degree of collusion. As empirical studies evaluating the total effect of such leniency rules on cartelization from observational data==== have an issue with the fact that undetected cartels are unobserved, research on the effectiveness of leniency policies prominently features experimental studies, where both detected and undetected cartels are observed by the researcher. These studies report that fewer cartels are formed, more cartels are reported, and average market prices are lower with a leniency rule than without it.====Previous experimental studies on cartel formation could not study the subtle effect of innocuous communication on the effectiveness of a leniency rule because most of them use interactions with structured price communication.==== Other studies allow for free-form communication,==== but have the firms vote on whether they want to communicate with each other. These studies treated a unanimous binary decision to communicate immediately as a cartel—thereby replacing cartel formation in the chat by the vote before the chat. This makes it unattractive for the firms to communicate innocuously because if communication—as soon as it takes place—is subject to fines anyway, they will likely prefer explicit cartel formation over more subtle communication content.====Our paper introduces an innovative experimental design that includes the human judgment of communication and competitive conduct in order to give firms the option to communicate without automatically running the risk of being fined for forming a cartel. In our experiment, firms interact repeatedly and a chat window opens automatically at the beginning of each round. Each market includes a participant in the role of the competition authority as an active player. Whenever a control takes place, this experimental competition authority reads the written free-form chat communication of the experimental firms, observes price setting, and decides about the fines for each firm, such that fines are positively correlated to the severity of the firm’s violation of the law. Decisions of the competition authority are incentivized properly by paying the participant in this role in proportion to the overlap between their judgments and those of an expert cartel lawyer, who judges communication content and prices after the experiment. In this experimental design, communication is only sanctioned if it serves the purpose of coordinating prices. In other words, if firms chat about innocuous topics, they may reach a relatively high level of price coordination without risking a fine.====We find a leniency rule to be ineffective. The frequency of cartel formation and average prices do not differ significantly between the two treatments with and without a leniency rule in our setup. Further, the communication content is surprisingly similar in the two treatments. This is in contrast with previous experimental studies and needs explanation. In the discussion of our results (in Section 5), we identify design features that may be causal for the failure of a leniency rule to reduce cartelization in our setup. As we depart from previous designs in various ways, we cannot attribute this difference to one single design aspect. Indeed, only the combination of different aspects might fully explain the discrepancy. In particular, we argue that the voting stage and the inhibition of innocuous communication in previous studies jointly amplified the effect of a leniency rule on cartelization.====In the following, we describe our experimental design in Section 2 and develop hypotheses in Section 3. We then present the experimental results in Section 4 and provide a discussion of them in Section 5. We conclude in Section 6.",The leniency rule revisited: Experiments on cartel formation with open communication,https://www.sciencedirect.com/science/article/pii/S0167718721000217,4 March 2021,2021,Research Article,125.0
"Affeldt Pauline,Duso Tomaso,Szücs Florian","Deutsches Institut für Wirtschaftsforschung (DIW Berlin) & Technische Universität (TU) Berlin,Deutsches Institut für Wirtschaftsforschung (DIW Berlin), Technische Universität (TU) Berlin, Centre for Economic and Policy Research (CEPR) & CESifo, Mohrenstr. 58, Berlin 10117, Germany,Wirtschaftsuniversität Wien, Austria","Received 19 May 2020, Revised 10 February 2021, Accepted 21 February 2021, Available online 4 March 2021, Version of Record 18 March 2021.",https://doi.org/10.1016/j.ijindorg.2021.102720,Cited by (4),"We study the determinants of common European merger policy over its first 25 years, from 1990 to 2014. Using a novel dataset at the level of the relevant antitrust markets and containing all relevant merger cases notified to the European Commission, we evaluate how consistently arguments related to structural market parameters – dominance, rising concentration, barriers to entry, and foreclosure – were applied over time and across different geographic market definitions. On average, linear probability models overestimate the effects of structural indicators. Using non-parametric machine learning techniques, we find that dominance is positively correlated with competitive concerns, especially in markets with a substantial increase in post-merger concentration and in complex mergers. Yet, its importance decreased following the 2004 merger policy reform. Competitive concerns are also correlated with rising concentration, especially if entry barriers and foreclosure are of concern. The impact of these structural indicators in explaining competitive concerns is independent of the geographic market definition and does not change over time.","Competition policy, that is, the design and enforcement of competition rules, is a cornerstone of the European Union (EU)’s policy to enhance the European single market and foster growth.==== The European Commission’s (EC) Directorate General for Competition (DG Comp) has jurisdiction over community-wide competition matters, making it the most important antitrust agency in Europe. Competition policy covers several areas ranging from the monitoring and blocking of anticompetitive agreements – such as cartels – to abuses by dominant firms, to mergers and acquisitions as well as state aid. Among these areas of antitrust enforcement, merger control plays a peculiar role. First, it is the only area with ==== enforcement. Second, it has important implications for other areas of antitrust: if anticompetitive mergers reduce competition and strengthen the dominant position of the merging firms, the ==== control of anticompetitive behavior becomes more difficult. Finally, mergers are the area of antitrust where the largest consensus on best practices exists. Therefore, of all competition policy tools, it is the one that attracts the most policy interest and economic research.====The European Communities Merger Regulation (ECMR), the legal basis for common European merger control, came into force in 1990. Over the course of the next 25 years, European merger control saw significant changes. In the early 1990s there were approximately 50 notified cases per year; this workload increased significantly in the late 1990s, reaching in the 2000s an average of about 280 cases annually. Procedurally, many novelties were implemented in the 2004 amendment to the ECMR: not only were new horizontal merger guidelines and the office of the chief economist introduced, but also, more importantly, a new substantive test, the ”significant impediment of effective competition” (SIEC) test and an efficiency defense clause were introduced. These amendments marked a substantial change in the legal basis for merger control enforcement in Europe. In this so-called effects-based approach, centered around a clearly stated theory of harm, the reliance on structural indicators of competition, such as market shares and measures of concentration was expected to decrease.====In this paper, we employ a new dataset containing all merger cases with an official decision by DG Comp (more than 5000 individual decisions) to evaluate the time dynamics of the EC’s decision procedures. Moreover, to obtain a more fine-grained picture of the decision determinants, we extend our analysis to the specific relevant product and geographic markets concerned by a merger (more than 30,000 individual markets). Thus, instead of only looking at the determinants of a merger decision in the aggregate, as commonly done in the literature, we also investigate those factors that cause competitive concerns in specific sub-markets and how they have changed over time. This step is particularly important because large mergers typically affect many different product markets in many different geographic regions. For example, the mergers in our data affect an average of six markets. Thus, the scope and depth of our data allow us to go beyond the existing literature by (i) not relying on a sample of decisions but instead reporting patterns for the whole population of merger cases examined by DG Comp; and (ii) allowing for heterogeneity within merger cases by analyzing the individual product and geographic markets concerned.==== This is one of the main contributions of our paper.====To make our findings comparable to the existing literature, we start by estimating the probability of intervention as a function of merger characteristics at the merger level. In particular, we focus on the role of four ”structural indicators”: (1) dominance when the joint market shares of the merging parties are above 50%; (2) a substantial increase in concentration, if the post-merger HHI is above 2,000 and the change in HHI is larger than 150; (3) entry barriers; and (4) the risk of foreclosure. We find that barriers to entry as well as a substantial increase of concentration are positively associated with the likelihood of an intervention, while our measures of dominance and the risk of foreclosure are not statistically significantly related to intervention. Moreover, the share of product markets with competitive concerns affected by the merger is also significantly related to an intervention decision. This approach naturally extends to the level of the individual markets, which represents the second main contribution of this paper: instead of estimating the overall probability of an intervention, we estimate the likelihood that competitive concerns are found in each specific product/geographical market affected by a merger. In this more fine-grained model, we find that both barriers to entry and the risk of foreclosure play a role. Structural indicators measuring joint market shares above 50% and a substantial increase in concentration show the expected positive and significant correlation with the likelihood of competitive concerns. While tightly defined (national) markets are positively correlated with the probability of concerns, the number of active competitors show a negative correlation with concerns.====The final contribution of this study is to assess the heterogeneity of the relation between the likelihood of competitive concerns and the four main structural indicators. Indeed, when and how these structural arguments are used might depend on the specificities of the observed market. To model this heterogeneity in the most flexible way, we employ non-parametric supervised machine learning (ML) techniques. Specifically, we implement a causal forest algorithm (Athey and Imbens, 2016), which allows us to model the heterogeneity in merger control decisions by making the association between structural indicators and the Commission’s decisions a function of ==== other covariates. In reporting our results, we focus on how the role of structural indicators has changed over time and along other relevant dimensions such as geographic market definition and the merger’s complexity. Moreover, we also study how the different structural indicators interact to explain the Commission’s competitive concerns.====First, we find that the importance of the merging parties’ market shares – our proxy for dominance – and, to a lesser extent, the role of a substantial increase in concentration due to the mergers have declined over time, while the importance of barriers to entry and the risk of foreclosure has not changed in DG Comp’s decision making. Yet, the impact of structural indicators appears to be less volatile than in the simple linear probability model. Thus, the arguments put forward by the EC to substantiate its decisions appear to be consistently applied over time once the process underlying these decisions is modelled in a flexible and rich way. Second, we observe that dominance plays a particularly important role in markets where the merger would substantially increase concentration, whereas rising concentration is more important when entry barriers and the risk of foreclosure are high. This is consistent with an effects-based approach where the interactions of various merger-specific characteristics play a key role to substantiate the theory of harm. Third, we show that the geographic market definition does not seem to affect the way theories of harm are put forward to motivate the competitive concerns. Finally, we observe that the role of dominance is more important, the more complex the merger under scrutiny.====The remainder of this paper is structured as follows. In Section 2, we discuss the institutional details of European merger control and review related studies. In Section 3, we describe the dataset used in estimation. We present the parametric model of EC merger interventions in Section 4, while section 5 presents the non-parametric estimations. We conclude in Section 6.",25 years of European merger control,https://www.sciencedirect.com/science/article/pii/S0167718721000138,4 March 2021,2021,Research Article,126.0
"Garella Paolo G.,Laussel Didier,Resende Joana","University of Milano, Department of Economics, Management, and Quantitative Methods Via Conservatorio, 7, Milano 20122, Italy,Aix-Marseille Univ., CNRS, EHESS, Centrale Marseille AMSE, France,CEF.UP, University of Porto, Portugal","Received 22 January 2020, Revised 2 February 2021, Accepted 3 February 2021, Available online 2 March 2021, Version of Record 15 March 2021.",https://doi.org/10.1016/j.ijindorg.2021.102717,Cited by (12)," almost identical. In equilibrium, only the firm with the largest natural market poaches customers from the rival. This firm has highest profits but not necessarily the largest market share. Aggregate profits are lower than under uniform pricing. All consumers gain, total welfare is higher herein than under uniform pricing if firms’ natural markets are sufficiently asymmetric. The low quality firm chooses the minimal quality level and a quality differential arises, though the exact choice for the high quality depends upon the cost specification.","In recent years, firms have remarkably enhanced their ability to engage in sophisticated price discrimination strategies. On the one hand, they are able to collect unprecedented amounts of information on their customers and use such information to produce very accurate customer profiling. On the other hand, firms have access to technologies that enable them to target prices individually according to customers’ preferences. We now have plethoric evidence==== on the use of such sophisticated pricing schemes, including personalized discounts, targeted coupons, personalized post-sale services or included warranties. In this paper, we investigate the competitive and welfare effects driven by firms’ ability to target personalized price offers==== to their returning customers within a vertical differentiation (VD) duopoly with behavior-based price discrimination (BBPD). In our set-up, one firm has a quality advantage over the other and a higher per unit production cost. We investigate how the interplay between this asymmetry and price personalization can affect the poaching strategies and the competitive process in general; whether both firms succeed in poaching or only one firm, and if so which of the two? Are profits related to quality and cost differentials, and how? Is there an advantaged seller? What will be the effects on prices and welfare? Finally, is there an effect on the choice of quality levels? We will show that the answers to these questions crucially hinge on the relative sizes of what we call the ”natural markets” of the firms====, which depend here upon the quality adjusted cost differential. This concept will play a key role in our model: on the one hand, it delimits the socially optimal allocation of varieties to consumers; on the other hand, it delimits, for each firm, the set of customers that cannot be profitably poached by the rival in the second period, significantly affecting the answer to the previous questions.====The possibility to engage in price discrimination by exploiting the information on the different evaluation of quality by heterogeneous consumers is at the heart of the theory of monopolist price discrimination. In this sense, the extension to duopoly under vertical differentiation is natural and it has already been the object of study in a number of works (e.g. Liu and Serfes, 2005 and 2004 deal with segmentation precision in a vertically differentiated static duopoly). Choudhary et al. (2005) look at price personalization within a static setting of vertical differentiation and they report several real-world examples of price discrimination practices in vertically differentiated industries. For instance, they refer to the use of ROI (Return On Investment) as an indicator of willingness to pay to personalize prices by software vendors (see also Jeffery et al., 2017). The authors also refer other real-world cases pertaining to the healthcare sector, the chemicals industry, and online retailers====; other instances include the airline and the hotel industry. Another good example of price discrimination in a vertical differentiation context is represented by wireless carriers, like Verizon, T-Mobile or Sprint. Indeed, these providers exhibit differences in regional coverage, an index of service quality (70% of the US territory for Verizon, 62% for T-Mobile and only 30% for Sprint)==== In addition, their pricing practices often involve offering discounts to new customers.====Among the many forms of price discrimination allowed by new generation technologies, the literature has devoted particular attention to intertemporal price discrimination practices based on customers’ purchase history, better known as BBPD (Behavior Based Price Discrimination). After the seminal works by Fudenberg and Tirole (2000) and Chen (1997), the many contributions investigating the outcomes and the mechanisms underlying BBPD mostly deal with a horizontal differentiation (HD) set-up (see Fudenberg and Villas-Boas, 2007 for a survey). BBPD is shown to usually lead to lower profits for firms; further, it causes welfare losses due to inefficient shopping by those consumers who switch from one to the other seller in the second period (excess ”transportation costs” in the Hotelling linear city).====Despite the evidence that a large number of firms are engaging in sophisticated pricing strategies within vertically differentiated set-ups, to the best of our knowledge the extant literature on BBPD has mostly focused on horizontal differentiation (with some exceptions, like for example Jing, 2016 and Rhee and Thomadsen, 2016). Moreover, in most studies on BBPD firms condition the second period prices on the first period shopping patterns of customers, engaging in third-degree price discrimination between new and old customers. However, many firms not only recognize who are their old customers but they also gather finer data on their preferences, using it to engage in market hypersegmentation and price discrimination.====In this vein, the recent paper by Choe et al. (2018) analyzes the outcomes of second period personalized pricing to old customers within a symmetric HD model.==== They find that firms are harmed by this possibility, which actually intensifies the negative profit effects identified by Fudenberg and Tirole (2000). In their setting, for initially fixed symmetric locations, contrary to standard==== BPPD models, (i) there are two mirror asymmetric equilibria (one firm prices more aggressively in the first period, ripping off higher intertemporal profits than the rival), (ii) only one-way poaching occurs, instead of two-way poaching.====We use the same information hypotheses as in Choe et al. (2018) but we look at the case of a vertically differentiated duopoly, in which the high quality good has a higher marginal cost than the low quality one. We subsequently show that, under our full market coverage assumption====, and with exogenous qualities, the present model is isomorphic to a generalization of the horizontal differentiation model ==== Choe et al. (2018), where their original model is extended to a (possibly) asymmetric setting with exogenous differences in reservation values (and/or differences in marginal costs/ firms’ initially exogenous locations). Both models lead indeed to the same equilibrium prices and profits as functions of two parameters: natural market share and price sensitivity of the (static) demand==== However, the two models differ in their interpretations, implications and in the initial stages (endogenous choice of qualities vs. endogenous location choices, for instance). The welfare analysis is also different in the two settings (see Schmidt, 2009). Moreover, there is no ==== reason to suppose in a vertical differentiation model that the natural market shares are equal. Differently, symmetry is a very natural and indeed the most common assumption in HD models (e.g. as in Fudenberg and Tirole, 2000 or Choe et al., 2018), possibly due to the symmetric outcomes of endogenous ”location” choices, which are obtained when cost symmetry is also imposed.====We find that the second period equilibria analysis of the pricing game is similar to Choe et al., 2018, and the one-way poaching property is shown to be robust. However, differently from them, herein, the poaching firm (in the second period) is always the one with the largest natural market====. In addition, as a consequence of the ex ante asymmetry between the firms’ natural market shares, our first-period equilibrium is unique in almost the whole parameter space. Mirror asymmetric equilibria exist only when the natural markets shares are close to equality.====In the competition process, we find that the firm with the largest natural market gives up a part of it to the rival in the first period. This is so because firms (especially the one with the smallest natural market) get informational advantages from serving a large fraction of the market in the first period. This incentive translates into lower first period prices in comparison with both benchmarks of uniform pricing and of BBPD under vertical differentiation (studied in Jing, 2016). The ranking of market shares in terms of size also follows a rich pattern and is different in the two periods, except for when one firm has ”extreme” market dominance (namely a natural market larger than 7/10 of the total). As a general rule, the firm that starts with the smallest natural market (the ”underdog”) carves a market segment beyond its natural market in the first period. In the second period the underdog looses only a part of the conquered natural market of the rival (while the underdog’s poaching price is ”ineffective” in that it will not attract new customers). The firm with the largest natural market, ”the top dog”, in spite of being the one giving up part of its ”natural customers” in the first period, ends up enjoying higher intertemporal profits than the underdog, except for a relatively small domain of parameters, for which the natural markets are almost equal at the start.====Concerning the overall profit effects, we find that firms are hurt by the possibility to engage in BBPD with personalized pricing (both with respect to the uniform pricing benchmark and with respect to the vertical differentiation setting with standard BBPD studied by Jing, 2016).==== In horizontal differentiation models consumers gain from BBPD; this is only due to the price decrease, absent any benefit from reallocation of varieties to consumers (to the contrary, when switching to the more distant firm in the second period, some consumers induce a loss in total travel costs with respect to the static benchmark). In our model, consumers gain (though not all of them) from lower prices in both periods and from being induced to switch to the socially efficient product choice. Overall, it is here confirmed that all consumers benefit from BBPD with personalized prices. The highest gains accrue to customers that are more contestable, who end up paying lower prices in both periods (other consumers—those with more extreme preferences—only pay lower prices in the first period, in comparison to the other two benchmarks).====In comparison with the uniform pricing benchmark, provided natural markets are asymmetric enough, the surplus loss generated from qualities misallocation is reduced: price personalization induces a market allocation closer to firms’ natural markets, reducing the amount of inefficient shopping from the contestable customers. This is never the case in Choe et al. (2018). Despite this welfare improvement, some degree of misallocation remains. This is due to the efforts of the underdog to attract customers from the rival’s natural market in the first period: this distortion is corrected in the second period, when the top dog poaches some customers from the rival, but it is not fully eliminated.====Finally, we briefly analyze endogenous quality choices. Under reasonable (but not general) assumptions on the dependence of marginal costs on qualities, a quality increase by the low quality firm will, ==== lead to a decrease in its natural market, with adversal effects on its profits. An increase in the quality difference also relaxes price competition. Thus, it is easy to predict that this firm will choose as low a quality as possible, as in the standard vertical differentiation model. The high quality firm decreases its natural market if it increases quality and hence the incentive to relax price competition runs opposite that of creating a hefty natural market. This firm will then go for a quality lead, but the precise quality level depends heavily on the specification of costs.====The plan of the paper is as follows. Section 2 lays down the assumptions and defines the model set-up. Section 3 introduces the notion of natural market and uses backwards induction to compute the equilibrium pricing decisions. Section 4 characterizes the economic properties of the equilibria regarding pricing patterns, consumers’ surplus, profits, and social welfare. Section 5 briefly analyzes the issue of endogenous qualities. Section 6 shows the isomorphism between our VD model and an asymmetric extension of Choe et al. (2018) HD model. Finally, Section 7 summarizes the main findings and discusses possible extensions. The proofs are left to the Appendix.",Behavior based price personalization under vertical product differentiation,https://www.sciencedirect.com/science/article/pii/S0167718721000102,2 March 2021,2021,Research Article,127.0
"Gehrig Thomas,Stenbacka Rune","Department of Finance, University of Vienna, Oskar-Morgenstern-Platz 1, Wien 1090, Austria,CEPR, ECGI and VGSF, Austria,Hanken School of Economics, P.O.Box 479, Helsinki 00101, Finland","Received 20 October 2019, Revised 4 February 2021, Accepted 7 February 2021, Available online 19 February 2021, Version of Record 5 March 2021.",https://doi.org/10.1016/j.ijindorg.2021.102718,Cited by (1),"We explore how the nature of the screening technology and the organization of the submission system affect the screening incentives of competing journals. We characterize the effect of market structure on screening by comparing a ==== with a monopoly in the journal market. Exclusivity requirements for submissions induce more screening than systems with parallel submission. With sequential submissions, competition between journals induces adverse selection effects, whereby the average quality of the pool of submissions is degraded in response to acceptance of high-quality manuscripts. We outline how information exchange between journals impact on this adverse selection mechanism.","Academic journals enjoy a unique position for disseminating new original scientific research. They attest quality and correctness of scientific results. Accordingly, in most scientific disciplines publications in academic journals play a key role for assessing research quality for evaluations of academic departments as well as academic job market candidates. However, there is considerable variation across disciplines as far as the role of journal publications and the organization of the whole review process are concerned. For example, whereas publishing innovative research in the natural sciences is largely evaluated according to its contribution to new positive knowledge, in the social sciences more weight is given to normative considerations as well as relevance for society in addition to the fact that published results are correct and extend positive knowledge. Accordingly, there is considerable variation in rejection rates and evaluation delay across disciplines.====Leading journals in economics and finance have become notorious for their increasing bottleneck of consequent evaluation delay. Ellison (2002a) documents a (more than) doubling of publication times in the leading economics journals around the turn of the millennium, and even editors of leading journals are starting advocating “refereeing less” in order to “progress more” (Spiegel, 2012). Moreover (Card and DellaVigna, 2013) document a three-fold increase in the length of published papers relative to the 1970s. Given the bottlenecks at the top-journals, some obvious suggestions come to mind for remedying this seemingly worrisome situation: entry of new (top-level) journals and parallel submissions. The bottleneck obviously is generated by capacity constraints at the leading journals. On the other hand, new journals will not easily be able to enter at the top as they need time and a sufficiently positive track record to establish reputation for top quality. Moreover, market perceptions might just extend historically developed rankings such as the widely perceived top-five journals in economics and finance (Heckman and Moktan, 2020).====Parallel submissions are applied not only for the academic book publishing, but also for journal publishing in disciplines like legal studies. But, also in that field a considerable amount of delay is observed in the publication process. On the other hand, law journals typically do not attain the same central role in the certification of research quality for the academic labour market as in economics and finance. This is reflected, for example, in the fact that the involvement of established researchers in editing and management of law journals is much lower than in economics and finance (Posner, 1995).====In this analysis we follow Armstrong (2015) and Hirshleifer (2015) in modelling academic journals as certifiers of research quality. In particular, we view the referee process not only as a mechanism to reduce serious flaws in published papers and to eliminate false claims, but also to make sure that the high-quality papers do pass the publication threshold. In their empirical analysis of three elite medical journals, (Siler et al., 2015) focus on these important roles of the peer review system, and they pay particular attention to the concern that journal evaluations may reject unconventional or outstanding ideas. An editorial of Nature (2003) raises similar concerns. Whereas other studies discuss the role of strategic behaviour within the review process, such as reviewer signal-jamming, as a source of delay, we concentrate on the basic role of the submission system itself as a device to certify quality when reviewers are “honest” and non-strategic.====We design a model which facilitates an explicit characterization of the strategic investments by competing journals to screen the quality of submitted manuscripts conditional on the imperfections associated with the available screening technology. In particular, we pay attention to the microstructure of the screening technology by distinguishing the screening benefits associated with the identification and acceptance of publication of high-quality articles (i.e. the elimination of type-I errors) from those associated with the identification and rejection of low-quality articles (i.e. the elimination of type-II errors). The model allows to compare the screening incentives of parallel submission systems with those of sequential submission systems. Furthermore, the model can equally well be viewed as an analysis of the effects of competition between organizations or agencies engaged in the funding of research grants. Such agencies typically apply parallel submission systems and our model sheds light on why that is the case.====We provide answers to the following questions: How does competition between journals affect the intensity of screening and the resulting quality of published research? Which role does exclusivity of submissions play in this regard? Which are the advantages or disadvantages of a sequential submission system relative to those associated with parallel submissions? And, what are the main effects of information exchange between journals within the framework of a system of sequential submissions?====We start with analyzing a system of parallel submissions. While this system seems very generous in exploiting resources for screening, it might provide the best chances for expedient publications by avoiding capacity-based bottleneck problems. Indeed we characterize under which conditions aggregate screening effort by duopoly journals exceeds that conducted by a monopoly journal. We also characterize conditions under which a monopoly would emerge endogenously as well as conditions under which all submitted manuscripts would be accepted.====We characterize the screening investments undertaken in a duopolistic journal industry operating within the framework of a sequential submission system without a clear ranking between the journals. In particular, we compare the screening investments in a system with sequential screening with those of a system with parallel screening. We find that the equilibrium investments in screening are higher with sequential submissions than with parallel submissions. The reason is that the sequential submission system has cost advantages as the cost of screening for the manuscripts accepted by the competing journal can be avoided. Hence, in the absence of a ranking both journals face the same pool characteristics as under parallel submissions, thereby enjoying the same benefits from screening, while the sequential system faces lower costs. With convex cost functions this implies that in equilibrium the average quality of accepted manuscripts is higher under sequential submissions since classification errors are reduced. However, the sequential submission system tends to cause delay as some of the published articles require two rounds of evaluation prior to acceptance.====Overall we find that the sequential system with a ranking system in place is particularly valuable, when costs of false acceptances are high. When the costs of falsely rejecting high-quality manuscripts are high, however, the uncoordinated parallel system is particularly good, and even more so when delay is costly.====A number of studies have developed mechanisms explaining various features associated with the evaluation procedures applied by the journals. Ellison (2002b) presents a theoretical model to explain the evolvement of quality standards applied by referees and explains empirical observations from different academic disciplines in light of that model. Relatedly, Hirshleifer (2015) designs a model of reviewer signal-jamming, which leads to excessive effort to correct blemishes in papers at the expense of true scientific innovation. Bayar and Chemmanur (2013) develop a model of the editorial process with a particular focus on how editors can optimally manage the potentially biased information from referees as self-interested experts. In contrast to all of these studies, we design a duopoly model for the analysis of strategic screening competition between journals. Our model facilitates to distinguish between models of parallel submission systems and single submission systems.====A number of studies, for example Armstrong (2015), McCabe and Snyder (2005) and Jeon and Rochet (2010), analyses the journal market with a particular emphasis on the effects of open access. Our study does not address the issue of open access.====Our analysis has relevance also outside the journal industry. To an increasing extent also in many service industries the implementation of customer value-based management requires the ability to acquire consumer-specific information. The most obvious examples are the banking and insurance industries, where evaluations of consumer-specific creditworthiness and riskiness, respectively, belong to the core activities. But, to an increasing extent also in many service industries the implementation of customer value-based management requires the ability to acquire consumer-specific information. An extensive literature in banking has analysed the effects of credit market competition on screening investments. Broecker (1990) is a seminal contribution exploring how lending market competition affects screening investments under circumstances where the banks evaluate loan applicants based on imperfect creditworthiness tests with exogenous precision. Broecker (1990) identifies a winner‘s curse problem with the feature that an increased number of competitors, each conducting independent creditworthiness tests of given precision, decreases the average quality of the funded projects.==== Banerjee (2005) endogenizes information acquisition by allowing banks to select between borrower tests with two different degrees of precision. He distinguishes between the ability of the screening technology to “screen in productive borrowers” (==== - screening) and “screen out unproductive borrowers” (==== - screening). Within such a framework (Banerjee, 2005) establishes that the two types of screening have different effects on the screening incentives and that these two types of screening imply qualitatively different information externalities between competing lenders. A related category of studies, exemplified by Bolton et al. (2012), has explored effects of competition between credit rating agencies with models focusing on particular industry-specific features like ratings shopping. The particular distinction between simultaneous and sequential evaluations seems to be an idiosyncratic feature of the journals industry, and it has not been in the focus of attention in screening-based studies in banking or financial markets more generally. In this respect, our study adds an important dimension to the analysis of screening-based competition.====The framework of our analysis builds on the pioneering work of Sah and Stiglitz (1986) on fallible communication and the aggregation of coarse signals. Gehrig et al. (2000) analyse optimal investments into signal quality for a single organization under different organisational forms, “polyarchical” versus “hierarchical”. In the present analysis a similar analysis is performed in a market context. Screening investments are compared in the market for scientific journals between multi-journal monopolists (hierarchies) and independent journals (polyarchies).====Only a few studies analyse screening-based competition within the framework of general models, which are not restricted to the financial sector. Hoppe and Lehmann-Grube (2008) explore the effects of customer-specific tests with exogenous precision on the intensity of price competition in a differentiated industry. They find that sufficiently differentiated information partitions eliminate price-undercutting, and may therefore support a price equilibrium in pure strategies. Aoki and Spiegel (2009) apply a similar type of model in their study of pre-grant patent publication in the context of a cumulative innovation model.====Farhi et al. (2013) present a general analysis of certification markets with a particular emphasis on transparency, coarseness of rating patterns and sequential certification strategies. Our study has a more limited perspective, but in contrast to Farhi et al. (2013) we focus on oligopolistic competition between certifiers and on screening technologies which distinguish between type-I and type-II errors.====Our study proceeds as follows. Section 2 introduces the model of screening investments, which determine the precision whereby journals certify the quality of published articles. Section 3 characterizes the screening investments within the framework of a parallel submission system, whereas Section 4 focuses on a sequential submission system with as well as without information exchange between journals. In particular, Section 4 compares the screening performance associated with a parallel submission system and a sequential submission system. Finally, Section 5 presents some concluding comments.",Journal competition and the quality of published research: Simultaneous versus sequential screening,https://www.sciencedirect.com/science/article/pii/S0167718721000114,19 February 2021,2021,Research Article,128.0
"Ahlin Christian,Kim In Kyung,Kim Kyoo il","Department of Economics, Michigan State University, 486 West Circle Drive, East Lansing, MI 48824,Department of Economics, Nazarbayev University, 53 Kabanbay Batyr, Astana 010000, Kazakhstan,Department of Economics, Michigan State University, 486 West Circle Drive, East Lansing, MI 48824","Received 16 March 2020, Revised 23 January 2021, Accepted 9 February 2021, Available online 19 February 2021, Version of Record 5 March 2021.",https://doi.org/10.1016/j.ijindorg.2021.102719,Cited by (1),"In this article, we study under what circumstances a gas station is more likely to commit fuel fraud. Using a new and hitherto unexploited list of fuel fraud detections, we find evidence that stations under less favorable ==== – higher operating costs and possibly more competitors – engage in fraudulent activity more often. Chain-affiliated stations commit fraud less often, suggesting an effectiveness in harnessing reputational incentives. Also, fuel fraud tends to cluster among nearby stations, consistent with propagation of illicit activity from one station to others nearby. As for pricing behavior, in general gas stations appear to keep price constant and take higher price-cost margins when selling adulterated fuel, suggesting that consumers are harmed by this kind of fraud.","Fraud has the potential to impair a market’s operation significantly. For one, the existence of fraud can lead to market breakdown. Concern about fraud lowers consumers’ willingness to pay, and it may do so to the extent that production without fraud is not viable. The only equilibrium may be one with significant fraud and suboptimal quality, or with no market at all (Akerlof, 1970). Second, fraud can lead to greater incidence of harm, if consumers are unaware that they are purchasing lower quality and do not take this into account in their usage of the good. Third, fraud can hamper the government’s ability to regulate and collect taxes. An example of the former is when firms circumvent safety or environmental regulations by fraud, advertising inferior products to be of higher quality or regulation-compliant. An example of the latter is when firms secretly adulterate a product with ingredients that face lower tax rates.====Despite its negative impacts, fraud plagues all countries to some degree. For example, fraud was at the top of the consumer complaint list in the U.S. in 2018.==== In the retail sector, documented consumer fraud takes various forms including false or misleading advertising (Barrage et al., 2020, Zinman, Zitzewitz, 2016), selling products with bogus certifications, and tampering with foods (Zhang and Xue, 2016).====The goal of this paper is to provide an empirical exploration of when and why firms commit fraud. We focus specifically on the effects of competition, reputation, and peer influence. Does competition exacerbate or temper incentives for fraud? Can reputation serve as a deterrent to fraud? Does commission of fraud tend to spread within a local market? Understanding when fraud arises is one way to illuminate the economic forces underlying fraud, enabling us to select models that have empirical backing. It can also inform policy by enabling monitoring resources to be concentrated on those settings where fraud is more likely, for example, in more (or less) competitive markets or in markets where fraud has (or has not) recently been detected.====This paper studies fraud in the retail gasoline industry. This industry has several features that give stations incentives to commit fraud. One, quality is not readily or quickly observable, and cheaper near-substitutes exist for the fuels that gas stations sell. Gas stations can thus sell a lower-quality product that, to the typical customer, is observationally equivalent to the advertised product. Two, the price differences between advertised products and near-substitutes are often significantly exacerbated by varying tax rates on different types of fuels. These differences allow stations to lower costs significantly by adding less heavily taxed adulterants to gasoline or diesel.====Retail gasoline fuel fraud can thus have serious negative impacts in ways discussed above. It results in large loss of government tax revenue from fuels. In the U.K., for example, lost tax revenue due to illicit diesel amounted to £1.6 billion in 2002,==== while in Korea the estimated share of adulterated fuel was 7–10 percent for gasoline and 15–20 percent for diesel in 2011.==== It can also result in the circumvention of environmental regulations, as exhaust emissions turn out to be higher than advertised. Fuel fraud is also harmful to consumers – particularly when unanticipated and unpriced – because adulterated fuel can shorten the life span of the vehicle and lead to extra maintenance costs.====Thus, retail gasoline provides an interesting case study for fraud commission. For our empirical analysis, we use a new and hitherto unexploited list of gas stations caught for fuel fraud in Korea from January 2010 to May 2015. There are 1083 cases of fuel fraud exposure in the list, which consists of the name of the station and the exposure date for each case, as well as the type of fraud – selling adulterated fuel or manipulating gas pumps – for approximately 70 percent of cases. We merge this list of fraud exposures with monthly panel data that contain both price and non-price information on the universe of Korean gas stations.====We first estimate a cross-sectional model to learn which station characteristics are more related to fraud detection over the sample period as a whole, and then conduct panel data analyses to predict both the timing and the occurrence of detection. Specifically, we estimate both linear and non-linear random effects models that control for station unobserved correlated heterogeneity, and also conduct fixed effects analysis allowing for the heterogeneity to be arbitrarily related to regressors. We find evidence that stations under less favorable economic conditions engage in fraudulent activity more often. The detection likelihood is lower by 0.2-0.3 percentage points for premium stations compared to non-premium stations, and is lower by 0.7-1.6 percentage points for self-service stations compared to stations with attendants. Further, typical panel estimates suggest that an additional competitor within a 0.2-mile radius is associated with a 0.2 percentage point higher likelihood of fuel fraud detection.==== Given that detection occurs in 0.5 percent of observations in our data, these differences are non-negligible. Moreover, independent stations are more likely to get caught for fuel fraud compared to chain-affiliated stations and government-supported thrifty stations.====Collectively, these findings are consistent with seminal models of crime (Becker, 1968) and fraud (Darby and Karni, 1973). Darby and Karni argue that if fraud puts at risk a future stream of profits, then lower anticipated future profits lower the cost of fraudulent activity and raise the likelihood of fraud. This perspective can explain our findings that stations in less profitable economic conditions – such as those facing more competitors or higher operating costs from providing full service – commit fuel fraud more often. The finding that stations offering premium gasoline commit less fraud can also be explained in this framework, since these stations typically have sunk larger capital investments (the more sophisticated pumping system and often other services, e.g. car wash or repair) that enable higher future margins and profits.====The finding that chain-affiliated stations commit less fraud suggests that reputation can be helpful in lessening fraud. As Tirole (1996) shows, a strong reputation can be self-sustaining, leading to higher markups and thus a greater cost of being discovered committing fraud (more generally, producing low quality). Given that gas station chains in Korea set and monitor their own quality standards, and are able to exclude and impose penalties on deviating members, they are well positioned to select into a high-quality reputational equilibrium. This is consistent with our finding that chain gas stations are less likely than independent stations to commit fraud.====We turn next to the question of whether fraud spreads across near neighbors. We find that one additional fraud detection in the neighborhood in the previous quarter is associated with a 0.08 percentage point increase in the likelihood of fraud detection of a station, ceteris paribus. This finding echoes that of Glaeser et al. (1996) for crime, and is consistent with illicit activity resulting from collusion, local norm diffusion, shared supply conditions, or competitive pressures. The analysis also reveals that once a station gets caught, it is much less likely to commit fuel fraud again.====Finally, our data enable us to analyze the pricing behavior of fraud-committing gas stations. In the cross-section, we find that fuel prices are 0.5-1.1 percent lower in stations selling adulterated fuel than in non-fraud stations. This could be because gas stations committing fraud lower their prices to match their lower costs, or because stations commanding low prices are more tempted to commit fraud, as theory suggests (Darby and Karni, 1973). Our panel data allow us to cast doubt on the first explanation. Specifically, we compare prices within stations shortly before and after fraud was detected, which we argue sheds light on price differences with and without fraud. We find no difference on average, suggesting that stations committing fraud do not lower price but instead take higher price-cost margins. Consequently, a station committing fraud appears to take the entire ill-gotten gains from fuel adulteration, while consumers (and the government) only suffer from fuel fraud.====This study’s findings lend confirmation to the economic theory of fraud in the context of a relatively competitive retail market. To our knowledge, it also provides the first evidence that competition may be associated with greater defrauding of consumers.==== Competition has been tied to fraud in some studies, but when so, it is a third-party rather than the customer being defrauded (e.g. the government or an external investor). While the government is the biggest victim of fraud in our study, consumers also often are. The paper also is one of the first to find evidence suggestive of reputation, facilitated through chain affiliation, as a mitigator of fraud. Finally, we are able to break ground in understanding pricing behavior of firms committing fraud. Overall, the findings suggest that fraud can harm consumers and government revenue, and suggest that increasing monitoring intensity may be fruitful when competitive conditions stiffen.====One shortcoming of the study is that we observe detected fraud, not actual fraud. This is not a problem if inspection and detection probabilities are largely random, as conversations with the monitoring agency suggest, or at least uncorrelated with key station characteristics. Any concern is also mitigated to a degree by the cross-sectional analysis, which is arguably less affected by any confounding factors due to fraud investigator strategy, yet yields results consistent with those from the panel data analysis. Moreover, results hold up when we control for some likely determinants of the probability of inspection – distance to the regional monitoring authority, and whether we observe the station to have been caught in fraud before. Still, this is a caveat to our results, though one shared by a number of studies of corruption (e.g. Glaeser and Saks, 2006) and corporate fraud (see discussion in Amiram et al. (2018)).==== A second shortcoming is that we are using observational data and reporting correlations. Relationships between station characteristics and environment variables can be complex, and we do not claim to be identifying causal relationships. That said, using the panel nature of the data and controlling for key observables, we are able to rule out some confounding explanations. In sum, the paper uncovers empirical facts and relationships that, in conjunction with theory, help shed light on the nature of fraud.====The rest of the article is organized as follows. We discuss a conceptual framework for commission of fraud in Section 2. Background information on the Korean retail gasoline industry, including fraudulent and anti-fraud activity, is provided in Section 3. Data are described in Section 4. We estimate empirical models of fuel fraud and interpret the results in Section 5, and analyze fuel prices in Section 6. Related empirical literature is discussed in Section 7, and Section 8 concludes.",Who commits fraud? evidence from korean gas stations.,https://www.sciencedirect.com/science/article/pii/S0167718721000126,19 February 2021,2021,Research Article,129.0
"Calvano Emilio,Calzolari Giacomo,Denicoló Vincenzo,Pastorello Sergio","Bologna University, Italy,Toulouse School of Economics, France,European University Institute, Italy,Centre for Economic Policy Research, CEPR, UK","Received 1 December 2020, Revised 18 January 2021, Accepted 18 January 2021, Available online 14 February 2021, Version of Record 26 November 2021.",https://doi.org/10.1016/j.ijindorg.2021.102712,Cited by (16),"We show that if they are allowed enough time to complete the learning, Q-learning algorithms can learn to collude in an environment with imperfect monitoring adapted from Green and Porter (1984), without having been instructed to do so, and without communicating with one another. Collusion is sustained by punishments that take the form of “price wars” triggered by the observation of low prices. The punishments have a finite duration, being harsher initially and then gradually fading away. Such punishments are triggered both by deviations and by adverse demand shocks.","In the modern economy, firms increasingly delegate important strategic choices to algorithms of various sorts. Some, powered by Artificial Intelligence (AI), are capable of learning autonomously by adapting their behavior to past experience. This has raised concerns that such reinforcement-learning algorithms may learn to collude without having been specifically instructed to do so and without communicating with one another.====To assess these concerns, two routes have proved useful. One tries to detect algorithmic collusion from market outcomes. For example, Assad et al. (2020) analyze German retail gasoline markets finding that the adoption of AI-pricing algorithms is associated with a sizeable increase in the stations’ margins. The other approach simulates the algorithms’ behavior under controlled conditions in artificial markets. For example, Klein (2019) conducts this kind of analysis for the Maskin and Tirole (1988) model of staggered pricing, and our previous work (Calvano et al., 2020a) considers an infinitely repeated Bertrand game where firms can perfectly observe rivals’ prices.==== Both studies conclude that algorithmic collusion can arise spontaneously in such environments, but both recognize that more work is needed to ascertain the robustness of this finding.====In this paper, we contribute to this latter strand of research by analyzing the case of imperfect monitoring. Previous research has focused on the case of perfect monitoring because pricing algorithms are often used in marketplaces, such as Amazon, where each seller can monitor rivals’ prices in real time, and because competition authorities stress that such markets are more vulnerable to collusion.==== However, theory shows that collusion is possible even under imperfect monitoring, and algorithms are increasingly used also in markets where rivals’ strategies are not easy to observe. One example is financial markets where agents exploit their inside information by hiding behind noise traders.==== Another example is markets for electricity.==== It is therefore important to study whether algorithmic collusion is possible when agents can observe aggregate outcomes (e.g., market prices) but not individual behaviors.====While the examples mentioned above present important specificities, here we develop the analysis at an abstract level. We use Green and Porter (1984) classic model of repeated interaction under imperfect monitoring, where firms set quantities and observe the price level but cannot perfectly infer rivals’ outputs because demand is stochastic. This simple model is well understood and has become the workhorse of the theory of collusion with imperfect monitoring.====To simplify the analysis, we assume that all firms use similar algorithms. In particular, we focus on algorithms of the Q-learning type, as we did in Calvano et al. (2020a). Maintaining the focus on the same type of algorithms helps compare the results and identify the specific effects of imperfect monitoring.====We show that if Q-learning pricing algorithms are allowed enough time to complete the learning, they can collude even with imperfect monitoring. Collusion is not full, which is natural given that the algorithms learn purely by trial and error, but still yields substantial supra-competitive profits. As monitoring becomes more imperfect, making it more costly to punish deviations that can be confounded with adverse demand shocks, the level of profit decreases.====The strategies that the algorithms eventually learn are remarkably similar to those considered by Green and Porter (1984). That is, when the price level falls below a certain threshold, the algorithms enter into a “price war” that lasts for several periods and then revert to the pre-deviation output. One notable difference between the fully rational agents of Green and Porter (1984) and our algorithms, however, is that the former possess an infinitely long memory and thus know when to end the punishment, whereas the memory of the algorithms is short-lived by design. Plainly, the boundedness of memory hinders the direct implementation of punishments of duration longer than the memory. However, the algorithms learn to circumvent this problem by resorting to an ingenious method, which will be described in greater detail below.====We take these findings as an indication that imperfect monitoring is not, in itself, an insurmountable obstacle to autonomous algorithmic collusion. Naturally, our “experimental” approach can only show that algorithmic collusion is a real possibility but cannot provide conclusive evidence about its occurrence in practice. Skeptics may point to a number of specific modelling choices made in our simulations and wonder whether the results are robust to changes in these choices. Only more work can dispel, or substantiate, these doubts. This paper focuses on a single factor, namely, imperfect monitoring. Others remain to be addressed.====The practical significance of our findings may also be questioned on the ground that it takes a large number of periods, in the order of hundreds of thousands, for the algorithms to stabilize their behavior.==== In fact, the algorithms start to collude much earlier than they converge to a limit strategy. Moreover, pricing algorithms are often trained off-line before being put to work, which may significantly reduce the time needed to learn. Yet, the time scale is another important issue to be addressed by future research. (It may be less of an issue in such markets as the foreign exchange market, where large volumes are traded at high frequencies. In these markets, a “period” may be a fraction of a second, so even millions of repetitions could take place in a few weeks.)====The rest of the paper is organized as follows. The next section provides a brief description of the Q-learning algorithms used in our simulations. Section 3 describes the economic environments where the simulations are performed. Section 4 discusses the results for the baseline case. Section 5 reports on various robustness exercises. Section 6 offers some concluding remarks.",Algorithmic collusion with imperfect monitoring,https://www.sciencedirect.com/science/article/pii/S0167718721000059,14 February 2021,2021,Research Article,130.0
"Byrne David P.,Martin Leslie A.","Department of Economics, The University of Melbourne Level 4, 111 Barry Street, Melbourne, Victoria 3010, Australia","Received 22 December 2020, Revised 2 February 2021, Accepted 3 February 2021, Available online 10 February 2021, Version of Record 26 November 2021.",https://doi.org/10.1016/j.ijindorg.2021.102716,Cited by (5),"Competition and consumer search costs can lead to ==== in an ====, urban, and behavioral economics for explanations as to why this pattern persists. Finally, we conclude that IO researchers have much to offer in identifying and quantifying the distributional impacts of market power, thereby contributing to current academic and policy debates on efficiency-equity trade-offs in policy design.","For more than a generation, governments worldwide have deregulated industries to enhance economic efficiency and reduce prices paid by consumers. But consumer gains from competition depend on how attentive consumers are to prices. There is an extensive empirical literature documenting that many consumers fail to shop enough to obtain the best details, from mortgages (Woodward, Hall, 2012, Allen, Clark, Houde, 2014, Alexandrov, Koulayev, 2017) to credit cards (Stango and Zinman, 2016), savings accounts (Adams et al., 2019), gasoline (Lach and Moraga-González, 2017), and pharmaceuticals (Sorensen, 2000). This work follows from an established theoretical literature that shows how search costs can explain equilibrium price dispersion (Burdett and Judd, 1983, Salop, Stiglitz, 1977, Stahl, 1989, Stigler, 1961). Empirical research on search costs has long focused on recovering and describing the distribution of search costs across consumers (e.g., Hortacsu, Syverson, 2004, Hong, Shum, 2006, Moraga-González, Wildenbeest, 2008, Kaplan, Menzio, 2015), but little is known as to who sits where in that distribution.==== engages most in search? And, perhaps more critically from a policy perspective, who fails at search? For policymakers considering deregulation and decentralized price-setting, a central question is whether or not lower income consumers actively and effectively search for prices and, as a result, whether they will end up paying higher or lower prices.====Retail electricity provides an important case in point. Energy market deregulation has led to substantial price dispersion in retail electricity prices in many countries (Giulietti et al., 2014, Hortaçsu, Madanizadeh, Puller, 2017, Byrne, Martin, Nah). Proponents of deregulation that would decentralize price-setting by retailers ==== anticipated price dispersion, but presumed it would be progressive (Thwaites et al., 2017): lower income households, with a lower opportunity cost of time and a higher valuation of search-related electricity cost savings, would actively engage in retailer search and switching and realize lower prices. With this form of consumer segmentation, cost savings achieved from deregulation would go disproportionately to lower-income households. However, after 20 years of deregulation, many governments report that low-income consumers exhibit the lowest propensity to search, have extreme retailer inertia, and pay some of the highest electricity prices (Competition and Markets Authority, 2016, Brattle Group, 2018, Australian Competition and Consumer Commission, 2018).====The experience of lower-income consumers being less likely to search for and obtain the best deals is likely to apply beyond energy markets. Other important industries that feature significant consumer search costs include healthcare, banking, telecommunications, retail, or air travel. There is, however, limited evidence to date into the relationship between a consumer’s income and their level of engagement in markets. Search and consideration sets are rarely directly observed, measures of search cost inferred from prices paid are often confounded by unobservables, and individual-level data on consumer search tends to be proprietary or only crudely matched to income, if at all. Researchers in IO are, however, beginning to gain access to administrative, scanner, and online search platform data that allows us to study search frictions over the income distribution and resulting distributional impacts of market power.====In this paper, we provide a literature review of research on the relationship between consumer search and income. Some of the papers that we describe tackle this relationship head on. Most, however, have different goals and address the search-income gradient only indirectly. We believe that the observed relationship between income and search behavior is a first-order question of interest for both researchers and policymakers, and that emerging data and econometric methods in IO present a real opportunity to better understand search-related linkages between market power and inequality.====The paper proceeds in four parts. In Section 2, we collect research examining how search varies over the income distribution. We add to this evidence base using new, individual-level data on search and income from retail electricity. Our findings, along with those from prior studies from different industries using different empirical methods, reveal an inverse-U shape relationship between consumers’ willingness to search and their income: both the very poor and the very rich tend not to search.====Section 3 develops an extensive analysis of potential mechanisms for the inverse-U relationship, and for understanding why low-income households are less-engaged in search behavior. In identifying potential mechanisms, we draw upon and organize findings from IO, financial literacy, urban, and behavioral economics. Section 4 then discusses supply-side responses to the search-income gradient and the potential for search-based price discrimination and regressive price dispersion.====We summarize and conclude in Section 5. Here, we connect the research agenda outlined in the paper to long-standing research programs in economics on inequality in areas such as political economy or trade, but also burgeoning research programs in macroeconomics and market design. IO economists can inform these academic and policy debates by establishing how search frictions in the many large concentrated industries that dominate the day-to-day lives of many households contribute to income inequality. Such analyses would see IO economists, like those in other fields, inform efficiency–equity trade-offs in policy design.",Consumer search and income inequality,https://www.sciencedirect.com/science/article/pii/S0167718721000096,10 February 2021,2021,Research Article,131.0
Kaplow Louis,"Harvard University and National Bureau of Economic Research, United States","Available online 3 February 2021, Version of Record 26 November 2021.",https://doi.org/10.1016/j.ijindorg.2021.102714,Cited by (4),"Economic analysis of competition regulation is most developed in the domain of horizontal mergers, and modern agency guidelines reflect a substantial consensus on the appropriate template for merger assessment. Nevertheless, official protocols are understood to rest on a problematic market definition exercise, to use HHIs and ΔHHIs in ways that conflict with standard models, and more broadly to diverge with how economic analysis of proposed mergers should be and often is conducted. These gaps, unfortunately, are more consequential than is generally appreciated. Moreover, additional unrecognized errors and omissions are at least as important: analysis of efficiencies, which are thought to justify a permissive approach, fails to draw on the most relevant fields of economics; entry is often a misanalyzed afterthought; official information collection and decision protocols violate basic tenets of decision analysis; and single-sector, partial equilibrium analysis is employed despite the presence of substantial distortions (many due to imperfect competition) in many sectors of the economy. This article elaborates these deficiencies, offers preliminary analysis of how they can best be addressed, and identifies priorities for further research.","The regulation of horizontal mergers is a central pillar of modern competition regulation throughout the world. The subject is now receiving increased attention due to concerns about rising concentration and market power in both traditional sectors and regarding newer technologies.====The good news is that modern merger policy has substantially converged across jurisdictions, reflected in the similarity of merger guidelines in the European Union, United States, and much of the developed world.==== Moreover, horizontal merger analysis is, relatively speaking, a state-of-the-art applied field in industrial organization economics, building on longstanding as well as increasingly sophisticated models of competitive interaction, empirical methods to estimate demand and costs that are used for merger simulation, and empirical analysis of mergers that focuses on particular industries (such as health care and airlines) and includes retrospective analyses of previous mergers.====Unfortunately, there is also bad news. It is well appreciated that market definition is a shaky enterprise, predictions of mergers’ likely anticompetitive effects are of uncertain accuracy, and efficiencies have been relatively neglected even though their perceived widespread presence is deemed to justify permitting most horizontal mergers.====Much less explored are a range of foundational problems with merger analysis. Standard merger protocols conflict with basic precepts of decision analysis and do little to leverage merging parties’ rationality constraint. Various proxies, screens, and shortcuts have never been validated and, on inspection, are deeply problematic. In addition, they do not align with long-run effects that are consequential for welfare and ignore the relevance of first-order distortions in untargeted sectors. Market definition’s flaws are much deeper than appreciated, and the standard approach—whether using merger guidelines’ hypothetical monopolist test (HMT) and resulting HHIs and ΔHHIs or otherwise—can never help and often substantially undermines the analysis of anticompetitive effects in every core application. (In an illustration to be offered, the U.S. Horizontal Merger Guidelines’ (2010) formulation presumptively allows a merger to monopoly that raises price more than thirty-five times as much as another merger in the same setting that is presumptively challenged.) Efficiencies must be merger specific to be credited—which often means that they cannot be achieved by contract—yet standard analysis does not draw on the theory of the firm or organizational economics, fields associated with a number of Nobel prizes. Analysis of ex post entry often engages in misconceived positive analysis and is also normatively problematic because, in many settings, the triggering of entry makes a merger worse for social welfare. Ex ante entry—induced by the prospect of subsequent buyouts—is only recently receiving attention, and its implications in some settings are often ignored or misanalyzed.====A substantial, across-the-board reassessment of horizontal merger analysis is in order. This article is part of a larger project—an in-process book and a series of articles—that begins this task.==== The focus here is on identifying many of the challenges, presenting preliminary analysis, and providing direction for future research. Two principles should be kept in mind. First, we must ask hard questions even if we cannot answer them well now or anytime soon. Second, the proxies, screens, presumptions, and shortcuts that competition agencies and tribunals inevitably must employ to assess particular mergers should be grounded in the proper analysis, not posited a priori.====Section 2 begins by stating the framework for decision-making that, although familiar, conflicts with many aspects of official merger assessment protocols, notably the sequentially siloed analysis of anticompetitive effects and efficiencies. Proper analysis keeps in mind the joint distribution of potential mergers’ effects as well as how the conditional distribution is influenced by merging parties’ rationality constraint and by evidence bearing directly on only one type of effect. Regarding objectives, many jurisdictions’ guidelines focus on consumer welfare, often motivated by distributive concerns. However, taxes and transfers are a dominant strategy for redistribution, one advocated by economists in other regulatory settings. Furthermore, in a long-run (even if imperfectly competitive) equilibrium, all costs are variable and prices with free entry equal average cost, so there is no (ex ante, risk-adjusted) producer surplus and thus no divergence between these two criteria. Considered last is the important second-best problem posed by the significant distortions in many sectors of the economy, rendering incomplete and in some cases highly misleading usual prescriptions grounded in standard, partial equilibrium analysis of mergers’ effects.====Section 3 analyzes price effects and market definition. Merger guidelines, court decisions, and other legal edicts often require market definition and, moreover, purport to draw inferences from market shares, but only shares in the so-called relevant market. However, it is impossible to conclude that one market definition is superior to another without already having a best estimate of relevant effects, rendering the entire process circular. Actually, standard methods are worse because they ultimately have to draw mysterious inferences from resulting market shares rather than using our best estimate. These and related deficiencies are developed schematically and more formally (with regard to any market definition and market share inference process). Finally, merger guidelines’ HMT and use of HHI and ΔHHI are examined in each of the standard applications (unilateral effects with homogeneous goods and with differentiated products, and coordinated effects). Not surprisingly, this algorithm and these familiar summary measures are substantially inapt and misleading in each setting, reflecting that they are disconnected from (and in conflict with) basic models for merger analysis.====Section 4 assesses efficiencies that, on one hand, are often regarded to be sufficiently ubiquitous to justify permitting most horizontal mergers but, on the other hand, are little analyzed and sometimes said to rarely tip the balance in favor of a merger. No plausible joint distribution of anticompetitive effects and efficiencies can rationalize such a state of affairs. Analysis of efficiencies—merger specificity in particular—is woefully underdeveloped: its relevance to anticompetitive effects is often overlooked, and relevant theory (notably, theory of the firm and organizational economics)—bearing on what a merger (a unified firm) can accomplish that a contract cannot—has not been applied to horizontal merger efficiencies. Examination of leading potential efficiencies—economies of scale and economies of scope—casts them in a new light. Last, the emerging “efficiency credit” account for existing practice is shown to be problematic despite the difficulties of scrutinizing efficiencies in particular proposed mergers.====Section 5 examines entry, beginning with the usual focus on entry that may be induced by the anticompetitive effects of a merger. It explains how the usual focus on the likelihood, speed, and magnitude of entry that will occur is misplaced: such entry will not typically be sufficient to eliminate a merger’s anticompetitive effects, and it is often associated with its own inefficiency. Instead, the ease of postmerger entry favors permitting mergers because it bears on inferences, via the merging parties’ rationality constraint, about the anticompetitive effects that would arise in the first place as well as merger efficiencies. The unduly neglected subject of ex ante entry—that induced by the prospect of subsequent buyout—is examined next. In some cases, easier entry can favor tougher merger policy because such entry is inefficient. More broadly, ex ante incentives for entry and other investment are importantly influenced by the anticipated permissibility of the merger regime. Recent attention to acquisitions that may thwart the disruption of nascent competitors is warranted, but discussions often improperly take the presence of such targets as given rather than as being endogenous to merger policy.====Section 6 offers concluding remarks, including important reservations regarding policy implications in light of limits to existing empirical knowledge and institutional features of different competition regimes.====Before proceeding, an important caveat is in order. Regarding some topics—particularly the decision framework and market definition—it is not clear the extent to which actual competition agency practice adheres to official protocols. My own discussions with relevant personnel have yielded conflicting accounts but on the whole indicate deviations in the directions suggested here. Nevertheless, it is important to develop the proper methods formally and explicitly as well as to bring them into the open, which should enhance transparency and accountability, assessments by agencies and courts, and the formulation of research agendas and policy.",Horizontal merger analysis,https://www.sciencedirect.com/science/article/pii/S0167718721000072,3 February 2021,2021,Research Article,132.0
"Miklós-Thal Jeanine,Shaffer Greg","Simon Business School, University of Rochester, United States","Received 1 December 2020, Revised 20 January 2021, Accepted 21 January 2021, Available online 2 February 2021, Version of Record 26 November 2021.",https://doi.org/10.1016/j.ijindorg.2021.102713,Cited by (4)," likely that price discrimination raises aggregate output. For linear demand functions, we establish necessary and sufficient conditions under which the output effect changes sign when input costs are endogenized.","Whether third-degree price discrimination raises or harms social welfare is a question of long-standing interest in economics. A critical ingredient to answering this question is the impact of price discrimination on aggregate output. This is because for a given level of aggregate output, price discrimination implies a misallocation of consumption relative to uniform pricing, by shifting output from high-value to low-value uses. An increase in aggregate output can offset this negative consumption allocation effect and make price discrimination welfare-improving.==== Not surprisingly, therefore, much of the literature’s attention since Pigou (1920) and Robinson’s (1933) seminal works has focused on deriving conditions under which price discrimination raises or lowers aggregate output.====Pigou (1920) and Robinson (1933) found that price discrimination by a monopolistic seller keeps aggregate output unchanged when demand curves are linear and all markets are served at the uniform price. Subsequent work has shown that the sign of the output effect depends on the relative curvatures of the demand curves and, in the case of competing sellers, the relative intensities of competition in the different markets. Aguirre et al. (2010), for example, have shown that third-degree price discrimination by a monopolistic seller tends to raise aggregate output if demand is more convex in the weak market (where price falls) than in the strong market (where price rises).==== In the context of symmetric oligopoly, Holmes (1989), Aguirre (2019), and others, have shown that, all else equal, aggregate output is more likely to increase under third-degree price discrimination if competition is more intense in the strong market than in the weak market.====Nearly all of this literature treats the input costs and other input supply terms faced by the firms that engage in price discrimination as fixed.==== In many cases, however, firms that practice price discrimination in final-goods markets procure inputs from upstream firms that possess some market power. These upstream firms will typically find it optimal to adjust their supply terms, including the marginal input prices charged to downstream firms, depending on the downstream pricing regime (price discrimination versus uniform pricing). This matters for the effects of price discrimination, because if marginal input prices are adjusted upward under price discrimination relative to uniform pricing, downstream prices will adjust upward as well and any aggregate output gain from price discrimination will be lessened, or turn into an output loss. The opposite holds if marginal input prices are adjusted downward.====This paper takes a first step toward understanding how aggregate output is affected when the input costs of the firms that practice price discrimination are determined endogenously by an upstream supplier with market power. We find that endogenizing input costs can affect both the magnitude and the sign of the output effect of price discrimination. Taking these changes into account can even lead to a reversal of some of the literature’s long-standing qualitative conclusions. Instead of output being ==== likely to increase under third-degree price discrimination if competition is more intense in the strong market than in the weak market, as in Holmes (1989), for example, we find that the opposite may hold: output may be ==== likely to increase if competition is more intense in the strong market than in the weak market.====Assuming linear demands, and focusing on the case of a monopolistic supplier offering two-part tariffs to symmetrically differentiated downstream firms that compete in prices (essentially the set-up in Holmes (1989), but with an upstream firm that sells an essential input), we derive necessary and sufficient conditions that determine whether the output effect changes sign when supply tariffs are endogenized. These conditions involve the diversion ratios across competing goods and the monopoly prices (i.e., the prices that maximize the joint profits of the supplier and downstream firms) in the strong and the weak market.====Specifically, we find that the output effect changes sign if and only if either (i) the diversion ratio is higher (competition is fiercer) in the strong market than in the weak market, and the monopoly price in the weak market does not exceed the monopoly price in the strong market by too much,==== or (ii) the diversion ratio is lower (competition is less fierce) in the strong market than in the weak market, and the monopoly price in the weak market is sufficiently less than the monopoly price in the strong market. In the first instance, aggregate output increases with discrimination under a fixed wholesale price, but it decreases when the supply tariff is determined endogeneously. In the second instance, aggregate output decreases with discrimination under a fixed wholesale price, but it increases when the supply tariff is determined endogenously.====Our paper is related to three strands of the literature on price discrimination. First, as discussed, our paper extends the vast literature on the effects of third-degree price discrimination in final-goods markets by incorporating a strategic upstream supplier.====Second, our paper relates to the literature on price discrimination in intermediate-goods markets (see, e.g., Katz, 1987, DeGraba, 1990, Yoshida, 2000, Inderst, Valletti, 2009, and O’Brien, 2014), which analyzes the effects of allowing a supplier to discriminate across different downstream firms.==== What distinguishes our paper from this literature is that, although we also consider a vertical industry structure, we focus on the effects of price discrimination in the final-goods market. Discrimination by the supplier across downstream firms plays no role in our model.====Third, the analysis in this paper is related to our work on input price discrimination by resale market in Miklós-Thal and Shaffer (2019). In that paper, we consider a setting in which a supplier sells to competing multi-market downstream firms that practice third-degree price discrimination, and we analyze the effects of allowing the supplier to set different wholesale prices for the same input depending on ==== it is resold by the downstream firms (e.g., online versus off-line, or in different geographic locations). In contrast, in the present paper, the supplier cannot discriminate by resale market, and we analyze the output effect of allowing the multi-market downstream firms to move from uniform pricing to third-degree price discrimination.",Third-degree price discrimination in oligopoly with endogenous input costs,https://www.sciencedirect.com/science/article/pii/S0167718721000060,2 February 2021,2021,Research Article,133.0
"Maican Florin,Orth Matilda","University of Gothenburg, CEPR, and Research Institute of Industrial Economics (IFN) Sweden,Research Institute of Industrial Economics (IFN), Box 55665, SE-102 15, Stockholm, Sweden","Received 11 July 2020, Revised 30 December 2020, Accepted 6 January 2021, Available online 19 January 2021, Version of Record 2 February 2021.",https://doi.org/10.1016/j.ijindorg.2021.102710,Cited by (4),"This paper studies the determinants of economies of scope and quantifies their impact on the extensive and intensive product margins in retail. We use a framework based on a multiproduct technology to model stores’ incentives to expand product variety. Using novel Swedish data on product categories and stores, we find that high-productivity stores offer more product categories and sell more products of all categories. Stores with high-demand shocks specialize in fewer product categories and sell more products of top-selling categories. Policy simulations of regional programs that target the determinants of economies of scope show that investment subsidies and mentoring support for low-productivity stores increase the number of product categories and sales per product category, especially benefiting stores in rural markets.","Services and retail businesses account for a rapidly growing share of economic activity. In recent years, ample investments have been made in new technologies such as mobile payment systems, and a drastic increase in warehouse clubs and a shift in consumer preferences from products to services have occured (Hortacsu, Syverson, 2015, Goolsbee, 2020). These structural changes require retailers to improve their businesses of buying multiple products from wholesalers and efficiently delivering such products to consumers while assuring quality. Buildings, equipment and supply chain facilities yield economies of scope that make it cheaper to sell many products together than to sell them separately (Panzar and Willig, 1981). Despite massive changes in the retail landscape, we lack knowledge of the determinants of economies of scope and their impact on the number of product categories (the extensive margin) and sales per product category (the intensive margin).====This paper studies the determinants of economies of scale and scope in retail using a framework that models stores’ multiproduct sales technology and the local market environment. We explore the trade-off between productivity and demand shocks for variety of products offered. While technology helps stores improve productivity and handle greater product variety, they can choose to offer less variety and allocate resources to provide convenience to customers, e.g., by improving service and shopping experience. We estimate the model using novel and detailed data on product categories and stores in Swedish retail between 2003 and 2009. Then, we evaluate how subsidies for investments in technology and mentoring support to foster productivity drive the number of product categories, sales per product category, store-level sales and market shares. The analysis explores differences between rural and urban markets that are of interest to policymakers in light of regional development programs including investment subsidies and mentoring support (Nordregio, 2011).====Descriptive patterns in the data motivate our research framework. Stylized facts show that stores frequently adjust their product categories. We measure product variety by the number of product categories a store offers for sale.==== Stores with high market shares have high labor productivity, offer many product categories, and sell more per product category. Our data also suggest that it is important to explore heterogeneity across local markets and dynamic patterns over time, as indicated by the increase in the median market share, the four-firm concentration ratio, and the Herfindahl index (HHI). This descriptive evidence is consistent with the idea that stores utilize economies of scale and scope and productivity improvements to offer a wider variety of products. Accordingly, our framework explicitly models complementarities between economies of scale and scope in a local market setting.====We provide a novel approach based on a multiproduct technology to understand economies of scope and the role of adjustment of store inputs in altering product variety.==== Our framework allows quantifying the increase in store-level sales resulting from offering product categories using the same resources and how the sales of a product category are affected by increasing sales of other product categories in the store. The gains from selling a larger variety of products arise from lower average costs or from increasing sales in new related product markets. Adjustments in product categories occur because retailers change their inputs or target a better match with local market demand. How many product categories to offer and how much to sell of each category are open empirical questions that depend on store resources and local demand conditions. Our framework is appealing for evaluating regional policy programs related to economies of scope and for exploring differences between rural and urban markets.====Our model highlights mechanisms through which productivity and demand shocks drive intensive and extensive product-category margins. We use the implications of the equilibrium behavior of the store’s dynamic optimization problem to recover two sources of store-level heterogeneity, i.e., revenue productivity and demand shocks, which are both observed by stores but not by the researcher.==== Revenue productivity follows an endogenous stochastic process, whereas demand shocks follow an exogenous process. Our measure of demand shocks includes features related to product quality, location, checkout speed, the courteousness of store employees, parking, bagging services, and cleanliness. To recover revenue productivity and demand shocks, we rely on two output equations – involving product-category sales and market share index functions – and store’s demand functions for labor and inventory, accounting for investment in technology, product variety and the local environment in which a store operates (Doraszelski, Jaumandreu, 2013, Kumar, Zhang, 2018, Maican, Orth, 2020).==== Market shares contain information about demand shocks, and rich sales data for the universe of stores allow us to use local market shares together with demand for inventory to recover store-specific demand shocks. It is important for identification that the sales equation depends on both productivity and demand shocks, whereas the market share index function depends on only demand shocks (Ackerberg et al., 2007). We allow stores to learn from demand, i.e., demand shocks provide information used by stores to improve their future productivity.==== This mechanism of learning about demand has not yet received much attention in the structural productivity literature, and can be used to evaluate mentoring support policies provided by regional development programs in retail.====This paper contributes to the recent literature on development in services and retail industries. The analysis focuses on the supply side to investigate the determinants of economies of scope and evaluate policy programs in rural and urban markets. We model the role of technology, inputs and the dynamic nature of product variety and store primitives.==== The proposed framework provides a tractable way of analyzing economies of scope at the firm/establishment level using census data combined with data on product categories and sales per category. Our framework is applicable to any industry where many firms operate and offer a wide range of products for which data on price and detailed product characteristics are unavailable. In a rare case that data on product-level prices are available and can be linked to census data on services firms, our framework can be integrated with a more general demand framework that allows for rich substitution patterns between products.==== While we do not consider a dynamic game, a store’s market share is affected not only by its own product variety choice but also by the product variety choices of other stores in the local market.====This paper also contributes to the literature that emphasizes the role of technology and demand in understanding firm performance, which mainly focuses on manufacturing (e.g., Olley, Pakes, 1996, Foster, Haltiwanger, Syverson, 2008, Collard-Wexler, 2013, Asker, Collard-Wexler, De Loecker, 2014, Collard-Wexler, De Loecker, 2015).==== We highlight the trade-off between productivity and demand shocks for key performance indicators such as sales per product category, store-level sales and market shares in rural and urban markets. In particular, we contribute to the literature that uses the implications of equilibrium behavior for firms’ decisions on inputs to estimate productivity (Olley and Pakes, 1996).==== Most of the literature on productivity estimation considers single-output technology, which renders inference for multiproduct technology questionable (Bailey and Friedlaend, 1982). We explicitly model a multiproduct technology function with known theoretical micro foundations for multiproduct production and profit maximization (e.g., Mundlak, 1964, Fuss, McFadden, 1978). Because our multiproduct approach uses inputs at the firm/establishment level, identification and estimation are based on the well-established two-step methods in the production function literature (see the survey Ackerberg et al., 2007). The analysis, applying our approach to data on product categories and stores, is linked to a recent strand of research on understanding the productivity of multiproduct firms in manufacturing (e.g., De Loecker, Goldberg, Khandelwal, Pavcnik, 2016, Dhyne, Petrin, Smeets, Warzynski, 2017) and a companion paper on entry regulations in retail (Maican and Orth, 2020).====The results show clear evidence that productivity improvements expand the intensive and extensive product-category margins. Stores sell more product categories and increase their sales, especially among bottom-selling product categories. Higher demand shocks, on the other hand, shrink the extensive product-category margin and encourage specialization. Stores with high-demand shocks thus focus on fewer product categories and sell more of their top-selling categories. Together, higher productivity and demand shocks increase store-level sales and market shares. We use the estimated model to quantify gains from improving economies of scope, which affect store sales. As a result, we observe that the increase in store median sales is two percentage points higher in rural than in urban markets if economies of scope improve by fifteen percent for all stores.====Counterfactual experiments examine regional policy programs in Sweden (Nordregio, 2011, SCB, 2015). Focusing on differences between rural and urban markets, we evaluate the impact of an investment subsidy through a thirty percent upward stock of technology and mentoring support that improves learning from demand to foster productivity. Implementing the policy on low-productivity stores shows that the number of product categories (the extensive margin) increases more in rural than in urban markets. Sales per category (the intensive margin) and store-level sales also increase, with magnitudes being larger for stores in rural markets. Store-level sales increase six percentage points more in rural than in urban markets. The corresponding difference for sales per category is four percentage points. The larger effects in rural markets are driven by more pronounced productivity improvements among stores there than in urban markets. Our results suggest that a policy that targets low-productivity stores will reduce the gap between rural and urban markets.====The next section introduces the Swedish retail industry and presents the data. Section 3 describes the model and discusses the identification and estimation. Section 4 presents the empirical results (Section 4.1) and shows the findings of policy experiments using the estimated model (Section 4.2). Section 5 describes robustness checks, and Section 6 summarizes the paper and draws conclusions.",Determinants of economies of scope in retail,https://www.sciencedirect.com/science/article/pii/S0167718721000035,19 January 2021,2021,Research Article,134.0
"Lampe Ryan,McRae Shaun","Department of Economics, California State University, East Bay, United States,Centro de Investigación Económica and Department of Economics, ITAM, Mexico","Received 17 July 2018, Revised 25 November 2020, Accepted 4 January 2021, Available online 14 January 2021, Version of Record 9 February 2021.",https://doi.org/10.1016/j.ijindorg.2021.102708,Cited by (1),"This paper studies the effect of self-regulation on the leniency of cinema age restrictions using cross-country variation in the classifications applied to 1922 movies released in 31 countries between 2002 and 2011. Our data show that restrictive classifications reduce box office revenues, particularly for movies with wide box office appeal. These data also show that self-regulated ratings agencies display greater leniency than state-regulated agencies when classifying movies with wide appeal. However, consistent with theoretical models of self-regulation, the degree of leniency is small because it is not costly for governments to intervene and regulate ratings themselves.","Self-regulation, in which an industry-level organization determines and enforces the rules and standards of conduct for firms in the industry, is an important feature of many sectors including advertising, financial services, legal services, and industrial chemicals. Potential benefits of self-regulation over government-level regulation include greater speed, flexibility, and lower costs (Gunningham and Rees, 1997). However, since self-regulatory organizations are formed to benefit their members, critics argue they may have inadequate incentives to protect the public (e.g., DeMarzo et al., 2005). For example, the Securities and Exchange Commission recently questioned whether U.S. exchanges should be responsible for monitoring and policing their own trading activities. As self-regulatory organizations, exchanges write market rules and are shielded from legal challenges from traders that lose money due to technical problems, including those from the contentious initial public offering of Facebook (Ackerman et al., 2013).====Self-regulation has been extensively studied in (Maxwell et al., 2000; Núñez, 2001; DeMarzo et al., 2005; Grajzl and Murrell 2007). But despite the prevalence and importance of self-regulation, few empirical analyses have assessed the performance of self-regulatory organizations, and existing analyses have been limited to case studies without cross-sectional variation in the source of regulation. In financial markets, historical accounts show that self-regulation of commodity exchanges between 1865 and 1922 was not an efficient means to reduce the exercise of monopoly power (Pirrong, 1995). For the chemical industry, studies of the Responsible Care program of 1989 find that membership in the program was a poor predictor of whether prescribed environmental standards were followed (Howard et al., 2000), and member firms did not improve their environmental, health and safety performance as fast as non-members (King and Lenox, 2000). However, empirical evidence indicates that the threat of government regulation induced U.S. firms to voluntarily reduce emissions from 17 toxic chemicals between 1988 and 1992 (Maxwell et al., 2000).====In this paper, we analyze the performance of self-regulation using the example of motion picture rating systems.==== Motion picture rating systems are designed to protect children from unsuitable material by issuing certifications that classify a movie’s violent and sexual content, and, in some cases, impose restrictions that require theater owners to refuse entry to minors. The existence of rating systems is motivated by a purported link between media content and the emotional and behavioral development of children. In 2000, the American Medical Association and five public-health organizations noted that research points “overwhelmingly to a causal connection between media violence and aggressive behavior in some children” (Anderson et al., 2003).==== Research into the effects of media violence also finds that exposure to media violence leads children to become desensitized to violence, and to believe the world around them is cruel (Robinson et al., 2001). Related research into the effects of sexual content suggests that depictions of sex in the media promote a belief that promiscuity is the norm (Zillmann, 2000; Escobar-Chaves et al., 2005).====The film industry is an appealing setting for studying self-regulation since rating systems are prevalent across countries and there is variation in the type of regulation. In the United States, movie ratings are issued by the motion picture industry’s trade association, the Motion Picture Association of America (MPAA). Its most common ratings are PG-13 and R. Movies rated R are restricted to audiences 17 years and over unless they are accompanied by a parent or legal guardian while movies rated PG-13 are recommended for ages 13 and over but impose no restrictions. Ratings in five other countries (Germany, Iceland, Japan, the Netherlands, and United Kingdom) are also issued by agencies created and administered by the film industry. In the majority of other countries, ratings for the same movies are issued by state-regulated agencies. In Australia, for example, certifications are issued by the Australian Classification Board, formed by the Australian government in 1970.====To assess the performance of self-regulated movie rating systems, we have assembled an extensive data set of 1922 movies released between 2002 and 2011 across 31 countries. Our data include classifications, box office revenue and admissions, and measures of violence, sex, and profanity for 29,004 movie-country pairs. Since we observe the same movie in multiple countries, we can include movie fixed effects to control for a wide range of privately observed movie characteristics that are correlated with ratings but are unavailable in our data.====Using these data, we first establish that restrictive ratings reduce box office revenues. A one-year reduction in the minimum age required for entry is estimated to increase box office revenue by 4.6 percent on average. These findings are robust to alternative controls for violent and sexual content, edits made across countries, alternative measures of a movie’s availability to minors, and to using attendance instead of revenues to measure box office performance. We also find that implied revenue losses are greatest for big-budget films targeted at wide audiences and for films targeted at teenage audiences. For example, coefficient estimates indicate that the average revenue loss from a restrictive classification in the U.S. is $19.8 million for a high-budget movie and $6.9 million for a low-budget movie.====We next review theoretical models of self-regulation to construct testable predictions about the behavior of self-regulated ratings agencies. Having established that restrictive ratings reduce box office revenues, particularly for movies appealing to wide and teenage audiences, two implications immediately follow. First, classifications issued by self-regulated ratings agencies will be more lenient than state-regulated agencies. Second, ratings will be particularly lenient on movies with larger box office potential and on movies targeted at teenage audiences. A third prediction is that the degree of leniency will be small. This is because the taxes required to fund a state classification system are not large, which makes the threat of government action credible.====To test these predictions, we first examine a 2006 regulatory change in Iceland. Difference-in-differences estimates reveal that Iceland’s change from state- to self-regulation increased the leniency of classifications by 4.7 months on average. We then examine cross-country differences in the relative leniency of classification decisions issued by self- and state-regulated agencies. Estimates reveal the difference in leniency between wide and narrow appeal movies is 3.1 months larger on average in countries with self-regulated ratings agencies. The corresponding difference in leniency between movies targeted at teenagers and those not targeted at teenagers is 1.9 months. The direction and magnitude of leniency exhibited by self-regulated agencies therefore provides strong support for our predictions.====The remainder of this paper is organized as follows. Section 2 describes the regulation of rating systems around the world. Section 3 describes our data on movie ratings, revenue, and violent and sexual content. Section 4 examines the impact of restrictive ratings on box office performance. In Section 5, we review theoretical models of self-regulation and develop testable predictions. In Section 6, we compare the leniency of movies ratings issued by self- and state-regulated agencies to test these predictions. A final section concludes.",Self-regulation vs state regulation: Evidence from cinema age restrictions,https://www.sciencedirect.com/science/article/pii/S0167718721000011,14 January 2021,2021,Research Article,135.0
"Foucart Renaud,Friedrichsen Jana","Lancaster University Management School, United Kingdom,Humboldt-Universität zu Berlin, Germany,WZB Berlin Social Science Center, Germany,DIW Berlin, Germany","Received 26 March 2020, Revised 18 December 2020, Accepted 3 January 2021, Available online 12 January 2021, Version of Record 23 January 2021.",https://doi.org/10.1016/j.ijindorg.2021.102709,Cited by (0),"We study a game in which two firms compete in quality to serve a market consisting of consumers with different initial consideration sets. If both firms invest below a certain threshold, they only compete for those consumers already aware of their existence. Above this threshold, a firm is visible to all and the highest investment attracts all consumers. On the one hand, the existence of initially captive consumers introduces an anti-competitive element: holding fixed the behavior of its rival, a firm with a larger captive segment enjoys a higher payoff from not investing at all. On the other hand, the fact that a firm’s initially captive consumers can still be attracted by very high quality introduces a pro-competitive element: a high investment becomes more profitable for the underdog when the captive segment of the dominant firm increases. The share of initially captive consumers therefore has a non-monotonic effect on the investment levels of both firms and on ====. We relate our findings to competition cases in digital markets.","Consumers typically differ in the set of firms that they consider before making a purchasing decision and firms are not able to easily change a consumer’s consideration set. When a product provides particularly high utility, however, most consumers become aware of its existence, be it through word-of-mouth recommendation, social networks or news reports. The firm offering this product is then considered by all consumers. In this paper, we study how the co-existence of initially captive market segments and of the possibility of reaching all consumers by providing exceptionally high utility affects competition.====We model this problem as a duopoly game in which firms simultaneously invest in their product to increase consumer utility. A share of the consumers considers both firms, while the other consumers are “captive” in the sense that they initially consider only a single firm. Captive consumers may, however, cease to be so if a product outside their initial consideration set provides a level of utility exceeding a certain threshold. A standard argument is that the existence of captive market segments is anti-competitive. This is partially true in our model: as there exists a contested consumer segment that is not captive to either firm, firms may indeed decide to compete only for those. Which thereby induces a positive reservation value from not investing at all, and decreases the incentives for a dominant firm to compete for the rest of the market. The presence of captive consumers however also introduces a pro-competitive element: The only way to become visible to the captive base of the competitor is to invest much more in one’s product in order to provide ==== high utility.====We show that a higher share of the consumers initially considering one firm only may then actually induce firms to invest more and lead to higher consumer surplus. This is more likely to be the case if the share of captive consumers of the dominant firm is already high. In contrast, if a firm with a given captive segment manages to increase the threshold at which captive consumers become aware of the competing firm’s offer, consumer welfare decreases unambiguously. These results relate to prominent competition cases in digital markets.====In 2013, Microsoft was fined by the European Commission for failing to make Windows users aware of competing web browsers despite being committed to do so since a 2009 settlement.==== We look at the case through the lens of our model: Microsoft uses the dominant position of its operating system Windows to increase its share of captive consumers in the market for internet browsers by locking Windows users into the use of Microsoft’s own browser Internet Explorer. Our theoretical analysis predicts that, in the presence of a competitor with a sufficiently high investment capacity—the actual competitor Google supposedly had this capacity—such an increase in the captive segment of the market leader may have actually led to higher incentives for the runner-up, here Google, to improve its own competing product. The intuition is that Google would know that being marginally better than Internet Explorer was not sufficient to attract Windows users. It also needed to be sufficiently good for Microsoft consumers to become aware of its existence. This prediction of our model is consistent with the fact that Google’s browser Chrome actually overtook Microsoft Internet Explorer as the market leader before the 2013 ruling, despite Windows still enjoying more than 80% market share at the time, and before mobile phones became a major source of Internet browsing.====In June 2017, Google—having further expanded its usership and the variety of its services—was fined for using its dominance as a search engine to demote comparison shopping services in search results that were competing with its own service.==== At the source of the complaint was a website called Foundem arguing that Google showed its users the comparison website Froogle (now Google shopping) only, hiding the competitors. In our model, the behavior of Google corresponds to a firm that purposefully makes it more difficult for already captive consumers to learn about a possible alternative product. Moreover, in this case the competitor Foundem did not have sufficient investment capacity to respond to Google’s practice by improving its own product to such an extent that it would have become visible to captive consumers. Both because Foundem was small and lacked resources and because the attention threshold created by Google was very high. These two elements suggest that the behavior of Google was unambiguously anti-competitive.====In 2018, Google was fined again for restricting Android device manufacturers in what they showed to consumers, in particular forcing them to pre-install Google search and Google’s Chrome browser.==== In our model, this is a much more ambiguous case than the previous ones: what Google does is to increase its captive share, a practice that may actually increase competition, in particular if there exists a competitor with sufficiently high investment capacity. The difference with the Windows example from above is that Google does not have such a high dominance with Android as Microsoft did in the market for Operating Systems. According to our model, it is thus not clear whether an increase in the loyal base of Android users would actually increase the investment incentives of competitors.====The question of the economic impact of consumers observing different sets of firms has received much attention in economics. Early models of price dispersion (Varian, 1980, Burdett, Judd, 1983) study consumers that are heterogeneous in the number of firms they observe. In Bulow and Klemperer (1998) (Appendix C), two firms compete for consumers, that can either be fully captive or uninformed. A general characterization of price competition in oligopoly settings with different exogenous consideration sets is provided by Armstrong and Vickers (2018).====The novelty of our approach is that we allow a firm to directly enter the consideration set of all consumers by providing a sufficiently high quality product. Among the more recent models using such a dichotomy between informed and uninformed buyers, the one by De Cornière and Taylor (2019) is of particular interest to us as the authors characterize the competitive impact of a dominant search engine guiding consumers towards a specific product. Our finding that captive consumers can increase competition resonates with their finding that biased intermediaries can improve consumer welfare. In their model, consumers reach one of two competing firms via intermediaries who are said to be biased if they direct consumers only to one firm thereby hiding the competitor. The reason behind their result that an intermediary bias on part of one firm may be beneficial to consumers is the possibility of congruence between firm and consumer payoffs (as opposed to conflicting payoffs). In case of congruence, the per-user profit of a firm increases in the utility it offers to consumers thereby giving the firm an incentive to improve consumer utility and thus welfare. Through the lens of our model, a biased intermediary leads to a share of consumers being captive to one firm with the competitor being unable to reach these consumers. In our paper, payoffs are neither congruent nor conflicting but we allow the competitor to overcome the bias of consumers with a sufficiently high investment and show that this induces a pro-competitive effect that can also increase consumer welfare. Our model thus provides an additional and complementary explanation for why a biased intermediary may benefit consumers.====By construction, our setting is similar to an all-pay auction (Baye et al., 1996) but in contrast to the standard model, the levels of investment endogenously determine the number of and the size of the prizes for which the firms compete when choosing their investments.==== This endogeneity of prizes is driven by the assumption that captive consumers remain captive only for low investments. If at least one of the two firms’ investments exceeds a certain threshold, there is only one prize to be won, but there are two prizes, one for each firm, if both firms choose investments below this threshold. The unique equilibrium of our game is in mixed strategies. The intuition is similar to that of the standard all-pay auction: for every given level of investment of the winning firm, the competitor could win instead by choosing a marginally higher level. A crucial difference is that investments below the threshold can only win a share of the “prize”, not-including the captive segment of the competitor. Depending on the size of the threshold, the equilibrium of the game is one of the following three.====In the limiting case where the utility threshold beyond which previously captive consumers consider a competitor is exactly zero, our model collapses to a symmetric all-pay auction: both firms always compete at high intensity, randomize over the same interval of investments in equilibrium, and make zero profit in expectation. On the other extreme, when the utility threshold is very high, the equilibrium is again symmetric but then no firm ever tries to reach the captive segment of the other: both firms randomize over the same interval of investments in equilibrium and make an expected profit equal to the value of their captive segment.====In the intermediate case, where the utility threshold is not too high but strictly positive, equilibrium bidding strategies are asymmetric. Both firms’ equilibrium strategies contain investments below and above the threshold and, therefore, both firms expect to sometimes keep their captive consumers even when having invested less than the competitor or, at the extreme, when having not invested at all. Hence, each firm makes a strictly positive profit in expectation. The higher is the threshold, the higher is the probability mass that the firms put on investments below the threshold. In consequence, the probability of one firm obtaining a monopoly position decreases. In this intermediate case, the support of the equilibrium mixed strategy exhibits a gap just below the minimum investment necessary to attract the competitor’s captive consumers. Choosing an investment at or just above this threshold does not only increase the probability of winning, but also the prize of winning, which is then the whole population, including all captive consumers, instead of just a share of it.====The starting point of our model is the well-known fact that consumers sometimes are biased in favor of certain options. Consumers may experience switching costs (Klemperer, 1987), or they may inspect competing firms in a certain order while bearing a search cost to observe an additional option (Arbatskaya, 2007, Armstrong, Vickers, Zhou, 2009). Such biases lead to qualitatively similar results. For instance, with consumer switching costs, the firm with a larger base of captive consumers charges a higher (ripoff) price because it relies on the profits to be made on its captive segment. In our model, the firm with the larger segment of captive (attached) consumers invests less in its product in expectation and, therefore, offers a lower utility than the competitor on average. In both cases, the larger segment of captive consumers makes the firm lazy so that it provides less utility to consumers in expectation than the initially disadvantaged competitor with a smaller captive segment.====The observation that the firm with a larger share of captive consumers employs a less aggressive strategy resonates well also with earlier results from the literature on the effects of brand loyalty. For instance, Narasimhan (1984) finds that a firm with a larger loyal customer base is less aggressive in using discounts to attract further consumers. Similarly, Raju et al. (1990) find that a stronger brand uses price promotions less frequently than a weaker one. As in our model, the rationale is that a weaker competitor has more to gain from being aggressive.====We introduce the model in Section 2. Then, we derive important properties of the equilibrium strategies in the simultaneous investment game and characterize the equilibrium in Section 3. We discuss the impact of the different parameters on equilibrium behavior in Section 4. We conclude in Section 5. For those results that do not follow directly from the text, formal proofs are collected in Appendix A.",All-pay competition with captive consumers,https://www.sciencedirect.com/science/article/pii/S0167718721000023,12 January 2021,2021,Research Article,136.0
Wang Jin,"Kansas State University, 246 Waters Hall, Manhattan, KS 66502, United States","Received 19 April 2018, Revised 17 December 2020, Accepted 19 December 2020, Available online 6 January 2021, Version of Record 26 January 2021.",https://doi.org/10.1016/j.ijindorg.2020.102698,Cited by (0),"We propose a model which incorporates both quality and quantity in end-users’ interactions and analyze how platforms can use quality screening to alleviate ==== and motivate end-users’ participations. We address the question from theoretical and empirical perspectives. In the theory, we build a model in which two platforms compete but only one of them screens sellers’ products. We show that the quality screening influences consumers’ expectations of product quality and their choices of sellers and platforms. The resulting screening effect, together with the network and competition effects, further drives sellers to enter different platforms. The equilibrium result indicates that sellers’ incentives to join the platform that screens products follow a non-monotonic relationship with respect to the observable quality of products. We estimate the model in Alibaba’s Platforms - Tmall and Taobao. The results are consistent with the theory. Counterfactual analysis suggests quality screening benefits consumers and Alibaba.","Many firms like eBay do their business by acting as an intermediary, or platform, which enables interactions between two groups of users: sellers and buyers. Through this or other platforms, end-users on the two sides trade products or services and realize their cross-group externality: the utility of users on one side depends on the performance of users on the other side. The platform operator profits by collecting usage fees from end-users on board. In this economy, both end-users’ benefits and the platform’ s profits depend heavily on how well the platform can help users to exploit their cross-group externality. This externality can involve end-users’ interdependence in two dimensions: quality and quantity. For instance, when a consumer visits an online trading platform, she cares not only about the number of sellers available to choose from but also about the quality of products these sellers provide.====Previous studies of two-sided markets have mainly focused on the quantity externality: a user’ s utility is purely determined by the number of users on the other side. Less attention has been paid to end-users’ quality concerns. When users evaluate the benefit of participating in a platform, they often take into account the quality of service and product exchanged in the interaction. If users know they may experience interactions of poor quality, they will likely reduce the usage of the platform. The situation worsens if there is asymmetric information in quality between users on two sides. Following the classical economic theory since Akerlof (1970), asymmetric information can result in end-users’ adverse selections and impede interactions which could have benefited users on both sides. Given the emphasis users place on quality, platform providers have a strong incentive to manage end-users’ quality.====In this article, we propose a model in which both quality and quantity play a role in the end-users’ interactions, and analyze how platforms can use quality screening to alleviate the asymmetric information problem and motivate end-users’ participation. Quality screening has a popular application in two-sided platforms: Prestigious shopping malls carefully select the brands that are going to enter the mall. Many academic journals maintain good reputations and large readership because they have a rigorous refereeing process. In spite of the broad use of quality screening in two-sided markets, there have been few studies to investigate the economic implications of screening for players’ choices and welfare. For instance, how does quality screening affect end-users’ interactions? If there are two platforms that implement different quality screening standards, which platform will end-users choose? Does screening benefit the platform provider? Our study will address these questions.====We study quality screening in the context of online trading platforms serving two types of end-users: sellers and buyers. Within a platform, sellers compete in selling a particular type of products to consumers. The quality of products is heterogeneous and composed of two parts: the quality observable to all players and the private quality signal known only by the seller. To overcome the information asymmetry, the platform can implement quality screening by charging sellers different usage fees depending on their products’ quality. More specifically, the platform collects each seller’ s quality information by randomly sampling products or through consumer feedback. The quality signal which the platform obtains mixes the true product quality plus a noise term. If the value of the signal lies above a threshold, the platform offers the seller a discount on the usage fee. Otherwise, the platform charges the seller a high usage fee which reduces the seller’ s profit to zero. Under this rule, the higher the product quality is, the more likely the seller will enjoy a lower platform usage fee.====To investigate the effect of quality screening on buyers’ and sellers’ entry decisions, we assume two platforms in the model. One platform employs quality screening as described above and we denote it as Platform ====. The other platform is a free-entry market and refers to Platform ====. Both buyers and sellers are single-homing. Our model illustrates that the number of consumers on a platform depends on the total utility provided by the entire seller group on a platform. That is, the platform which gathers more sellers of good quality can seize a larger market share on the consumer side. On the seller side, a merchant’ s platform decision is governed by three effects: screening, network and competition. First, the screening effect means that sellers can convince consumers that their products are of good quality by showing their willingness to pay a usage fee correlated with quality. Entering Platform ==== enables a seller to signal the unobserved quality of the product and increase the chances of sales. The network effect and competition effect characterize two ways that a seller’ s participation decision is influenced by the number of consumers on the platform. On one hand, a merchant wants to enter the platform with a larger number of consumers. By interacting with more consumers, the merchant can sell more products. On the other hand, the number of consumers on a platform is positively correlated with the utility provided by the entire seller group. So for a seller, joining a platform which has more consumers means that it has to face fiercer competition from other sellers. This may reduce the seller’ s market share. The relative magnitude of the network and competition effects determines how a seller responds to the consumers’ participation. If the network effect dominates, a seller is willing to join the platform that attracts more consumers. Otherwise, the seller would rather avoid peer competition by choosing the platform which has fewer consumers.====Under the influence of the screening effect, the proportion of sellers choosing Platform ==== first increases with the observable quality. Once the observable quality reaches a certain point, the network and competition effects take over and may reduce the proportion of sellers on Platform ====. The intuition works as follows: For a merchant that sells a product of low or medium observable quality, entering Platform ==== can signal the product’ s unobserved quality and increase sales. But this may also bring the risk of a high usage fee if the product fails Platform ====’ s quality screening. When the observable quality increases, the product is more likely to meet the standards of Platform ==== and therefore the sellers are more willing to enter this platform. If the product is of sufficiently high observable quality, a seller will pass the quality screening anyway. As a result, the unobserved quality of the product can not be inferred by consumers regardless of the platform the product is located on. So the screening effect vanishes and only the competition and network effects matter in the sellers’ decisions. The seller weighs the comparative magnitude of two effects and may choose Platform ==== if it is more desirable.====To test the theory, we apply the two-sided market model to the empirical study. By doing this, we are able to estimate the end-users’ responses to quality screening and quantify the effect of screening on the market share of platform provider. We select Alibaba because of both its size and business strategy. Firstly, Alibaba is a monster E-commerce firm. It has so far captured over ==== of the online market in China. In 2014, its total sales were 370 billion dollars - more than eBay and Amazon combined. Second, Alibaba operates two online-trading platforms: Taobao and Tmall. Taobao is a free-entry market in which any product can be posted for sale, whereas Tmall sets a quality standard and charges sellers usage fees depending on their quality. Therefore, Taobao and Tmall are exactly the counterparts of Platform ==== and Platform ==== in the theory, and their business models fit the setup of our theory well.====The data is collected from the websites of Taobao and Tmall. It contains information including each seller’s rating score, price, monthly sales and platform (Tmall or Tabao). The variable rating score is treated as the proxy of the seller’ s observable quality. To proceed with the estimation, we show that the two-sided market model can be equally characterized as a game in which sellers simultaneously choose the desired platform, holding incomplete information about other sellers’ quality and decision. As for the equilibrium, we demonstrate that the equilibrium of the two-sided market model is readily transformed into a set of Bayesian Nash equilibrium choice probabilities. These probabilities are the fixed point determined by the mapping from a seller’ s conjecture of competitors’ choices to its competitors’ conjectures of this seller’ s choice of platform. And they are to be estimated together with the consumer’s demand and seller’ s profit function.====The estimation is carried out using the Nested Pseudo Likelihood (NPL) estimator proposed by Aguirregabiria and Mira (2007). On the demand side, the estimate of screening effect varies by seller type. For sellers with low or medium rating scores, the estimator is positive and significant, whereas for sellers with high rating scores, the estimator is almost zero. This supports our theory that in equilibrium the screening effect only takes effect among sellers of low and medium observable quality. These types of sellers can convince consumers of the product quality and make more sales if they submit to the quality screening of Tmall. But for the sellers that have already gained good reputation (a high rating score in this case), the screening effect disappears. The estimation also shows that the network effect dominates the competition effect. It implies that, holding other conditions as constant, a seller is more willing to pool the products with those of sellers with better quality so as to attract more consumers. As for seller profit function, the comparative profit of joining Tmall is estimated to be an increasing function of seller’s rating score and unobservable quality. This is consistent with the spirit of quality screening: sellers of better quality are more likely to meet the quality standards and pay less usage fee. We also find in the data, as the rating score increases, the share of Tmall sellers first increases then decreases. The trend coincides with the predictions of our theory: the number of sellers participating in Platform ==== can have a non-monotonic relationship with the observable quality.====Last, we conduct two counterfactual experiments to investigate whether the strategy of quality screening improves consumers’ welfare and Alibaba’s business. We calculate the consumer’ s total expected utility if Alibaba would remove quality screening or loosen the quality standard of Tmall, and compare the results with the true scenario. We find that the total utility provided by the entire seller group gets increased in the presence of quality screening. And by employing quality screening, Alibaba improves its market share.==== This article contributes to the theoretical studies of two-sided markets and network externalities (Katz, Shapiro, 1985, Katz, Shapiro, 1986, Laffont, Rey, Tirole, 1998, Laffont, Rey, Tirole, 1998, Rochet, Tirole, 2002, Rochet, Tirole, 2006, Caillaud, Jullien, 2003, Armstrong, 2006, Weyl, 2010). The two-sided markets are featured by interactions between two groups of agents with cross-group externalities. To make profits, platforms need to solve the so-called “Chicken-and-Egg” problem and “get both sides on board” (Caillaud, Jullien, 2003, Rochet, Tirole, 2003, Rysman, 2009). That is, when platforms design their business strategies, they should consider the responses of agents on both sides. The existing literature studies two-sided markets from various perspectives, including the pricing schemes and price allocation on both sides (Rochet, Tirole, 2003, Rochet, Tirole, 2006, Armstrong, 2006), platforms’ price commitments (Hagiu, 2006), price discrimination and bundling (Damiano, Li, 2007, Chao, Derdenger, 2013), exclusive contracts (Armstrong and Wright, 2007), platform or end-users coordination (Rochet, Tirole, 2002, Ambrus, Argenziano, 2004) and so on. These studies, although addressing different questions of two-sided markets, usually focus on the quantity externality: a user’ utility depends only on the number of users on the opposite side. There are some work which allow end-users to have heterogeneous benefits from interactions, but the heterogeneity in their models is assumed to be pre-determined and does not change with platform’ s decisions. Little attention has been paid to the importance of quality in end-users’ interactions and the possibilities that platform can influence the interactions through quality screening. Jeon and Rochet (2010) is one of few studies which uses a two-sided market framework to analyze the quality standard decision of a monopoly academic journal and how the decision changes with open access on the reader side.==== Our work differs from Jeon and Rochet (2010) in twofold: first, we focus on asymmetric information problem and study how a platform can use quality screening to alleviate this problem and promote end-users’ interactions and the platform’ s market share. Second, in the model setup, we consider a duopoly platform competition and analyze the equilibrium participation decisions of consumers and sellers over the two platforms.====This article also joins the empirical literature of two-sided markets. Rysman (2004) identifies the positive network effect between advertisement and consumer usage in the market for Yellow Pages and examines the welfare trade-off between monopoly and competitive market structure. Similar network effects are also shown to exist in payment card usage and ACH electronic payment adoption (Rysman, 2007, Ackerberg, Gowrisankaran, 2006). As for the pricing strategies of two-sided markets, Kaiser and Wright (2006) estimate a model about the pricing decisions of competitive German magazine publishers. Argentesi and Filistrucchi (2007) explore the market power in Italian newspaper industry. Chandra and Collard-Wexler (2009) and Fan (2013) study the effect of mergers on prices and other product characteristics in newspaper industry. There are also some studies about the exclusive contract of this economy. Lee (2013) develops a dynamic model to study the interactions of consumers, hardware and software providers in video game industry and evaluates the welfare implication if the integration and exclusive contract are prohibited. Derdenger (2014) studies the impact of technologically tying on console pricing and the incentives of tying. In the second part of this study, we structurally estimate the interactions between consumers and sellers in Alibaba online markets. The empirical work, on one hand, identifies the screening, network and competition effects associated with the choices of end users in the presence of quality screening and platform competition, and on the other hand, provides supports to the predictions of the theoretical model.====Our study is also closely related to the literature of the estimation of discrete choice games and its applications. This stream of literature starts from the seminal work by Bresnahan, Reiss, 1990, Bresnahan, Reiss, 1991 and Berry (1992). They analyze firms’ strategic entry decisions in the framework of a discrete choice game. Stavins (1995), Mazzeo (2002) and Toivanen and Waterson (2005) adopt the same framework and study firms’ entry decisions in different scenarios. These articles conduct the estimation work under the assumption that firms possess complete information of rivals’ characteristics. During the estimation process, researchers have to check every firm’ s equilibrium conditions, which could drastically increase the computation burden, especially when the number of firms and their alternative choices are very large. In many contexts, firms do not completely know other firms’ decision variables. This makes the incomplete information structure a more favorable choice. Under the structure of incomplete information, firms’ equilibrium choices can be formalized as a set of Bayesian Nash equilibrium beliefs and the estimations therefore become easier (Rust, 1996). Estimators proposed by Hotz and Miller (1993), Aguirregabiria, Mira, 2002, Aguirregabiria, Mira, 2007, Pesendorfer and Schmidt-Dengler (2008) and Pakes et al. (2007) can be easily implemented in games with a large number of players or alternative choices. The discrete game with imperfect information has many applications including the empirical study of firms’ entry and special competition (Seim, 2006, Zhu, Singh, 2009, Vitorino, 2012) and social interactions (Brock and Durlauf, 2001). In this article, we show that the theoretical model can be equally transferred to a discrete choice game where sellers simultaneously enter different platforms with imperfect information of rival sellers’ quality and entry decisions. Unlike the previous studies of entry game which often directly assume a linear profit function, we obtain sellers’ profit from an explicit analysis of the interactions between sellers and buyers. The estimation is carried out by using NPL estimator proposed by Aguirregabiria and Mira (2007).====The rest of the article is organized as follows. Section 2 presents the theoretical model and analyzes the equilibrium of the game. Section 3 transfers the theoretical model to a simultaneous entry game with incomplete information and discusses estimation strategies. Section 4 introduces the background of Alibaba and its two online trading platforms: Tmall and Taobao, and describes data and variables, and then presents the results of estimation and counterfactual analysis. Section 5 concludes.",Do birds of a feather flock together? Platform’s quality screening and end-users’ choices theory and empirical study of online trading platforms,https://www.sciencedirect.com/science/article/pii/S0167718720301211,6 January 2021,2021,Research Article,137.0
Bet Germán,"Department of Economics, University of Florida. 224 Matherly Hall, P.O. Box 117140, Gainesville, FL 32611-7140 United States","Received 21 February 2020, Revised 12 December 2020, Accepted 21 December 2020, Available online 31 December 2020, Version of Record 13 January 2021.",https://doi.org/10.1016/j.ijindorg.2020.102705,Cited by (4),"I examine how incumbent airlines adjust their departure times in response to the threat of entry by Southwest Airlines. I find that incumbents space their flights more evenly throughout the day when faced with potential entry. This reaction depends strongly on the level of the incumbent’s market share and hub status at the endpoint airports of a market. The evidence suggests that incumbents’ actions are designed to deter, rather than accommodate, entry. I do not find effects on flight frequency, suggesting that incumbents may rely more on the strategic choice of product attributes than on product proliferation to deter entry.","When faced with the threat of entry, incumbent suppliers may alter both the prices they charge and non-price product characteristics. Reactions to potential entry might represent efforts to discourage entry (i.e., entry deterrence), or to minimize the harm that competitor entry may cause (i.e., accommodation). Understanding what incumbents can do when faced with potential entry, as well as the motivations behind their actions, is important both for enforcing the provision of antitrust laws and protecting competition, and for elucidating the efficiency losses created by the practices dominant firms pursue in a market.====This paper empirically examines how incumbents alter their product characteristics when faced with the threat of competitive entry. I study this question in the context of the U.S. airline industry by analyzing how incumbent airlines adjust their flight schedules (i.e., departure times for nonstop flights) in the face of potential entry by Southwest Airlines. In this case, one of the product’s attributes is a flight’s departure time; the space in which airlines locate their flights can be viewed as a 24-hour clock. Passengers have a distribution of most preferred departure times around the clock, and airlines set their flight schedules (or departure times) by taking this distribution into account, along with competitors’ schedules and the possibility of enabling one-stop city-pair market service by creating potential connections. Passengers obtain utility from the vertical attributes of the product, but experience disutility from the price paid and the schedule mismatch, which is the difference between passengers’ most preferred departure time and a flight’s departure time.====Estimation of the effects of interest presents several challenges. First, it requires a reliable measure for threats of entry. Researchers typically do not observe when incumbents realize that the chances of competitors entering the market have risen. As a consequence, it typically is difficult to separately identify the threat of entry from actual entry. Second, a measure for quantifying product specification and changes in departure times is required. This measure needs to address the discreteness in the number of departures and the fact that airlines typically schedule multiple flights per day on a given route. Finally, learning about the motivation behind schedule changes requires testable theoretical predictions and suitable data to test for the validity of alternative theoretical hypotheses.====I view a directional airport pair as a market. Following Goolsbee and Syverson (2008), I assume an entry threat occurs when Southwest establishes a presence at both endpoint airports of a market, but before it starts flying nonstop flights in the market itself.==== I quantify product specification using measures of spatial differentiation (Borenstein and Netz, 1999). Using a within-market regression model of an airline’s schedule decision over time, I ask whether the distribution of departure times for the nonstop flights of an incumbent airline is affected when Southwest threatens entry, and whether departure schedules are closer together or further apart under a threat of potential competition than in the absence of such a threat. I rely on predictions from economic theory to infer the motivation behind schedule changes (e.g., Bonanno (1987)): We should observe a non-monotonic relationship between schedule changes and an exogenous measure of the likelihood of Southwest’s entry if deterrence is the motivation.====I find that incumbent airlines change their departures times when Southwest threatens to enter a market. On average, this response takes the form of flights being more evenly spaced around the clock (i.e., incumbents increase the degree of differentiation in departure times in response to the threat of entry), which is accompanied by an increase in the incumbent’s range (i.e., difference between the last and first flights of the day) and interquartile range of the distribution of departure times. Perhaps surprisingly, I do not find responses in terms of flight frequency or capacity.====Incumbent responses vary with the likelihood of entry by Southwest. In markets where Southwest’s entry is very likely—and consequently entry deterrence is not possible—incumbents do not significantly change their flight schedules. A similar finding is obtained in markets where the probability of Southwest’s entry is low, and therefore deterrence is unnecessary. On the other hand, in markets where entry is uncertain but probable (i.e., markets characterized by an intermediate probability of entry by Southwest), incumbents adjust their flight schedules. Consistent with the deterrence motive, I find that incumbents do not take preemptive actions when Southwest preannounces entry into a market (instances in which Southwest’s entry is guaranteed).====The observed preemptive actions may reflect efforts by incumbents to reduce Southwest’s profitability, should it enter the market, by placing flights around times that would constitute niches of the market for a potential entrant or closer to Southwest’s expected departure times. It may also reflect efforts to reduce displacement costs (i.e., the difference between departure times and passengers’ most preferred departure times) for incumbents’ existing customers, with the goal of reducing the chance they switch to Southwest should it enter. Alternatively, changes in departure times might be used to congest airport resources or facilities, raising rivals’ costs and eventually deterring competitor entry.====Finally, I analyze heterogeneities in incumbent responses by characteristics of the market and the incumbent. I find a more pronounced response in terms of changes in flight schedules when the incumbent has a higher market share. This finding may reflect the possibility that entry deterrence is a public good when there are several incumbents in the market, as well as the fact that a dominant incumbent is less constrained by competition and has more at stake. Having a hub at a destination airport (as opposed to flights departing from a hub) is also a strong determinant of an incumbent’s response. This finding may reflect the higher costs of adjusting departures at a hub airport (as opposed to doing the same in a non-hub airport) and the fact that airport presence is an important component of product differentiation and cost structure, and may provide airlines with an alternative tool to deter or accommodate entry.====The empirical literature on entry deterrence has focused on different actions, including price cuts and capacity investment (e.g., Dafny (2005); Goolsbee and Syverson (2008); Tenn and Wendling (2014); and Sweeting et al. (2020)); strategic alliances (e.g., Goetz and Shapiro (2012)); advertising to affect potential market demand (e.g., Ellison and Ellison (2011)); and the responses of incumbent airlines in on-time performance measures (e.g., Prince and Simon (2015)).==== The present study complements this literature by examining product design (i.e., location) choices, which typically offer a rationale for preemptive actions.==== The paper also adds to the empirical evidence on tests of theoretical models of spatial product differentiation, as well as the literature on airline competition. Despite the importance of understanding the effects of different product positioning strategies within a market, most of the literature on airline competition has focused on other sources of market power in the industry (such as airport presence), or the effects of entry on market outcomes after entry occurs, while allowing for product differentiation.==== To the best of my knowledge, this paper is the first to empirically detect preemptive motives behind product design choices in terms of the space of a horizontal product attribute.====The paper is organized as follows. Section 2 describes an airline’s scheduling decision problem and its relationship to models of spatial horizontal differentiation. Section 3 describes the data, and Section 4 the estimation and identification strategies. Section 5 discusses the main results, and Section 6 presents evidence on the explanation for the preemptive actions. Section 7 concludes.",Product specification under a threat of entry: Evidence from Airlines’ departure times,https://www.sciencedirect.com/science/article/pii/S0167718720301284,31 December 2020,2020,Research Article,138.0
"Jost Peter-J.,Reik Steffen,Ressi Anna","WHU – Otto Beisheim School of Management, Chair of Organizational Theory, Burgplatz 2, Vallendar 56179, Germany,Ulm University of Applied Sciences, Faculty of Mathematics, Natural and Economic Sciences, Prittwitzstr. 10, Ulm 89075, Germany,WHU – Otto Beisheim School of Management, Burgplatz 2, Vallendar 56179, Germany","Received 26 November 2019, Revised 1 October 2020, Accepted 30 November 2020, Available online 24 December 2020, Version of Record 25 December 2020.",https://doi.org/10.1016/j.ijindorg.2020.102694,Cited by (2)," markets, such as those for car repairs and medical treatments, are generally characterized by an ex-ante and ex-post ==== between the uninformed customers and the informed expert. In this paper, we allow for both uninformed as well as informed customers to exist in a monopolist credence goods market. We analyze the implications of this kind of informational heterogeneity for the expert’s pricing decisions, incentives to commit fraud, as well as ==== and social welfare under different institutional arrangements. Most importantly, our approach enables us to evaluate endeavours to improve the level of customers’ information. Contrary to basic intuition, we find that recent developments and policy measures originally aimed at improving social welfare by increasing the level of information might actually backfire.","Credence goods markets are an important part of daily life, as they encompass common transactions such as medical treatments, repair works, financial consulting, or taxi rides. Often, these markets are plagued by asymmetric information. The seller (also called “expert”) is usually better informed about the customer’s need and the quality that is necessary to satisfy it. The asymmetric information usually persists even after consumption and creates incentives for fraudulent behavior. This might lead to large inefficiencies from inappropriate treatments as well as from customers anticipating to be defrauded and leaving the market.====Up to now, the existing theoretical literature has focused on the information asymmetries between sellers and buyers, while abstracting from informational differences that exist across customers. This is not surprising, as information asymmetry is the fundamental source of distortion in credence goods markets. Therefore, basic intuition suggests that an improved level of information will always benefit social welfare. This paper challenges this assumption by allowing for heterogeneity in the level of information among customers. We ultimately provide a more nuanced picture and warn against hidden side-effects of recent developments and policy measures that aim at improving the level of information to enhance social welfare.====Whereas it has always been the case that some buyers have more information than others, recent technological developments, in particular, have reinforced the difference in customers’ level of information.==== In the past few years, it has become increasingly common to conduct online research before trading in markets for expert services. Typical examples include online navigation applications that incorporate real-time traffic situations and provide taxi passengers with an idea of the shortest route and the approximate travel time. Further, the internet provides potential investors with sophisticated tools for creating their own portfolio strategies. It also enables people to get a better understanding of the severity of an electronic or mechanical damage and how to repair it. Online information seeking is particularly common for medical treatments. A survey by Pew Research Center found that more than one in three Americans self-diagnose their symptoms online (Fox and Duggan, 2013). According to Agrawal et al. (2018), curated information sources offer reliable and credential information and second opinions for health diagnoses. However, preliminary evidence suggests that the Web is skewed towards a more educated population and as such contributes to creating inequalities in health information accessibility (Weaver, 2013, Jacobs, Amuta, Jeon, 2017). This implies that nowadays experts should be more prepared than ever before to face customers with different levels of information. Whereas some customers may be completely uninformed about their need and therefore reliant on the seller’s diagnosis expertise, others know exactly which product or service they require.====In our model, we allow for this kind of information heterogeneity by introducing two types of customers in a market with a monopolist expert: the first type of customers has full information about the service required to fulfil their need, the other one remains uninformed. This is a simple, yet straightforward way to capture information differences among buyers and examine the associated influence on the expert’s pricing decision, the incentive to commit fraud as well as market efficiency and social welfare. Most importantly, this approach lays the foundation that ultimately enables us to comment on the efficiency of efforts to improve the customers’ level of information in markets for credence goods under different institutional settings.====The functioning of credence goods markets has been scrutinized in light of different institutional arrangements that pertain to the expert’s liability and the verifiability of the treatment’s quality. Against this backdrop, previous literature shows that if the expert can be held liable for insufficient treatments, a monopolist credence goods market will be efficient and fraud-free, irrespective of whether the treatment’s quality is verifiable or not (see Dulleck and Kerschbamer, 2006, henceforth DK06). Moreover, according to Hyndman and Ozerturk (2011), a market with liability but without verifiability will also be efficient and fraud-free when there exists both uninformed as well as (noisily) informed customers. Hence, it is reasonable to assume that in cases where the expert can be made liable, improving the customers’ level of information will be ineffective.==== For this reason, we focus our main analysis on the implications of heterogenously informed customers in markets where the expert cannot be held liable, and conduct a differentiated analysis with respect to whether the treatment’s quality is verifiable or not.====All in all, our analysis leads to the following novel results and policy implications. We show that the intuition regarding the beneficial effects of an improved level of information crucially depends on the prevailing institutional arrangements and can therefore not be unambiguously confirmed. More specifically, increasing the level of information will predominantly lead to clear-cut welfare gains under the setting of missing verifiability, where these improvements are most costly if not impossible to realize. However, under the setting where the treatment’s quality is verifiable and that would therefore basically render an improvement in the level of information most feasible, social welfare may deteriorate in response, or will at best remain unaffected. Hence, we strikingly demonstrate that policy measures aimed at improving customers’ level of information to enhance social welfare might actually backfire.====Finally, in an extension, we also verify the intuition that under the remaining institutional arrangements where the expert can be made liable, disseminating information lacks any influence on social welfare, as the market remains efficient and fraud-free. In conjunction with our main results, this also implies that with heterogenously informed customers, the expert’s liability can be considered as most decisive for determining market outcomes, whereas the verifiability of the treatment’s quality does not prove to be a sufficient condition for market efficiency and is therefore only of secondary importance. This is in contrast to previous literature that argues that a monopolist credence goods market will be efficient and fraud-free if ==== the expert can be held liable for insufficient treatments ==== the treatment’s quality is verifiable and hence considers these two institutional factors as equally decisive (see DK06).====The remainder of the paper is structured as follows. The next section gives an overview of the relevant literature. We then introduce our model in Section 3 and derive the equilibrium outcomes under different institutional settings in Section 4. Thereafter, we discuss our findings in Section 5 before we conclude with a short summary and prospects for further research.",The information paradox in a monopolist’s credence goods market,https://www.sciencedirect.com/science/article/pii/S016771872030117X,24 December 2020,2020,Research Article,139.0
"Callejas Jerónimo,Mohapatra Debi Prasad","University of Massachusetts, Amherst, United States","Received 19 October 2019, Revised 10 December 2020, Accepted 13 December 2020, Available online 24 December 2020, Version of Record 18 January 2021.",https://doi.org/10.1016/j.ijindorg.2020.102697,Cited by (2),"This article evaluates the welfare implications of a public procurement program, where the Ecuadorian government procures medicines used for cancer treatment and distributes it to patients for free with the aim to benefit the poor. Using a unique dataset on Ecuador’s pharmaceutical market, we estimate a structural model of demand and supply, and focus on two research questions related to this program. First, we consider a targeting strategy commonly implemented in various developing countries, where patients below a given income threshold qualify for the free drug. We compare this with a simpler drug distribution mechanism where every patient is a potential recipient of the free drug and the patients are served on first-come-first-serve basis. Our results show that the poor patients do self-select into the program, and the first-come-first-serve strategy does benefit the poor more compared to the relatively rich. However, the targeting strategy does a much better job in serving the poorest patients. Second, we study the supply side implications of this program. Our counterfactual exercises show that when the government procures low-cost drugs and provides them for free, it distorts the supply side incentives, and hence, market prices of similar low-cost drugs may increase by about 7% in response. Prices of the high cost drugs remain mostly unaffected. Therefore, the policy may end up negatively affecting near-poor patients that did not qualify for the free government drug.","Access to drugs is a contentious issue in the context of developing and underdeveloped countries, where limited access has excluded many patients from the benefits of pharmaceutical innovations. This is especially crucial in case of life-saving drugs (e.g. cancer drugs), where high prices impose significant economic burdens on poor consumers. Governments in less-developed countries undertake various public welfare programs aimed at the poorer sections of the population in order to ensure that they have access to life-saving drugs. Under such programs, the below-poverty-line patients receive drugs at subsidized prices (even at zero cost). However, implementing such income-based programs in developing countries can be a challenging task, as the potential recipients may lack credible income records (Alatas et al. (2012))==== Consequently, there is an increased emphasis on targeting strategies that do not rely directly on observing incomes. As an alternative strategy, the government may choose to procure a stock of low-cost generic version drugs==== and provide those to patients on first-come-first-serve basis until the stock is over. This simple strategy will be an effective distribution mechanism if the poor patients self-select into the welfare program, consume the free generic drug, and rich patients choose to opt for branded drugs by buying those from the market.==== The efficacy of this mechanism in serving the poorest depends on the heterogeneity of consumer preferences for branded and generic drugs that varies among rich and poor patients.====Additionally, implementation of a public welfare program may involve supply side implications. On the one hand, providing subsidized medicine unambiguously benefits the below-poverty-line consumers and increases their consumer welfare. However, this policy may have unintended consequences through supply side interactions. Under this program, the poorest among all consumers have access to the free medicine and hence are much less likely to buy those medicines from the market. If the low-income consumers are also highly price sensitive, the residual demand curve that the firms face is less elastic as a result of this policy. As a consequence, entry by the government and provision of free medicine for the poor may lead to firms charging higher prices in equilibrium. The extent of price increase in response to this policy depends on the level of government provision, and the elasticity of substitution among products in the market. The resulting price increase may lead to lower consumer welfare for the near-poor (i.e. above-poverty line) consumers who do not qualify for the welfare program. Hence, the overall effect of the policy on consumer welfare is an empirical question that we address in this paper by examining the effects of a drug procurement policy in Ecuador while accounting for firms’ incentives to adjust their product prices in the market.====Our analysis is carried out in the context of drugs that are used for breast cancer treatment. Breast cancer is one of the most common types of cancer in the 40–60 age group and is the twelveth most common cause of death among females in Ecuador. Our dataset records sales and prices of four different types of molecules used to treat breast cancer between 2007 and 2014.==== Since we observe consumer choices as well as the price setting behavior of the firms, we can ask: ‘would low-income patients self-select and choose the government drug if the government enters the market and provides a generic drug for free?’ ‘What would have happened to equilibrium drug prices and consumer welfare if the government offered free medicine to below-poverty-line consumers?’ To answer these questions, we take a structural approach and estimate a model of supply and demand where product prices are endogenously determined. Specifically, we allow heterogeneity in price sensitivity among consumers by flexibly modeling price coefficient as a function of consumer income. We recover marginal costs for drug production using equilibrium first-order conditions resulting from firm’s profit maximization.====Our demand estimates reveal the presence of heterogeneity across different consumers in terms of demand characteristics such as price sensitivities and willingness to pay. In particular, high income consumers are much less price sensitive compared to low income consumers. Additionally, high income consumers also derive higher utility from branded drugs compared to the generic products. The high price sensitivity of low income consumers implies that introduction of free medicine by the government would lead to significant substitution away from other products available in the market.====With demand and supply estimates in hand, we perform two counterfactual policy simulations. In our counterfactual world, the government procures a generic drug offered in the market and provides it to the cancer patients at zero cost.==== Our first counterfactual exercise considers a targeted drug distribution strategy based on income, and compares it with a first-come-first-serve mechanism. The former (targeted drug distribution strategy) provides free drugs to the consumers who earn below a given income threshold. It therefore requires detailed information on consumer incomes, and hence may be difficult to implement in the informal developing country setting. Under the latter (first-come-first-serve strategy), any patient irrespective of her income may choose the free government drug until the stock of free drug is available. This is simple and easy to implement. Our analysis reveals that the low income consumers do self-select into the welfare program, and hence the first-come-first-serve policy benefits the low-income section of the society more compared to the relatively rich. Our simulation exercise shows that the consumers with higher income would opt for the products offered in the market even when the free government drug in available in their choice sets. This is driven by the heterogeneity in consumer preferences, as high income consumers are less price sensitive and derive higher utility from branded drugs even when they have access to the free government drug. However, we can not conclude that the first-come-first-serve policy dominates the targeting policy, as targeting directs the benefits of the welfare program more effectively among the poorest.====In our second counterfactual policy simulation, we allow the firms to adjust the equilibrium prices in response to the government entry. The government decides the income threshold, and provides free medicine to the consumers whose incomes fall below the threshold. Hence, the consumers below poverty line may choose to buy a drug offered in the market and pay the market price or opt to receive free medicine from the government program. The government provides a generic version drug for free under its program. Note that, if the preferences for product variety dominates the price effect, then a consumer may choose to opt for the market product even when the free government drug is available in her choice set. All other consumers who do not qualify for the government drug choose from the range of products available in the market conditional on buying a product.====In our analysis, we fix the income threshold at different levels and compute the equilibrium prices as well as the distribution of consumer welfare. Our counterfactual simulations reveal that the public procurement program leads to an increase in the aggregate consumer welfare. The welfare effects are heterogeneous among consumers. The low-income consumers being highly price sensitive, choose the free government drug in most of the cases and enjoy significant increases in consumer surplus. However, the choices made by the low-income consumers renders the residual demand curves faced by the firms relatively inelastic. Our exercise shows that the firms would increase prices in response to the government entry. In particular, the firms with low-priced generic products would increase prices by around 7%. Therefore, the near-poor patients (i.e., the poor patients right above the income threshold) are more likely to buy the low-priced products, and are most negatively affected by the public policy. The rich consumers who used to opt for branded products are not affected by the policy as the prices of the branded products is not affected due to entry of the government. Our flexible demand and supply model captures those differential effects by allowing heterogeneity across consumers.====Our article relates to two sets of research literature. First, this paper contributes to an active literature on government intervention and targeted social safety net programs for addressing the poorest section of the society (for example, see Bokhari et al. (2007), Alatas et al. (2012), Alatas et al. (2016), Chatterjee et al. (2015), Duggan et al. (2016b), Polyakova and Ryan (2019) among others). The majority of studies conducted in developing countries have focused on the demand side of the targeting mechanism, by considering the role of consumer behavior in the adoption of a welfare program. Our study contributes to this literature by highlighting the supply side implications of the targeting policy, its effects on the firm behavior and its implications for consumer welfare.====Second, we also contribute to the growing literature on pharmaceutical product market in developing countries.====Chaudhuri et al. (2006) study the Quinolones antibiotic segment in India, and investigate the welfare implications of patent policy while allowing firms to adjust prices. Dutta (2011) also addresses welfare implications of patent policy by allowing firms to respond to policy changes. Duggan et al. (2016a) estimates the price effects of pharmaceutical product patents in the context of India. Similarly, Mohapatra and Chatterjee (2020) studies the price control policy and its effect of drug availability across various regions in India. Our work complements this literature by estimating the price effects as well as the consumer welfare effects of a public procurement program aimed at helping the low-income consumers. In a closely related work, Brugués (2020) uses detailed data from the public procurement auctions for a large set of products in the Ecuadorian pharmaceutical market, and studies the welfare effects of the procurement policy by flexibly modeling the strategic concerns in firms’ participation decisions in the auctions. In contrast, our study takes the auction stage as given, focuses on understanding the drug distribution mechanisms in the post-auction stage, and quantifies the variations in the price responses with varying levels of the income threshold as the number of low-income consumers who qualify for free medicine varies in a market.====The rest of the article is organized as follows: Section 2 describes Ecuadorian health system, and the data. Section 3 presents the framework for analysis. Section 4 discusses estimation details and the results from estimation. Section 5 explains the set up of our counterfactual exercises and reports results from counterfactual analysis. Section 6 concludes.",Welfare effects of public procurement of medicines: Evidence from Ecuador,https://www.sciencedirect.com/science/article/pii/S016771872030120X,24 December 2020,2020,Research Article,140.0
"Ekinci Emre,Theodoropoulos Nikolaos","Koç University, Rumelifeneri Yolu Sarıyer, Istanbul 34450, Turkey,University of Cyprus, Cyprus","Received 14 March 2020, Revised 27 November 2020, Accepted 5 December 2020, Available online 10 December 2020, Version of Record 24 December 2020.",https://doi.org/10.1016/j.ijindorg.2020.102696,Cited by (1),"To investigate delegation decisions within organizations, we develop a principal-agent model in which the principal can only informally delegate authority to the agent and the parties openly disagree with each other in the sense of differing prior beliefs about the optimal course of action. Our analysis shows that the degree of disagreement determines what kind of delegation policy the principal can commit to and this, in turn, alters the agent’s effort for information acquisition. Notably, at moderate degrees of disagreement, conditional delegation may arise in equilibrium, whereby the principal credibly commits to allowing the agent to exercise his authority only if he generates additional information about the optimal action. Further, we discuss two extensions in which the principal undertakes an investment that reduces the agent’s cost of acquiring information, and the agent discloses his ==== strategically.","As having access to expertise and local knowledge is critical for success, the allocation of decision rights to employees continues to be a fundamental challenge for organizations.==== Aghion and Tirole (1997) show in their seminal work that delegating authority involves a trade-off between increased initiative and loss of control. That is, the employee to whom authority is delegated may have stronger incentives to collect information useful for the organization; yet, delegation provides the same employee with an opportunity to pursue his or her own interests at the expense of those of the organization. Besides this “double-edged-sword” nature of the delegation of authority, certain features of organizations render the design of delegation policies even more challenging.====First, it is not uncommon that the organization and the employee disagree about the right course of action. In other words, despite having access to the same information as the organization, the employee may have a different intuition and therefore disagree with the organization on which actions are more likely to lead to a successful outcome. As differences of opinion alter the employee’s incentives for collecting information (Van den Steen, Che, Kartik, 2009), organizations should adjust their delegation policies accordingly. Second, as discussed by Bolton and Dewatripont (2013), delegation of formal authority within organizations may not be credible due to the so-called business judgment rule, which induces courts not to enforce any contracts written between parties in a single organization.==== This means that even though certain decisions may be informally delegated to the employee, the organization (that is, the party endowed with the formal authority) can choose to overrule the employee’s decision.====In this paper, we investigate the organization’s incentives to allocate decision rights in a setting in which the delegation decision is noncontractible, and the organization and the employee openly disagree on the optimal course of action. Our main objective is to determine how the equilibrium organizational mode depends on the degree of disagreement between the players and the cost of revoking the delegation decision. To this end, we develop a single-period principal-agent model whereby the goal of the employment relationship is to take an action in order to implement a given project. The action taken then determines the payoff of each party depending on the state of the world initially unknown to either player. In the spirit of Aghion and Tirole (1997), the principal can delegate authority to the agent who collects information about the state of the world. Regardless of being granted authority, the agent exerts costly effort to generate a signal, which is, if generated, publicly observed.==== After the agent’s investigation is over, the principal decides whether to stick to her initial delegation decision and then the party with authority chooses an action to implement the project. Notably, the principal incurs an implementation cost when she, rather than the agent, chooses the action to be implemented.====Central to our analysis is the disagreement between the principal and the agent concerning the right course of action. Along the lines of Van den Steen (2008) and Che and Kartik (2009), the players disagree openly by having different prior beliefs about the state.==== Differing priors not only result in different ==== preferred actions but also lead the players to interpret any information about the state differently—even though they observe exactly the same information—and therefore to have differing ==== preferred actions. Hence, in our setup, the conflict of interest between the players arises not because they have different underlying preferences but because they have differences of opinion.==== The other key ingredient of our model is noncontractible decision rights as in Baker et al. (1999). That is, even though the principal can informally delegate authority to the agent, before the agent exercises his authority, the principal can revoke it, at a cost, if it is in her interest to do so. The cost of retracting the agent’s authority may arise, for example, from the principal’s reputational concerns.====We begin our analysis with a benchmark case in which the principal has the ability to commit to a delegation policy.==== This benchmark, which is largely based upon Che and Kartik (2009), provides key insights. The players’ disagreement concerning the optimal action is mitigated with additional information; thus, the party with no authority is better off when the decision is made (by the party with authority) after a signal is observed than when the decision is made without observing any additional information. When decision making is centralized, the agent has additional effort incentives because by generating a signal he persuades the principal to take an action closer to his own preferred action. Since persuasion incentives arise only when the principal retains authority, the agent’s effort provision is higher under centralization than under delegation. This difference is decisive for the principal’s choice of organizational mode. In equilibrium, there exists a threshold degree of disagreement below which the principal delegates authority to the agent.==== That is, as long as the implementation cost is not too high, the principal chooses centralization to benefit from higher effort provision, i.e., higher likelihood of making an informed decision.====In our main analysis, we assume that the principal’s delegation is not credible in the sense that she takes authority back from the agent when it is in her interest to do so. In this case, the degree of ==== disagreement between the players and the informational setting determine the kind of delegation policy the principal can commit to. In the one extreme case in which the degree of disagreement is high, the principal finds it optimal to retract the agent’s authority regardless of whether the agent has generated any information. This follows because the principal’s cost of allowing the agent to choose his preferred action is higher than the cost of reneging on her promise by taking back the agent’s authority. In the other extreme case, in which the degree of disagreement is low, the principal can commit to not taking authority back from the agent since the cost of retracting authority exceeds the cost of being exposed to the agent’s decision-making. At moderate levels of disagreement, the principal can commit to ==== in the sense that the agent is allowed to exercise his authority if and only if he generates additional information prior to decision making. Intuitively, the principal’s cost of retracting authority is lower than the disutility caused by the agent’s decision only when the agent chooses an action upon observing a signal. The signal’s role in mitigating the degree of disagreement is essential to conditional delegation.====When granted authority, the agent determines his effort provision according to whether he will be able to exercise it. As a result, the agent’s effort is the lowest when the degree of disagreement is low. A logic similar to that of the benchmark case applies here. Because the agent can choose his preferred action independently of the outcome of his investigation, he has no incentives to persuade the principal and therefore has weak effort incentives. Interestingly, the agent exerts higher effort when the disagreement is severe even though he knows that his authority will be retracted before exercising it. The reason is that the agent behaves as if decision making is centralized and therefore exerts higher effort to persuade the principal by generating a signal. Finally, when the disagreement is moderate, in which case the principal can commit to conditional delegation, the effort provision is the highest. Because retaining authority is tied to the outcome of the investigation, the agent’s marginal benefit from generating a signal is the highest.====The organizational mode observed in equilibrium depends primarily on the level of effort exerted by the agent, which is, in turn, determined by the degree of ==== disagreement. When the disagreement is high, the principal retains authority because the persuasion incentives exist only under centralization while delegating authority to the agent and then taking it back does not strengthen the agent’s effort incentives but brings a cost due to the agent’s decision (or, in the terminology of Aghion and Tirole (1997), a cost due to loss of control). In the other extreme case in which the disagreement is low, the analysis is reminiscent of the benchmark case: the principal can delegate authority and commit credibly to not taking it back. As the lack of persuasion incentives curtails the agent’s effort, the principal’s incentive for delegation arises solely from avoiding the implementation cost. Hence, the principal opts for delegation only if the degree of disagreement is sufficiently low. At moderate levels of disagreement, the principal may delegate authority because, as indicated, she elicits higher effort from the agent under conditional delegation than under centralization. In particular, there exists a threshold level of reneging cost below which the principal opts for conditional delegation.====The novel result of the model is that a conditional delegation regime may arise in equilibrium at moderate degrees of disagreement. That is, even though decision rights are noncontractible, the principal can commit to allowing the agent to exercise his authority (or, in the terminology of Fama and Jensen (1983), to “ratifying” the agent’s decision) only when the agent collects additional information. Our analysis reveals that when formal delegation is not possible, the principal becomes weakly better off under some parameterizations. More specifically, when the cost of retracting authority is not high and the degree of disagreement is moderate, the principal switches from centralization to conditional delegation even though her payoff from centralization remains unaltered. Hence, losing her commitment ability may be beneficial for the principal.====To complete the analysis, we provide two extensions of our model. First, we investigate the principal’s incentives to be involved in the information acquisition process. Departing from the literature in which the principal’s involvement takes the form of conducting her own investigation to figure out the optimal course of action (e.g., Aghion, Tirole, 1997, Rantakari, 2012), we examine the principal’s incentives to undertake a cost-reducing investment in the information acquisition process. More specifically, the principal’s investment reduces the agent’s marginal cost of effort in information acquisition. Our analysis yields two main results. First, unlike in the no-investment case, persuasion incentives are present also under delegation. In particular, they provide the principal with additional investment incentives to elicit higher effort from the agent. Intuitively, because the principal’s marginal benefit from an informed decision is higher under delegation and investment fosters effort, the principal invests more under delegation. Second, investment facilitates delegation. For moderate degrees of disagreement, because the level of investment made under (conditional) delegation is higher than under centralization, the benefits from delegating authority, as opposed to retaining it, are even higher relative to the no-investment case, and this raises the threshold level of reneging cost below which the principal opts for conditional delegation.====In the second extension, we introduce strategic disclosure by allowing the agent to withhold his private information. Strategic disclosure improves effort incentives under centralization due to the prejudicial effect (Che and Kartik, 2009). We show that at moderate degrees of disagreement, conditional delegation may still arise in equilibrium unless the prejudicial effect is sufficiently strong—otherwise, the effort provision under centralization is higher and therefore conditional delegation is not optimal for the principal. Two features of this equilibrium are worth noting. First, strategic disclosure lowers the threshold cost of retracting authority, below which the principal opts for conditional delegation, because centralization is more attractive, due to higher effort provision, relative to the public information case. Second, conditional delegation entails full disclosure as the agent discloses any information he has collected.====The outline of the paper is as follows. Next section discusses the related work in the literature. Section 3 presents our theoretical analysis. In the subsections, we first present the setup of our model and then examine a benchmark case in which the principal can commit to delegation. Next, we derive the equilibrium behavior when the principal lacks the ability to commit to delegation. In the last part of the analysis, we provide the two aforementioned extensions of the model. Section 4 presents concluding remarks.",Disagreement and informal delegation in organizations,https://www.sciencedirect.com/science/article/pii/S0167718720301193,10 December 2020,2020,Research Article,141.0
"Haucap Justus,Heimeshoff Ulrich,Klein Gordon J.,Rickert Dennis,Wey Christian","Düsseldorf Institute for Competition Economics (DICE), Heinrich-Heine University Düsseldorf, Germany,Westfälische Wilhelms-University Münster, Germany,CERNA Mines ParisTech, France","Received 28 October 2020, Revised 24 November 2020, Accepted 24 November 2020, Available online 2 December 2020, Version of Record 24 December 2020.",https://doi.org/10.1016/j.ijindorg.2020.102693,Cited by (3),"We examine how different pass-through rates from input prices to retail prices and different vertical contracts affect upstream market definition. Simple theoretical considerations suggest that vertical restraints induce higher pass-through rates and thus lead to a larger upstream market definition when compared to linear wholesale pricing, given that contracts with linear pricing are associated with lower pass-through rates under ====. Data from grocery retailing is used to quantify the empirical implications of our theoretical assertion. We find that resale price maintenance leads to larger upstream market definitions than linear pricing. We therefore advise competition authorities to carefully model vertical market structures, whenever they expect incomplete pass-through to be important.","The SSNIP test (“Small but Significant Non-transitory Increase in Price”) is the workhorse for market definition in both US and EU competition law. It is frequently used to assess (i) cases of mergers and acquisitions, (ii) competitive effects of horizontal and vertical restraints, and (iii) potential abuses of dominant positions or other measures of market power.====The SSNIP test searches for the narrowest set of products for which a hypothetical monopolist could ==== raise prices by 5–10% above the competitive level. The SSNIP test therefore compares the additional revenues resulting from a hypothetical price increase with the additional cost coming from a potential demand loss. An unprofitable price increase implies that the cost of the price increase outweighs the benefit because of high consumer substitution to products outside the candidate relevant market. The true market must therefore include the next best substitute products that impose competitive constraints. The final market is defined by the product set for which the benefits outweigh the costs of the hypothetical price increase.====When manufacturers sell their products through retailers to final consumers, two fundamental problems of upstream market definition arise. First, retailers—such as in grocery, automotive and computer industries—could strategically dampen input price increases in order to prevent losses at the consumer level (see Chevalier, Kashyap, Rossie, 2003, Villas-Boas, 2007a). Second, the specific contracts are often not observable. They can be quite complex given that firms may specify wholesale prices, but also many other elements, such as fixed payments, rebates or even implicit agreements on resale prices. Thus, making specific assumptions about vertical contracts can lead to different predictions about the profitability of a price increase at the manufacturer level. This problem has been articulated quite clearly in the literature (see, e.g., Hastings, 2004, Lafontaine, Slade, 2008) and is an open issue to which we aim to contribute.====Our study theoretically and empirically examines the role of vertical relations in upstream market definition. While previous literature has clarified how consumer demand and horizontal competition affect the outcomes of the market definition procedures (e.g., Ivaldi and Verboven, 2005, Pereira, Ribeiro, Vareda, 2013), it is not yet well understood how market definition is implemented in the presence of vertical contracting between retailers and manufacturers. Assumptions on the type of vertical interaction determine how wholesale price increases in the SSNIP test are passed-through to consumer prices, which ultimately affect the market definition outcomes. A recent literature strand studies the magnitude of the pass-through rate in vertical contracts and finds that vertical restraints are likely to increase cost pass-through (Bonnet, Dubois, Villas-Boas, Klapper, 2013, Hong, Li, 2017). We extend these approaches by analyzing how pass-through rates of different vertical contracts affect upstream market definition outcomes.====We develop a theoretical model that relates the retail price change in several vertical contracts to changes of changes of profitability after hypothetical wholesale price increases. Our model predicts that, under reasonable conditions, the upstream market size, determined by a SSNIP test, increases with higher cost pass-through rates. The intuition is as follows: In markets with complete pass-through, the price increase at the retail level is strictly higher than in markets with incomplete pass-through. Higher retail price changes imply higher market share losses and larger profit decreases. Thus, the cost of a hypothetical wholesale price increase is higher in a complete than in an incomplete pass-through scenario, given that demand decreases more with larger pass-through rates. SSNIP tests ignoring incomplete pass-through rates erroneously overestimate retail price changes, thereby overestimating the costs of the hypothetical upstream price increase. Consequently, the test defines the upstream market more narrowly compared to a “true model” of incomplete pass-through. Integrating pass-through rates, in contrast, allows correctly estimating the true retail price increase and therefore the manufacturers' true cost of a wholesale price increase. As a result, the hypothetical monopolist market is larger and more aligned to the true model whenever cost pass-through is incomplete. This effect, however, decreases with increasing pass-through rates and is likely to vanish in the special case of complete pass-through.====We further propose a novel empirical framework that integrates the role of vertical contracts into the upstream market definition procedure. For this purpose, we follow Brenkers and Verboven (2007), who develop a SSNIP market definition test in the presence of double marginalization. We extend their approach by adding models of (i) strategic retail pricing and upstream competition as in Villas-Boas (2007a) and (ii) two-part tariff contracts with and without resale price maintenance as in Bonnet and Dubois (2010). Our approach uses data on prices and market shares to test vertical conduct and infer substitution patterns, profit margins, and costs, which are all necessary ingredients for the SSNIP test. We focus on integrating vertical restraints—in the form of resale price maintenance clauses—into the market definition analysis, which is highly relevant given the increasing number of vertical agreements that potentially restrict competition.====Furthermore, we empirically quantify how resale price maintenance clauses affect the manufacturers’ ability to pass on upstream supply shocks to consumers. Our study therefore also contributes to the literature on pass-through rate estimation, such as Goldberg and Verboven (2001), Bonnet et al. (2013), and Friberg and Romahn (2018). Goldberg and Verboven (2001), for instance, find that double marginalization can serve to dampen cost pass-through. Bonnet et al. (2013) extend their results by showing that resale price maintenance can increase the pass-through of a cost shock in the case of non-linear contracts with resale price maintenance. Our methodology extends these approaches by (i) considering retailers’ private label pricing in the analysis of vertical restraints and (ii) integrating pass-through rates into a market definition setup.====We implement the following three-step strategy. In a first step, we estimate consumer substitution patterns as well as a range of vertical supply-side models, recently developed in the Empirical Industrial Organization literature. In particular, we consider several collusive and non-collusive linear pricing models as well as two-part tariffs with and without resale price maintenance. Subsequently, we use the Rivers and Vuong (2002) test to select the channel margins with the best fit to the observed data. In a second step, to assess the retail pass-through rate across linear and non-linear pricing contracts, we simulate a cost shock in all vertical structures and re-compute the industry equilibria that would emerge. Following Bonnet et al. (2013), we interpret the differential retail price responses as a measure of how vertical structures allow for different strategic margin adjustments. These pass-through rates therefore inform us about how vertical contracts affect the capability of transmitting upstream supply shocks to consumers, which we interpret as a measure of competitive constraints.==== In a third step, we propose a method to integrate the vertical relations into a framework that is consistent with the SSNIP test proposed by pertinent merger guidelines.====This methodology is applied to study the German disposable diaper market for which we use rich and detailed category-level data obtained from a representative household home-scan survey—including the actual retail store choices of consumers and actual transaction prices. The diaper market is well suited for the analysis given that all diaper products in the category, with a single brand manufacturer and several private labels, are a perfect first guess for the candidate-relevant market. Our empirical evidence suggests that retailer-manufacturer relations are governed by non-linear pricing contracts with vertical restraints in the form of resale price maintenance clauses. This finding could be due to two reasons: first, the law might not be effectively enforced; second, firms often find ways to replicate this equilibrium with alternative, more sophisticated contracting mechanisms that would not actually involve explicit resale price maintenance clauses. In this contracting regime, manufacturers have the market power to seize profits that are close to the monopoly case, which is consistent with anecdotal evidence that retailers' profits from diapers are quite low, as retailers use diapers to attract consumers.====The SSNIP test in this “preferred” resale price maintenance scenario finds that the relevant market consists of the manufacturer brand’s products and the private labels of drugstores and discounters. This is consistent with consumer tests finding that discount and drugstore private labels, but not supermarket private labels, are perceived as substitutes. This is an important finding because antitrust authorities often exclude products ==== based on anecdotal evidence about production processes or opinions obtained from questionnaires, but without considering demand-substitution patterns.==== We show how such simple market segmentation approaches might be misleading because they provide incorrect policy advice. In our particular case, private labels are the only source for inter-brand competition in a market structure where a strong manufacturer appears to conduct resale price maintenance. Excluding private labels, based on an ill-advised market definition procedure, would therefore erroneously suggest that there is a monopoly on the supply-side.====In the next step, we analyze how differences across vertical contracts affect the market definition outcomes. We put a specific focus on the comparison of resale price maintenance and linear pricing models. To that end, we conduct a “simulation exercise”, in which we use the price-cost margin from the preferred model to simulate prices, margins, and cost pass-through for several equilibrium pricing models. In the simulation exercise, we hold marginal costs constant to analyze the effect of pass-through rates on market definition outcomes under several vertical structures. Furthermore, we conduct a “misspecification exercise”, in which we intentionally select the “non-preferred” supply models that imply the wrong cost and profit structure. The misspecification exercise is informative about how (wrong) assumptions on marginal costs and profit margins affect market definition outcomes.====The findings from the “simulation exercise” show that manufacturer profits are highest when resale price maintenance is prevalent in the market. The manufacturer maximizes industry profits in this quasi-integrated industry outcome by internalizing the cross-price elasticities of all products. Channel profits and prices, however, are highest in the double marginalization scenario, where both the retailers and the manufacturer earn a margin on the branded products. In order to learn about the competitive constraints across models, we conduct a cost pass-through analysis simulating how increases of channel costs are passed through to retail prices. We find that resale price maintenance increases the pass-through rate of a 10% cost shock by almost 10 percentage points relative to the case of linear pricing contracts. The intuition behind this result is the existence of the so-called double markup problem that arises in linear vertical supply models. Retailers set their prices conditional on the manufacturer decision, which limits the channel members’ ability to pass-on cost increases to consumers. The elimination of the double markup, e.g., by the implementation of vertical restraints, allows a higher pass-through of the channel cost increase. Our results thus yield important insights for market power assessment in general, and market definition analysis in particular, by showing that channel members can use vertical restraints to increase their margins and the cost-pass-through rates.====We then conduct a SSNIP market definition test that considers the varying levels of competitive constraints across supply models. Most notably, our analysis highlights that resale price maintenance leads to a wider market definition outcome compared to linear pricing models, which is in line with our theoretical prediction. Resale price maintenance implies a full pass-through rate from wholesale to retail prices, while linear pricing regimes imply incomplete pass-through rates. Given that market shares are a decreasing function of prices, the market share loss is increasing with retail pass-through rates. Thus, the cost of the hypothetical upstream price increase is strictly higher with resale price maintenance than with linear pricing. Consequently, we find a market that is defined more widely when firms use resale price maintenance clauses.====The results in the “misspecification exercise” outline the existence of a bias from poor model specification even when assuming the correct pass-through rate. For instance, the linear pricing model with a full pass-through rate leads to a market that is defined too widely. The reason is that the profitability changes after hypothetical price increases also depend on the absolute levels of marginal costs and price cost margins. Interestingly, this assumption is often applied in the counterfactual exercises in the Empirical Industrial Organization literature (see, e.g., Berry et al., 1995).====All in all, our insights call for a careful investigation into how the SSNIP test is performed. Given that most firms sell their products via intermediaries with potentially complex vertical contracts (see e.g., Chevalier et al., 2003), we draw the attention of competition authorities and researchers to the importance of considering the strategic retail behavior in upstream market definition. Our study derives the following policy implications from the analysis: First, it is crucial to carefully model the entire supply chain in the upstream market definition procedure whenever incomplete pass-through is an important market characteristic. Second, models of complete pass-through, such as resale price maintenance, produce results closer to a market definition procedure that ignores the vertical structure. Third, we find that antitrust authorities can avoid modeling the supply chain whenever they expect full pass-through.====Although we focus on a subset of infinitely many possible contract types in the empirical implementation, our results can be generalized to other market structures. In particular, we model take-it-or leave-it-offers with exogenous outside options for retailers, which implies that manufacturers have full bargaining power. A different literature strand models bilateral negotiations with endogenous outside options (see, e.g., Crawford, Yurukoglu, 2012, Ho, Lee, 2017, Draganska, D., Villas-Boas, 2011). Hristakeva (2019) looks at a contract where the lump-sum payments of the two-part tariffs are the reverse of our setup. The type of contract choice naturally affects the inference on the profit-sharing rule between manufacturers and retailers. Our general findings on market definition outcomes, however, hold for a range of model specifications, where pass-through rates are important market determinants.====A distinct advantage of our approach is that it allows credible inference on product-level costs and profit margins, requiring solely retail scanner data and assumptions on channel conduct (Nevo and Whinston, 2010). Many standard market definition models, in contrast, use information submitted by firms, such as diversion ratios (often based on consumer questionnaires), profits, and cost information. These measures, however, are (i) prone to a reporting bias and (ii) often not available at the product level due to the difficulties of separating common costs from true economic costs (see e.g., Nevo, 2001). Our approach can be used instead of (or complementary to) reduced-form approaches, such as price correlations, analysis of accounting data, and questionnaires. It stands undisputed that these reduced-form approaches remain useful screening devices in so-called phase I proceedings, where quick decisions need to be made based on easy-to-interpret summary statistics. However, these tools typically cannot capture all important market features. We propose to apply a more structural approach like ours in so-called phase II investigations, where accuracy is more important than practicality (Friederiszick and Roeller, 2010).====The remainder of the paper is organized as follows: Section 2 introduces our theoretical model. Section 3 describes the market and the data. Section 4 develops the empirical framework. Section 5 presents results from the demand side, the supply models, and the market definition exercise. Section 6 concludes.","Vertical relations, pass-through, and market definition: Evidence from grocery retailing",https://www.sciencedirect.com/science/article/pii/S0167718720301168,2 December 2020,2020,Research Article,142.0
"Cosnita-Langlais Andreea,Johansen Bjørn Olav,Sørgard Lars","Economix, UPL, University Paris Nanterre, CNRS, University Paris Nanterre and EconomiX-CNRS, Nanterre F92000, France,Department of Economics, University of Bergen, Bergen, Norway,Department of Economics, Norwegian School of Economics, Bergen, Norway","Received 26 November 2017, Revised 19 November 2020, Accepted 21 November 2020, Available online 1 December 2020, Version of Record 7 December 2020.",https://doi.org/10.1016/j.ijindorg.2020.102692,Cited by (1)," the impact of a change in margin on one side of the market, either due to a price change or to efficiency gains, on the pricing incentives on the other side. We propose modified versions for the indices of pricing pressure (UPP and GUPPI) that take this into account. We show that in two-sided markets where the cross-group externalities are positive the upward pricing pressure will typically be overstated if the rebalancing effect is ignored. Our approach explains why competition agencies should look at both sides of the market when assessing platform mergers.","Merger assessment has undergone substantial changes since the 90s, from an approach based on market definition, market shares and concentration index, to the application of methods that can directly indicate the potential price increase. Merger simulation has started being used as a modeling tool for predicting the unilateral anticompetitive effect (see Werden and Froeb, 1994, for instance), but the quality of the prediction is known to be sensitive to the various specifications that need to be made (the demand curvature and the pass-on rate being crucial assumptions). Implementing the merger simulation is also prone to practical difficulties, which is why it is useful to resort to less expensive, quick but still reliable alternative merger screening tools. Simplified approaches that focus instead on the ==== post-merger pricing incentives of the insiders have thus been proposed.====For instance, the approaches of Werden (1996) and Shapiro (1996) only require data on markups (obtainable from the insiders) and diversion ratios (empirically measurable and relatively easily available). In order to circumvent the problem of demand curvature and efficiencies pass-through, Werden (1996) focused on the marginal cost reductions necessary to offset the post-merger price increase, evaluated at the pre-merger prices.==== In contrast, Shapiro (1996) computed the post-merger price increase assuming linear or constant elasticity of demand to obtain the indicative price increase (IPR). The latter has been further developed by Farrell and Shapiro (2010) into the Upward Pricing Pressure (UPP) approach by focusing on the pre-merger prices.==== Importantly, UPP calculations appear to be a good substitute for full merger simulations.==== Since 2005 various antitrust authorities have applied this simplified approach in many cases, and it has become a standard method referred to in merger guidelines.==== It is often used as a filtering device for separating unproblematic deals from those that require a more in-depth review, but also as an important input for the analysis of the competitive harm when banning mergers or solving them with remedies.====Mergers on digital markets are attracting growing attention and increased antitrust scrutiny.==== It is now quite generally acknowledged that insights from traditional merger analysis may not directly apply to platforms (see e.g. Wright 2004, or Evans and Noel 2008). Using tools developed for one-sided markets may lead to wrong decisions, such as clearing anti-competitive mergers or banning pro-competitive ones, because the type and magnitude of indirect network externalities are likely to affect firm behavior on two-sided markets.====This paper belongs to a recent strand of the literature addressing the way in which antitrust analyses, in particular the UPP approach, should be adapted for two-sided markets.==== More precisely, we focus on the UPP index derived by Affeldt et al. (2013) for horizontal mergers in two-sided markets. Instead of having two firms producing one product each, Affeldt et al. (2013) consider two merging firms that serve two different groups of users, such as advertisers and readers. To derive the two-sided version of an UPP for each of the products sold by one of the insiders, Affeldt et al. (2013) assume that the prices of ==== other products are fixed, including that of the same firm on the other side of the market. In so doing, they build on the simpler UPP promoted by Farrell and Shapiro (2010), for which it is also assumed that the merged firm’s other price is unchanged. Farrell and Shapiro (2010) acknowledge that it would be more accurate to take into account the merged firm’s incentives to change ==== its prices. However, they also note that there is only a relatively small loss in accuracy when instead using the simplified, any-other-price-kept-constant version of the UPP index, because this test is always more conservative compared to the more accurate measures proposed by e.g. Werden (1996), hence still appropriate to flag mergers in need of further scrutiny.====In this paper we argue that this may not be the case in two-sided markets. Implicitly assuming that the platform makes enough cost savings on one product/side to write off its incentive to change the price of that product can scale down but also up the UPP for its other product/side, depending on the cross-group externality. We refine the UPP indices of Affeldt et al. (2013) by no longer assuming that the insider’s price on one side is constant when looking at its incentive to raise its price on the other side. We show as a result that in response to a price increase on one side, the insider’s other price may either increase or decrease, feeding back into the price on the side that the UPP is supposed to analyze.====This is explained by the fact that there are actually three effects to be considered when assessing the incentives for a price raise on one side of the platform: an effect on demand from users on the same side, a second effect from demand of users on the opposite side, and finally a third one from the price on the opposite side. The first effect is negative, because demand will fall on the side where the price increases. The second effect is also negative as long as the cross-group externality is positive, because then demand on the opposite side will fall as well. The third effect is that, given positive cross-user externality, the firm has incentives to set a lower price on the opposite side, which increases demand on that side and thereby also increase demand on the initial side. The reason for the price drop on the opposite side is that increasing the margin (by a price increase) on the initial side increases the incentive to raise participation on the opposite side, since this extra participation attracts more high-margin sales on the initial side. Hence, when the cross-platform network externality is positive, this third effect is likely to work against the first and second effects. Overlooking this third effect may therefore overstate the incentives to raise price on the initial side.====Note that the pricing of complements in a standard one-sided model shares a relationship with the problem of how a platform should set prices on each side of a two-sided market.==== An important difference between the two-sided framework and the one-sided complements model is that generally for platforms the cross-group externality between sides can also be negative – while obviously indirect externalities cannot ==== be negative. As a result, on two-sided markets the feedback from the other product’s price into the price increase on the first product may go both ways, enhance it but also mitigate it.====To calculate the adjusted pricing pressure indices that take into account the impact of a platform’s price change on one side, we propose to use the very intuition behind the more accurate version of Farrell and Shapiro’s (2010) UPP index, the one which incorporates Werden’s (1996) compensating marginal cost approach. This does not require more data than that used by Affeldt et al. (2013) for their UPP measures. By using the compensating marginal cost approach we are able to show that a merger leading to upward pricing pressure on one side of the market may lead to a downward pricing pressure on the other side of the market, even if there are no efficiencies and margins on both sides are non-negative. This price effect on the other side of the market is not taken into account by the UPP index of Affeldt et al. (2013). We show that using their version of the UPP measure in some situations may overstate the upward pricing pressure on both sides of the market, and hence it may predict an upward pricing pressure when in reality there may be a downward pricing pressure. On one-sided markets the simpler indices (that ignore the firms’ incentives concerning prices on other products) are conservative, so screening mergers based on them may ==== lead to false negatives. The point we make in this paper is that this conclusion may not hold for mergers on two-sided markets, for which using the simpler version of the UPP may also lead to false positives. In such cases, the use of the simpler, any-other-price-kept-constant UPP measure (either as a screening device or as part of the in-depth analysis of mergers) loses some of its value, because we can no longer be certain of which way the error goes. This also suggests that an approach where the competition agency only focuses on one side, which we have seen some examples of, can lead to an erroneous decision.==== Hence our analysis indicates that competition agency’s decision may depend on whether they consider each side of the market separately or both sides in total,==== and as such fuels the long-lasting academic and policy debate on how to properly assess competitive harm on platform markets. The recent AmEx decision in the U.S.==== partially settled this debate by making clear that ====, i.e. two-sided, incentives on parties’ pricing and output decisions must be taken into account in both defining a market and assessing the harm in that market for transaction platforms. This integrated approach is nonetheless recommended by recent academic contributions for all platforms as far as the competitive assessment goes,==== and our analysis of the UPP measure on two-sided markets supports this: in order to properly assess the incentives for a price increase on one side, we argue it is necessary to account for the two-sided impact of that price raise.====In what follows we first examine the difference between the simpler and the more accurate versions of the UPP for one-sided markets, to remind that both yield similar conclusions for the screening of mergers. We then turn to the two-sided UPP measure, and propose a modified (and in many cases more conservative) version of the measure proposed by Affeldt et al. (2013). We also provide a comparison of the two approaches based on the data used by Affeldt et al. (2013), and discuss our results before concluding on their implications for the practice of competition agencies.",Upward pricing pressure in two-sided markets: Incorporating rebalancing effects,https://www.sciencedirect.com/science/article/pii/S0167718720301156,1 December 2020,2020,Research Article,143.0
"Briglauer Wolfgang,Dürr Niklas,Gugler Klaus","Vienna University of Economics and Business (WU), Research Institute for Regulatory Economics, Welthandelsplatz 1, 1020 Vienna, Austria,School of Business, Economics & Information Systems, University of Passau, 94032 Passau, Germany,ZEW – Leibniz Centre for European Economic Research, L 7, 1, 68161 Mannheim, Germany,Department of Economics and Research Institute for Regulatory Economics, Vienna University of Economics and Business (WU), Welthandelsplatz 1, 1020 Vienna, Austria","Received 1 October 2019, Revised 31 October 2020, Accepted 14 November 2020, Available online 19 November 2020, Version of Record 2 December 2020.",https://doi.org/10.1016/j.ijindorg.2020.102677,Cited by (12),"This study aims to assess the economic benefits of high-speed broadband within and across neighboring counties in Germany. Utilizing a balanced panel dataset of 401 German counties with data from 2010 to 2015 as well as different panel estimation techniques, we find that an increase in average broadband speed has a significantly positive effect on regional GDP in the average German county. Furthermore, we find that broadband deployment in German counties induces not only substantial economic benefits in terms of direct effects within counties but also positive regional externalities across counties. According to our estimation results, an increase in average bandwidth speed by one unit (1 Mbit/s) induces a rise in regional GDP of 0.18%. This effect is almost doubled if we also take regional externalities into account (0.31%). Moreover, we find that regional agglomeraton effects are of particular relevance for rural counties. Our cost-benefit analysis of subsidies based on conservative estimates suggests efficiency gains, as the total economic per capita benefits (€164) of subsidy programs to encourage broadband expansion exceeded their associated per capita costs (€114).","The economic benefits of “old” broadband networks for consumers have been increasingly emphasized by economic research (Bertschek et al., 2016). Proponents of comprehensive broadband availability underscore its character as a general purpose technology (GPT) that induces positive externalities in major economic sectors (Bresnahan and Trajtenberg, 1995). Similarly, the wide-scale roll-out of “new” broadband networks which are largely based on fiber-optic transmission technologies in most or all parts of the network is believed to spur job creation in information and communications technology (ICT) and other related industries, and, more generally, is ascribed enormous potential for facilitating productivity increases, product innovations and economic growth.====Accordingly, in 2010 the European Commission (EC) launched the Digital Agenda for Europe (DAE), which “seeks to ensure that, by 2020, (i) all Europeans will have access to much higher internet speeds of above 30 Mbit/s[ec] and (ii) 50% or more of European households will subscribe to internet connections above 100 Mbit/s[ec]” (European Commission, 2010, p. 19). While the first target is a goal for the supply side, the second refers to a minimum level of household adoption on the demand side. Achieving these goals promises economic returns, but they also entail substantial deployment costs (Bock & Wilms, 2016; FTTH Council Europe, 2012). There is, however, hardly any empirical evidence on whether positive externalities beyond those associated with basic broadband networks will emerge under the new broadband infrastructure.====In order to achieve the DAE's goals, ambitious targets have been implemented in most EU member states. In Germany, for instance, the DAE informs the government's goal of providing at least 50 Mbit/s to all households by 2018 in its “Digital Agenda 2014–2017” strategy, which was adopted in August 2014.==== Note that high-speed broadband infrastructure enabling ≥ 50 Mbit/s must be at least in part fiber-cable based in the access network, or, with a view to wireless broadband, must be based on (advanced) fourth generation (4G+) mobile technology (Long Term Evolution, LTE) which was introduced in Europe in 2010.====Our study employs a unique balanced panel data set from 2010 to 2015 for all 401 German counties.==== Using various panel estimation techniques, we investigate the following five research questions: (i) What is the impact of high-speed broadband speed on economic outcome (in terms of regional gross domestic product (GDP))? (ii) Are there increasing returns to scale with respect to higher broadband speed levels? (iii) Are there positive or negative==== externalities among neighboring counties at a regional level? (iv) Is there a difference in effect in urban vs. rural counties? (v) Are the total benefits sufficient to cover past public expenditures for the funding of high-speed broadband infrastructure?====Understandably, the economic outcomes associated with the adoption of a given policy is of crucial concern. This is particularly true for public broadband funding, which in Germany primarily aims to extend high-speed broadband to areas of the country where commercial providers do not see sufficient profitability, primarily due to low population density. In order to reach the ubiquitous coverage target, Germany's federal and state governments have provided substantial funding. However, there is hardly any empirical ex post assessment on the actual economic benefits of such programs.==== Our study aims to assess the economic benefits of high-speed broadband within and across neighboring counties in Germany. We find that broadband deployment in German counties induces substantial economic benefits in terms of direct effects and regional externalities. An increase in average broadband bandwidth speed by one unit (i.e. 1 Mbit/s) induces a rise in regional GDP of 0.18%. This effect is almost doubled if we also take regional externalities into account (0.31%). We thus find evidence of strong regional agglomeration effects which are of particular relevance for rural counties. The latter is of importance for broadband policies aimed at closing the digital divide between urban and rural areas. Taking diminishing returns of average bandwidth speed into account, our estimates further imply an optimal broadband bandwidth coverage of 37.4 Mbit/s for regional GDP. Finally, our cost-benefit analysis of subsidies suggests efficiency gains, as the total economic per capita benefits (€164) related to German subsidy programs to encourage high-speed broadband expansion and increase average broadband speed exceeded their associated per capita costs (€114).====The remainder of this article is organized as follows. The second section presents a brief review of the existing empirical literature on the economic impact of broadband networks and related speed levels. The third section provides a simple regression model framework and a characterization of our panel data set. The forth section presents our identification strategy, while section five discusses our main estimation results. Drawing on our estimation results, section six compares the estimated benefits and costs of implementing the “Digital Agenda 2014–2017” in Germany. The final section concludes the paper with a review of our main findings. It also summarizes the key insights generated by our research for policy makers and outlines an agenda for future research.",A retrospective study on the regional benefits and spillover effects of high-speed broadband networks: Evidence from German counties,https://www.sciencedirect.com/science/article/pii/S0167718720301004,19 November 2020,2020,Research Article,144.0
Cumbul Eray,"TOBB University of Economics and Technology, Turkey","Received 24 May 2019, Revised 23 July 2020, Accepted 24 October 2020, Available online 7 November 2020, Version of Record 25 November 2020.",https://doi.org/10.1016/j.ijindorg.2020.102674,Cited by (5),"We compare an ==== firms’ expected profits form a decreasing sequence from the first to the ==== in the Stackelberg game. The last mover earns more expected profit than the first mover if ==== or the ratio of the signals’ informativeness to the prior certainty is sufficiently low. Lastly, there is a discontinuity between the Stackelberg equilibrium of the perfect information game and the limit of Stackelberg perfect revealing equilibria, as the noise of the demand information of firms vanishes to zero at the same rate. We provide various robustness checks for the results when the precision of signals are asymmetric, there is public information or cost/quality uncertainty, or the products are differentiated.","The technological, marketing, or regulatory characteristics of markets often dictate the relative timing of investment decisions of firms. For example, with the emergence of 5G technology in the telecommunication sector, there are three and four-tier Stackelberg leader-follower market structures (Townsend, 2019a, Townsend, 2019a, Townsend, 2019b, Moorhead, 2019). Entrants or telecom companies with insufficient network capacity decide whether to install new networks or expand their existing networks to offer the 5G service. These companies have to make this decision in advance based on their demand forecasts for the new technology, because the network expansion process is time consuming (Demiguel and Xu, 2009). However, some follower incumbents with sufficient network capacity have the flexibility to wait until they observe the leaders’ supply decisions to infer signals about the potential demand. They can then decide the level of their network capacity to re-allocate from the existing services to the new service.====Regulatory frameworks in many countries require incumbents to allow new entrants to use the incumbents’ infrastructure at regulated fees, which can be covered by providing subsidies to the entrants (Wallsten, 2007). Such regulations lead to simultaneous supply decisions between new entrants and incumbents, and reduce market concentration. Therefore, it is important for a social planner to compare the equilibrium outcomes between the simultaneous-move Cournot and sequential-move Stackelberg quantity-setting games among firms. As firms collect demand information for the new 5G services, it is crucial to perform these comparisons when firms have private information about stochastic demand.====In this article, we compare an ====firm Cournot game with a Stackelberg model, where ==== firms choose outputs sequentially, in a stochastic demand environment with private information.==== Demand is linear and stochastic in the intercept. Firms have symmetrically precise private signals about the unknown demand parameter. In this model, we show that expected price and total profits are higher, while expected total output, consumer surplus, and total surplus are lower in the Stackelberg perfect revealing equilibrium (PRE) than in the Cournot-Bayesian equilibrium for any finite number of firms and noise levels of signals and demand shocks. These rankings are novel because they are the opposite of the rankings of earlier studies, which assumed perfect information about demand.====We demonstrate four effects that explain our first main result about the total surplus rankings between our Cournot and Stackelberg models with demand uncertainty. These effects appear only under the Stackelberg competition. The first effect is the traditional first-mover advantage strategic effect. As the production decisions of followers are strategic substitutes of their predecessors’ production decisions in the absence of demand signaling through outputs, early-mover firms preempt their followers by making large investments. Therefore, this effect induces total output and total surplus to increase under the Stackelberg competition as compared to the Cournot competition.====The second effect is the information acquisition effect. In a leader-followers game, followers perfectly infer the private demand signal of their predecessors by observing their predecessors’ output choices in the PRE. Therefore, they are better informed about demand than a Cournot oligopolist. Accordingly, followers are likely to produce more when the demand is high and produce less when it is low under the Stackelberg competition relative to the Cournot competition. This greater output volatility induces higher welfare, implying that this effect favors Stackelberg over Cournot in welfare terms.====The third effect, the so-called signaling effect, accounts for the negative output effect of information acquisition by the followers on their predecessors. Under this effect, any non-last mover firm is reluctant to choose a high quantity to avoid signaling high demand to its successor(s). Thus, this effect makes the Cournot game more favorable to society than the Stackelberg game.====The last effect is the negative externalities from the information acquisition effect. As demand shocks are common for all firms, a more informed follower firm leads the residual demand of its competitor(s) to be less variable. Therefore, rival firms will obtain less value from exploiting their demand information. This lower variability in the demand intercept of competitor firms would translate into lower total surplus. Therefore, this effect favors the Cournot competition over the Stackelberg competition in welfare terms.====In sum, only the first two effects favor the Stackelberg competition over the Cournot competition in terms of welfare. Nevertheless, the impact of the signaling and negative externalities of information acquisition effects dominates the impact of the remaining two effects on total surplus. Thus, the simultaneous-move quantity setting game generates more total surplus than its sequential counterpart.====Our second main result characterizes the expected profit rankings of firms in our Stackelberg game. We show that at the PRE, the first ==== firms’ expected profits form a decreasing sequence from the first to the ====. The last mover always obtains more profit than the ==== mover. It earns the highest profit when ==== the private signals of firms are sufficiently uninformative, or the demand uncertainty is sufficiently low. The pioneer, on the other hand, is likely to lead the market when the number of firms is greater than four and the signals of firms are sufficiently informative (or prior uncertainty is high).====The above results occur because whereas the quantity strategy of a firm is a strategic complement to the quantity strategy of its immediate predecessor, it is a strategic substitute to the quantity strategies of all its other preceding movers. Thus, the number of followers of a firm with which it has strategic substitutability relations decrease as we approach the last mover in the hierarchical chain. Therefore, early-mover advantages prevail among the first ==== firms. As the last-mover firm does not face any signaling effect, it can easily increase its output to earn more profit. However, as the number of firms and the precision of demand signals increase, this information advantage of the last-mover decreases and the leader can also get the highest expected profit.====There is also a discontinuity between the Stackelberg equilibrium of the perfect information game and the limit of Stackelberg perfect revealing equilibria, as the noise of the demand information of all firms vanishes to zero at the same rate.==== The discontinuity occurs because even if demand is almost certain, extreme outcomes with extremely large demand can never be completely ruled out because of perfect correlation between the common prior values of firms and the uncertainty range. At ==== for the follower, its signal and the leader’s inferred signal are equally informative because the firms’ signals are perfectly correlated. Therefore, even when the noise of each firm’s demand information becomes arbitrarily small, the follower continues to use the signal of the leader in updating its belief about demand. Thus, the signaling distortions on outputs remain even in the limit case.====Finally, we test the robustness of our main results in three different ==== set-ups: i) firms receive asymmetric quality of signals in the presence of public information, ii) they face cost (or quality) uncertainty, and iii) the products are differentiated. In the first set-up, all of our proposed rankings are reversed if and only if a follower has a sufficiently more precise signal than the leader. More precise public signals create greater signaling distortions, which increases the followers’ relative profitability. In the second set-up, our proposed rankings are preserved if and only if the degree of correlation between the cost values of two firms is sufficiently high. The discontinuity vanishes when both firms’ signals are imperfectly correlated, as there is no signaling distortion when the signals become very accurate. In the last set-up, the follower earns more expected profit than the leader at any degree of horizontal differentiation between products. However, the Stackelberg competition generates higher expected total output and welfare than the Cournot competition if firms produce complementary products.====In Section 1.1, we present a literature review. In Section 2, we provide our ====-firm set-up. In Sections 3 and 4, we derive the equilibrium outputs in our base Cournot and Stackelberg oligopoly models. In Section 5, we present our main results. In Section 6, we study the robustness of our results in three different set-ups. Section 7 concludes. Proofs of the base model and the extended models are in the Appendix A and Online Appendix B, respectively. Online Appendix C contains the auxiliary calculations for our proofs using ==== 12.1.",Stackelberg versus Cournot oligopoly with private information,https://www.sciencedirect.com/science/article/pii/S0167718720300977,7 November 2020,2020,Research Article,145.0
"Hawthorne Ryan,Grzybowski Lukasz","University of Cape Town, School of Economics, Rondebosch, Cape Town, 7701, South Africa,Telecom ParisTech, 46 rue Barrault, 75013 Paris, France & University of Cape Town, School of Economics, Rondebosch, Cape Town, 7701, South Africa","Received 18 October 2019, Revised 4 May 2020, Accepted 26 September 2020, Available online 6 November 2020, Version of Record 18 November 2020.",https://doi.org/10.1016/j.ijindorg.2020.102673,Cited by (1),"We test for the distributional effects of regulation and entry in the mobile telecommunications sector in a highly unequal country, South Africa. Using six waves of a consumer survey of over 134,000 individuals between 2009–2014, we estimate a discrete choice model allowing for individual-specific price-responsiveness and preferences for network operators. Next, we use a demand and supply equilibrium framework to simulate prices and the distribution of welfare without entry and mobile termination rate regulation. We find that, in the South African context, regulation benefits consumers significantly more than entry does, and that high-income consumers and city-dwellers benefit more in terms of increased ====.","Economists have devoted substantial efforts to study the impact of regulatory interventions on consumers and firms. However, there is little granular evidence on the effects of these decisions on different groups of consumers. In particular in developing countries, where the levels of income inequality are high, the benefits from regulation may not be distributed equally. In this paper, we use a partial equilibrium model to assess the distributional welfare effects of entry and regulation in the mobile telecommunications sector in South Africa, which is the most unequal economy in the world.====The functioning of this sector is typically controlled by the government or independent regulators in two key areas. Firstly, entry is regulated through the licensing of radio frequency spectrum.==== Secondly, mobile termination rates (‘MTRs’), which are the wholesale prices for terminating calls on mobile networks, are usually regulated.==== The impact of entry and the effects of the regulation of MTRs have been extensively studied in previous papers. For example, Economides et al. (2008) quantify the benefits of entry into local telecommunications service markets. They find that consumers benefit significantly, though rather than resulting in reduced prices, entry results in product differentiation and new plan introductions. In another paper, Genakos et al. (2018) study the impact of market concentration levels on prices and investment in 33 OECD countries in the years between 2002 and 2014. They report that prices and concentration are positively related and that increased concentration may lead to higher investment. Nicolle et al. (2018) use hedonic price regressions to analyse the impact of entry, regulation and investment on prices for mobile services in France. They find that quality-adjusted prices declined between May 2011 and December 2014, mainly due to new entry and investment in 4G networks. The impact of market structure on consumer welfare is also studied by means of merger simulation (see, for example, Nevo (2001) in a study on cereals, Ivaldi and Verboven (2005) in relation to the truck industry and Grzybowski and Pereira (2007) on mobile telecommunications).====Among studies on the impact of regulation on prices of telecommunications services, Genakos and Valletti (2011) analyze how the regulatory intervention to cut fixed-to-mobile (F2M) termination rates impacts mobile retail prices.==== Using panel data of prices and profit margins for mobile operators in more than 20 countries in a period of over six years, they find that a reduction in F2M termination rates leads to an increase in retail prices, which they call the ‘waterbed effect’.==== In a more recent paper, the same authors estimate the impact of the regulation of F2M and mobile-to-mobile (M2M) termination rates on mobile phone bills using a large panel covering 27 countries (Genakos and Valletti (2015)). They find that the ‘waterbed’ phenomenon becomes insignificant on average over the 10-year period, 2002–2011. They argue that this is due to the changing nature of the industry, whereby mobile-to-mobile traffic surpassed fixed-to-mobile traffic.====Our paper extends this literature by showing how entry and regulation impact on different segments of society, by studying changes in prices and consumer surplus at the level of the individual. We disentangle the effects of entry and regulation, which occur at the same time, through counterfactual simulations. We are also not aware of any paper which estimates the effect of MTR regulation on consumer welfare. Our paper is relevant for developing economies, where the fixed-line telecommunications infrastructure is poor or non-existent; many people do not have Internet access and use mobile phones to make voice calls. In these countries, the MTR regulation is critical in driving down telecommunications bills. Many developing economies have also high income inequalities, and it is worth investigating how the benefits from regulation are distributed in those societies.==== Our analysis is less relevant for developed economies, where the cost of mobile telecommunications services is low relative to disposable income and there is good coverage by competing networks.====In order to study the effects of entry and regulation, we use six waves of a survey of about 134,000 individuals collected between 2009 and 2014.==== We estimate a discrete choice model allowing for individual-specific price-responsiveness and preferences for network operators. We take into account the endogeneity of prices by means of a two-stage control function approach following Petrin and Train (2010). In the first stage, we regress retail prices on termination costs and a set of operator and tariff segment dummy variables. We include the residuals from the first stage in the deterministic part of the utility function to account for unobserved quality. Furthermore, we identify the price coefficient by exploiting the availability of operator choices due to entry and price variation between operators and customer segments over time.====We use the estimates of demand parameters and individual price-responsiveness to conduct counterfactual simulations. First, we simulate market outcomes in the absence of a new, fourth entrant, Telkom Mobile, which launched mobile services late in 2010, and in addition without Cell C, a third entrant which launched services in around 2002. Second, we simulate a counterfactual situation without the regulation of termination rates which took place between 2010 and 2014. We find that rich people benefited more from entry and regulation in terms of changes in consumer surplus. This is linked to a greater decline in prices experienced by higher-income consumers. Based on our partial equilibrium model, we find that entry has a limited impact on consumer surplus and mobile adoption overall. In addition, we find that regulation of MTRs results in significantly lower prices. We also find that, absent regulation over the period between 2011 and 2014, mobile penetration would have been eight percentage points lower among low-income consumers compared to four percentage points among high-income consumers.====The remainder of the article is organized as follows. Section 2 describes the market being analysed. Section 3 introduces the econometric framework. Section 4 presents the data which we use in the estimation. Section 5 presents the estimation results and finally, Section 6 concludes.",Distribution of the benefits of regulation vs. competition: The case of mobile telephony in South Africa,https://www.sciencedirect.com/science/article/pii/S0167718720300965,6 November 2020,2020,Research Article,146.0
"Brown David P.,Eckert Andrew","Department of Economics, University of Alberta, Edmonton, Alberta, Canada","Received 21 October 2019, Revised 7 October 2020, Accepted 28 October 2020, Available online 6 November 2020, Version of Record 24 November 2020.",https://doi.org/10.1016/j.ijindorg.2020.102676,Cited by (6),"Using data from Alberta’s wholesale electricity market, we demonstrate the challenges that can arise when characterizing a firm’s unilateral expected profit-maximizing offer curve. We illustrate that the residual demand curves faced by firms can be highly non-linear, resulting in non-monotonic, downward sloping, optimal best-response offer curves violating common restrictions imposed on bidding behavior. This can have important implications on the conclusions drawn from such empirical analyses. We identify features of residual demand curves that can lead to these problems, providing guidance to researchers utilizing these methods. We find that a simplified monotonic smoothing of the unconstrained ==== optimal offer curve can achieve the majority of the expected profits, offering an alternative to calculating the ==== expected profit-maximizing offer curve that can be computationally burdensome.","The measurement of market power and evaluation of firm behavior is an important issue in concentrated wholesale electricity markets where firms interact repeatedly. Market power has been shown to lead to substantial market inefficiencies and rent transfers from consumers to producers (Borenstein, Bushnell, Wolak, 2002, Wolak, 2003a, Brown, Olmstead, 2017).==== Further, evaluations of firm behavior have raised concerns of coordinated conduct (Sweeting, 2007, Brown, Eckert, 2021). These concerns have led to various market reforms and the implementation of policies to regulate the degree to which firms are able to bid above marginal costs (FERC, 2014, AUC, 2017).====The use of concentration measures to evaluate industry structure such as the Herfindahl–Hirschmann Index has been shown to be a poor measure of market power in electricity markets due to their inability to account for transmission constraints, elasticity of demand, and the intertemporal variation in firms’ abilities to exercise market power (Borenstein et al., 1999). This led to the proliferation of a literature that compares observed firm behavior to a perfectly competitive benchmark (e.g., Wolfram, 1999, Borenstein, Bushnell, Wolak, 2002, Brown, Olmstead, 2017). This literature often finds that firms exercise substantial market power elevating wholesale prices and resulting in large productive inefficiencies.====In addition, to address the possibility of coordinated conduct, there is a growing empirical literature that establishes methods to characterize a firm’s unilateral expected profit maximizing supply function (Wolak, 2000, Wolak, 2003a, Wolak, 2003b, Wolak, 2007, Wolak, 2010, Hortaçsu, Puller, 2008, McRae, Wolak, 2014, Mercadal).==== While the details and methods differ across each study, these analyses utilize models that characterize a firm’s unilateral expected profit maximizing (best-response) supply function facing a stochastic residual demand function that represents market demand net of its rivals’ supply functions.==== Wolak, 2000, Wolak, 2003b and Hortaçsu and Puller (2008) discuss conditions under which the expected profit maximizing supply function can be characterized by finding the optimal price-quantity pair that maximizes a firm’s ==== profit for any potential realization of residual demand. Hortaçsu and Puller (2008) utilize this methodology to map out each firm’s ==== optimal offer curve and compare it to observed behavior in Texas’s electricity market. The authors find that large firms bid in a manner that is consistent with unilateral expected profit maximization, while smaller firms deviate from this benchmark.==== Mercadal (2018) extends this analysis to consider forward financial markets and transmission constraints.====Wolak, 2007, Wolak, 2010, Wolak, 2015 discusses the potential for the ==== optimal offer curve to be unobtainable in practice because of bidding restrictions (such as the requirement that firms submit monotonically non-decreasing supply functions). For example, Wolak (2015) notes through a graphical discussion that the ==== profit maximizing offers may trace out a downward sloping offer curve. Wolak, 2003b, Wolak, 2007 present an econometric methodology to estimate the expected unilateral profit-maximizing offer curve subject to bidding restrictions imposed on firms’ offer curves. Notably, while these studies point out the potential for the ==== optimal offer curve to violate practical constraints such as monotonicity, this literature does not consider the extent to which such violations will occur in practice, or the particular reasons and circumstances under which this is likely to occur. These questions, which are important to understand the practical limitations of employing ==== offer curves and the need for a methodology to identify ==== expected profit maximizing supply functions, motivate the current paper.====In this article, we utilize data from Alberta’s wholesale electricity market for a time period during which generating firms were alleged to have engaged in coordinated behavior.==== We focus our analysis on three firms that make up approximately 40% of the market, and consider the reasons and extent to which the ==== optimal offer curves violate monotonic bidding restrictions.====We show that a firm’s residual demand can be highly non-linear with large shelves and vertical portions; similar residual demand characteristics appear in Australia (Wolak, 2007), Italy (Bigerna et al., 2016), and New York (Benatia, 2018). We find that these features can result in profit functions that are non-concave with multiple local optima. We demonstrate that both the multiplicity of local optima and local convexity of residual demand can result in backward bending (downward sloping) ==== best-response offer curves violating restrictions that supply functions must be monotonically increasing.==== We characterize conditions under which these problems will arise.====Within the restrictions of our empirical framework, we present evidence suggesting that two of the three large strategic firms in our sample bid in a manner that is inconsistent with the supply function that passes through the ==== profit maximizing price-quantity pairs, whereas the third firm bids closely to this benchmark. However, because the ==== optimal offer curve is often non-monotonic, these results could be driven in part by the fact that firms are unable to bid in a manner consistent with this benchmark. To investigate this possibility further, we construct alternative offer curves that represent a monotonic smoothing of the optimal offer curves. We document that two of the three firms earned expected profits considerably below those that would have been achieved by utilizing the simplified monotonically smoothed optimal offer curves. This suggests that the non-monotonicity of the ==== optimal offer curves was not the key constraining factor for these two firms.====A finding that firms are not behaving according to the predictions of unilateral profit-maximizing behavior made by a particular model may be the results of restrictions of the model being used, as opposed to evidence of true deviations from unilaterally profit-maximizing behavior. While it is outside the scope of our analysis to relax every assumption of our framework, we consider several alternative explanations for why these firms may be deviating from the ==== optimal offer curve by considering the impact of dynamic costs, misspecification in the estimated forward contract positions, and transmission congestion. These robustness checks do not change our conclusions.====In addition, our paper assesses the extent to which a simplified monotonic smoothing of the ==== optimal offer curve can capture potential expected profits when the ==== optimal is unobtainable in practice (e.g., due to a violation of monotonicity). We find that employing the outer envelope of the ==== optimal offers achieves the vast majority of available firm profits in our sample. Further, we argue why this simplified benchmark can serve as an initial conservative test for coordinated behavior in this setting.====Our analysis makes several contributions to the existing literature. First, our context represents an empirical illustration of the conditions under which a firm may not be able to achieve the full ==== unilateral profit-maximizing profits due to monotonicity restrictions imposed in practice, and emphasizes the importance of accounting for such restrictions when considering whether firms are behaving unilaterally optimally. Second, we identify easily recognizable features of residual demand curves under which this issue is more likely to arise, providing guidance to researchers regarding whether non-montonicity concerns may be present in their particular empirical context. Third, our finding that the simplified monotonic smoothing of the ==== optimal offer curve is able to achieve the majority of expected profits suggests a useful and practical alternative to the computationally and time-intensive derivation of the monotonically constrained ==== optimal offer curves.====Finally, there is a broader literature that analyzes the impacts of restrictions imposed on bidding behavior in multi-unit divisible goods auctions. Kastl, 2011, Kastl, 2012 and Holmberg et al. (2013) analyze the impact of restricting the number of steps in an agent’s bid function. Kastl (2012) and Woodward (2019) account for monotonic restrictions imposed on bidding in a theoretical framework. These studies demonstrate that these bidding restrictions can induce bidders to adjust the their bid functions impacting equilibrium outcomes. Our analysis contributes to this literature by providing additional empirical evidence on settings under which bidding restrictions and conditions on residual demand can induce agents to adjust their offers in multi-unit auctions.====The paper is organized as follows. Section 2 presents the theoretical foundations. Section 3 presents our main empirical application, data, methodology, and results. Several extensions of our main analysis are considered in Section 4. Section 5 concludes.",Analyzing firm behavior in restructured electricity markets: Empirical challenges with a residual demand analysis,https://www.sciencedirect.com/science/article/pii/S0167718720300990,6 November 2020,2020,Research Article,147.0
"Bisceglia Michele,Cellini Roberto,Siciliani Luigi,Straume Odd Rune","Toulouse School of Economics, Esplanade de l’Université 1, Toulouse 31080, France,Department of Economics, University of Bergamo, Via dei Caniana 2, Bergamo 24127, Italy,Department of Economics and Business, University of Catania, C.so Italia 55, Catania 95129, Italy,Department of Economics and Related Studies, University of York, Heslington, York YO10 5DD, UK,Department of Economics/NIPE, University of Minho, Campus de Gualtar, Braga 4710-057, Portugal,Department of Economics, University of Bergen, Norway","Received 23 January 2020, Revised 22 October 2020, Accepted 26 October 2020, Available online 2 November 2020, Version of Record 13 November 2020.",https://doi.org/10.1016/j.ijindorg.2020.102675,Cited by (6),"We consider a model of optimal price regulation in markets where demand is sluggish and asymmetric providers compete on quality. Using a spatial model, which is suitable to investigate the health care and education sector, we analyse within a dynamic set-up the scope for price premiums or penalties on volume. Under the assumption of symmetric cost information, we show that the socially optimal time path of quality provision off the steady state can be replicated by a simple dynamic pricing rule where the dynamic part of the rule is ==== in the sense that the price premium or penalty on volume is common across providers, despite their differing production costs. Whether the price schedule involves a penalty or a premium on volume relates to two concerns regarding production costs and consumer benefits, which go in opposite directions. Price adjustments over time occur only through the price penalty or premium, not time directly, which highlights the simplicity and thus applicability of this regulation scheme.","In markets such as health care or education, prices are commonly regulated across a range of OECD countries. Providers instead compete for consumers on product quality. But quality is not easily observable to regulators, although one of the key objectives of regulation is to improve quality. Instead, regulators observe volumes (e.g., patients, pupils, students) and these can be used indirectly to incentivise quality improvements. In this study, we investigate optimal price regulation under the assumption that demand is ====, and it is modelled such that only a fraction of consumers respond to quality changes at each point in time. This implies that it will take some time before potential demand is fully realised. Sluggish demand is a plausible assumption in health care and education, due to asymmetry of information between providers and consumers (Arrow, 1963) and related uncertainty and noise in the observed quality, or due to consumer habits, trust or confidence in particular providers.====In markets where demand responds sluggishly to changes in quality, adjustments to any type of shock (e.g., exogenous cost changes or entry of new providers) are likely to take a long time, implying that such markets might seldom be characterised by allocations of qualities and demand that are close to a steady state outcome. This implies, in turn, that a price regulation scheme based on theoretical insights from static equilibrium analyses is unlikely to produce a socially optimal outcome. What is instead called for is a dynamic analysis that allows for a characterisation of the equilibrium (as well as the socially optimal) dynamic path off the steady state.====Finding simple rules for price regulation that induce a more efficient quality provision is a challenge for regulation authorities, even in a static context. Dynamic effects of price regulation on volume and quality, due to demand sluggishness, make the challenge considerably more difficult. The main contribution of the present study is to characterise the properties of a specific form of dynamic price regulation, in which the unit price paid to the provider is an affine function of demand. We suggest a price regulation scheme that allows for the price to increase with demand, a form of price ==== on volume, or to decrease with demand, a price ==== on volume. This scheme is not only simple and thus applicable, since volume is easily observable while quality is not, but it is also potentially efficient. Under some assumptions, including symmetric cost information, we show that a welfare maximising regulator, by optimally choosing this pricing rule, is able to induce the socially optimal dynamic path of quality provision.====This kind of volume-based price regulation is in line with examples of price regulation in health care markets, which is the main motivating example for our analysis.==== An early example is what were known as ‘cost and volume’ contracts when the purchaser-provider split was introduced in England in the early nineties under the internal markets. Hospitals facing higher-than-agreed volumes would face a reduction in the DRG tariff of up to 30% of the of agreed tariff below the first volume threshold, and with the tariff gradually decreasing with higher volume thresholds Fenn et al. (1994). In recent years, concerns for cost containment have again led purchasers to make increased use of volume caps or reduced tariffs for volumes in excess of expected ones (Allen and Petsoulas, 2016).==== There are proposals to introduce ‘blended’ payments that comprise a fixed amount and a variable volume-related element that reflects actual activity with a payment as low as 20% of the regular tariff (NHS Improvement, 2018). Is such a price regulation scheme, with penalties for higher volumes, likely to be socially efficient in a dynamic sense? Or would instead social welfare improve by the adoption of a pricing scheme that ==== higher volumes? This study identifies some general conditions that can answer these questions.====We use a Hotelling model of quality competition under regulated prices. Although this model in some sense represents a convenient simplification, such a spatial competition framework does have some features that seem particularly relevant for health care markets (and also education markets, as we discuss in Section 8). These are markets where demand decisions are in line with the unit demand assumption (e.g., each patient demands one medical treatment), and where travel distance is a key factor (in addition to quality) in determining consumers’ choice of provider.==== The assumption of fixed total demand is also a reasonable approximation to markets with non-price competition and small (or even zero) consumer copayments, implying that total demand is highly inelastic.====We consider an infinite-time horizon differential game with two providers, located at the extremes of a unit line, offering one product each. We allow providers to differ in production costs (due to differences in land, capital and labour costs), and such differences are observable to the regulator. In turn, such differences in costs can affect the optimal regulated price, which we also allow to differ across providers. As an extension, we also consider an alternative scenario in which cost differences are not observable to the regulator, and therefore the price scheme is identical across providers.====We first characterise the optimal provider plans regarding product quality over time, under what is known in differential games as the state feedback solution, where current choice of quality depends on current demand (the state variable) which is observable by providers.==== We then proceed by taking a social welfare perspective and deriving the optimal price regulation rule, which provides the most novel insights. We show that, under asymmetric information on costs, the socially optimal time path of quality provision off the steady state can be replicated by a dynamic pricing rule where the dynamic part of the rule is ====, so that the price premium or penalty on volume is common across providers, despite differing production costs. Instead, the fixed price component differs across providers to reflect different costs, with a higher value for the provider with lower marginal production costs. Price adjustments over time occur only through the price penalty or premium on volume, not time directly, which highlights the simplicity and applicability of this regulation scheme.====Whether the price schedule involves a penalty or a premium on volume relates to two conflicting concerns regarding production costs and consumer benefits. Under the assumption of decreasing returns to scale, concerns for cost-efficient production dictate that demand should be steered towards the provider with lower demand, which can be achieved by a quality reduction for the high-demand provider. Instead, concerns for consumer welfare dictate that the high-demand provider should invest more in quality, implying that demand is steered away from the low-demand provider. If the former concern for cost efficiency dominates, so that welfare is increased by reducing (increasing) quality of the high-demand (low-demand) provider, this can be achieved by introducing a price penalty on volume, which reduces the price-cost margin, and thus incentives for quality investments, of the high-demand provider relative to the low-demand provider. This is optimal if the convexity of production costs is sufficiently high.====Although some dimensions of costs are observable to the regulator (e.g., related to land, capital and labour costs), others dimensions of costs might not be observable. Therefore, as an extension, we assume that discriminatory (provider-specific) pricing is not possible due to asymmetric information on costs. We show that this might increase the scope for a price premium on volume, particularly if the cost difference between the providers is sufficiently large.====The rest of the study is organised as follows. In Section 2 we provide a brief overview of the relevant literature before presenting the model in Section 3. In Section 4 we derive the providers’ optimal choice of quality under the state feedback solution concept. In Section 5 we introduce the welfare analysis by deriving the first-best solution, and in Section 6 we show how this solution can be implemented by an optimally chosen dynamic price rule. Section 7 extends the analysis to asymmetric information on costs. Concluding remarks are provided in Section 8.",Optimal dynamic volume-based price regulation,https://www.sciencedirect.com/science/article/pii/S0167718720300989,2 November 2020,2020,Research Article,155.0
"Choné Philippe,Linnemer Laurent","CREST, 5 avenue Henry Le Chatelier, 91120 Palaiseau, France","Received 11 February 2020, Revised 7 September 2020, Accepted 7 September 2020, Available online 17 September 2020, Version of Record 28 September 2020.",https://doi.org/10.1016/j.ijindorg.2020.102663,Cited by (31),"Linear demand systems and quasi-linear quadratic utility models are widely used in industrial economics. We clarify the link between the two settings and explain their exact origin as it seems to be little known by practitioners. We offer practical recommendations to achieve consistency, tractability and a reasonable degree of generality when using the linear demand framework. We show that all tractable versions of the model used in practice are (almost) identical and have a mean-variance structure. We provide concise, ready-to-use formulae for the symmetric model. Finally, we revisit and extend the asymmetric model of Shubik and Levitan.","The usage of a Linear Demand System for differentiated goods (henceforth LDS) is widespread in oligopoly theory, especially when closed-form solutions are needed. Historically, the micro-foundation of an LDS has been the Quasilinear Quadratic Utility Model (hereafter QQUM).====As Amir et al. (2017) point out “this framework has become so widely invoked that virtually no author nowadays cites any of the(se) early works when adopting this convenient setting” (see page 642 of their paper). This lack of reference is not new, however. Some researchers find it so natural to use linear (direct or inverse) demands (and to derive them from a QQUM) that they do not try to give a source.==== This lack of reference, however, can confuse other economists who try to cite a source but have difficulties coördinating on the correct one. Whereas users can also be disoriented by the various existing ways to write an LDS and/or a QQUM, we show that models presented as different are in fact isomorphic.====The goal of this paper is to help IO economists find their way into the intricacies of these models. For that purpose, we structured the paper as follows. In Section 2, we start by briefly recounting how LDS and QQUM were introduced and extensively used by Richard E. Levitan and Martin Shubik==== in the 1960s and we show these pioneers deserve credit for having paved the way. In fact, by analogy with the Cobb-Douglas utility function, it would not be farfetched to name QQUM after Levitan and Shubik. Next, in Section 3, we introduce a general LDS and explain its relation to QQUM. We then survey the main properties of an oligopoly game based on such linear demands. In Section 4 we explain the ins and outs of a simple yet rich enough symmetric model. We provide concise ready-to-use formulae and emphasize the mean-variance structure of the model. Finally, in Section 5, we extend the symmetric model to a richer asymmetric set-up (revisiting the asymmetric model of chapter 9 of Shubik and Levitan (1980)). Closed-forms formulae can still be obtained and could be of more use in future research to emphasize results absent in the symmetric context.",Linear demand systems for differentiated goods: Overview and user’s guide,https://www.sciencedirect.com/science/article/pii/S0167718720300862,17 September 2020,2020,Research Article,156.0
"Gautier Axel,Somogyi Robert","HEC Liege, University Liege, LCII. Bat B31, Quartier Agora, Place des orateurs 3, Liege B4000, Belgium,CORE (UCLouvain), Belgium,Department of Finance, Budapest University of Technology and Economics, Hungary,Institute of Economics, Centre for Economic and Regional Studies, Hungary,CESifo (Munich) Germany","Received 12 June 2019, Revised 9 August 2020, Accepted 7 September 2020, Available online 17 September 2020, Version of Record 26 September 2020.",https://doi.org/10.1016/j.ijindorg.2020.102662,Cited by (16),"This paper analyzes two business practices on the mobile internet market, paid prioritization and zero-rating. These practices allow the internet service provider to discriminate different content types. With prioritization, the ISP delivers content at different speeds; with zero-rating, the ISP charges different prices. In recent years these practices have attracted considerable media attention and regulatory interest. When the asymmetry between content providers is limited, in particular with regard to their ability to attract traffic or to monetize it, we first show that the ISP can extract more surplus from consumers by privileging the relatively weaker content and restoring symmetry between content providers. Next, we show that the ISP chooses prioritization when traffic is highly valuable for content providers and congestion is severe, and zero-rating in all other cases. Finally, we find that a policy banning prioritization can lead to zero-rating and a reduction in consumer surplus.","Net neutrality is the principle of equal treatment of all data packages sent over the internet, irrespectively of their content, origin, destination and type of equipment used to access it. This regulatory principle prohibits discrimination on the internet and mandates Internet Service Providers (ISPs) to treat all data packets equally. In recent years, two business practices, prioritization and zero-rating, have been widely discussed and debated both in policy circles and the media. These practices violate the net neutrality principle and allow the ISP to discriminate between content in terms of ==== for prioritization and in terms of ==== for zero-rating. Thus they allow the ISP to add an additional layer of differentiation between Content Providers (CPs) beyond their intrinsic differentiation. Our objective in this paper is to build a model that evaluates the costs and benefits of such an empowerment of the ISP and to contrast and compare the two practices.====The practice of ==== consists of creating a fast lane on the internet where privileged content circulates in case of congestion. The prioritized content has a better quality –faster delivery– while the non-prioritized content types experience congestion in the form of throttling, jitter and/or delays. Prioritization is thus equivalent to quality differentiation. The ==== practice consists of creating financial differentiation between content providers. In this case, the ISP makes the consumption of some data packets more expensive than others. A typical example of financial discrimination is mobile data plans with zero-rated content. With such a contract, users subscribe to a package with a monthly data cap but the usage of some content (e.g. Facebook Messenger or Netflix) does not count against this data cap, i.e. the ISP charges different marginal rates for different types of content. Zero-rating is thus equivalent to price discrimination. With prioritization and zero-rating, the ISP directly influences the competition between content providers. Therefore, they might be ready to compensate the ISP for having priority or being zero-rated.====Net neutrality is the only regulation that is specific to the internet and it is a highly contentious issue (see for instance the detailed discussion in Greenstein et al., 2016). Different countries adopt widely different approaches to net neutrality and regulators often have a contrasting attitude towards prioritization and zero-rating. The objective of this paper is to compare the two types of business practices within the same model, thus evaluating the often differentiated policy towards them.====We consider a monopolistic ISP that connects CPs and consumers, i.e. the ISP is a two-sided intermediary. In a neutral internet, the ISP must charge the same price to all consumers and CPs, the latter being equal to zero.==== In a non-neutral internet, the ISP can discriminate between content, and our model shows that it does so in equilibrium. Discrimination gives an advantage on the content market to the privileged CP, an advantage for which the ISP may ask for compensation. This, as a consequence, changes the price structure of the ISP, which then charges a positive price on the CP side of the market as well.==== In our model, there are two competing content providers. The CPs are financed by advertising, and they compete to attract internet users. They offer differentiated content, represented by the standard Hotelling line. We suppose asymmetric content providers with one CP (called the strong CP) benefiting from a larger home market, i.e. more consumers have an intrinsic preference for its content. Importantly, we assume that the asymmetry between content providers is limited in several aspects, i.e. in the size of the home market, in the advertising revenue they can generate and also in their potential reach of new consumers in the model variants where demand is variable. To access content, users must subscribe to an internet service provider (ISP), that we suppose to be in a monopolistic position. The ISP faces a capacity constraint and thus may not be able to deliver the best available quality to all users.====Under prioritization, the ISP discriminates between content providers and gives the content supplied by ==== priority access over the content supplied by ====. Therefore, if there is congestion on the internet, the download quality is not identical for the two types of content. The priority content provider ==== may or may not compensate the ISP for getting priority, but consumers pay the same price to the ISP for accessing both types of content.==== Under zero-rating, consumers are not charged the same price by the ISP when they access the content of ==== as when they access the content of ====. In other words, the ISP financially discriminates the two content providers, but they remain identical in terms of download quality. The zero-rated content provider ==== may or may not compensate the ISP for this service.==== Under net neutrality, the ISP cannot discriminate between content providers, so this regime can serve as a natural benchmark. In particular, net neutrality prohibits quality differentiation like prioritized content and financial differentiation like applying a lower rate for some types of content. A consequence is the absence of financial transfers between the ISP and the CPs under net neutrality.====Both prioritization and zero-rating create distortions on the CP market compared to net neutrality, i.e. market shares on the CP market are affected by the ISP. The ability to distort content consumption depends, in the case of prioritization, on the bandwidth capacity with a lower bandwidth creating more distortion and, in case of zero-rating, on the consumers’ preferences for the strong content with a higher share of captive consumers creating more distortion.====Our first main result is that with price discrimination (zero-rating), the ISP can extract more surplus from consumers than with quality discrimination (prioritization). Hence, in the absence of revenues from CPs, the ISP prefers zero-rating to prioritization and net neutrality. Moreover, we show that the ISP chooses prioritization only when traffic is highly valuable for CPs and congestion is severe. In all other cases, i.e. for low congestion and/or low value of traffic, the ISP chooses to use zero-rating. The intuition is the following. Without payments from the CPs, the ISP prefers to discriminate in price rather than in quality, i.e. it prefers zero-rating to prioritization. Then, the ISP will choose prioritization only if it receives a large payment from the prioritized content provider. This will be the case when traffic has much value for content providers ==== prioritization is associated with substantial benefits on the market for content, i.e. when it creates large distortions in content consumption, which is the case when congestion is severe. In all other cases, the ISP chooses zero-rating.====When prioritization is implemented, it can benefit both the ISP and the consumers. The reason is that the ISP has less ability to extract surplus from consumers under prioritization than under zero-rating, but it can compensate this by larger payments from the content provider side. Thus for severe congestion, the price paid by consumers under prioritization will be low, while the contribution of the CPs will be high and there is a form of ‘cross-subsidization’  of consumers by the CPs. Such a situation can arise as the ISP is a two-sided platform and a higher price on one side (the CP side) can lead to a lower price on the other side (the consumer side). Hence, consumers might be better-off under prioritization than under zero-rating and, in some cases than under net neutrality. We show that the interests of the consumers and the ISP could be congruent on the choice of prioritization, whereas it is never the case for zero-rating.====Our second main result is to show that both under zero-rating and prioritization, the ISP will privilege the weak content provider.==== Discrimination against the strong content is a way for the ISP to mitigate the initial asymmetry between CPs and increase the revenue it collects from consumers by making them pay relatively more to consume the strong content. Hence, the ISP prefers to privilege the weaker content provider. Furthermore, in a fixed demand configuration, prioritization and zero-rating only displace content consumption and the market share lost by one CP is gained by the other one. Therefore the two CPs will bid the same amount to be the privileged content if they value traffic equally.====Next, we relax the limited asymmetry assumption between content types and we consider cases in which the strong content provider can attract more consumers, more traffic or more advertising revenue than the weak one. Intuitively, when the strong content has a large advantage either in the size of its home market, or in its reach of new potential consumers, or in its revenue-generating ability; zero-rating the strong CP or giving priority to it may generate more revenue from both the user side and the CP side. We show that this asymmetry must be large enough in order to convince the ISP to give a preferential treatment to the strong instead of the weak content. In other words, we derive upper limits on asymmetry between content types.====Finally, we consider several further extensions of the model. First, we consider the possibility for the ISP to combine zero-rating and prioritization. In this case, priority will be, as in the baseline model, given to the weak CP, but we show that zero-rating reinforces prioritization only when congestion is limited. Otherwise, it is optimal to prioritize the weak content and zero-rate the strong one. In the second extension, we consider the incentives of the ISP to invest in network improvements, and we show that, under prioritization, incentives to invest are suboptimal. The intuition is that the more severe congestion is, the more impact prioritization has on the market for content and hence, the more surplus the ISP can extract from content providers. For this reason, incentives to invest in capacity extension are limited. On the contrary, we find that under zero-rating, the ISP’s investment incentives are socially optimal. Third, we discuss why we believe that our results considering a monopolistic ISP apply to some settings with ISP competition as well.====Our research project is closely related to the rich body of literature in theoretical industrial organization about net neutrality. This literature has focused on four practices that are violations of the net neutrality principle: exclusion of lawful content (====), throttling, prioritization and zero-rating.====Concerning prioritization, in the context of congested networks, a two-tiered internet service with a fast lane for prioritized content can be considered as a more efficient tool for traffic management than a strict net neutrality rule, see ==== for an explicit analysis of this issue. Another important aspect of prioritization is that it changes the market for internet content, which in turn affects the optimal prices ISPs charge to both sides of the market.==== This aspect of prioritization is also made explicit in our modeling of prioritization (and zero-rating as well). Finally, departing from a neutral internet and pricing congestion also changes the incentives of the ISP to invest in network capacity. There is no consensus in the literature on the impact of prioritization on the investments in capacity. In our model, prioritization reduces the incentives of the ISP to upgrade the network as congestion is then a valuable resource.====Our modeling of prioritization is closest to ==== and Cheng et al. ==== as we also use CPs located on a Hotelling segment populated with single-homing consumers. The logic of these models is also similar: prioritization creates value by distorting the content market, and the ISP is able to capture part of the increased value from the prioritized CP. Some of our results are in line with previous findings. ==== (====, proposition 1), finds that for high levels of advertising revenue the ISP prefers prioritization to net neutrality. In our model, prioritization is also the ISP’s preferred regime for high advertising revenue whenever it is combined with a relatively high congestion level. Otherwise, the preferred regime is zero-rating and never net neutrality. Clearly, this result has welfare effects with a potential to alter policy implications. Our main contribution is considering zero-rating agreements as an alternative business strategy to prioritization for the ISP to create distortion on the content market.====To date there have been few formal economic studies modeling zero-rating. Jullien and Sand-Zantman ==== model zero-rating as a coupon from content providers to end users, a potential way to overcome the misallocation problem stemming from the free content business model. ==== investigates the trade-off between congestion and increased utility from consumption by modeling data caps explicitly, distinguishing the two types of zero-rating programs (open and exclusionary) currently in use. Jeitschko et al. ==== focus on vertical integration between a content provider and the internet service provider in a zero-rating regime. Inceoglu and Liu ==== focus on the consumer side of the internet market and model zero-rating as a screening device for price discrimination. Hoernig and Monteiro ==== analyze zero-rating when payments from content providers to the ISP are banned and the profitability of zero-rating arises solely from network effects. ==== model zero-rating with two CPs that are located at the extremes of a Hotelling line, and highlight the importance of the transportation cost parameter in determining the equilibrium market outcomes. ==== model exclusionary zero-rating offers with and without side-payments from the CPs to the ISP, assuming tariffs with data caps and content-specific prices; and compare the outcomes to conventional, i.e. one-sided pricing practices. Finally, ==== highlight the prevalence of throttling of zero-rated content. Our model is well suited to analyze this case, as throttling of a content provider can be seen as prioritizing its rivals.====The fact that congestion is still a salient feature of the internet today has recently been documented by the thorough empirical studies of ==== and ====. In particular, by analyzing fixed residential broadband data they estimate a large willingness-to-pay to avoid congestion. Several factors, including hard technological constraints make mobile internet even more susceptible to congestion, including the scarcity of spectrum and limits on cell size reduction (see e.g. ====). Indeed, ==== find that in most countries packet loss, a standard measure of congestion, is higher on the mobile internet.====Finally, our model is also related to the literature of simultaneous horizontal and vertical product differentiation. The two seminal articles in the literature; ====, and ==== show that firms choose maximal differentiation in one dimension and minimal differentiation in the other dimension. One finding of our model is similar in spirit to these results. In particular, when prioritization and zero-rating are both available tools for the ISP to differentiate CPs it finds it optimal to use only one of them, zero-rating.",Prioritization vs zero-rating: Discrimination on the internet,https://www.sciencedirect.com/science/article/pii/S0167718720300850,17 September 2020,2020,Research Article,157.0
"Grossmann Martin,Hottiger Dieter","Department of Business Administration, University of Zurich, Plattenstrasse 14, 8032 Zurich, Switzerland,School of Business, Lucerne University of Applied Sciences and Arts, Zentralstrasse 9, 6002 Lucerne, Switzerland","Received 28 March 2018, Revised 21 July 2020, Accepted 26 July 2020, Available online 2 September 2020, Version of Record 7 October 2020.",https://doi.org/10.1016/j.ijindorg.2020.102658,Cited by (2),"We analyze the effects of future ==== on contestants’ investment in a dynamic contest model. Contestants invest in two consecutive contests to win a prize in each period. The loser of the first-period contest can be liquidity-constrained in the second period due to too little remaining ====. The winner of the first contest can reinvest the prize in the second contest. We show that future ==== mainly affect the imbalance of the contest in the future but not today. Surprisingly, larger contest prizes decrease contestants’ future investments and amplify the imbalance of future contests. However, the contest organizer can reduce this imbalance by increasing the share of the prize money in the second contest.","In competition, the importance of having immediate success is crucial for most firms in any sector. Startups in particular must be successful in their early stages in order to keep previous investors and/or attract new investors (see Stuart and Abetti, 1987).==== Analyzing over 100 startup failure post-mortems, the research firm ==== concludes that the second most common reason for a startup’s failure is its running out of capital.==== The need for early success is even more important in countries in which capital markets are highly imperfect==== as the lack of a well-functioning capital market can foster liquidity problems for firms. Liquidity constraints limit the scope of action for unsuccessful firms and imply a disadvantage in future competitions against the successful competitors.====Regulations can contribute to liquidity constraints. In the European Soccer industry, for instance, the Union of European Football Associations (UEFA) implemented a regulation called Financial Fair Play (FFP) in 2013/2014 to reduce the excessive expenditures (e.g., Madden, 2015, Sass, 2016).==== According to FFP, the expenditures of clubs are limited to their revenues. Clubs are allowed to smooth their break-even condition over a period of three years, though. But sporting success will certainly be more decisive for the clubs’ investment opportunities under the FFP rule. On the one hand, winning the championship guarantees additional income for clubs, so that more resources are available for investments in the upcoming season. On the other hand, unsuccessful teams may be restricted in the subsequent season if their financial resources are limited and therefore profitable investments cannot be made. Thus, the FFP policy tightens the connection between revenues and expenditures.====In this paper, we use a dynamic contest model to analyze the effects of liquidity constraints on investment and competitive balance. Two contestants compete to win an exogenous prize in each of two consecutive contests. Contestants are endowed with an initial wealth that they can irreversibly invest to increase their probabilities of winning the contest in each period. In period 1, both contestants partly invest their (financial) resources, and the relative investments determine the probability of winning the contest prize of period 1. The winner of the first contest receives the contest prize. As this prize can be reinvested, the winner has greater wealth available for investments in the second contest. The analysis provides insights into the influence of future constraints on today’s investment behavior, the strategic relevance of having a successful start in repeated contests, and the emergence of inequality. Specifically, we show that a tighter liquidity constraint reduces investments in both periods as well as the competitive balance in the second period. Moreover, a larger contest prize for both periods can, surprisingly, lead to smaller second-period investments and a larger imbalance of the second-period contest. Our analysis suggests, however, that the contest organizer may counteract the emerging inequality and thus solve the problem. Shifting the prize money partly to the future contest reduces the imbalance of the future contest.====Applied to the soccer industry, our results imply that a regulation such as FFP may generate an unwanted side-effect, as leagues become more imbalanced under the FFP policy. According to the uncertainty of outcome hypothesis of Neale (1964), an imbalanced league has a negative influence on the suspense for spectators and makes leagues less attractive. As a consequence, league revenues decrease. Moreover, larger prizes may potentially jeopardize the long-term success of a league because they lead to a larger imbalance of future contests. This result theoretically explains the following empirical finding by Pawlowski et al. (2010). In the 1999/2000 season, the UEFA significantly increased their payouts to clubs participating in the UEFA Champions League. The authors empirically find that the competitive balance subsequently decreased in the major European soccer leagues. According to the uncertainty of outcome hypothesis, this imbalance can potentially harm leagues in the long run.====Our analysis is based on a classical Tullock contest. Thus, we have to embed our paper in the contest literature. In a contest, competitors invest irreversible resources, namely money, time, or effort, to win a prize. A variety of different contest types exist: rent-seeking, lobbying, patent race, litigation, sports tournament, beauty contest, and so forth.==== Although the corpus of specific literature on dynamic contests as well as on contests with liquidity constraints is large, little attention has been paid to the combination of dynamic contests and liquidity constraints. Several articles consider contestants’ behavior in multistage contests==== that link the different stages in various ways. Möller (2012) and Clark and Nilssen (2013), for instance, provide theoretical models in which past effort or past success improves a contestant’s future ability. Both articles analyze the contest organizer’s optimal allocation of the fixed prize money between and within two consecutive contests in order to maximize total effort. Möller (2012) rationalizes the organizer’s choice of multiple prizes in the first contest to preserve incentives in the second contest. Clark and Nilssen (2013) highlight the organizer’s trade-off: higher prizes in the first contest increase incentives in the first period and reduce the effort costs in the second contest. But, at the same time, the remaining prize decreases in the second contest, so that the contestants’ incentives diminish. Schmitt et al. (2004), Grossmann et al. (2010) and Grossmann et al. (2011) analyze multistage contests in which the contestants invest and accumulate capital over time. The capital can partly be reused in subsequent contests. The accumulated capital determines the contestants’ competitiveness at each stage. Due to the reuse of capital, incentives to invest are higher in the early stages. While Schmitt et al. (2004) consider symmetric contestants and empirically test the model, Grossmann et al. (2010) and Grossmann et al. (2011) concentrate on the speed of convergence of the contestants’ competitiveness when one contestant has a head start in the first period. Ryvkin (2011) analyzes a dynamic tournament theoretically and empirically. He assumes that a relatively higher level of effort on the part of contestant A compared to contestant B leads to A’s being fatigued in later stages. In line with the theoretical predictions, Ryvkin finds experimentally that contestants increase their effort in later periods. Moreover, contestants decrease their effort if fatigue costs increase. Some papers discuss the organizer’s possibility of biasing a contest by changing the information they convey to competitors. Analyzing a dynamic and noisy rank-order promotion contest, Meyer (1991) discusses how a contest organizer can optimally bias the contest sequentially depending on the history of the contest.==== She shows that the organizer distorts the final contest in favor of the leader of the previous rounds in order to identify the more talented contestant.====Liquidity constraints (or, similarly, budget constraints) are also discussed in the contest literature.==== Che and Gale (1997) compare the effects of budget constraints in lotteries and all-pay auctions. They conclude that rent dissipation is lower in an all-pay auction than in a lottery. Che and Gale (1998) show that caps on political lobbying can, surprisingly, increase the aggregate expenditures in a static all-pay auction. Kaplan and Wettstein (2006) extend the model of Che and Gale (1998) and study a situation in which contestants are able to exceed the caps. Even though this exceeding is costly, they show that the introduction of a cap decreases the aggregate expenditures.==== Stein and Rapoport (2005) discuss the effect of budget constraints in a two-stage game in which contestants compete against each other in separate groups and the winners of the first round thereafter compete against each other in a subsequent contest for one final prize. Stein and Rapoport conclude that the share of investments in different stages remains constant for a larger contest prize if the budget constraint is not binding. However, in the case of a binding budget constraint, a larger share of expenditures is invested in the first competition when the prize is larger.==== In a static contest model, Grossmann and Dietl (2012) conclude that an underdog’s profit can increase for a tighter liquidity constraint. This means that an underdog may prefer a situation in which his wealth is lower because the favorite plays less aggressively and, therefore, the competition becomes less intense. Dynamic aspects of liquidity constraints are not considered in their paper.====The novelty of this paper is its analysis of a dynamic contest in which a contestant’s future budget depends on past success. The literature has neglected this aspect so far. Our setting is interesting because the participants choose their investments while being uncertain about their future budgets. The model delivers new insights how prize and initial wealth variations change contestants’ behavior as well as the balance of the competition.====The remainder of this paper is structured as follows. Section 2 introduces the model and presents the main assumptions. Furthermore, we discuss the optimal behavior of contestants in the absence of liquidity constraints. Afterwards, we consider a one-shot contest with an exogenous and unequal distribution of wealth, reflecting the second-period contest. Section 3 starts with an analysis of binding liquidity constraints in which an unequal distribution of wealth in the second period results endogenously. Section 4 presents an extension of the model in which we analyze the contest organizer’s allocation of the prize money between the two consecutive contests. Section 5 concludes the paper.",Liquidity constraints and the formation of unbalanced contests,https://www.sciencedirect.com/science/article/pii/S0167718720300813,2 September 2020,2020,Research Article,158.0
Heinsalu Sander,"Research School of Economics, Australian National University HW Arndt Building, 25a Kingsley St, Acton ACT 2601, Australia","Received 18 December 2019, Revised 7 August 2020, Accepted 14 August 2020, Available online 26 August 2020, Version of Record 6 September 2020.",https://doi.org/10.1016/j.ijindorg.2020.102660,Cited by (2),"I analyse a market with asymmetric information and interdependent values. Accessing the market is costly and stochastic, e.g. requires lobbying, licences or R&D. For a range of parameter values, interventions that raise production or entry costs for all seller types increase trade, investment and payoff for all types. This effect is driven by a novel feedback loop between the market composition and the investment in access.","I study a market with private information in which the informed party can increase her probability of entry at a convex cost. A higher production cost for the seller implies a larger valuation for a buyer. Examples of costly stochastic entry are obtaining qualifications to enter the labour market, lobbying to enter a regulated industry and developing a new product. Higher quality typically costs more, e.g. a more productive worker has a better outside option, so asks for higher pay. Conversely, low quality sellers are more eager to trade—there is adverse selection.====In markets with asymmetric information, raising the gains from trade has been found to reduce adverse selection and increase trade and welfare (Akerlof, 1970). I show that with costly investment to enter, the uncommon policy intervention of raising production or entry costs for all types (decreasing the gains from trade) can Pareto improve payoffs and increase investment in entry. These comparative statics are the polar opposite of the large literature on adverse selection. The difference is due to endogenous entry at a convex cost, as shown by the main result and the comparison with the benchmark of exogenous entry.====The players are an informed seller and uninformed buyers. The seller is of high or low type, and knows the type. Buyers only have a common prior belief about the type. The high type’s product is more valuable to the buyers than is the low type’s, and costs more to supply.====The seller first chooses the probability of entering the market. The cost of raising this probability above its lower bound is strictly increasing and convex.==== If the seller enters, then the buyers offer prices without observing the seller’s type or cost. The seller chooses to accept or reject the highest price.====The probability of entry may also be interpreted as production capacity that the buyers do not observe,==== for example due to non-exclusive dealing (the seller distributes the quantity produced among a continuum of buyers). If the buyers offer prices before the seller chooses entry, then capacity may be interpreted as quantity produced.====I characterise the parameter region in which decreasing the costs of entry or production reduces all payoffs, even when all seller types are strategic, unlike in the previous literature. The fraction of cost reductions that cause a Pareto worsening is large in numerical simulations.====The comparative statics are driven by a novel feedback loop between the investment in entry and the market composition, which determines the price. To describe this feedback, suppose the trading surplus increases or the entry cost falls. Iterated best responses of the seller and buyers converge to the new equilibrium as follows. First, both types of the seller increase entry. If the low type’s entry probability rises relatively more, then the average type in the market becomes worse. In response, the buyers offer reduced prices, which makes both seller types decrease entry. The fall in the entry probability is proportionately larger for the high type because the high type enters with a smaller probability to begin with. If the high type’s entry reduction is relatively larger, then the expected value of an entering seller’s product to the buyers decreases. The resulting price fall further reduces entry, especially by the high type, etc. The high type may finally drop out of the market, so her payoff falls to zero. The low type receives a low price, so a low payoff. The buyers obtain less quantity and quality.====The model is formalised in Section 2 and the main comparative statics derived in Section 4. The benchmark of exogenous entry in Section 3 demonstrates that the feedback loop relies on endogenous entry decisions. This explains why in adverse selection without investment in entry, an increase in the trading surplus always increases trade and welfare. Section 5 discusses two applications in detail: the job market for entry-level lawyers and the market for military services of rebel groups in Syria. Policy implications are presented in Section 6. The Appendix shows the intuition is robust and is supported under variations of the model assumptions, such as buyers screening sellers before entry.====The comparative statics are robust to having type-specific entry costs, a continuum of types, a supply curve, a demand curve, and a monopsonist or an oligopsony (Appendices A.1–A.5). Similar results occur when the uninformed buyers screen the seller (A.6). Appendix A.7 considers a lump-sum entry cost,==== which is popular in the literature, and shows that a Pareto worsening cannot occur in response to a decrease in the entry or production costs. The preceding economics literature has used lump-sum entry costs and thus has not found the counterintuitive comparative statics of this paper.==== The seminal model of adverse selection is Akerlof (1970), to which Kremer (1994); . (1996) and Kremer and Morcom (1998) add investment in entry at a quadratic cost. Their application is epidemiology and the focus is on the behaviour of the types with high and low probability of infection, as well as how social contacts change the type. The current paper uses fixed types and a general convex cost and derives the opposite comparative statics to Akerlof (1970) in economic applications, focusing on the payoffs to each type.====Chen and Zhang (2018) model buyers who sequentially search firms which decide whether to enter at a fixed cost. Buyers observe both price and quality, which deters the worst potential entrants, resulting in advantageous selection, the opposite of the adverse selection in the present paper. If many firms enter in Chen and Zhang (2018), then additional entry harms the buyers through the direct effect of entrant quality, because the worst firms obtain the lowest profit and thus enter last. The marginal benefit of increased variety is small at that point, so does not compensate for the worsening quality. By contrast, in the current work, the worst types make the most profit and enter first, so additional entry provides higher expected value to the buyers, especially when a large fraction of potential entrants enter.====In Auster et al. (2017), a privately informed buyer pays a lump-sum cost to increase the probability of making a take-it-or-leave-it offer to one uninformed seller. The seller’s cost does not depend on the buyer type (private values). The low-value buyer has a dominant strategy. The uninformed seller’s payoff may increase in his cost, but the informed buyer’s payoff is constant. By contrast, in the current paper, each type’s best response depends on the action expected from other players. Payoffs may rise in both the production and the entry costs of the informed side.====Iannaccone (1992) and Poutvaara and Wagener (2010) both use an adverse selection market with a lump-sum entry cost to model religious sects. In Iannaccone (1992), the utility of sect members depends on the average type in the sect. The types imposing a higher cost on the sect have a lower valuation, unlike in the current paper, and there is no feedback loop. A lump-sum cost of entry helps the sect screen out freeriders, which raises the payoff of the sect and reduces that of the freeriders. In the present work, a higher entry or production cost may improve every player’s payoff. In Poutvaara and Wagener (2010), potential leaders start a sect at a lump-sum cost. A leader knows his ability to extract money from sect members. Potential members join if the expected payment is below the fixed benefit of membership. The focus is not the comparative statics, but the set of equilibria in which reducing the cost of founding a sect improves welfare. The current paper characterises the parameter region in which higher costs improve welfare.====In a dynamic market with adverse selection, Zryumov (2015) and Kim and Pease (2017) model the low type seller’s investment to enter early or raise the matching rate. The high type cannot leave the market (cease search) even when obtaining zero surplus. In the current paper, both types invest to increase entry and can choose to stay out. Entering high and low types impose opposite externalities on other traders, so ==== it is not clear whether the results in the literature survive endogenising the investment of the high type. The present paper shows which results are overturned and checks the robustness of the remainder.====In Zryumov (2015), greater gains from trade make the average quality and quantity spike up, and then fall to a new steady state. The new quantity is greater and quality worse because the entry of high types is fixed by assumption and low types enter more in response to a larger trading surplus. In the current work, endogenous entry of both types leads to more nuanced comparative statics: quantity and quality may fall or rise in the gain from trade. In Zryumov (2015), both types gain from a rising trading surplus, but in the present paper, both may lose or gain, or the low type may gain and the high lose. In the current paper, high types enter inefficiently little, but low types may enter too much or too little. In Zryumov (2015), the entry of high types is fixed, thus efficient, but the entry of low types is too large.====Kim and Pease (2017) assume severe adverse selection, which in a static model would cause market breakdown. In the present paper, by contrast, adverse selection would not affect trade under fixed participation. In Kim and Pease (2017), a privately informed seller sequentially contacts buyers, each of whom offers a price. Similar monopsony power is studied in Section A.5 below. In Kim and Pease (2017), a smaller entry cost increases the low type’s payoff and decreases that of the buyers. The current paper shows that reducing the entry and production costs of both types may decrease all players’ payoffs.====The companion paper, Heinsalu (2017), examines two-period adverse selection with the entry cost paid only in the first period. The focus is on the tradeoff between impatience and signalling, and the comparative statics in the first period are similar to the current paper. Two-period adverse selection is also studied by Fuchs et al. (2016). They assume costless entry and examine the impact of price observability, not increased gains from trade. Their comparative statics are standard: reducing costs increases trade and welfare.====Camargo and Lester (2014) and Chiu and Koeppl (2016) study a dynamic search and matching market without entry. In Chiu and Koeppl (2016), announcing later asset purchasing programmes can increase total surplus because selling pressure builds up from sellers randomly needing liquidity. In the present work, delaying a subsidy may also improve welfare, but the reason is different: feedback between market composition and entry. In Camargo and Lester (2014), subsidising trade only initially can improve welfare because the expectation of a future subsidy delays trade. The current paper finds the opposite with a convex entry cost in a one-shot market: subsidising trade may reduce welfare and delay trade.====One of the models in Lauermann and Wolinsky (2016) has an informed buyer investing to enter and then sequentially searching sellers who each receive a signal about the buyer’s type. The focus is on comparing information aggregation at different signal structures, not on payoffs decreasing in the gains from trade. Kaya and Kim (2018) assume exogenous seller entry and sequential buyers, each with a noisy signal about the seller’s quality.====Pagnozzi and Piccolo (2017) model seller entry at a lump-sum cost before a single buyer screens the seller types. They show that total welfare may increase in the entry cost in one of the equilibria, but all players’ payoffs decrease in the entry cost in the other. The present paper demonstrates a Pareto worsening of payoffs when competing buyers offer prices, and extends the result to monopsony and pre-entry screening in Sections A.5 and A.6.====Creane and Jeitschko (2016) examine an adverse selection market with a fixed cost of entry that is paid before learning the type. Paying before knowing the type is similar to the exogenous stochastic entry in Section 3, so a higher trading surplus raises payoffs and trade. The current paper has type-dependent entry and shows that greater gains from trade may reduce all payoffs and trade.====In Chemla and Hennessy (2014), Fukui (2018) and Caramp (2017), a low quality asset originator can pay a fixed cost to become high quality. An equivalent formulation in Vanasco (2017) is an originator who screens assets at a convex cost, randomly finds a better asset and improves asset quality. As gains from trade rise, trade increases and the quality of assets decreases, because when all assets are traded, there is no incentive to improve their quality. This moral hazard mechanism differs from the feedback between adverse selection and entry in the current paper. Here, higher gains from trade reduce trade, payoffs and the quality of assets traded, so quantity and quality co-move, unlike in models of asset improvement.====Philippon and Skreta (2012) and Tirole (2012) use mechanism design in a static lemons market with exogenous entry to show intervention cannot increase welfare. In the current paper, endogenous entry allows unusual interventions (raising the cost for all types) to Pareto improve payoffs.",Investing to access an adverse selection market,https://www.sciencedirect.com/science/article/pii/S0167718720300837,26 August 2020,2020,Research Article,159.0
Casner Ben,"The Ohio State University 410 Arps Hall, 1945 N. High St. Columbus, OH 43210 United States","Received 12 August 2019, Revised 8 May 2020, Accepted 10 August 2020, Available online 24 August 2020, Version of Record 4 September 2020.",https://doi.org/10.1016/j.ijindorg.2020.102659,Cited by (10),"This article explores why market platforms do not expel low-quality sellers when screening costs are minimal. I model a platform market with consumer search. The presence of low-quality sellers reduces search intensity, softening competition between sellers and increasing the equilibrium market price. The platform admits some low-quality sellers if competition between sellers is intense. Recommending a high-quality seller and this form of search obfuscation are complementary strategies. The low-quality sellers enable the recommended seller to attract many consumers at a high price and the effect of the recommendation is strengthened as low-quality sellers become more adept at imitating high-quality sellers.","Platforms which connect buyers and sellers suffer a loss of reputation if they host too many low-quality sellers. Reputational effects can have a significant impact on customer retention (Nosko and Tadelis, 2015). However, many platforms seem unwilling to curate the quality of sellers on their marketplace. For example, many sellers of low-quality exercise nutritional supplements on Amazon use fake reviews to imitate high-quality sellers. Amazon seems reluctant to respond to this problem even though many of these fake reviews are easy to detect algorithmically.==== While the cost of screening could potentially explain some of this behavior, costs cannot explain why this reluctance appears even in cases when simple measures could significantly increase the average quality of products on offer. The computer game platform Steam has notoriously low barriers to entry, with one notable example where users found that software they had purchased was just an empty folder. It would be a relatively simple matter to ensure that a game must at least run before the platform allows it to be sold.==== It therefore seems likely that these platforms have an incentive to not screen out low-quality sellers.====To address the question of why a platform might want a lax screening policy I present a model where a monopoly market platform hosts sellers in exchange for a percentage of their revenue.==== Sellers can either be low- or high-quality, and the platform sets the proportion of low-quality sellers on its market. Consumers participating in the platform’s market must search before they can purchase from a seller. The presence of the low-quality sellers creates two countervailing effects: The ==== reduces consumer search intensity, softening competition between sellers and raising equilibrium market prices. The ==== reduces consumer confidence in the quality of goods on the market which reduces their willingness to pay. The platform will admit a positive proportion of low-quality sellers if the obfuscation effect is stronger than the lemon effect. If the platform is able to recommend a high quality seller, then that seller will benefit from obfuscation without being subject to the lemon effect which further increases platform profits and increases the likelihood that it will adopt a lax screening policy.====The obfuscation effect is so named because I show that the low-quality sellers can act as a search obfuscation mechanism (Ellison and Ellison, 2009). Consumers never knowingly purchase from a low-quality seller, so if a consumer searches and encounters a low-quality seller then they will pay the search cost to visit another seller, and they will continue searching until they encounter a seller they believe to be high-quality. The possibility of encountering a low-quality seller upon searching decreases the value of searching as the proportion of low-quality sellers increases, which in turn leads to higher equilibrium prices.==== Although decreasing the value of search drives some consumers away from the market, the platform increases seller profits, and consequently its own revenue per consumer, by softening competition between sellers.====The low-quality sellers in my model attempt to fool consumers into believing that they are high-quality sellers. Consumers know the probability that low-quality sellers successfully imitate high-quality sellers, but cannot distinguish between a high-quality seller and a low-quality seller who is successful in their deception. The lemon effect reflects the fact that the presence of low-quality sellers reduces consumers’ confidence in the quality of any prospect they are considering because it is possible that it is a low-quality seller in disguise. Consumers will have diminished willingness to pay for sellers’ products and are less eager to participate in the platform’s market because of the possibility that they may be scammed. The lemon effect is harmful to the platform, so if it is significantly stronger than the search obfuscation effect then the platform does not admit any low-quality sellers.====In independent work, Barach et al. (2019) show that highlighting recommended sellers can be a powerful tool for platforms. At first glance this practice might initially seem to be contradictory to the obfuscation effect in my model. In fact, by combining the literatures on search obfuscation and seller recommendations, we can see that recommending a high-quality seller and obfuscating search via low-quality sellers are ==== for the platform.==== It is profit maximizing for the platform to recommend a high quality seller, so the recommendation is credible and the recommended product is not subject to the lemon effect. Consumers are therefore willing to pay a higher price for the recommended seller’s product than the product of a random seller of uncertain quality. Consumers, the recommended seller, and the platform are better off with the recommendation for any fixed proportion of low-quality sellers. However, by mitigating the lemon effect the recommended seller reduces the platform’s incentive to screen. The ability to recommend a seller can even cause the platform to allow low-quality sellers under parameter sets where it would not absent the ability to recommend. Consumer gains from the recommendation may thus be diminished or even eliminated by an increase in the proportion of low-quality sellers.",Seller curation in platforms,https://www.sciencedirect.com/science/article/pii/S0167718720300825,24 August 2020,2020,Research Article,160.0
"De Silva Dakshina G.,Hubbard Timothy P.,Kosmopoulou Georgia","Department of Economics, Lancaster University Management School, Lancaster LA1 4YX, UK,Department of Economics, Colby College, 5242 Mayflower Hill Drive, Waterville, ME 04901, USA,Department of Economics, University of Oklahoma, 729 Elm Avenue, Norman, OK 73019-2103, USA","Received 12 June 2019, Revised 11 August 2020, Accepted 16 August 2020, Available online 24 August 2020, Version of Record 15 September 2020.",https://doi.org/10.1016/j.ijindorg.2020.102661,Cited by (3),"In an effort to accommodate a change in the Federal Highway Administration’s goals towards race-neutral methods concerning the involvement of Disadvantaged Business Enterprises in contracting, the Texas Department of Transportation created a bidder training program. Using ten years of data, we examine the effects this program had on bidder behavior, project costs for the government, and the ability of these firms to compete. Unlike other policies that target these firms, we find the program generated substantial savings for the state which come at a very low cost.","The U.S. Federal Highway Administration (FHWA) has used government policies since at least the early 1960’s to encourage minority participation in procurement contracting. Many states employ bid preference programs, which discount the bids of qualified firms for the purpose of evaluation. Other programs require government agencies to set aside a certain percentage of a contract to be subcontracted out to disadvantaged business enterprises (DBEs) or other qualified firms. Over the decades and largely in response to court decisions (see, for example, the Supreme Court’s 1995 ruling in ====, U.S. Report 515 U.S. 200), the nature and administration of DBE programs has changed. Individual state agencies that administer the programs, are asked to achieve as much of the goal as possible by “race-neutral methods” before employing other perhaps identity-conscious policies.==== For example, qualified DBE firms are not simply determined by belonging to a particular demographic group (e.g., being owned by a minority, veteran, or woman) but also by their economic circumstances (e.g., small business enterprises—SBEs) or whether such firms have received a “fair” share of state business (e.g., historically underutilized businesses—HUBs).====In response to the shift in the disposition of FHWA policy, and because the Texas Department of Transportation (TxDOT) felt having a diverse set of active firms was critical to the competitiveness of its transportation industry, TxDOT created its own Learning, Information, Networking, Collaboration (LINC) training program in September 2000. The rationale was that many DBEs, SBEs, and HUBs interested in doing business with TxDOT had not been successful and faced disproportionate barriers in doing business with the Department. As such, the program was eligible only to firms certified as DBEs, SBEs, and HUBs. The LINC program assigned participating firms a mentor from TxDOT’s Business Opportunity Programs Section that helped participants understand business opportunities, provided information to assist them in bidding and executing TxDOT contracts, and introduced the firms to other contractors to foster networking opportunities. Participants received construction management training which included instruction on pre-qualification requirements and guidance on searching for contracts. Most importantly, the program’s purpose was to prepare these firms to bid and perform on TxDOT contracts. For example, part of the training program involves working with “providers” which are firms on contract with TxDOT to supply marketing, estimating, and bidding services. By focusing on bidding and the execution of contracts the LINC program helps maintain and support the role such firms play in the TxDOT procurement industry.====Texas, being both large and diverse, makes for a good place to study such a program. The state boasts the second-largest state economy in the U.S. and a diverse population with 37.62% of its residents identifying as Hispanic and 11.94% as Black in the 2010 Census. During our ten-year sample period which spans September 1997 to August 2007, the total value of contracts awarded to LINC-eligible bidders was $1.98 billion. We use all procurement contract data from this period to examine the impact of the LINC program on the participation decisions of firms, bidding behavior, their likelihood of success, and ultimately their potential for remaining active in the industry.====We find the most convincing effects LINC has on firms is with respect to their bidding behavior—LINC-trained bidders submit more competitive tenders after graduating from the program. Average bids from LINC graduates are lower relative to firms that are ineligible for the program as well as relative to those firms which are eligible but have not undergone training. A bulletin is circulated to all prime contractors interested in working with TxDOT announcing the firms that have completed the LINC training, making other industry participants aware of which firms have graduated from the program. When rivals learn that a LINC-trained firm holds plans for a certain project, an indirect competition effect results in which ineligible firms (by far our most frequently-observed class of bidders) bid lower than they otherwise would have. The lower bids carry through to generate cost-savings for TxDOT in two ways: first, when LINC-trained firms win their bids are lower, on average, than those of all other firms; second, when other firms compete at auctions which attract interest from LINC-trained firms, the average winning bid is also significantly lower. These two channels generate substantial savings for the state. In contrast, the LINC program requires a budget of only about $200,000. Moreover, eligible firms that do not get trained are more likely to exit the industry than firms that are not eligible, but this concerning effect goes away for firms that graduate from the LINC program.====Our program evaluation relates to the work of researchers who have investigated alternative policies at procurement auctions which target the same firms qualifying for LINC. These policies include set-asides, bid preference policies, and minority subcontracting goals. Denes (1997) compared bids submitted for solicitations restricted to small businesses with unrestricted solicitations, finding that bids were no higher in restricted settings. He suggests that costs did not increase for the government because the contracts set-aside for small businesses attracted more bidders than the open contracts.====Bid preference schemes favor bids from qualified firms for the purposes of evaluation only, thereby making favored firms more competitive within a given auction. The effect of such programs on the government’s cost is ambiguous even at the theoretical level; see McAfee and McMillan (1989) and Hubbard and Paarsch (2009). Marion (2007) found that in data from California Department of Transportation (Caltrans) auctions for road construction contracts, the price paid by the state was 3.8 percent higher for auctions which used preferences. Krasnokutskaya and Seim (2011) also analyzed bid preference programs in Caltrans highway procurement contracts and found that the preferential treatment of small businesses creates losses in efficiency but no change in the overall cost of procurement. Rosa (2019) found that, while accounting for affiliation in firms’ costs (which the previous studies did not consider), the New Mexico Department of Transportation could favor resident (in-state) bidders by even more without realizing a major increase in its project costs.====Minority subcontracting goals are often used in federal procurement contracts and may constrain the make-or-buy decision of prime contractors, could require outsourcing production of tasks to less efficient subcontractors, and can affect the competition intensity in the subcontracting market. Marion (2011) used data from Caltrans to show that the subcontracting goals set for highway construction contracts in California raise DBE usage significantly, so that the constraints appear to bind. In fact, Marion (2009) found that after California’s Proposition 209 was passed (which prohibited DBE subcontracting goals concerning race or gender), state-funded contracts realized a 5.6 percent fall in prices relative to federally-funded projects which still involved subcontracting goals. De Silva et al. (2012) evaluated the impact of a federal subcontracting policy years after its original implementation and found that minority subcontracting goals did not increase procurement costs in Texas. Marion (2017) evaluated an exemption granted by the Iowa Department of Transportation for its subcontracting requirements to firms that had a history of actively involving DBE subcontractors. He found projects with affirmative action goals had higher bids than those without, and that this disparity increased when bidders could no longer be exempt from the subcontracting requirements. Most recently, Rosa (2020) proposed a model in which subcontracting regulations can inspire relationship building which might increase current-period costs, but can lead to savings in later periods that are driven by relationships formed. This insight can apply to the LINC program which has networking and collaboration as important elements.====While the LINC program applies to the same class of firms as these other policies, our work and findings differ from those empirical studies. The set-aside and preference policies as well as the subcontracting goals apply for a given auction, whereas the LINC program aims to improve the behavior and outcomes for participating firms in the industry, not just within one auction. In fact, at the initial LINC meeting, participating firms must sign an agreement acknowledging that the information provided at program sessions is general and not specific to a particular project. To our knowledge, we are the first to study the effects of a bidder-training program, which we’ve learned exists or is being introduced with small variations in the majority of U.S. states. Given the prevalence and interest in such training programs, we hope our work has important policy implications as there is potential for our findings to suggest alternatives to meet the FHWA’s original goals in a way that can actually generate clear cost savings (benefits), something that has not been demonstrated for set-asides, preference policies, and subcontracting goals.====We describe the LINC program in more detail in the next section. In Section 3, we summarize our data, which we use to develop some intuition about the program’s effects before considering our core empirical analysis in Section 4. While our focus is on TxDOT’s LINC program, other states do have similar opportunities for DBEs, HUBs, and SBEs. We have contacted representatives at every state’s Department of Transportation office and have learned two things: first, bidder training opportunities are quite common as more than thirty states have in place a program with many of these elements; second, Texas seems to be one of the first states to introduce such a program and its program seems to be one of the largest in terms of participation.====In our correspondence with employees at state offices we have learned that these programs which all have different names (e.g., Calmentor in California, Connect2DOT in Colorado, and Mission 360° in Rhode Island) are often administered through economic or local development offices. In general, such training programs seem to be on the rise. Some states have either implemented new programs (e.g., the Oklahoma Department of Transportation’s Small Enterprise Training Program) or are re-emphasizing or revamping old programs (e.g., the Washington Department of Transportation recently expanded its program targeting minority- and women-owned firms to include small businesses in general), and a number of representatives for states that do not currently have any programs indicated that they felt such opportunities would be a good idea. These programs are also not unique to Department of Transportation offices—the leading inspiration for such programs seems to be the Stempel Program for the Port of Portland in Oregon.==== Most programs have bidder training, formal mentoring, educational seminars, outreach components such as trade shows and business fairs, technical assistance, financial and management consulting services, and/or networking as key elements. Nearly all programs have goals of promoting effective business development by improving the performance of trained firms, ultimately hoping for a higher survival rate of such firms. As such, in Section 5, we consider whether firm survival in the industry has been affected by participation in LINC before concluding our research in Section 6.",An evaluation of a bidder training program,https://www.sciencedirect.com/science/article/pii/S0167718720300849,24 August 2020,2020,Research Article,161.0
Rosa Benjamin V.,"Virginia Tech, Department of Economics, 3053 Pamplin Hall, Blacksburg, VA 24061, USA","Received 1 May 2019, Revised 8 June 2020, Accepted 25 July 2020, Available online 12 August 2020, Version of Record 28 August 2020.",https://doi.org/10.1016/j.ijindorg.2020.102657,Cited by (2),"I study affirmative action subcontracting regulations in a model where governments use auctions to repeatedly procure goods and services at the lowest possible price. Through using disadvantaged subcontractors, prime contractors build relationships over time, resulting in lower subcontracting costs in future periods. I find that regulation in the form of a minimum subcontracting requirement expands bidder asymmetries, favoring prime contractors with stronger relationships over those with weaker ones. Simulations show that the manner in which relationships evolve determines not only the utilization of disadvantaged subcontractors but also the procurement costs attained under affirmative action.","Public procurement is a substantial part of government spending. In 2015, government procurement accounted for 29.1 percent of all government spending and 11.9 percent of GDP in OECD countries (OECD, 2017). Embedded within many of these procurement markets are affirmative action regulations, which are implemented to facilitate the participation of disadvantaged==== firms in government contracting. Although affirmative action can take on many different forms, a common brand of policies in procurement are mandatory subcontracting goals. Under these policies, a prime contractor (or prime) must set aside a percentage share of a contract for subcontractors (or subs) designated as disadvantaged.====A key factor in a prime’s disadvantaged subcontractor selection and associated subcontracting cost is their relationship with their pool of disadvantaged firms. In a needs assessment report written for the Minnesota Department of Transportation, surveys revealed that prime contractors rely on relationships to identify and hire disadvantaged subcontractors and that primes prefer to hire disadvantaged firms with whom they have existing relationships (The Improve Group, 2016). The report concluded that relationships and credibility were integral to primes meeting their affirmative action requirements. The economics literature indicates that relationships can serve as a mechanism to lower coordination costs, promote learning-by-doing (Kellogg, 2011), and establish reputations (Banerjee and Duflo, 2000). Gil and Marion (2013) find that prior subcontracting interactions reduce bids on California highway procurement contracts, which is suggestive of lower subcontracting costs. Thus, the goal of expanding disadvantaged subcontractor utilization and the cost of affirmative action are both tied to the relationships primes build with their disadvantaged subs.====A shortcoming of the literature is that it does not directly address this relationship dynamic. Instead, the literature often relies on a static framework or uses proxies for future demand to approximate continuation values.==== In this paper, I seek to fill that gap in the literature by investigating how dynamic relationship formation impacts procurement auctions in a model with affirmative action. To do so, I numerically solve for the Markov-perfect equilibrium of a repeated auction game with relationship dynamics and contrast the equilibria obtained with and without affirmative action subcontracting quotas. Primes in my model stochastically improve relationships through the continued utilization of disadvantaged subs, leading to an expectation of lower disadvantaged subcontracting costs in future periods. This relationship-building dynamic endogenously creates asymmetries between bidders, where primes with stronger relationships have a cost advantage over primes with weaker ones. By requiring primes to use disadvantaged firms, affirmative action expands this asymmetry and increases the marginal value from attaining a better state of relationships in the future. As a result, farsighted primes, which are primes that have a positive discount factor, have more of an incentive to subcontract with disadvantaged firms relative to myopic primes, which are primes with a discount factor of zero, and this incentive is amplified in markets with affirmative action.====The dynamic framework in this paper can answer several questions that a static framework cannot. Given that an objective of these programs is to remove barriers to the participation of disadvantaged firms in contracting,==== and in the subcontracting case, one of those barriers is a lack of established relationships – a dynamic analysis can explore how relationships evolve and how much affirmative action contributes to that evolution. Equally relevant is the long-run impact of removing affirmative action programs, which has been implemented through laws such as California’s Proposition 209.==== In this case, a dynamic structure can provide insights on how relationships – and therefore, disadvantaged subcontracting – will adjust when the quotas are removed.====I explore these questions through a model simulated under a range of different parameter and starting values. I find that the manner in which relationships transition between periods has implications for how affirmative action affects a given market. When relationships are long-lasting, affirmative action has negligible effects on bids yet leads to marked increases in disadvantaged subcontracting. When relationships deteriorate, affirmative action still improves subcontracting but at the cost of higher bids. These simulations highlight the importance of accounting for relationships in evaluating affirmative action programs.====My paper is closely related to the literature on mandatory subcontracting goals in government procurement contracts. Rosa (2018) studies Disadvantaged Business Enterprise (DBE) subcontracting goals in New Mexico using an estimated model of bidding and subcontracting that is similar to my paper. He finds that subcontracting goals may not lead to significant changes in bids because primes have to use a common pool of disadvantaged subs, leading to lower markups in equilibrium. I extend his model by including relationship dynamics and allowing for asymmetries in the cost of using disadvantaged firms. Other empirical papers on DBE subcontracting goals include Marion (2009) who finds that subcontracting goals significantly increased subcontracting and the winning bids and De Silva et al. (2012) who use a structural model to compare costs across contracts with and without subcontracting goals in Texas, finding negligible differences in project costs. Marion (2017) studies DBE subcontracting goals in Iowa comparable to the ones I investigate in my paper, focusing on how exemption policies impact DBE utilization.====My model borrows methods from the dynamic auction literature. In that literature, the model that is closest to mine is Jeziorski and Krasnokutskaya (2016). They use a dynamic model of subcontracting and bidding to explore how subcontracting affects capacity-constrained bidders, finding that subcontracting reduces bidder asymmetries by allowing primes to modify current costs and control future costs via backlog accumulation. Although I borrow their subcontracting model, our papers differ in dynamics; their paper has dynamic capacity, while my paper has dynamic relationship formation. This distinction fundamentally changes the role of subcontracting. In my model, primes subcontract with disadvantaged firms to gain a cost advantage in future periods through relationship formation; therefore, subcontracting ==== bidder asymmetry in future periods. My dynamics also differ from Saini (2012) – which is an earlier paper that proposed a framework for investigating equilibrium bidding with capacity dynamics, except without subcontracting. Our papers relate in their study of equilibrium behavior in dynamic auctions.====My simulation results and the asymmetries generated through relationship formation connect to the literature on asymmetric auctions and how auction formats rank in different settings. Kirkegaard (2012) and Kirkegaard (2014) use a mechanism design approach to rank revenues from the first-price and second-price auction formats when there are two bidders, finding that the first-price auction is generally more profitable under certain value distributions. Maskin and Riley (2000) consider three separate cases where it is possible to rank the revenue from first- and second-price auctions. In contrast to this literature, I explore auctions with varying degrees of affirmative action and relationship formation instead of auctions with different formats, although the model’s complexity limits me to simulations. Additionally, I consider how these different environments affect yet another outcome – disadvantaged subcontracting – along with the procurement analog of revenues (i.e., procurement costs). This additional consideration is an integral part of evaluating the effectiveness of affirmative action regulations.====The remainder of the paper has the following structure. Section 2 outlines the model, and Section 3 characterizes the model’s equilibrium. Section 4 contains solved examples of the model, which I use to show how affirmative action subcontracting regulations affect bidding and disadvantaged subcontracting and to study the long-run implications of subcontracting regulations. Section 5 concludes.",Affirmative action subcontracting regulations in dynamic procurement auctions,https://www.sciencedirect.com/science/article/pii/S0167718720300801,12 August 2020,2020,Research Article,162.0
"Lefouili Yassine,Pinho Joana","Toulouse School of Economics, University of Toulouse Capitole, France,Católica Porto Business School and CEGE, Universidade Católica Portuguesa, Portugal","Received 22 February 2018, Revised 16 July 2020, Accepted 17 July 2020, Available online 28 July 2020, Version of Record 18 August 2020.",https://doi.org/10.1016/j.ijindorg.2020.102656,Cited by (6),"We study the price and welfare effects of collusion between two-sided platforms and show that they depend on whether collusion occurs on both sides or a single side of the market, and whether users single-home or multi-home. Our most striking result is that one-sided collusion leads to lower (resp. higher) prices on the collusive (resp. competitive) side if the cross-group externalities exerted on the collusive side are positive and sufficiently strong. One-sided collusion may, therefore, benefit the users on the ==== side and harm the users on the ==== side. Our findings have implications regarding cartel detection and damages actions.","Several cartels involving newspaper publishers have been uncovered all around the world. In 1969, a U.S. District Court convicted of monopolization the two daily newspapers of general circulation in Tucson, Arizona, for jointly setting subscription and advertising rates.==== In 1996, several Venezuelan newspapers were convicted of forming a cartel to fix advertising rates for movie theaters.==== In 2005, the Brazilian antitrust authority fined the four largest newspapers in Rio de Janeiro for forming a cartel, after a simultaneous increase in cover prices by 20%.==== In 2010, the Croatian antitrust authority established that nine publishers of daily newspapers engaged in concerted practices that translated into a uniform increase in newspapers’ cover prices.==== In 2014, the Hungarian antitrust authority convicted the four major newspaper publishers in the country of price-fixing conspiracy.==== Also in 2014, the Montenegrin antitrust authority convicted the three major daily newspaper publishers in the country for price-fixing conspiracy.====Newspapers are two-sided platforms that enable the interaction between two distinct types of agents: advertisers and readers. As pointed out by Evans and Schmalensee (2013)(p. 2), ==== However, the theoretical literature on collusion in two-sided markets is remarkably scarce, which is striking given the empirical evidence on collusion in these markets.==== In particular, our understanding of ==== collusion among two-sided platforms, i.e., collusion that does not yield the monopoly outcome, is very limited.====In this paper, we study the price and welfare effects of collusion between two horizontally differentiated platforms, allowing for ==== degree of collusion. Our baseline model is an infinitely repeated version of the canonical Armstrong (2006)’s model, with single-homing on both sides and either positive or negative cross-group externalities. We first consider the scenario in which platforms engage in ====, that is, collusion on the prices set on both sides of the market. We show that the most profitable collusive agreement involves a price structure that minimizes the platforms’ incentives to deviate from the agreement. Using this result, we show that (optimal) collusion distorts the price structure (relative to the static Nash equilibrium) by leading to more rent extraction from the side in which the degree of differentiation is the highest. We also establish that two-sided collusion may either lead to higher prices on both sides of the market or to ==== prices on one side of the market (the one with the lowest degree of differentiation) and higher prices on the other side of the market. The latter scenario occurs when the degree of differentiation on one of the sides is sufficiently low (relative to cross-group externalities) and the discount factor is not too large.====We then consider the scenario in which platforms engage in ====, i.e., they set their prices cooperatively on one side of the market and non-cooperatively on the other side. Such a collusive behavior can be explained by the existence of coordination or antitrust costs that make it optimal for platforms to collude on a single side of the market, and has been documented empirically in the case of newspapers. For instance, using data from the Italian daily newspaper market from 1976 to 2003, Argentesi and Filistrucchi (2007) found empirical evidence that the four biggest newspapers colluded on cover prices, but found no evidence for collusion on advertising rates.====One-sided collusive agreements affect the prices on the non-cooperative side of the market because of the existence of cross-group externalities. If increasing the price on the collusive side softens competition on the non-cooperative side, the most profitable one-sided collusive agreement leads to supra-competitive prices on both sides of the market. This happens when the cross-group externalities exerted on the collusive side are negative. By contrast, if increasing the price on the collusive side strengthens competition on the non-cooperative side, the price on one of the two sides will be above its static Nash level, while the price on the other side will be below its static Nash level. This scenario occurs when the cross-group externalities exerted on the collusive side are positive. Interestingly, if these externalities are sufficiently high (relative to the degree of differentiation on the collusive side), the price on the collusive side is ==== its static Nash level, while the price on the non-cooperative side is ==== its static Nash level. As a result, one-sided collusion may benefit the users on the collusive side and harm the users on the non-cooperative side.====Next, we extend our analysis to a setting in which there is single-homing on one side of the market and multi-homing on the other side. A key difference between this extension and the baseline model (assuming full market coverage) is that ==== demand on the multi-homing side increases (resp. decreases) when prices on the multi-homing side decrease (resp. increase). Therefore, collusion in that setting can affect total welfare while it does not in our baseline model. We first show that two-sided collusion has no impact on the price on the multi-homing side but leads to a price increase on the single-homing side. Consequently, users on the multi-homing side and total welfare are not affected while users on the single-homing side are harmed. Turning to one-sided collusion we show that, as in the baseline model, collusion on a single side leads to a decrease in the price on that side and an increase in the price on the other side if the network externalities received by the collusive side are positive and large enough. We further establish that when collusion occurs on the single-homing (resp. multi-homing) side only, it raises total welfare if and only if the cross-group externalities received by the collusive side are strong enough (resp. positive and large enough).==== Ruhmer (2011) is the closest paper to ours. She also considers a repeated version of Armstrong’s model but her setting is substantially less general than ours. First, in the context of two-sided collusion, she focuses on perfect collusion (i.e., collusion at the monopoly prices), while we allow for imperfect collusion as well. This is natural when platforms are differentiated: in this case, perfect collusion may not be sustainable, while (profitable) collusion at other prices could be. The distinction between perfect and imperfect two-sided collusion turns out to be crucial: a focus on perfect collusion leads to the prediction that prices always increase if platforms collude on both sides of the market, while this is not always true under imperfect collusion. Second, in the context of one-sided collusion, Ruhmer (2011) focuses on the profitability and sustainability of a very specific collusive agreement in which platforms set the price on the collusive side at the maximum level that allows them to fully cover that side of the market (which is above the static Nash level). In contrast, we do not restrict the type of one-sided collusive agreements that platforms can achieve and show that they may find it optimal to decrease the price on the collusive side below its static Nash level. This explains, in particular, why one-sided collusion may be unprofitable in Ruhmer’s setting, while this is never the case in our setting. Finally, we examine both the scenario in which there is single-homing on both sides and a competitive bottleneck scenario with multi-homing on a single side, while Ruhmer only deals with the former. Our analysis of the case where users on one side are allowed to multi-home brings additional insights as it allows us to have a demand expansion effect on the multi-homing side.====Our paper is also related to the work of Dewenter et al. (2011) who build a model to investigate the welfare effects of collusion between newspaper publishers. They consider a static setting where newspapers compete in prices in the reader market and in quantities in the advertising market, and compare the platforms’ profits when there is two-sided perfect collusion, one-sided perfect collusion (on the advertising side) and two-sided competition. In contrast, we investigate, in a ==== setting, the most profitable sustainable agreement, allowing for intermediate degrees of collusion and analyzing the incentives for platforms to comply with the collusive agreement. Dewenter et al. (2011) find that, when newspapers only collude on the advertisers’ side, the price is lower on the non-cooperative side, while it is higher on the collusive side (as compared to the static Nash prices). By contrast, we show that one-sided collusion may also lead to a price lower than the competitive price on the ==== side.====Another paper our work is related to is Boffa and Filistrucchi (2014). These authors build a model of collusion between two TV channels and use it to show that prices above the two-sided monopoly price may prevail on one side of the market as a means to enhance cartel sustainability. However, they assume that the price on the viewer side is zero and study collusion in quantities, which makes their paper complementary to ours. Moreover, they focus on the case of two-sided collusion while we also deal with one-sided collusion.====There is also a small literature on collusion with network externalities in one-sided markets. Pal and Scrimitore (2016) show that the relationship between market concentration and collusion sustainability depends on the strength of network externalities. In the same vein, Song and Wang (2017) show that the presence of strong network externalities can reverse the traditional result that collusion between firms is easier with differentiated products Deneckere (1983). Finally, Rasch (2007) studies the relationship between firms’ incentives to introduce compatibility and collusion and finds that it is non-monotonic.====Finally, our paper is also linked to the work by Choi and Choi and Gerlach (2013) on firms’ incentives to collude when they interact in multiple markets and demands in these markets are interrelated. The main goal of Choi and Gerlach (2013) is, however, fundamentally different from ours. They focus on antitrust enforcement issues and, in particular, on whether the discovery of a cartel in one market favors the emergence or collapse of a cartel in another market. Moreover, they restrict their attention to homogeneous goods, which implies in particular that collusion at the monopoly price is sustainable whenever some collusion is sustainable. In contrast, we consider a setting with differentiated platforms and possibly imperfect collusion, and abstract away from antitrust enforcement issues.====The remainder of the paper is organized as follows. In Section 2, we investigate the price and welfare effects of two-sided and one-sided collusion in a setting with single-homing on both sides of the market. In Section 3, we extend our analysis to the scenario in which there is single-homing on one side of the market and (partial) multi-homing on the other side of the market. We discuss some of our assumptions and derive the policy implications of our findings in Section 4. Finally, we conclude in Section 5. Most of the proofs are relegated to the Appendix.",Collusion between two-sided platforms,https://www.sciencedirect.com/science/article/pii/S0167718720300795,28 July 2020,2020,Research Article,163.0
"Martimort David,Pouyet Jérôme","Paris School of Economics (EHESS), PSE, 48 boulevard Jourdan, Paris 75014, France,CY Cergy Paris Université, CNRS, THEMA and ESSEC Business School, ESSEC Business School, 3 Avenue Bernard Hirsch, BP 50105, Cergy 95021, France","Received 11 June 2019, Revised 29 June 2020, Accepted 2 July 2020, Available online 14 July 2020, Version of Record 28 July 2020.",https://doi.org/10.1016/j.ijindorg.2020.102643,Cited by (2),"Motivated by a recent merger proposal in the French outdoor advertising market, we develop a model in which firms are initially endowed with some advertising capacities and compete on two fronts. First, firms compete to acquire additional advertising capacities on an upstream market; a first stage modeled as a second-price auction with externalities. Second, those firms, privately informed on their own costs, use their capacities on the downstream market to supply advertisers whose demand is random; a second stage modeled by means of mechanism design techniques. We study the linkages between the equilibrium outcomes on both markets. When a firm is endowed with more initial capacity, through the acquisition of a competitor for instance, whether it becomes more or less eager to acquire extra capacity on the upstream market depends a priori on fine details of the downstream market. Under reasonable choices of functional forms, we demonstrate that a downstream merger does not create any bias in the upstream market towards the already dominant firm.","An extensive literature studies how a stronger market power on the upstream market impacts the downstream market, yet less attention is paid to the reverse question, namely in what manner a stronger market power on the downstream market affects the upstream market. Motivated by a recent merger proposal in the French outdoor advertising market, we are particularly interested in understanding how a downstream merger impacts competition on the upstream market to acquire capacities.====In 2015, JCDecaux filed a merger application for the acquisition of Metrobus. Both firms were operating in the French market for outdoor advertising, which is one of the most traditional ways to market products and services. Outdoor advertising includes billboard advertising, street furniture, and transit advertising, such as mobile billboards found on buses, for instance. Although other types of traditional advertising have been in decline since the rise of online advertising, outdoor advertising was continuing to grow in a highly competitive environment thanks to new forms of display, such as LED screens.====Outdoor advertising is a vertically related industry. On the upstream market, firms acquire advertising spaces that are typically sold through tenders. These ‘advertising capacities’ are then packaged and sold on a downstream market to meet the advertisers’ specific needs. For instance, an advertiser aiming to reach a business audience may be willing to advertise in major airports and train stations, while a restaurant may be more interested in advertising locally.====Metrobus was specialized on advertising in underground railways and train stations. JCDecaux and Clear Channel Outdoor - the largest player in the French market and its main competitor, respectively - were rather absent from this segment of outdoor advertising. Thus, the JCDecaux-Metrobus merger proposal was expected to significantly increase JCDecaux’s advertising capacity and market power on the downstream market. However, after the completion of a Phase II investigation launched by the French Competition Authority and the remedies proposed by JCDecaux, the merger proposal was abandoned. Although the debate showcased the well-known impact of a merger on the downstream market, another perhaps less straightforward concern was a possible extension of market power towards the upstream market, following the downstream merger. To what extent would the merger have also changed JCDecaux’s incentives to acquire additional capacities on the upstream market?====To answer these questions, we proceed in two steps. First, we suppose that, by acquiring Metrobus, JCDecaux would secure more initial capacity yet the structure of the downstream market would remain unchanged. Specifically, we assume that Metrobus was not a direct competitor to either JCDecaux or Clear Channel Outdoor on the downstream market. Whether Metrobus was on a substitutable or a complementary market was, however, a point of contention during the investigation. On the one hand, although Metrobus was present in two distinctive submarkets (advertising in metro and train stations) that targeted specific audiences (e.g., professionals), these audiences were also targeted by JCDecaux in other segments (e.g., advertising in airports). On the other hand, advertising in these two submarkets may have been used in combination with other advertising channels to promote global campaigns. It was therefore difficult to assess whether these firms are competitors or complementors.====Our analysis shows that, as the dominant firm enjoys more initial capacity, it does not necessarily behave more aggressively to acquire additional capacity on the upstream market. Whether this is the case depends largely on fine details of the downstream market. Yet, using simple functional forms (which assume that firms have costs uniformly and independently distributed, a linear demand and exponential demand shocks on the downstream market), we show that the downstream merger does not increase the position of the dominant firm on the upstream market. This finding suggests that antitrust authorities may be comfortable in dealing with the consequences of a merger by adopting a restricted stance on what happens on the downstream market, and ignoring possible feedback effects on the upstream market.====In a second step, we extend the analysis and examine a merger concerning two direct competitors on the downstream market and then discuss how our results are modified in this specific case. It turns out that our findings are to a large extent robust to the increased complexity of the setting.====Our model considers a vertically related market whereby two firms, initially endowed with some capacities, compete on two fronts. On the upstream market, those firms first compete to acquire extra capacities, and on the downstream market, they also compete to supply advertisers. Firms are asymmetric in terms of their initial capacities, such that the dominant firm has a larger capacity than the weak firm.====On the downstream market, a representative advertiser views the firms’ advertising capacities as perfect substitutes. The advertiser demand is random and the firms have private information about their marginal cost. Efficiency requires that the firm with the lowest marginal cost supplies all the advertiser’s needs. The most efficient firm may be capacity-constrained, however, and in such case the least efficient firm supplies the advertiser’s residual demand up to its own capacity. Rather than specifying a particular extensive form to describe the interaction between firms selling their advertising capacities on the downstream market, we take an alternative route and adopt a mechanism design approach to characterize Bayesian equilibrium outcomes on the downstream market. Following the mechanism design tradition (Myerson, 1982), this approach allows us to fully characterize all possible allocations (i.e., market shares and profits) that might arise on the downstream market at any Bayesian equilibrium, without specifying the underlying game forms which those firms might actually be playing (possibly allowing multiple rounds of communications and side-payments). As such, this approach is powerful both from a theoretical and from a practical viewpoint. On the theory side, it allows us to handle in a very tractable way the possibility that firms are privately informed by means of simple Bayesian incentive compatibility constraints. On the practical side, this approach also offers powerful guidance for competition policy authorities, since the latter are likely to ignore fine details on the extensive form that firms are actually playing.====Of course, and it is a well-known aspect of mechanism design when compared with more direct game-theoretic approaches, the choice of an allocation on the downstream market follows from the maximization of a specific criterion. For most of our analysis, we assume that firms compete fiercely on the downstream market so that the induced equilibrium allocation maximizes the advertiser expected surplus (subject to incentive compatibility and participation constraints, of course). Yet, our results are robust to other specifications that could account for more collusive downstream behavior. We view this robustness as particularly appealing for antitrust authorities when in quest for robust responses.====The mechanism design approach allows us to derive the firms’ downstream profits in terms of their initial advertising capacities. These reduced forms satisfy quite intuitive properties. A firm’s profit is increasing in its own initial capacity and decreasing in that of its rival. It would then be tempting to conclude that a merger that increases a firm’s capacity makes that firm more eager to buy additional capacities on the upstream market, leading to a snowball effect. That reasoning is incomplete, though, because it does not properly take into account the functioning of the upstream market and how a firm’s incentives to acquire extra capacity are determined.====The upstream market is modeled as a second-price auction whereby firms bid to acquire some additional capacity. How much a firm is willing to pay for that extra capacity depends on how it impacts competition on the downstream market, both if the firm or instead its rival gets the extra capacity. The upstream auction is therefore an auction with externalities. The willingness to pay of a given firm is actually the difference between the firm’s profit if it wins the auction, and its profit if it loses the auction and instead its competitor acquires the additional capacity.====We show that the willingness to pay of the larger firm is decreasing in its own capacity. This firm has therefore less incentives to acquire extra capacity as its own capacity grows. The intuition is that such an increase impacts the firm’s profit only when this dominant firm is also the most efficient firm downstream, but it is capacity-constrained in case the advertiser expresses a high demand. Having more capacity reduces the likelihood of such events. At the same time, the willingness of a weak firm to pay for extra capacity also decreases with the dominant firm’s capacity. Indeed, the profit of the weak firm only depends on this capacity, and negatively so, when the dominant firm is capacity-constrained and must allow a smaller and less efficient firm supply the advertiser’s residual demand.====Because the willingness of both dominant and weak firms to pay for additional capacity decreases with the capacity of the dominant firm, the impact of the merger on the upstream market is a priori ambiguous. Put differently, a merger that increases the capacity of the dominant firm does not necessarily imply that that firm becomes more aggressive and wins more often in the upstream auction for extra capacity. Any presumption of an extension of market power from the downstream to the upstream market following a merger, often referred to by practitioners as a ‘snowball effect,’ should then be viewed with a word of caution. We use a particular specification of the model that assumes marginal costs are uniformly distributed, demand shocks are exponentially distributed and a linear demand. We show that the characteristics of the downstream market, such as the slope of the advertiser’s demand and the distribution of the shock impacting the latter’s demand, are key drivers for understanding the impact of the merger on the functioning of the upstream market. Nevertheless, this specification also shows that there is not necessarily an extension of market power towards the upstream market following a downstream merger.====Our welfare analysis reveals that the merger benefits the advertiser. On the one hand, the merger increases the average capacity available in the market, which ultimately benefits the advertiser and reduces the likelihood of firms being capacity-constrained. On the other hand, the merger also increases the asymmetry between firms, which negatively affects the advertiser. The former effect dominates in our particular specification of the model, which explains why the merger ends up benefiting the advertiser.====Finally, we also analyze a situation where, before the merger, three firms compete on the downstream market, and the merger involves two of those. Put differently, the merger now also impacts the market structure on the downstream market. Our findings are robust to that increased complexity of the setting, although the impact of the merger on the upstream market now sometimes depends on the degree of asymmetry between the initial capacity levels of the firms.====Stahl (1988) provides an earlier analysis of a setting where firms compete both on an upstream market and on a downstream market. Firms bid to acquire a homogeneous good, which is then resold to consumers via price competition. In some circumstances (one winner takes all, even when tying), the output price may be excessive. Yanelle (1997) studies a related model in which banks and borrowers compete for funds, leading to a coordination problem between lenders. Fingleton (1997) considers that direct trade between upstream suppliers and downstream buyers is also possible. Although we share with those papers the topic of ‘middlemen’ competing both to acquire inputs and to sell outputs, our analysis differs in several ways. For instance, we assume that there is a fixed quantity of input available.====Ghemawat (1990) builds on Kreps and Scheinkman (1983)’s analysis of capacity choice followed by price competition to study how two firms, initially endowed with different capacity levels, compete to buy additional capacity. He shows that the initially larger competitor ends up absorbing all investment opportunities in order to keep product prices high. Eső et al. (2010) assume that firms are Cournot competitors on the downstream market and compete to buy capacity in an upstream market that allocates capacity efficiently. In Burguet and Sákovics (2017b), the input is provided competitively by a large number of small capacity-constrained suppliers, but the same supplier may be approached by multiple potential buyers simultaneously. Firms become more aggressive upstream in the attempt to foreclose their rivals, which leads to a higher input price and a larger downstream quantity. We differ from those papers on two main grounds. First, we are interested in a different set of issues, namely how a merger on the downstream market impacts competition on the upstream market. Second, instead of a priori specifying the nature of competition on the downstream market, we rely on a mechanism design approach to characterize outcomes on this market. As argued above, this approach is attractive from a practitioner’s viewpoint since it provides a guide for decision-making that does not rely on the interactions’ details, which are often impossible to grasp for Antitrust authorities.====Indeed, and this points at its theoretical benefits, the mechanism design approach implicitly presumes that all possible sorts of communication procedures can a priori be entertained by competitors, yet it does not specify any extensive form to model their interactions. Any equilibrium allocation is therefore necessarily bound to satisfy a set of Bayesian incentive compatibility constraints that fully characterize the set of incentive-feasible allocations. Our mechanism design approach thus proposes an alternative route to study price competition with capacity constraints, thereby avoiding some of the technical difficulties encountered by the extant literature. Indeed, and contrary to our model, this literature focuses on complete information environments for which pure strategy equilibria often fail to exist, and only mixed-strategy equilibria can be characterized in some structured environments; see, for instance, Kreps and Scheinkman (1983); Davidson and Deneckere (1986); Osborne and Pitchick (1986); Deneckere, Kovenock, 1992, Deneckere, Kovenock, 1996, and Allen et al. (2000), or Burguet and Sákovics (2017a) who show that a pure strategy equilibrium exists when firms can use personalized pricing. The mechanism design approach provides a somewhat more tractable characterization of downstream outcomes, which in turn allows to perform several important exercises of comparative statics (e.g., on the market structure).====A large body of research addresses how a stronger market power on the upstream market impacts the downstream market (see Rey and Tirole, 2007, for a recent appraisal of the debate between the viewpoints of the so-called ‘traditional foreclosure theory’ and ‘Chicago school’). Although we do not consider vertical contracting and foreclosure, as mentioned above, we are interested in understanding how a stronger market power on the downstream market impacts the upstream market.====One building block of our analysis models the upstream market as an auction with externalities, a topic investigated previously by Jéhiel et al. (1999) and Jéhiel and Moldovanu (2000) among others. Unfortunately, this abstract literature is of little relevance to provide guidance for Antitrust analysis in practice since, contrary to our analysis, it usually does not endogenize those externalities by means of profit functions inherited from downstream behavior. Assuming Cournot competition on the downstream market, McAfee (1998) finds that small, capacity-constrained firms might often outbid larger, unconstrained firms. Mayo and Sappington (2016) study the foreclosure incentives of rivals bidding for an input and competing downstream á la Hotelling. They are interested in conditions that ensure the welfare-maximizing allocation of inputs. Our modeling of the downstream market differs from this approach and we focus instead on a different issue, namely how the asymmetry between firms, in terms of their initial capacity levels, affects the incentives to bid for extra capacity.====The paper is organized as follows. Section 2 describes the model. Section 3 characterizes the outcome on the downstream market when those firms are endowed with some given capacities. Section 4 studies how the upstream auction for additional capacity is biased when the dominant firm’s capacity increases exogenously. Section 5 considers the same issue, but when the dominant firm’s capacity increases following the acquisition of another downstream competitor. All proofs are relegated to an Appendix.",Downstream mergers in vertically related markets with capacity constraints,https://www.sciencedirect.com/science/article/pii/S0167718720300667,14 July 2020,2020,Research Article,164.0
"Allain Marie-Laure,Avignon Rémi,Chambolle Claire","CREST, Institut Polytechnique de Paris, France,CNRS, France,ALISS UR1303, INRAE, Université Paris-Saclay, Ivry-sur-Seine F-94200, France,Ecole Polytechnique","Received 7 June 2020, Accepted 8 June 2020, Available online 27 June 2020, Version of Record 27 November 2020.",https://doi.org/10.1016/j.ijindorg.2020.102641,Cited by (3),"We analyze the impact of purchasing alliances on product variety and profit sharing in a setting, in which capacity constrained retailers operate in separated markets and select their assortment in a set of differentiated products offered by heterogeneous suppliers (multinationals ====. local SMEs). Retailers may either have independent listing strategies or build a buying group, thereby committing to a joint listing strategy. This alliance may cover the whole product line (full buying group) or only the products of large suppliers (partial buying group). We show that a buying group may enhance the retailers’ buyer power and reduce the overall product variety to the detriment of consumers. Our most striking result is that partial buying groups do not protect the small suppliers from being excluded or from bearing profit losses; they may even be more profitable for retailers than full buying groups.","Buying groups are purchasing alliances between retailers designed to enable them to negotiate together with their suppliers over the listing of products and/or tariffs. Those agreements are widespread, and they often gather retailers that operate in different countries.==== Such alliances are not supposed to affect downstream competition, as retailers keep operating their stores independently, but they are a mean to enhance buyer power, which is usually well perceived by competition authorities.====The pro-competitive effects of buyer power have been first coined by Galbraith (1952), who explains how this “countervailing power” enables retailers to obtain discounts that translate into lower consumer prices. Since then, the economic literature has reconsidered these conclusions. First, discounts obtained by retailers may not translate into lower consumer prices: the countervailing power effect relies on strong assumptions regarding the shape of tariffs, namely linear contracts (see von Ungern-Sternberg, 1996 and Iozzi and Valletti, 2014) and intense retail competition (see Gaudin, 2018). Yet it has been widely documented that tariffs in the retail sector are scarcely linear (see Berto Villas-Boas, 2007 and Bonnet and Dubois, 2010), and that the retail sector has achieved a high level of concentration both in Europe and in the United States (see Allain et al., 2017, Barros, Brito, de Lucena, 2006, and Hosken et al., 2018). Furthermore, recent empirical and theoretical developments point out potential adverse effects of buyer power on product variety and innovation (see European Economic Community, 2014 and Inderst and Mazzarotto, 2008 for a survey, Inderst, Shaffer, 2007, Caprice, Rey, 2015 and Chambolle and Villas-Boas, 2015.)====Despite the potential adverse effects highlighted in the above literature, purchasing alliances are usually not subject to ==== approval by competition authorities, contrary to mergers. In the European Union, buying groups are subject to scrutiny under Article 101 of the Treaty on the Functioning of the European Union, as any horizontal co-operation agreements: they are lawful if and only if their restrictive effects are more than outweighed by pro-competitive effects, provided that consumers receive a “fair share” of the resulting benefits. There is no ==== control by the European Competition Authority, but firms entering into a purchasing agreement must carry out a self-assessment of the legality of such agreement, based on the ==== (henceforth the Guidelines) and on the rules on the vertical agreements displayed in the ====. Section 5 of the Guidelines acknowledges that “joint purchasing arrangements [... ] may force suppliers to reduce the range or quality of products they produce, which may bring about restrictive effects on competition such as quality reductions, lessening of innovation efforts, or ultimately sub-optimal supply” (§194 and 202). However, the Guidelines consider that if “competing purchasers co-operate who are not active on the same relevant selling market (for example, retailers which are active in different geographic markets and cannot be regarded as potential competitors), the joint purchasing arrangement is unlikely to have restrictive effects on competition [... ]” (§212 and 223). In this paper, we therefore focus on the case where retailers cannot be regarded as potential competitors.====Recent waves of buying alliances in the grocery industry have attracted the attention of several Competition Authorities, including the European Commission==== and the French==== and Belgian==== national authorities. Between September and December 2014, three large purchasing agreements have been signed in France: between System U and Auchan, between Intermarché and Casino, and between Carrefour and Cora. In its 2015 Opinion (15-A-06), the French Competition Authority claims that these buying groups are likely to have limited anticompetitive effects because their scope is restricted to national brand products: they cannot affect products manufactured by small suppliers or fresh agricultural products, that are more likely to be in a situation of dependence. A second wave of international purchasing agreements involving French retailers started in 2018: besides Horizon (see footnote 1), two new agreements involve Carrefour and System U on the one hand, and Carrefour and Tesco on the other. An important difference with the previous wave is that the new buying groups gather retailers operating on separate markets. Furthermore, they cover a wider scope of brands.==== The French competition authority states that new agreements “differ from the alliances made in 2015 due to their larger scope involving an international dimension, and because they include not only national brand products but also store-brand products”.==== The retailers argue that this may give opportunities of international development to the suppliers of private labels.====In this paper, we study the effect of alliance strategies on product variety, and we compare two types of alliances: partial buying groups, in which the retailers negotiate jointly with the suppliers of leading brands, and full buying groups, in which they also negotiate jointly with local SMEs. We deliberately abstract from the effects of such alliances on downstream competition, and thus consider two retailers acting as monopolists on two independent markets, (==== two countries).==== We model two types of suppliers: a large supplier who offers two products in both markets (typically a multinational company selling leading brands across markets), and, in each market, a small local supplier who offers only one product (typically, a SME producing a private label). Each small supplier must incur export costs to enter in the other market. We assume that there is heterogeneity of the products profitability across markets.==== We consider that retailers may either adopt an independent listing strategy or build a buying group, thereby committing to listing the same product assortment. Buying groups may cover the whole product line (full buying group) or only part of it (partial buying group, targeting only the products of the large producer).====In each of these situations, retailers and suppliers contract over three part tariffs following the timing of Chambolle and Molina (2019). First, on each market, suppliers compete for being listed by the retailer by simultaneously offering lump-sum slotting fees. After the listing decision, which is publicly observed, retailers engage in a “Nash-in-Nash” bargaining over efficient two-part tariff contracts, with the supplier(s) of the selected products. Finally, retailers sell their products on the downstream markets.====Absent buying group, we first highlight that each retailer chooses the efficient assortment of products in its market, excluding the least efficient product - this efficient assortment differs however across market. Hence, with a buying group, committing to a similar assortment in the two markets always generates inefficiencies in one of the markets and in some cases in both. Despite this inefficiency, retailers may find this strategy profitable because the alliance enhances their buyer power, as it increases competition among the suppliers for being listed. Indeed, in one market the excluded product is no longer the least efficient: its supplier is therefore ready to pay a higher slotting fee to be listed, and this, in turn, leads to an increase in the slotting fees paid by the selected supplier. In that case, the buying group enables the retailers to receive “a larger share of a smaller pie”. As a result, it may be jointly profitable for the retailers to create a buying group when their bargaining power is low, as retailers have relatively more to win from the intense competition for slots than they loose from bargaining over a reduced industry profit. Our most striking result is that partial buying groups do not protect the small suppliers from being excluded or from bearing profit losses; they may even be more profitable for retailers than full buying groups.====This article contributes to the growing theoretical literature on buying groups. A large part of the existing literature on buying groups focuses on the rationality of purchasing cooperation between retailers who compete on the downstream market. In such a framework, Caprice and Rey (2015) show that a joint listing decision enhances each retailer’s buyer power by increasing its outside option in the negotiation with a supplier: in case of a breakdown in the negotiation, the profit of the retailer decreases less, as its competitors also delist the products of this supplier. We consider instead the incentives of non-competing retailers to form a buying group. Chipty and Snyder (1999) have shown that retailers active on separate markets benefit from buying together when bargaining with a supplier with convex production costs, because it decreases their relative gains from trade with such a supplier (see also Inderst and Wey, 2003 and Jeon and Menicucci, 2019). The most closely related paper is Inderst and Shaffer (2007), which analyzes the impact of a cross-border merger between two single product retailers active in two separated markets with different consumer preferences. They show that the merger can enhance the retailers buyer power when they commit to a single sourcing strategy. This creates inefficiency in one market because of the reduction of the overall product variety.==== Building on the vertical contracting process developed by Chambolle and Molina (2019), we extend the framework of Inderst and Shaffer (2007) to multi-product suppliers and retailers. This multi-product setting allows us to consider different types of buying alliances that differ in their scope, and to analyze their effects on different types of suppliers (single- or multi-products). We also depart from their analysis by highlighting possible inefficiencies of the alliance in the two markets.====Our model clearly leaves aside good reasons for buying groups to be welfare enhancing, such as the reduction of double marginalization, investment incentives, or synergies leading to cost reduction that may be passed through to consumers (see, for instance, Inderst and Wey, 2007). A recent empirical analysis by Molina (2019) confirms that buying groups may lead to a decrease in retail prices through a countervailing power mechanism. We also do not consider possible pro-collusive effects of buying groups. Piccolo and Miklós-Thal (2012) and Doyle and Han (2014) show that buying groups agreements can improve retailers’ ability to sustain collusive retail prices, by coordinating on high wholesale prices and using back margin payments.==== Here we consider retailers active on separate markets to abstract from the effects of buying alliances on retail competition.====This paper is also related to the literature on endogenous network formation in vertically related markets. Marx and Shaffer (2010) show that retailers can strategically use capacity constraints in order to increase their buyer power towards suppliers.==== In the same vein, Ho and Lee (2019) develop a bargaining procedure called ”Nash-in-Nash with threat of replacement” to explain the hospital network reduction of American health insurers by profit extraction motives. Rey and Vergé (2017) and Nocke and Rey (2018)) also endogenize the retail network in more complex vertical structure with both upstream and downstream competition and show that, absent any capacity constraint, in equilibrium not all products are sold at all retailers when retail competition is fierce, which harms consumer surplus and welfare.====The article is organized as follows. Section 2 presents the main insights of our results in a streamlined example. Section 3 presents the setup and notations. Section 4 derives the equilibrium outcomes in the three cases : No buying group, partial buying group, and full buying group. Section 5 endogenizes the retailers decision to form a buying group and analyzes the effects of these buying groups on the sharing of profits in the industry, on product variety, and on welfare. Section 6 concludes.",Purchasing alliances and product variety,https://www.sciencedirect.com/science/article/pii/S0167718720300643,27 June 2020,2020,Research Article,165.0
"Jochem Annabelle,Parrotta Pierpaolo,Valletta Giacomo","RBB Economics, Kasernenstraße 1, 40213 Düsseldorf, Germany,IESEG School of Management, 3 rue de la Digue, 59000 Lille, France,EDHEC Business School, 24 avenue Gustave Delory, 59057 Roubaix, France","Received 29 May 2019, Revised 22 April 2020, Accepted 6 June 2020, Available online 17 June 2020, Version of Record 1 July 2020.",https://doi.org/10.1016/j.ijindorg.2020.102640,Cited by (4),"The objective of this study is to empirically assess whether the 2002 reform of the EU leniency program, which brought the EU leniency policy much closer to the US policy, has increased the European Commission’s capability to destabilize cartels while making prosecution more efficient. More specifically, relying on a difference-in-difference approach, we estimate the impact of the 2002 reform on cartel duration, fines before and after applying leniency reductions and duration of investigation. We find that the 2002 reform decreased cartel duration by about 87 percent, but did not significantly affect the other outcome variables. Thus, our findings seem to suggest that the 2002 reform has improved the cartel-destabilizing effect of the EU leniency program, without enhancing however its effectiveness in prosecuting cartels.","Cartels are among the most welfare-reducing practices that firms may engage in (Motta (2004)). Antitrust authorities (AA hereafter) actively fight such practices by implementing policies aimed at deterring their formation while detecting and prosecuting their activities. Historically, horizontal infringements have been a conspicuous part of the focus of the antitrust activity at EU level==== and Leniency Programs (LP hereafter), which provide immunity and/or fine reductions to cartel members who decide to collaborate with the AA, have been widely adopted to fight cartels by many OECD countries (OECD (2012)).====Even if in law enforcement LPs share some similarities with plea bargain (lenient treatment in exchange of information), as stressed by Spagnolo (2008), they have some specific features that are essential for their effectiveness. First, LPs may act before a wrongdoing has been detected and prosecution has started. Moreover, LPs are general, they apply to anyone who is in a specific situation, and public, they should take the form of codified and predictable policies. The last two aspects are essential to reduce uncertainty and discretionality in fine reductions as they may both inhibit self-reporting.====In this respect, transparency, and not only generosity, is essential for a LP to meet its ultimate objective, that is to help law enforcement in three ways. First, in an ==== perspective, the introduction of a LP should increase cartel deterrence by increasing the probability that a potential conspirator could self-report the cartel to the AA: the LP should erode the possibility of trusting other potential cartels members. Moreover, a successful LP should enhance ==== the capacity of AAs to uncover existing cartels by making them less stable, by encouraging their members to blow the whistle. Finally, and still in an ==== perspective, a LP should ease prosecution by inducing conspirators to provide information to the AA.====The European Commission (EC hereafter) introduced its first LP in 1996 (European Commission, 1996). This first attempt was considered rather ineffective at eliciting self-reports from cartel members (see Brenner (2009) and Spagnolo (2008)). This was mostly due to the fact that the amount of fine reduction, for self-reporting firms, was uncertain and discretionary. In 2002, the EC introduced a substantial reform of the LP (European Commission, 2002) aiming mainly at making the program more transparent and generous, by decreasing the level of discretion in granting leniency, and increasing the amount of fine reductions for leniency applicants. The reform was welcomed as a clear improvement and many considered it as very successful “in persuading companies to come clean” (Van Barlingen (2003)).====Since the introduction of the reform, the share of decisions in which immunity was granted to the first undertaking cooperating with the EC has reached about 90% (Wils (2016)). It seems rather clear that, after 18 years, leniency has been playing a prominent role in the EU antitrust enforcement.====Most of the existing empirical literature has focused, in an ==== perspective, on the LP’s objective of reducing the number of cartels by deterring their formation. Obvious observability issues make this task rather difficult: Miller (2009) and Harrington and Chang (2009) propose two different methodologies meant to provide observable conditions that, if met, would provide indirect evidence of increased deterrence. Several studies have then tried to assess the impact of the EU (and the American) LP, in this context, with mixed results.====This paper diverts from the beaten path by relying on a more ==== approach that specifically focuses on the 2002 reform and on its effect on existing cartels. We start from the observation that, as suggested by Spagnolo (2008), the 2002 reform could be considered as a kind of “natural experiment” as it “took place in discrete steps” and was “not likely to have been fully anticipated by firms and lawyers”. Because of the timing and structure of the reform, we could rely on a quasi-experimental design that has allowed us to assess the effect of the reform on the ex-post objectives of the program. More precisely, we have evaluated whether the reform managed to increase the detection capability of the program, by making existing cartels less stable, while also facilitating prosecution, by making more information available to the AA.====To do so, we rely on difference-in-difference (DID hereafter) approach that allows us to identify the effect of the 2002 LP reform by comparing, before and after its introduction, our treatment group, self-reported cartels, with our control group, cartels detected (i.e. not self-reported) by the EC.====We have focused on three outcome variables: cartel duration, fines before and after applying for leniency reductions, and duration of investigation. We use cartel duration as a proxy of the stability of a cartel while the two other variables provide an assessment of the prosecution capacity of the AA. Indeed, as in Brenner (2009), we use fines (before and after leniency) as a proxy of the amount of information gathered by the AA, and duration of investigation as a proxy of the cost of investigation.====Our results are highly policy relevant as we find that, with the introduction of the reform, the duration of self-reported cartels decreased on average by about 87 percent, compared to cartels detected directly by the EC. Hence the reform seems to have boosted competition and possibly consumer surplus to some extent: the longer a cartel lasts, the higher is the harm to consumers in terms of high prices and to markets in terms of lack of allocative and dynamic efficiency. However, we find that the 2002 reform had no significant effects on both levied fines and duration of investigation. Thus, it emerges that the cartel-destabilizing effect attributable to the 2002 reform is not coupled with evidence supporting an enhancement in the effectiveness of cartel prosecution. In this sense, our findings suggest that the effects of 2002 leniency reform may have not been as extensive as predicted or thought by the EC itself.====In order to carry our investigation, we have built our own data-set by looking at all decisions issued by the EC between 1990 and 2017, in which a fine was imposed. However, the bulk of our empirical analysis focuses specifically on cartels detected over the period 1998–2006. This is due to the fact that in 2006 the EC introduced a further reform of the LP (European Commission, 2006a) and this naturally limits the time horizon of our analysis. More specifically, for cases detected or self-reported after 2006, it would not be possible to disentangle the effect of the 2002 reform on our outcome variables, from the effect of the 2006 reform. Nonetheless, we also perform robustness checks by considering a longer time span: from the first introduction of the EU LP in 1996 until 2012. We find that results arising from such a sensitivity analysis are very much in line with our main findings. In any case, our main objective is to evaluate the 2002 reform (given its specificity), so our analysis should not be interpreted as an overall assessment of the EU LP since its beginnings.====The paper is organized as follows. Sections 2 and 3 describe the theoretical and empirical background, respectively. Section 4 presents the data, describes the empirical model and provides descriptive evidence of the relationship under analysis. Section 5 discusses the empirical findings. Section 6 concludes the paper. The Appendix reports further descriptive and empirical evidence.",The impact of the 2002 reform of the EU leniency program on cartel outcomes,https://www.sciencedirect.com/science/article/pii/S0167718720300631,17 June 2020,2020,Research Article,166.0
Yan Haomin,"University of Maryland, College Park, MD 20740, United States,Wayfair Inc. (unaffiliated during this research), Boston, MA 02116, United States","Received 28 January 2019, Revised 2 April 2020, Accepted 22 May 2020, Available online 11 June 2020, Version of Record 5 July 2020.",https://doi.org/10.1016/j.ijindorg.2020.102638,Cited by (0),This paper studies the design of license auctions when the number of licenses allocated in the auction determines structure of the post-auction market. I first show that a sequence of conditional reserve prices that specify minimum acceptable bid at each supply level can be used to determine supply endogenously. Then I construct a static auction called multi-dimensional uniform-price auction and a dynamic auction called Walrasian clock auction that allow the auctioneer to condition reserve price on supply and allow bidders to condition bids on supply. I show that both proposed auctions can implement the efficient market structure that maximizes total surplus in the post-auction market in a dominant strategy equilibrium. I next characterize the optimal auction and show that the two proposed auctions can yield the optimal revenue under a sequence of optimal reserve prices.,"The classical auction theory literature studies the problem of allocating an exogenously given set of items, assuming that bidders’ valuations over each item or subset of items are exogenously given, and the final allocation only enters payoffs of auction participants and does not enter payoffs of any outsider who does not participate in the auction. However, these assumptions can fail to hold in many auction markets. This paper studies the design of efficient and optimal auctions when each bidder’s value from winning is decreasing in the total quantity of items allocated in the auction, and the total quantity of items allocated in the auction enters payoffs of outsiders who do not participate in the auction. Moreover, the auctioneer not only cares about surplus created within the auction, but also cares about surplus of outsiders who are affected by the auction outcome. A few examples that motivate this research are described below.====In many countries, governments use auctions to sell production rights that determines the market structure of the corresponding industry.==== For example, the US government uses auctions to allocate spectrum licenses to mobile service providers.==== In this setting, a firm has to obtain a license to enter the industry, and such licenses can only be obtained through auctions. The value of a license to each firm depends on the total number of licenses allocated in the auction, as each firm’s profit upon entry depends on the number of competitors who also win a license to enter the industry. Selling more licenses allows more firms to enter the market but reduces the payoff from winning a license to each firm. At the same time, consumers can benefit from having more firms competing in the industry, so selling more licenses also increases consumer surplus. The government often cares about efficiency and revenue generated within auction as well as consumer surplus in the post-auction market.====The Shanghai government uses auctions to sell car license plates to residents who want to purchase new cars for private use in Shanghai, China. A car buyer has to obtain a license plate in order to enter the road, and car license plates can only be obtained through auctions. In this auction, the value of a license plate to each car buyer is decreasing in the total number of license plates allocated in the auction,==== as selling more licenses allows more cars to enter the road and leads to worse traffic congestion, which reduces each car buyer’s value of owning a car. Selling more licenses also increases demand for new cars, leading to higher equilibrium prices in Shanghai's automobile market, further reducing each individual car buyer’s surplus, while increasing car sellers’ surplus at the same time. The government cares about both car buyers’ surplus and car sellers’ surplus and therefore faces a trade-off when deciding the number of car license plates to allocate in the auction.====Many online platforms such as Amazon.com and eBay.com use auctions to allocate sponsored advertising slots to sellers. A sponsored advertising slot can be viewed as a license for entering the sponsored product list. The value of a slot is decreasing in the total number of slots allocated in the auction, as the click-through rate and conversion rate of each sponsored product are likely to be lower when there are more sponsored products listed on the platform.==== At the same time, consumers shopping on the platforms may benefit from having more products in the sponsored product list, as having more products on the sponsored list not only provides more choices to the consumers, but also promotes price competition among suppliers. The platform designer cares about both revenue generated from advertisers and surplus of consumers shopping on the platform.====In all the aforementioned examples, auctions are used to sell licenses that are effectively entry permits to some post-auction markets. The number of licenses sold in the auction determines the number of participants on one side of the post-auction market. When more licenses are sold in the auction, more bidders will be competing on the same side of the post-auction market, which decreases the payoff of each individual bidder who enters the market and increases the payoffs of participants on the other side of the post-auction market. Therefore, two types of “quantity externalities” present in the auction. First, selling an additional license imposes negative externalities on all winning bidders, as each bidder’s value from winning is decreasing in the total number of licenses allocated in the auction. Second, selling an additional license imposes positive externalities on auction outsiders who participate on the other side of the post-auction market.====Under the presence of these quantity externalities, the auctioneer faces a trade-off when selecting the total number of licenses supplied in the auction. Selling all licenses up to the capacity constraint may not maximize total surplus in the post-auction market. Therefore, it would be natural for the auctioneer to choose the quantity supplied endogenously within auction. How to design an auction to select both the quantity of licenses to sell and the assignment of licenses to bidders is an interesting and challenging problem. On one hand, bidders can fail to consider the negative externalities imposed on other bidders when bidding for licenses. This may result in overbidding and oversupply of licenses, causing too many bidders entering the post-auction market. On the other hand, bidders can fail to take the positive externalities imposed on outsiders into account. This can result in underbidding and undersupply of licenses, causing too few bidders entering the post-auction market. This paper shows that both type of quantity externalities can be “priced” correctly by designing two auctions that can implement the efficient market structure under the presence of quantity externalities.====In this paper, I revisit the model in Gebhardt and Wambach (2008) where a license auction is used for selling entry permits to a post-auction market. In this model, each bidder’s value from winning a license depends on its private type and final supply of the auction. Moreover, the final supply of auction also affects payoffs of some auction outsiders who participate on the other side of the post-auction market. I first show that a sequence of conditional reserve prices can be used to determine supply endogenously in the auction. That is, the auctioneer conditions reserve price on final supply by setting a minimum acceptable bid for every additional unit to be sold. For every possible supply level ====, a ====th unit is sold only if the ====th highest bid meets the conditional reserve price ====. Then I construct a static auction called multi-dimensional uniform-price auction that allows the auctioneer to condition reserve price on supply and allows bidders to condition bids on supply. I show that this auction can implement the efficient allocation in a dominant strategy equilibrium under a sequence of efficient conditional reserve prices. Moreover, I construct a dynamic auction called Walrasian clock auction that can dynamically implement the efficient allocation. With efficient conditional reserve prices, both the multi-dimensional uniform-price auction and the Walrasian clock auction are outcome equivalent to the VCG mechanism. I also characterize the revenue-maximizing mechanism and the optimal conditional reserve prices. Both the multi-dimensional uniform-price auction and the Walrasian clock auction can also implement the optimal revenue under optimal conditional reserve prices.====The two proposed auctions have many desirable properties to the auctioneer. First, conditioning reserve price on final supply facilitates price discovery process at every feasible supply level and allows the auctioneer to easily evaluate the marginal benefit and marginal cost of selling every additional license given any bid profile. The auctioneer simply needs to compare the ====th highest bid to the conditional reserve price ==== to decide whether a ====th license should be allocated. This feature makes the outcome determination straightforward and allows the auction to conclude in a timely manner. Second, both auctions have very little informational requirement on the auctioneer. The auctioneer can set the efficient conditional reserve price schedule without having any information about the distribution of bidders’ private types.====In addition to having desirable simplicity and efficiency properties to the auctioneer, the two proposed auctions also have many desirable properties to the bidders. First, bidders are able to protect themselves from instances where the market is over-flooded with supply, making the ex-post payoff from winning significantly lower than the expected payoff at the time of bidding. At the same time, bidders are also able to protect themselves from losing after underbidding and then realizing that they could have received higher payoffs from winning than what they expected. Allowing bidders to condition bids on final supply effectively removes any potential strategic considerations that bidders might otherwise have in order to prevent overpayment when too many licenses are sold or underbidding when too few licenses are sold. Second, both auctions have little informational requirement on bidders, as truthful bidding at every supply level is a dominant strategy equilibrium in both auctions. This feature greatly increases the simplicity of auction design and reduces uncertainty in the auction.====The main contribution of this paper is to construct both static and dynamic auction mechanisms to implement the efficient market structure under the presence of quantity externalities. With correctly designed efficient conditional reserve prices, the two proposed auctions are outcome equivalent to the Jumping English auction proposed in Gebhardt and Wambach (2008) but have simpler design and take shorter time to conclude. Moreover, the two proposed auctions can implement the VCG outcome in a dominant strategy equilibrium, contributing to the literature on practical implementation of the VCG mechanism. Under optimal reserve prices, the two proposed auctions can also yield the optimal revenue, contributing to the literature on optimal auction design. The main implication of this paper is that auctions can be used as regulation devices in markets where firms fail to consider negative externalities on competitors or positive externalities on consumers when making entry decisions, leading to market failure in firm entry and product selection. By imposing a reserve price at every supply level and allowing bidders to condition bids on supply, the auctioneer can price both negative and positive externalities associated with entry correctly under either surplus-maximizing or revenue-maximizing objective.",Auctions with quantity externalities and endogenous supply,https://www.sciencedirect.com/science/article/pii/S0167718720300618,11 June 2020,2020,Research Article,167.0
"Friese Maria,Heimeshoff Ulrich,Klein Gordon J.","Compass Lexecon, Kö-Bogen, Königsallee 2b, Düsseldorf 40212, Germany,Düsseldorf Institute for Competition Economics (DICE), Heinrich-Heine-University Düsseldorf, Universitätsstraße 1, Düsseldorf 40225, Germany,Centrum für angewandte Wirtschaftsforschung Münster (CAWM), Westfälische Wilhelms-Universität Münster, Am Stadtgraben 9, Münster 48143, Germany","Received 8 May 2020, Accepted 18 May 2020, Available online 10 June 2020, Version of Record 28 July 2020.",https://doi.org/10.1016/j.ijindorg.2020.102637,Cited by (3),"This paper provides evidence that ownership and organization matters for the efficiency of the provision of public services. The results confirm trade-offs implied by the property rights literature and provide important policy implications regarding the organization of public service provision. We find that pure private ownership is more efficient than pure public ownership, and public ownership is more efficient than mixed ownership. The delegation of management in different legal forms also has an impact, highlighting the importance of the design of the government-operator relationship. We apply a structural approach of a production function estimation, ensuring precise determination of total factor productivity for a panel of German garbage collection firms between 2000 and 2012, followed by a projection of those total factor productivity estimates on ownership and organization.","For many decades, there has been an extensive public debate over public versus private provision of utilities and infrastructure, which are regarded as basic tasks of the government due to the existence of major externalities. While the 1990s saw the privatization of many public enterprises, for instance, in the telecommunications sector, there are plenty of current examples where the opposite, a renationalization of utilities and infrastructure, takes place. One example is New Zealand’s railway system, which was first privatized in the 1980s and 1990s and was renationalized in the 2000s.==== As an example from Germany, the City of Hamburg’s formerly privately owned electricity network was purchased by the municipality following a public referendum.==== Aside from policy debates of pure public versus private ownership, there are discussions on mixed ownership, such as public-private partnerships, which aim at combining the advantages of private and public players.====The traditional view of economists generally favors the private provision of services and utilities (Bennett and Johnson, 1979). However, there are influential papers showing that the public provision of goods and services may lead to ambiguous outcomes depending on the characteristics of the service considered (Hart et al., 1997), with some also mentioning potential conflicts in the organization of the services provided (Martimort and Pouyet, 2008).==== Likewise, the empirical literature shows that, depending on the circumstances, private or public provision may be more efficient (for a survey, see Villalonga, 2000).====Our paper adds to the literature by analyzing the efficiency of basic public services in Germany, considering the role of ownership as an important determinant linking the findings to the results predicted by the property rights literature departing from Grossman and Hart (1986) and applied to the public-private context by Hart et al. (1997). Thus, we are able to test whether the predictions made in this theory can be found in empirical examples.====However, our study goes beyond simply looking at ownership by also taking into account additional aspects of productivity if firms are organized under public ownership. The underlying hypothesis is that besides the primary ====, there is a second ==== that is related to the legal status of a public firm and may influence the way a firm operates. The idea is that in the public sector the organization determines the discretion of decision-making and the degree of control of the organization that provides public services. That means it can either be organized according to private law with potentially high-powered incentives and more degrees of freedom in decision-making or it can be organized fully integrated in the municipality with lower powered incentives but full control rights. In addition, between these extremes, there is an organizational form combining characteristics with some discretion in decision-making and relevant direct control rights.====Thus, an important determinant for productivity lies in the relationship between the government as the organizer and the management of the firm. This may include, for example, how detailed service requirements are given to the firm, the precise role of the government and the firm in terms of control, organization, and management, or a firm’s freedom in decision-making. Since there is substantial heterogeneity, in particular, regarding public firms, we include these factors in the analysis by considering the legal status as an efficiency driver in addition to ownership.====To identify the effects without bias, we first estimate a production function in value added using the technique introduced by Ackerberg et al. (2015) and project the total factor productivity (TFP) derived from this function on ownership and legal status. This approach overcomes the classical endogeneity problem that exists when unobserved productivity correlates with input choice by using moment conditions that are exogenous to the stochastic element of productivity. Second, we estimate the effect of ownership and organization on the productivity estimates, relying mainly on panel techniques and a wide set of controls.====We utilize a unique self-created data set for the German market for public services, which includes refuse collection and related services. This sector is likely to have an outstanding feature, namely that the service is not complex and the quality dimension seems to be less important than the reduction of costs and is therefore recommended by the literature as a sector that is likely to be organized according to the private property holders Hart et al. (1997, p. 1154).====The most important result is that, in general, our findings support the predictions of the property rights literature, which would argue that services where quality deterioration is unlikely to occur, ownership should be given to private firms (Hart et al., 1997).==== In addition, we uncover that with other forms of ownership there is a non-linear relationship of efficiency to the degree of private ownership.==== Mixed-ownership models such as public-private partnerships are less efficient than pure public entities, which speaks for conflicts within the semipublic firm.====Moreover, we find that the legal form (i.e. the ====) also has an effect, as we find productivity differences within public firms that vary in organizational structure without being subject to heterogeneity in ownership. In cases where (public) ownership is separated from firm management, but the service lacks incentives from a private legal form, the lowest productivity is observed.==== On the other hand, when the service is either integrated within the municipality government (which is seemingly the most efficient organization under public ownership), or contracted out to a public limited-liability company, productivity tends to be higher. These findings can be related to the basic trade-offs in the agency literature regarding monitoring, control, and incentives.====Given the influence of the organizational form and ownership, a careful view in the attempt to combine the best of the private and public spheres is warranted. Mixed ownership models, as well as certain public legal forms, can be associated to an inferior outcome in terms of productivity than either of the pure cases.====Importantly, our findings are conditional on the particular sector of refuse collection, as well as the institutional environment in Germany and thus do not necessarily apply to other constellations. In fact, the diversity of outcomes that arise depending on the actual conditions is demonstrated by a series of empirical contributions on efficiency of public services.==== In line with the theoretical literature on the public-private debate, it is important to consider the precise circumstances when making predictions on public service provision.====The paper is structured as follows: The following section discusses the background of the German refuse collection market with an emphasis on regulatory issues and structured ideas around TFP in public utilities. Section 3 presents our empirical strategy. In Section 4 we present and discuss the results and add robustness checks in Section 5. Section 6 concludes.",Property rights and transaction costs – The role of ownership and organization in German public service provision,https://www.sciencedirect.com/science/article/pii/S0167718720300606,10 June 2020,2020,Research Article,168.0
"Siegert Caspar,Ulbricht Robert","Bank of England, London, United Kingdom,Boston College, Newton, MA, USA","Received 21 October 2019, Revised 19 May 2020, Accepted 26 May 2020, Available online 10 June 2020, Version of Record 24 June 2020.",https://doi.org/10.1016/j.ijindorg.2020.102639,Cited by (12),"We explore how pricing dynamics in the European airline ==== vary with the competitive environment and with customer heterogeneity. We document three main findings. First, the rate at which prices increase towards the scheduled departure date is significantly reduced in more competitive markets. Second, the sensitivity of the intertemporal slope to competition increases in the heterogeneity of the customer base. Third, ex-ante predictable advance purchase discounts account for 83 percent of within-flight dispersion in prices and for 17 percent of cross-market variation in pricing dynamics.","The tendency for airline ticket prices to rise as the scheduled departure date approaches is a well-known regularity of airline markets. Yet, while there is an active literature researching pricing dynamics in monopoly markets, our understanding of dynamic oligopoly markets is still limited. How does competition affect the scope for intertemporal price differentiation? Which markets are most likely to see steep intertemporal price gradients? Are price increases predictable conditional on the market environment, or do they appear random from the perspective of an outside observer?====In this paper, we use novel data on the time path of prices from the European airline industry to study these questions. We empirically explore how pricing dynamics vary with the competitive environment, document a pivotal role of customer heterogeneity for determining the intertemporal gradient, and investigate the importance of ex-ante predictable advance purchase discounts relative to (possibly stochastic) residual volatility for realized price dynamics.====To explore how moving from monopoly markets to oligopoly markets affects pricing dynamics, we begin our analysis by estimating the intertemporal slope of prices and its sensitivity to competition. Overall, we find that prices in our sample increase substantially over time, but at a rate that is highly sensitive to competition. While monopoly prices increase by an average of 1.31 percent with every day that a customer waits to book, this slope is reduced to 1.19 percent in duopolies and continues to decrease monotonically to a slope of 0.90 percent in markets with 5+ competing airlines. A nonparametric treatment of pricing dynamics further reveals that these differences are mainly driven by the last 5 weeks before departure. Over this period, prices on monopoly routes increase by roughly twice as much as they do on routes served by 5+ competitors (159 vs. 80 percent).====In light of this discrepancy, a natural question then is, “Does competition universally flatten the intertemporal price gradient in all markets, or does it do so only in certain markets?” We approach this question by looking at one potentially important enabling factor—customer heterogeneity. To do so, we construct a novel index of customer heterogeneity that is based on the realization that high levels of customer heterogeneity are likely to induce ==== airlines to engage in intertemporal price differentiation.==== Building on this realization, our index combines eight market indicators, each designed to capture variations in the intensity of tourist to business travelers, by identifying the particular linear combination that is most likely to steepen the intertemporal price gradient on monopoly markets.====Our results provide strong support for the notion that competition affects pricing dynamics ==== by customer heterogeneity. In markets with a highly heterogeneous customer base, competition flattens the intertemporal slope from a daily rate of 1.42 percent in monopoly markets to 0.90 percent in markets with five competitors. By contrast, in markets with little customer heterogeneity, we find virtually no effect of competition on the intertemporal slope.====Having documented these patterns, we then ask how much of the observed within-flight dispersion in prices is due to the documented advance purchase discounts, and how much is due to residual price volatility around the competition-specific trends? We find that for the average flight the documented advance purchase discounts account for 83 percent of the overall intertemporal dispersion in ticket prices. That is, while there is some residual volatility around the systematic price gradient, a large share of the intertemporal price dispersion that we observe is explained by ex-ante predictable advance purchase discounts. Lastly, looking at the ==== in pricing dynamics across markets, we find that about 17 percent of the observed cross-market variation is explained by competition and customer heterogeneity.====From a theoretical perspective, our understanding of pricing dynamics in ==== markets is currently limited, mainly due to the challenge of incorporating strategic interactions into stochastic control problems. While a few studies explore dynamic oligopoly models, these treatments are rare, typically focus on stylized settings with two selling periods as in Anton et al. (2014) and Dana and Williams (2019), and their predictions are sensitive to modeling choices.====The primary aim of this paper is to provide empirical guidance for future theoretical developments. Nevertheless, we offer some ==== interpretation of our results in Section 6. In our view, the documented impact of customer heterogeneity on pricing dynamics hints towards intertemporal price discrimination as a likely source of advance purchase discounts. Intuitively, monopoly airlines discriminate against late booking customers with inelastic demand, but are restrained in their ability to do so in more competitive environments. In line with this intuition, we expect that the intertemporal price gradient is particularly sensitive to competition when there is a high potential to discriminate against late booking customers in the first place, which is precisely what our findings regarding customer heterogeneity have shown.====On the other hand, our findings also indicate that even in highly competitive markets, prices tend to systematically increase over time, suggesting another force at play. One possibility is that airlines face aggregate uncertainty regarding their demand, which in combination with capacity constraints may support advance purchase sales to low-valuation customers even in perfectly competitive markets (Dana, 1998).==== A corollary to such a stochastic demand interpretation is that even though prices may ==== increase over time, the ==== price path will depend on the realized demand and cannot be perfectly predicted by ex-ante market characteristics. Our results regarding the predictability of within-flight price dispersion and cross-market differences indicate that this is indeed the case.====From an empirical perspective, the analysis of pricing dynamics has proven difficult mainly due to a lack of public data. In the airline industry, public price data is available only at a route-quarter level that pools prices across different itineraries and travel dates, preempting the study of pricing dynamics for a given flight. For a long time, the literature has therefore focused on the impact of competition on broad measures of price dispersion (e.g, Borenstein, Rose, 1994, Hayes, Ross, 1998, Gerardi, Shapiro, 2009, Dai, Liu, Serfes, 2014).====In this paper, we address this issue using posted price data collected from a leading online booking website. In particular, we construct a panel including about 1.4 million prices for airline tickets on the intra-European market where for each route-date pair, we record a time series of posted prices ranging from 10 weeks to 1 day prior to departure. Using this time series dimension permits us to shift the focus on pricing dynamics and their determinants.====To relate our findings to the earlier price dispersion literature, we also use our data to disentangle the impact of competition on different dimensions of price dispersion. In line with the documented pricing dynamics, we find that competition has an unambiguously negative impact on within-flight price dispersion. However, if we compute price dispersion by pooling across different travel dates or across different flights within a given route, that relation becomes diluted (in the former case) or even overturned (in the latter case). Hence, while we find competition to have an unambiguous negative impact on intertemporal price dispersion, the relation between competition and cross-flight dispersion in our data is less clear, which may explain seemingly contradictory findings in the earlier literature.==== Related to our attempt to disentangle various dimensions of price dispersion are Puller et al. (2015), and especially Gaggero and Piga (2011) who have also documented a negative relationship between competition and within-flight price dispersion.====In using hand-collected data from the internet, we join a recent generation of papers with similar data strategies. Lazarev (2013) and Williams (2017) use scraped data to estimate dynamic monopoly models, but do not have data for other competitive environments. By contrast, Gaggero and Piga (2011), Escobari (2012) and Escobari et al. (2019) have price data for oligopoly markets, but are interested in the determinants of price levels and dispersion rather than pricing dynamics. Specifically, Escobari (2012) explores how prices adjust to demand shocks, and Escobari et al. (2019) study price-discrimination between bookings in business hours versus bookings in the evening. To the best of our knowledge, this is the first paper which investigates empirically how pricing dynamics vary with the competitive environment.====The paper is structured as follows. Section 2 describes the data. Section 3 documents how pricing dynamics vary with the competitive environment. Section 4 introduces our measure of customer heterogeneity, and investigates how it interacts with competition in determining pricing dynamics. Section 5 decomposes price dispersion along various dimensions. Section 6 offers some interpretation of the findings.",Dynamic oligopoly pricing: Evidence from the airline industry,https://www.sciencedirect.com/science/article/pii/S016771872030062X,10 June 2020,2020,Research Article,169.0
Schmidt Peter,"Department of Economics, Bocconi University, Italy","Received 2 August 2019, Revised 13 April 2020, Accepted 23 April 2020, Available online 21 May 2020, Version of Record 9 June 2020.",https://doi.org/10.1016/j.ijindorg.2020.102622,Cited by (12),"Free-Floating Car Sharing is a potential substitute to private car ownership. Its staggered rollout in German cities is exploited with a difference-in-difference methodology using an original administrative panel dataset on car registrations to estimate the effect of Free-Floating Car Sharing on new car sales. One car sharing vehicle reduces annual new car sales by three vehicles. This effect is driven by a reduction in sales of small, compact and medium-sized car models.","Populations of large cities around the world are forecast to increase by 2.5 billion people by the year 2050.==== Should urbanites have to rely on privately owned cars for transportation, traffic congestion, scarcity of parking as well as air pollution will cause massive welfare losses. It is therefore crucial to identify and implement at large scale alternatives to private vehicle ownership in large cities.====Car sharing is a promising way to enable a more efficient use of vehicles.==== It is part of a broader trend called the ”Sharing Economy”. Sharing Economy business models enable access to durable goods without a transfer of ownership, thereby potentially increasing allocative efficiency====. While both Business-to-Consumer (B2C) and Peer-to-Peer (P2P) car sharing schemes have been operating for decades, advances in smartphone and internet platform technology have enabled technologically superior car sharing services to emerge in recent years. The most impactful new B2C car sharing service is called Free-Floating Car Sharing. Companies offering this service provide vehicles with which users can do one-way trips within a city and are not required to drop the car sharing vehicle off where they picked it up, as is the case with the longer-established Station-Based Car Sharing.==== The new service has been picked up enthusiastically in Germany since the early 2010s (see Fig. 1).====In this paper I examine whether the increase in Free-Floating Car Sharing usage has had an effect on private vehicle sales. More specifically, do inhabitants of cities where Free-Floating Car Sharing is available buy fewer new cars, because they substitute private car ownership with using Free-Floating Car Sharing? The decision to buy or share is the result of a trade-off between the high fixed costs of buying and maintaining a car, but lower subsequent operating costs per km driven and using car sharing, which entails upfront costs close to zero, but is relatively expensive per km driven. Potential complementarities with public transportation may make car sharing attractive to commuters who, for example, can use a shared vehicle to drive to a train station in the morning and back at night without incurring costs for parking their own vehicle at the train station or in the downtown area for the duration of the work day. Finding a parking spot for a privately owned vehicle in inner cities is becoming more and more difficult and expensive, increasing the fixed costs of owning a car and making car-sharing relatively more attractive. From a public policy perspective, freeing up valuable downtown real estate by reducing space reserved for parking privately owned cars is highly desirable.==== The effect of Free-Floating Car Sharing on new car sales is of interest not only to city planners, but also other policy makers, who decide on the regulation of car sharing and other emerging Sharing Economy platforms.==== Regulation should be more favorable towards car sharing providers if they are found to provide a public benefit, such as a reduction in private vehicle sales in big cities.====To quantify the development of Free-Floating Car Sharing in Germany, I assemble a new panel dataset containing launch dates and car sharing fleet sizes on a monthly basis in German cities. To quantify its effect on new vehicle sales, I obtain an original administrative panel dataset of monthly new car registrations split by car model in all German cities from January 2008 until December 2016.==== Having monthly data enables me to leverage the timing of the introduction of Free-Floating Car Sharing in the different cities, while the split at the car model level allows the effect on new car sales to differ across car segments. This paper is the first in which the effect of car sharing on car sales is quantified in a revealed preference framework using observational data, as opposed to surveys of car sharing users, which were commissioned by the car sharing providers.====The two dominant providers of Free-Floating Car Sharing in Germany, ==== and ====, staggered their launches and have varying fleet sizes across German cities (see Table 1 and Fig. 2), creating variation in the availability and usefulness of the service, which allows the identification of its effect on new car sales using a difference-in-difference methodology. Exogeneity of the treatment is given, because launch dates and the timing of increases in fleet sizes were not freely chosen by the car sharing companies, but subject to regulatory approval and logistic considerations. Furthermore, concerns about treatment exogeneity and potentially diverging trends in car sales in treatment and control cities are alleviated by excluding control cities from the estimation sample, using only the variation in the timing of entry in the treated cities to identify the effect.====In contrast to the revealed preference approach taken in this paper, existing studies on the subject survey active car sharing users and extrapolate to the general population to estimate the impact of Free-Floating Car Sharing on private vehicle ownership. The car sharing providers involved in the studies have an incentive to overstate the magnitude of the impact, as a decrease in private vehicle sales is seen as beneficial to society and may be rewarded with more favourable regulation by the authorities. More generally, a stated preference approach is likely to overestimate the effect, as those car sharing users who choose to participate in the survey are likely active users with a high opinion of the service, whose propensity to substitute car purchases with car sharing may be much higher than that of the general population. For these reasons, having reliable empirical evidence obtained through a revealed preference approach is crucial for this research question and this paper is the first to provide it.====According to the difference-in-difference estimation results in this paper, an additional car sharing vehicle in a city leads to a decline in annual new car sales of three vehicles, corresponding to an average decrease in overall new car sales of around 1.5%. This decline is due to decreased sales of small, compact and medium-sized models; larger and/or more expensive models, such as SUVs, luxury cars or vans are not affected. The quantitative similarity and constant statistical significance of the results across a wide variety of different specifications allow the conclusion that Free-Floating Car Sharing is indeed a substitute to private car ownership.",The effect of car sharing on car sales,https://www.sciencedirect.com/science/article/pii/S016771872030045X,21 May 2020,2020,Research Article,170.0
"Heijnen Pim,Schoonbeek Lambert","Department of Economics, Econometrics and Finance, Faculty of Economics and Business, University of Groningen, P.O. Box 800, AV Groningen 9700, the Netherlands","Received 19 April 2019, Revised 16 March 2020, Accepted 6 May 2020, Available online 21 May 2020, Version of Record 3 June 2020.",https://doi.org/10.1016/j.ijindorg.2020.102625,Cited by (1),"We consider a Tullock rent-seeking contest with two firms and two investors. Each investor owns a majority share in one firm and a silent minority cross-shareholding in the other firm. We measure competition by either firms’ aggregate efforts or rent dissipation. We show that aggregate efforts are smaller in our contest than in the benchmark case without cross-shareholdings. Next, we provide the necessary and sufficient conditions such that equilibrium rent dissipation in our contest is larger than in the benchmark case. Rent dissipation is larger under cross-shareholdings if and only if one firm is much more efficient than the other firm, and the cross-shareholding in the more efficient firm is sufficiently smaller than the cross-shareholding in the less efficient firm.","Horizontal minority cross-shareholdings are prevalent in many industries. They can arise in the form of cross-ownership, where firms own shares in rival firms within the same industry, or as common-ownership, where investors external to an industry own shares in different firms in the industry (Brito et al., 2018). Cross-shareholdings are often viewed as an instrument to soften competition. Intuitively, this makes sense. By acting aggressively in a market, a firm’s own profit may increase at the expense of the profits of its rivals. With cross-shareholdings the latter effect is internalized, which may reduce the benefit of acting aggressively.====A recent white paper by the European Commission (EC, 2014) discusses (non-controlling) minority cross-shareholdings between rival firms as a factor that needs to be addressed when judging whether a merger or acquisition can proceed: a precedent was set when, in the metal plant building industry, Siemens acquired VA Tech even though Siemens owned a non-controlling stake in SMS Demag, a competitor of one of VA Tech’s subsidiaries. Another recent report by OECD (2017) discusses the large increase of minority cross-shareholdings with common ownership by institutional investors in US public-listed companies, in particular in sectors like airlines, pharmacies, banks, breakfast cereals and soft drinks. For example, the fraction of US public firms held by institutional investors that simultaneously hold at least 5% of the common equity of other same-industry firms has increased from below 10% in 1980 to about 60% in 2014.==== The same phenomenon holds for Europe, e.g. in the banking sector, and the German chemical industry. Schwalbe (2018, p. 596) reviews the current debate about common-ownership with minority cross-shareholdings and concludes that ”further empirical and theoretical work is needed for a thorough assessment of the extent of common ownership and its effect on competition”.====In this paper we investigate theoretically whether minority cross-shareholdings also imply less fierce competition in the rent-seeking contest of Tullock (1980). This contest can be applied to study, for example, competition in patent races, lobbying behaviour for monopoly rents, and firms’ advertising campaigns for market shares. See Congleton et al. (2008) and Konrad (2009) for comprehensive overviews of the relevant literature.====Specifically, we consider a Tullock contest with common-ownership, with two firms and two investors. The firms compete by exerting effort to win a given, single prize, that will be awarded to one of them. The investors have given financial and (corporate) control rights with respect to the firms (cf. López and Vives, 2019). We suppose that each investor has a (financial) majority shareholding in one firm (its ‘own’ firm) and fully controls this firm’s decisions in the contest. Each investor also has a given silent minority cross-shareholding in the other firm. This means that the investor receives a share of the profits generated by the other firm but has no control of its actions in the contest.====We allow that the marginal costs of effort are different for the firms, i.e. one firm might be more efficient than the other. We investigate the degree of competition in the equilibrium of our contest by either the size of firms’ aggregate efforts or firms’ aggregate expenditures (i.e. the money value of firms’ efforts). We compare the values of these measures to their counterparts in the benchmark case without cross-shareholdings. We refer to the aggregate expenditures as rent dissipation.====As an example, our analysis can be applied to a patent race with two firms and two investors with cross-shareholdings, in which the firms compete with R&D efforts. The firm that innovates first will win a patent of fixed value, the other firm receives nothing (cf. Baye and Hoppe, 2003). Aggregate efforts are most relevant here as a measure of competition, as high efforts accelerate the speed of the innovation. Another example, is a situation where a policy maker will award a license for a monopoly position on a local bus market to one of two rival bus companies, which are owned by two investors with cross-shareholdings (cf. Mathisen, 2016). Suppose that both companies lobby the policy maker to obtain the license for the given monopoly rent. Rent dissipation seems now most relevant as it measures the aggregate lobbying expenditures incurred by both firms in their competition for the prize.====We find that aggregate equilibrium efforts are smaller in the contest with cross-shareholdings than in the benchmark case. Hence, according to this measure, there is less competition in the case with cross-shareholdings. However, this is not necessarily the case if we employ the second measure of competition. We derive the necessary and sufficient conditions such that equilibrium rent dissipation in the contest with cross-shareholdings is larger than in the benchmark case. Loosely speaking, rent dissipation is larger in the contest with cross-shareholdings if and only if one firm is much more efficient than the other firm, and (given this efficiency difference) the cross-shareholding in the more efficient firm is sufficiently smaller than the cross-shareholding in the less efficient firm. We discuss in detail the incentives that drive these results.====Our analysis is closely related to Fu and Lu (2013). However, these authors investigate cross-shareholdings in an all-pay auction with two firms and two investors, whereas we examine a Tullock contest. Fu and Lu (2013) provide the necessary and sufficient conditions such that aggregate equilibrium efforts in their all-pay auction with cross-shareholdings are larger than in the benchmark case without it. Contrary to our result for the Tullock contest, Fu and Lu find that aggregate efforts can be larger in the case with cross-shareholdings. Notice that Fu and Lu (2013) do not examine rent dissipation. One of our contributions vis-á-vis Fu and Lu (2013) is also that we provide a thorough investigation of the incentives that the firms face, based on the distinction between strategic substitutes and strategic complements in the best-response functions (cf. Dixit, 1987). This approach is not possible in the all-pay auction, since in that case the equilibrium is in mixed strategies.====Several other theoretical papers have studied the impact of cross-shareholdings on the degree of competition. Konrad (2006) studies an all-pay auction in which a single firm has cross-shareholdings in one other firm, whereas Clark et al. (2007) compare competition in all-pay-auctions and sealed-bid first-price auctions with cross-shareholdings. Mathisen (2016) presents an application on procurement auctions for Norwegian public transport contracts. The effect of cross-shareholdings has also been examined in oligopoly models. The seminal studies Reynolds and Snapp (1986) and Farrell and Shapiro (1990) show that there is less output and thus less competition in static Cournot models if the degree of cross-shareholdings increases. Malueg (1992) investigates the likelihood of collusion in a repeated Cournot model with cross-shareholdings; for a related study, see Gilo et al. (2006). Dietzenbacher et al. (2000) study static Cournot and Bertrand models and measure competition in terms of the size of price-cost margins. They show that cross-shareholdings are anti-competitive.====Azar et al. (2018) empirically study the effect of cross-shareholdings on competition in the US airline industry. The main concept employed in their study is the Modified Herfindahl-Hirschman-Index (MHHI) developed by O’Brien and Salop (2000). The MHHI is a measure of the degree of concentration on a market with cross-shareholdings, generalizing the traditional Herfindahl-Hirschman-Index. Azar et al. (2018) find that ticket prices are approximately 3–7% higher in the average US airline route than would be the case under separate ownership. Azar et al. (2019) and Brito et al. (2018) find empirical evidence of anticompetitive effects of cross-shareholdings in the US banking sector and wet shaving industry, respectively. On the contrary, Koch et al. (2020) do not find robust evidence of anticompetitive effects of common ownership in other US industries.====As a final observation we mention that our analysis also links to the literature on cross-shareholdings in oligopoly models. One property of the Tullock contest is that it can also be interpreted as a Cournot oligopoly with a constant elasticity demand function (Szidarovsky and Okuguchi, 1997). Effort is then interpreted as quantity produced and the probability of winning the prize is the market price. We will briefly discuss this application of our model, focusing on the size of the MHHI.====The rest of the paper is organized as follows. Section 2 presents the contest with cross-shareholdings and derives its (pure-strategy Nash) equilibrium. It also briefly discusses the equilibrium of the benchmark case. Section 3 compares the equilibria of the two contests and examines our measures of competition. We conclude in Section 4. Proofs are in the Appendix.",Cross-shareholdings and competition in a rent-seeking contest,https://www.sciencedirect.com/science/article/pii/S0167718720300485,21 May 2020,2020,Research Article,171.0
"Loertscher Simon,Marx Leslie M.","Department of Economics, Level 4, FBE Building, 111 Barry Street, University of Melbourne, Victoria 3010, Australia,Fuqua School of Business, Duke University, 100 Fuqua Drive, Durham, NC 27708, USA","Received 28 October 2019, Revised 5 March 2020, Accepted 27 April 2020, Available online 19 May 2020, Version of Record 31 May 2020.",https://doi.org/10.1016/j.ijindorg.2020.102623,Cited by (17),"Increasing returns to scale in data gathering and processing give rise to a new form of monopoly, referred to here as ====. Digital monopolies create new challenges for regulators and antitrust authorities. We address two in this paper: market power arising from improved match values and from reduced privacy. The digital monopoly’s profit and social surplus always increase as privacy decreases. However, consumer surplus is non-monotone in privacy. Without privacy, the match value is perfect but completely extracted by the digital monopoly. In contrast, as privacy goes to infinity, match values and social surplus go to zero. With regulated prices, consumer surplus is maximized without privacy protection. As with natural monopolies, price regulation thus remains an appropriate tool in the digital age to capture the social benefits from increasing returns to scale without harming consumers.","Larger markets are better, all else equal, because they can execute the same trades as smaller, standalone markets, and sometimes execute more or more valuable trades. Consistent with this, Internet-based matchmakers that realize powerful data-driven increasing returns to scale, such as Amazon, Google, and Spotify, have come to prominence in the digital age. Firms that operate in environments for which efficiency dictates that a single firm is optimal are naturally referred to as ====.==== Just as was the case with natural monopolies, digital monopolies call for antitrust scrutiny and possibly regulation. Indeed, recently digital monopolies have received intense scrutiny from antitrust authorities around the world.====Traditionally, regulation and policy intervention have worked best when they were guided by well-defined objectives such as consumer or social surplus. In this tradition, we analyze the pros and cons of interventions in an environment in which a digital monopoly can use data to either improve matching only or, instead, to improve matching ==== to adjust pricing. Although this distinction has typically not been formulated explicitly, it is a key issue in ongoing antitrust debates. As a case in point, it makes a difference to advertisers whether Google uses its data only to better match advertisers to consumers or, alternatively, to improve matching ==== to adjust the (reserve) prices that it charges advertisers.====Based on a parsimonious model in which more data improves the distribution from which the consumer draws its value, with the improvement being in the sense of hazard rate dominance, we show that the distinction has striking implications for the consumer surplus effects of privacy protection. If data are used exclusively to improve match values, then consumer surplus increases monotonically in the data to which the digital monopoly has access. Put differently, in this case privacy protection unambiguously harms the consumer. In sharp contrast, when the monopoly also uses the information about the consumer’s preferences for pricing purposes, the consumer surplus consequences of privacy protection are less clear cut. To a lesser or greater extent, the monopoly extracts part of the additional surplus generated by improvements in matching. In the limit, as the matching becomes perfect, consumers have no private information left and hence lose their entire information rent, while the monopoly captures the entire social surplus. In both cases, social surplus is maximized when all information is revealed to the digital monopoly. However, when data are also used for pricing, the monopoly is not only able to perfectly match the product to the consumer, but also to match the price to the consumer’s value, thereby, in the limit, depriving the consumer of all surplus.====As an example, consider the online firm Ziprecruiter, which matches potential employers to jobseekers. The data collected by Ziprecruiter regarding the characteristics of a potential employer both improves match values, to the benefit of the employer, and allows Ziprecruiter to more precisely estimate the employer’s willingness to pay for the service, to the detriment of the employer.====From a consumer surplus perspective, the central issue is ==== the protection of privacy but rather the protection of information rents. In our model, fixing the level of data held by the digital monopoly, the protection of information rents can be achieved by regulating prices.==== If the price is fixed, then data can only be used to improve match values, and improving match values is in the digital monopoly’s best interest because it increases the probability of a trade, and, of course, is in the consumer’s best interest.====The obvious flip side to the dire implications for consumer surplus when privacy vanishes completely and the digital monopoly’s pricing is not restricted is that producer surplus increases and becomes identical to social surplus. Digital monopolies can thus be expected to resist attempts to regulate their pricing. Apart from this natural, and in many ways inevitable, conflict about the division of social surplus, a potential drawback to price regulation is that it may decrease the digital monopoly’s incentives to invest in data analytics and product quality. If price regulation decreases equilibrium investments substantively, then there is a tradeoff between the social surplus and consumer surplus that can be achieved via regulating pricing.====That being said, this paper is exclusively concerned with issues pertaining to what are sensibly called ==== settings, in which matching individuals’ preferences as closely as possible is what a benevolent social planner would do.====The remainder of this paper is organized as follows. In Section 2, we describe the model. In Section 3, we derive results and discuss price regulation. In Section 4, we extend the model to allow for investments into data analytics and product quality. In Section 5, we provide an extension to allow competition. In Section 6, we discuss related literature and provide additional discussion of the implications of our results for policy debates surrounding property rights. Section 7 contains conclusions.",Digital monopolies: Privacy protection or price regulation?,https://www.sciencedirect.com/science/article/pii/S0167718720300461,19 May 2020,2020,Research Article,172.0
Li Yanhai,"School of Economics and Management, Fuzhou University, Fuzhou 350108, China","Received 21 July 2018, Revised 25 April 2020, Accepted 28 April 2020, Available online 18 May 2020, Version of Record 25 May 2020.",https://doi.org/10.1016/j.ijindorg.2020.102624,Cited by (3),"The reference effect and loss aversion are incorporated into the buyer’s utility in the symmetric independent private value models of sealed-bid auctions. The buyer’s equilibrium bidding strategy and the seller’s optimal reserve price are derived for the first-price and second-price sealed-bid auctions. In both auction mechanisms, the seller’s optimal reserve price and expected revenue are increasing in the reference point. We compare the seller’s expected revenues as well as the optimal reserve prices in the two auctions. The results show that the seller will set a higher optimal reserve price but obtain lower optimal expected revenue in the second-price auction compared to the first-price auction. Further, we extend the model to the gain-seeking case, and endogenize the reference point as the ex-ante expected price of the item in equilibrium. In contrast to the loss-averse case, the seller will set a lower optimal reserve price but obtain higher optimal expected revenue in the second-price auction compared to the first-price auction if the buyers are gain-seeking. With an endogenous reference point, similar results are obtained in terms of revenue comparison between the two auctions.","Auction is an ancient and effective mechanism for price discovery and resource allocation. Several auction formats have been invented, such as British auction, Dutch auction, American auction and sealed-bid auction. Although auctions have long been used in practice, the academic research on auctions only began about sixty years ago, marked by the pioneering work of Vickrey (1961). Under the assumption that the values of the auctioned item for all bidders are independently and identically distributed, Vickrey (1961) establishes the prestigious symmetric independent private value (SIPV) model. Within this framework, many researchers have conducted extensive studies and contributed significantly to the literature. For example, Myerson (1981) demonstrates the well-known revenue equivalence principle, which states that the seller will obtain the same expected revenue under all auction mechanisms if the allocation and payment rules satisfy certain conditions. In the same vein are the studies of Riley and Samuelson (1981) and Harris and Raviv (1981).====On the basis of these seminal papers, an abundant interest and a rich literature on auction theory have emerged. Among various extensions, the psychological aspects of bidder behavior have attained increasing popularity, such as the regret theory (Filiz-Ozbay and Ozbay, 2007), the anticipated joy of winning (Roider and Schmitz, 2012), and risk aversion (Maskin, Riley, 1984, Matthews, 1987, Janssen, Karamychev, 2009). One of the most prominent findings related to decision-making behavior is the reference effect: people assess utilities in comparison with reference points. This effect has been experimentally assessed by Tversky and Kahneman (1974). Following, Kahneman and Tversky (1979) propose a model of individual behavior, where the total utility is a function of both the final consumption bundle and its relation to a reference point. Since then, this behavioral model has received substantial attention, particularly in the finance, economics, marketing, and management literature. The reference effect has also been introduced into the auction theory to explain the deviation of behaviors observed in experiments from the prediction of the traditional model.====Rosenkranz and Schmitz (2007) appear to be the first to explore the implications of the reference effect in auctions by employing an analytical model within the SIPV framework. In their model, each buyer has a reference point for the price of an auctioned item, and the winner will perceive extra utility (disutility) if he pays less (more) than the reference point. They show that the seller’s optimal reserve price increases with the number of buyers. This is in sharp contrast to the standard SIPV model, which predicts that the size of an auction has no impact on the seller’s optimal reserve price. Shunda (2009) extends this work by incorporating a buy-it-now price into the model and assumes that the reference point is a weighted average of the reserve price and the buy-it-now price. The results explain why a seller would like to augment the auction with a buy-it-now price and indicate that the impact of the buy-it-now price on the reference point will induce the seller to set a higher reserve price.====Although the above-mentioned papers have included the reference effect into auctions, they assume the buyers to be loss/gain-neutral and adopt a linear reference-dependent utility function. However, the buyers’ perception of gains and losses could be different. In reality, people are usually more averse to losses than they are attracted to gains. Thus, losses resonate more than same-sized gains. Kahneman and Tversky (1979) are the first to acknowledge loss aversion. Tversky and Kahneman (1991) further consider loss aversion in riskless choices and explain the phenomenon that choice depends on the reference level. More specifically, they indicate that the changes in reference points often lead to preference reversal. In the past decades, the effect of loss aversion has been extensively explored by researchers from various areas raising many noteworthy insights. For example, Wang and Webster (2009) introduce loss aversion into the classical newsvendor problem. They show that the optimal order quantity of a loss-averse newsvendor may increase with the cost and decrease with the selling price, which can never occur in the standard newsvendor model.====Besides the functional form of the reference-dependent utility, another important aspect regarding the reference effect is the method of formulating the reference point. For example, Shalev (2000) extends the game-theoretical models to include the loss aversion characteristics of the players and sets the reference point equal to the expected utility of the player in equilibrium. Köszegi and Rabin (2006) also consider loss aversion but define the reference point in a different way. Specifically, their main argument is that the reference point should be a belief distribution and a player will take expectations over the reference point when evaluating the final consumption bundle. Both these articles assume that the reference point is endogenously determined by the decisions of the players in the game, which in turn will influence the decisions of the players. A similar strand of literature has considered the impact of bidding prices on the reference point. In this respect, Lange and Ratan (2010) use a reference-dependent utility model based on Köszegi and Rabin (2006), while Ahmad (2015) establishes a model based on Shalev (2000). The differences in modelling the reference point and the bidder’s utility lead to very different predictions. For the first-price auction, Lange and Ratan (2010) indicate that bidders with high values overbid while those with low values underbid. On the contrary, Ahmad (2015) obtains the opposite result. For the second-price auction, Lange and Ratan (2010) find that bidders should continue to value bid, while Ahmad (2015) points out that all bidders will deviate from value bidding.====The literature which concurrently incorporates the reference effect and loss aversion into auctions is very limited, and few of them investigate the seller’s optimal decision on the reserve price. Within the SIPV framework, Riley and Samuelson (1981) derive a necessary condition that the optimal reserve price must satisfy, and McAfee and McMillan (1987) further obtain a sufficient condition. Levin and Smith (1996) demonstrate that the seller’s optimal reserve price monotonically converges to the seller’s valuation as the number of buyers increases if the valuation information of the buyers is correlated. Hu et al. (2010) consider the risk aversion of the buyers and the seller, and analyze its impact on the optimal reserve price in the first-price and second-price auctions.====The role and effect of the reserve price has also received substantial attention in the literature. Engelbrecht-Wiggans, 1987 regards the reserve price as a screen level that can affect the size of an auction and further investigates how the seller’s revenue is influenced by the reserve price. Rosar (2014) offers a theoretical explanation for the use of secret reserve prices in first-price auctions. Although the literature on the reserve price is rich, few papers have studied the seller’s optimal decision on the reserve price in an auction with loss-averse buyers.====In light of the above, this paper incorporates both the reference effect and loss aversion into the standard SIPV model, and studies the buyer’s equilibrium bidding strategy as well as the seller’s optimal reserve price in first-price and second-price sealed-bid auctions. We investigate the impact of the reference point on the seller’s optimal reserve price and expected revenue, and conduct a comparison between these two auction mechanisms. Furthermore, our model is extended to the gain-seeking case. Finally, the reference point is endogenized to be the price expectation of the auctioned item in equilibrium. This work is a direct extension of Ahmad (2015) by considering an arbitrary number of bidders and adopting a more general functional form for the reference-dependent utility. Our work is also related to Maskin and Riley (1984) in the sense that both papers generalize the functional form of the bidder’s utility in the standard SIPV setting and obtain similar conclusions with respect to the revenue comparison between the first-price and second-price sealed-bid auctions. Our work differs from Maskin and Riley (1984) in the following ways. First, Maskin and Riley (1984) consider risk aversion, whereas this paper focuses on loss aversion. Maskin and Riley (1984) adopt a very general functional form to describe the bidder’s utility, which contains ours as a special case if solely judging from the functional form. However, Maskin and Riley (1984) impose pretty strong restrictions on the property of the bidder’s utility function, e.g., thrice continuous differentiability. In contrast, we just require the reference-based utility function to be concave. Therefore, our model includes some cases that are not included in theirs. For example, the two-part-linear utility function most commonly used for loss aversion in the literature is concave but not differentiable. Second, we derive the seller’s optimal reserve prices in the two auction mechanisms and compare them, whereas Maskin and Riley (1984) are silent on that. Third, our model is extended by endogenizing the reference point as the expected price of the auctioned item in equilibrium. This makes the bidder’s utility more complicated, which cannot be covered by Maskin and Riley (1984) even in the functional form. Finally, we study the gain-seeking case, while Maskin and Riley (1984) neglect it.====The contribution of our work to the literature is threefold. First, a general form of utility function is adopted to reflect the buyer’s reference-dependent preference. In such a setting, the buyer’s bidding strategy in equilibrium and the first-order condition of the seller’s optimal reserve price are derived for the first-price and second-price sealed-bid auctions. Second, we compare the seller’s optimal reserve price and expected revenue in these two different auction mechanisms. If the buyers are loss-averse, the seller will set a higher reserve price but obtain lower expected revenue in the second-price sealed-bid auction than in the first-price sealed-bid auction. However, the comparative results reveal the opposite relationship if the buyers are gain-seeking. Third, we obtain similar results regarding the comparison of the seller’s expected revenue in the two auction mechanisms when the reference point is endogenized to be the expected price of the item in equilibrium.====The remainder of this paper is organized as follows. In Section 2, we present the model assumptions. In Section 3, we study the buyer’s bidding strategy and the seller’s optimal reserve price in the first-price auction, followed by the second-price auction in Section 4. A comparison analysis of the two auction mechanisms is conducted in Section 5. We extend the model to the gain-seeking case and endogenize the reference point as the expected price of the auctioned item in equilibrium in Section 6. Conclusions and future research directions are provided in Section 7. All the proofs are placed in the Appendix.",Optimal reserve prices in sealed-bid auctions with reference effects,https://www.sciencedirect.com/science/article/pii/S0167718720300473,18 May 2020,2020,Research Article,173.0
"Czarnitzki Dirk,Hünermund Paul,Moshgbar Nima","KU Leuven, Dept. of Management, Strategy and Innovation (MSI), Naamsestraat 69, 3000, Leuven, Belgium,Centre for R&D Monitoring (ECOOM), KU Leuven, Naamsestraat 61, 3000, Leuven, Belgium,Centre for European Economic Research (ZEW), L7,1, 68161, Mannheim, Germany,Maastricht University, School of Business and Economics, Tongersestraat 53, 6211, LM Maastricht, Netherlands","Received 11 March 2019, Revised 25 March 2020, Accepted 11 April 2020, Available online 15 May 2020, Version of Record 15 June 2020.",https://doi.org/10.1016/j.ijindorg.2020.102620,Cited by (24),"Using public procurement to promote private innovation activities has attracted increasing attention recently. Germany implemented a legal change in its procurement framework in 2009, which allowed government agencies to specify innovative aspects of procured products as selection criteria in calls for tender. We analyze a sample of 3410 German firms to investigate whether this reform stimulated innovation in the business sector. Across a wide range of specifications – OLS, nearest-neighbor matching, IV regressions and difference-in-differences – we find a robust and significant effect of innovation-directed public procurement on turnover with new products and services. At the same time, our results demonstrate that public procurement mainly stimulates innovations of more incremental nature rather than true market novelties.","Governments sponsor private research and development (R&D) activities in various ways and the theoretical justifications for such public interventions are well understood in the literature (Arrow, 1962; Hall and Lerner, 2010). In addition to a functioning system of intellectual property rights, supply-side policies such as R&D tax credits (Dechezleprêtre, Einiö, Martin, Nguyen, Van Reenen, 2016) and direct subsidies for private R&D projects (Einiö, 2014; Howell, 2017; Hünermund and Czarnitzki, 2019) are the most important policy instruments at the disposal of governments to date. In recent years, however, also demand-side alternatives such as ==== have attracted increasing attention by policy makers as well as academics (EFI 2013; OECD, 2017; Slavtchev and Wiederhold, 2016).====Governments spend large budgets in various product categories in order to provide their services to citizens. If a share of this public spending were redirected towards more innovative products and services, compared to already established alternatives – so the idea goes – the demand for innovation in the economy would increase significantly (Edler and Georghiou, 2007). At the same time, private firms would have larger incentives to invest in R&D, especially in sectors where there is insufficient private demand, such as green energy, transportation, or health care (Kremer and Glennerster, 2004). The idea is particularly appealing to policy makers in times of continued budgetary pressure, as the additional demand for innovation, and the ensuing incentives for R&D investments, might be created with little to no extra money (OECD, 2016).====In the past, however, the way in which public procurement contracts were awarded constituted a major obstacle for the success of the policy. Procurement agencies tried to define as precisely as possible the products and services they were willing to buy in their call for tender. Moreover, legal frameworks did not offer the possibility to explicitly specify innovative aspects or more broadly defined performance characteristics as selection criteria in procurement auctions. In other words, standard procurement tenders did not allow to specify products or services that were not yet invented or developed. Recognizing this problem, the European Union (EU) passed revised public procurement directives in 2014 that aimed at facilitating the procurement of innovations. Under the new regulatory framework, innovative solutions and yet to be developed technologies can be an explicit part of the contractual arrangements in the procurement process (====, PPCI). This is permitted as long as the transparency of the process is guaranteed and fair rules of competition are not affected. The new directives favor functional specifications in tender calls, which reduce the risk of overly rigid contracts and provide more flexibility for suppliers to come up with innovative ways of meeting buyers’ demands. The revised framework also encourages procurers to take life-cycle cost considerations into account rather than deciding solely on the basis of initial purchasing costs.====In this paper, we exploit the fact that Germany was a forerunner in this policy development and adopted regulations very similar to the 2014 EU directives already five years earlier, in 2009. Thus, although public procurement reforms were only recently initiated at the European level and data for evaluation still need to accumulate, we are able to infer the effectiveness of PPCI as a policy instrument for innovation from the German experience. To this end, we analyze a sample of 3410 firms stemming from the German part of the European Community Innovation Survey in the period of 2010 to 2012.==== As a baseline, we estimate the effect of PPCI on firms’ share of turnover with new products and services by ordinary least squares. To relax parametric assumptions, we then present nearest-neighbor matching results. Subsequently, in order to rule out endogeneity concerns as much as our data permit, we estimate IV regressions based on a procedure suggested by Lewbel (2012), which establishes identification based on higher moment restrictions, without the need for traditional outside instruments. Finally, augmenting our data with additional information from the Tenders Electronic Daily (TED) database, provided by the European Commission, we are able to estimate panel fixed-effects models (i.e., generalized difference-in-differences). We find a positive and statistically significant effect of PPCI on firms’ share of turnover with innovations in all of these specifications. The marginal effect amounts to 8.7 percentage points according to our OLS baseline results and remains at very similar levels for the other specifications too. Based on these estimates, we calculate that the introduction of PPCI increased turnover with new products and services in the German business sector by EUR 13 billion in 2012, which represents 0.37% of GDP. Standard procurement tenders without innovation-related components, by contrast, show no detectable relationship with innovation success in our analyses. Moreover, our results reveal that the effect of PPCI is primarily driven by an increased share of turnover with products and services that are new-to-the-firm, while we find the effect on market novelties to be insignificant.====Our paper is not the first to investigate the effectiveness of demand-side innovation policies empirically (see Appelt and Galindo-Rueda, 2016, for a detailed review). A seminal study by Lichtenberg (1988) establishes a positive relationship between competitively awarded procurement contracts and company-sponsored R&D in a panel of 169 US firms in the period between 1979 and 1984.==== Draca (2013) finds a positive effect of defense-related procurement on firm's patenting activities and R&D expenditures in the US. In particular, related to public procurement for civilian purposes, there are three recent studies that empirically investigate the effectiveness of demand-side innovation policies. In a sample of 1149 German firms, for the time period of 2000 to 2002, Aschoff and Sofka (2009) document higher shares of turnover with market novelties for firms that introduced product or process innovations as a result of demand from public sources. Guerzoni and Raiteri (2015) investigate both supply-side and demand-side technology policies and find a robust positive impact of the latter on innovation spending in a sample of firms from 27 EU member states. Most recently, Slavtchev and Wiederhold (2016) show that public purchases of high-tech products can positively affect private R&D investments at the state-level in the US. These studies do not differentiate between standard public procurement and public procurement contracts with innovation-related components, however. We argue both theoretically and demonstrate empirically that the latter is a necessary condition for reaping the full potential of demand-side driven innovation policies.==== Compared to previous papers in the literature, our empirical setup exploits variation that is induced by a legislative reform of the public procurement law in Germany. Furthermore, we contribute to the literature by investigating the differential effect of public procurement on firms’ success with radical versus more incremental innovations and show that PPCI is more effective in stimulating technology diffusion rather than the development of true market novelties.====The remainder of the paper is organized as follows. Section 2 discusses the theoretical rationale for demand-side innovation policies and outlines recent reforms of public procurement frameworks in the EU. Section 3 describes our data and empirical strategy. Estimation results are presented in Section 4. Section 5 discusses theoretical and policy implications of our findings and Section 6 concludes.",Public Procurement of Innovation: Evidence from a German Legislative Reform,https://www.sciencedirect.com/science/article/pii/S0167718720300436,15 May 2020,2020,Research Article,174.0
"Deltas George,Evenett Simon","Department of Economics, University of Illinois, Urbana-Champaign, United States,Department of Economics, University of St. Gallen, Switzerland","Available online 5 May 2020, Version of Record 27 November 2020.",https://doi.org/10.1016/j.ijindorg.2020.102616,Cited by (2),"Provision of government contract information in English reduces the barriers to participation by foreign suppliers. We measure this effect using data from the country of Georgia, where English translations of government tenders were provided above specified contract size thresholds, which varied over time and across contract types. The provision of English documentation more than doubles foreign company participation for low value contracts, and leads to smaller, though still substantial, increases for higher value contracts. Because foreign bidder participation in Georgia is relatively small, the resulting impact on prices is in the order of only one percent.","Trade barriers have been steadily declining since 1960. The first phase of this decline was formalized through the General Agreement on Tariffs and Trade (GATT); since 1995, multilateral reductions in trade barriers were negotiated through the World Trade Organization apparatus, which also mediated trade-related disputes. The major focus of these trade liberalization efforts was on tariffs, which as a result are now (generally) at very low levels. However, total trade costs remain high, accounting for a third of the import price (Anderson and van Wincoop, 2004). An examination of the nature and the importance of each constituent component of these costs can improve our understanding of the remaining sources of trade frictions, and whether they can be further reduced by suitable polices.====The major portion of the remaining trade costs do not involve tariffs. They consist, among others, of transportation costs, border inspections, product regulation, accessing a distribution channel, obtaining local market knowledge, and cultural factors which may impede marketing and transactions. Language is one such non-tariff barrier: It increases the cost of communicating with distributors, retailers, and customers; it raises the difficulty of assessing the local market environment; it may also raise the transaction costs associated with any commercial documents and contractual agreements. Measuring the degree to which a common language between an exporting and importing country facilitates trade is an old staple of gravity-style regressions; the meta-analysis of Egger and Lassmann (2012) reports of 701 estimates with an average estimated effect of a common language on trade of approximately 60 percent. However, obtaining credible causal effects of language differences is complicated by the fact that a common language is typically associated with many other cultural commonalities. Recent contributions, such as Melitz (2008) and Melitz and Toubal (2014), have increased our understanding of the channels through which language impacts trade, by distinguishing between commonality of mother tongues versus official languages, by accounting for linguistic proximity, and by controlling for a large set of trade-relevant observable characteristics. They have done so, however, by side-stepping to some extent the possible presence of unobserved heterogeneity in trade-propensity at the country-pair level that is correlated with a common language.==== Some of these residual unobserved factors may have an effect on the trade intensity between the countries, e.g., similar tastes for products, affinity for each other's brands, and increased trust, and thus measured language effects are typically the composite of all unmeasured factors correlated with sharing a language between two populations.==== To put it more succinctly, a challenge in estimating causal effects is that language tends not to be time-varying; or at least not exogenously.====In this paper we identify the extent to which language is a barrier to trade using a micro-level dataset of procurement transactions (tenders) in the country of Georgia. Some of these transactions were contracted in the Georgian language, while others were contracted based on documentation available in both the English and the Georgian languages. For firms operating in international government tenders, we treat English as the ====, i.e., we implicitly equate the provision of English documentation as eliminating language barriers. Though this is not unreasonable, it is certainly a lower bound of the effect of using a common language. The partition of transactions in the group with documentation that must be provided in both languages and the group with documentation that need only be provided in Georgian is exogenous, based on estimated value thresholds. These thresholds differ across different types of products and over time. The resulting variation permits identification of causal effects based on a series of discontinuities across the cut-offs, i.e., allows us to estimate the effects of a common language for different transaction sizes; to do so, we use both cross-section (regression discontinuity) and time-series (difference-in-difference) variation.====Though our estimates are based on government tenders, these are likely to be informative for business-to-business transactions as well. Government procurement covers a wide range of products, since it includes purchases of goods, services, and infrastructure projects by bodies at all levels of government (including state-owned or state-controlled enterprises). It also varies in transaction value from small transactions, measured in the thousands of dollars, to transactions reaching into the millions. This variation is reflected in our dataset. Moreover, government procurement is an important part of the economy in its own right. Hoekman (2015) reports that internationally contestable public procurement markets represent five to eight percent of gross domestic product. One difference between government procurement and other transactions is that government tenders are often not based on posted prices, but rather on some type of bidding mechanism. However, many business-to-business transactions are also based on a competitive process.====An important reason to focus on non-tariff barriers to government procurement is that there is a wide range of procurement policies around the world, and plenty of scope for changing these policies. Governments frequently pursue objectives other than maximizing value for money. Those include favoring certain types of suppliers, e.g., small and medium sized enterprises, suppliers from certain regions, or firms owned by persons from certain societal groups.==== Government purchasing decisions are also influenced by industrial policy objectives. For these reasons, governments often discriminate against foreign suppliers. Economic research tended to focus on explicit discrimination, such as bans on foreign bidding, price preference policies that “inflate” foreign bids, performance requirements that discourage foreign firms, and reserving a share of procurement for bids from local firms. Implicit discrimination has been less studied.====This paper investigates one such form of implicit discrimination: providing tender information only in a local language. Not providing documentation in an international lingua franca, such as English, is a form of discrimination because it impacts domestic and foreign firms differentially. For this reason, providing documentation for government tenders in English is recognized as an important facilitating practice by the World Trade Organization; in fact, it is a proscribed practice for large contracts under the Agreement on Government Procurement to which many developed countries are a party to. Provision of documentation in English is also considered an important practice for increasing overall transparency of the government tendering process.====In the country of Georgia, translation has been mandated for contracts that exceed a certain level of estimated value, but the relevant thresholds differ both across the type of contract and over time. For the periods that are pertinent to our dataset, these thresholds are as follows. Tenders for goods and services were required to be translated into English for values that exceeded 0.5 million Lari from April 2011 until August of 2015, at which point the threshold was increased to 2 million Lari.==== The corresponding threshold for construction tenders was one million Lari from April 2011 until August of 2015, at which point it increased to 4 million Lari.==== This type of across contract type and across time period variation provides us with the source of identification for the effects of translation into English on bidder participation and tender outcomes.====Our results show that provision of documentation in English increases participation of foreign bidders dramatically. This effect is particularly pronounced for contracts of moderate to low value, i.e., in the half a million to a million-dollar range. The effects on contract prices are very small, however, because a large increase in foreign participation rates results in only a small increase in total participation: foreign bidders are a small fraction of all bidders. Foreign bidders do not bid more competitively than domestic bidders; thus, increased foreign participation seems to increase competition without changing it qualitatively. There is, though, some weak evidence that domestic bidders themselves bid marginally more competitively in auctions where there is increased likelihood of foreign competition.====This paper is related to two other strands of literature. The first is research on government discriminatory prices against foreign competitors in public tenders. A conclusion of this work is that a government can increase domestic welfare by mildly price discriminating against foreign suppliers (e.g., see Branco, 1994); in fact, it may even be able to lower procurement cost when foreign suppliers have (on average) lower costs than domestic producers (McAfee and McMillan, 1989).==== These gains can be non-trivial (Evenett and Deltas, 1997).====The second is research on auctions with endogenous bidder participation. Entry is defined as the set of costly activities necessary prior to bid submission. The early theoretical literature on this topic considered two extreme cases: prospective bidders observe their contract cost prior to entry (Samuelson, 1985), or they observe their contract cost following entry (McAfee and McMillan, 1987; Levin and Smith, 1994). The importance of entry was highlighted in the work of Bulow and Klemperer (1996), who showed that attracting a single additional bidder can provide more revenue to a seller (or lower cost to a government in a procurement tender) than the best possible negotiating mechanism. When bidders are ex ante heterogeneous in terms of the cost (or valuation) distributions, endogenous entry can lead to some unexpected comparative statics that are pertinent to this paper. For example, when the entry costs of the strong bidder type (e.g., foreign firms) go down, the participation of the weak (domestic) bidder type might be reduced to the point that overall competition declines (see Klemperer, 1999, for a detailed overview of the theory of auctions with endogenous entry). The empirical strand of this literature, which mainly adopts a structural econometric paradigm, focuses on the estimation of the distribution of bidder costs following entry, the nature of these costs (e.g., the extent to which they are firm-specific or correlated across firms), and how much information firms have about them prior to their entry decision. The two polar entry models are nested in modern econometric estimation approaches (Marmer et al., 2013; Gentry and Li, 2014; Gentry et al., 2015).====In the next section we describe the Georgian procurement process and the type of data in our disposal. We then discuss main features of the data, and proceed sequentially to analyze the effect of English documentation on bidder participation, bidder competitiveness, and the level of winning bids. We end the paper with some concluding remarks.",Language as a barrier to entry: Foreign competition in Georgian public procurement,https://www.sciencedirect.com/science/article/pii/S0167718720300382,5 May 2020,2020,Research Article,175.0
"Feng Hong,Fu Qiang,Zhang Lan","School of Economics and Management, Harbin Institute of Technology (Shenzhen), Shenzhen University Town, Shenzhen, China, 518055,Department of Strategy and Policy, NUS Business School, National University of Singapore, Singapore,Institute of Advanced Studies in Humanities and Social Sciences, Beijing Normal University at Zhuhai, Zhuhai, China, 519087","Received 9 January 2019, Revised 29 March 2020, Accepted 8 April 2020, Available online 25 April 2020, Version of Record 11 May 2020.",https://doi.org/10.1016/j.ijindorg.2020.102621,Cited by (4),"We study the increasingly popular “hunger marketing” strategy (the combination of an artificially low price and a supply limit) adopted by many high-tech startups to launch their products. In a two-period model, a firm offers an artificially low introductory price and also imposes a limit on the quantity available for sale in the first period, which leads to a shortage in the equilibrium. We show that when effective word of mouth is present, such a strategy allows a firm to credibly convince the market of the premium quality of its product. We demonstrate that word of mouth plays a critical role in catalyzing the signaling mechanism. When word of mouth becomes more efficient, e.g., enabled by social media, shortage is larger in the equilibrium, and the introductory price falls further. Our study provides a rationale for hunger marketing.","How can a new product quickly build a strong brand image? The phenomenal rise of Chinese smartphone manufacturer Xiaomi is an inspiring tale that deserves thoughtful examination. Xiaomi, which started receiving orders in 2011, has transformed itself from a startup to the third largest smartphone manufacturer in the world.==== Many have praised Xiaomi’s “hunger marketing” strategy and attributed the firm’s success to it. Xiaomi sells its phones exclusively via online platforms in limited batches, which can sell out within seconds; this forces the many disappointed prospective customers to wait for the next flash sale. Xiaomi’s hunger marketing practice differs substantially from that of premium brands, such as Apple and Sony, in selling their flagship products. Besides restricting its supply to create buzz in the market, Xiaomi couples tantalizing scarcity with the lure of bargains on price, which leads to a demand that far exceeds the supply. For instance, its flagship model M4 is similar in terms of configuration to Apple’s iPhone 6, but sells for about half the price.====Many high-tech startups in the United States have adopted similar strategies that combine low prices with limited supply to launch their products. One notable success is the launch of Pebble Watch as “the first affordable smart watch.” In its unprecedentedly successful campaign on Kickstarter, a crowdfunding website, Pebble Technology offered the watch at a discounted price while limiting the number of pre-orders.====These observations puzzle market observers. Take Xiaomi, for example: Its rationed sales created arbitrage opportunities for online vendors, who were able to resell the phone for substantially more than its official price; the M4 was being traded for more than RMB 2,300 on major online consumer-to-consumer trading platforms, despite its official price tag of RMB 1,999.==== Why does Xiaomi forgo the obvious excess demand, while paradoxically leaving money on the table?====Market observers have often attributed such supply limits to sellers’ capacity constraints and viewed the buzz they create on social media as efforts to improve visibility. This paper, however, provides an economic rationale from a signaling perspective. Three features are common to the campaigns described above, which are similar to those of many other high-tech startups. First, in contrast to Apple, both Xiaomi and Pebble Technology were new entrants in the markets without established brand names or loyal clientele. Second, they sell high-tech durable goods that have relatively long life cycles, thereby precluding repeat purchases by experienced consumers within a short time window. As we will show later in the paper, conventional signaling devices could lose their bite in this context because of these two features. Third, social media has been used intensively by these firms to catalyze interactions between users and prospective consumers. Xiaomi, for instance, created an online community that allows users to exchange information and provide feedback. Pebble Technology promoted itself and its watch heavily and became one of the hottest topics on Kickstarter’s discussion boards, as well as other popular social media platforms such as Facebook and Twitter. New companies are constrained in their ability to convince the market of their product’s superior quality, given the lack of an existing reputation and the durable-good nature of its product. We demonstrate that the combination of low price and quantity limit, in the presence of the active and powerful word of mouth (WOM) enabled by social media, gives rise to an effective signaling mechanism.====A two-period model is created to depict the case described above. A durable good is launched in the market at the beginning of the first period, and consumers can purchase it in either period. Its quality can be either high or low and is privately known to the firm only. We find that when WOM is sufficiently powerful, a Pareto-dominant separating equilibrium exists. The high-quality firm sets an artificially low price in the first period, while imposing a limit on the quantity available for sales in the period. Consumers infer its premium quality, and the restricted supply thus leads to shortage in the presence of an overwhelming demand, thereby forcing unserved consumers to wait until the next period to pay a full-information price.====The success of the signaling mechanism largely relies on effective WOM. In our context, this refers to ====—the most common form—which accounts for 50% to 80% of WOM activities.==== The product is considered to be an experience good, so its quality is fully revealed upon consumption. Early buyers spread their feedback, e.g., by posting reviews online, thereby allowing a portion of unserved consumers to learn about the product. Speedy communication limits the profitability of a low-quality firm’s mimicry, and facilitates credible signaling. To understand the logic, note that the high-quality firm in a separating equilibrium lowers its first-period price below the marginal production cost of its low-quality counterpart. Hence, sales in the first period incur a loss, even on a mimicking low-quality firm, and the firm must recoup its loss by profitable future sales. However, experiential WOM exposes the mimicking firm’s inferior quality to the portion of unserved consumers who have received feedback, which squeezes its potential demand in the second period. WOM catalyzes a separating equilibrium and does not directly contribute to the high-quality firm’s sales: Consumers purchase the high-quality firm’s product not because of the recommendation via WOM; instead, they are convinced of its value upon observing the firm’s credible signal, i.e., rationed sales at discounted price.====The high-quality firm’s quantity choice in the first period involves subtle strategic trade-offs. On the one hand, the quantity available for sale must be limited to cause a significant shortage. The shortage caps its first-period loss and leaves sufficient remanent market for sales at the complete-information price in the second period. That is, offering only a small quantity for early sales signals the firm’s confidence in future demand. On the other hand, the quantity cannot be excessively small. First, the loss, as a part of the signaling cost, must be sufficiently large to deter mimicry. Second, sufficiently broad experiential WOM must be in place to deter mimicry, which requires that nonnegligible early sales jump-start active interaction between early buyers and unserved consumers. The equilibrium choice must strike a balance between these concerns.====Our paper yields important insights about how WOM and social media contribute to marketing practices. WOM’s power has long been recognized by both academia and practitioners. Consumers highly value opinions delivered to them directly, and WOM is estimated to have been the primary factor for up to 50% of all purchasing decisions.==== The conventional view typically stresses the tailwind that positive feedback creates to promote sales.==== Our study, however, stresses an alternative channel to harness its power: Negative WOM deters mimickry.====In the equilibrium, the more effective the WOM, the lower the introductory price and the more severe the shortage (i.e., the smaller quantity available) in the first period. This prediction provides a theoretical account for the anecdotes of many high-tech product launches, such as the aforementioned Xiaomi smartphones and Pebble Watch. The ascent of social media has transformed WOM from traditional one-to-one communication into a one-to-many broadcasting network, which tremendously expands the outreach of user feedback and accelerates its outspread. Our predictions could add to the playbook for marketing such products in the digital age.====In addition, we demonstrate—by comparing the outcome of the separating equilibrium to that of a pooling equilibrium—that the aggressive signaling strategy can be excessively costly. Firms may prefer other marketing options when launching a product. Our results delineate the conditions under which such a signaling strategy is (not) preferable, which provides straightforward implications for marketing practice. The practical implications of our results will be further detailed at a later point.====The rest of the article proceeds as follows. We discuss our study’s link to the relevant literature in the next section. The model is laid out in Section 3. Our analysis is described in Section 4; we begin with separating equilibria, then consider pooling equilibria. Section 5 concludes and summarizes the managerial implications of our findings.",How to Launch a New Durable Good: A Signaling Rationale for Hunger Marketing,https://www.sciencedirect.com/science/article/pii/S0167718720300448,25 April 2020,2020,Research Article,176.0
Emons Winand,"Universität Bern, CEPR, Swiss Competition Commission, Switzerland","Received 19 October 2018, Revised 26 March 2020, Accepted 8 April 2020, Available online 23 April 2020, Version of Record 1 May 2020.",https://doi.org/10.1016/j.ijindorg.2020.102619,Cited by (9),"An antitrust authority grants leniency pre- and post-investigation. It chooses the probability of an investigation. Firms pick the degree of collusion: The more they collude, the higher are profits, but so is the probability of detection. Firms thus trade-off higher profits against higher expected fines. If firms are sufficiently patient, leniency is ineffective; it may even increase collusion. Increasing the probability of an investigation at low levels does not increase deterrence. Increasing the probability of an investigation at high levels reduces collusion, yet never completely. With bare pre-investigation leniency, deterrence is better than without leniency. If firms are sufficiently impatient, granting leniency pre- and post- is better than merely pre-investigation.","A corporate leniency program reduces the sanctions for self-reporting cartel members. In 1993 the US Department of Justice revised its Corporate Leniency Program, committing itself to the lenient prosecution of the first confessor. It allows amnesty to be awarded even when an investigation has already been started. This revision is considered as the most significant policy innovation in antitrust. It substantially increased the number of detected and convicted cartels. The apparent success led the EU to adopt its own leniency program in 1996. Other countries followed suit.====The literature on leniency typically assumes that firms either fully collude or they do not collude at all: they set, for example, either the monopoly or the competitive price. In this paper we give up this binary choice. Our firms choose the degree of collusion—a continuous variable. They may, e.g., pick the fraction of markets on which they collude; or they may set any price between the competitive and the monopoly one.==== Firms’ profits are increasing in the degree of collusion, yet so is the probability of detection: the more markets firms collude on, the higher is the probability that the antitrust authority (AA) finds out the illegal behavior once it opened an investigation. Firms thus trade-off higher profits against higher expected fines.====Legislation specifies the fine and full leniency for the first reporting firm. We first focus on generous pre- and post-investigation leniency, i.e., leniency granted before and after an investigation has started. Then we consider pre-investigation leniency which is granted only before an investigation commenced.==== The fine is proportional to the degree of collusion. The AA picks the probability with which it starts an investigation. For each probability of investigation we determine the corresponding degree of collusion.====We consider two collusive strategies which differ in firms’ behavior in case of an investigation. Either firms do not reveal the illegal behavior once an investigation started; they make the collusive profits, yet both firms pay the fine when detected. Or firms exploit leniency: if the AA opens an investigation, they simultaneously reveal and stop collusion during the investigation; both firms then have a 50% chance of receiving leniency. Firms continue collusion after the agreed upon reporting.====First we show that if firms are sufficiently patient, leniency does not increase deterrence. Either firms collude and do not reveal in case of an investigation: then the incentive to report the cartel and get leniency is too small for patient firms. Or firms collude and reveal in case of an investigation: then firms actually collude on all markets.====Next we look at the degree of collusion as a function of the probability of an investigation. If an investigation is unlikely, firms collude and reveal in case of an investigation. Under this strategy firms collude on all markets. Increasing the probability of an investigation does not lower the degree of collusion. By contrast, if an investigation is sufficiently likely, firms collude and do not report in case of an investigation. Here the degree of collusion decreases with the probability of an investigation. Nevertheless, firms always choose a positive degree of collusion. The fine is proportional to the degree of collusion. Slightly colluding has no first-order effect on the fine, yet it raises profits.====Pre- and post-investigation leniency thus produces mixed results in our set-up. With patient firms it has no bite and is, therefore, redundant. Moreover, it opens the door for the strategy collude and reveal in case of an investigation which, in turn, goes together with full collusion. Firms actually play this strategy for low probabilities of investigation. Thus, in this case leniency induces full collusion. More statements about the effectiveness of leniency are not possible without further specifying the model. Yet, our set-up generates another message. As long as the fine is proportional to the degree of collusion, firms will always collude: a small increase in the degree of collusion has no first-order effect on the expected fine but a positive first-order effect on profits.====Two of our results do not hold if firms can only choose between no and full rather than from a continuum of degrees of collusion: First, that leniency has no bite with patient firms and second, that there is always some collusion. The assessment of the efficacy of leniency thus depends on the degree of collusion firms can choose from.====Next we consider pre-investigation leniency. If leniency is not granted after an investigation started, the strategy collude and reveal is no longer a valid option. We are thus left with the strategy collude and not reveal. If firms are sufficiently patient, leniency does not affect the degree of collusion: the incentive to report the cartel and get leniency is too small for patient firms. By contrast, if firms are sufficiently impatient, leniency reduces the degree of collusion, yet never to zero. Pre-investigation leniency is, therefore, always desirable as compared to no leniency, a standard result in the literature. Nevertheless, post-investigation leniency provides better deterrence than pre-investigation leniency if firms are impatient.====Our paper builds on the analysis of leniency programs by Motta and Polo (2003), Spagnolo (2004), Aubert et al. (2006), Harrington (2008), and Chen and Rey (2013).==== This literature analyzes the effects of leniency on the frequency of collusion and derives optimal fine structures.====Our basic set-up is related to Motta and Polo (2003). Besides in the degree of collusion (binary versus continuous), our framework differs from theirs in another respect: In Motta and Polo (2003) the AA chooses the probability that it opens an investigation and the probability that it successfully concludes the investigation. In our set-up the resources the AA puts into an investigation are exogenously given. Firms determine by their choice of the degree of collusion the probability that the investigation leads to a conviction. The AA, in turn, chooses the probability of an investigation. Moreover, following Spagnolo (2004), we take the optimal deviation from collusion as “undercut and report” (using the Bertrand game terminology); the deviator thus gets the entire profits and avoids the fine. By contrast, Motta and Polo (2003) take the deviation from collusion as “compete and report” so that the deviator makes zero profits and avoids the fine.====A few papers look at variable degrees of collusion, their focus is, however, not on leniency. In Block et al. (1981) the probability of detection is an increasing function of the price; in Harrington (2004) and Harrington (2005) it increases with the price change. In Bos et al. (2018) the probability of detection is a non-decreasing function of price. It is positive even when firms charge the competitive price, and so is the fine. This means firms face a positive expected fine at the competitive price. Therefore, cartels with low overcharges do not form in the first place.====The rest of this paper is organized as follows: The next section describes the model. In Section 3 the AA grants leniency before and after an investigation has started. In Section 4 we look at the case where only pre-investigation leniency is available. In Section 5 we discuss our approach. Section 6 concludes.",The effectiveness of leniency programs when firms choose the degree of collusion,https://www.sciencedirect.com/science/article/pii/S0167718720300424,23 April 2020,2020,Research Article,177.0
He Leshui,"Department of Economics, Bates College, Pettengill Hall, Bates College, Lewiston, Maine 04240, United States","Received 18 September 2019, Revised 10 January 2020, Accepted 1 April 2020, Available online 17 April 2020, Version of Record 8 May 2020.",https://doi.org/10.1016/j.ijindorg.2020.102618,Cited by (3)," equilibria with pre-filing settlements exhibit a cutoff feature. These equilibria offer an explanation for the distinctive behaviors of ====, who aggressively file lawsuits for settlements, and Portfolio PAEs, who obtain licensing revenues through pre-filing settlements. Our comparative statics results lead to testable hypotheses regarding how ==== and the plaintiff’s reputation affect the settlement amount before and after the cases are filed, and the frequency of lawsuits from a given plaintiff.","Individuals and entities resolve countless disputes in the shadow of litigation. Most of these resolutions involve a settlement even before the case is formally filed for a lawsuit—a fact beckoning a number of intriguing questions. First, a dispute not reaching the filing stage may end entirely differently—either settled pre-filing or abandoned. When can we expect disputes to reach pre-filing settlements? Moreover, disputes before the filing stage are plagued by non-credible threats of a lawsuit, i.e. ==== cases, whose expected court judgment is dwarfed by the legal costs involved in pursuing litigation, such that they would be abandoned even without a settlement. Can we expect NEV cases necessarily be abandoned? If not, what externalities do they cast on positive expected value cases that could have been resolved pre-filing? How do they affect cases and settlements that proceed to formal litigation? The understanding of litigation bargaining would be incomplete without thorough consideration of the strategic interactions and the consequent selections prior to the filing decision.====We use to term ==== to refer to settlements that occur before a dispute is filed for a lawsuit. Although mostly omitted in the law and economics literature, the pre-filing settlement is no trivial matter. Patent assertion entities (PAEs) or “patent trolls,” who derive most of their revenues by asserting patent rights against alleged infringers instead of applying patents for productive uses, are a case in point. Their entire business model hinges on legal settlements, which are frequently called ==== licensing fees. If obtained through the demands from the PAEs, such fees are effectively pre-filing settlements under the potential threat of lawsuit. Commission (2016) documents that a sizable portion of PAE revenue comes from such pre-filing settlements in the PAEs of the wireless chipset industry. Two distinct practices stand out in their study. Some PAEs “predominantly negotiated licenses covering large portfolios, often containing hundreds or thousands of patents, frequently without first suing the alleged infringer.” Others “sued potential licensees and settled shortly afterward by entering into license agreements with defendants covering small portfolios, often containing fewer than ten patents,” often through layers of affiliated corporate entities. FTC refers to the former type as “Portfolio PAEs” and the latter as “Litigation PAEs.”==== And it finds that “Portfolio PAE licenses generated total royalties that were much greater, on average, than those of Litigation PAE licenses”.====It is puzzling why the Portfolio PAEs, who have potential lawsuits with high merit and considerable damage, often choose not to litigate and are able to receive higher payments, while the Litigation PAEs, with much weaker cases, resort to litigation to obtain much lower compensations.==== Adding more mystery to this narrative, when Portfolio PAEs litigate, the suits involved more patents and took much longer to settle. If Portfolio PAEs can extract payments before litigation, why bother filing suits at all? Meanwhile, why do the Litigation PAEs choose not to imitate the practices of the Portfolio PAEs to file fewer suits, extract larger settlements and save on legal costs? A framework explaining these observations consistently would add new insights for policy analysis and discussions regarding PAEs and other contexts involving pre-filing settlements.====This paper analyzes a model of pre-filing settlement driven by strategic communications. A risk-neutral (prospective) plaintiff sustains some privately observed legal damage from a (prospective) defendant. The risk-neutral defendant has a commonly known belief over the distribution of the damage, but not its exact value. Before formally filing a lawsuit, the plaintiff may choose to demand a settlement payment from the defendant in exchange for release from further liability.==== If the defendant pays a desirable settlement, the dispute is resolved. Otherwise, if negotiations break down, and the case has sufficient merit, the plaintiff may choose to file suit, which incurs further legal costs for both parties. Once the lawsuit is filed and some legal costs incurred, the defendant learns the true damage through discovery. Then, both sides may have another opportunity to settle before going to court for adjudication.====The model explores a context in which a risk-neutral plaintiff makes a demand to an uninformed defendant, while (1) the threat of a lawsuit may not be credible; (2) the plaintiff has strong commitment power to make take-it-or-leave-it demands; and (3) entry to the settlement bargaining game is free. These three features are defining characteristics in contexts involving pre-filing threats such as PAE-related lawsuits. The second feature sets the current model apart from previous works concerning NEV cases (Nalebuff, 1987, Bebchuk, 1988, Meurer, 1989, He). The third feature highlights the distinction from the well-studied pre-trial bargaining context (Farmer, Pecorino, 2007, Reinganum and Wilde, 1986). We characterize the key features of all pure and mixed-strategy perfect Bayesian equilibria with any pre-filing settlement that satisfies the intuitive criterion (IC) (Cho and Kreps, 1987) (henceforth ==== for short) under continuous and differentiable probability distributions over the legal damage.====In the pre-filing bargaining game, the well-known separating equilibria (Farmer, Pecorino, 2007, Reinganum and Wilde, 1986) no longer exist due to the combination of free-entry to the bargaining game, and the presence of non-credible threats. All equilibria with pre-filing settlements exhibit a partial pooling structure, in which a plaintiff with a private case value below a settlement cutoff sends out a demand that induces the defendants to pay acceptable pre-filing settlements with certainty, while a plaintiff with a value above the cutoff files suit with strictly positive probability. The pure-strategy equilibrium is unique and fully characterized by a pre-filing settlement cutoff. Although the set of mixed-strategy equilibria is complex, we characterize its essential properties and relate them to the pure-strategy equilibrium.====In these equilibria, all plaintiffs with NEV cases send a demand, but not all defendants receiving a demand face NEV cases. Given the belief of the plaintiff’s strategy, the defendant evaluates the chance that the case will be filed if she ignores the received demand. The defendant calculates the expected loss by weighing potential legal damages plus legal costs with the chance that the plaintiff will file the case, and is, therefore, willing to pay up to this amount pre-filing to avoid a lawsuit. However, because the defendant’s highest willingness to pay before litigation is weighed down by low-value cases and the possibility of empty threats, plaintiffs with high enough case values would rather incur extra legal costs to litigate and force the defendant to discover the case value. Once the high case value is credibly revealed to the defendant through discovery, the pre-trial settlement net of the legal costs can still exceed the pre-filing settlement amount—leading to potential partial separation in the equilibria.====Interpreting our model in the context of two PAE types, each prospective plaintiff’s public reputation and the cited patents in a demand letter induce a prior belief of potential legal damages from the defendant’s perspective. Different beliefs lead to different equilibria that resemble the stylized facts stated in the FTC report. For instance, a prospective defendant may believe that a PAE with an extensive patent portfolio or a history of successful litigation is more likely to own a patent with higher legal damages against the prospective defendant; whereas a PAE with a history of frivolous lawsuits is more likely to own a patent with lower legal damages. It is this defendant’s belief and the plaintiff’s own evaluation of the underlying damages that jointly determine the equilibrium outcome. Our comparative statics show that a plaintiff associated with a greater likelihood of high legal damage can reach pre-filing settlements that others cannot. They also earn greater pre-filing settlements and receive greater average pre-trial settlements. In other words, with a greater perceived legal damage, an organization such as a portfolio PAE can extract higher pre-filing settlements, therefore settling a greater range of patent disputes, and only resorting to litigation on the very high-value cases. On the other hand, the limited credibility from a prospective plaintiff with lower believed damages, such as a litigation PAE, can only extract a lesser amount of pre-filing settlement. As a result, the litigation PAE is forced to file lawsuit for an extensive range of cases. This endogenous selection effect leads to more frequent litigation, a lower average value of filed lawsuits, and smaller pre-trial settlements for litigation PAEs. Our results, therefore, consistently reproduce the puzzling stylized facts regarding Portfolio PAEs and Litigation PAEs. We study comparative statics of the effect of changing legal costs on the existence and character of equilibria. The results are summarized in Table 1.====Recent legal studies literature proposes several policy instruments to mitigate the adverse impact of PAEs’ litigation threats on producing entities, including reducing litigation cost for the defendant or raising litigation cost for the plaintiff. Our comparative statics analysis shows that these policies may have complex effects. For example, according to our model, reducing litigation costs for the defendant leads to lower settlement amounts both pre-filing and pre-trial, but also to a higher frequency of lawsuits. When the prospective defendant faces lower cost from litigation, her willingness to pay for pre-filing settlement abates. This effect lowers PAEs’ ability to extract pre-filing settlements through demand letters, but also drives more intermediate-value cases toward lawsuits, thus increasing the frequency of lawsuits, but weighs down the average pre-trial settlement amount from higher-value cases.====The rest of the article proceeds as follows. Section 2 summarizes the most closely related literature. Section 3 sets up of the model. Section 4 characterizes the common features of all equilibria, fully characterize the pure-strategy equilibria, and illustrates their key features. Section 5 analyzes the comparative statics, and interprets them in light of the PAE context. Section 6 characterizes mixed strategy equilibria. Section 7 discusses additional extensions and robustness, and Section 8 concludes. All proofs are presented in the appendix.",A theory of pre-filing settlement and patent assertion entities,https://www.sciencedirect.com/science/article/pii/S0167718720300400,17 April 2020,2020,Research Article,178.0
"Maruyama Masayoshi,Zennyo Yusuke","Graduate School of Business Administration, Kobe University, 2-1 Rokkodai, Nada, Kobe, Hyogo 657-8501, Japan","Received 19 February 2019, Revised 31 January 2020, Accepted 29 March 2020, Available online 11 April 2020, Version of Record 21 April 2020.",https://doi.org/10.1016/j.ijindorg.2020.102617,Cited by (2),", and social welfare. The results suggest a possible conflict between platforms and competition authorities.","Online platforms are usually serving their users in an effort to provide a better buying environment. For instance, online marketplaces (e.g., Amazon, eBay, iBookstore, and Kindle Store) and online travel agents (e.g., HRS, Booking.com, and Expedia) provide valuable services ranging from user-friendly search interface to on-target recommendation system. While investing in such demand-enhancing facilities, they earn commissions from the sale of goods.==== Consequently, they strive to prevent sellers from charging lower prices on rival platforms. To do so, they typically impose a platform most-favored-customer (PMFC) clause in their contracts with sellers, which is a general promise by a seller to treat the platform as favorably as the seller’s most-favored-customer related to price, availability, and the other terms of a transaction.==== When related to prices, a PMFC clause is an agreement between a seller and a platform stipulating that the seller does not charge a higher price on the platform than it does on another.====Recently, PMFC clauses have been the subject of several antitrust investigations.==== For the PMFC clauses used by Amazon, Amazon offered commitments not to enforce the MFC clauses for e-book publishers in Europe. The European Commission announced its decision to make the commitments offered by Amazon legally binding on 4 May 2017. Also in Japan, as a result of the review on the proposal by Amazon Japan G.K. not to enforce MFC clauses, the Japan Fair Trade Commission decided to close the antitrust investigation of this case on 1 June 2017.====There is an ongoing debate about how to assess the PMFC clause. There have been several arguments about its potential to harm competition. LEAR (2012) identified some possible ==== effects of such across-platforms parity agreements (PMFC clauses): foreclosing entry of other platforms; softening competition among platforms; facilitating collusion among platforms; and signaling information about platforms’ costs.==== Parallel investigations by competition authorities revealed anti-competitive effects stemming from PMFC clauses. By contrast, LEAR (2012) also described the possible ==== effects of the PMFC clauses to generate efficiencies that might protect investments by platform owners. Although PMFC clauses have received attention in the economics literature, analysis of their competitive effects remains limited in theoretical models. The aim of this paper is to build a model to elucidate effects of PMFC clauses on the incentives for platforms to invest in demand-enhancing investments.====This paper studies a bilateral duopoly model that incorporates both competition between two platforms and competition between two sellers. Sellers make their differentiated goods available on both platforms. Platforms adopt an agency model for their dealings with sellers, where retail pricing is delegated to sellers and the revenues earned from selling each good is allocated between the parties according to a fixed sharing rule. Instead of retail pricing, platforms invest in demand-enhancing investments that might involve spillover effects. Our central research question is whether the industry-wide adoption of PMFC clauses encourages competing platforms to invest more. To this end, we compare the case in which both platforms use PMFC clauses with the case in which neither platform uses a PMFC clause.====Our main result is that industry-wide PMFC clauses engender greater investment if platform competition (i.e. intrabrand competition) is fiercer than seller competition (i.e. interbrand competition). We emphasize that the mechanism underlying this result is not a simple story such that the clauses resolve an underinvestment problem stemming from externalities of investments (i.e., spillover effects) that increase the demand for goods sold on the rival platform. In the present paper, by contrast, whether industry-wide PMFC clauses increase investment incentives, or not, depends on the degree of competition in the vertically related market, not on the extent of spillover effects of investments.====The intuition underlying the main result can be explained as shown below. The presence of PMFC clauses has two effects. First, investments by a platform increase consumers’ willingness to pay, which induces sellers to charge a higher price for goods sold on that platform. Without PMFC clauses, however, sellers need not raise their prices for the rival platform as well. This outcome is expected to make the platforms reluctant to invest. By contrast, the PMFC clause can resolve this underinvestment problem by requiring that sellers raise their prices for the rival platform by the same amount. The fiercer platform competition becomes, the greater this positive effect becomes. Second, the presence of PMFC clauses mitigates price competition between sellers. Less competition shrinks the total quantities demanded, which discourages the platforms from additional investment. The fiercer the seller competition becomes, the stronger this negative effect becomes. Overall, when platform competition is fierce compared to seller competition, the positive effect dominates the negative one, implying that the industry-wide adoption of PMFC clauses can heighten the platforms’ investment incentives.====Additionally, we show that whenever the presence of industry-wide PMFC clauses increases the platform investment, it engenders higher prices but expands the resulting demand eventually, thereby yielding greater seller profits and consumer surplus. In other words, sellers and consumers share the same preference for the clauses. By contrast, the platforms would not necessarily do so. They can earn greater profits with PMFC clauses, which can nevertheless be harmful to sellers and consumers. Our results support an earlier theory that PMFC clauses can be harmful, but with important caveats.",Platform most-favored-customer clauses and investment incentives,https://www.sciencedirect.com/science/article/pii/S0167718720300394,11 April 2020,2020,Research Article,179.0
"Li Shengyu,Luo Rong","School of Economics & Centre for Applied Economic Research, University of New South Wales, Sydney, Australia,School of Economics, Renmin University of China, China","Received 24 October 2018, Revised 5 February 2020, Accepted 17 February 2020, Available online 28 March 2020, Version of Record 1 May 2020.",https://doi.org/10.1016/j.ijindorg.2020.102591,Cited by (0),Retailer differentiation exists in most ,"Retailer differentiation and non-exclusive dealing are very common business practices. Retailers differ in their geographic locations, loyalty programs, target customer groups, and so on. Such differentiation leads to a ==== of non-exclusive vertical contracts. That is, the demand for a manufacturer’s product under non-exclusive contracts is higher than that under exclusive contracts for given wholesale prices. This effect arises when differentiated retailers (e.g., AT&T and Verizon, differentiated in terms of services and coverage) convert a ==== product (e.g., iPhone X) of a manufacturer into ==== differentiated products. As a result, the manufacturer can sell to more customers with a non-exclusive contract than with an exclusive contract. This effect gives manufacturers an incentive to adopt non-exclusive vertical contracts.====Although the market penetration effect is intuitive, it is unclear when such a penetration effect exists and how it affects the vertical relationship between upstream and downstream firms. For example, the penetration effect does not exist if every consumer purchases a product regardless of the contract type or there is no outside option, as often is assumed in the literature. Even if an outside option exists, the strength of the penetration effect may depend on factors such as product quality. Overall, the literature does not focus on the market penetration effect when studying the exclusivity of vertical contracts (e.g., Bakó, 2016, Chang, 1992, Dobson and Waterson, 1996, Mauleon, Sempere-Monerris, Vannetelbosch, 2011, Moner-Colonques et al., 2004). This paper contributes to the literature by analyzing how the market penetration effect is determined and how it influences vertical contract exclusivity in an oligopolistic model.====We consider a model of two differentiated manufacturers and two differentiated retailers with a general demand function. Under the exclusive contract, each manufacturer sells its product exclusively to a retailer. Under the non-exclusive contract, each manufacturer sells to both retailers. To fix ideas, we first analyze scenarios in which the manufacturers both choose exclusive contracts or both choose non-exclusive contracts. The manufacturers have all the bargaining power. We compare the equilibrium outcomes of these scenarios and show that, when the market penetration effect is strong, non-exclusive contracts imply higher profits for the manufacturers and retailers.====We then endogenize the manufacturers’ choice of contract type with Nash bargaining between the manufacturers and the retailers to analyze contract exclusivity in equilibrium. We model this as a three-stage game. First, the manufacturers simultaneously choose whether to adopt exclusive or non-exclusive contracts. Their choices can be symmetric or asymmetric. The manufacturers take into account the pricing and demand outcomes in the subsequent stages. Second, given the contracts choices, the manufacturers and retailers engage in pairwise negotiations over the wholesale prices. Both parties have bargaining power. The negotiations are interdependent because the manufacturers’ and retailers’ disagreement values in one negotiation are their profits from other negotiations. Third, given the negotiated wholesale prices and contract choices, the retailers simultaneously choose retail prices while competing with each other. This model enables us to analyze the impact of the market penetration effect and bargaining power on the equilibrium contract choices.====A few effects of the non-exclusive contracts determine the level of market penetration. First, a manufacturer’s product that is sold by differentiated retailers is viewed as different varieties of the product. This variety effect relies on retailer differentiation, and it helps the manufacturer to reach more customers. This increases the market penetration effect. Second, intra-brand competition arises because the two retailers compete on the same manufacturer’s product under non-exclusive contracts. Given the wholesale prices, this effect drives down the retailers’ prices of the product and strengthens the market penetration effect. Third, each retailer can internalize the inter-brand competition between the two products under non-exclusive contracts. This effect increases the retail prices because consumers may switch within the same retailer when the product’s price rises. Hence, the internalization effect lowers the sales of the product and thus reduces the market penetration effect. In the pairwise negotiations, choosing a non-exclusive contract also reduces the demand for a manufacturer’s product through increasing the disagreement value of the retailers and thus decreasing the opponent’s product prices. We call this the disagreement value effect.====These effects of non-exclusive contracts influence the manufacturers’ profits not only through the market penetration effect, but also through the consumers’ demand elasticities to the wholesale prices. The variety effect and intra-brand competition lower the wholesale price elasticities, whereas the internalization effect increases them. Therefore, the comparison of the manufacturers’ profits under the two types of contracts depends on the relative strength of the three forces. When the variety effect and intra-brand competition dominate the internalization effect, the market penetration effect prevails and the wholesale price elasticities are lower with non-exclusive contracts.====The market penetration effect relies on the existence of an outside option with a positive market share, which is very common. If consumers do not have an outside option (as in the Hotelling model), the penetration effect does not exist, because every consumer already buys a product under the exclusive contracts and there is no room for the manufacturers to penetrate the market. Since product quality influences the market share of the outside option under exclusive contracts, it is an important factor that affects the market penetration effect. Specifically, the strength of the penetration effect decreases as product quality increases. With high-quality products, the outside option’s market share in the exclusive case is small, so the market penetration effect of non-exclusive contracts is weak. On the contrary, as product quality decreases, the outside option’s market share in the exclusive case increases, which implies a larger potential market for the manufacturers to capture using non-exclusive contracts. Therefore, if product quality is low, the manufacturers can earn more profits in the non-exclusive case than in the exclusive case.====We apply the model to an example with logit demand functions to illustrate the market penetration effect and analyze the equilibrium outcomes.==== We solve for the equilibrium of the three-stage game and find three main results. First, the market penetration effect exists for wide ranges of the price coefficient of demand, bargaining power, and asymmetric product quality and costs. Second, choosing a non-exclusive contract is a dominant strategy for the manufacturers. In particular, the manufacturers’ asymmetry in product quality and costs does not lead to asymmetric contract choices in equilibrium. Third, a prisoner’s dilemma occurs if manufacturers have high product quality or low costs. The manufacturers’ profits are lower under non-exclusive contracts, because the market penetration effect is small when the products already have high demand under exclusive contracts due to high quality or low costs.====Our paper is closely related to Dobson and Waterson, 1996, who also study vertical contract exclusivity with differentiated retailers. Our paper is different in the following three aspects. First, we explicitly study the market penetration effect. That is, by choosing a non-exclusive contract, a manufacturer’s product can reach more consumers when retailers are differentiated. We analyze what determines this effect and examine how it influences vertical contract exclusivity in an oligopolistic model. Second, we consider the negotiations between the manufacturers and retailers on setting the wholesale prices. Given contract choices, the negotiations are interdependent and both parties have bargaining power. In Dobson and Waterson, 1996, manufacturers have all the bargaining power under non-exclusive contracts, and they maximize their joint profits with retailers under exclusive contracts. Third, we consider not only asymmetric contract combinations, but also asymmetric manufacturers in terms of product quality and costs. We explore how the market penetration effect is related to product quality and costs and analyze how this asymmetry affects the equilibrium contracts.====This paper sheds light on how retailer differentiation and the outside option together can affect the comparison of exclusive and non-exclusive contracts. Although retailer differentiation and outside options exist in most industries, the theoretical literature on exclusive contracting has mostly focused on models with identical retailers (e.g., Rey, Stiglitz, 1988, Besanko, Perry, 1993, Rey, Stiglitz, 1995) or differentiated retailers without an outside option (e.g., Besanko, Perry, 1994, Gabrielsen, 1996, Gabrielsen, Sørgard, 1999, Allain, 2002, Kourandi, Vettas, 2010). Our model incorporates retailer differentiation and the outside option, and we emphasize that together they may generate a strong market penetration effect, which provides an incentive to the manufacturers to adopt non-exclusive contracts. Using the logit demand model, we show that choosing a non-exclusive contract is a dominant strategy for the manufacturers and a prisoner’s dilemma occurs when manufacturers’ products have high quality or low costs.====Our study also contributes to the literature that investigates manufacturers’ incentives to engage in exclusive dealing.==== These incentives include reducing intra-brand competition and imposing the foreclosure effect (e.g., Rey, Stiglitz, 1988, Rasmusen, Ramseyer, Wiley, 1991, Rey, Stiglitz, 1995, Segal, Whinston, 2000, Sass, 2005, Hortaçsu, Syverson, 2007, Asker, Bar-Isaac, 2014, Nurski, Verboven, 2016). Some papers study the impacts of the externalities of producers’ investment and retailers’ promotional efforts on vertical contracts (e.g., Besanko, Perry, 1993, Desiraju, 2004). This paper focuses on the market penetration effect as an incentive for manufacturers to adopt non-exclusive contracts.====The remainder of the paper is organized as follows. Section 2 sets up a general oligopolistic vertical model for exclusive and non-exclusive contracts. Section 3 compares the equilibrium manufacturers’ profits, retailers’ profits, and consumer surplus under exclusive and non-exclusive contracts. Then, in Section 4, we endogenize the manufacturers’ choice of exclusivity and allow the manufacturers and retailers to negotiate the wholesale prices via Nash bargaining. Section 5 presents a specific numerical example using a model with logit demand. We analyze the manufacturers’ endogenous choice of exclusivity under symmetric and asymmetric setups in the numerical example in Section 6. We conclude in Section 7.",Non-Exclusive Dealing with Retailer Differentiation and Market Penetration,https://www.sciencedirect.com/science/article/pii/S0167718720300138,28 March 2020,2020,Research Article,180.0
Hunold Matthias,"University of Siegen, Unteres Schloß 3, Siegen 57068, Germany","Received 11 February 2019, Revised 5 March 2020, Accepted 10 March 2020, Available online 20 March 2020, Version of Record 1 May 2020.",https://doi.org/10.1016/j.ijindorg.2020.102615,Cited by (9),"This article demonstrates that entry deterrence can occur when downstream incumbents hold non-controlling ownership shares of a supplier that does not price-discriminate. Such backward ownership implies a rebate on the input price for the incumbents and a competitive disadvantage for downstream entrants. An ==== can use non-controlling ownership to change the pricing of a supplier in a way that appears to be accommodating but in fact deters entry. The supplier benefits from an obligation or a commitment to supply the customers under equal terms, as this induces profitable sales of ownership stakes to incumbent downstream firms.","Partial ownership between vertically-related firms exists in various industries and competition authorities are increasingly wondering how to deal with this. For instance, the European Commission (EC) is considering whether and, if yes, how to extend its merger control to the acquisition of so-called minority shareholdings.==== The EC currently only has the jurisdiction to review an ownership acquisition if it yields the acquiring firm ==== over the target firm.==== A firm may thus acquire ownership stakes, even if that yields substantial or material influence over the target firm, without being subject to EC merger control as long as it does not reach the threshold of ==== in the sense of the European Union merger regulation.==== There is thus a potential enforcement gap with respect to partial ownership. There is also still a research gap, especially in relation to ownership links between vertically-related firms.====I contribute by demonstrating that non-controlling backward ownership stakes, which downstream firms hold of their supplier, can lead to entry deterrence at the downstream level through partial input foreclosure, thereby increasing industry profits and harming consumers. Interestingly, this occurs when the efficient upstream firm is committed or obliged to supply all downstream firms under equal terms and, in particular, not charge entrants higher prices. The intuition for the result is as follows: In a situation without ownership between upstream and downstream firms, an obligation or commitment of a supplier to charge uniform prices generally ensures a level playing field for the downstream firms, and thereby protects newcomers. Suppose now that each of the incumbent downstream firms owns a share of the supplier, but the entrant does not. The downstream incumbents receive part of the input price back through the profit participation. A uniform ==== wholesale price implies a lower ==== input price for the partially integrated downstream incumbents than for the non-integrated downstream entrant, due to the implicit rebate. If the supplier follows a uniform wholesale price policy, it consequently puts the non-integrated downstream firm at a cost disadvantage. This disadvantage reduces a non-integrated entrant’s profits and can thus deter entry.====It is important for entry deterrence that the upstream firm charges equal prices to the incumbents and the entrant in the case of entry. Instead, if the supplier could freely price discriminate, it would benefit from raising the prices for the incumbents until the rebate implied by the ownership stakes vanishes. Consequently, the effective downstream prices would be equal for the incumbents and the (potential) entrants and there would be no barrier to entry. I show that acquiring non-controlling partial ownership in their supplier is attractive for the incumbent downstream firms if the supplier charges uniform (that is, non-discriminatory) prices, as this makes entry less attractive. For the supplier, this makes a policy of charging downstream firms a uniform price profitable. In summary, the anti-competitive effects arise either when the upstream firm is required by regulation to not price discriminate or when it is able to commit to charging equal prices.====The combination of a uniform pricing policy of the supplier and non-controlling partial backward ownership by the incumbent downstream firms can be advantageous for these firms. As entry can intensify competition and may lead to business stealing, it can decrease the incumbent industry’s profits, so that the incumbents are collectively better off without entry. A supplier may individually not fully internalize these disadvantages of downstream entry, though. Instead, it tends to benefit from downstream entry through an expansion of demand for its inputs, while such entry typically hurts downstream incumbents. From the perspective of the established industry as a whole, an independent upstream firm therefore tends to accommodate too much entry. By deterring entry, non-controlling backward ownership of the incumbents in combination with a uniform price commitment of the upstream firm can thus be profitable for the industry. Such non-controlling backward ownership shares do not reduce double marginalization, and thus the downstream price level absent entry, because the resulting effective input prices of the downstream firms remain unchanged. This makes partial backward ownership a virtually costless device for entry deterrence, which differs from a vertical merger. Absent further assumptions, a vertical merger tends to reduce downstream prices to the benefit of consumers.====The pattern that customers partially own their suppliers is present in various industries, such as banks and payment providers (Greenlee and Raskovich, 2006), cable operators and broadcasters (Brito et al., 2016), as well as stock exchanges and clearing houses. The present article provides insights for competition authorities and regulators when incumbent downstream firms partially own their supplier. In these situations, a regulator might fear that the partially integrated downstream firms unduly influence the supplier’s sales strategy, such that independent downstream rivals receive worse offers. A non-discrimination policy might therefore appear as the obvious safeguard to prevent input foreclosure. This article shows that restricting an upstream firm to charging uniform prices might not be a cure, but may instead deter entry by inducing a cost disadvantage on downstream entrants. Indeed, there are legal provisions in jurisdictions such as the US and the EU that limit or forbid price discrimination among customers. These include general competition law, but also more specific regulations, for instance, for financial clearing houses.====A case that features several key elements of the theory is the acquisition of partial ownership by the London Stock Exchange Group (LSEG) of LCH.Clearnet Group (LCH). LCH is a clearing house that provides clearing services for financial products such as derivatives to trading venues, including LSEG, and their customers. The UK’s Office of Fair Trading (OFT) investigated this case in 2012. In view of the non-discrimination obligation and fiduciary duties of LCH, the OFT sketched the theory of harm in that LCH adopts a uniform price rise while LSEG reduces its trading fees for customers who trade at an LSEG venue and clear that trade through LCH. For these customers, the overall cost of trading and clearing with LSEG and LCH combined would be more attractive than remaining with a rival trading venue and LCH. However, the OFT eventually dismissed the foreclosure concerns. The theory presented in this article could be useful for the analysis of similar cases in the future. For instance, the authority’s foreclosure theory apparently relies on a price rise, which is not necessary for foreclosure to occur according to the theory presented in this paper.====The structure of the remaining text is as follows. After the literature review, Section 2 sets up the model. Section 3 studies the entry decision and product market outcome for the case of non-controlling partial ownership, downstream quantity competition, and uniform linear wholesale prices. Section 4 analyzes the profitability of such ownership acquisitions. Section 5 shows that a supplier benefits from committing to non-discriminatory pricing as this makes backward ownership acquisitions profitable. Section 6 demonstrates that entry deterrence with partial backward ownership can reduce welfare. Section 7 concludes with a discussion of implications for regulation and competition policy. The annex contains (I) a description of the above-mentioned competition policy case and relates it to the model, (II) extensions of the model with respect to downstream price competition and two-part tariffs, as well as (III) proofs of the lemmas and propositions.","Non-Discriminatory Pricing, Partial Backward Ownership, and Entry Deterrence",https://www.sciencedirect.com/science/article/pii/S0167718720300370,20 March 2020,2020,Research Article,181.0
"Deck Cary A.,Thomas Charles J.","Department of Economics, Finance and Legal Studies, The University of Alabama, 200 Alston Hall, Tuscaloosa, AL, 35487, USA,Econonomic Science Institute, Chapman Univeristy, One University Dr., Orange, CA 92866, USA,Department of Economics, Clemson University, 228 Sirrine Hall, Clemson, SC 29634, USA","Received 17 May 2019, Revised 26 February 2020, Accepted 3 March 2020, Available online 19 March 2020, Version of Record 17 April 2020.",https://doi.org/10.1016/j.ijindorg.2020.102612,Cited by (2),"We conduct unstructured negotiations in a laboratory experiment designed to empirically assess the predictive power of models of the multilateral negotiations observed in diverse strategic settings. For concreteness we consider two sellers negotiating with a buyer who wants to make only one trade, and we categorize the models by whether introducing a second seller to bilateral negotiations ====, or ==== increases the buyer's payoff. Our experiment features two scenarios within which the three categories of models have observationally distinct predictions: a ==== scenario with one high-surplus seller and one low-surplus seller, and a ==== scenario with identical high-surplus sellers. In both scenarios the buyer tends to trade with a high-surplus seller at terms indistinguishable from those in bilateral negotiations with a high-surplus seller, meaning that ====. Our findings match the predictions from models in the never-matters category, supporting their use when modeling multilateral negotiations.","Multilateral negotiations are an exchange mechanism frequently observed when one party wishes to trade with one of several others offering potentially different amounts of surplus to be split. For example, procurement often involves negotiations with suppliers who differ in their quality or goodness-of-fit, as illustrated by Express Scripts’ decision to include on its formulary AbbieVie’s hepatitis C drug Viekira Pak rather than Gilead’s blockbuster drug Sovaldi.==== The takeover contest for Pep Boys pitted Carl Icahn against Bridgestone, with both acquirers presumably having different benefits from the transaction.==== General Electric’s decision to relocate its corporate headquarters to Boston concluded a lengthy battle among 40 municipalities, reportedly including ones in Connecticut, Georgia, New York, and Texas.====Several theoretical models yielding distinct predictions are applicable to multilateral negotiations, and in this paper we provide evidence to discriminate among them based on how well they predict actual negotiated outcomes. Selecting an appropriate model matters for analyzing specific negotiation settings, such as the three mentioned in the prior paragraph. It also matters for analyzing other strategic problems to which the negotiation model merely is an input, such as investment, product design, mergers, hold-up, dual-sourcing, entry, collusion, or R&D. For example, using a different bargaining model might qualitatively change one’s conclusions regarding whether countervailing power in the supply chain reduces prices to final consumers, as in von Ungern-Sternberg (1996); merger decisions and technology choice, as in Inderst and Wey (2003); or the competitive impact of resale price maintenance, as in Rey and Verge (2010).====In Section 2 we describe three models that we frame as procurement settings in which two sellers negotiate with a buyer who wants to make only one trade, then we categorize them and related models based on how their predicted outcomes change when moving from bilateral to multilateral negotiations. In our framework these categories differ in whether introducing a weakly inferior seller to a bilateral setting ====, or ==== increases the buyer’s negotiated payoff, as a function of how close of a substitute is the new seller for the original one.====We assess the models’ empirical relevance by comparing their predicted outcomes to actual outcomes of unstructured negotiations conducted in the laboratory. Using an experiment allows us to control variables that might not be observable with naturally-occurring data: we induce players’ preferences, specify the possible trading partners, and perfectly observe negotiated outcomes.====We conduct unstructured negotiations rather than implement structured protocols from the models, because those models’ purpose is to make meaningful predictions about situations lacking such structure. The models impose structure on behavior not to reflect actual protocols by which negotiations are conducted in practice, but to enable derivation of equilibrium strategies that lead to predicted outcomes. Consequently, assessing those theoretical predictions’ practical reliability requires empirical evidence about unstructured negotiations.====Our approach follows insights from Friedman (1953), who recognizes that the measure of a model’s value is how well it predicts rather than how closely its assumptions match reality. Our approach also follows that taken in the many early experimental studies of bargaining surveyed by Roth (1995), and in more recent research such as Leider and Lovejoy (2016).====The experimental design we present in Section 3 describes two scenarios we use for which the three categories of models have observationally distinct combinations of predictions: a ==== scenario with one high-surplus seller and one low-surplus seller, and a ==== scenario with two identical high-surplus sellers. In the always-matters category of models, the second seller affects the outcome no matter how poor of a substitute it is for the first seller. In the never-matters category, the second seller is irrelevant to the outcome no matter how close of a substitute it is for the first seller. In the sometimes-matters category, the second seller affects the outcome if and only if it is a sufficiently close substitute for the first seller.====In Section 4 we show that in both scenarios the buyer tends to trade with a high-surplus seller at terms indistinguishable from those in a ==== scenario with the buyer and a single high-surplus seller negotiating bilaterally, meaning that ====. Our findings match the predictions from the never-matters models, supporting their use when modeling multilateral negotiations.====While our main goal is informing model selection in applications, our analysis also contributes to understanding how rivalry affects the intensity of competition. This issue has long been of interest in industrial organization, as shown by the industry studies in Weiss (1989) and the historical empirical and theoretical references in Schmalensee (1989) and Shapiro (1989). In Section 5 we relate our findings to those from theoretical and empirical research on procurement that finds wrinkles in the conventional wisdom that increasing the number of sellers benefits buyers by increasing the intensity of competition.",Using experiments to compare the predictive power of models of multilateral negotiations,https://www.sciencedirect.com/science/article/pii/S0167718720300345,19 March 2020,2020,Research Article,182.0
Foucart Renaud,"Lancaster University Management School, United Kingdom","Received 16 April 2019, Revised 24 February 2020, Accepted 10 March 2020, Available online 19 March 2020, Version of Record 13 April 2020.",https://doi.org/10.1016/j.ijindorg.2020.102614,Cited by (1),"Competing intermediaries search on behalf of consumers among a large number of horizontally differentiated sellers. Consumers either pick the best deal offered by an intermediary, or compare the intermediaries. A higher number of intermediaries has the direct effect of decreasing their search effort. Hence, if an exogenous share of consumers do not compare, more competition hurts them. More competition however also increases the incentives for consumers to compare. A higher share of informed consumers in turn increases the search effort of intermediaries. If consumers are ex-ante identical and rationally choose whether to become informed, the total effect of a higher number of intermediaries is to make each of them (weakly) choosier. Moreover, it always decreases the price offered by sellers. Allowing intermediaries to bias their advice by making sponsored links prominent has a similar effect of making all consumers better off in expectation.","Consumers often rely on intermediaries to help them find the product that best suits their needs. In the case of online intermediaries, it is easy - yet costly - for consumers to compare the different recommendations received and pick the best offer. A natural question in this market is whether consumers benefit from having a large number of intermediaries at their disposal. More precisely, could limiting entry or, to the contrary, mergers of intermediaries increase consumer welfare and market efficiency?====In this paper, I show that higher market concentration helps to protect uninformed buyers if consumer information is a “behavioural” trait: an exogenous share of buyers do not understand the market, regardless of how much they could benefit from becoming informed. If consumer information is a rational choice of ex-ante identical buyers however, higher market concentration lowers consumer welfare. The reason is that it decreases the incentive a buyer has to compare recommendations - the “metasearch” among intermediaries searching on one’s behalf-, and the positive externalities it generates on the others by doing so. This in turn has three consequences. First, the quality of advice provided by intermediaries becomes (weakly) lower. Second, if intermediaries are less choosy, demand in the product market is less elastic and prices increase. Third, for a given choosiness of intermediaries, a lower share of informed consumers makes demand less elastic in the product market, and increases prices.====In terms of aggregate market efficiency, there is therefore a trade-off between better advice for consumers and the additional cost of intermediaries investing in advice and receiving smaller market shares. Finally, I show that introducing the possibility for deal finders to bias their results and offer prominent sponsored recommendations alongside their truthful organic ones actually helps ex-ante identical consumers. The reason is - again - that the existence of “fake” advice makes it more valuable for consumers to compare their options.====A deal finder can be an individual recruitment agency hired to search for job candidates, a real estate agency searching for prospective tenants (for the owner) or properties (for the tenants), an insurance broker, or one of the many “deal finding websites” on the Internet. The question of whether free entry should be granted in these markets is a long-lasting debate. Historically, intermediaries have been subject to limitations and certifications to protect consumers from dishonest advisors. For instance in the UK, from 1977 to 2005, the Insurance Broker Registration Act limited entry to the market to make sure no deal finder was acting as a representative of a single insurance company.==== This point is however much less obvious on the Internet, where consumers are only a few clicks away from comparing their options. Moreover, digital markets seem to often converge towards very concentrated structures making the question of excessive entry less relevant.==== For instance, in 2015 in the US, Expedia==== (Expedia.com, tripadvisor.com, orbitz.com, hotels.com, venere.com, trivago.com,...) and Priceline==== (priceline.com, kayak.com, booking.com,...) controlled 95 percent of the online travel-marketplace after a number of successful fusions and acquisitions.====Innovation in search quality is an essential part of the competition in advice markets. To keep the travel example, Andrew Warner of Expedia reports in a 2014 interview==== that “for a standard trip from LA to New York, Expedia has 65,000,000,000 different combinations of travel for each consumer - given variations in flight times, airlines, car rentals, hotels, offers.” Being able to use consumer data to provide the best personalized advice (and beat competitors) is thus a huge and costly challenge, with Expedia claiming to spend £500 million yearly in R&D. Warner describes the objective of such investment as being able to do more than mechanically answering a query and providing the cheapest price. Today’s competition in the online travel industry is thus largely based on being able to provide a good individual match to a specific consumer.====I set up a model in which a large number of consumers want to buy one unit of a product in a market with a large number of horizontally differentiated sellers. I assume that competing deal finders search (at a cost) for the best product to recommend to a specific consumer. I use two standard search models. In the main part of the paper I study a linear random sequential search within a distribution of deals, in the tradition of Wolinsky (1986) and Anderson and Renault (1999). In Appendix B I use a model of non-sequential search and show that my results are robust to this alternative setup.====I start by solving a benchmark model in which consumers are of two exogenous types. Some are “savvy” and pick the best deal among all the deal finders. Some are “non-savvy” and take the best deal offered by a deal finder chosen at random. I borrow this dichotomy from a literature started by Varian (1980) to study price dispersion. I also start by making the assumption that the revenue of deal finders depends linearly on the volume of sales. This is the case for instance if they are financed by selling information about buyers in a competitive market for advertising, or if they collect fixed commissions.==== I find the “direct” effect that lower market concentration decreases the quality of advice. The intuition behind that result is that competition among deal finders resembles an all-pay-auction (see for instance Baye et al., 1996): each sale benefits one deal finder only, but all bear the cost of providing the search technology. Hence, the higher the number of competitors, the smaller the marginal return from providing a better service.====Then, I solve the model for endogenous consumer information. I first derive a classic result from this literature: the existence of search externalities (Armstrong, 2015). The savvy consumers protect the non-savvy, as deal finders cannot discriminate among types, so that fiercer competition for the savvy types make all consumers better off. Thus, in any equilibrium, not enough consumers choose to be informed. I find that lower market concentration has the indirect effect that savviness matters more, hence increasing the incentives to become informed. This indirect effect outweighs the direct one: as more consumers are savvy, demand in the product market is more elastic, and prices decrease. Hence, the main result of the paper: lower concentration in the market for intermediaries actually benefits all consumers when consumer information is endogenous.====I compare this “Varian” setting to one in which consumers bear a linear cost of non-sequentially observing an additional deal finder, in the spirit of Burdett and Judd (1983). I find that the main result of the paper holds, albeit the only channel through which more competition benefits consumers is price competition.====I then compare the welfare gain for consumers to aggregate market efficiency. I identify the following trade-off: while more competition in the market for deal finders always makes consumers better off, it also increase the aggregate costs for deal finders. Hence, while the number of deal finders maximizing aggregate welfare is often not equal to 2, it is not an infinite number either.====Finally, I solve two modified versions of the model. In the first one, deal finders auction prominent sponsored links to sellers, displayed alongside their truthful recommendation. As those “fake” advices decrease the expected payoff of uninformed consumers, they increase the incentives to become informed. Hence, if the information choice of consumers is endogenous, the existence of sponsored links benefits all consumers in expectation. In the second, I allow for heterogeneous costs for consumers to be informed and show that this intermediary case between exogenous and endogenous consumer information yields more balanced results.====To the best of my knowledge, this paper is the first to study how concentration in the market for intermediaries affects the incentives to invest in the quality of the advice they offer.====This paper relates to the literature on advice and delegated search. In the literature on delegated search, it relates to Lewis (2012) and Ulbricht (2016). The novelty of my approach is to add competition on the side of the deal finders, and to study different types of buyers.====I consider a world in which consumers face a consideration set (Eliaz and Spiegler, 2011a), but this set is not directly determined by competition among sellers. Buyers instead rely on intermediaries to make them a recommendation. In the advice market, most of the focus has been on a single intermediary. For instance, Armstrong and Zhou (2011) and Chen and Zhang (2017) study a large number of possible transactions between sellers of a product and the adviser choosing how to present the information to consumers.====The question of competition among advisers has been discussed in an extension of Inderst and Ottaviani (2012), who study financial advice and compare the case of competitive advisors to the one of a monopolist. Competition among two search engines is also studied in Section 5 of de Cornière (2016), in a two-sided framework were search engines compete in order to attract both consumers and advertisers by auctioning “keywords.” In related models, Eliaz and Spiegler (2011b) study competition among two search engines, taking the quality of search ability as given and Taylor (2013) studies the trade-off faced by two search engines offering both organic and sponsored links and choosing how precise their organic advice should be.====In the case of online platforms, Karle et al. (2017) study competition among platforms charging fees for sellers to compete. Sellers want to be active on a popular platform to be matched with more buyers, but also want to avoid competing with too many similar sellers. There is however no active role in providing search quality for the platforms. Edelman and Wright (2015) study intermediaries competing by investing in a technology increasing the utility consumers get from a given product. Those benefits are however not linked to the quality of search, and therefore do not affect consumer information.",Metasearch and market concentration,https://www.sciencedirect.com/science/article/pii/S0167718720300369,19 March 2020,2020,Research Article,183.0
"Chen Zhiqi,Ross Thomas W.","School of Economics, Nanjing University, Nanjing, China,Department of Economics, Carleton University, Ottawa, Canada,Sauder School of Business, University of British Columbia, Vancouver, Canada","Received 28 February 2020, Accepted 9 March 2020, Available online 18 March 2020, Version of Record 27 November 2020.",https://doi.org/10.1016/j.ijindorg.2020.102613,Cited by (1),"While strategic alliances and joint ventures have become important organizational forms promising a variety of efficiency benefits for the economy, a body of research has been building showing that alliances between competitors can have significant anticompetitive consequences. This paper explores a particular kind of arrangement, here called a “buffer joint venture”, in which parent firms create an entity selling products located between their own locations in product or geographic space. Depending upon the governance structure of the joint venture and the timing of price-setting by the joint venture and its parents, the buffer joint venture may reduce competition between the parents leading to higher prices and profits and lower social welfare. By altering the per-period profits from collusion and deviation payoffs, a buffer joint venture can also affect the stability of collusion between parents in a repeated game context.","As has been recognized in a long literature, strategic alliances and joint ventures have become increasingly common in a wide variety of industries, perhaps most famously among airlines, pharmaceutical companies and automobile manufacturers. While definitions vary, in our use of the terms, strategic alliances involve the ongoing cooperation of firms to provide new or improved products or services (on the output side), or to improve production/distribution methods (on the input side). Airline alliances, for example, can include the sharing of aircraft and interlining agreements to transfer passengers efficiently. Joint ventures can be interpreted as the efforts of two or more independent firms to create a new, shared entity, to provide a new product or service. As such they represent a particular kind of strategic alliance. Standing somewhere intermediate in the spectrum between standard market-mediated relations between firms and complete integration, such arrangements promise a number of benefits to participants including the sharing of fixed costs, the combining of complementary talents and a way to facilitate the sharing of intellectual property.====An extensive body of scholarly work on strategic alliances has developed in the management and economics literatures. Management scholars have, for example, measured the increased frequency of these types of arrangements in a variety of industries, undertaken detailed case studies and created guidance on the factors that contribute generally to successful collaborations.==== The economics literature has provided a number of models to explain why firms might want to cooperate these ways, and explored the potential effects on rivals and consumers.====As has been shown in a number of papers, when the firms cooperating in an alliance are otherwise competitors in some markets, these kinds of arrangements can pose risks to competition. For this reason, a number of competition authorities have been paying increasing attention to alliances, for example by subjecting them to a kind of review similar to that designed for mergers.==== Some competition authorities have gone so far as to issue guidelines on competitor collaborations.====This paper explores the potential for anticompetitive effects from a type of alliance not previously studied for this purpose. This type of alliance involves the creation by parent firms of a joint venture that lies between them in product or geographic space. Through their joint control of this new entity, the parents place a buffer between themselves, softening competition and potentially raising prices. While the new product can undoubtedly add value for some consumers, the overall effect on consumer surplus and total welfare can be negative.====The best examples of these kinds of alliances may be found in the automotive sector where firms might jointly produce products that combine key characteristics of the parents.==== Also relevant would be alliances in the pharmaceutical industry in which, for example, brand name pharmaceutical companies partner with generic producers to manufacture branded generic drugs.==== Certain commercialization agreements might also fit the model well.====We study two ways through which this “buffer JV” can affect competition. First, we examine the effects on parents’ prices and welfare of the creation of the JV when the parents’ prices are still set non-cooperatively. Second, we explore the effect of introducing a JV on incentives to collude as well as on the effects and stability of that collusion.====We show that the buffer JV can indeed raise the prices and profits of parents even if the parent brand prices are set non-cooperatively. The magnitudes of the effects depend on the governance structure of the JV and whether or not the JV is assigned the role of price leader in the market. We also find that adding a buffer JV can make collusion more profitable (even taking into account the new fixed costs that must be expended), and can also render collusion more stable.====We view this work as supportive of the concerns shared by many competition authorities regarding the possible harmful effects on competition that can arise when competitors cooperate even on what might not be viewed as competitive variables. We demonstrate a new set of conditions under which such harm is possible. It is important to recognize that the negative effects we describe below arise even though the JV is introducing a new product to the market: a common defense of JVs has been that they must be improving welfare since without them the new product would not exist. In our model, the benefits of the new product can be less than the costs due to reduced competition by parent firms.====Section II reviews some of the key literature related to the effects of joint ventures on competition and describes the place of this paper in that literature. Section III then lays out the basic model of competition on which we build, and describes the pre-JV benchmark against which our joint venture outcomes will be compared. It does this, first identifying the basic forces at work in a general model, and then introducing the more specific model that allows us to advance the analysis. The following sections examine equilibria with joint ventures in cases where all price-setting is simultaneous (Section IV) and where the JV is made a price-leader (Section V). Section VI explores questions related to the profitability and effects of collusion with and without a JV operating between its parents. Section VII then describes some implications of these results for competition policy and offers our conclusions.",Buffer joint ventures,https://www.sciencedirect.com/science/article/pii/S0167718720300357,18 March 2020,2020,Research Article,184.0
"Chowdhury Subhasish M.,Crede Carsten J.","Department of Economics, University of Bath, 3 East, Bath BA2 7AY, UK,Bundeskartellamt, Kaiser-Friedrich-Straße 16, Bonn 53113, GER","Received 1 April 2019, Revised 9 December 2019, Accepted 9 February 2020, Available online 17 March 2020, Version of Record 4 April 2020.",https://doi.org/10.1016/j.ijindorg.2020.102590,Cited by (6),"We experimentally investigate the determinants of post-cartel tacit collusion (PCTC), the effects of PCTC on market outcomes, and potential policy measures aimed at its prevention. PCTC occurs robustly with or without fines or leniency and is determined both by collusive price hysteresis and learning about cartel partners’ characteristics and strategies. As a result, it is also strongly related to the preceding cartel success. PCTC generates a downward bias in the estimated cartel overcharges. This threatens the effectiveness of deterrence induced by private damage litigation and fines imposed on colluding firms based on the overcharge. This bias further increases with preceding cartel stability such that especially more stable sets of colluding firms may be deterred less when PCTC is present. Rematching colluding subjects with strangers within a session prevents PCTC. This indicates that barring colluding managers from their posts could help impede PCTC in the field.","Post-cartel tacit collusion (PCTC) occurs when firms tacitly collude after an explicit cartel, in which they were involved in before, breaks down. Such PCTC intensifies the negative welfare effects of collusion and, at the same time, undermines the effectiveness of policies aimed at the deterrence of cartels. In the presence of PCTC prices do not immediately return to the level of competition even after the cartel is detected. As a result, firms continue to earn supernormal profits and the harm induced by the cartel on welfare extends to post-cartel periods. Moreover, fines that are strongly related to the cartel price gains (called overcharges) in the cartel periods cannot fully deter collusion. These cartel overcharges are predominantly used by antitrust authorities to impose fines and are important in private damage litigation to calculate damages awarded to the cartel customers. Hence, given their large size and growing importance at an international level, these damages provide an important factor in deterrence. PCTC results in underestimated cartel overcharges if the supernormal markup created by PCTC is not accounted for. This leads to lower damages that are insufficient to deter collusion and to fully compensate customers. This downward bias in overcharge estimates is in particular a problem in some of the price-based approaches commonly used in court cases, in which post-cartel periods are used as competitive counterfactuals to establish the cartel overcharge (see, e.g. Davis, Garcés, 2009, Harrington, 2004).==== Despite these important consequences of PCTC, little is known under which circumstances PCTC might occur, to what extent the overcharge estimates may be biased due to PCTC, and how antitrust law can be designed to prevent PCTC. Thus, a better understanding of the determinants of PCTC and of potential tools aimed at its prevention is vital. This study aims to add to this knowledge.====PCTC has been observed (or at least suspected) in various industries with results being based on different methodologies. Harrington (2004b) provides a theoretical model, Fonseca and Normann (2012) experimental results, and Connor, 1998, Connor, 2001, de Roos (2006), Ordóñez-de Haro and Torres (2014), Kovacic et al. (2007), and Crede (2019) empirical observations that point towards the emergence of tacit collusion after the end of cartels.==== Connor (1998) notes that prices in the citric acid industry did not decline significantly even 18 months after the breakdown of the cartel. However, it is not certain whether this observation was triggered by rising input prices or by tacit collusion. Similar suspicions are raised in Connor (2001) and de Roos (2006) for the lysine cartel. de Roos (2006) provides two potential explanations for the lack of post-cartel price reductions in the lysine industry, in which prices actually rose after the detection of the cartel. First, it could have been possible that the conspirators learnt enough about each others’ behaviour through several years of explicit communication and cooperation that enabled them to collude tacitly. Knowing that communication to dissolve disputes was no longer possible after such a breakdown, the firms were particularly careful to prevent a price war. Second, it is also possible that the firms simply continued to set collusive prices to reduce fines to be paid under the U.S. antitrust sentencing guidelines (that refers to post-cartel prices to determine the cartel overcharge). Harrington (2004b) shows that firms have the strategic interest to keep the prices high after cartel detection during litigation. As such, overcharge estimates based on post-cartel prices tend to underestimate the true harm caused by the cartel.==== This yields lower estimates of damages and minimises the “harm” of private damage litigation on former cartelists. Erutku (2012) provides empirical evidence in support of this idea. Ordóñez-de Haro and Torres (2014) examine the breakup of several Spanish food cartels that relied on the signals of trade associations. Significant levels of price hysteresis (i.e. prices remained high and were subject to a reduced variance) can be observed in most of the cartels after antitrust intervention. This evidence suggests that the firms may have continued to post prices based on past signals from their trade associations. Fonseca and Normann (2012) provide experimental evidence for the existence of tacit collusion after periods of explicit communication that suggests that the chance of PCTC to arise in industries as well as its magnitude are negatively correlated with the number of firms in the market. Similar findings are reported by Kovacic et al. (2007), who empirically study multiple markets that were engaged in the Vitamins cartels.====Although these studies hypothesise the possible sources of PCTC, these hypotheses have never been formally tested. This lack of empirical evidence prevents tackling inappropriate overcharge estimates and the development of policies aimed at deterring PCTC. Therefore, the aim of this study is to focus on the possibility of tacit collusion to arise after periods of explicit communication,==== and to shed light on the following research questions: (1) is PCTC an abnormal phenomenon for a specific competition regime, or is its occurrence robust over various competition regimes such as the existence of antitrust fines, leniency programmes, etc.? (2) What are the determinants of PCTC? (3) What effects does PCTC have on attempts to estimate cartel overcharges? (4) Can any policy measures be implemented to deter PCTC?====To our knowledge, this is the first study to systematically investigate the driving factors and the related consequences of PCTC as well as possible preventive measures aimed against it. For this, we carry out a laboratory experiment that allows for an analysis of the marginal contribution of different market characteristics to tacit collusion in a controlled environment.==== Lack of sufficient data prevents to carry out a similar exercise in the field. Our results show that PCTC is a robust phenomenon across competition regimes. Learning about other players’ collusive types through successful cartel formation as well as collusive price hysteresis are found to be the main determinants of PCTC. Furthermore, the downward bias in cartel damage estimates induced by PCTC increases with the preceding cartel success. Rematching subjects in the experiment is found to be a promising measure to prevent or reduce PCTC.","Post-cartel tacit collusion: Determinants, consequences, and prevention",https://www.sciencedirect.com/science/article/pii/S0167718720300126,17 March 2020,2020,Research Article,185.0
Birulin Oleksii,"School of Economics, Sydney University, Social Sciences A02, Sydney, NSW 2006, Australia","Received 9 September 2019, Revised 2 March 2020, Accepted 2 March 2020, Available online 16 March 2020, Version of Record 6 April 2020.",https://doi.org/10.1016/j.ijindorg.2020.102610,Cited by (3),"We consider procurement auctions for the projects where the cost of production is subject to ex-post shocks—cost overruns. The contractor may default due to these overruns, which affects the buyer’s expected cost. Here the lowest-bid auction emerges as the procurement mechanism that: (i) minimizes the expected transfers to the contractors, and (ii) requires the lowest surety bond to achieve a given probability of default. Since surety bonds are costly to post, the above makes a combination of the lowest-bid auction with the surety bond the optimal, i.e., the expected cost minimizing procurement mechanism in a wide range of parameters.","Procurement of goods and services is a major component of public and private spending. In 2013 the government procurement alone represented 12.1% of GDP in OECD countries.==== Given such volumes of expenditures it is extremely important to design procurement methods to ensure the optimal, i.e., cost effective use of funds. The Agreement on Government Procurement (GPA), signed by most industrialized countries in 1996, requires “... transparent, nondiscriminatory, and competitive tendering for public procurement...” Tender rules are not suggested by the GPA, however, the procurers’ choices often converge. If the design of the procured object can be settled before the tender, the sealed lowest-bid auction (or its per unit version) is the most prevalent method of selecting the contractor.==== Despite their widespread use as a method of sales, sealed bid second-price or open ascending auctions are virtually never used for procurement, see Carpineti et al. (2006).====Clearly, cost conscious procurers benefit from strong price competition that auctions induce. Nevertheless, the pervasiveness of a particular auction format in procurement practice is puzzling. Indeed, when the potential contractors privately know their production costs, running procurement tender as an auction would be optimal for the buyer. However, with risk-neutral contractors all standard auction formats would deliver the same expected cost. Risk aversion on the contractors’ side may make the lowest-bid auction more appealing to the procurer then, say, the second lowest-bid auction. However, unlike in this paper, with risk-averse contractors the lowest-bid auction is ==== the optimal mechanism. Moreover, limited liability restricts the contractors’ potential losses from below, making their payoffs convex, not concave.====A clue to the puzzle, we believe, lies in the uncertainty with respect to the production costs that the contractors face themselves. Many of the objects being procured do not exist at the time of the procurement tenders. The full costs of such objects are rarely known in advance even to the contractors and during production may well exceed the estimates available at the time of the tender.==== When cost overruns are significant the contractors may default and leave behind an unfinished project. Such default substantially affects the procurer, she may have to experience delays, search for another contractor, face litigation, etc. This equivalently applies to the conflicts (triggered by cost overruns) that lead to halts in the project execution. For expositional simplicity we focus on the extreme form of such conflicts – defaults.====Asymmetric information and uncertainty, cost overruns and defaults are all ubiquitous features of the procurement process. The analysis of the optimal mechanism in such setting is challenging. The expected cost of the procurer includes the transfers to be made to the contractor(s) selected at the tender and additional payments that rectify the consequences of possible defaults. The structure of these additional payments, indeed, as we see later, can lead to revenue superiority of particular procurement mechanisms. The existing theoretical studies, however, suggest that the mechanism that minimizes the procurer’s expected cost is quite complex and cannot be implemented by an auction, see e.g. Burguet et al. (2012) and Chillemi and Mezzetti (2014).====Even more puzzling is then the fact that real world procurement mechanisms are fairly simple: once the project design is determined, and there is a pool of potential contractors, the winner is by and large selected at the lowest-bid auction. After the allocation surety bonds are extensively used to “insulate” buyers from contractors’ defaults. This paper reconciles these real world practices with theory. In our setting the lowest-bid auction together with a surety bond—the compensation that the contractor pays to the buyer in case of default—emerges as the optimal procurement mechanism.==== This result is due to a novel combination of the model ingredients that arguably describe the procurement process reasonably well.====Crucially, the cost of the project is subject to the ex-post shock (cost overrun) that is realized ==== the contractor makes a significant irrecoverable initial investment.==== Such assumption is particularly plausible in procurement context, where a serious amount of works always takes place before the cost overrun becomes apparent.==== We view the project execution as if it occurs in two distinct stages. The cost of the first stage is all that a contractor (privately) knows at the time of the project allocation. The cost of the second stage—the value of the cost overrun—is unknown at that time. It will be discovered by the contractor working on the project after the cost of the first stage is sunk.====A cost overrun may lead to the contractor’s default, and subsequently, to complete the project, the buyer has to cover the cost overrun herself. The buyer’s expected cost then consists of the expected transfers to the contractors, of the further payments proportional to the expected probability of default and of the direct costs of deterrence against defaults. As we show, a combination of simple instruments: the lowest-bid auction together with the surety bond minimizes the buyer’s expected cost, i.e. such combination ==== procurement mechanism in our setting.==== We further provide the intuition on why this was not the case in the prior literature.====In all of the earlier models of both sales and procurement where default is a concern the types used at the allocation also directly affect the default decision.==== This leads to the so called ====. In a standard auction IPV setting the bidder’s expected transfer depends only on his expected allocation. With insolvency effect the contractor’s expected transfer also depends on his probability of solvency. This is a major obstacle for the analysis of the optimal mechanism since any measure (say, a surety bond) that decreases the probability of default also increases the contractor’s expected transfer.====In our model the cost overrun is realized after the type used at the allocation is sunk. The contractor’s decision to default depends on this type, but importantly, only ====, through the equilibrium award. As we show, the insolvency effect is then absent, and the expected transfer to the contractor is determined by his expected allocation, see Proposition 1. The expected cost equivalence for the seller, however, does not follow. The expected probability of default and hence the buyer’s expected cost depend both on the allocation and payment rules and on the surety bond level. Since the insolvency effect is absent, however, the expected transfers and the expected probability of default can be minimized by separate instruments that do not conflict with each other.====We show that for a given allocation and given surety bond, the buyer’s expected cost is minimized by the “pay-what-you-say” mechanism where the contractor’s award only depends on his own private information, as in the lowest-bid auction, see Proposition 2. Conversely, for a given allocation the pay-what-you-say mechanism requires the lowest surety bond to achieve a given expected probability of default. The cost of posting the bond increases with the bond size, and it may or may not be optimal to eliminate defaults altogether. If defaults are eliminated at the optimum, the expected transfers reflect the (virtual) costs of production. These will be minimized by allocating the project efficiently, to the lowest cost contractor, as in Myerson (1981). The lowest-bid auction is an efficient pay-what-you-say mechanism, hence it requires the lowest surety bond to eliminate defaults. Therefore, in combination with the appropriate surety bond it is the optimal procurement mechanism. If some defaults are allowed in equilibrium, this conclusion stays true whenever the “virtual costs” are increasing. These virtual costs are endogenous and depend on the mechanism itself. Despite this, we show that in a wide range of parameters the combination of the lowest-bid auction and surety bond remains the optimal procurement mechanism.====With only notational changes our analysis applies to the sales of experience durable goods of uncertain value, say apartments with a view. The buyer learns his full value only after using the good for an “introductory” period. After learning the full value she can either keep the good and pay the price set at the allocation stage, equivalent to completing the project in the procurement setting, or return the good to the seller for a pre-specified fee, i.e., default. The seller then resells the good with some discount. The buyer’s type has two components, the first is the consumption value she derives during the introductory period, the second is the further value that can be observed only after using the good for the introductory period. The seller prefers to avoid the return of the good, just as the buyer is averse to defaults in the procurement context.====The rest of the paper is organized as follows. Section 2 reviews the literature, Section 3 sets up the model, Section 4 deals with the optimal mechanism, Section 5 provides concluding remarks.",Optimality of simple procurement auctions,https://www.sciencedirect.com/science/article/pii/S0167718720300321,16 March 2020,2020,Research Article,186.0
Lee Chung-Ying,"Department of Economics, National Taiwan University, Taiwan","Received 31 March 2018, Revised 5 March 2020, Accepted 6 March 2020, Available online 14 March 2020, Version of Record 8 April 2020.",https://doi.org/10.1016/j.ijindorg.2020.102611,Cited by (6),"Branded drug manufacturers issue copay coupons to compete with generics as their brands are coming off ====. To explore the impact of copay coupons on pricing and welfare, I estimate a model of demand and supply using data on sales, advertising, and copayment for cholesterol-lowering drugs and perform a counterfactual analysis to simulate equilibrium pricing with copay coupons used for price discrimination and moral hazard. Copay coupons issued for price discrimination make the drug with coupons affordable for more consumers and increase consumer welfare even when a small fraction of consumers receive a coupon. Coupons used for moral hazard significantly mitigate price competition and improve consumer welfare only when coupon penetration is sufficiently high.","Coupons have long been prevalent in consumer goods. In 2005, coupons also started to play an important role in the pharmaceutical industry. The coupons distributed by drug manufacturers, called copay coupons or copay cards, reduce consumers’ out-of-pocket costs of prescription drugs. Many top-selling drugs, including cholesterol fighter Lipitor, blood thinner Plavix, and blood pressure drug Diovan, started to offer copay coupons as they were coming off patent. As of 2016, 20% of branded prescriptions in commercial insurance plans were associated with copay coupons.==== Copay coupons have been highly controversial as they become more and more popular. UnitedHealthcare, one of the major pharmacy benefit managers in the US, claims that copay coupons encourage “more use of expensive brand medications while taking the focus away from clinically equivalent and much lower cost generics.”==== On the other hand, PhRMA, a drug industry trade group, argues that “without these copay coupons, many patients would not be able to afford their medicines and would leave the pharmacy empty-handed.”====Despite the fast-growing use of copay coupons and simmering debate over the coupon programs, little empirical work has been done to examine what makes drug manufacturers issue coupons, how pricing changes under different incentives to issue coupons, and how social welfare is affected by copay coupons. In this paper, I provide a counterfactual analysis of drug pricing and welfare with the introduction of copay coupons, using a model estimated with data from the market for cholesterol-lowering drugs.====I consider two incentives for drug manufacturers to issue coupons: price discrimination and moral hazard. Coupons are widely used by firms in the consumer products industry to compete for price-sensitive consumers. In a market with consumer heterogeneity in price sensitivity, firms can rely on consumer self-selection (Narasimhan (1984); Sweeney (1984); Levedahl (1984); Varian (1989)) or targeting (Shaffer and Zhang (1995)) to price discriminate consumers using coupons. These strategies can apply to the pharmaceutical industry in the U.S. where drug manufacturers compete in price for patients who share prescription drug costs.====In addition, the special market structure in the pharmaceutical industry creates a moral hazard incentive to issue coupons. For most prescription drugs, doctors and patients make purchase decisions, and insurance companies pay for most of the drug costs.==== To reduce spending, insurance companies in the U.S. usually ask for a lower cost share when patients choose less expensive drugs. By issuing copay coupons directly to patients, drug manufacturers can circumvent this insurance benefit design and lower patients’ out-of-pocket cost to induce them to choose the drugs with coupons, which can greatly increase spending by insurance plans.====The goal of my counterfactual analysis is to quantify the total effect of copay coupons on the market through strategic interactions. The direction of effects stemming from the moral hazard incentive is clear. Coupon issuers would try to raise prices by as much as they can to take advantage of insurance companies, leading to a higher copay for consumers without coupons and decreasing overall consumer welfare when the coupon penetration is low. On the other hand, the price discrimination motive has a more ambiguous effect since pricing for coupon users and nonusers depends on the distribution of consumer price sensitivity. In either case, the size of the effects hinges on the substitution patterns in the markets.====To lay the foundations for the counterfactual analysis, I estimate a model with rich substitution patterns and consumer heterogeneity in price sensitivity, using unique data on sales, advertising, and copayments. The model captures competition among drugs along different dimensions, including class, molecule, form, and version. The model also allows price sensitivities to be drawn from a binary distribution, which helps to explain why branded drug prices usually stay high after patent expiration. The estimates suggest that drugs with more shared characteristics are closer substitutes and that 13% of the consumers are much less price sensitive than the rest.====In the counterfactual analysis, I simulate the outcomes of a copay coupon program introduced by the manufacturer of a branded cholesterol-lowering drug right after patent expiration. I consider different assumptions about the take-up of coupons, the ability of the branded manufacturer to direct coupons to the most price-sensitive consumers, and the insurer’s tolerance for a price increase of the drug with coupons. To disentangle the welfare effects driven by different incentives to issue coupons, I calculate the equilibrium results in three cases: 1) price discrimination only, 2) moral hazard only, and 3) price discrimination and moral hazard.====The counterfactual results differ a lot across the three cases. Copay coupons, when used only for price discrimination (through coupon targeting), slightly mitigate price competition and make the drug with coupons more affordable for price-sensitive consumers, which increases consumer welfare even when only 1% of the price-sensitive consumers receive a coupon. When coupons are used only for moral hazard, the coupon issuer raises the full price of the drug with coupons by as much as possible. At the same time, the coupon issuer sets a higher price for its other brands to make the drugs with coupons more attractive. These pricing strategies greatly soften price competition, increasing the profits of the coupon issuer and the spending by the insurer. Consumer welfare increases only when coupon penetration is sufficiently high. When the two incentives to issue coupons are jointly considered by the coupon issuer, price competition is further alleviated. Compared to the moral hazard case, the average prices of competitors’ brands are higher, the coupon issuer makes more profits, the insurer spends more, and consumer welfare gets lower.====The counterfactuals help to inform the legal and policy debate over copay coupons in several ways. First, I show how pricing differs under the two incentives to issue coupons. We can look at how drugs are priced in practice after coupons are introduced to markets to determine whether the argument about copay coupons from drug manufacturers or insurance companies is well founded. Second, we can better understand how the agents in pharmaceuticals, including drug manufacturers, insurance companies, and patients, are affected by copay coupons. Learning who gains and who loses from copay coupons, we can make policies that take care of certain groups without hurting the others too much. Furthermore, it is shown that the welfare implications of copay coupons vary with coupon penetration. The results can serve as the basis for policies aiming to regulate the scale of copay coupon use.====The study contributes to the literature on coupons by considering the moral hazard incentive to issue coupons which can exist in a market with the agency problem. Theoretical work by Holmes (1989) and Corts (1998) show that offering coupons helps price discrimination in an oligopoly setting since coupons can attract price-sensitive consumers. Shaffer and Zhang (1995) considers coupon targeting another mechanism to price discriminate in a competitive market. Empirically, Narasimhan (1984) finds that coupon users are more price sensitive than nonusers in several categories of consumer packaged goods. Nevo and Wolfram (2002) uses breakfast cereal data to show that coupons can spur price competition and lower shelf prices when they are widely available. In pharmaceuticals, doctors and patients make drug choice decisions, and insurance companies pay for most drug costs. The separate roles of decision maker and payer create a moral hazard problem, incentivizing drug manufacturers to use coupons to attract consumers by lowering their copays. I investigate the welfare impact of copay coupons driven by both price discrimination and moral hazard.====The empirical paper on coupons closest to my work is Dafny et al. (2017), who study substitution between branded drugs and their generic equivalents caused by the introduction of copay coupons. Using difference-in-differences models that utilize cross-state variation in the availability of coupons, they show that copay coupons significantly reduce the percentage of prescriptions filled with generics. My paper differentiates from theirs in three major ways. First, I consider substitution among all drugs in a therapeutic category while they focus on the competition between branded and generic drugs of the same molecule. Studying copay coupons at the category level and incorporating strategic interactions among all drugs for the same medical purpose provides a bigger picture of the welfare impact of copay coupons.====Second, branded drugs and their generic equivalents are assumed to have exactly the same quality in Dafny et al. (2017). While branded drugs and generics share active ingredients, Branstetter et al. (2016), Bronnenberg et al. (2015), and Kamenica et al. (2013) provide evidence that suggests difference in quality and consumer perspectives between brands and generics. In my demand model, I relax the assumption that generics are perfect substitutes for branded drugs, which helps capture the effect of physical and psychological drug differences on consumer welfare.==== Third, the counterfactual analysis conducted in my paper simulate equilibrium outcomes under various assumptions on the incentives to issue coupons, insurer’s exclusion threat, and coupon penetration. The results from the flexible settings can be helpful to inform legal and policy debate over copay coupons.====In addition, the structural model in the paper incorporates two important features of pharmaceutical demand. First, I apply a generalized extreme value (GEV) model developed by Bresnahan et al. (1997) to capture substitution patterns along multiple dimensions. Arcidiacono et al. (2013) use a similar model to study the welfare impact of me-too and generic drugs. The model has a nesting structure that allows for consumer switching based on different drug characteristics, including class, molecule, form, and version. This strength facilitates simulation of introducing copay coupons since the model well captures how consumers’ choices change when they are given coupons.====Second, following Berry et al. (2006) and Ching (2010a), I consider two types of consumers who differ in price sensitivity. The consumer heterogeneity helps to explain branded drugs’ pricing after they lose patent protection. Also, the consumer heterogeneity allows coupon targeting in the counterfactuals. The coupon issuer is assumed to have the information about consumer types and target the coupons at the price-sensitive consumers who are more likely to use coupons.====Finally, this paper combines several unique data sets that cover the major aspects in pharmaceutical demand, including prescription drug sales, physician advertising, direct-to-consumer advertising (DTCA), and copayments. Jayawardhana (2013) and Ridley (2015) are two of the few empirical studies that use such rich data in the demand estimation for pharmaceuticals. Most of the other papers in the literature use a single source of advertising to represent marketing efforts and/or include full prices in demand, which can lead to serious problems in estimation especially for the cholesterol-lowering drug markets. Physician advertising and DTCA both play an important role in demand for cholesterol-lowering drugs, as shown later in the estimation results. Ignoring any of the advertising efforts can make the estimates less precise or even biased. On the other hand, using full prices in the demand for prescription drugs can underestimate the price coefficient since consumers face only a small portion of the costs in making a drug choice.====The rest of the paper is organized as follows. Section 2 provides industry background and relevant information about copay coupons. Section 3 describes data. Section 4 develops models for copayments, demand, and supply. Section 5 discusses estimation strategies and results. Section 6 presents counterfactuals for introducing copay coupons under different scenarios. Section 7 concludes.",Pricing strategy and moral hazard: Copay coupons in pharmaceuticals,https://www.sciencedirect.com/science/article/pii/S0167718720300333,14 March 2020,2020,Research Article,187.0
"Katsoulacos Yannis,Motchenkova Evgenia,Ulph David","Department of Economic Science, Athens University of Economics and Business, Patission 76, Athens 104 34, Greece,Department of Economics, Vrije Universiteit Amsterdam, TILEC and Tinbergen Institute, De Boelelaan 1105, 1081 HV Amsterdam, the Netherlands,School of Economics and Finance, University of St Andrews, St Andrews, KY16 9AR, Scotland United Kingdom","Received 29 February 2020, Accepted 29 February 2020, Available online 7 March 2020, Version of Record 27 November 2020.",https://doi.org/10.1016/j.ijindorg.2020.102604,Cited by (6),"In many countries antitrust enforcement by Competition Authorities through prosecution and the imposition of penalties is complemented in price-fixing cases by private damage actions, which should affect both cartel deterrence and the prices set by those cartels that do form. We show that the impact of combining penalties and damages on cartel prices is not clearcut, and depends on both the nature of the penalty regime and the way that damages are calculated. We demonstrate this by focusing on two ways of calculating damages that have been advocated in practice and two different forms of the widely used revenue-based penalty regime. When the simple form of revenue-based penalties is in force, the standard method of calculating damages worsens its harmful pricing effects, whereas the proposed alternative method of calculating damages can overturn them. When a more sophisticated form of revenue-based penalties is in operation, imposing damages will improve its beneficial pricing effects under both methods of damage calculation, but the alternative method is more effective. In all cases combining penalties and damages improves deterrence.","Following the US and Canada, in many countries private damage actions against companies violating competition law, particularly by price fixing, are now common practice. This is increasingly so in Europe following the publication of the European Directive on Antitrust Damages Actions in 2014.==== Public enforcement sanctioning and private damage actions serve primarily different purposes. Public enforcement's main objective is to bring existing cartel activity to an end and to deter future violations by imposing sanctions on companies violating competition law. On the other hand, private damages focus on compensating those who have suffered from price-fixing. While the two policies are complementary, each can contribute to the objectives of the other.==== Public enforcement can increase the chances of successful private damage actions and the possibility of obtaining private damage actions could enhance detection by giving incentives to customers to discover and report price-fixing.====There is an extensive literature analysing the effects of public cartel enforcement in isolation from private damages, see e.g. Rey (2003), Buccirossi and Spagnolo (2007), Schinkel (2007), Bageri et al. (2013), Katsoulacos and Ulph (2013), Bos et al. (2018), Katsoulacos et al. (2015, 2019). Similarly, the isolated effects of treble damage awards in private antitrust law suits have been analysed in Salant (1987), Baker (1988), Block et al. (1981) and Besanko and Spulber (1990). The interplay of public and private antitrust enforcement has been discussed in e.g. Landes (1983), Segal and Whinston (2006), McAfee et al. (2008) and Wils (2017), but the focus has been mainly on deterrence effects.====The literature on the cartel pricing effects of combining public penalties and private damages is more sparse. Hunold (2013) analyses the effects of private damage actions on cartel pricing incentives in the setting where downstream firms, buying at excessive prices, are compensated for the lost profits or overcharges. In the context of industries selling to final consumers, Harrington (2004a, 2005) analysed the cartel pricing effects of combining private damages with a ==== public penalty.==== However, currently most jurisdictions, including the US and the EU, employ penalties based on cartel revenue.==== In this paper we focus on the implications for cartel pricing of combining private damages with these revenue-based penalties, under which the financial penalty to be imposed on a cartel is determined by applying a penalty rate to the cartel's revenue. We show that the implications depend crucially on the details of how the penalty rate is determined and on the method used for calculating damages. In some cases combining private damages with public penalties exacerbates the adverse pricing effects of penalties in isolation. In other cases combining the two can improve the already good pricing effects of the penalty regime in isolation.====Rather than aiming at very general results we establish these conclusions by focusing on just two forms of revenue-based penalties and two methods of calculating damages.==== These are theoretical characterisations of penalty regimes and penalty calculation methods that are used or have been advocated for use in practice, and allow us to demonstrate the range of conflicting conclusions mentioned above. Although our focus is on cartel pricing behaviour, we also examine the combined impact of penalties and damages on deterrence, and confirm the unsurprising conclusion that, in all cases, deterrence is higher when public penalties are combined with private damages, than under public penalties alone.====Before setting out the two forms of revenue-based penalties that we consider, we note that these penalty regimes typically take the form of a ==== that is imposed for the anti-competitive offence of forming a cartel and raising price above the but‑for price. As discussed below this penalty rate may or may not be set in a way that takes account of the severity of the offence – essentially the size of the overcharge. This rate is subsequently adjusted to reflect a variety of aggravating and mitigating factors such as how co-operative the parties were in the investigations, recidivism, proportionality, bankruptcy concerns etc., all of which are unrelated to the cartel price set in the particular offence that is being penalised.==== Moreover these behaviours and considerations lie outside the scope of this paper, which focuses on how the penalty and damages system affects cartel pricing behaviour, so we follow the extensive literature on how the penalty system affects cartel behaviour in essentially treating the penalty system as if it comprised just the ====The two forms of revenue-based penalty that we consider are (i) a simple revenue-based penalty under which the total size of the financial penalty is determined by multiplying cartel revenue by a fixed penalty rate that is the same across all cases and unrelated to the price actually set by the cartel; (ii) a linear sophisticated revenue-based penalty regime under which the penalty rate applied to cartel revenue varies directly in proportion to the proportional overcharge set by the cartel, where this factor of proportionality is the same across all cases and is also publicly known. Here the proportional overcharge is defined as the absolute overcharge (difference between the cartel price and the but‑for price) divided by the but‑for price. As we have argued elsewhere – Katsoulacos et al. (2015, 2019) – the first form of penalty regime is taken by many scholars who analyse revenue-based penalties to characterise the existing penalty regime in many countries.==== The second can be thought of as characterising, albeit in a very stylised way, the more recent guidance by the UK Competition and Market Authority CMA (2018) under which the baseline penalty rate can vary between 0% and 30% depending on the seriousness of the infringement. From the existing literature we know that, in the absence of private damages, under the first form of revenue-based penalty the cartel price will be above the monopoly price - see for example Bageri et al. (2013), Katsoulacos and Ulph (2013), and Katsoulacos et al. (2015) – while under the second it will be below the monopoly price – Katsoulacos et al. (2019).====Turning to the two methods for calculating damages, the first is what we call the traditional measure of damages applied by courts. Here the actual quantity purchased is multiplied by the ==== - the difference between the actual price and the but‑for price (see e.g. Harrington (2004b) or Brander and Ross (2017)). This method is widely employed by the Canadian and US Courts, for example, ====, 2013 SCC 58 [====] in Canada.==== However, as noted by many authors - Fisher (2006), Han et al. (2008), Basso and Ross (2010), Paha (2012), Rosenboom et al. (2017), and Franck and Peitz (2017) – this traditional measure of damages underestimates the true total harm, since it does not account for the dead-weight loss arising because some purchases, which would have taken place at the but‑for price, do not occur at the collusive price.====A variant of this traditional method is that proposed by Brander and Ross (2017) in which the cartel revenue is multiplied by the proportional overcharge defined as the absolute overcharge divided by the cartel price. Although formally equivalent to the traditional measure, Brander and Ross (2017) argue that it can be easier to calculate in situations when the same product is sold for different prices to different consumers; or when there are multiple variants of at-issue products commanding different prices; or where products are bundled and it is hard to differentiate between the prices and the quantities of each particular good in the bundle.====Building on this approach we define an alternative method of calculating damages that we call the ==== method in which cartel revenue is multiplied by the ==== defined, as in the case of the linear sophisticated revenue-based penalty, as the absolute overcharge divided by the but‑for price. This requires exactly the same information as the Brander and Ross measure and so has the same computational advantages as the measure they propose. Moreover, as explained below, it has two other advantages.====Our major conclusions are as follows. When private damages are combined with the simple form of revenue-based public penalties, then, if damages are calculated in the traditional way, the cartel price will be even further above the monopoly price than would be the case under the penalty regime alone. However, if, under simple revenue-based penalties, damages are calculated by the alternative method, then there is a range of cases where the cartel price will not only be lower than that under the penalty regime alone but will lie below the monopoly price and so totally eliminate the distortion caused by the penalty regime. When private damages are combined with a linear sophisticated form of revenue-based public penalties then, under both methods of calculating damages, the cartel price will be further below the monopoly price than would be the case using penalties alone. However, the lowest cartel price is achieved when the alternative method of calculating damages is used.====This suggests that there is a strong case for using the alternative method of calculating damages more widely, on the grounds that, in addition to mitigating the under-compensation property of the traditional method, it also has better pricing properties. A more general conclusion is that it can be misleading to examine the effects of damage claims and public enforcement in isolation from one another: their interaction is important but depends on the precise details of each of the two policies.====The structure of the paper is as follows. Section 2 outlines the model set-up. Section 3 provides a detailed analysis of the price effects and the deterrence properties of the two types of revenue-based penalty regimes combined with the two main methods used for calculating recoverable damages. Section 4 examines the robustness of our conclusions to several extensions of the workhorse model. Section 5 concludes.",Combining cartel penalties and private damage actions: The impact on cartel prices,https://www.sciencedirect.com/science/article/pii/S0167718720300266,7 March 2020,2020,Research Article,188.0
"Kesternich Iris,Schumacher Heiner,Van Biesebroeck Johannes,Grant Iris","KU Leuven, Belgium,CEPR, UK,InterMutualistisch Agentschap, Belgium","Received 10 April 2019, Revised 21 February 2020, Accepted 2 March 2020, Available online 7 March 2020, Version of Record 27 March 2020.",https://doi.org/10.1016/j.ijindorg.2020.102605,Cited by (5),"An active empirical literature estimates entry threshold ratios (ETRs), introduced by Bresnahan and Reiss (1991), to learn about the impact of firm entry on competition. We show that in the standard homogeneous goods ==== model, there is no monotonic relationship with the price-cost margin, one measure for the strength of competition. Regardless of the shape of demand, the ETR is hump-shaped in the number of active firms. It can also increase with entry in the Salop model of product differentiation or in a game of repeated interactions where collusion is possible. Empirical applications should use caution and only interpret changes in the ratio as indicative of a change in competition when the number of firms is sufficiently large.","The entry threshold ratio (ETR) was introduced in a seminal paper of Bresnahan and Reiss (1991). It is defined as the ratio of two consecutive entry thresholds, which are both normalized by the number of active firms. The entry threshold ==== indicates the minimum market size or number of customers needed for ==== active firms to break even. The ratio ==== then measures the increase in market size per firm that is needed for an additional firm to be able to enter a market with ==== incumbents without incurring a loss.====The ETR is intended to measure the rate at which variable profits fall with entry. If competition becomes more intense such that the price-cost margin falls when additional firms compete and fixed entry costs are nondecreasing in the order of entry, the ratio will exceed one. When market competition approaches the monopolistically competitive benchmark, additional entry no longer changes the price-cost margin and the ETR will converge to unity.====The original application studied a few local service professions, namely doctors, dentists, druggists, plumbers, and tire dealers, but it has since been applied in a wide range of circumstances. This includes industries as varied as banking (Feinberg and Reynolds, 2010), hospitals (Abraham et al., 2007), brewing (Manuszak, 2002), broadband (Xiao and Orazem, 2011), newspapers (Pfann and Van Kranenburg, 2003), or TV stations (Nishida and Gil, 2014), among many more applications.====While most applications identify entry thresholds solely from cross-sectional variation in market size, a few studies have relied directly on actual entry or exit events (Varela, 2018) or verified whether entry or exit occurs in the expected direction when the market size changes (Carree and Dejardin, 2007). The ETRs have also been adapted to product differentiation where entry can expand the market (Schaumans and Verboven, 2015) or where two types of competitors have asymmetric competitive effects (Dranove, Gron, Mazzeo, 2003, Cleeren, Verboven, Dekimpe, Gielens, 2010).====Each of the studies cited above reports a sequence of ETRs to gauge how the intensity of competition changes with the number of active firms. Using ==== for the per-firm entry threshold (====/====), an estimate of ====/==== that exceeds one is interpreted as evidence that a duopoly is more competitive and leads to lower variable profits than a monopoly. Results are generally discussed relative to a benchmark of a diminished effect on the price-cost margin for each additional entrant, and an expectation that the ratio ====/==== will be lower than ====/====. However, we show that such a monotone decline in ETRs is not predicted by the standard model that assumes Cournot quantity competition and a linear demand curve. With constant marginal costs and no heterogeneity across firms, we show that ====/==== < ====/====, i.e., the ratio for the third entrant is higher than for the second, even though the price-cost margin is lower. In the benchmark model, ETRs decline monotonically only once there are more than three entrants, but it takes seven firms before it falls below the value for ====/====.====The intuition for this surprising theoretical finding can be seen from the fact that market size per firm and industry variable profits change at the same rate, but in opposite directions, with the number of firms. The first rises and the second declines monotonically with aggregate quantity, and thus with the number of active firms, but they do not change at a constant rate. Entry has two opposing effects on aggregate profits, a negative effect through a reduced price-cost margin, but also a positive effect due to higher total output. The rate of decline is determined by the net effect. It turns out that profits decline only slowly initially, for low values of ====, and the rate of decline increases with ==== over an initial range.====This hump-shaped pattern in the dependency of the ETR on the number of active firms is not limited to the simple benchmark model. We show that it also holds for more general forms of demand. However, when one considers only integer numbers of firms, the hump will not appear in the data if demand is highly inelastic. I.e. the peak of the hump might be that close to ====/==== that ====/==== > ====/====. We further show that alternative models of competition can also generate a hump, i.e. a rising ETR with entry when the number of incumbents is low, but for different underlying reasons. In the product differentiation model of Salop (1979), the expansion of the relevant market size with entry lowers the necessary market size per firm and depresses the ratio for low ====.==== In a model of repeated interaction, it is well known that collusion is more likely to prevail when the number of competitors is low (e.g., Tirole, 1988). When collusion becomes unsustainable, the ETR jumps.====These insights are not only interesting from a theoretical perspective, they are important to keep in mind from an applied perspective. They imply that the evolution of estimated ETRs with the number of active firms does not map directly into a change in intensity of market competition or a change in the effect of entry on the price-cost margin.==== The change in ETRs only becomes a reliable predictor for the evolution of competition once the number of firms is sufficiently large.====A solution could be to add additional assumptions, for example, a functional form for demand and cost homogeneity, in which case more features of the economy could be identified. However, the ETR’s appeal primarily comes from the light modeling and data requirements. The main message then is to avoid attaching any interpretation to the change from ====/==== to ====/====. It is particularly important to keep this in mind given that the approach is most frequently used for small markets with only a few active firms.====The remainder of the paper is organized as follows. In Section 2, we derive the hump-shaped pattern for the ETRs in a model of Cournot competition between homogeneous goods’ producers. We use the case of linear demand for illustration and to discuss intuition, but then establish that it holds as well for more general demand functions. In Section 3, we show that a hump-shape pattern can occur for other modes of competition as well. In Section 4, we review implications for empirical work and Section 5 concludes.",Market size and competition: A “hump-shaped” result,https://www.sciencedirect.com/science/article/pii/S0167718720300278,7 March 2020,2020,Research Article,189.0
"Amelio Andrea,Giardino-Karlinger Liliane,Valletti Tommaso","European Commission, DG Comp, Belgium,Imperial College London, United Kingdom,University of Rome Tor Vergata, Italy","Received 10 February 2020, Accepted 12 February 2020, Available online 28 February 2020, Version of Record 27 November 2020.",https://doi.org/10.1016/j.ijindorg.2020.102592,Cited by (9),"This paper studies the incentives to engage in exclusionary pricing in the context of two-sided markets. Platforms are horizontally differentiated, and seek to attract users of two groups who single-home and enjoy indirect network externalities from the size of the opposite user group active on the same platform. The entrant incurs a fixed cost of entry, and the incumbent can commit to its prices before the entry decision is taken. The incumbent has thus the option to either accommodate entry, or to exclude entry and enjoy monopolistic profits, albeit under the constraint that its price must be low enough to not leave any room for an entrant to cover its fixed cost of entry. We find that, in the spirit of the literature on limit pricing, under certain circumstances even platforms find it profitable to exclude entrants if the fixed entry cost lies above a certain threshold. By studying the properties of the threshold, we show that the stronger the network externality, the lower the thresholds for which incumbent platforms find it profitable to exclude. We also find that entry deterrence is more likely to harm consumers the weaker are network externalities, and the more differentiated are the two platforms.","Two-sided markets are an area of considerable research in the field of Industrial Organisation. Based on the initial work of Caillaud and Jullien (2003), Rochet and Tirole (2003) and Armstrong (2006), the literature has studied, among other problems, the role of the price structure in solving coordination problems. Caillaud and Jullien (2003) and Armstrong (2006) show that platforms may set “negative prices” on one side in order to enhance participation. This paper builds on this work on two-sided markets, merging it in particular with the literature dealing with exclusionary pricing.====Two-sided platforms generally refer to situations where (at least) two distinct user groups (i.e. two demands) interact with each other through a common platform and the participation of at least one of these groups affects the value of participation for the other group(s). Following Evans (2003), “a platform constitutes the set of the institutional arrangements necessary to realize a transaction between two users groups”.====Two-sided platforms are very common and are present in many markets including: stock exchanges, internet portals, payment card systems, newspapers, television broadcasters, directories, smartphones, mobile and fixed telecommunication networks and estate agents. These examples cover very diverse industries affecting many different aspects of consumers’ lives. It is therefore essential to have a thorough understanding of these platforms not only from an academic perspective but also from a more practical perspective, for instance in order to properly enforce antitrust scrutiny.====Often, in the scientific domain as well as in the public debate, it is advocated that multi-sided platforms deserve a special (typically, more relaxed) scrutiny by antitrust authorities (see Evans and Schmalensee, 2007). The aim of this paper is to examine whether the presence of (indirect) network externalities makes platforms indeed less prone to exclusionary conduct, as is often maintained, or whether instead the opposite is the case.====We answer this question in a setting where we apply the concept of entry deterrence in a sequential move game à la Dixit (1980) to a canonical two-sided market that we borrow from Armstrong (2006). Users (final consumers) on one side derive a positive externality from participation on the other side. We study both the monopoly and the duopoly equilibria of this model, where duopoly arises as the result of an entry game in which an incumbent platform can set its prices to the two user groups (and commit to these prices) before a potential entrant gets to make its entry and pricing decisions. Thus the incumbent has the ability to deter entry in our setting, and possibly also the incentives if it is profitable to do so. We study when this indeed arises, and its welfare properties.====The debate on platforms and the role of competition policy is very much at the forefront of the current policy discussion. There is a widespread climate of reflection around the world, as evidenced by many reports and hearings, notably the FTC hearings in the US, the European Commission special advisors report (European Union, 2019), the French and German reports (Autorité de la concurrence, Bundeskartellamt, 2016), the Furman report (HM Treasury, 2019) and the House of Lords report (House of Lords, 2016) in the UK, the JFTC report on data in Japan (JFTC, 2017), and the recent final report from the Australian Competition Authority (ACCC, 2019). All these reports and hearings explore how competition authorities should meet the challenges arising from analysing competition between platforms and show a high degree of convergence when it comes to identifying these challenges. Among the most important ones, it is worth recalling (i) the reflection on how competition authorities should define relevant markets with multi-sided platforms, (ii) understanding the role of data, (iii) the need to scrutinise acquisitions of innovative start-ups by the digital incumbents, (iv) analysing the source of platforms’ market power, and (v) last but not least (and crucially for our paper) how platforms compete in markets where network externalities and returns to scale are strong, especially in the absence of multi-homing, or protocol and data interoperability. Eventually they all seem to advocate for a closer and more thorough scrutiny of platforms recognising their ability and incentive to hamper competition.====This paper is very much inspired by the current debate, and its aim is to provide some guidance to competition authorities on the likelihood of anticompetitive foreclosure in a context where indirect network externalities are strong and the platforms are differentiated. The paper reaches the conclusion that under certain circumstances the presence of network externalities does not make platforms less prone to embracing exclusionary conduct.==== The paper also provides two extensions that preliminarily study, on the one hand, the restriction on the platforms to set non-negative prices and, on the other hand, the introduction of the option for customer to multi-home on one side of the platform, i.e. so called competitive bottlenecks.====The remainder of the paper is structured as follows. Section 2 reviews the relevant literature. Section 3 builds the benchmark model. Section 4 characterises the equilibria. Section 5 studies the welfare implications. Section 6 presents a discussion on non-negative prices, and on competitive bottlenecks. Section 7 concludes.",Exclusionary pricing in two-sided markets,https://www.sciencedirect.com/science/article/pii/S016771872030014X,28 February 2020,2020,Research Article,190.0
"Calzolari Giacomo,Denicolò Vincenzo","European University Institute and CEPR, Department of Economics, Via Fontanelle 18, 50014, Fiesole, Italy,University of Bologna and CEPR, Department of Economics, Piazza Scaravilli 2, 40126, Bologna, Italy","Received 6 February 2020, Accepted 7 February 2020, Available online 26 February 2020, Version of Record 27 November 2020.",https://doi.org/10.1016/j.ijindorg.2020.102589,Cited by (3),"We analyze, by means of a formal economic model, the use of the discount-attribution test to assess the competitive effects of loyalty discounts. (The discount-attribution test is a variant of the price-cost test, where the discount is attributed only to the share of total demand that is regarded as effectively contestable.) In the model, a dominant firm enjoys a competitive advantage over its rivals and uses market-share discounts to boost the demand for its own products. In this framework, we show that the attribution test is misleading or, at best, completely uninformative. Our results cast doubts on the applicability of price-cost tests to loyalty discount cases.","Loyalty discounts are a highly controversial issue in competition policy.==== There is no consensus on the rationale for these practices, their competitive effects, and the appropriate antitrust treatment. Much of the policy debate centers around the problem of whether the legality of loyalty discounts should be assessed by means of some sort of price-cost test. It is this last question, in particular, that we address in this paper.====Price-cost tests come in different varieties. All of them compare the dominant firm’s relevant price to its own cost, on the grounds that only those practices that are capable of foreclosing an as-efficient competitor ought to be regarded as anticompetitive. But different versions of the test use different prices for the comparison.====The traditional test takes the relevant price to be simply the average price, which is obtained by spreading the total rebate over all of the dominant firm’s sales. Critics however argue that this inflates the relevant price, making the test too easy to pass. In this form, in fact, the price-cost test had almost become a synonym for tolerant policy in loyalty discounts cases.====But a new test has recently been introduced by antitrust authorities and the courts, which does not attribute the total rebate to the entire demand but only to the part of it that is regarded as effectively contestable. This new test is often referred to as the ====, or the ====. Since the rebate is attributed to a lower volume, the price used for the comparison is lower. This implies that the new test is generally more demanding than the traditional one.====We contend, however, that neither variant of the test really helps screen out cases where loyalty discounts are anti-competitive. For the traditional variant, our analysis simply confirms arguments already made in the literature. The more original contribution of the paper is the analysis of the discount-attribution test. This test, too, has been vigorously criticized but early critics (with the exception of Greenlee et al., 2008) have focused on the practical difficulties of implementing the test, and on antitrust agencies’ tendency to underestimate the contestable demand. If this were the only problem, though, the solution would simply be to use more precise estimates. We argue, in contrast, that the problem is not only that of measurement errors. Indeed, we show that even if contestable demand were measured perfectly, the discount-attribution test would produce many false positives and false negatives. This means that the test is in itself misleading or, at best, completely uninformative.====The paper closest to ours is Greenlee et al. (2008). These authors analyze the case of bundled discounts and show, by numerical examples, that the incremental test may fail when the discounts increase social welfare (false positives), and pass when they decrease it (false negatives). We focus instead on the case of market-share discounts, and show that type I and type II errors are not only possible, but actually prevalent. Our results thus complement and reinforce those obtained by Greenlee et al. (2008). Taken together, they cast doubts on the usefulness of price-cost tests in loyalty discount cases.====The rest of the paper is organized as follows. Section 2 provides a more extensive discussion of the scholarly and policy debate. The paper then proceeds to the formal analysis. Section 3 sets out a simple model of two firms, a dominant firm and its rival, which supply differentiated products. The model’s equilibrium is characterized in Section 4, while in Section 5 we analyze the profitability and welfare effects of the discounts. Section 6 analyzes the application of the discount-attribution test and shows that it systematically produces many type I and type II errors. Section 7 discusses how the attribution test might be modified so as to better account for product differentiation but shows that even the proposed variant of the test suffers from the same drawbacks. Section 8 offers some concluding remarks.",Loyalty discounts and price-cost tests,https://www.sciencedirect.com/science/article/pii/S0167718720300114,26 February 2020,2020,Research Article,191.0
"Mariuzzo Franco,Ormosi Peter L,Majied Zherou","Centre for Competition Policy, University of East Anglia, Norwich NR4 7TJ, UK,School of Economics, University of East Anglia, Norwich NR4 7TJ, UK,Norwich Business School, University of East Anglia, Norwich NR4 7TJ, UK,Comtech Systems Inc., Victoria, BC, Canada","Received 17 January 2020, Accepted 20 January 2020, Available online 21 February 2020, Version of Record 21 February 2020.",https://doi.org/10.1016/j.ijindorg.2020.102584,Cited by (6)," techniques on a sample of 339 listed cartel member firms, prosecuted by the European Commission between 1992 and 2015. Our results offer evidence that in the context of cartels, public and reputational sanctions act as substitutes: where there is a reputational penalty, increasing this penalty reduces the effect of the public sanction. One the other hand, in the absence of a reputational punishment, the effect of the cartel fine steps in.","Most illegal behaviour faces two types of sanctions: a public sanction, fines imposed directly by an administrative body or the court; and a reputational sanction, which materialises indirectly through market mechanisms. The law and economics literature that studies reputational and public sanctions seems to agree that reputational sanctions can deter corporate misbehaviour, inasmuch as the offenders internalise the social cost of the offences; and concords on the point that public sanctions are needed when the harm caused by the offence is not internalised.====An important upshot of previous literature is that reputational sanctions tend to work in contractual relationships, particularly, in repeated purchase settings. Klein and Leffler (1981), Shapiro (1983), and Lott Jr (1988) provide theoretical models where customers of a business change their custom in response to a breach in their contractual agreement. The literature distinguishes between ‘related-party’ and ‘third-party’ crimes to refer to behaviour that affects contractual parties (for example some cases of fraud, as in Karpoff and Lott Jr, 1993) and behaviour that impacts third parties (for example environmental violations, as in Karpoff et al., 2005). The credibility of the reputational sanction derives from the change in purchasing patterns that accompanies a variation in beliefs about the offence. In this case, some, or all costs of the damages caused by the misbehaviour can be internalised by the offending firm through its repeated contracting with customers, suppliers, employees, and investors. Reputational sanctions should be sufficient where the only damaged party is a customer of the unlawful business and the total amount of the damages are internalised by the offender. The above literature argues that in this setting public sanctions would only be required to the extent the reputational sanction does not encompass the total social cost of the offence.====Empirically, Peltzman (1981), Karpoff and Lott Jr, (1993), Alexander (1999), Karpoff et al. (2008) studied how markets react to fraudulent or misleading behaviour. Jarrell and Peltzman (1985) and Mitchell and Maloney (1989) examined the reputational impact of product recalls (faulty products). Both of these behaviours are related-party conducts. Jones and Rubin (2001) and Karpoff et al. (2005) looked at environmental violations (a third-party offence). Their findings are in line with the theory: reputational sanctions are effective mostly in related-party offences, but less so in third-party offences, where therefore more reliance on public sanctions is warranted. Armour et al. (2017) provide further empirical evidence to this distinction.====One of the key difficulties in these previous empirical works is that too often reputational sanctions (and largely their determinants) are difficult to directly observe and measure, and this complicates the empirical evaluation of their effectiveness. To overcome this, previous works have used an indirect measure of this reputational impact by decomposing the share price effect of various corporate wrongdoings into: the effect of the public sanction, a readjustment effect (without the wrongdoing, profits are expected to be lower), and a residual which can be associated to reputational loss (see for example: Alexander, 1999, Karpoff, Lee, Martin, 2008, Karpoff and Lott Jr, 1993, Peltzman, 1981). This has been driven by the intuition that reputation is an intangible asset, the value of which is expressed as a component of the share price valuation of the business. In this setting, reputational loss is manifested by a stock price drop because the share price valuation reflects investors’ expectations of loss in future profits at lower levels of reputation. This might entail expectations that future sales will drop, or that firms will have to spend on correcting measures (extra advertising or price drops) to mitigate the reputational damage.====In comparison to these previous works, we provide a more direct way to approximate the magnitude of the reputational effect. Recent developments in opinion mining and natural language processing allow us to extract the opinion in the media coverage of corporate offences, and to study the relationship between share price valuation and opinion (Van de Kauter et al., 2015). To the best of our knowledge, no such analysis has been conducted to investigate the relative effectiveness of public and reputational sanctions on firms’ behaviour. The context of illegal cartels gives us an ideal ground to test our approach. When the regulator discovers a cartel, this information is not automatically distributed to all related parties (especially not to atomistic consumers). Various information channels are in action to pass the news on the cartel conduct to the public, and because without this information there would be no reputational impact, we posit that the reputational effect is directly related to the sentiment and the intensity of the information.====Our proposal offers a workable, and easy-to-implement way to study reputational and public sanctions (in our case cartel fines).==== Information on public sanctions are directly obtained from administrative and court decisions, and reputational sanctions are approximated by the intensity and the sentiment of media exposure of the misconduct. We estimate the effect of these measures on the market valuation of offending firms. In this, we borrow from the work of Aguzzoni et al. (2013), which we use as a benchmark study, who employ a similar event study approach to estimate the impact of the public fine on firm valuation. Compared to theirs, we use an updated dataset, which includes the cases that were announced between the completion of their study and our own data collection. We first partly replicate their event study estimates of the effect of the public sanction. Our estimates, conducted independently of Aguzzoni et al. (2013) confirm their results. But then, as our main contribution, using our approximation of reputation, we also estimate the effect of reputational sanctions, and study the interaction between these and the public sanction, and how they impact on the cumulative abnormal rate of return of shares.====One advantage of this approach is that data on the public, and our measure of private sanction are freely available. To demonstrate this, we use a publicly available dataset on EU cartels assembled by the European Commission (Commission), which we merge with a database on news articles that document and describe the illegal cartel (and cartel members) behaviour. We do a count of news sources per cartel member as indicator of exposure of each cartel member to the news, and then quantify the opinion of each news’ item using sentiment analysis. Loss or gain in market valuation is used as a proxy for the deterrence power of these two types of sanctions.====Our approach also provides an opportunity to look at the relative role of public and reputational sanctions in deterring misconduct. Based on the premise that businesses are averse to falling market valuation, we assume that sanctions with larger negative effect on market valuation act as stronger deterrent. We then estimate the relative effect of public and reputational sanctions on firms’ market valuation. If the two are substitutes (complements), the marginal effect of public sanctions on firm performance will be reduced (augmented) by the effect of higher reputational sanctions. This follows the same logic as a number of previous theoretical models, such as in Karpoff and Lott Jr (1993).==== Ganuza et al. (2016) also offer a theoretical take on substitutability and complementarity between legal and reputational sanctions. Although their attention is on product liability, their model is highly relevant to us, as it is fundamentally a simplified version of the collusion model by Green and Porter (1984). They describe the conditions in which public and private sanctions are substitutes, and when they are complements. Moreover, they highlight the importance of public sanctions in circumstances where reputational sanctions are less effective (such as when the informational asymmetry is severe, the firmâs surplus from future trade is not large, or the time horizon of many market participants is not long) - in which cases the role of the legal system in encouraging trade becomes more relevant, perhaps essential.====Part of our results confirm the findings of Aguzzoni et al. (2013): cartel members are financially more hurt at detection than at decision, and fines have an impact on firm valuation in the proximity of the decision. The remaining results are new. Our main findings show that fines play a key role on a narrow window around the decision, while reputational sanctions reflect value losses on a larger time period. Using three different methods (estimating marginal effects, direct testing, and regression trees), we also offer support to previous theoretical findings, such as Ganuza et al. (2016), that the sentiment of media coverage and the fine imposed on the cartel act as substitutes (one’s respective effect intensifies when the other’s magnitude diminishes).====The paper proceeds with a description of the institutional setting. Section 3 introduces the various types of data sources needed for the study. In Section 4 we present the methodologies. Section 5 discusses the results and looks at whether public and reputational sanctions are substitutes or complements in deterring misconduct, and offers a set of robustness checks. Section 6 concludes with a more general discussion.",Fines and reputational sanctions: The case of cartels,https://www.sciencedirect.com/science/article/pii/S0167718720300060,21 February 2020,2020,Research Article,192.0
"Hellwig Michael,Schober Dominik,Cabral Luís","ZEW Centre for European Economic Research and MaCCI Mannheim Centre for Competition and Innovation, Germany,University of Mannheim, ZEW Centre for European Economic Research, MaCCI and MISES, Germany,Leonard N. Stern School of Business, New York University, and CEPR, United Kingdom","Received 23 January 2020, Accepted 30 January 2020, Available online 11 February 2020, Version of Record 27 November 2020.",https://doi.org/10.1016/j.ijindorg.2020.102587,Cited by (6),"We propose a difference-in-differences approach to estimating the impact of incentives on cost reduction in the context of German electricity networks. When subject to a lower-powered regulation mechanism, relatively more efficient operators pile up more costs in the year used to determine future prices. This pattern is consistent with the idea that incentives matter: higher-powered incentives lead to cost reduction. The results are also consistent with an equilibrium where more efficient firms pool with less efficient ones under the threat of ratcheting.","The regulation of electric utilities is a topic of great research interest and practical relevance. In the past few decades, theoretical and empirical scholars, as well as policy makers, have addressed various issues related to mechanism design and cost-efficiency incentives, especially in the presence of information asymmetries between regulator and regulated firm. At the risk of oversimplifying, one might say that, in terms of the investment incentives provided, regulation mechanisms can be high-powered or low-powered. In a high-powered incentive mechanism, price caps are largely independent of firms’ costs. This provides regulated firms high incentives for cost reduction, but at the cost of setting prices that may be too high or too low. In a low-powered incentive mechanism, prices are set in line with the regulated firms’ costs; this prevents major misalignments between prices and costs, but at the cost of providing low incentives for cost reduction.====The trade-off between high- and low-powered incentive mechanisms is largely an empirical question: do cost-reduction incentives really matter? Do regulated firms subject to higher-powered regulation mechanisms invest more in cost reduction?====The German system for regulating electricity distribution system operators (DSOs) provides a natural setting for addressing these questions. A legal exemption in the German incentive regulation system effectively results in two different regulatory regimes, one with higher-powered incentives than the other. Specifically, the default regulatory mechanism unfolds over a five-year period. While revenue caps are initially based on the DSOs’ own costs, caps gradually decrease over time and are eventually determined by the industry’s most cost efficient firm (which the regulator identifies beforehand by means of efficiency analyses). In this sense, the default regulatory regime is a hybrid of cost-based regulation (first year) and yard-stick regulation (last year of the regulatory period).====Small DSOs (those with less than 30,000 connected consumers) can opt for an alternative regulation regime. As in the default regime, revenue caps are initially based on the DSOs’ own costs. However, unlike the default regime, where prices adjust toward the fifth-year yardstick cap, under the alternative system prices adjust at an exogenously given rate. In this sense, the alternative regulatory regime provides lower incentives for cost reduction: even fifth-year prices remain a function of first-period costs. This regime is thus based to a larger degree on cost-based regulation than the default regime and disregards the individual DSOs’ true cost efficiency when demanding cost reductions. The default regime relying on a yardstick element is thus much closer to the theoretical ideal of a price cap regulation determining exogenous prospective price targets (while also being cost-based with respect to the initial base costs).====The German regulatory system is idiosyncratic in several ways. However, the relation between high-powered incentives and cost reduction is of more general interest. In order to better understand the essential features, we develop a simple model of regulation and cost reduction. We derive three results, all corresponding to testable predictions. First, everything else constant, firms under the low-powered regime (the “revenue-cap” regime, or simply ====) inflate their costs to higher levels. Second, this cost inflation is particularly significant for firms that are more efficient to begin with (and for which the scope for cost inflation is greater). Third, cost inflation is particularly significant for operating expenditures (as opposed to capital expenditures), given that the former can more easily be “targeted” at one specific year.====To test these predictions, we propose a difference-in-differences (DiD) approach. The first level of difference in our DiD analysis compares the base year, that is, the year which determines subsequent years’ caps, to the years when this cost-inflation incentive is absent. The second level of difference in our DiD analysis compares DSOs subject to a high-powered mechanism (the “yardstick” regime, or simply ====) to DSOs subject to a low-powered mechanism (the “revenue-cap” regime, or simply ====).====The DiD approach allows us to control for potentially confounding factors such as a heterogeneous expansion of power plants for decentralized renewable electricity generation. Moreover, it enables us to account for the potential selection bias due to the non-random assignment of treatment. We argue that the participation choice of small DSOs is driven by expected gains that depend on time-invariant unobservables (such as propensity to take regulatory risks). The average treatment effect on the treated can then still be consistently estimated with DSO-specific effects (Blundell and Dias, 2009).====We use data on 150 German DSOs over the period 2010–2013. Revenue caps for the regulatory period 2014–2018 are based on each DSO’s cost in 2011, the base year. We compare costs in the base year to costs in the other years of the first regulatory period. Our base results show no difference between the ==== and ==== regimes. However, when we restrict to more efficient firms, we estimate (with precision) that the ==== regime implies a 3 to 5% inflation in DOS costs during the base year. If we further distinguish operating from capital expenditures, then we find a higher effect on the former (4 to 6%) and no significant effect on the latter.====Overall, these results are consistent with the basic idea that incentives matter: If a regulated firm can keep a greater fraction of its cost savings, then cost savings are greater. The fact that the effect is particularly strong for firms that are more efficient is consistent with two different ideas, both of which we discuss in detail in the theory section of the article: First, more efficient firms have a greater ability to add wasteful expenditures to their cost base. Second, in a world of asymmetric information and sequential regulation without regulator commitment, efficient regulated firms have an incentive to pool with inefficient firms: the ratchet effect (Laffont and Tirole, 1993).====The paper is organized as follows. The next section discusses related literature. Section 3 provides an overview of the German regulatory setting; a stylized theoretical model; and a set of testable hypotheses. Our empirical approach is explained in Section 4, and the results are presented and discussed in Section 5. Section 6 concludes.",Low-powered vs high-powered incentives: Evidence from German electricity networks,https://www.sciencedirect.com/science/article/pii/S0167718720300096,11 February 2020,2020,Research Article,193.0
Ma Chao,"Wang Yanan Institute for Studies in Economics, Department of Finance at School of Economics, and MOE Key Laboratory of Econometrics, Xiamen University, China","Received 2 August 2019, Revised 10 January 2020, Accepted 4 February 2020, Available online 11 February 2020, Version of Record 9 April 2020.",https://doi.org/10.1016/j.ijindorg.2020.102588,Cited by (1),"Theoretically, if firms face a regulatory per-customer quantity limit, they should have an incentive to discriminatively charge high-demand customers higher prices and make them just willing to buy a quantity equal to the limit. In the U.S. residential mortgage industry, mortgages with origination balances above the conforming loan limits cannot be guaranteed by government-sponsored enterprises, which make lenders face a per-customer quantity limit. This paper finds that borrowers bunching at the limit pay higher interest rates due to price discrimination. This study rules out the alternative explanation that those borrowers are of higher risk (lending cost) than other borrowers.","Theoretically, if firms are subject to a regulatory quantity limit when selling their products to each customer and are capable to conduct a third-degree price discrimination, those firms should have an incentive to discriminatively charge high-demand customers higher prices and make them just willing to buy a quantity equal to the limit.====In the U.S. residential mortgage industry, government-sponsored enterprises (GSEs) such as Fannie Mae and Freddie Mac, are restricted by law from purchasing or guaranteeing single-family mortgages originated by mortgage lenders with balances above a specific amount. This amount, known as the “conforming loan limit,” is set by the Federal Housing Finance Agency (FHFA) every year. Mortgages with origination balances less than or equal to this limit are known as “conforming mortgages,” whereas mortgages with origination balances above this limit are known as “nonconforming mortgages” or “jumbo mortgages.” Mortgage lenders, including banks and mortgage companies, generally charge higher interest rates on nonconforming mortgages than on conforming mortgages, as nonconforming mortgages cannot be purchased or guaranteed by Fannie Mae or Freddie Mac. The purpose of this loan limit policy set by the FHFA is to enhance the mortgage credit supply to low-to-moderate income households that would otherwise be underserved and to help them build homeownership.====Within the conforming mortgage category, do lenders charge borrowers at the limit higher interest rates than other borrowers? Because borrowers have downward sloping loan amount demand as a function of the interest rate (see Martins and Villanueva, 2006), theoretically, lenders should have an incentive to discriminatively charge borrowers with origination balances just at the conforming loan limit higher interest rates than those charged to other borrowers. If there were no loan limit, lenders could increase profits by reducing interest rates and making those borrowers willing to borrow more if the price elasticity of demand at that point is not very low. In this case, the equilibrium loan amount for those borrowers if there were no loan amount limit is larger than the actual loan amount limit. Consequently, given the loan limit restriction, lenders would like to increase interest rates for those borrowers to the levels such that their willingness-to-borrow is equal to the loan limit, as long as the conforming mortgage option is still more attractive than the non-conforming mortgage option with flexible loan amounts but at even higher interest rates. Correspondingly, bunching of borrowers should be observed at the limit.====This study provides empirical evidence that the type of interest rate discrimination discussed above exists. Borrowers with origination balances at the loan limit on average pay higher interest rates due to this price discrimination.====In the first step of the empirical analysis, we provide evidence that the interest rates of borrowers with origination balances at the loan limit are significantly higher than those of borrowers with origination balances closely below the loan limit after controlling for a rich set of covariates. The study rules out the possibility that the reason for this fact is that borrowers with origination balances at the loan limit have higher default risks than borrowers closely below the loan limit.====In the second step, we employ the loan limit policy change in 2008 to conduct a difference-in-difference analysis. Before 2008, the FHFA set a national loan limit for all areas each year. The national loan limit increased from $93,750 in 1980 to $417,000 in 2006 and 2007. Due to the 2007 financial crisis, the Economic Stimulus Act of 2008 increased the loan limit for a set of high-cost areas from $417,000 to $729,750.==== This loan limit increase was an emergency measure to assure that mortgage credit was widely available during a time when private lending options were severely constrained. In 2009 and 2010, the loan limit in low-cost areas remained at $417,000, while the loan limit in high-cost areas was at $729,750.====In the difference-in-difference analysis, we compare the following two changes: the change in the interest rates of borrowers with origination balances of $417,000 from 2006 and 2007 to 2009 and 2010 in those high-cost areas for which the loan limit was increased and the change in the interest rates of borrowers with origination balances of $417,000 from 2006 and 2007 to 2009 and 2010 in other areas. The results indicate that the latter change is significantly lower after controlling for a rich set of borrower characteristics. Borrowers with origination balances of $417,000 in 2009 and 2010 were no longer at the loan limit in high-cost areas, which should mitigate price discrimination, while borrowers with origination balances of $417,000 in other areas were still at the limit and should not experience a change in price discrimination.====This study mainly provides two contributions to the literature. First, it provides empirical evidence that although the original intention of the conforming loan limit policy is to enhance the mortgage credit supply to low-to-moderate income households that would otherwise be underserved, the policy generates a side effect that lenders have opportunities to engage in price discrimination against borrowers bunching at the limit.====The second major contribution of this paper is that, leveraging the loan limit policy, the paper identifies lenders’ price discrimination based on borrowers’ demand or willingness to borrow in the residential mortgage industry, a situation in which borrower characteristics are related not only to demand elasticities but also to risks or lending costs.==== Previous studies on demand-based price discrimination mainly examined industries in which either the serving costs are the same across consumers with different price elasticities or the variables related to consumer price elasticities differ from cost-related variables====In the banking (or lending) industry (e.g., mortgages, auto loans, and credit cards), current regulations mainly focus on price discrimination against female or minority borrowers instead of price discrimination in the dimension of willingness to borrow or demand elasticity. The Fair Housing Act and Equal Credit Opportunity Act (ECOA) prohibits interest rate discrimination based on race, national origin, religion, or gender. The Home Mortgage Disclosure Act (HMDA) requires lenders to disclose their transaction data for investigations by regulatory agencies such as the Office of the Comptroller of the Currency (OCC), who have the power to take enforcement actions.====However, in many other industries, regulations mainly focus on price discrimination based on customers’ willingness to pay or demand elasticities because this type of price discrimination can reduce consumer surplus and social welfare. According to the Robinson-Patman Act, price discrimination is unlawful if it is not due to the different costs of dealing with different buyers. While the Federal Trade Commission (FTC) and the U.S. Department of Justice (DOJ) Antitrust Division are the two general regulatory agencies who enforce the federal law, there are also industry-specific regulators that regulate firms’ pricing behavior.====Given the evidence provided by this study that lenders discriminatively charge borrowers at the loan amount limit a higher interest rate, it is natural to suspect that lenders also engage in certain price discrimination on other borrowers based on their demand or willingness to pay. Therefore, it is worthwhile for regulators to allocate more attention to demand-based price discrimination in the banking (or lending) industry. Meanwhile, it is worthwhile for researchers with more detailed data (that can accurately estimate the lending costs for each borrower, including bank servicing costs, securitization costs, and potential foreclosure costs) to investigate potential demand-based price discrimination on borrowers other than those bunching at the limit.====The remaining portion of this paper is organized as follows. Section 2 reviews the relevant literature and discusses the contributions of our study. Section 3 provides an overview of the industrial background. Section 4 introduces the data used in this paper. Section 5 discusses the empirical strategies and baseline results. Section 6 discusses concerns on omitted-variable bias. Section 7 discusses other issues related to the results. Section 8 conducts a difference-in-difference analysis. Section 9 discusses the magnitude of the price discrimination. Then, we conclude in Section 10.",Per-customer quantity limit and price discrimination: Evidence from the U.S. residential mortgage market,https://www.sciencedirect.com/science/article/pii/S0167718720300102,11 February 2020,2020,Research Article,194.0
"Gu Yiquan,Wenzel Tobias","Management School, University of Liverpool, Chatham Street, Liverpool L69 7ZH, United Kingdom,Department of Economics, University of Sheffield, 9 Mappin Street, Sheffield S1 4DT, United Kingdom,Düsseldorf Institute for Competition Economics (DICE), Germany","Received 19 January 2018, Revised 7 January 2020, Accepted 16 January 2020, Available online 10 February 2020, Version of Record 6 March 2020.",https://doi.org/10.1016/j.ijindorg.2020.102582,Cited by (2),"This paper develops a market model where consumers refrain from buying products that they are unable to understand and a firm can influence the probability of a consumer understanding its offer. In equilibrium, firms artificially increase product complexity, and firms that offer more transparent products choose on average higher prices. We study two sets of public policies. We show that consumer side policies may have the unintended consequence of encouraging obfuscation while firm side policies are always effective in curbing obfuscation. Interestingly, a consumer side policy can even harm consumers when it protects consumers so much that it greatly increases the marginal effectiveness of obfuscation. Policies on both sides can either increase or decrease social welfare depending on the marginal effectiveness and the marginal cost of obfuscation. Our main insights hold in both asymmetric and symmetric obfuscation equilibria.","Consumers in modern retail financial markets are facing increasingly many complicated choices. Given the required levels of financial knowledge and product information for making good decisions, it is not surprising to see widespread mistakes in consumer decision making being documented in the recent literature. Campbell (2006) highlights a number of common mistakes including under-participation in the equity market, inadequate portfolio diversification, failure to re-mortgage, etc. Indeed, Agarwal et al. (2016) find that 59% of borrowers in their study refinance mortgages sub-optimally. Choi et al. (2010) and Duarte and Hastings (2012) find investors fail to minimize return adjusted fund management fees. Choi et al. (2011) find about 36% of the employees in their study of employer matched retirement plans under-contribute, leaving about 1.6% of their annual pay on the table. Other common mistakes, e.g., those observed in credit card and payday loan markets, were noted in Agarwal, Driscoll, Gabaix, Laibson, 2009, Agarwal et al., 2009. A natural question to ask here is: How could public policies help consumers make better decisions and achieve better outcomes?====A direct initiative is to empower consumers by improving their financial sophistication.==== However, it should be noted that consumers are not making financial decisions in a static environment. Recently, Hastings et al. (2017) evaluated the effectiveness of a government intervention in Mexico’s privatised social security system where providers’ reaction to the public policy played an important role. Indeed, the supply side is often blamed for making consumer decisions more difficult than necessary. For instance, financial institutions might shroud certain elements of their pricing strategies (Gabaix, Laibson, 2006, Campbell, 2006) or highlight irrelevant information (Choi et al., 2010). It has also been well documented that firms take advantage of consumers’ different information levels regarding price and product attributes leading to price dispersion for almost identical products (Christoffersen, Musto, 2002, Hortaçsu, Syverson, 2004).====To better understand the effects of public policies, one needs to take an explicit account of supply side obfuscation. There are naturally two sets of public policies: one that focuses on preparing consumers for better decision making and the other that regulates the conduct of supply side providers. For example, recognizing that greater regulation of the financial market is needed in the wake of 2008 financial crisis, the Dodd–Frank Wall Street Reform and Consumer Protection Act was signed into law in 2010 in the US. In particular, under title X, the Bureau of Consumer Financial Protection (CFPB) was established to “help consumer finance markets work by making rules more effective, by consistently and fairly enforcing those rules, and by empowering consumers to take more control over their economic lives.” This statement highlights the two sets of policies at the disposal of policy makers: firm side regulation and consumer empowering. The former may be interpreted as making it more difficult or more costly for firms to adopt obfuscation strategies while the latter as making consumers less easily confused by obfuscation strategies. With an annual budget of over $600 million in recent years, it is important to understand the commonalities and the differences of these two sets of policies.====In this paper, we analyse and contrast these two sets of policies from a regulatory point of view. To this aim, we develop a simple duopoly game of strategic obfuscation where in the first stage firms independently select the level of price complexity at a cost and in the second stage they compete in prices. A strengthening of firm side regulation corresponds to an increase in the cost of obfuscation while a consumer empowering policy reduces the effectiveness of obfuscation. We then investigate how changes in these two types of policies affect firms’ obfuscation choices and consumer welfare in equilibrium.====Our contribution is two-fold. First, we propose an alternative consumer decision rule within the standard obfuscation framework of Carlin (2009). Second, we provide a model that allows for comparing and contrasting the two sets of public polices and derive general results in this regard.====Specifically, differing from previous obfuscation models, a firm in the current paper can only confuse its own consumers.==== Intuitively, when a firm obfuscates more, it reduces its own chances of being understood and patronised. We assume that this firm’s obfuscation practice does not affect how consumers perceive competitors’ offers. We take the view that consumers are averse to obscurity. In particular, our consumers would not buy from a firm whose offer they do not understand. If they understand neither, they will resort to a transparent outside option. If there’s only one offer they understand, they buy it if the reservation price is not exceeded. They buy from the lower priced when they understand both offers. This mechanism is related to the one in the search model by Wilson (2010) which will be discussed in more detail in our literature survey.====Our consumer decision rule is primarily motivated by ambiguity/obscurity aversion and non-participation in the financial markets. Theoretically, Dow and Werlang (1992), Cao et al. (2005), and Easley and O’Hara (2009), among others, show that ambiguity aversion can explain under-participation in the equity market.==== Recent empirical studies such as Bossaerts et al. (2010) and Dimmock et al. (2016) lend support to this explanation. Our consumers who are confused by both offers will abstain from this particular market. For example, confused retail investors would rather leave money in their savings account instead of investing in a mutual fund with fees as well as terms and conditions they do not understand. More broadly, other explanations for under-participation such as cognitive ability (Grinblatt et al., 2011) or financial knowledge (van Rooij et al., 2011) are also consistent with our modelling strategy. In these cases, obfuscation can be interpreted as making financial decisions mentally more demanding or as inventing new financial jargon with the aim to reduce consumers’ ability in understanding their offers. A couple of recent papers (discussed in more detail in our literature survey) also adopt the idea that consumers prefer simple contracts (Papi, Bachi, Spiegler, 2018). Relatedly, consumers may prefer to visit firms with lower search costs as in Wilson (2010).====In this setup, obfuscation can only reduce demand. Then why do we still observe firms obfuscate? In a nutshell, given that the other firm is setting a low level of obfuscation, it is better off to obfuscate than not because the second stage price competition will be less fierce when the firm obfuscates. This is so because by obfuscation the firm makes more consumers understand only the other firm’s price, and hence the other firms will have more captive consumers, find its demand less price elastic and ultimately compete less aggressively in the pricing stage.==== As a result, the firm that offers a more transparent price usually has a higher price in equilibrium. This mechanism may explain the casual observation that simpler prices can be more costly for consumers than more complex ones such as all inclusive, no hidden charges, flat-rate contracts versus traditional ones in retail finance, holiday or telecom markets.====There are two types of obfuscation equilibria in this model. In the asymmetric equilibria, exactly one firm obfuscates and the other selects no obfuscation. There exists also a unique symmetric equilibrium in which both firms obfuscate according to a cumulative distribution function. In our view, the asymmetric equilibria are better predictions of the longer run outcome where certain firms have the reputation of choosing simple and transparent prices while others usually have more complex prices. On the other hand, the symmetric equilibrium in mixed strategies captures the outcome in young markets where firms are not yet able to coordinate on an asymmetric equilibrium. Given its relevance, we are interested in characterizing this equilibrium although it is technically much more involved.====Policywise we show, as the main difference between the two sets of polices, that consumer side policies may have the unintended consequence of encouraging obfuscation and even ==== consumer welfare while firm side policy is always effective in curbing obfuscation and promoting consumer surplus. On the other hand, both policies can either increase or decrease social welfare depending on the marginal effectiveness and the marginal cost of obfuscation. When the return on obfuscation is higher than the cost, stricter polices can decrease social welfare because of elevated total obfuscation costs - either through encouraged obfuscation in the case of consumer side protection or because obfuscation is more costly due to firm side regulation.====These main insights hold in both the asymmetric and symmetric obfuscation equilibria. However, consumption is less efficient in the symmetric obfuscation equilibrium because some consumers understand neither offer and opt for the low valued outside option. To obtain more concrete results, we investigate the situations where the marginal effects and marginal costs of obfuscation stay constant. We find consumer surplus is always higher in the asymmetric than in the symmetric equilibrium due to higher consumption efficiency. More interestingly, the scope for policy intervention is larger in the symmetric equilibrium case than in the asymmetric one. That is, a policy intervention is more likely to improve welfare in the symmetric equilibrium case than the asymmetric one.====Finally, in an extension to ==== ≥ 2 firms, we demonstrate that insights derived in the duopoly setup carry over to the oligopoly case. Moreover, we show that a larger number of firms is beneficial for consumers (even if there are more obfuscating firms), and more interestingly, policy interventions tend to be more effective with a larger number of firms. The latter points to a complementarity between traditional competition policy (promoting firm entry) and consumer protection policies. Both policies are more effective in combination than alone.",Curbing obfuscation: Empower consumers or regulate firms?,https://www.sciencedirect.com/science/article/pii/S0167718720300047,10 February 2020,2020,Research Article,195.0
Asuyama Yoko,"Waseda University, 1-6-1 Nishiwaseda, Shinjuku-ku, Tokyo 169-8050, Japan,Japan Society for the Promotion of Science, 5-3-1 Kojimachi, Chiyoda-ku, Tokyo 102-0083, Japan","Received 19 June 2019, Revised 3 January 2020, Accepted 30 January 2020, Available online 8 February 2020, Version of Record 20 February 2020.",https://doi.org/10.1016/j.ijindorg.2020.102586,Cited by (3),"The degree to which firms delegate authority to non-managerial and non-supervisory workers varies substantially across countries and industries. By examining worker-level data from 14 countries, I empirically explain this variation by region-specific social capital that proxies workers’ degree of self-centeredness and the industry-specific need for coordination. The empirical results provide the first confirmation of Alonso et al.'s (2008) theoretical predictions: In particular, when self-centeredness of workers is very low, the degree of delegation is always high regardless of the need for coordination. Consistent with the theory, as coordination becomes important, better horizontal communication among co-workers occurs to sustain the same delegation level.","Decentralization, or delegation of decision-making from higher to lower organizational levels, is beneficial for firms in many ways.==== Theoretically, these benefits include better adaptation to the business environment owing to more efficient use of local knowledge (Holmström, 1984; Dessein, 2002; Hart and Moore, 2005; Dessein and Santos, 2006; Alonso et al., 2008), speedier decision-making because of more efficient information processing (Radner, 1993; Bolton and Dewatripont, 1994), and an increase in job satisfaction/motivation and effort of workers to whom authority is delegated (Aghion and Tirole, 1997; Zábojník, 2002). Empirically, Fehr et al. (2013) have confirmed the last motivation-enhancing effect of delegation in a laboratory experiment. Appelbaum et al. (2000, Chapter 9) also find a positive association between delegation to non-managerial employees and employee job satisfaction.====Despite these benefits, firms do not always pursue decentralization, and the degree of delegation varies substantially across locations and industries. For example, Fig. 1 displays a significant variation across countries in the extent to which the average non-managerial and non-supervisory workers can choose or change their way of work. Regional variations are much greater: the corresponding discretion scores range from 1.72 in Altai Krai, Russia, to 4.05 in Bremen, Germany. A large variation across industries is also observed (Appendix Fig. A1).====What explains these variations in the actual degree of decentralization, which are assumed to be the result of firms’ optimal behavior? One important factor is that firms’ optimal degree of decentralization is determined not only by the benefits of delegation but also by its costs: decentralization is not pursued in countries/regions or industries in which the expected costs of delegation are relatively large. Theoretically, such costs mainly include (i) loss of control (Holmström, 1984; Aghion and Tirole, 1997; Dessein, 2002; Alonso et al., 2008) and (ii) difficulty in coordination (Bolton and Dewatripont, 1994; Hart and More, 2005; Dessein and Santos, 2006; Alonso et al., 2008). First, delegation results in a loss of control for the principal. As the interests of the principal (firm) and the agent (worker) increasingly misalign, the possibility of workers’ rent-seeking behaviors, which are harmful for firm profit, increases. Second, when coordinating worker behaviors is important for firm profit, decentralization is costly because adaptation to local conditions is prioritized over coordination under decentralization.====This paper focuses mainly on these two costs of decentralization as determinants of its implementation.==== First, the size of expected cost owing to rent-seeking behaviors is measured by region-specific social capital that proxies self-centeredness of workers.==== Second, the magnitude of cost owing to difficulty in coordination is measured by industry-specific coordination needs. Based on the theoretical framework by Alonso et al. (2008), I empirically examine the effect of these two cost measures on the degree of delegation to non-managerial and non-supervisory workers utilizing worker-level data from 14 countries. The following theory-consistent results are obtained: First, the degree of decentralization is lower when its costs, industry's needs for coordination, in particular, are higher. Second, the negative association between coordination needs and decentralization is mitigated in regions with lower self-centeredness of workers. Third, in regions where self-centeredness of workers is very low, the degree of delegation is always high, regardless of the industry's coordination needs. Similarly, in industries with very low coordination needs, the degree of decentralization is high in general, regardless of the level of self-centeredness of workers. As mentioned below, the second and third findings are empirically new. These results are robust to alternative indices of industry's coordination needs and self-centeredness of workers. The possibility of endogeneity bias is minimized because the region-specific and industry-specific cost measures used in this paper are likely to be exogenous to individual firms. In addition, the baseline results are still robust even when instrumenting the social capital variable. Finally, consistent with Alonso et al.'s (2008) model, I find that as coordination becomes important, better horizontal communication among co-workers occurs to sustain the same delegation level.====This study contributes to the literature in three main ways. First, to the author's knowledge, this study is the first to test the theoretical predictions of Alonso et al. (2008) empirically. Past empirical studies find a negative association between coordination needs and the degree of decentralization (Colombo and Delmastro, 2004; McElheran, 2014; Meagher and Wait, 2014; Dessein et al., 2019). However, Alonso et al. (2008) theoretically show that even when coordination is extremely important, decentralization is optimal if misalignment of interests between principal and agent are sufficiently small. Such interaction effects of coordination needs and the misalignment of interests are empirically examined in this paper for the first time.====Second, this study also contributes to the growing literature on the effect of culture (or social capital) on economic activities (Guiso et al., 2006, 2011; Durlauf and Fafchamps, 2005; Fernández, 2011; Alesina and Giuliano, 2015). In particular, this paper is closely related to Bloom et al. (2012), Meagher and Wait (2019), Appelbaum et al. (2000, Chapter 9), and Cingano and Pinotti (2016).====By examining firm data in the United States (US), Europe, and Asia, Bloom et al. (2012) empirically show that firms headquartered in high-trust regions delegate more authorities to plant managers and that higher levels of bilateral trust from the country of headquarter towards that of subsidiary increases decentralization. Meagher and Wait (2019) find that employees’ trust in management results in more delegated authority to them in Australia. Appelbaum et al. (2000, Chapter 9) also find a positive association between delegation and employees’ trust in managers in three manufacturing industries in the US. However, none of the three papers examine the effect of coordination needs or social capital measures other than trust. By contrast, to the author's knowledge, the current paper is the first to simultaneously test the effects of both coordination needs and social capital, including trust. This paper shows that the effect of social capital on decentralization depends on the importance of coordination. In fact, social capital does not matter for decentralization in general, when coordination needs are very low. The results in this paper also indicate that “trust” may be reinterpreted as a measure that represents workers’ low self-centeredness because alternative social capital variables (participation in volunteer activities and attitude toward wealth accumulation) are also found to be significant determinants of decentralization. These remarks also apply to Cingano and Pinotti (2016) who examine industry-level data across Italian regions and European countries. They find that higher regional- (or country-) level trust is associated with a larger share of value-added and export in delegation-intensive industries. However, they treat the industry's degree of decentralization as exogenous, while I assume and empirically show that the degree of decentralization is determined by social capital (e.g., trust) and coordination needs.====The third contribution of this study is its data characteristics. Most past empirical studies on decentralization focus on delegation to managers (Colombo and Delmastro, 2004; Bloom et al., 2012; Meagher and Wait, 2014; Graham et al., 2015; Dessein et al., 2019). By contrast, this study contributes to the recent growing literature that examines delegation to non-managerial employees (Foss and Laursen, 2005; Frenzen et al., 2010; Lo et al., 2016; Flores-Fillol et al., 2017; Hong et al., 2019; Meagher and Wait, 2019).==== The current paper also covers both manufacturing and service industries across 14 countries in Europe (including Russia and Eastern Europe) and Asia. By contrast, the coverage of previous literature is mostly limited, especially in terms of the country coverage, although a few exceptions exist; for example, Bloom et al. (2012) cover manufacturing firms in 12 countries in US, Europe, and Asia.====The rest of this paper is organized as follows: Section 2 briefly explains the model by Alonso et al. (2008) regarding firm decentralization/centralization choice. Section 3 presents the empirical strategy. Section 4 describes the data. The estimation results on the determinants of decentralization are reported in Sections 5 and 6. Section 7 concludes.",Delegation to workers across countries and industries: Interacting effects of social capital and coordination needs,https://www.sciencedirect.com/science/article/pii/S0167718720300084,8 February 2020,2020,Research Article,196.0
"Rösner Anja,Haucap Justus,Heimeshoff Ulrich","DICE, University of Düsseldorf, Universitätsstr. 1, 40225 Düsseldorf, Germany","Received 23 January 2020, Accepted 26 January 2020, Available online 31 January 2020, Version of Record 27 November 2020.",https://doi.org/10.1016/j.ijindorg.2020.102585,Cited by (4),We investigate the effect of an EU-wide consumer protection regulation on consumer trust as well as consumer behavior. The Unfair Commercial Practice Directive (UCPD) was implemented by EU member states between 2007 and 2010. We utilize data from the Special and Flash Eurobarometer for the years between 2006 and 2014 and experts’ evaluation on consumer protection levels before the introduction of the regulation. This rich data set allows us to apply a difference-in-difference estimator with multiple time periods. We find a significant relationship between the introduction of the UCPD and consumer trust and cross-border purchases for countries with a low consumer protection level before the introduction of the UCPD. The relationship increases over time and stays then relatively constant.,"In 2017, more than half of the 560 million consumers in the European Union shopped online, but only 13 percent of them shopped cross-border (Eurostat, 2018). Although digital technologies have the potential to reduce information costs, negative distance and border effects still exist in online business-to-consumer (B2C) cross-border trade. As e-commerce is a global phenomenon, it is connected with several issues such as language barriers, cultural differences or trust frictions (Blum, Goldfarb, 2006, Cowgill et al., 2013, Gomez-Herrera et al., 2014, McCallum, 1995). To support the development of an integrated European market – a digital single market – the European Commission has long engaged in extensive harmonization exercises. Moreover, consumer authorities argue for international standards regarding consumer protection and data security (Craswell, 1982, Pitofsky, 1977).====Consumer protection has gained more prominence in recent years. Many recent developments demonstrate the high relevance of consumer protection and regulation of e-commerce in the European Union. In 2018, the European Commission published a draft for a new guideline called “New Deal for Consumers” (European Commission, 2018).==== The increasing focus on e-commerce is not confined to Europe but also illustrated by initiatives of the World Trade Organization for a public-private dialogue on e-commerce in 2017 (World Trade Organization, 2017).====E-commerce has a significant impact on economic growth and trade (see e.g., Terzi, 2011). As information costs are reduced and distance becomes less important, markets expand in size and competition intensifies. While consumers unambiguously benefit from market expansion and more intensive competition, effects on sellers are more ambiguous: Although they benefit from market expansions (e.g., Grandon and Pearson, 2004), as they can reach out to more potential customers, they also face more intensive competition.====The EU single market policy seeks to eliminate barriers to cross-border flows of goods, services, capital and labor between the EU member states. E-commerce contributes to this and thus plays an important role in EU policy. Differences in consumer protection levels between EU memeber states have been regarded as a barrier to trade and cross-border shopping that impede market integration. Hence, to boost consumer confidence and to make it easier to trade across borders, the European Parliament and the European Council passed the Unfair Commercial Practice Directive (UCPD) as Directive 2005/29/EC (Council of European Union, 2005). It regulates unfair business practices in the European Union, as part of European consumer law, based on the principle of ==== harmonization. In order to remove internal market barriers and to increase legal certainty for both consumers and businesses, the UCPD was passed by the European Parliament and the European Council in 2005 and enacted into national law by member states from 2007 on. The aim was a European minimum standard for consumer protection at a specific level. Consumers’ uncertainty about different consumer protection standards was seen as a significant barrier to online cross-border shopping by final consumers. Hence, EU-wide protection of consumer rights is a key pillar in the EU’s consumer agenda.====Our paper now analyzes the UCPD’s effects on consumer trust and shopping behavior within the EU. More specifically, in terms of consumer attitudes we analyze the UCPD’s effects on consumer trust vis-à-vis retailers and services located in consumers’ home countries and on consumers trust vis-à-vis public enforcement authorities. By analyzing purchases consumer have made cross-border, i.e., from other EU member states, as well as purchases from their own country, we can compare consumers’ shopping behavior and how it is affected by the UCPD. As online shopping has gained more and more relevance in recent years and the main channel for cross-border purchases is online-shopping, we are focusing on consumers’ attitudes and shopping behavior towards online B2C purchases.====We use data from different sources: First, the Eurobarometer survey which contains information about consumer attitudes concerning trust as well as their behavior in terms of online shopping. Second, data from private consultancy Civic Consulting is used which includes different indicators, most importantly evaluations of consumer protection levels, provided by legal and consumer protection experts. As we expect different outcomes for different consumer protection levels, these evaluations allow us to build different groups of consumer protection level which are used for the empirical analysis.====Applying a multiple difference-in-difference (DiD) approach, we show that the UCPD has indeed a significant effect on (i) consumer trust and trust into public authorities as well as on (ii) cross-border purchases while homeshopping is not affected. The introduction of the UCPD increased consumer trust vis-à-vis retailers and services in their home country and trust vis-à-vis public authorities. Moreover, online purchases from other EU countries increased after the introduction of the UCPD. We show that the effect is increasing over time for both trust measures and relatively constant for cross-border purchases. Furthermore, the effects are estimated to be robust and not sensitive to our tests.====This paper is related to different strands in both the economics and legal literature. There are several studies that examine consumer trust in the digital age in general without any focus on the UCPD (Culnan, Armstrong, 1999, Doney and Cannon, 1997, Gefen, Straub, 2004, Hoffman et al., 1999, Jarvenpaa, Tractinsky, Vitale, 2000, Lee, Turban, 2001, Lim et al., 2006, McKnight, Choudhury, 2006, Palvia, 2009, Teo and Liu, 2007, Wright, Gutwirth, Friedewald, De Hert, Langheinrich, Moscibroda, 2009). Conditions under which consumer trust in online retailing increases are, to some extent, addressed by the UCPD. Of course, other relevant but non-regulatory factors exist that contribute to consumers’ trust in online retailers as Lim et al. (2006) have shown. Our study contributes to previous research by examining consumer trust and cross-border purchase after the introduction of minimum consumer protection regulations within the European Union. Previous studies have focused on the consumer-retailer relationship and how retailers may gain consumer trust. Our study analyzes the regulatory framework that may support consumer trust in retailers and services as well as cross-border purchase. We also contribute to the strand of regulation literature. To the best of our knowledge, the effects of the harmonization of consumer protection regulations in the European Union have not been empirically analyzed before. Hence, we are the first to investigate whether the UCPD did actually affect consumers’ attitudes and shopping behavior.====This paper is also related to legal studies that have examined the introduction of the UCPD. In contrast to our study, these papers have analyzed the UCPD from a purely legal perspective (Collins, 2005, Collins, 2010, Gomez, 2006, Schulte-Nölke, 2007, Velentzas et al., 2012, Wright, Gutwirth, Friedewald, De Hert, Langheinrich, Moscibroda, 2009). As most studies suggest, the UCPD may be a first step to full harmonization in terms of consumer protection and to contribute to the goal of a digital single market. Among others, especially Collins (2010) and Osuji (2011) state that the UCPD alone will not be sufficient for full harmonization. This is especially relevant concerning our results. We contribute to this strand of literature as we show that the UCPD had a significant treatment effect on consumers’ behavior although it does not achieve a full harmonization of consumer protection regulations in the EU. We leave it open whether full harmonization is necessary or preferred over the current UCPD.====As our study analyses the effects of the Unfair Commercial Practice Directive, we contribute to the broad economic literature on policy evaluation. Early policy evaluation studies were conducted by Ashenfelter (1978), Ashenfelter and Card (1985), Heckman and Robb Jr (1985), Angrist (1990), Angrist and Krueger (1991), Angrist et al. (1996), Card (1990), Card et al. (1994), Heckman (1990), Manski (1990). More recently, policy evaluation focuses on the examination of treatments as we do in our study (among others Angrist, Lavy, 1999, Angrist and Pischke, 2008, Athey and Imbens, 2017, Blundell, Dias, 2002, Donald and Lang, 2007)====. The growing literature on causal treatments in program evaluation often uses a difference-in-difference estimators with multiple treatments and multiple time periods. This method, developed by Athey and Imbens (2006) and refined by Imbens and Wooldridge (2009), is also used in this paper. With respect to consumer protection measures, the program evaluation literature is relatively small. In fact, most of the consumer protection measures implemented at the EU level are not subject to any systematic ex post evaluation. Hence, our paper contributes to the growing literature on evidence-based policy analysis. In particular, we contribute to the literature by analyzing whether the UCPD has achieved its own objective, which has been formulated by the European Commission as follows: “The objective of the new EU rules on unfair commercial practices from 2005 was to boost consumer confidence and make it easier for businesses, especially small and medium-sized enterprises, to trade across borders.”====Our paper is structured as follows: Section 2 describes the underlying economic problem that the Unfair Commercial Practices Directives addresses and our theoretical expectations about its introduction. In Section 3, data and the identification strategy are described. Results are discussed in Section 4, before Section 5 concludes.",The impact of consumer protection in the digital age: Evidence from the European Union,https://www.sciencedirect.com/science/article/pii/S0167718720300072,31 January 2020,2020,Research Article,197.0
Johnson Justin P.,"Johnson Graduate School of Management, Cornell University, United States","Received 15 March 2013, Revised 8 January 2020, Accepted 18 January 2020, Available online 31 January 2020, Version of Record 26 February 2020.",https://doi.org/10.1016/j.ijindorg.2020.102581,Cited by (28),"I analyze a model of dynamic competition between retail platforms in the presence of consumer lock-in. Two different revenue models are considered, one in which platforms set final retail prices and one in which the suppliers set final retail prices. Platforms have long-term (or strategic) pricing incentives but suppliers do not, which implies that the inter-temporal price path faced by consumers depends on the revenue model in place. When suppliers set prices instead of platforms, prices may be higher in early periods but lower in later periods, suggesting that appropriate antitrust enforcement ought to consider more than initial price changes when an ==== shifts to the agency model. Indeed, consumers may (but need not) prefer the agency model even when prices increase in initial periods. A potential downside of the agency model is that it may align the incentives of suppliers and platforms and thereby encourage platforms to lower the competitiveness of the supplier market, harming consumers; no such incentives exist under the wholesale model. I relate my results to events in the market for electronic books.","I consider a model of dynamic competition between retail platforms for products such as content when there is consumer lock-in to platforms. I investigate the inter-temporal path of prices and how this path depends on the revenue model that platforms use. In particular, I consider the consequences of retail platforms using as a revenue model either the “wholesale model” (in which the retail platforms set retail prices) or instead the “agency model” (in which suppliers set retail prices) in the pricing of products bought by consumers on the platforms. I also consider how the choice of revenue model influences the incentives of platforms to either increase or decrease competition among suppliers, decisions which in turn influence consumer choice and prices. Overall, my analysis allows me to assess the effect of the agency model and wholesale model on consumer welfare.====By definition, one important difference between the wholesale model and the agency model is that in the wholesale model it is the platforms (which I interchangeably refer to as retailers) that determine the final retail prices faced by consumers whereas in the agency model suppliers determine these prices. This means that the primary determinant of retail prices under the agency model is the differentiation between suppliers, whereas under the wholesale model both supplier and retailer (that is, platform) differentiation influence these prices. A second important difference between the two models is that in the wholesale model suppliers receive compensation in the form of per-unit wholesale prices paid by the platform whereas in the agency model suppliers and platforms split sales revenue according to pre-specified revenue sharing agreements.====A main goal of my analysis is to assess prices over time for a given sales model and how the choice of sales model influences consumer surplus through the impact on prices. I show that the agency model may lead to higher prices in initial periods but lower prices in future periods. Because of this tradeoff, it is shortsighted to conclude that consumers are harmed simply because prices increase following a move to the agency model. Indeed in some cases, considering the overall inter-temporal impact on prices, consumers may be better off under the agency model even when it raises prices in initial periods. An implication is that a correct regulatory perspective ought to consider not only the immediate impact on prices from a change in revenue models but also an assessment of longer term impacts, most particularly in situations where suppliers and retailers have different long-term pricing incentives.====The intuition for my results on price paths builds on the fact that platforms have strong strategic motivations to charge low prices in initial periods. The reason is simply that lock-in is assumed to exist at the platform level and so a higher early market share for a platform increases the future profits of that platform. These incentives do not exist when suppliers set prices because lock-in is assumed not to exist at the product level. Hence, suppliers have different long-term pricing incentives than platforms do.====Put slightly differently, the reason that prices may be lower under the agency model in future periods compared to the wholesale model is as follows. Future prices are low because the agency model ensures that robust competition exists directly between suppliers. In contrast, once consumers are locked in, because the wholesale model allows the platform to set the final prices to consumers, the platform internalizes competition between suppliers so as to more fully harvest locked-in consumers.====Although one possibility is that consumers prefer the agency model, it is also possible that some or all consumers prefer the wholesale model. Indeed, it is possible that the agency model will lead to price increases in all periods, thereby harming all consumers. Other possibilities also exist and are considered in my analysis. In all cases, key determinants include the structure of pricing contracts between suppliers and retailers as well as the differentiation between platforms compared to the differentiation between suppliers.====I also identify a different potential downside of the agency model, which is that it may lead to an alignment of incentives between suppliers and retailers that harms consumers. In particular, I extend the model to allow each retailer to make a decision that both increases the gross value that consumers place on using that retailer’s platform and which lowers the margins that suppliers are able to claim. For example, a retailer may be able to alter the number of suppliers on its platform or change consumer search costs on its platform. For instance, by increasing the number of competing products available on a platform, consumer surplus would increase for any fixed prices which would increase demand to use the platform. Additionally, such an increase may drive down the margins that suppliers claim.====Because retailers share in the profits generated by suppliers’ pricing decisions under the agency model, there may be an alignment of incentives between suppliers and retailers that encourages retailers to limit consumer choice and raise prices, for example by limiting the number of suppliers. In other words, this alignment encourages retailers to take steps to lower the competitiveness of the supplier market, potentially undoing any otherwise positive effects of the agency model on prices. Importantly, this potentially harmful alignment of incentives does not exist under the wholesale model. An implication is that a complete analysis of the consumer welfare effects of the agency model must also involve an assessment of the incentives of platforms to influence the supplier market, especially given that many online platforms have the tools to do so.====My analysis provides some possible insight into events in the electronic book (“e-book”) market and complements the analysis of Johnson (2017), where I provide an assessment of the agency model that does not consider dynamic competition or consumer lock-in but does consider retail price-parity (or “most-favored nation”) restrictions.====In the US, there was an antitrust case in which Apple and some book publishers (who adopted the agency model) were accused of conspiring to raise prices for e-books.==== More precisely, following the introduction of the iPad and industry adoption of the agency model for e-books in 2010, the prices of many e-books significantly increased, leading to global antitrust scrutiny. The EU pressured many industry players to abandon the agency model because of these price increases. In the e-book market, lock-in may exist because a consumer becomes accustomed to using, for example, Amazon’s e-book store or e-book reading app. In some cases, lock-in exists because hardware either ties consumers to or guides them towards particular e-book reading apps.====Here I describe some recent work on the agency model, starting with those that consider differences between the agency model and wholesale model. Hagiu and Wright (2015) consider a monopolist retailer choosing between the two sales models in a static setting where the sales model impacts a non-contractible decision by suppliers and there are spillover effects across products. Gaudin and White (2017) also consider a static monopolist retailer, but focus on the effect of the agency model on prices.==== Johnson (2017) introduces a static model that allows for competition at both the retailer and supplier level and considers a number of questions, including how the agency model impacts prices and the distribution of profits between suppliers and retailers. Loertscher and Niedermayer (2015) and Wang and Wright (2017) examine how the agency model can achieve better overall pricing for a platform selling many unrelated products that each possess different demand characteristics. The present paper differs from all of these papers because competition occurs over several periods and there is consumer lock-in. However, like some of these papers I am also interested in the effect of the agency model on prices (in fact, the intertemporal path of prices) and the consequence for consumer welfare.====Gans (2012) identifies a hold-up problem when app providers set prices after consumers invest in joining a new and more efficient platform. This problem may prevent consumers from adopting the new platform in equilibrium, but price-parity provisions can effectively impose a price cap on the app and solve the problem. These price-parity provisions (sometimes called most-favored-nation clauses) stipulate that, under the agency model, suppliers must charge the same price for their product through all platforms. In my model this holdup problem plays no role and the number of active platforms is taken as given.====A number of other papers also consider price-parity provisions but without focusing on the type of hold-up problem and new-platform launch that Gans (2012) does. These contributions include Edelman and Wright (2015), Boik and Corts (2016), Gaudin and White (2017), De los Santos and Wildenbeest (2017), Johansen and Vergé (2017), Johnson (2017), Loertscher and Niedermayer (2018), and Foros et al. (2013). Depending on the model considered, such contracts can have varied effects but the central question is whether such contracts have an anticompetitive effect, for example by raising prices or deterring entry. Here I do not consider such contracts.====My work is also related to the literature on strategic managerial delegation.==== As first emphasized by Schelling, 1956, Schelling, 1960, it may be beneficial to delegate decisions to another agent when so doing provides commitment power. Vickers (1985), Fershtman and Judd (1987), and Sklivas (1987) expand upon this point. In this paper, the agency model in effect delegates pricing to suppliers instead of retailers.====The remainder of my paper is structured as follows. Section 2 presents the main model, which is based on two periods. This model is meant to capture that there is an important initial period of time in a new market and to identify the role played by different sales models in influencing industry outcomes. Section 3 instead considers an infinite-horizon model of overlapping generations.",The agency and wholesale models in electronic content markets,https://www.sciencedirect.com/science/article/pii/S0167718720300035,31 January 2020,2020,Research Article,198.0
Dana James D.,"Department of Economics and D’Amore-McKim School of Business, Northeastern University, 360 Huntington Avenue, Boston, MA 02115, United States","Received 11 August 2019, Revised 26 December 2019, Accepted 31 December 2019, Available online 30 January 2020, Version of Record 20 February 2020.",https://doi.org/10.1016/j.ijindorg.2020.102579,Cited by (2),"When consumers have noisy information, bundling ==== can signal high quality in a simple static model. When consumers are partially informed, bundling has a bigger negative impact on the sales of a low-quality firm than on the sales of a firm with two high-quality products. Bundling can also signal high quality by restricting total sales even when consumers are uninformed. However, I argue this is not a likely explanation for bundling because raising prices is always a more profitable way to restrict total sales than bundling.","Product bundling can signal that a firm’s experience goods are high quality even in simple static settings. This paper contributes to an important literature on the potential efficiency effects of product bundling, and it gives a simple, theoretical model that formalizes an argument that is commonly used by defendants in antitrust cases. For example, the Hilti corporation, a leading producer of nail guns and supplies, defended its practice of bundling its guns with its nails by arguing that using a competitor’s nails would “give rise to uncertain fixing reliability and, consequently, safety risks in load bearing applications.”==== Similar examples exist from legal cases brought against firms in the automobile and computer industries.====This paper shows that bundling can act as a signal of high quality when consumers have noisy information about quality. Bundling acts as a signal because it reduces output more for low-quality types who bundle than for high-quality types who bundle. This is because when consumers observe one negative signal about product quality, then with bundling the consumer will not purchase either good, but absent bundling the consumer has an incentive to purchase the second product alone.====Bundling, like raising price, may also restrict output, so bundling can also signal that the firm sells a high-quality product even when consumers have no information about product quality. However, I show that raising price is always a more profitable way for the firm to restrict output. More specifically, bundling does not satisfy the intuitive criterion when consumers are uninformed, so I argue that bundling works as a signal of high-quality only when consumers have noisy information.====I make the simplifying assumption that consumers’ valuations for the two products sold by the firm are identical, though valuations do vary across consumers. This implies that bundling can be costless for a firm with high-quality products, but is costly for a firm with one or two low-quality products. It is easy to relax this assumption, in which case bundling will be costly for both types but more costly for the type with one or two low-quality products.====The paper is organized as follows. Section 2 discusses some of the related literature. Section 3 describes the model. Section 4 considers the one-product firm and shows that restricting output can signal high quality, but doing so with price always dominates the alternatives. Section 5 considers the two-product firm and product bundling. Section 6 considers potential extensions. Finally, Section 7 provides some concluding remarks and discussion.",Bundling can signal high quality,https://www.sciencedirect.com/science/article/pii/S0167718720300011,30 January 2020,2020,Research Article,199.0
Porter Robert H.,"Department of Economics, Northwestern University and NBER, United States","Received 13 January 2020, Accepted 15 January 2020, Available online 24 January 2020, Version of Record 27 November 2020.",https://doi.org/10.1016/j.ijindorg.2020.102583,Cited by (14), investigations.,"A common practice in merger evaluation is to follow a three step procedure in order to predict outcomes should the merger be approved. The first step involves the estimation of a differentiated products demand system. Second, one either (i) infers marginal cost from static Nash equilibrium pricing equations, given the estimated demand system and the pattern of ownership of the various products, or (ii) estimates the supply side assuming Nash equilibrium pricing. In the former case, it is often assumed that there are constant returns to scale. Marginal costs are therefore constant, although they may vary across firms. Marginal costs can then be recovered from a firm's first order condition for its prices, given estimates of own and cross price elasticities of demand. In the latter case, estimation of the supply side entails recovering estimates of the marginal cost function, in which marginal costs can be a function of output, factor prices or other cost shifters. The third step is to simulate the effect of the prospective merger on prices, given the new product ownership patterns that would result, maintaining the assumption of Nash equilibrium pricing. An early and influential example is Nevo's (2000) study of the ready-to-eat cereal industry. (See also Hausman et al. (1994) and Werden and Froeb (2008) for a discussion of related research). One can readily adapt such a procedure to allow firms to set quantities instead of prices, or to account in the third step for any cost changes that might result from the merger.====The analysis described above restricts attention to the unilateral effects of the merger, insofar as there is an assumption that pre- and post-merger outcomes are Nash equilibria of the respective one shot games.====However, many prospective mergers also raise concerns about coordinated effects, or collective dominance, whereby the merger may result in less competitive conduct than that predicted under static Nash equilibrium. One might also question whether pre-merger conduct is consistent with a static Nash equilibrium. Nevo (2001) addresses this latter issue in his study of the breakfast cereal market, by comparing estimates of marginal costs (as obtained in case (i) of the second step described above) to those implied by accounting data.====The Nash equilibrium assumption also matters in evaluating the post-merger incentives to alter product characteristics or the set of available products. A relatively recent literature considers the effects of mergers on product introductions and withdrawals, or product repositioning more generally, while maintaining a Nash equilibrium behavioral assumption. (See, for example, Fan (2013)).====The 2010 revision of the U.S. Horizontal Merger Guidelines discusses coordinated effects in Section 7. A few quotations bear repeating here. “A merger may diminish competition by enabling or encouraging post-merger coordinated interaction among firms in the relevant market that harms customers. Coordinated interaction involves conduct by multiple firms that is profitable for each of them only as a result of the accommodating reactions of the others.” As noted above, this behavior entails a departure from static Nash equilibrium pricing.====“Coordinated interaction includes a range of conduct. [It] can involve the explicit negotiation of a common understanding of how firms will compete or refrain from competing. Coordinated interaction also can involve a similar common understanding that is not explicitly negotiated but would be enforced by the detection and punishment of deviations that would undermine the coordinated interaction. Coordinated interaction includes conduct not otherwise condemned by the antitrust laws.” For the most part, I will not discuss the effects of mergers on explicit collusion, where coordination is achieved through an explicit negotiation process, which constitutes a ==== violation of antitrust laws. I instead focus on tacit arrangements, with occasional reference to lessons from explicit conspiracies.====The Horizontal Merger Guidelines go on: “The Agencies examine whether a merger is likely to change the manner in which market participants interact, inducing substantially more coordinated interaction. The Agencies seek to identify how a merger might significantly weaken competitive incentives through an increase in the strength, extent, or likelihood of coordinated conduct. There are, however, numerous forms of coordination, and the risk that a merger will induce adverse coordinated effects may not be susceptible to quantification or detailed proof. Therefore, the Agencies evaluate the risk of coordinated effects using measures of market concentration in conjunction with an assessment of whether a market is vulnerable to coordinated conduct.====“The Agencies are likely to challenge a merger if the following three conditions are all met: (1) the merger would significantly increase concentration and lead to a moderately or highly concentrated market; (2) that market shows signs of vulnerability to coordinated conduct; and (3) the Agencies have a credible basis on which to conclude that the merger may enhance that vulnerability.”====As indicated in the penultimate paragraph quoted above, the study of potential coordinated effects does not lend itself to a common quantitative procedure, in contrast with the three step procedure to study unilateral effects described above. In particular, the effect of a merger on the incidence or the degree of collusion is likely to be case specific, in large part because the theory of collusion does not offer a unique prediction. The purpose of this paper is to discuss the underlying theory of collusion and the implications of this theory for the effects of a merger. I also discuss empirical evidence of coordination and the effects of mergers on coordination, as well as some recent cases where coordinated effects concerns played an important role.====The remainder of the paper is organized as follows. The theory of collusion is described in Section 2. Much of the literature employs a repeated games framework, whereby cooperation is maintained by the threat of future retaliation in response to defections from the agreement. I discuss the effect of mergers on enforcement and participation constraints for the firms involved in coordinated interaction (respectively, their incentive compatibility and individual rationality constraints). The theory of collusion predicts that a number of outcomes may satisfy enforcement and participation constraints. I therefore also discuss the effect of mergers on outcomes (that is, on equilibrium selection), and the role of communication. In Section 3, I describe some recent empirical studies of the effect of mergers on the degree of coordination, and the underlying measurement issues. I discuss some recent merger cases in Section 4, including both ==== merger reviews and ==== investigations, in which coordinated effects played an important role in the merger review process. Section 5 contains some concluding remarks.",Mergers and coordinated effects,https://www.sciencedirect.com/science/article/pii/S0167718720300059,24 January 2020,2020,Research Article,200.0
"Lampert Hodaya,Wettstein David","Faculty of Industrial Engineering and Management, Technion, Israel Institute of Technology, Israel,Department of Economics, Ben-Gurion University of the Negev, Israel","Received 8 March 2018, Revised 3 January 2020, Accepted 6 January 2020, Available online 17 January 2020, Version of Record 5 February 2020.",https://doi.org/10.1016/j.ijindorg.2020.102580,Cited by (2),"We study sequential innovation in two pyramidal structures, in which ==== pools are socially desirable. Patent pools are stable, in a regular pyramid structure but not in an inverse pyramid structure under a standard pool formation process. We propose a more elaborate formation protocol that allows for the creation and stability of the largest possible pool. We also examine the welfare implications of introducing patent protection in both structures. Patent protection increases the likelihood of innovation as the pyramid becomes wider in the regular pyramid structure while patent protection always decreases the likelihood of innovation in the inverse pyramid structure.","The world’s first transparent transistor was developed by John Wager’s group at Oregon State University in 2003.==== They also developed the world’s first simple integrated circuit which was also transparent in 2005.==== These inventions in turn formed the “basis” for innovations in other diverse fields such as solar cell technology, display manufacturing and others.====The patent application for a semiconductor laser was filed by Robert N. Hall on October 24, 1962 while working at General Electric (see “Diode, 2012”). This invention has led to a variety of applications, such as, fiber-optic cables, DVD’s, scanning devices and others.====These examples are typical of an innovation pattern where each innovation is in turn followed by a spur of innovative activity across various industries. A different pattern of innovative activity involves several innovations coming together to produce a viable solution to a given problem. Consider, for example, the development of flash memory drives, where several innovations formed the basis for producing a viable solution for conveniently storing and accessing large amounts of data. The process began with the invention of the flash memory chip by Fujio Masuoka, leading a team of engineers. He applied for a patent for EEPROM (electrically erasable, programmable read-only memory) in 1981. In 1994, Ajay V. Bhatt, Bala Sudarshan Cadambi, Jeff Morriss, Shaun Knoll and Shelagh Callahan created the first USB (universal serial bus) port. These two inventions led to the first flash memory drives, also known as memory sticks (introduced by Sony in October 1998) and the disk on key (introduced by M-Systems in 1999).====Such examples can be found in most areas of sequential innovation and indeed are often associated with the formation of patent pools. Sequential innovation occurs when a current innovation depends on the successful realization of previous ones. Patent pools form when several patentees agree to jointly determine the price of a license to use their patents.====In what follows, we analyze sequential innovation and patent pools within the two innovation structures mentioned above, namely the ==== (the transparent transistor and the semiconductor laser cases) and ==== (the flash memory drives case) structures. In the former the number of innovators grows in each period, where a successful innovation gives rise to several additional ones in the subsequent period. In an inverse pyramid, the number of innovators in each period shrinks, leaving only one in the final stage. The innovators in each period solve more and more of the remaining problems thus, leading to a smaller set of necessary innovations in the next period. These two scenarios represent two extreme points of sequential innovation environments and their analysis is important for understanding the forces that might be at play in intermediate scenarios.====We model both scenarios as extensive-form games and analyze the resulting equilibrium outcomes. The analysis examines three institutional arrangements: patent protection but no pools; patent protection with pools and two different pool formation protocols; and finally no patent protection.====We find that the creation of a pool increases the probability of innovation and the innovators’ profits. In a regular pyramid, a “natural” offer-and-acceptance protocol leads to the formation of a patent pool, whereas a more elaborate protocol was called for in the case of an inverse pyramid.====Finally, in an inverse pyramid, the introduction of patent protection reduces the likelihood of innovation, whereas in a regular pyramid it might lead to higher innovation probabilities when the pyramid is not overly narrow.====A crucial aspect of the analysis is the innovator’s bargaining position (power). In a regular pyramid the earlier innovators have greater bargaining power, since they are more instrumental in subsequent innovations than those that follow them. In an inverse pyramid, in which each innovation is critical in achieving the final innovation, all innovators enjoy equal bargaining power. Later it is shown that these bargaining positions make it possible for a pool to form in a regular pyramid endogenously and render the endogenous formation of a patent pool highly unlikely in an inverse pyramid. In order to overcome this barrier for pool formation, we suggest a novel pool formation protocol with non-uniform sharing of bargaining power for the inverse pyramid.====Anecdotal evidence regarding the endogenous formation of a pool supports the findings. For example, in the field of wireless technology, in which patents follow a regular pyramid structure, the LTE (Long-Term Evolution) patent pool formed endogenously and includes several patents for advanced communication technology. These patents are then used across several subsequent domains of innovation, such as enhanced voice communications, interactive video and television, gaming and social media.====The efforts to find an effective cure for AIDS exhibited the inverse pyramid structure, which is characteristic of medical innovations. Partial solutions for different stages and symptoms of the disease were found. These efforts culminated in the AIDS cocktail that combines several drugs, each addressing a specific concern, such as revitalizing immunologic functions, suppressing viral load and improving quality of life. A patent pool consisting of these innovations, while highly desirable from a welfare point of view, failed to form endogenously. In 2010, a Medicines Patent Pool was formed with the backing of the United Nations and funded to a large degree by public donations. It led to increased access to innovation in HIV, hepatitis C and tuberculosis treatments and promoted further innovation.",Patents and pools in pyramidal innovation structures,https://www.sciencedirect.com/science/article/pii/S0167718720300023,17 January 2020,2020,Research Article,201.0
"Pavan Giulia,Pozzi Andrea,Rovigatti Gabriele","Compass Lexecon, Spain,EIEF and CEPR, Italy,Bank of Italy, Italy","Received 19 March 2019, Revised 9 December 2019, Accepted 13 December 2019, Available online 7 January 2020, Version of Record 24 January 2020.",https://doi.org/10.1016/j.ijindorg.2019.102566,Cited by (3),"We provide novel evidence on the effect of the threat of potential competition on the timing of entry in a new and growing ====. Exploiting a change in regulation in the Italian retail fuel market that generates exogenous variation in the number of potential entrants in the emerging Compressed Natural Gas segment, we show that markets with a higher number of potential entrants witness speedier entry decisions. We document that this result is likely driven by an increase in the incentives to preempt the market due to heightened risk of being anticipated by competitors.","In this paper, we document the impact of potential competition on the incentives to preempt entry in a new and growing industry. The decision on when to enter a market is one of the most critical faced by managers given that the order of entry has important implications for post-entry market share (Lieberman, Montgomery, 1988, Gandal, 2001, Bronnenberg, Dhar, Dube, 2009) and survival probability (Kalyanaram et al., 1995). Understanding the determinants of the timing of entry also bears relevant implications for policy seeing as preemption races can cause entry to occur earlier than socially optimal (Fudenberg and Tirole, 1985) and lead to inefficiencies in the order of entry (Argenziano and Schmidt-Dengler, 2012).====We examine the effect of competition on entry preemption exploiting a novel dataset that documents the early years of the compressed natural gas (henceforth, CNG) retail fuel industry in Italy. CNG is a car fuel alternative to gasoline and diesel that can power cars designed or retrofitted to run on it. Italian legislation originally prevented filling stations selling gasoline and diesel from offering CNG, due to safety concerns. It also forbade selling CNG in establishments located near populated area or major roads. For years, this confined the market for CNG to a tiny niche served by monofuel stations placed in hard to reach locations.====The lifting of the restrictions to CNG distribution in the late 1990s\early 2000s brightened the prospects for the sales of retail CNG, confronting potential entrants with a major strategic choice: that of investing into the infrastructure to sell CNG to retail customers. On the one hand, although demand for CNG was expected to grow after the reform, it was initially weak and the fixed entry costs were large. Firms had then reasons to delay entry. On the other hand, most markets could plausibly sustain a number of CNG-serving stations well short of the number of potential entrants. Hence, being beaten to the market may mean losing out on future persistent market power rents. The latter consideration pushes in the direction of preempting competitors’ entry by rushing to the market.====The trade-off between “acting” and “waiting” is well studied in the theory literature (Dixit, Shapiro, 1986, Levin and Peck, 2003, Rasmusen, Yoon, 2012) and applies not only to entry into geographical markets but also to product introduction (Greenstein and Rysman, 2006) and standard adoption (Dranove, Gandal, 2003, Kretschmer, 2008).==== The evidence we provide from the Italian CNG market points to market structure having a significant impact on the timing of entry.====Establishing a relationship between the number of potential entrants in a market and the strength of the preemption incentives presents several challenges. To begin with, theory is of little guidance. In the presence of preemption motives, the first entry occurs earlier when there are two potential entrants than if a single firm could enter, causing inefficiencies (Fudenberg and Tirole, 1985). Although intuition may suggest that increasing the number of competitors beyond two should lead to even more hurried entry and further efficiency losses, theoretical predictions in this sense are ambiguous (Shen, Villas-Boas, 2010, Argenziano, Schmidt-Dengler, 2014). Under certain conditions, the presence of an additional competitor may even delay entry (Argenziano and Schmidt-Dengler, 2013).====Shedding light on the issue has proven equally hard on the empirical front. The estimation of dynamic entry games is computationally intensive, forcing much of the extant literature (Schmidt-Dengler, Igami, Yang, 2016) to analyze only duopolies or oligopolies with a limited number of players. Further, preemption incentives should respond to the number of potential entrants which, unlike actual entrants, are not typically observed. Finally, variation in the number of potential entrants across markets is, in general, not exogenous.====The institutional context of the Italian fuel retail market delivers two key advantages. First, it hands us a way to identify potential entrants in the emerging CNG market. In fact, the opening of new filling stations for traditional fuels remained heavily regulated until well after the constraints on the distribution of CNG were relaxed. Furthermore, the cost of adding a CNG pump to an existing station is much lower than that of building a new station. Both of these factors contribute to make greenfield entry into CNG unappealing and designate existing filling stations already selling traditional fuels as the obvious candidates to enter the CNG market. Second, since the change in the regulation of CNG distribution was unanticipated, expectations on CNG profitability in the area should not have influenced the location decisions of the filling stations that entered before the law was changed. Hence, as long as the correlation between demand for gasoline and CNG conditional on market observables is not high, we can think of the number of potential entrants in a location as exogenous.====Fig. 1 documents the evolution of CNG retail supply in the market of Bergamo, one of the largest in Northern Italy. Although Bergamo is a large market -with many more CNG entrants than in the median market in our data- it provides a good illustration of the pattern we described. In 2005, shortly after the legislation change, there were very few stations offering CNG, marked by yellow triangles. These establishments were all monofuel and their distance from main roads (inversely proportional to the size of the markers on the map) was significant. By 2015, the situation has dramatically changed. First, a number of new stations offering CNG (marked by red circles) have entered, raising the total number of establishments distributing natural gas by a factor of four. This confirms that market expansion has been large and quick after the change in legislation. All the new entrants are multifuel stations, meaning that they sell natural gas along with gasoline and diesel. In all cases in which we have information on entry mode, the new entrants in the CNG segment were pre-existing fuel stations which added a CNG pump.==== Finally, the location of the new entrants is different from that of the old monofuel supplier: they operate in areas closer to major roads.====We identify the effect of competition on preemption by estimating a market-level Cox model for the hazard of observing entry in the CNG market by a filling station in a geographical market. We make the hazard dependent on the number of potential entrants in the market. Our main findings is that, controlling for a number of market characteristics, entry occurs significantly faster in areas with a higher number of potential entrants. Moving a market from the bottom to the top tercile of the distribution of the number of potential entrants raises by ten times the hazard that a filling station chooses to enter the CNG market in that area.====We rely on variation created by a natural experiment to document a shift in the incentive to preempt. This approach has the merit of avoiding the computational burden and some of the most restrictive assumptions needed to identify and estimate full fledged dynamic games. In particular, we can entertain cases where the preemption race involves dozens of potential entrants, which are common in retail markets but would be hard to handle in the context of a structural model. However, pinpointing preemptive conduct without the help of an explicit model of firm behavior presents challenges: the empirical outcomes of dynamic strategies could often be just as well rationalized in the context of a static framework. In our case, the positive correlation we report between potential competition and rate of entry could arise even in absence of preemptive behavior simply because a larger number of potential entrants implies mechanically a higher chance that one of them may find the market condition statically profitable.====To tackle this issue, we offer several independent pieces of evidence suggesting that the relationship between the timing of entry and the number of potential entrants does descends from a shift in the incentives to preempt. First, using estimates from our baseline duration model we simulate survival probabilities under the assumption that there is no strategic component to the entry decision. Hence, the only reason why the odds of observing faster entry in market with more potential entrants would be higher is mechanical. This model captures only a fraction of the shift in the speed of entry induced by changes in market structure meaning that the strategic preemption motive has explanatory power. Second, a calibration of stations’ profits performed using data from industry sources indicates that early entry could not yield positive static profits in most of the markets in our sample but that several of them became profitable later on. Furthermore, we show that at the time when the investment in the distribution of CNG is made variable profits would have grown more if the station manager had invested in expanding the supply of traditional fuels. The early entry we observe in the data can, therefore, be rationalized as a preemption investment that leads to suffer initial losses but to obtain deferred gains. Third, we perform a test à la Ellison and Ellison (2011) verifying that the effect of potential competition on the speed of entry is much smaller in markets where the gains from preemption should be lower because of higher anticipated post-entry competition. Finally, we exploit exogenous shifts to the probability that a station could be beaten to the market by one of its potential competitors and show that establishments facing higher risk of being hedged out tend to enter the CNG market earlier. This means that not only the number but also the composition of the competition faced by a potential entrants matters, which would not be the case if the relationship between timing of entry and market structural were entirely mechanic.====This paper adds to the stream of contributions documenting how the incentives to enter early are shaped by firm (Scott Morton, 1999, Franco, Sarkar, Agarwal, Echambadi, 2009, Cookson, 2018) and market characteristics (Koski, Kretschmer, 2005, Claussen, Essling, Peukert, 2018). Our focus on market structure links us to Toivanen and Waterson (2005). Whereas they study how market structure affects the decision to enter, we provide the first direct evidence of its impact on the timing of entry. We also complement structural studies of preemption games (Schmidt-Dengler, Gil, Houde, Takahashi). Whereas estimating a structural model of preemption would have given us the chance to perform policy experiments, it would have involved a computational burden that would have made it hard to handle a large number of players, which is the typical situation of the industry we study and of many other retail markets. Finally, since exploiting a regulation change we can identify the potential entrants in the CNG market, our analysis also relates to the scant literature providing evidence on the effect of the threat of entry on firms’ strategies (Goolsbee, Syverson, 2008, Seamans, 2012).====The rest of the paper is structured as follows. In Section 2 we describe some institutional details of the retail CNG fuel market and present the dataset we constructed to study it. In Section 3, we estimate the effect of competition on the speed of entry in the CNG market and in Section 4 we document that our main result derives from an increase in the intensity of preemption in markets with more potential entrants. Section 5 concludes.",Strategic entry and potential competition: Evidence from compressed gas fuel retail,https://www.sciencedirect.com/science/article/pii/S0167718719300943,7 January 2020,2020,Research Article,202.0
"Cazaubiel Arthur,Cure Morgane,Johansen Bjørn Olav,Vergé Thibaud","CREST, ENSAE Paris, Institut Polytechnique de Paris, France,Department of Economics, University of Bergen, Norway","Received 31 July 2018, Revised 28 August 2019, Accepted 27 December 2019, Available online 7 January 2020, Version of Record 31 January 2020.",https://doi.org/10.1016/j.ijindorg.2019.102577,Cited by (10),"Using an exhaustive database of bookings in one large chain of hotels active in Oslo (2013-2016), we estimate a nested-logit demand model that allows us to evaluate substitution patterns between online distribution channels. Making use of the chain’s decision to delist from Expedia’s platform, we compare the simulated and actual effects of such an event on prices and market shares and identify ways to improve on simulated counterfactual outcomes.","Retail e-commerce sales have been rapidly growing over the last 20–25 years. According to Statista, online sales will reach 2.8 trillion US dollars worldwide in 2018, having almost doubled in the last three years. In some markets such as music, books or travel, a large majority of sales are now made online rather than offline. Even groceries are now more commonly bought online.====The rapid growth of online retailing has led economists and competition agencies to look at the importance and impact of multi-channel distribution, and at the degree of substitution between online and offline sales.==== Among others, Gentzkow (2007) and Pozzi (2013) analyze the cannibalization effects of online distribution on offline sales. Gentzkow (2007) shows that the introduction of a digital version of the ==== reduced sales of the print edition. Pozzi (2013) concludes that the introduction of an online shopping service by a large US grocery retailer had a limited cannibalization effect on brick-and-mortar sales while increasing total revenues. Another important question has been to identify whether online retailing has led consumers to benefit from increased competition, i.e., to focus on across-firm substitution (see for example Prince (2007); Duch-Brown et al. (2017) and Ellison and Fisher Ellison (2018)).====Substitution between online and offline distribution is also an important issue for competition authorities. In merger control, delineating product markets is essential to assess the competitive impact of mergers and this now frequently involves identifying whether online sales should be part of the same relevant market as offline sales.==== The role of online sales and the interaction between brick-and-mortar, click-and-mortar, and pure online players has also been a major issue when revising the European rules applicable to vertical agreements.==== Many cases involving restraints related to online sales have been evaluated by competition agencies in the last decade: restriction of online sales in selective distribution networks [e.g., Pierre Fabre (France, 2007 and CJEU, 2011)], dual pricing or resale price maintenance [e.g., BSH (Germany, 2013) and United Navigation (UK, 2015)], exclusive territories or geo-blocking [e.g., Sector inquiry into e-commerce (European Commission, 2016)].====More recently, the policy debate has shifted to the impact of specific types of vertical restraints in online retailing, restraints usually related to the role of third-party platforms. Recent cases have involved restrictions imposed by manufacturers on online retailers with respect to the use of third-party platforms [e.g., Coty (Germany, 2014 and CJUE, 2017) or Adidas and Asics (Germany, 2014)], and by platforms on suppliers with respect to pricing, such as price parity (or MFN) clauses [e.g., eBooks (European Commission, 2017), Amazon (UK and Germany, 2013)].====Throughout Europe, platform price parity clauses have been the subject of several investigations in the market for online booking platforms/online travel agencies (OTAs). Such price parity clauses imposed by a platform to suppliers constrain the supplier’s ability to freely set prices on different distribution channels. A wide price parity clause covers all potential channels, that is, the clause prevents a supplier from selling a product at a price lower than the price charged on the platform imposing it (and this applies anywhere else including on the supplier’s own website). When all platforms used by a supplier impose wide price parity clauses, the supplier has to set the same price everywhere (it may only sell at a higher price on its own website). By contrast, a narrow price clause only constrains the price set for the supplier’s direct sales: the supplier can freely set prices on different platforms, but it cannot sell on its own website at a lower price than the price set on the platform imposing the constraint. Price parity clauses thus limit the supplier’s ability to set low prices for direct sales. In addition, when the clauses are wide, they may also lead to uniform prices on all platforms. Competition authorities in Europe consider that wide price parity clauses reduce incentives for platforms to compete on commission rates because they cannot expect suppliers to lower prices on cheaper platforms.====In Germany, the Bundeskartellamt prohibited price parity clauses imposed by HRS (December 2013) and Booking (December 2015). In April 2015, the French, Italian and Swedish competition agencies simultaneously accepted commitments offered by Booking to remove any availability requirements from their contracts and to switch from wide to narrow price parity clauses.==== Although it did not formally offer commitments to competition agencies, Expedia announced similar changes to its contracts throughout Europe.====Market definition has been an important part of the debate, with agencies ultimately concluding that the hotels’ direct sales do not belong to the same market as sales made through OTAs. Authorities have indeed taken the view that OTAs offer a bundle of services that includes search and comparison as well as the possibility to book online, whereas hotels’ websites only offer the opportunity to book. They also concluded that hotels view OTAs more as a complement than as a substitute to their own direct sales.====The issue of substitution between online channels also has important theoretical implications when considering the effects of price parity clauses. Boik and Corts (2016) (in a context with a monopolist supplier) and Johnson (2017) (with competing suppliers) both show that when suppliers sell through competing platforms, price parity clauses lead to higher commissions and thus higher final prices.==== However, their results rely on the assumptions that the platform commissions either are linear tariffs (i.e., a fixed price per sale) or based on revenue-sharing. Once these assumptions are relaxed, the effects of price parity clauses may well be different. For example, Rey and Vergé (2016) show that with non-linear commissions, price parity clauses do not affect final prices, but only affect the division of profits. Johansen and Vergé (2017) consider linear commissions but assume that suppliers can also reach final consumers directly. In such a setting, price parity clauses have an ambiguous effect on commissions, final prices, and suppliers’ profits. In particular, when inter-brand competition (i.e., competition between suppliers) is sufficiently fierce, price parity clauses may well lead to lower commissions and prices, while simultaneously increasing suppliers’ and platforms’ profits. However, their result relies on the assumption that it is a viable option for a supplier to delist from one of the platforms. This requires that, when delisting from a platform, a sufficiently large share of the lost sales are indeed recaptured through the direct channel and not exclusively through the rival platforms.====In this paper, we use an exhaustive database of bookings in 13 Oslo hotels (all belonging to the same chain) to evaluate the degree of substitution between online distribution channels, including the two largest OTAs (Booking and Expedia) and the chain’s own online distribution channel. We can then try to check whether selling directly constitutes a credible alternative to selling through OTAs. Contrary to recent papers that have focused on the effects of price parity clauses in this industry by using scrapped price data from metasearch engines (see, e.g., Hunold et al. (2018); Mantovani et al. (2017) and Larrieu (2019b)), we use a large dataset of actual bookings to estimate a nested logit demand model that allows us to evaluate substitution patterns between online distribution channels. Our results suggest that, while a substantial share of consumers seem to be loyal to the OTAs, and would switch to the other hotels (i.e., our “outside good”) in case of the hotel chain’s decision to delist from a platform (or after a substantial price increase by the hotel chain on the same platform), the chain’s direct sales channel remains a reasonably credible alternative to the OTAs. Still, among the consumers that would continue to book a room at the same hotel (after the hotel’s decision to delist from one of the OTAs), only a minority (about one in four) would book directly from the hotel rather than from the competing OTA.====We then use the demand estimates to uncover the hotels’ marginal costs through a structural model of price competition with differentiated products. We thus solve the system of first-order conditions, in a Bertrand-Nash model where hotels compete in prices, each hotel setting prices for each channel it uses: we thus consider an agency model where hotels keep control of the final prices and pay commissions to OTAs that they use as service providers. We can then use these marginal cost and demand estimates to run counterfactual simulations. In particular, we simulate the effects of a common decision by the 13 hotels to stop using on of the distribution channels (e.g., delisting from Expedia’s platforms). Making use of the actual chain’s decision to delist from Expedia, we can compare simulated and actual effects of such an event on prices and market shares. In that sense, we try to contribute to the debate on the effectiveness of structural IO models initiated by Peters (2006); Angrist and Pischke (2010) and Nevo and Whinston (2010).==== Comparing the simulated and observed outcomes, we observe discrepancies in terms of prices and market shares. Following Peters (2006), we thus try to identify sources for these differences and see how to improve the counterfactual simulation. Accounting for changes in the product characteristics changes the simulated outcome and provides results that are comparable to the effects of the actual delisting decision.====The rest of the paper is organized as follows. After presenting our dataset and the specific context in which the 13 hotels operated during the sample period (Section 2), we proceed to the estimation of our nested logit demand model and derive substitution patterns between online distribution channels (Section 3). We then use the estimated demand parameters and a structural pricing model to obtain per-channel marginal costs (Section 4). We then perform a counterfactual analysis and compute equilibrium prices and market shares assuming that all hotels decide to stop selling through one channel. Taking advantage of the hotels’ decision to delist from Expedia in 2013, we then compare the simulated outcome to the observed data (Section 5). Section 6 concludes.",Substitution between online distribution channels: Evidence from the Oslo hotel market,https://www.sciencedirect.com/science/article/pii/S0167718719301055,7 January 2020,2020,Research Article,203.0
"Fumagalli Chiara,Motta Massimo","Department of Economics, Università Bocconi, Italy,CSEF, Italy,CEPR, UK,ICREA-Universitat Pompeu Fabra and Barcelona Graduate School of Economics, Spain","Received 16 December 2019, Accepted 24 December 2019, Available online 3 January 2020, Version of Record 27 November 2020.",https://doi.org/10.1016/j.ijindorg.2019.102567,Cited by (1),"We show that the incentive to engage in exclusionary tying (of two complementary products) may arise even when tying cannot be used as a defensive strategy to protect the incumbent’s dominant position in the primary market. By engaging in tying, an incumbent firm sacrifices current profits but can exclude a more efficient rival from a complementary market by depriving it of the critical scale it needs to be successful. In turn, exclusion in the complementary market allows the incumbent to be in a favorable position when a more efficient rival will enter the primary market, and to appropriate some of the rival’s efficiency rents. The paper also shows that tying is a more profitable exclusionary strategy than pure bundling, and that exclusion is the less likely the higher the proportion of consumers who multi-home.","It is well known that, when market structure evolves over time, tying can have an anti-competitive effect.==== In particular, consider an incumbent firm that is monopolist in one market (that we denote as the “primary” market ====) and faces actual or potential competition in the market for a complementary product (the “secondary” market, ====). Consumers need both products in order to enjoy utility. If only the secondary market is under the threat of entry, while the primary market is a safe monopoly, the incumbent has no incentive to use tying to exclude more efficient rivals from the secondary market: it is more profitable to accommodate entry and use the price of the primary product to extract (some of) the rival’s efficiency rents. Hence the incumbent would have the ability but not the incentive to leverage its dominance to the secondary market by means of tying. However, if current entry (or expansion of the rival) in the secondary market paves the way for future entry in the primary market, the incentive to engage in exclusionary tying arises. As shown by Carlton and Waldman (2002), tying represents a defensive strategy that enables the incumbent to protect its dominant position in the primary market.====In this paper we extend Carlton and Waldman (2002) and show that the incentive to engage in exclusionary tying still arises even when the incumbent’s dominant position in the primary market cannot be protected from future entry in ====. To see why, suppose there exist scale economies on the supply side in the secondary market. Tying limits significantly the entrant’s sales of the secondary product in the initial period, when the incumbent’s position in the primary market remains unchallenged. If scale economies are sufficiently important, this denies the entrant profits that are crucial to cover its entry cost, and discourages its entry in the secondary market altogether. As a consequence the incumbent will be in a favorable position in the future, when entry in the primary market will occur (irrespective of lack of entry in the secondary market): being the unique producer of the secondary product, the incumbent will not only benefit from market power in the secondary market, but it will also appropriate some of the rival’s efficiency rents in the primary market. Therefore, it will earn higher profits relative to the counterfactual in which there is no tying and it faces competition both in the primary and secondary market in the future period.====Of course, this exclusionary strategy comes at the cost of sacrificing profits in the initial period, when tying prevents the incumbent from extracting efficiency rents from the rival in the secondary market. Tying turns out to be a profitable exclusionary practice if the rival’s advantage (and hence its efficiency rents) in the primary market is large enough.====The same insight also applies to the extension developed in Carlton and Waldman (2002), in which tying does not aim to protect the incumbent’s dominant position in the primary market (because that position is assumed to be safe) but to monopolise a newly emerging market complementary to the secondary one i.e., in their terminology, to “swing” its monopoly from one market to another over time. Indeed, our paper shows that the incentive to engage in tying exists even when the new market cannot be monopolised because a more efficient competitor will enter for sure: by excluding the rival in the secondary market, tying allows the incumbent to be in the position to appropriate some of the efficiency rents that entry in the new market will create.====A policy implication of this result is that entry in the secondary market needs not be a pre-condition for future entry in the primary market (or in a newly emerging market) to have an anti-competitive rationale for tying. Indeed, it would be sufficient that future entry in the primary (or new) market is reasonably ====, irrespective of whether it would occur anyway or it depends on successful entry in the secondary market.====Our paper also shows that integration of the entrants (i.e. the entrants belonging to the same group/firm) limits the scope for inefficient exclusion. In our model lack of entry in the secondary market harms period-2 consumers (who will have to pay a higher price for the monopolised secondary product) and the new competitor in the primary market (whose profits will be squeezed by the incumbent’s control of the secondary product). If those agents could take part in period-1 contracting, tying would not manage to exclude: they would be willing to compensate the entrant in the secondary market to ensure that it manages to cover the entry cost so as to benefit from lower prices and higher profits in the second period.==== Integration of the entrants makes it harder for the incumbent to exclude even when payments across time are not feasible, because the integrated firm internalises the beneficial effect that entry in the secondary market exerts on the profits of the future entrant in the primary market.====The case in which future entry in the primary market cannot be deterred allows us to highlight the difference between tying and pure bundling. Pure bundling refers to the case in which a firm only offers the bundle as a package and sells none of the products on a stand-alone basis. Tying, instead, refers to the case in which the sale of one product (the tying product) is conditional upon the purchaser also buying some other product (the tied product), but the tied product is also available as stand-alone product.==== In the existing literature on exclusion whether the dominant firm engages in tying or pure bundling does not make a difference, with the exception of Choi and Stefanadis (2006). Instead, it does make a difference in our model, where the incumbent always prefers tying over bundling because of the higher flexibility ensured by tying. More precisely, tying and bundling are equivalent in period 1, when the incumbent’s monopoly position in the primary market is unchallenged: in both cases consumers cannot obtain the primary product on a stand-alone basis, and thus the entrant does not manage to sell its secondary product (or its sales are substantially limited). However, tying and bundling are not equivalent in period 2, when there is scope for entry in both markets: first, under bundling the incumbent cannot use the price of the secondary product to compete with the rivals. Then, ==== makes higher profits in period 2 if it enters the secondary market and this makes exclusion harder to achieve. Second, when entry in the secondary market is discouraged, the fact that under tying product ==== is available on a stand-alone basis enables ==== to sell its higher quality product, thereby allowing the incumbent to exploit its monopoly position in the secondary market to appropriate part of ====’s efficiency rents. Under bundling, instead, ==== does not manage to sell its primary product in the absence of entry in the secondary market, and the incumbent loses the possibility to extract efficiency rents in period 2. Overall, bundling is not only less capable than tying of excluding the more efficient rival from the secondary market, but also less profitable when it manages to exclude.====Finally, another contribution of this paper is that we allow for the possibility that some consumers are ====, i.e. they can add another secondary product to the system composed of the incumbent’s primary and secondary products. We show that the anti-competitive concern of tying (or bundling) is the more severe the lower the share of multi-homers. We believe this helps shed light on an issue which is central in high-profile antitrust cases such as ==== and the recent European Commission’s ====. In Microsoft, the Department of Justice argued that although consumers could have downloaded other browsers, they would not do so. Similarly, the European Commission is aware that people may easily download new applications but limited capacity storage in Android phones, especially the mid-range ones, implies that a relatively small percentage of people ends up doing so. Finally, several recent policy reports emphasise that behavioural biases often imply that people tend to use default apps rather than searching and downloading new ones.==== Of course, whether multi-homing is rare or frequent is something which varies across times and markets. Therefore it is crucial to look at the evidence to understand which one prevails in the specific case at hand.====We also study a variant of the model in which scale economies in the secondary market arise on the demand side because of the existence of network externalities, and show that the mechanism described above extends to this case. By engaging in tying the incumbent limits the rival’s sales of the secondary product in the initial period and hence prevents the rival from achieving the network size that is critical to make the quality of its product superior to that of the incumbent’s. In turn this allows the incumbent to earn higher profits in the future, despite the rival’s entry in the primary market.====In the model with network externalities the presence of non-negative price constraints is crucial for exclusion. When negative prices are not feasible, in the absence of tying competition in the secondary market is softened, which prevents the equilibrium price in that market from decreasing too much. In turn, in the initial period when its dominant position is still unchallenged, this prevents the incumbent from using the price of the primary product to extract a sufficient share of the rents that the more efficient rival in the secondary market will make in the future period. For this reason engaging in tying and excluding the rival in the secondary market may turn out to be the optimal strategy for the incumbent.====This paper is related to the literature on the leverage theory of tying. Economic theory has moved into three main directions to show why it may be profitable for a firm that is dominant in one market to engage in exclusionary tying or bundling. One strand of the literature relies on imperfect rents extraction, i.e. it identifies some reasons why the incumbent may fail to appropriate, through the price of the primary (or core) product, enough efficiency rents either currently or in the future. Those reasons include regulation of the price of the primary product, the impossibility to set negative prices (Choi and Jeon, 2020), and restrictions to the set of feasible contracts (Carlton, Waldman, 2012, Grenlee, Reitman, Sibley, 2008). Another strand of the literature identifies the circumstances under which tying (or bundling) allows the monopolist in a market to commit to aggressive behaviour which discourages entry in the adjacent market, as in Whinston (1990) and Hurkens et al. (2016), Choi (1996a), Choi (1996b), Choi (2004). The third strand of the literature relaxes the assumption that the incumbent’s dominant position in the core market is safe as in Choi and Stefanadis (2001) and Carlton and Waldman (2002). As discussed above, our paper is closely related to Carlton and Waldman (2002) and contributes to this literature by showing that the incentive to engage in exclusionary tying also arises when it does not represent a defensive strategy, that is when entry in the primary market cannot be deterred.==== Our paper is also related to Choi and Stefanadis (2001). Their model and ours share the fact that the incumbent engages in bundling (or tying) so as to make entry in ==== the complementary markets less likely, a scenario in which the incumbent is completely displaced. However, the two models differ in the mechanism through which bundling (or tying) leads to exclusion. In our model tying, by limiting significantly the entrant’s sales of the secondary product in the first period, when the incumbent still monopolises the primary market, deters entry altogether: since those sales are crucial to achieve efficient scale, the rival will not enter in the second period either, despite the presence of the independent rival in the primary market. Instead, in Choi and Stefanadis (2001) there is a single period in which both independent entrants take their respective entry decisions, conditional on the success of their ==== investment. Bundling, by allowing an independent rival to sell its product only if the other component is available separately, makes entry in a given market profitable only when the ==== processes of both firms are successful. In turn, this reduces the marginal returns of each firm’s ==== investment relative to the case in which the incumbent does not engage in bundling, thereby inducing the entrants to invest less and making the scenario in which both entrants are active less likely.====Our paper is also related to the literature on exclusionary conduct in general, beyond tying. In this perspective, it is probably closest to another paper of ours Fumagalli and Motta (2018) in which we examine the incentives for a vertically integrated firm to engage in refusal to supply so as to monopolise the downstream market, either to protect the upstream monopoly, or as a way to extract more rents from an upstream entrant.====An antitrust case that might be interpreted in the light of our model is ====.==== At the time of the decision Genzyme was the unique producer of Cerezyme, then the only drug available for the treatment of the Gaucher disease (a rare metabolic disorder). For home patients the drug needed to be administered by specialised nurses or doctors. Up until 2000 Genzyme had relied on an independent company, “Healthcare at Home”, as an exclusive provider of delivery and homecare services for Cerezyme. Later, it started operating its own company of delivery and homecare services, “Genzyme Homecare”, and sold to the National Health Service (NHS) the drug together with the additional services at the same price at which it sold the drug only to “Healthcare at Home”.====The Office of Fair Trading (OFT) found that by this practice Genzyme had engaged in anti-competitive behaviour, bundling and margin squeeze, two separate abuses (see Paragraph 293 of the OFT Decision). (We discuss the case here as bundling, while in Fumagalli, Motta, Calcagno, 2018 we present it through the lenses of margin squeeze and vertical foreclosure. Note that the practice at issue is the same, which can be interpreted in two different ways.)====While it is uncontroversial that Genzyme’s behaviour left no potential scope for competition in the market for delivery and homecare services,==== the OFT and the Competition Appeal Tribunal (CAT) differ in the assessment of the extent to which it might have prevented further entry in the market for the supply of drugs for the Gaucher disease. (The OFT argued that monopolisation of the home delivery service would have raised barriers to entry into the drug market.==== Instead, the CAT was more skeptical about the potential for foreclosure in the drug market.====)====Based on the available evidence we cannot take a position on whether Genzyme’s conduct would limit future entry in the market for the supply of drugs for the Gaucher desease. However, our paper highlights that Genzyme’s incentives to exclude “Healthcare at Home” would exist even if lack of competition in the market of homecare services did ==== discourage future entry in market for the drug. In that case Genzyme’s conduct would not be rationalised by the intent to protect its dominant position in the drug market; but by the aim to gain a favorable position in the complementary market of the homecare services so as to appropriate some of the efficiency rents produced by future entry in the drug market.====The paper proceeds as follows. Section 2 analyses the baseline model with supply-side scale economies in the secondary market. Section 3 studies the case in which there exist network externalities in the secondary market. Section 4 concludes the paper.","Tying in evolving industries, when future entry cannot be deterred",https://www.sciencedirect.com/science/article/pii/S0167718719300955,3 January 2020,2020,Research Article,204.0
"Kim Jin-Hyuk,Komatsu Takehiko,Owan Hideo","Department of Economics, University of Colorado at Boulder, 256 UCB, Boulder, CO 80309, USA,Amazon Japan G.K., Arco Tower Annex 1-8-1 Shimomeguro, Meguro-ku, Tokyo 153-0064 Japan,Faculty of Political Science and Economics, Waseda University, 1-6-1 Nishiwaseda, Shinjuku-ku, Tokyo 169–8050, Japan","Received 1 March 2019, Revised 9 December 2019, Accepted 17 December 2019, Available online 24 December 2019, Version of Record 7 January 2020.",https://doi.org/10.1016/j.ijindorg.2019.102565,Cited by (1),"We analyze a model of vertical (dis)integration between manufacturing and design in a monopolistically competitive market. Specialized input manufacturers can serve multiple design firms and the manufacturer-designer pairs negotiate a non-binding contract to share input customization cost and production surplus. Hand-collected data on 387 product lines from 118 semiconductor firms are used to predict the firm’s decision to outsource manufacturing. We find that, for instance, the use of design tools that facilitate collaboration and process technologies that facilitate learning are both positively associated with outsourcing, consistent with the model’s prediction.","With the technological advancement in the 20th century and the establishment of such pioneering firms as Fairchild Semiconductor, practicable semiconductor devices, especially in the form of integrated circuits (ICs, or ‘chips’), have shaped the evolution of electronics industries. Originally, semiconductor devices were produced by the so-called integrated device manufacturers (IDMs) having both the capacity to design and manufacture semiconductor devices. A well-known example is Intel, which developed the first microprocessor in 1971 and still dominates the microprocessor product market today.====By the late 1980s, however, the so-called foundry model came into being. The foundry model refers to the separation of chip design and fabrication process into different business entities.==== Design firms are often called ‘fabless’ and specialized manufacturers are called ‘foundries.’ Concurrently, most of the foundries were established in Asia, such as Taiwan Semiconductor Manufacturing Company (TSMC, founded in 1987), and there are numerous fabless companies today in many parts of the world specializing in chip designs for a broad range of applications from mobile phones to internet-of-things to self-driving cars.====Although a number of industry reports (such as McKinsey & Co’s) and newspaper articles have talked about what might have given rise to the popularity of the foundry model, there have been relatively few studies that rigorously investigate such arguments theoretically and/or empirically, especially using data from the 21st century (see below). This paper fills this gap by presenting an industry equilibrium that explains the conditions under which a stable outsourcing equilibrium may arise and empirically showing that the factors that are highlighted in the model as the drivers can indeed explain a significant variation in the data.====The central argument behind the emergence of the foundry model among industry experts is that the development of design tools and standardization of process technologies played a key role in facilitating the specialization in the vertical chain (e.g., Fuller, Akinwande, Sodini, 2003, Macher, Mowery, 2004, Saito, 2009). Further, as the applications for semiconductor devices became more sophisticated over time, the capital requirements for developing new chip designs and process technologies became too large for some IDMs. Thus, some firms decided to focus on chip design while others on fabrication given their relative capabilities and strength.====To be precise, the chip design practice was facilitated by the development of electronic design automation (EDA) tools and cell libraries, which helped the layout and interface construction as well as performance simulation of novel chip designs. At the same time, the so-called “complementary metal oxide semiconductor” (CMOS) processes emerged as a prominent manufacturing process. The CMOS process is standardized, hence, applicable to a variety of types of semiconductor products, so the foundries could aggregate demands from multiple design firms, process on bulk standard CMOS wafers, and achieve high yield improvements.====Since yields determine productivity and the CMOS process is more versatile (i.e., can be used to manufacture a wide range of products) than others (e.g., the production process for analog logic devices), the CMOS process tends to benefit from both economies of scope and scale. For instance, TSMC succeeded greatly by developing and launching CMOS processes for smart-phones as well as other customers in the communications industry. Other foundries follow more or less the same pattern of launching a new process, pooling demands and ramping up production until they launch another process and further expand their customer base.====We aim to contribute to the literature on vertical (dis)integration by incorporating the above arguments into an economic model of outsourcing and presenting some systematic evidence on it. In this regard, this paper is consistent with and complementary to the management literature on the semiconductor industry based on detailed qualitative studies. For instance, Linden and Somaya (2003) put forward arguments and facts that are broadly consistent with ours in that the CMOS process and the EDA tools helped develop the vertical disintegration of the supply chain and the modular production of various products, especially system-on-a-chip.====The literature on outsourcing that is closest to our approach is the transaction cost economics (TCE) literature (e.g., Coase, 1937, Williamson, 1975), on which most of existing work on the “make-or-buy” decision is based.==== The dominant prediction of the TCE literature is that specific assets create an ex-post opportunism (or hold-up of quasi-rents), so firms tend to internally produce outputs rather than outsource (Williamson, 1985). We build on this framework a monopolistic competition model and empirically measure the asset or technology specificity at a finer level of product lines than existing papers do.====For instance, Monteverde (1995) is a seminal paper on the semiconductor industry, where technical dialog (i.e., unstructured design-fabrication interaction) is used to explain the “curious phenomenon of fablessness.” It was argued that analog and memory chips require close coordination between design and process engineers while logic chips are less likely to do so because of automated design tools and documented process rules. To confirm this claim, industry experts were asked to rate each of the sample ==== on the relative magnitude of technical dialog necessary for the product strategy each company was pursuing.==== It was found that, when the volume of technical dialog is large, integration was more likely than outsourcing.====Leiblein and Miller (2003) embed the technical dialog put forward by Monteverde (1995) into a similar empirical framework. The novelty is that they expand the dataset both in terms of sample size and the unit of analysis.==== Specifically, their sample comes from a 1996 survey of global IC manufacturers (with 117 responding firms), where the unit of observation is defined by a combination of seven product categories (analog; application-specific integrated circuit (ASIC); discrete; digital signal; memory; microprocessor; telecommunication) and manufacturing process nodes (from 1- to 0.25-micron). Similar to Monteverde (1995), asset specificity is coded high for analog, ASIC, and memory and low for all other categories.====We follow this stream of literature by refining the data sample on a couple of dimensions. First, our sample comes directly from a trade association, covering 118 semiconductor firms from around the world, and we hand-collected reported data on technical specifications of the firms’ products from data sheets and company brochures. Second, we classify product lines into five main categories (analog; discrete; logic, memory; micro) and 19 subcategories. The latter gives us a richer variation to exploit because each product line can have a different set of technical specifications even within the same main category, and firms can also make a different sourcing decision across product lines.====Since both Monteverde (1995) and Leiblein and Miller (2003) code asset specificity at a higher level category, it means that the contribution of underlying design and process technologies are not clearly identified if there are nontrivial variations within main product categories. Thus, to refine the contribution of these technological factors, we would separately code the use of EDA tools and the adoption of CMOS processes for all our sample product lines, which vary across product subcategories. By including these variables in addition to the (traditional) product category dummies, we are better able to identify the effects of the said factors.====Macher (2006) provides the closest analysis to ours in this regard by using data on 179 process groups from 36 manufacturing facilities that started between 1995 and 2001. While the focus is on how organizational structure (i.e., specialized versus integrated manufacturers) moderates the effects of TCE (which is proxied at the category level; e.g., analog and memory chips have ill-structured problems while logic is not) on process performance (such as development time and die yields), one of their results shows that the EDA tools and the market share held by CMOS technologies at the start of each process affect the outsourcing decision.====To our knowledge, the literature has not integrated the empirical evidence with a model of industry equilibrium that illustrates the effects of input customization and learning curve. Further, the foundry-fabless model has become increasingly important in the global semiconductor industry in the mid-2000s when the Moore’s Law was slowing down (e.g., Waldrop, 2016).==== More firms have since diversified their business opportunities rather than necessarily moving to leading-edge processes. Thus, our paper contributes to the literature by bringing an outsourcing model to recent 2007 data when the foundry model became more prominent.====The rest of the paper is organized as follows. Section 2 presents the model of outsourcing and shows the stability of market equilibria. Section 3 describes the dataset and documents the empirical evidence. Section 4 concludes.",The role of design method and process technology in stable outsourcing equilibria,https://www.sciencedirect.com/science/article/pii/S0167718719300931,24 December 2019,2019,Research Article,205.0
"Lam Wing Man Wynne,Liu Xingyi","Norwich Business School and Centre for Competition Policy, University of East Anglia, Norwich Research Park, Norwich NR4 7TJ, United Kingdom,Aston Business School, Aston University, Birmingham B4 7ET, United Kingdom","Received 24 February 2019, Revised 30 November 2019, Accepted 9 December 2019, Available online 13 December 2019, Version of Record 9 January 2020.",https://doi.org/10.1016/j.ijindorg.2019.102564,Cited by (9)," effect: the prospect of easier switching due to data portability may entice consumers to provide even more data to the incumbent, which strengthens the incumbency advantage. Hence, the effectiveness of data portability in fostering competition will depend on what types of data are portable. More generally, in analysing the effectiveness of polices aiming at reducing ==== actions that build up endogenous entry barrier.","Competition in non-price characteristics, such as functionality and data services, has become increasingly common in Internet markets. Many platforms offer consumers free services in exchange for consumer data being used for data analytics (e.g., Google search and Facebook). On one hand, this has prompted concerns over consumer privacy; on the other hand, data analytics may give platforms a competitive advantage and market power that affect future competition and innovation.====In face of these challenges, the General Data Protection Regulation (GDPR) has come into force since May 2018, which grants consumers a set of rights with more control over the collection and use of their data. Notably, consumers are now given a new right to data portability under Article 20 of the GDPR, defined as follows:====A clear aim of data portability is to facilitate consumer switching between different service providers, prevent lock-in, and foster entry and competition. However, in contrary to many other existing markets where switching costs are largely exogenous (e.g., physical costs of opening a new bank account) or determined by firms (e.g., coupons offered to loyal consumers, high degree of incompatibility between firms’ products), consumers play an important role when it comes to data related services. Specifically, when consumers provide more data to an incumbent, they may find themselves more locked-in with the incumbent, due to a range of services offered by the incumbent that analyse their data and encourage stickiness. Hence, it is important to understand the impact of data portability on switching and entry, when consumers react to the new policy and adjust their behaviour. More generally, we attempt to look into the impact of policies that aim at reducing switching cost, which can be endogenously generated by consumers’ consumption decisions.====Specifically, we consider a two-period model, where an incumbent acts as the monopolist in the first period and an entrant can potentially enter and compete in the second period. Both firms can provide a basic data service to a unit mass of homogeneous consumers. The incumbent can provide, if available, additional big data service in the second period, which reflects the advantage of the incumbent in analysing data over the entrant. The entrant can enter in the second period if it provides a better service than the incumbent, i.e., when the value of its service is above a certain threshold. We are interested in how data portability affects this threshold.====Certainly, entry becomes easier with data portability, should everything be portable. There are, however, boundaries to the right to data portability. In particular, it applies only to data “provided by” the data subject but not data “inferred or derived by” the data controller. For instance, whereas data on a consumer’s search history fall within the scope of data portability, inferred consumer data for personalising products or making recommendations fall outside. Another example is that “the outcome of an assessment regarding the health of a user or the profile created in the context of risk management and financial regulations [... ] are inferred or derived from the analysis of data provided by the data subject [... and hence] will not be within the scope of this new right”.==== Therefore, data analytics enable firms (also referred to as data controllers) to provide non-portable value added services to consumers (also referred to as data subjects), which can lock customers in a relationship with the data controller. Thus, data portability does not completely eliminate the incumbency advantage, and we show that it may even enhance such advantage under certain conditions.====To be more specific, we find that data portability affects entry in two ways. First, for a given level of data provision in the first period, it facilitates consumer switching and entry. This is the ==== effect, which is one of the most compelling reasons for promoting data portability. Second, allowing the level of data provision to vary, data portability encourages consumers to provide more data in the first period as the value of data becomes higher when they can be ported across service providers. This is the indirect ==== effect, which raises the value of the incumbent’s service and strengthens the incumbency advantage. More generally, data provision can be seen as an investment made by the consumers to increase the value of the relationship with a data controller, and this investment is relationship-specific when data cannot be ported. With data portability, it reduces investment specificity, which facilitates ==== switching. However, data portability also raises the value of data provided in the first period and increases consumers’ ==== incentive to invest and, hence, leads them to provide more data. Most notably, such a ==== effect is recognised in the discussion on the introduction of data portability in Singapore:==== “====”====The latter effect is largely ignored in the literature on exogenous switching cost, where switching cost creates consumer lock-in only on the extensive margin. However, in our model, data portability changes the intensive margin of consumer demand, which may create endogenous entry barrier. More specifically, we find that without data analytics, data portability facilitates switching and entry, as the ==== effect dominates the ==== effect. With data analytics in addition to data portability, the ==== effect dominates if the big data service is valuable enough, in which case data portability can make entry more difficult. Interestingly, this is more likely to be the case with network effects (i.e., when the value of big data service depends on large population data and hence increases with the size of the user base) compared to without. The reason is that with network effects, an individual consumer ignores the positive externality of his data provision on other customers and hence provides too little data, which weakens the ==== effect. On the other hand, less data provision and a higher degree of data portability means that a consumer is more likely to switch and port their data, which makes their data provision more responsive to enhanced data portability, i.e., the ==== effect is stronger. Combining both effects, data portability is more likely to raise entry barrier when network effects are at work. Furthermore, entry deterrence is more likely when the entrant adopts a more innovative strategy, i.e., when the entrant is more likely to obtain a sufficiently innovative product and enter the market. The reason is that anticipating a better firm will enter with high probability, consumers are less likely to stay with the incumbent in the future, which reduces the value of providing data to the incumbent. This reduces first period data provision and weakens the ==== effect, compared to the situation with a less innovative entrant. However, even if the amount of data provided in the first period is smaller, consumers are more likely to port these data to the more innovative entrant, which strengthens the ==== effect. Hence, entry becomes more difficult. In addition, the availability of data portability by itself is sufficient for the above effects to emerge. In GDPR, the right to data portability comes together with a set of other rights that grants consumers more control over their data and alleviates consumers’ privacy concerns (see, for instance, Tucker, 2014), which may further amplify the ==== effect, due to higher willingness to provide data in the post-GDPR era, and make entry even more difficult.====Thus, although data portability may benefit consumers in the short run, it can have an adverse effect on entry and long-run efficiencies. Under certain circumstances, it can result in “excess inertia” which locks generations of consumers in with the incumbent (see also, for instance, Farrell and Saloner, 1986). This becomes more prominent when consumers enter the market sequentially and a sufficient scale is necessary for successful entry (e.g., a large enough database for data analytics).==== In such situations, early generations of consumers are incentivised to provide more data when data portability becomes viable, which allows the incumbent to accumulate even more data and makes future entry increasingly more difficult. This could have the further consequence of slowing down innovation.====The results point to the potential limit of data portability in fostering competition under the current framework of legislation, especially when established firms such as the GAFAM (Google, Amazon, Facebook, Apple, and Microsoft) rely more and more on derived data services. In fact, we also see incumbent firms actively and voluntarily working together on data portability, such as the launch of the Data Transfer Project by Microsoft, Facebook, Google, and Twitter in 2017.==== Furthermore, our results shed light on why Google remains popular in spite of the introduction of its data portability service, Google Takeout, for 27 products in 2011, which was extended to other core services such as Google Search in 2016.====In summary, in accordance with the recent Stigler Center Report (2019) (pp. 26 and 88) and the Vestager Report==== (p. 58), the effectiveness of the right to data portability will depend on the way it will be implemented in practice, specifically, what types of data can be ported. Our results point out that the role of data portability in facilitating entry and competition may be limited, when inferred data are not covered under the current legislation. This is more likely so when we take consumer behaviour into account. Similar ideas may also apply to other markets where consumers can build up their own switching costs. For instance, in markets where reputation is important (e.g., online trading, peer-to-peer sharing), consumers may be incentivised to trade more on these platforms to build a better reputation, when their endorsements such as customer feedbacks, credit scores, trust scores become portable to potential entrants, and this can make future entry of new providers harder. This may also apply to markets for professional advice (e.g., medical, legal, financial services), where better services rely on information provided by clients. When these information become portable, clients may use their current service provider more intensively (e.g., stick with the same doctor, lawyer or mortgage advisor) and become more reluctant to switch in the future. Hence, it is important to understand how consumers react to policies that intend to lower switching cost, and our paper attempts to pave the way for further studies on the implementation of these policies.",Does data portability facilitate entry?,https://www.sciencedirect.com/science/article/pii/S016771871930092X,13 December 2019,2019,Research Article,206.0
"Mrázová Monika,Neary J. Peter","Geneva School of Economics and Management (GSEM), University of Geneva, Bd. du Pont d’Arve 40, 1211 Geneva 4, Switzerland,Department of Economics, University of Oxford, Manor Road, Oxford OX1 3UQ, United Kingdom,CEPR and CESifo","Received 26 January 2019, Revised 14 October 2019, Accepted 8 November 2019, Available online 29 November 2019, Version of Record 25 May 2020.",https://doi.org/10.1016/j.ijindorg.2019.102561,Cited by (4),", paying particular attention to competition effects, pass-through, selection effects, and linking distributions of firm characteristics and outcomes. A recurring theme is that CES preferences are extremely convenient for deriving analytic results, but also extremely restrictive in their theoretical and empirical implications. We introduce the class of “constant-response demand functions” to describe some related families of demand functions that provide a unifying principle for much recent work that explores alternatives to CES demands.","In this paper we aim to provide an overview of recent research in international trade, from the perspective of the theory of industrial organization (henceforth IO). There are two aspects to this perspective, mirroring the two different interpretations of our title. On the one hand, we are interested in “IO for ====”: what is the structure of export markets?; how does that structure affect which firms export?; does trade foster competition?; and a host of other similar questions. On the other hand, we want to think about “IO for ====”: which IO models are used in trade? As is well-known, the answer to the latter question is not any of the many varieties of oligopoly used in IO itself, but rather the monopolistically competitive model of Chamberlin (1933), as formalized by Dixit and Stiglitz (1977); especially with Constant-Elasticity-of-Substitution (CES) preferences, introduced as a special case by Dixit and Stiglitz (1977), brought to center stage by Dixit and Norman (1980) and Krugman (1980) among many others, and extended to firm heterogeneity by Melitz (2003). Ironically, though monopolistic competition originated in IO, it is used relatively rarely there, so it could be described as “IO for export ====.”====While monopolistic competition is by far the dominant paradigm in international trade, it is by no means the only approach. First there is a large literature on “strategic” trade policy, dating from the 1980s, that uses off-the-shelf IO models of oligopoly to address standard issues of trade policy, mostly at the level of a single industry.==== Second, there have been some attempts to explore the implications of oligopoly in general equilibrium for trade questions, both of the traditional kind such as trade patterns, gains from trade and pricing-to-market,==== and also with applications to topics such as cross-border mergers and multi-product exporters.==== Finally, there is a small literature on the theory of superstar firms that compete oligopolistically, while coexisting with a monopolistically competitive fringe.====However, there are good reasons why the monopolistically competitive paradigm has remained dominant in trade. The availability of large data sets on exporting firms, and the desire to allow for entry and exit and to take account of general-equilibrium feedbacks to and from factor markets, make bespoke modelling of individual sectors infeasible, and a “one-size-fits-all” approach to the choice of market structure very convenient.====Two general themes emerge from our analysis, one old, one new. First, the widespread assumption of CES preferences, implying demand functions that are log-linear in price, is extremely convenient for deriving analytic results, but also extremely restrictive in its theoretical and empirical implications. Second, we introduce the class of “constant-response demand functions” to describe some related families of parametric demand functions that imply a constant absolute or relative response of some firm-level variable to an exogenous change in marginal cost. As we shall see, this class of demands, all of which nest the CES case, provides a unifying principle for much recent work, including our own, that explores alternatives to CES demands.====The plan of the paper is as follows. Section 2 introduces the basic underlying model and an approach to comparing different demand functions that we use throughout. It also discusses recent evidence that questions a key implication of CES preferences. Each of the four subsequent sections considers a substantive question, and asks, which demand functions arise naturally in trying to answer it? As we show, each question can be formulated in terms of a constant response of a target function to a change in costs, and so we are led to seeking demand functions that are consistent with such a constant response. Section 3 considers the central topic of competition effects, and shows that the CES demand function is a key benchmark in categorizing them, both at the firm level in the cross-section and in general equilibrium when the economy responds to exogenous shocks. Section 4 considers the issue of how cost changes are passed through to goods prices, and introduces the Bulow–Pfleiderer (B-P) and Constant-Proportional-Pass-Through (CPPT) demand functions that correspond to constant rates of absolute and proportional pass-through respectively. Section 5 turns to consider the central topic of selection effects, and introduces the Constant-Elasticity-of-Marginal-Revenue (CEMR) demand functions that provide a natural reference point for understanding them. Section 6 explores the relationship between the distribution of firm productivity, on the one hand, and the distributions of the level and growth rate of sales, on the other; and it introduces the Constant-Revenue-Elasticity-of-Marginal-Revenue (CREMR) demand functions that arise naturally in this context. Finally, Section 7 provides an overview of the constant-response demand functions introduced in earlier sections, and introduces a general demand function that nests three of them. Section 8 concludes. Proofs and further details can be found in our other papers: Mrázová and Neary (2017) for 2 Preliminaries, 3 Competition effects and 4, Mrázová and Neary (2019) for Section 5, and Mrázová et al. (2015) for Section 6.",IO for exports(s),https://www.sciencedirect.com/science/article/pii/S016771871930089X,29 November 2019,2019,Research Article,207.0
Chioveanu Ioana,"Department of Economics and Finance, Brunel University London, Uxbridge, UB8 3PH, UK","Received 11 April 2019, Revised 1 October 2019, Accepted 16 November 2019, Available online 28 November 2019, Version of Record 6 January 2020.",https://doi.org/10.1016/j.ijindorg.2019.102563,Cited by (0),"This paper analyses a model of competition where the firms set not only prices but also the complexity levels of their prices (which determine how difficult it is for consumers to assess the price offers). Unlike previous work, in this model, the firms’ confusion technology may be non-linear in the aggregate complexity level. The equilibrium probability of using high complexity increases in the number of firms but decreases in the convexity of the confusion technology. In large markets, the firms use high complexity almost surely. However, the ==== profit converges to the highest level with concave technologies and to the lowest level with convex technologies. An increase in consumer sophistication, which benefits the consumers, may not reduce market complexity.","In many markets, including banking, finance, and energy retail, firms’ use of partitioned prices, differentiated price formats, or technical language in their price disclosures makes it harder for consumers to assess prices and identify the best offer. Recent research associates price complexity with consumer inertia and lack of sophistication, price and format dispersion, positive mark-ups, and unintended responses to intensified competition. It also raises concerns about its strategic use by firms to create consumer confusion and soften competition.====Discussing the challenges of consumer financial regulation, Campbell (2016) points out that “financial ignorance is pervasive and unsurprising given the complexity of modern financial products”. The UK Competition and Markets Authority (2015), investigation of theretail banking market found that price complexity may prevent consumers from receiving good value and identifying the best deals. A European Commission (2007) study of EU mortgage credit markets and the report by the UK Independent Banking Commission (2011) echo these concerns.====Related theoretical work sends a consistent message that, in homogeneous product markets, the positive effects of stronger competition are weakened by increases in complexity and equilibrium obfuscation, and identifies the possibility that an increase in the number of firms harms consumer welfare; see Spiegler (2016) for a synthesis. Although existing results raise concerns about the performance of markets with price complexity and the impact of competition, they provide relatively limited guidance for competition and consumer protection policy.====This paper aims to inform policy and explores some key questions. Is the degree of complexity a good indicator of market performance? Does an increase in consumer sophistication increase market transparency? Under what conditions does an increase in the number of firms harm consumer welfare? What tests can be used to assess the impact of competition on welfare? Robust answers to these questions are crucial to the practical relevance of extant findings.====The analysis focuses on homogeneous product oligopoly markets where the firms compete in price and price complexity. Like in Carlin (2009), complexity makes it harder to assess prices and prevents some consumers from identifying the best deal. Given the firms’ complexity choices some consumers are ‘informed’ or ‘experts’, while others are ‘confused’. Like in Varian (1980), the experts are able to assess all prices and buy the lowest-price product, while the confused consumers buy from a randomly selected firm (or make random mistakes). The shares of experts and confused consumers depend on the firms’ price complexity choices. A unilateral increase in a firm’s price complexity increases the share of confused consumers as it raises the difficulty of assessing prices.====This model generalizes the Varian-Carlin framework to accommodate confusion technologies which may be concave or convex in the aggregate complexity level. In contrast, in Carlin (2009) the confusion technology is linear, while in Varian (1980) the share of confused consumers is exogenous. In the proposed model, the increase in the share of confused consumers triggered by an increase in a firm’s complexity may be either reduced or magnified by the complexity of the rivals’ price offers.====As a result, this analysis considers a wider gamut of consumer behaviors. For instance, consider a firm which increases its price complexity by including more technical terms or sophisticated jargon. If there is consumer learning by doing, the effect of the increase in complexity would be smaller if the rivals’ aggregate complexity were higher. The consumers may get better at deciphering technical language the more they are exposed to it, and so there would be some reciprocal cancellation of the firms’ complexity levels. On the other hand, if the consumers are more likely to be demoralized or make mistakes as the informational load increases, the effect of an incremental increase in complexity may be larger when the rivals’ aggregate complexity is higher.====Consistent with closely related analyses, in our model, price complexity underlies consumer heterogeneity and in symmetric equilibrium the firms randomize on prices from a closed interval according to a continuous distribution function.==== When setting a relatively low price, a firm benefits from low complexity, but when setting a relatively high price, it benefits from high complexity. As a result, in equilibrium there is a positive relationship between prices and complexity levels, and dispersion in both dimensions. Despite product homogeneity, prices are strictly above marginal cost and the expected profits are strictly positive.====Our extended framework distinguishes between two properties of the confusion technology, the convexity/concavity (i.e., the curvature) and the degree of consumer sophistication, and cleanly separates their comparative statics effects. To isolate these properties, our oligopoly model parametrizes the curvature of the confusion technology. However, in the preliminary duopoly analysis, this distinction does not depend on the parametrization of the curvature.====In the symmetric duopoly equilibrium, an increase in the convexity of the confusion technology (or a reduction in its concavity) increases the probability of using low complexity, decreases the lowest price associated with low complexity, weakly increases the lowest price associated with high complexity, decreases the average price and expected industry profit, and increases expected consumer surplus. For a given probability of using low complexity, the expected share of confused consumers decreases in the convexity of the confusion technology. Greater convexity delivers a greater expected market share increment to a firm reducing complexity and competing aggressively. So, as the confusion technology becomes more convex or less concave, the firms rely less on confused consumers and the equilibrium probability of using low complexity increases.====Our duopoly analysis shows that an increase in consumer sophistication decreases the average price and expected firm’s and industry’s profits, but has no impact on the frequency of using low complexity. This indicates that although consumer protection programmes which raise consumer awareness (e.g., financial literacy programmes) boost consumer surplus, they may not reduce overall market complexity. So, market transparency may not be a good indicator of the effectiveness of consumer awareness initiatives or, more generally, of market performance.====The oligopoly analysis generalizes some of these results and explores the impact of changes in the number of firms and their policy implications. An increase in the number of firms increases a firm’s probability of using high complexity and so lowers market transparency. In relatively small markets, expected industry profit is not monotonic in the number of firms. In large markets with concave or linear confusion technology, the expected share of confused consumers and industry profit are bounded away from zero and converge to the highest possible level as the number of firms goes to infinity. In contrast, in large markets with convex confusion technology, the expected number of confused consumers and industry profit converge to zero. Hence, competition policy which increases the number of firms may backfire in relatively small markets and be undesirable in large markets with concave or linear confusion technology.====As concave confusion technologies are consistent with consumer learning by doing, our results suggest that the latter may coexist with poor outcomes for consumers in large markets. However, large markets with convex confusion technologies may perform well. More generally, in markets with many competitors, although the firms use high complexity almost surely, the impact of complexity on consumer surplus and an effective policy approach depend crucially on the characteristics of the confusion technology.====Our analysis is closely related to Piccione and Spiegler (2012), Chioveanu and Zhou (2013), and especially, to Carlin (2009). Piccione and Spiegler (2012) explore a general comparability structure in a duopoly market and identify a necessary and sufficient condition for the firms to earn max-min profits in equilibrium. In their model, the consumers are initially attached to one firm and can only compare offers whose price formats are compatible. In the oligopoly analysis in Chioveanu and Zhou (2013), where the firms can choose one of two price formats, an increase in the number of firms induces them to rely more on complexity and may harm consumer welfare. These studies focus on price comparability and how it is affected by price presentation. Closer to Carlin (2009), in our analysis, the share of consumers who gather price information depends on the overall complexity of the market.====Our work complements these analyses by examining the impact of confusion technology’s curvature on market outcomes and highlighting its policy implications. The only other analysis of concavity/convexity in confusion technology we are aware of is de Roos (2018), which studies the impact of limited product comparability on the viability of collusion. His work differs substantially both in focus and in modelling terms: it defines convexity in relation to format similarity rather than overall complexity, it considers repeated interaction, and it explores confusion due to limited comparability.====The next section introduces the model and a taxonomy of confusion technologies. Section 3 presents preliminary results, while Section 4 provides a duopoly analysis. Section 5 characterizes the unique symmetric mixed-strategy equilibrium in the general oligopoly model. Section 6 presents comparative statics and convergence results, and discusses consumer protection and competition policy implications. All proofs missing from the text are relegated to the appendix.",A more general model of price complexity,https://www.sciencedirect.com/science/article/pii/S0167718719300918,28 November 2019,2019,Research Article,208.0
Kastl Jakub,"Department of Economics, Princeton University, NBER and CEPR, United States","Received 27 January 2019, Revised 19 August 2019, Accepted 9 November 2019, Available online 28 November 2019, Version of Record 25 May 2020.",https://doi.org/10.1016/j.ijindorg.2019.102559,Cited by (3),"In this article I discuss how auctions and tools developed for their empirical analysis can inform empirical analysis of financial markets. Since virtually all markets organized as auctions have well-specified and known rules that map nicely into game-theoretical models, I demonstrate using several applications that one can often leverage particular details to study issues that have nothing to do with the auction per se. To do so, I first review an estimation method, which is widely applicable in many settings where a researcher needs to recover agents’ beliefs, in order to establish a link between observables and unobservables using some version of a necessary condition for optimality. I then discuss applications to quantification of front-running, evaluation of quantitative easing operations and estimation of a demand system for financial products.","The main goal of this article is to show how the tools recently developed in the Industrial Organization literature to analyze auction markets can be utilized to address questions that often arise in financial markets. The main theme throughout is that often particular details of how the various auctions are run provide us with an opportunity to look at the markets in a different and novel way. This paper complements an earlier article (Kastl, 2017) that also discusses the applications using IO tools in financial markets, but the focus here will be particularly on leveraging details of the auction mechanisms.====For example, a seemingly innocuous detail - that auctions of various maturities of government securities are frequently sold simultaneously and virtually the same bidders participate in all of these - allows us to sidestep one important problem typically encountered during estimation. Since we are able to observe how the willingnesses to pay for various maturities move together within a bidder and we are essentially able to observe a vector of those at the same time, we can study the linkages between the willingnesses to pay for different securities and recover the underlying demand system while not being exposed to issues involving unobserved heterogeneity, i.e., that other features of the environment might be changing and affecting the marginal valuation in an unobservable way. Another interesting detail from many Treasury bill auctions is that bids are often recorded with exact timestamps and they may also be revised before the auction deadline. Since some large participants, such as pension funds, are either required or choose to route their bids through primary dealers, we can utilize this observed timing of the bids and their dynamics to quantify (a particular version of) front-running. Finally, in the third application I will discuss how one can use the auction data to potentially analyze externalities across implementations of unconventional monetary policies when central banks do not coordinate their actions.====Theoretical analysis of auctions can be traced back at least to Vickrey (1961). The seminal article of Milgrom and Weber (1982) that methodically laid out equilibrium analysis of different auction formats in different information environments really opened the door for empirical analysis, both model-based and descriptive. Hendricks and Porter (1988) is an early empirical examination of oil lease auctions which provides a convincing argument that the behavior of actual bidders is close to what the Milgrom and Weber (1982) theory predicts. Laffont and Vuong (1996) and Guerre et al. (2000) then provide the key building block for structural analysis of auction data: the observation that bids can be inverted into values using the necessary condition for optimal bidding. This approach is reminiscent of the (Bresnahan, 1989) approach to identifying (unobserved) marginal cost from equations governing optimal pricing behavior within a differentiated Bertrand oligopoly.====In finance, the market microstructure foundational papers are Glosten and Milgrom (1985) and Kyle, 1985, Kyle, 1989. There are many papers that try to estimate the price impact of individual market participants based on Kyle’s model. Shleifer (1986) uses stocks that are being added (or withdrawn) from a market-wide index - and the thus created shifter in demand due to the behavior of index funds - to show that the demand curve for US equities is downward-sloping. This implies that there is potential for price-impact of large orders. There has been a burgeoning (theoretical) recent literature on market microstructure, including Vives (2011) or Rostek and Weretka (2012). There has been little work taking these models to the data though. One important exception includes primary markets for government debt, because these are often organized as auctions.====Unlike many, if not most, other empirical settings, auctions have a major advantage. The rules that the market participants need to follow are known pretty much to the last detail and are often very close to how we write down theoretical models. Since we have the necessary theories that describe optimal behavior in various settings that frequently occur, we can thus use the appropriate models to learn about the models’ fundamentals, such as the distribution of underlying values that are unobserved to the econometrician. This typically proceeds in two steps: for any action that we observe, we aim to use the rule for optimal behavior from the model to establish a link between an observable variable (e.g., the bid) and the unobservable that we need to recover (e.g., the marginal value). This rule, however, typically includes various expectations: the bidders need to optimize against actions of their rivals that depend on realizations on various variables that are unobserved to their rivals. Hence, the researcher needs to estimate the beliefs the bidders posses in order to make progress. Under rational expectations one can use the distribution of realized actions (typically together with some assumptions on the dependence of the random unobservables) to approximate the distribution of rivals’ actions against which a bidder needs to optimize. Under stronger assumptions (such as iid), one can in fact augment the observed realized actions via simulations in order to approximate the beliefs without having to pool data across auctions, which helps alleviating potential issues with (auction-level) unobserved heterogeneity.====This paper proceeds as follows. In Section 2 I go through the typical model and how to approach estimation. Athey and Haile (2002) provides a similar general setup for single-unit auction environments. Since this paper is aimed at applications in financial markets I start with a theoretical model of a general share auction that yields a set of necessary conditions for optimal bidding which link the observables (bids) to the unobservables (marginal values). Since the applications in this paper will involve only the discriminatory (or pay-your-bid) auction format, I will focus solely on this format. Since for any particular auction rules these optimality conditions involve beliefs about rivals’ actions, I review the resampling approach originally proposed in Hortaçsu (2002) that allows for easy estimation of these beliefs using observed behavior. Then I go through few applications from financial markets which utilize the details of the auction mechanisms to address various questions. First, in Section 3 I discuss two applications studying Canadian Treasury auctions. In Section 3.2 I show how to quantify a particular version of front-running using exact timestamps from bids submitted by primary dealers in auctions from Canadian Treasury bills. Second, in Section 3.3 I illustrate, using the data from Canadian Treasury bills as well, how to estimate a whole demand system for (a subset of) government securities utilizing timing of the auctions themselves. Finally, in Section 4 I discuss how data from QE auctions run in the U.K. can help us study the impact of unconventional monetary policy on secondary markets. Section 5 concludes.",Auctions in financial markets,https://www.sciencedirect.com/science/article/pii/S0167718719300876,28 November 2019,2019,Research Article,209.0
"Florez-Acosta Jorge,Herrera-Araujo Daniel","Universidad del Rosario, Calle 12C 4-59, Bogotá, Colombia,Université Paris Dauphine, LEDa, UMR CNRS [8007], UMR IRD [260], PSL, Place du Maréchal de Lattre de Tassigny, 75016 Paris, France","Received 10 August 2018, Revised 14 September 2019, Accepted 11 November 2019, Available online 22 November 2019, Version of Record 18 December 2019.",https://doi.org/10.1016/j.ijindorg.2019.102560,Cited by (4),"We empirically examine the role of shopping costs in consumer ==== in a context of competing differentiated supermarkets that supply similar product lines. We develop and estimate a model of demand in which consumers can purchase multiple products from multiple stores in the same week, and incur transaction costs of dealing with supermarkets. We show that a similar model without shopping costs predicts a larger proportion of multistop shoppers and overestimates own-price elasticities and product markups. Further, we use our model along with a model of competition between supermarkets to study two practices that are commonly used by supermarkets: product delisting and loss-leader pricing. We show that the presence of shopping costs makes product delisting less profitable whereas it makes loss-leader pricing more profitable compared to a context in which consumers do not incur shopping costs.","The modern grocery retail industry is dominated by a small number of powerful large-scale supermarket chains==== that attempt to entice customers to favor one-stop shopping through aggressive non-price strategies, such as: first, the proliferation of superstores with vast floor areas (20,000+ sq. meters) that offer a large product range (200,000+ different brands);==== second, a joint location with suppliers offering parallel services (e.g., shopping malls, beauty salons, restaurants, car wash facilities, gas stations, and playgrounds for children); third, the promotion of private labels (PLs), whereby supermarkets are less dependent on branded products (the so-called national brands, NBs) and induce consumer loyalty; and lastly, the increasing emphasis on strategies that induce consumer retention and reinforce store loyalty, such as loyalty programs.====In such a context, it becomes important to understand the role that consumers play in the way in which a set of differentiated supermarkets offering similar product lines interact. There are a number of key features that determine the effectiveness of supermarket price and non-price strategies and the exercise of market power. In particular, whether customers prefer concentrating their purchases within a single supermarket or sourcing multiple store chains in the same period, the transaction and opportunity costs associated with shopping activities (so-called ====), and how these costs and shopping patterns shape the way in which consumers substitute across products and supermarkets. Precisely, a number of theoretical papers on multiproduct retailing shows that allowing customers to incur heterogeneous shopping costs in the analysis of multiproduct demand and supply may dramatically change policy conclusions (see Klemperer (1992); Klemperer and Padilla (1997); Armstrong and Vickers (2010); Chen, Rey, 2012, Chen, Rey, 2019).====The contribution of this article is to study the role of shopping costs in explaining the consumer choice of multiple products and multiple shopping locations, and in the measurement of supermarkets’ market power. To this end, we develop a multiple-discrete choice model in the context of competition between supermarkets that offer the same product line to the same customers. Consumers can purchase baskets of products from either a single store (====) or multiple stores (====) during a given period. Our key modeling strategy is to explicitly account for this observed heterogeneity by introducing consumer transaction costs related to shopping. Following Klemperer (1992), we comprehensively define ==== as all of the consumer’s real or perceived costs of using a supplier.==== These may include transportation costs and opportunity costs related to time spent parking, selecting products in the store, and waiting in line at the checkout; they may also account for the taste for shopping (Chen and Rey, 2012).====Our general empirical strategy is to estimate basket-level demand using standard techniques from the discrete-choice literature, along with simulated methods. We specify the utility of each product as a function of observed and unobserved product and store characteristics, as well as parameters to be estimated. On every shopping occasion, each consumer faces idiosyncratic shopping costs that increase with the number of supermarkets visited. Each consumer weighs up the extra benefits of dealing with an additional store against the additional costs involved. If the benefits exceed the costs, the individual will visit an additional supermarket. Otherwise, she will make all her purchases at a single location. The total utility of a basket of products is the sum of the product-specific utilities minus the shopping costs. To consistently estimate the parameters of the model, we encounter this challenge: shopping costs vary across individuals and are unobserved (by the econometrician). We deal with this by decomposing shopping costs into two components: first, a mean shopping cost, which is common to all consumers; and second, an idiosyncratic deviation from the mean cost, which depends on both observed demographic characteristics and a random shock, which is known to consumers and assumed to follow a known parametric distribution. This shock captures all individual (unobserved) characteristics that may cause individual costs to differ from the average shopping cost.====Once the parameters of the model are estimated, we perform two exercises that allow us to assess the relative importance of explicitly accounting for shopping costs in predicting reasonable substitution patterns in a multistop shopping environment. In the first exercise, we take our estimated model and simulate a scenario in which shopping costs fall to zero; that is, we assume that consumers no longer incur positive shopping costs. In the second exercise, we estimate an alternative specification that does not include any shopping costs, and compare the results with those obtained from our preferred specification with shopping costs.====Furthermore, we apply our models of demand and supply to empirically examine some of the implications of two practices that are commonly used by supermarkets: ==== and ==== pricing. The first case arises when a supermarket removes a particular product from its shelves while rivals keep supplying it. Although it can happen purely for commercial reasons (Davies, 1994), it can also be used by the supermarket in a strategic way to exploit manufacturers. In any case, delisting of products can entail losses for the delisting supermarket if a high proportion of its customers face high shopping costs.==== We simulate a large price increase for one of the products sold by a given supermarket so that it becomes prohibitively costly for consumers, while the same product continues being supplied by competing supermarkets at observed prices. The second case, which involves setting the price of certain products at or below its retail marginal cost (Lal and Matutes, 1994), often comes as a result of aggressive temporary price discounts. Even though this practice has been banned in a number of countries because it can be predatory, recent literature shows that in the presence of shopping costs loss-leading strategies and cross-subsidies are not predatory (see Chen, Rey, 2012, Chen, Rey, 2019).==== We simulate a situation in which one supermarket sets the price of one product at its marginal cost which turns out to be a discount of nearly 56% on the observed price. In both applications, we measure the net effect of the respective price change on demand and supply by allowing rival supermarkets to adjust their prices to a new equilibrium. For the sake of documenting the role of shopping costs, we perform two counterfactual experiments: one in which shopping costs are present and another in which we set shopping costs to zero for all consumers.====Perhaps the most significant limitation of our approach is the dimensionality problem that arises when estimating demand for both baskets of products and multiple shopping locations. In our data set, we observe that a household purchases baskets containing 24 different products from two separate stores each week, on average, while some households can purchase up to 275 different products from up to nine separate grocery stores in the same week. We deal with the potential dimensionality problem as follows. First, we restrict our focus to three categories of products that are staple food items, among the most frequently purchased, and usually subject to unit demand. In our case, these categories are yogurt, biscuits, and refrigerated desserts. Second, we aggregate brands to the category level so that we end up with a reduced set of composite products. To flexibly evaluate the effects of product delisting and loss-leading, we allow for two alternatives of yogurt, namely, the leading national brand (NB) in France in 2005, and a composite yogurt “brand” that includes all of the remaining alternatives. Therefore, consumers have a set of four products from which they can choose at most three: one of the two alternatives of yogurt, biscuits, and desserts.==== Finally, we focus on a reduced set of three supermarket chains that were the leading grocery retailers in France in 2005 based on market share. The remaining supermarkets in our data set, along with the no purchase of the included goods option, are left as part of the outside good.====We obtain several interesting results. First, from descriptive regressions, we find a significant relationship between the number of supermarkets visited by a household in a week and household characteristics that are a proxy for the opportunity cost of time. Second, our structural model allows us to retrieve consumer total shopping costs, which we estimate to be 104.7 euro cents per supermarket visited, on average. This cost includes a fixed shopping cost of 96.1 euro cents per supermarket visited and a transport cost of 8.6 euro cents per trip to a supermarket located at the average distance from consumer location. Third, when we unilaterally set shopping costs to zero, our simulations indicate that all consumers would visit at least one store every week with positive probability and, in particular, the probability of doing multistop shopping is similar to that of doing one-stop shopping. Once shopping costs are accounted for, the predicted probabilities of both one- and multistop shopping are lower, and consumers are less likely to visit a supermarket on a week-to-week basis. Fourth, when we compare the substitution patterns predicted by an alternative specification without shopping costs with those of our model, we find that shopping costs reinforce the complementarities between product categories that emerge when customers are able to purchase baskets of products. Such complementarities can be thought of as the “economies of scope” of buying related products from a single supermarket, as discussed by Klemperer (1992). In fact, the cross-price elasticities predicted by our model are larger than those of the alternative specification, suggesting that shopping costs make those economies of scope more valuable to consumers.====Further, from the delisting application we find that the supermarket delisting the product strategically decreases the price of substitutes to try to encourage intra-store substitution. Conversely, we find that the price of complement products increases and that this increase offsets the decrease in the price of the substitute product, which suggests that an optimal strategy for the supermarket delisting the product is to keep the overall value of the basket constant and retain one-stop shoppers. Yet, the high shopping costs faced by one-stop shoppers implies that the delisting supermarket losses some demand to rivals and experiences a decrease in its revenues. Finally, from the aggressive discounting application, we find that the supermarket granting the discount increases the price of complement products up to a 10% and that the extent to which it is able to compensate the lower revenues on the loss leader depends on shopping costs. When shopping costs are not present, the supermarket giving the aggressive discount still increases the price of complements but up to 2%. The aggressive price discount generates a considerable increase in the demand for the loss leader whereas the demand for all other products of the supermarket giving the discount decreases. Rival supermarkets also experience a decrease in the demand for all of their products and the share of the outside good also decreases. Our results suggest that the loss-leading strategy stimulates some multistop shopping and a market expansion effect for the loss leader in addition to the inter-store substitution effect.====This paper relates to a growing body of empirical literature that models consumer choice problems explicitly accounting for opportunity costs associated with shopping activities. This literature has focused on two types of costs, namely, search costs (e.g., Hortaçsu and Syverson (2014), Hong and Shum (2006), Koulayiev (2014), Moraga-Gonzalez et al. (2013), Kim and Bronnenberg (2010), Wildenbeest (2011), De los Santos et al. (2012), Honka (2014), and Dubois and Perrone (2015)), and switching costs (e.g., Shy (2002), Viard (2007), and Honka (2014)). Less attention has been paid to shopping costs. Brief (1967) models consumer shopping patterns in a Hotelling framework, and estimates transportation costs to account for consumer shopping costs. Aguiar and Hurst (2007) evaluate how households substitute time for money by optimally combining shopping activities with home production. Customers incur a time cost of shopping that is explicitly accounted for.====This paper also relates to literature that has developed demand models for multiple products. Hendel (1999) develops a multiple-discrete choice model to explain how firms choose multiple alternative brands of personal computers. Further, Dubé (2004) applies Hendel’s model to the case of carbonated soft drinks. Gentzkow (2007) develops a flexible framework in which similar products can be either substitutes or complements. None of these studies incorporate consumer transaction costs into the choice problem. Wildenbeest (2011) sets out a search cost model in which consumers want to purchase a basket of products in a single stop and care about the total price of the basket.====Finally, in analyzing multiproduct and multistore choice with shopping costs, our paper is closely related to that of Thomassen et al. (2017), who study pricing by grocery stores in the context of competition between specialized stores and supermarkets. To this end, they develop a model of demand in which consumers make discrete–continuous choices over multiple categories. Consumers can purchase from up to two stores in each period and incur a choice-specific fixed cost of shopping. Our approach, which we have developed contemporaneously and independently, differs from theirs in several important ways. First, our primary focus is on the role of shopping costs in predicting consumer substitution and shopping patterns. Second, we use our model to explore the effects of product delisting and loss-leader pricing, and though we add structure to the supply side in order to present it in a more realistic way, our main focus is on the demand side effects of such practices. Third, our empirical analysis is oriented by a model that is in line with the theoretical literature on multiproduct retailing with shopping costs. In our setting, the number of stores visited by a consumer is endogenously determined by a stopping rule involving the extra utility and extra costs involved in visiting an additional store. This enables us to empirically identify the distribution of shopping costs. Last but not least, while we are interested in analyzing the competition between supermarket chains of similar size and characteristics that supply the same product range to customers, they focus on competition between small specialized stores and large supermarkets.====The rest of the paper is organized as follows. Section 2 outlines our structural model of multiproduct demand and consumer shopping behavior in the presence of shopping costs, as well as a supply model of supermarket oligopolistic competition. Section 3 describes the data and presents a preliminary analysis of consumer shopping behavior, our empirical strategy and discusses identification. Section 4 reports the estimation results and describes the role of shopping costs in predicting substitution patterns and supermarket markups and marginal costs by comparing the market equilibrium with shopping costs with the counterfactual in which shopping costs are set to zero. We then present a robustness analysis in which shopping patterns are defined in a different way, which gives rise to an alternative way of quantifying shopping costs. Section 5 presents and discusses the results of the applications on product delisting and loss-leader pricing. Finally, Section 6 concludes and discusses possible directions for further research.",Multiproduct retailing and consumer shopping behavior: The role of shopping costs,https://www.sciencedirect.com/science/article/pii/S0167718719300888,22 November 2019,2019,Research Article,210.0
"Brueckner Jan K.,Flores-Fillol Ricardo","Department of Economics, University of California, Irvine, United States,Departament d’Economia and CREIP, Universitat Rovira i Virgili, Spain","Received 16 June 2019, Revised 1 November 2019, Accepted 6 November 2019, Available online 22 November 2019, Version of Record 9 December 2019.",https://doi.org/10.1016/j.ijindorg.2019.102557,Cited by (21),"This paper explores the effect of market structure on quality determination for complementary products. The focus is on the airline ==== and the effect of airline alliances on flight frequency, an important element of service quality. With zero layover cost, the choice of flight frequencies has the same double-marginalization structure as in the usual alliance model, leading to a higher frequency in the alliance case as double marginalization is eliminated, along with a lower full trip price and higher traffic. The surprising result of the paper emerges with high-cost layover time, where double marginalization in frequencies is absent and where an alliance reduces service quality via a lower frequency, with the full price potentially rising (in which case traffic falls).","Analysis of the choice of product quality has a long history in industrial organization. In early work, Spence (1975) and Sheshinski (1976) showed that the product quality chosen by a monopolist can be higher or lower than the socially optimal quality, with the outcome depending on the shape of the demand curve. Swan (1970) and Levhari and Peles (1973) demonstrated, however, that if quality takes the form of product durability, then a monopolist and a competitive firm choose the same quality level, with the monopoly choice thus being socially optimal even though quantity is too low. Much subsequent research was devoted to exploring the robustness of this finding, with Schmalensee (1979) providing a survey.====Despite this attention to product-quality issues, the literature has mostly overlooked a highly relevant question: how does market structure affect quality determination for complementary products? For such products, consumption of one of the goods enhances the benefits from consumption of the other. As is well known, pricing of complementary goods by separate producers is inefficient due to (horizontal) double marginalization, with prices lower when both goods are produced by a single firm. When the qualities of the two complementary goods must also be chosen, production by separate firms widens the scope for double marginalization, which now affects quality as well as price. The intriguing question is then whether a change in market structure from separate to single-firm production has beneficial effects on prices and qualities. In other words, does a single producer offer both lower prices and higher qualities than separate firms in providing the complementary goods? Economides (1999) offers the only study of this question and reaches an affirmative answer, although his price conclusion depends on functional forms.====Given the importance of the link between market structure and complementary-product quality and its slender treatment in the previous literature, further research is clearly needed. Accordingly, the present paper offers a new analysis, one that is grounded in a particular industry where a noteworthy type of product complementarity exists that guides the formulation of the model. The industry is airlines, and the complementarity arises in trips that require the use of two different carriers. In particular, suppose that travel from city X to city Y cannot be carried out using a single airline. Instead, the passenger must first fly on airline 1, which provides service from city X to an intermediate airport H, then switching to airline 2, which provides service from H to Y. The two complementary products are then travel from X to H and travel from H to Y, and they represent perfect complements that must both be consumed in making a two-airline XY trip.====Historically, such two-airline trips were provided by individual carriers operating at arm’s length with minimal coordination. In the early 1990s, however, a market structure revolution occurred: international airline alliances came into being, with the goal of making two-carrier trips more like travel on a single airline. With a grant of antitrust immunity, alliances gained the ability to coordinate in setting fares and flight frequencies for such two-airline trips as if they were a single firm (before, airlines would set a separate ==== for their own portions of the trip and could not coordinate on schedules). The result was the elimination of double marginalization in fare setting, which characterized the pre-alliance world, leading to lower fares for XY-type trips.====The purpose of the paper is to analyze the effect of this market-structure change on the choice of airline ====, as captured by the ==== offered by the collaborating carriers. Although service quality includes many different dimensions on the ground (bag handling, gate location) and in the air (in-flight services, legroom, seat characteristics), higher flight frequency is an important quality attribute since it reduces schedule delay, defined as the gap between a passenger’s preferred and actual arrival times.==== With more frequent flights, a passenger can arrive closer to his or her preferred time.==== The question addressed by the paper is thus: what happens to the two carriers’ flight frequencies on the XH and HY routes, and to the XY fare they charge, when the airlines become alliance partners? Does the XY fare beneficially fall and do flight frequencies beneficially rise? Or are other outcomes possible? As will be seen, the answer depends on aspects of passenger preferences.====The paper therefore provides what is, in effect, a theoretical case study of the impact of market structure on product quality, focusing on a particular industry where a specific type of product complementarity exists. The results are more broadly useful, however, given the sparsity of research on this question. The paper simultaneously advances the theoretical literature on airline alliances (partly built on Brueckner, 2001) and an associated empirical literature, both have which have solely focused on price effects (with the empirical literature confirming the predicted XY fare reduction).====To capture service quality, the passenger demand function in the model incorporates aversion to both schedule delay, as discussed above, and connecting layover time.==== This disutility depends on the flight frequencies of the two carriers. When the frequencies are equal ex ante, as in the case of an alliance (which sets common values), layover time is zero and schedule delay is proportional to the reciprocal of the common frequency, as in the previous models that ignore flight connections. But when the carriers are nonaligned and choose their frequencies independently, the values differ ex ante, so that airline 1’s arrivals at H are potentially mismatched with airline 2’s departures, requiring a layover. The challenge, then, is to derive an expression for passenger disutility from schedule delay and layover time in the no-alliance case, where frequencies differ ex ante. This derivation is a major contribution of the paper.====In order to show how the particular form of complementarity affects the analytical conclusions, we carry out the required derivation under two different polar-case assumptions that turn out to affect complementarity: a zero cost for layover time and a layover cost high enough so that passengers completely avoid layovers. The first case could apply to leisure passengers and second to business passengers. In both cases, the ex-post layover cost is zero, either by assumption (first case) or by choice (second case), so that passenger disutility (denoted ====) consists entirely of schedule-delay cost. With zero-cost layover time, we show that ==== is proportional to the sum of the reciprocals of the individual carrier flight frequencies, so that the qualities of both products matter to consumers in a standard way. This disutility expression is added to the sum of the subfares charged by the carriers for their portions of the XY trip, leading to the full trip price. With this additive structure for both subfares and frequencies, we might expect that the standard double marginalization result on fares could also extend to frequencies, with frequency rising and the overall fare falling in moving from the no-alliance to the alliance case.====Alternatively, with high-cost layover time, passengers choose flights whose arrival and departure times at the hub coincide so as to avoid any layover. In this case, we show that ==== is proportional to the reciprocal of the ==== of the two airline frequencies, so that only the one of the two product qualities matters despite complementarity. With the additive structure eliminated, we would no longer expect double marginalization in the choices of frequencies by nonaligned carriers. Exploration of this alternate case thus provides another perspective on product-quality determination for complementary goods, with consumers just caring about the quality of one good. This is the good with the lower quality, namely, the service offered by the airline with the lower of the two flight frequencies.====The results of the paper can be summarized as follows. With zero-cost layover time, an alliance raises flight frequency relative to the no-alliance case, in line with the predicted double marginalization story. Interestingly, however, the same conclusion need not apply to fares, with the overall fare being either higher or lower than in the non-alliance case. However, an alliance does beneficially reduce the full trip price (fare plus schedule-delay cost), thus yielding the same increase in traffic as in the standard model.====With high-cost layover time, an alliance reduces the overall fare, as in the standard model. But since the high-cost case does not exhibit the double-marginalization structure of the low-cost case with respect to frequencies, the opposite frequency impact occurs, with an alliance leading to a reduction in flight frequency. Because of lower frequency, the full trip price can either rise or fall, so that an alliance could lead to a reduction in traffic, in a surprising reversal of the standard result. The upshot is that, when a service-quality dimension involving flight frequencies is added to an alliance model, the conclusions it generates may be unfamiliar. More generally, these results show that, when the quality of only one of two complementary goods matters to consumers, single-firm production may lead to unexpected effects.====Our findings can be applied to analyze the effect of airline alliances on service-quality heterogeneity across business and leisure markets. The different values of time for these two passenger types affects their attitudes toward layover time and schedule delay. This difference generates a service-quality gap between business and leisure markets in the no-alliance case, with frequencies being higher in business markets. After an alliance is formed, the flight-frequency difference between the two types of markets narrows but does not disappear.====Finally, we add hub-airport congestion to the model. The literature on airport congestion shows that airlines internalize the congestion they impose on themselves, neglecting the congestion they impose on other airlines. Therefore, given that allied carriers will take into account the congestion imposed on their partners, alliances are expected to increase the extent of internalization. Since this internalization effect puts downward pressure on frequencies in the alliance case, it is conceivable that the positive alliance frequency effect in the absence of congestion (with zero-cost layovers) could be reversed when congestion is present. The results of the analysis show, however, that this conjecture is not upheld, with an alliance still leading to higher frequency and service quality when layover cost is zero.====The present paper complements the closely related work of Czerny et al. (2016), which explores the effects of carrier cooperation on service frequency in a general model that can be interpreted as applying to airlines. In addition to being simpler, our model differs by providing microfoundations that translate individual flight frequencies into overall service quality (as described above) rather than relying on an abstract general relationship.====In order to orient the reader to the question being studied, Section 2 of the paper provides an analysis of the effect of market structure on product quality for complementary goods that is more general than the subsequent airline analysis as well as more general than that of Economides (1999). The results show that the impact is ambiguous and depends on the form of preferences, in particular the sign of a demand cross-derivative. The analysis then proceeds to the airline model, showing again that the market structure impact is sensitive to assumptions on preferences, this time on the passenger’s attitudes toward schedule delay and layover time. The analysis therefore does not resolve the general ambiguity of the market-structure effect, but it provides specific results for a particular and important industry context. Section 3 presents the setup for the airline model and derives the service-quality expression in the cases of zero-cost and high-cost layover time. Section 4 derives the conditions for profit-maximization in the no-alliance and alliance cases and provides a general analysis of alliance effects. Section 5 derives results under the assumption that demand is linear. Section 6 considers the effects of congestion, and Section 7 offers conclusions.",Market structure and quality determination for complementary products: Alliances and service quality in the airline industry,https://www.sciencedirect.com/science/article/pii/S0167718719300852,22 November 2019,2019,Research Article,211.0
Mertens Matthias,"Halle Institute for Economic Research (IWH) and the Competitiveness Research Network (CompNet), Kleine Märkerstraße 08, 06108 Halle, Germany","Received 15 March 2019, Revised 23 September 2019, Accepted 12 November 2019, Available online 18 November 2019, Version of Record 3 December 2019.",https://doi.org/10.1016/j.ijindorg.2019.102562,Cited by (20),"This article examines how final product trade with China shapes and interacts with labor market imperfections that create market power in labor markets and prevent an efficient market outcome. I develop a framework for measuring such labor market power distortions in monetary terms and document large degrees of these distortions in Germany's manufacturing sector. Import competition only exerts labor market disciplining effects if firms, rather than employees, possess labor market power. Otherwise, increasing export demand and import competition both fortify existing distortions, which decreases labor ====. This widens the gap between potential and realized output and thus diminishes classical gains from trade."," creates profound challenges for firms operating in the market economy. Global integration has increased the size of firms’ product markets and the amount of their competitors, while global production networks and dramatically falling transportation costs redefine the nature of production activities. How firms respond to these new market conditions has fundamental implications for domestic workers, productivity levels, and living standards.====Traditionally, most research studying firm responses to trade exposure relies on perfectly functioning labor markets. By design, this limits the analyses to scenarios where wages are always on competitive levels and where firms do fully pass gains and losses from trade exposure through to labor expenditure adjustments. Recent work, however, has raised awareness to the role of imperfect functioning labor markets for understanding firms’ responses to trade exposure: By affecting how firms adjust to changes in product and input market conditions, labor market imperfections may alter distributional outcomes from trade and may change aggregate trade gains compared to a baseline scenario with competitive labor markets (e.g. Egger and Kreickemeier, 2009; Kambourov, 2009; Dix-Carneiro, 2014). Therefore, understanding how international trade interacts with labor market imperfections has a first order priority in evaluating welfare effects and distributional impacts from trade.====In this article, I contribute to this understanding by using a simple micro-econometric partial equilibrium framework to study how final product trade causally affects and interacts with labor market power distortions in the German manufacturing sector. The production side framework of this article does not depend on specific product market demand characteristics and identifies distortions in labor markets by firm-level wedges between workers’ output contributions and wages.==== The existence of such wedges reflects market power in labor markets that affects distributional outcomes and signals market inefficiencies that decrease aggregate output compared to a scenario with competitive labor markets (Petrin and Sivadasan, 2013).====Intuitively, final product trade has the potential to affect and interact with labor market power distortions through various channels: Trade influences firms’ labor demand and gives an impetus for reorganizing existing structures within firms as well as for reallocating labor between firms (Caliendo and Rossi-Hansberg, 2012; Mayer et al., 2014). On the other hand, existing labor market power distortions create adjustment barriers (e.g. employment protection agreements) and influence rent sharing processes between firms and employees. This might determine how firms adjust their labor expenses when being exposed to foreign competition and demand. Yet, how international trade influences labor market imperfections, to what extent prevalent labor market power distortions determine distributional outcomes from trade, and whether trade exposure can function as a disciplining tool for distorted labor markets remain open empirical questions that this study answers.====While doing so, this article adds two new insights to the literature. First, it presents new evidence on the causal effect of final product trade on firms’ labor market power. This contributes to our understanding on how trade related changes in product market conditions influence rent sharing processes between employees and their firms. Second, this study presents first empirical results on the causal effect of final product trade on market inefficiencies emerging from imperfect labor markets. This offers insights on potential gains and losses from trade, and more generally, from changes in product market competition and demand, in terms of labor market efficiency – a topic on which our knowledge is rather limited, so far.====My main results document that an increase in export demand strengthens the labor market power of firms, whereas final product import competition increases employees’ labor market power. When uncovering the mechanisms behind these effects, I find that existing structures of labor market power prevent a complete adjustment of firms’ labor expenses. Firms with labor market power do not fully pass export profit gains through to workers, whereas firms facing a workforce with positive labor market power cannot fully adjust to import competition by shrinking or lowering wages. These incomplete pass-through processes increase existing labor market distortions and, therefore, decrease the efficiency of labor markets. Hence, due to imperfect labor market adjustments, final product trade can increase gaps between realized and potential output, which prevents a full realization of classical gains from trade under counterfactually competitive labor markets. In addition, I find that import competition might exert labor market disciplining effects, if firms rather than employees possess labor market power. Yet, these disciplining effects are sensitive to the empirical specification.====To conduct my analysis, I use administrative firm-product-level data for the German manufacturing sector. I exploit the eight-digit product-level information in this data to calculate exceptionally fine measures of final product import competition and export opportunities for each individual firm. Measuring trade flows at the firm rather than the industry level reduces mismeasurement in the explanatory variables, creates additional identifying variation, and accounts for multi-product firms being active in multiple industries. In line with most of the literature, the analysis of this article focuses on trade with China, whose unexpected and rapid rise to dominance in the global market constitutes an epochal shift in product market conditions for firms throughout the world (especially in the manufacturing sector) and offers an excellent playing field to study how international trade effects domestic firms and markets (Autor et al., 2016). To draw causal inferences, I instrument my trade measures in the spirit of Autor et al., (2013) and Dauth et al. (2014) by using trade flows between China and countries similar to Germany.====This study connects to the recent discussion on the prevalence and causes of high and rising market power put forward by De Loecker and Eeckhout (2018) and De Loecker et al. (2018). Much of the current discussion on the extent of firm market power focuses on product markets. In contrast, this study belongs to a fast-growing literature that investigates the extent of market power on labor markets as an alternative source of firm market power (e.g. Ashenfelter et al., (2010); Azar et al., (2017); Berger et al. 2019). There are growing concerns that labor market power creates substantial welfare losses that are comparable or even larger than welfare losses from product market power (Naidu et al., 2018; Berger et al., 2019). In response, recent research in economic law calls for extending existing antitrust regulations, which currently mostly focus on preventing excessive product market power, to address the prevalence of market power in labor markets (e.g. Marinescu and Posner, 2018). With this study I contribute to this young literature, by offering evidence on final product trade as a determinant of labor market power. While most of the recent work on labor market power emphasizes the presence of monopsonistic firm market power, I show that worker-side labor market power is also a key factor in shaping firm responses to changes in product competition and demand. In particular, I find that the existence of employee labor market power, similar to the existence of firm labor market power, creates efficiency losses from (trade related) changes in product market conditions.====This study also ties into the literature investigating how international trade affects wage bargaining processes. Rodrik (1997) already noted that imported products substitute domestic with foreign workers, weakening the position of the former within the firm. Moreover, Hornstein et al. (2005) provide evidence that competitive pressure may lead to deunionisation. Most closely related to this paper, Boulhol et al. (2011) find a negative impact of imports from developed countries on workers’ bargaining power for the UK, while Nesta and Schiavo (2018), by focusing on the subset of firms within an efficient bargaining regime, find the same for imports from China and OECD countries in the case of France. Similarly, Ahsan and Mitra (2014) document that a reduction in output tariffs is associated with a decrease in workers’ bargaining power for India. However, my study complements all mentioned contributions in several aspects. First, in contrast to this study, existing work focuses on distributional aspects and does not investigate the causal link between labor market efficiency and international trade. Second, I do not restrict my analysis to import competition. In fact, I find that labor market distortions react several times stronger to an increase in foreign demand than to an increase in import competition. Third, my results show that final product trade interacts with existing structures of labor market distortions and tends to fortify prevalent labor market power levels. This is reflected in a widening of existing positive and negative firm-level gaps between marginal products and wages and is exactly the source of losses in terms of labor market efficiency from final product trade.====Finally, this article complements recent work discussing how incomplete pass-through processes of trade related productivity gains to consumer prices give rise to output market distortions. De Loecker et al. (2016, henceforth DLGKP) find that Indian firms do not fully pass productivity gains from cheaper imported intermediate products through to consumer prices, increasing firm-markups. Arkolakis et al. (2018) show that under non-homotheticity in preferences it is unclear whether trade integration increases or decreases output market distortions. Weinberger (2017) illustrates this by incorporating a possible non-optimal market share reallocation into the Melitz (2003) model. In his model, heterogeneous output market power allows firms to heterogeneously pass productivity gains from cheaper imported inputs through to consumer prices. Through this mechanism, more productive firms increase their markups relatively more, which reallocates production to the less efficient firms, giving rise to misallocation.====My study transfers these findings for output market distortions to labor markets. Closely related to this literature, I find that the underlying mechanism giving rise to labor market distorting effects from trade is based on an incomplete pass-through of firm profit changes to workforce adjustments. That final product trade has the potential to worsen the efficiency of labor markets is an alarming finding, as it implies that models assuming competitive labor markets might overestimate the gains from trade.====The remainder proceeds as follows. Section 2 describes the data and explains the construction of trade measures. Section 3 derives the framework for measuring labor market power distortions. Section 4 presents the empirical results. Section 5 concludes.",Labor market power and the distorting effects of international trade,https://www.sciencedirect.com/science/article/pii/S0167718719300906,18 November 2019,2019,Research Article,213.0
Ulsaker Simen A.,"Norwegian School of Economics, Norway","Received 21 December 2017, Revised 30 October 2019, Accepted 1 November 2019, Available online 7 November 2019, Version of Record 27 November 2019.",https://doi.org/10.1016/j.ijindorg.2019.102556,Cited by (3),"The article illustrates how a seller profitably can prevent entry of a potential competitor, even when entry would increase ==== profit. Entry is prevented by offering exclusive contracts to the buyers. The buyers are assumed to be differentiated firms, competing in a downstream market. Exclusion occurs in equilibrium as long as there is some degree of competition among the downstream firms, and even when there are no economies of scale in upstream production.","Why would a buyer agree to sign an exclusive contract with a seller, if this precludes it from obtaining a valuable product from a potential entrant? The present article illustrates that when an established seller supplies two buyers that compete in a downstream market, the entry of an additional seller can reduce the joint profit of the buyers and the incumbent seller. When buyers compete, a buyer’s flow profit gain from buying from the entrant is partly the other buyer’s loss. The existence of these externalities among the buyers can enable the entrant to extract more profit from the industry than its contribution, something that would give the buyers and the incumbent a joint incentive to exclude the potential entrant. The article shows how and when the incumbent can profitably convince the buyers to sign exclusive contracts and thereby exclude the entrant.====As pointed out by several authors, it is not obvious how an incumbent seller would find it profitable to sufficiently compensate its buyers to convince them not to trade with a more efficient seller.==== A possible explanation is that economies of scale imply that the entrant will only enter if it is allowed to sell to several buyers. By exploiting coordination failures among the buyers or by making discriminatory offers, the incumbent may be able to convince a sufficient number of buyers to sign exclusive contracts, even if the buyers would jointly be better off if entry occurred. Support for this explanation is given by Rasmusen et al. (1991) and Segal and Whinston (2000), who model an industry where the upstream firms offer identical products, and where the buyers are end-users (or firms operating in independent downstream markets).====Fumagalli and Motta (2006) were among the first to consider exclusive contracts with competing buyers. They model an industry in which two identical buyers compete in prices in a downstream market. As in Rasmusen et al. (1991) and Segal and Whinston (2000), the upstream firms produce identical products. Fixed costs in the retail market make it prohibitively expensive for the incumbent to exclude the entrant, and entry occurs in equilibrium. The reason is that if one buyer has signed an exclusive contract, the other has much to gain by refusing to do so, and being the only buyer free to trade with the more efficient entrant.====Abito and Wright (2008) qualify the findings of Fumagalli and Motta (2006).==== By removing the assumption of fixed costs of retailing, they illustrate how downstream competition can make exclusion a more, and not a less, likely outcome. Under exclusion, the incumbent can monopolize the downstream market by offering appropriate contracts to the downstream firms. When there is entry, competition between the upstream firms will drive the input prices down. It follows that when the downstream firms compete fiercely, total profit under entry can be very low. The incumbent may therefore be able to sufficiently compensate both buyers and exclude the entrant.====Wright (2008) and Argenton (2010) consider the case where the upstream firms offer differentiated products. Wright (2008) models an industry where the upstream firms are horizontally differentiated, and where the downstream firms are identical and compete in prices. In this setting, exclusion can be profitable for similar reasons as the ones highlighted by Abito and Wright (2008): Exclusion leads to less competitive pricing and higher industry profit. Argenton (2010) studies a similar framework, but assumes that the upstream firms’ products are vertically differentiated. He also finds that competition among the downstream firms facilitates exclusion. Under entry, the presence of the incumbent’s inferior product would exercise competitive pressure, reducing the profit available through the sale of the entrant’s product. As a result, the incumbent is able to sign exclusive contracts with both downstream firms.====I model an industry where two upstream firms offer differentiated products, and where the two buyers are differentiated competitors in a downstream market. A maintained assumption is that the fixed cost of entry is such that absent exclusive contracts, the entrant would always enter and industry profit would increase. The incumbent is still able to profitably exclude the entrant without relying on miscoordination among the buyers or discriminatory exclusive contracts, as long as there is some degree of competition among the downstream buyers.====In the baseline model, each buyer demands one unit of an input that comes in two types: a low-quality version produced by an incumbent seller and a high-quality version produced by a potential entrant. The profit of a downstream firm increases in its own input quality, and decreases in the rival downstream firm’s input quality. While industry profit is greater when both downstream firms buy from the entrant, exclusion will be jointly profitable for the two downstream firms and the incumbent. This is not because exclusion allows them to dampen competition, but because, in the absence of exclusive contracts, the entrant can exploit the fact that the buyers compete to extract higher payments from the downstream firms than what it “contributes” to the industry profit. The underlying intuition is simple. A downstream firm’s willingness to pay to buy the entrant’s product (rather than the incumbent’s) is the profit gain such a purchase would bring. Part of this profit gain is however the rival downstream firm’s loss.====Externalities among the buyers are what enable the entrant to extract more of the industry profit than its contribution. These externalities are also what enable the incumbent to profitably exclude the entrant without relying on discriminatory contracts or coordination failures among the buyers: Because the payments the entrant extracts exceed its contribution to the industry profit when both downstream firms are free to buy from it, the incumbent can offer exclusive contracts that compensate both downstream firms according to what they can expect to earn under entry, and extract strictly more than this from the downstream firms after entry is prevented.====The baseline model is highly stylized. This allows me to obtain precise predictions about equilibrium outcomes and to easily illustrate the underlying intuitions of the main results. The mechanisms behind the main results are however not driven by the particular restrictions of the baseline model. To illustrate this I also consider a more general setting where each downstream firm may find it profitable to use the products of both upstream firms, and where I do not restrict attention to unit demand.====The main results from the baseline model carry over to this setting. In any equilibrium in which both upstream firms sell to both downstream firms, the entrant extracts more of the industry profit than its contribution. Consequently, whenever the failure to sign exclusive contracts with the downstream firms would bring about such an equilibrium, and the entry cost is such that the entrant will have to be free to sell to both downstream firms to cover its fixed entry costs, there will exist equilibria in which both downstream firms sign exclusive contracts with the incumbent. In such exclusionary equilibria the joint payoff of the incumbent and the downstream firms is strictly greater than in any entry equilibrium in which both downstream firms buy from both upstream firms. The underlying intuition is the same as in the baseline model. A downstream firm’s gain from buying from the entrant (and not only from the incumbent) is partly the rival downstream firm’s (and the incumbent’s) loss. The entrant thus extracts a higher share of the industry profit than its contribution, and the entrant can fully compensate both downstream firms and still gain from excluding the entrant.====The rest of the article is organized as follows. Section 2 introduces the framework of the baseline model. In Section 3 a linear demand example that satisfies the assumptions of the baseline model is considered. Some preliminary results are derived in Section 4. Section 5 examines the effect of letting the incumbent offer exclusive contracts. Section 6 relaxes some of the assumptions of the baseline model to consider a more general setting. Section 7 concludes.","Competing buyers, rent extraction and inefficient exclusion",https://www.sciencedirect.com/science/article/pii/S0167718719300840,7 November 2019,2019,Research Article,214.0
"Chorniy Anna,Miller Daniel,Tang Tilan","Feinberg School of Medicine, Northwestern University, 420 E Superior St., Chicago, IL 60646, USA,Clemson University, USA,Temple University, USA","Received 16 August 2018, Revised 1 October 2019, Accepted 18 October 2019, Available online 3 November 2019, Version of Record 18 November 2019.",https://doi.org/10.1016/j.ijindorg.2019.102548,Cited by (4),"We empirically examine horizontal mergers amongst Part D insurers with the aim of assessing how market power, cost efficiencies, and bargaining power affect premiums and coverage characteristics, including drug access and out-of-pocket (OOP) cost. Our results reveal that market power raises premiums, but this is only a local effect that occurs in markets where the merging firms overlap. ==== alter the bargaining process with upstream suppliers at both local and national levels, affecting drug access and OOP cost. We find evidence of cost efficiencies when firms restructure by consolidating their plan offerings.","The landscape of competition in the health insurance industry has experienced many changes in the past years, including the introduction of managed care plans in the 1980s, privatized Medicare plans, expanded prescription drug coverage, and most recently the reforms in the 2010 Patient Protection and Affordable Care Act. Throughout this period there have been waves of merger and acquisition (M&A) activity as insurers adapted to the evolving marketplace (Park and Town, 2014).====In this paper, we examine the effect that horizontal M&A activity amongst health insurers has on prices and coverage characteristics of prescription drug plans offered in the Medicare Part D market. Part D established a regulated and subsidized insurance exchange for Medicare beneficiaries to purchase prescription drug coverage from competing private insurers. It is the largest health exchange in the U.S., insuring over 40 million individuals. Since the program’s inception in 2006, there have been more than a dozen large scale horizontal M&A deals involving the parent companies of insurers offering Part D plans. Twenty three of the top 25 Part D insurers have gone through at least one horizontal merger. Each year in our sample, M&A deals affect an average of 15% of all plans in the market.====Theory posits three major channels through which mergers affect markets. First, horizontal mergers increase market concentration which gives firms more market power. Reduced competition can lead to higher premiums for consumers or lower product quality if firms compete on quality dimensions. The program rules regulate general coverage parameters such as deductibles. However, Part D contracts vary considerably along other coverage dimensions that could be eroded by market power: namely, drug access (the comprehensiveness of formulary coverage) and drug pricing which determines the amount enrollees pay out-of-pocket (OOP) in copays.====Second, horizontal mergers offer benefits if they result in increased productive efficiency on national or local levels. In health insurance, efficiency gains can be achieved through scale economies that appear as insurers streamline their administrative and marketing activities. These cost have taken on a greater importance as new minimum loss ratios (MLR) in the Affordable Care Act require 85% of premium dollars to be spent on drug claims, leaving only 15% available for administrative and marketing expenses.====Third, horizontal mergers alter bargaining dynamics with upstream suppliers as the combined firm gains monopsony power. For health insurers the upstream suppliers are the providers of healthcare goods and services (doctors, hospitals, drug manufacturers, and pharmacies). With greater bargaining power, an insurer may be able to negotiate more favorable coverage terms and lower its cost. This merger effect is particularly important in Part D. The program relies heavily on the ability of private insurers to bargain with drug suppliers and pharmacies and explicitly prohibits the government from participating in negotiations (Duggan, Scott-Morton, 2010, Frank, Newhouse, 2008). Mergers could have a positive effect if the increased bargaining power allows insurers to increase the scope of covered drugs or negotiate lower drug acquisition costs, which can be passed to enrollees either directly through reduced cost sharing on drug copays or indirectly through lower insurance premiums.====These deals have come under the scrutiny of anti-trust authorities. They are tasked with determining whether the beneficial effects of mergers (cost efficiencies and bargaining power) in fact exist, and if so, whether they outweigh negative market power effects. Two major deals between Aetna and Humana and Anthem-Cigna were blocked by the court in early 2017 after years of virtually no anti-trust or regulatory action to block or restrain merging insurers. In the former, the judge specifically cited harm to Medicare consumers as one of the reasons. Stylized facts about Medicare Part D, indeed, give reason for concern. Table 3 shows that by 2012, just six years since the program’s inception, premiums increased by more than 26% in real terms. While the typical consumer still had many choices — an average of 25 plans available in each market in 2012 — there has been a drastic 31% decrease in the number of plan offerings (Table 4). Coverage has declined and drug costs have risen. The number of drug offerings on plans’ formularies has fallen by 31% and OOP costs paid by enrollees for the most popular drugs have nearly doubled (Table 4). The latest government projections forecast a 3.8% annualized per capita cost growth rate for the Part D program.==== Understanding whether mergers contribute to or thwart these glooming trends is critical for the viability of the program.====In our application to Medicare Part D, we analyze the effects that horizontal mergers have on market outcomes with the aim of shedding light on the interplay of the three channels through which M&As affect plans: cost efficiencies, bargaining power with upstream suppliers, and market power. Although our results do not fully attribute the effects to each channel, we develop a theoretical framework that provides a compelling interpretation of the empirical findings on the role of the three forces. We contribute to the health economics literature and, more broadly, to the literature on horizontal merger analyses in three ways.====First, we use rich panel data on all stand-alone Part D plans from 2006 to 2012 that include detailed product-level characteristics such as plan premiums and coverage parameters. We combine these data with all M&A deals consummated during this time period between insurers offering Part D plans, among other services. There are 10 mergers – a much larger sample of merger events than in many M&A articles.====These detailed data on plans also make our work stand-out conceptually. Commonly, the M&A literature focuses on price effects of mergers. We explicitly address the effects of mergers on multi-dimensional non-price plan characteristics — formularies, drug OOP costs, and measures of access — to flesh out a quite complicated bargaining process between insurers and suppliers in which coverage terms, beyond just drug prices, are being negotiated. Part D program data are also uniquely suitable for the analysis of the heterogeneous effects of mergers. The program rules draw market boundaries that allow us to distinguish overlapping from non-overlapping markets and document spillover effects and the interplay of national-level bargaining with drug manufacturers and local level bargaining with pharmacies.====Second, we write down a multi-lateral, multi-level bargaining game in the spirit of Ho and Lee (2017), in which insurers bargain with drug manufacturers and pharmacies over drug rebates, pricing, and access. Combining the model with rich panel data on plans and Part D institutional design (overlapping and non-overlapping markets), we can shed some light on the merger effects coming through bargaining channels.====Third, we provide suggestive evidence on local-level cost efficiencies. The Part D market has experienced not only a wave of mergers, but also a flurry of plan consolidations. Understanding whether they have similar effects to mergers is important for anti-trust and regulatory purposes. In 2010, CMS issued its first regulations to encourage insurers to consolidate low enrollment and “meaningfully” similar plans.==== Changes to the subsidy rules design in 2009 have also reduced insurers’ incentives to have many plans. If an insurer can realize the beneficial effects of mergers by organically consolidating its own plans without engaging in a merger with an outside firm, then there is a weaker case to be made in favor of mergers and stronger case for mandating reductions in the number of plans per insurer.====Empirically, to identify the treatment effect that M&A deals have on plans we use a differences-in-differences approach. First, we examine how plans of merging firm change in the year following a merger as compared to the control group of plans that do not undergo a merger in the previous year in that market. Due to the extent of mergers in this industry virtually all markets are impacted by at least one merger in any given year in our data. Therefore, our approach can not measure the full equilibrium effect of mergers and instead can be interpreted as the lower bound of the effects. The combined effect of all three channels reveals whether the beneficial effects of mergers outweigh the negative effects. For anti-trust purposes, this test provides perhaps the most important metric for evaluating merger outcomes. However, there are limitations; simply comparing outcomes of merged and non-merged plans is not informative about the magnitudes of the three competing effects. For example, if the results were to reveal no effect of mergers on premiums, that could indicate each of the three channels has zero effect or it could be indicative of large market power effects that are canceled out by equally large cost efficiency and bargaining power effects. Moreover, this test indicates nothing about whether the benefits of mergers can be achieved internally through plan consolidation nor does it provide guidance about how specific characteristics of a merger deal affect outcomes.====Second, we exploit variation in the market overlap of merging insurers to distinguish local merger effects from national merger effects. The majority of merger deals involve near-national insurers operating in many geographic markets delineated by state boundaries, but not necessarily all of them. While the overlapping markets are affected by all of the merger channels, non-overlapping markets, due to the program design, can experience changes only through spillover effects from overlapping markets and more nuanced interactive bargaining effects across Part D upstream suppliers: pharmacies and drug manufacturers.====Finally, we modify the differences-in-differences set up to distinguish plan consolidation from mergers. We are interested in comparing similarities and contrasting differences between consolidation and merger effects, which is useful for determining whether regulatory decisions should promote or discourage mergers or consolidation. We also test for synergy effects when merging firms consolidate plans.====We acknowledge that the difference-in-difference treatment effect of consolidation may be a less accurate measure of the causal impact than our results for the merger effect. This is due to the fact that consolidation events occur at the plan level, whereas mergers occur at the firm level. To support the analysis, we conduct two robustness exercises related to adverse selection and gaming of the subsidy design that recent studies on Part D (Polyakova, 2016, Decarolis, 2015) have shown to be significant factors in insurers’ decisions to consolidate plans.====Our results show that mergers have a strong market power effect on premiums that rise by an average of 5.2% across all market and 7.3% in markets in which the merging parties overlap. In overlapping markets, the premium rise reflects the combined effects of increased market power and cost savings that insurers bargain for in the form of rebates from drug manufacturers. In non-overlapping markets, we do not find evidence of large effecs. The premium declined slightly, by $0.69 (1.54%), although the point estimate is imprecise. Since a merger does not induce any change in market structure in non-overlapping markets, this outcome is a result of higher rebates achieved in overlapping markets spilling over to non-overlapping markets. However, our empirical estimates suggest that the gain obtained through national bargaining for drug rebates is rather small. Much higher premiums in overlapping markets indicate that rebate increases dwarf in comparison to merging insurers’ gain in market power and no sizable cost efficiencies are realized and/or passed to the enrollees through premiums.====On average, post-merger plans cover more drugs at negligibly higher OOP costs and somewhat higher level of restrictions (e.g. prior authorization). But similar to premiums, in non-overlapping markets the plan coverage outcomes are divergent from the outcomes in overlapping markets. In overlapping markets, there are nearly no gains in formulary comprehensiveness relative to non-merger plans and usage restrictions became more stringent. These results suggest that local market power not only raises premiums, but also negates benefits of mergers related to drug access. However, in overlapping markets, enrollees benefit from a reduction in drug OOP copay/coinsurance rates. In contrast, the drug basket cost rises and access improves in non-overlapping markets. The divergent results across markets and across outcomes (rebates, access, and drug OOP costs) highlight the nuanced interactions in the bargaining process between insurers, drug-suppliers, and pharmacies.====The results for plan consolidation stand in stark contrast to those for mergers. Premiums of consolidated plans decrease by an average of 9.6%, larger in magnitude than the price increase attributed to mergers. In other words, cost efficiencies arise through plan consolidation, not merger. The premium decrease can be primarily attributed to marketing/administrative cost efficiencies. We find little evidence of a bargaining power effect; coverage quality with respect to drug access and OOP costs decreases modestly. However, we find a very large effect on coverage when merging insurers engage in plan consolidation. All measures of drug coverage improve dramatically, suggesting that there exist synergies to restructuring plan offerings following a merge.====These results suggest that anti-trust authorities should scrutinize merger deals involving a large share of overlapping markets for market power and be skeptical of cost efficiency claims. They should weigh a trade-off between coverage access and drug OOP costs in their assessment of bargaining gains. Our suggestive evidence on the effects of plan consolidation points to a possibility of allaying these concerns by defining the rule on plan offerings that induce plan consolidation.","Mergers in Medicare Part D: Assessing market power, cost efficiencies, and bargaining power",https://www.sciencedirect.com/science/article/pii/S0167718719300761,3 November 2019,2019,Research Article,215.0
"Burani Nadia,Mantovani Andrea","Department of Economics, University of Bologna, Strada Maggiore 45, Bologna 40125, Italy","Received 17 July 2018, Revised 10 October 2019, Accepted 21 October 2019, Available online 1 November 2019, Version of Record 12 November 2019.",https://doi.org/10.1016/j.ijindorg.2019.102549,Cited by (1),"We consider a duopolistic market in which a green firm competes with a brown rival and each firm sells two quality-differentiated products. We study optimal non-linear contracts offered by the two firms when consumers: (i) Are privately informed about their willingness to pay for quality, and (ii) differ in their environmental consciousness. We characterize how consumers with different valuations for quality self-select into firms and show that the ranking of qualities, relative prices and profits all depend on the interplay between consumers’ valuations and firms’ cost heterogeneity. Interestingly, when consumers’ valuations for quality are relatively low, the brown firm does not offer a low-quality variety. This contrasts with the situation of full information, in which both firms commercialize a high- and a low-quality variety. Hence, the lack of information about consumers’ valuations may not only favor the green firm in terms of higher prices and profits, but also reduce the product range offered by the brown rival.","In recent years, environmental awareness and increased sensitivity towards socially responsible consumption has modified the purchasing behavior of many customers in different parts of the world. This shift of mindset has been driven by a combination of factors. The evolution of social norms has helped people realize that caring about the environment also means caring about their own health and that of their families and friends (see Ostrom, 2000, Carrigan, Attalla, 2001, Heffner, Kurani, Turrentine, 2007, Carlsson, Garcia, Löfgren, 2010, Deltas, Harrington, Khanna, 2013). This process has also been the result of informative campaigns carried out in combination with environmental education policies and initiatives (see van der Made, Schoonbeek, 2009, Sartzetakis, Xepapadeas, Petrakis, 2012, Kaufman, 2014, Allcott, Rogers, 2014). Examples of such campaigns and initiatives are widespread in different sectors, ranging from automotive and energy to food and textile.====For their part, firms have become increasingly responsive to the new consumers’ needs, expanding their range of strategies to include corporate social activities and green practices. Consequently, firms ended up competing across different dimensions of quality in order to attract consumers often characterized by heterogeneous willingness to pay for each dimension (Rosen, 1974). In terms of pure quality, products are typically differentiated along the vertical ladder depending on their intrinsic or hedonic attribute, which is related to the functionality of the product itself. However, goods may also differ in another important dimension of quality, depending on their environmental or social sustainability. For the ease of exposition, in our analysis we will refer to green and brown products (or, to be more precise, to goods delivered by a green and a brown firm), however our main results can be extended to include the distinction between products sold by socially responsible firms and by standard profit-maximizing enterprises. In order to increase customer satisfaction along every possible quality dimension, and ultimately to capture a bigger market share, many firms made their products environmentally-friendlier or adopted environmentally sustainable production processes, though these usually require additional costs.====Understanding the driving factors that affect consumers’ sorting across firms according to these different quality dimensions represents therefore a relevant research question. Indeed, while most of the literature focuses on competition between firms in the presence of a one-dimensional quality attribute, we depart from this approach and focus on the interplay between the respective costs of the different production technologies, the degree of consumers’ environmental consciousness, and the willingness to pay for intrinsic quality.====We also acknowledge that firms are often unable to collect precise information regarding the willingness to pay (henceforth WTP or, simply, valuation) of the customers they want to reach. For this reason, another important element of our analysis is related to the presence of asymmetric information. Firms are uninformed about both consumers’ WTP for intrinsic quality and their environmental consciousness. In particular, we assume that there are two types of consumers, who differ in terms of their WTP for intrinsic quality, and refer to them as high- and low-valuation consumers, respectively. We also assume that environmental consciousness is uniformly distributed across the same population of customers. The latter is interpreted as a ‘warm glow’ (see Andreoni, 1989) non-monetary benefit a consumer enjoys when purchasing (from the green firm) a good which embodies socially desirable attributes, independently of its intrinsic quality.====In order to formally address our research problem, we embed the Mussa and Rosen (1978) model of quality differentiation and monopolistic non-linear pricing into a richer setup in which there is strategic interaction between two firms, one of them being a green firm that further differentiates its products from the rival by offering environmentally-friendly varieties. We assume that producing green products entails higher costs. We are interested in optimal non-linear contracts offered by the two firms. In particular, following Rochet and Stole (2002), we let each firm offer a menu of at most two incentive-compatible contracts, contingent on consumers’ valuations for intrinsic quality. Consumers’ other attribute, namely environmental consciousness, does not enter the screening contracts but rather determines how consumers sort themselves between the brown and the green firm. The formal model is outlined in Section 2.",Non-linear pricing and conscious consumption,https://www.sciencedirect.com/science/article/pii/S0167718719300773,1 November 2019,2019,Research Article,216.0
"Aryal Gaurab,Gabrielli Maria F.","University of Virginia, United States,CONICET and Universidad Nacional de Cuyo","Received 30 September 2018, Revised 16 June 2019, Accepted 30 September 2019, Available online 18 October 2019, Version of Record 2 November 2019.",https://doi.org/10.1016/j.ijindorg.2019.102538,Cited by (5)," decreases; (v) but its distribution, across consumers, does not change.","Second-degree price discrimination is widely used by profit maximizing seller(s), with market power, when faced with consumers with heterogeneous preferences. Nonlinear pricing is an example of second-degree price discrimination where price is not strictly proportional to the quantity/quality of the good. For instance, in our data on Yellow Pages advertisements, each publisher offers size discounts to induce high-valued customers to choose bigger and colorful ads that are more expensive. Consequently, marginal prices and average prices differ (Fig. 2). The sellers also distort cheaper ads to make these ads less attractive to the high-valued consumers. The welfare effect of price discrimination is, however, ambiguous especially with multiple sellers and ====. In this paper, we separately estimate the effects of (multidimensional) asymmetric information, price discrimination, and competition on welfare and its distribution across consumers.====In particular, we empirically answer the following two questions. Does competition improve welfare under (multidimensional) asymmetric information? How are the gains from competition shared among heterogeneous consumers? The answers are a priori ambiguous, thus making them empirical questions. For instance, if consumers choose only one product, competition can improve consumer welfare by providing better outside options. If consumers can buy from multiple sellers then competition can “reduce” the product space (as we show later for our data), which can then reduce welfare. It is well known that monopolies generate deadweight loss because they sell at a price above marginal cost, and price discrimination can reduce this deadweight loss but most of this gain will accrue only to the seller. But, which of these opposing effects dominate, and how they vary across heterogenous consumers, will depend on the market and we estimate these effects.====To this end, we use data on advertisements bought by local businesses in Central Pennsylvania from two Yellow Page Directories, published by Verizon and Ogden. These directories compete using menus of ad options and prices, i.e., nonlinear prices. To capture all salient features of our data we use the ==== framework of Ivaldi and Martimort (1994) and determine duopoly nonlinear pricing with multidimensional preferences–each business has a separate, but possibly correlated “valuation” (or taste or willingness to pay) for advertising with each directory. Under the assumption that the two directories are (weak) substitutes and that the sellers know the joint distribution of consumer preference and their publishing costs, we determine equilibrium competing nonlinear prices and use the optimality conditions for identification.====Our data consist of ads bought by ==== business (e.g., doctor, salon) operating in Central Pennsylvania in 2006 as long as the said business has a phone number registered as a business phone number. If a business (henceforth, a consumer) buys ad(s) then we also know the payment(s) to the publisher(s). Although we have rich data on choices, we do not observe any socio-economic characteristics that are typically used in the literature as “demand shifters.” Thus for the identification we rely on optimality conditions of both demand and supply. For instance, we use the monotonicity of optimal allocation under nonlinear pricing to “invert” the optimal allocation rule for the identification of consumer types. An important feature of our data is that some choose to advertise with both, some choose only one directory while others choose neither. So we do not observe joint variation in choices and payments that is crucial for nonparametric identification of the joint density of consumers types. So we use parametric Joe Copula to estimate the density.====To identify cost and utility parameters we use other properties of our model. For instance to identify the constant marginal cost of publishing, we use the “no-distortion-on-top” property of nonlinear pricing. In other words there is no size distortion for the largest ads and hence their marginal costs equal marginal utilities, and from consumer’s optimality conditions we get that the marginal utilities equal marginal prices. The marginal prices can be estimated directly from the data, thereby identifying the marginal costs.====Using the estimated parameters, we determine outcomes under perfect information. Comparing this counterfactual with the estimated welfare we find that the welfare increases substantially. In particular, the total welfare increases by 20% of the consumer surplus under incomplete information, and we find that low-type consumers benefit more than high-type consumer. The last feature is consistent with the fact that under asymmetric information high-type consumers get information rents.====Next, we compare welfare when there is only one seller. However, it is very well known in the literature (see Rochet and Choné, 1998) that determining optimal nonlinear pricing for a multi-product monopolist with multidimensional asymmetric information is very difficult. So instead, we follow Carroll (2017) and determine the worst-case, i.e., lowest, producer surplus, which is associated with the best-case, i.e., highest, consumer surplus, which are sufficient to assess the effects of decrease in competition on producer and consumer surpluses. We find that under multi-product monopoly total producer surplus increases considerably. There are two channels through which higher market power affects the profit. First, the seller increases the product line for Ogden. In fact the largest ad offered doubles from a one-page ad to a two-page ad. Second, the seller excludes many more low-type consumers, which means the seller has to give smaller discount to the high-type consumers to induce them to buy larger ads. Consumer surplus decreases by 17%, and only the high-types consumers are better off under monopoly.====Finally, to quantify the role of varieties (two directories versus one), we determine welfare when the monopoly seller discontinues Ogden. Removing Ogden hurts consumers who prefer Ogden to Verizon. The total consumer surplus decreases by 24%. The producer surplus also decreases by 29 percentage points, but removing Ogden does not affect the distribution of consumer surplus.====The remainder of the paper is organized as follows: Section 2 presents related literature, Section 3 and Section 4 describe the data and the model, respectively. The identification, estimation and counterfactual analysis are in 5 Identification, 6 Estimation and 7, respectively. All other information and proofs are in Appendices A, B and C.",An empirical analysis of competitive nonlinear pricing,https://www.sciencedirect.com/science/article/pii/S0167718719300669,18 October 2019,2019,Research Article,217.0
"Lundin Erik,Tangerås Thomas P.","Research Institute of Industrial Economics (IFN), Stockholm, Sweden","Received 7 July 2018, Revised 6 September 2019, Accepted 16 September 2019, Available online 14 October 2019, Version of Record 2 November 2019.",https://doi.org/10.1016/j.ijindorg.2019.102536,Cited by (17),"Horizontal shifts in bid curves observed in wholesale electricity markets are consistent with ====. Quantity competition reduces the informational requirements associated with evaluating market performance because the price-cost margins of all producers then depend on the same inverse residual demand curve instead of one for each firm. We apply the model to the day-ahead market of the Nordic power exchange, Nord Pool, for the years 2011-13. We reject the null hypothesis of perfect competition in all specifications. Results suggest that the average price-cost margin across the sample period was around four percent.","The performance of electricity markets has been a major topic among market monitors and in empirical industrial organization since restructuring of electricity markets began in the 1990s. One reason is the high degree of ownership concentration of generation capacity, which together with a highly price inelastic demand, creates opportunities for exploiting market power. Another reason is the rich set of available data that have made it possible to estimate less parameterized models than what is usually possible in other markets, where researchers often have relied on the conjectural variations approach developed by Bresnahan (1982) and Lau (1982).====The most notable feature of restructured electricity markets is that wholesale markets typically are organized as Walrasian auctions: Producers, retailers and large industrial consumers submit price-dependent offers or bids to a centralized clearing house, such as a power exchange, that aggregates these offers and bids to obtain market-clearing prices and quantities. On the basis of these bids, supply and demand curves can be separately ==== rather than ==== by means of simultaneous equation methods. It is even possible to calculate a residual demand curve facing each individual firm. The price sensitivity of the residual demand curve measures the firm’s ability to influence the wholesale price, i.e. the individual firm-level market power.====The practical applicability of the above approach is limited by the fact that power exchanges often do not give external parties access to bid data at the firm level. Our theoretical contribution in Section 2 is to show that the data requirements are substantially reduced if firms with market power bid their quantities inelastically into the wholesale market at a low price. Under such Cournot competition, firms’ market power depends on the properties of the aggregate inverse residual demand curve instead of individual demand curves. This curve can be computed on the basis of aggregate bid data that more often are publicly available. In our theoretical model, exogenous shifts in demand affect the total Cournot output via two additively separable channels. The first is the direct effect of a price change, which is positive when marginal costs are increasing. The second effect only occurs under imperfect competition and works through a change in the slope of the inverse residual demand curve. These theoretical results can form the basis of empirical analysis of wholesale electricity markets characterized by Cournot competition.====We demonstrate in Section 3 the usefulness of the approach by an examination of data from the Nordic power exchange, Nord Pool, during 2011-13. This company operates the most important platform for trading wholesale electricity in the Nordic market, the day-ahead market ====. Our first observation is that the majority of within-week supply bid variation on Elspot stems from horizontal shifts in the supply curve, consistent with large firms competing in quantities. We then use an instrumental variables approach to regress Cournot output on price and a variable that appropriately measures firms’ incentive to exercise market power. This variable, the ”semi-elasticity” of residual demand, is constructed by multiplying the slope of the aggregate inverse residual demand curve by the Cournot supply net-of-forward market obligations. In particular, we find that the semi-elasticity has a statistically and economically significant negative effect on Cournot output in all specifications. This relationship is consistent with firms exercising market power by withholding production from the market, but it is inconsistent with perfect competition. Hence, we reject the hypothesis that Elspot was perfectly competitive during our sample period. Based upon the coefficient estimates, we compute an average implied price-cost margin (Lerner index) of four percent across the sample period. This margin is robust to assumptions about the curvature of the marginal cost functions, as well as assumptions about firms’ hedging positions. When we sequentially remove outliers in the semi-elasticity variable, the average markup increases to at most 16 percent. Alternative function form assumptions on the instruments reduce the average price-cost margin down to a minimum of one percent.====The methodology does not require the total demand curve to rotate, unlike in the Bresnahan-Lau framework. Identification of imperfect competition in our context instead relies on the non-linearity of the supply function of the competitive fringe. We believe our approach could be fruitfully applied also to other wholesale electricity markets where aggregate bid data are available. Examples include Austria, France, Germany, and Switzerland. These countries are all part of the European Power Exchange, ====. Visual inspection of bid data from Germany suggests that horizontal supply shifts are important sources of supply variation also here, as depicted in Fig. B1. Consistent with this observation, Willems et al. (2009) find a Cournot model to explain short term price variations in the German market just as well as a more complex supply function equilibrium model. It also appears that this bidding behavior is not restricted to the European markets. An example of horizontal supply shifts in the US Midwest market is depicted in Fig. B2 adapted from Mercadal (2016). We round off the paper by some concluding remarks in Section 4.==== Our paper is among the first to take advantage of novel bid data that have become available in recent years, to assess market performance in the Nordic electricity market. Nord Pool now releases data that enables us to reconstruct aggregate supply and demand bids on the Elspot market. Lundin (2016) uses shifts in the aggregate supply curve to estimate strategic aspects of maintenance scheduling among Swedish nuclear power plants. Tangerås and Mauritzen (2018) exploit differences between the day-ahead and intra-day markets to analyze market power. That method relies explicitly on the inter-temporality of hydro power markets and does not permit computation of implied price-cost margins. Earlier studies, such as Bask et al. (2011), typically are applications of the Bresnahan-Lau model. Damsgaard (2007) and Kauppi and Liski (2008) are exceptions. They build simulation models to account for hydro production. More recently, Fogelberg and Lazarczyk (2019) analyze market power by means of announced production failures. These papers find evidence of market power to varying degree.====Bidding data at the firm level are available in some countries. Wolak (2003), for California, and McRae and Wolak (2014), for New Zealand, demonstrate that firms submit higher-priced bids when residual demand is less price elastic. Well-known studies of the British market are Green and Newbery (1992), Wolfram (1999), Wolak and Patrick (2001) and Sweeting (2007), all of whom find evidence of market power. Fabra and Toro (2005) find indications of periods with collusion and price wars in the Spanish market. Ito and Reguant (2016) show that large firms exert market power by shifting output between the Iberian day-ahead and intra-day markets.","Cournot competition in wholesale electricity markets: The Nordic power exchange, Nord Pool",https://www.sciencedirect.com/science/article/pii/S0167718719300645,14 October 2019,2019,Research Article,218.0
"Brancaccio Giulia,Kalouptsidi Myrto,Papageorgiou Theodore","Department of Economics, Cornell University, 464 Uris Hall, Ithaca, NY 14850, USA,Department of Economics, Harvard University, CEPR and NBER, Littauer Center 124, Cambridge, MA 02138, USA,Department of Economics, Boston College, Maloney Hall 343, Chestnut Hill, MA 02467, USA","Received 19 March 2019, Revised 14 August 2019, Accepted 3 September 2019, Available online 19 September 2019, Version of Record 25 May 2020.",https://doi.org/10.1016/j.ijindorg.2019.102533,Cited by (3),"We provide a guide to estimating matching functions in a spatial context. Several interactions in space take place in a decentralized fashion, such as passengers searching for taxis, ships meeting cargo, exporters meeting importers etc. A convenient modeling device to capture these meetings is the matching function, which has been used extensively in labor market settings. However in the spatial context, data availability is often limited to only one side of the market; for instance it is usually hard to find data on the number of passengers searching for a taxi. We discuss an approach to estimating matching functions that allows the researcher to recover the unobserved side of the market with relatively few assumptions. In addition, our approach obtains the matching function non-parametrically, allowing for significantly more flexibility than is commonly assumed. This additional flexibility can be key when deriving welfare and policy implications.","In recent years there has been an increasing focus on the process that governs how agents match with each other in spatial settings. For instance, how do taxis meet passengers? How does a bulk ship meet a cargo owner? How do exporters meet importers? A nascent literature on transportation (Buchholz, 2019, Frechette, Lizzeri, Salz, 2019, Brancaccio, Kalouptsidi, Papageorgiou, 2019a) – henceforth, BKP) have modeled this process using a matching function, which is a reduced-form approach of modeling who matches with whom in a decentralized market. This function, which has been used extensively in labor market models to study employment outcomes, takes as inputs the stocks of searching agents on either side of the market and dictates the number of matches to be formed. The matching function captures the nature of frictions in our models.====In this paper, borrowing from our previous work, we discuss an estimation method for matching functions in spatial models. In this context, it is often the case that data is very rich on the supply side, but scant on the demand side. For instance, Buchholz (2019) and Frechette et al. (2019) employ rich information on taxi rides, but do not observe hailing passengers. BKP observe rich ship movements, but do not observe searching exporters. Similarly, Eaton et al. (2016) employ data on matches between importers and exporters, but the set of potential importers is not known with certainty.====A second issue with the empirical matching function is the need for flexibility. Indeed, the matching function captures the nature of search frictions in the market in a parsimonious fashion; as such, it may capture a host of different features, such as “information imperfections about potential trading partners, heterogeneities, the absence of perfect insurance markets, slow mobility, congestion from large numbers, and other similar factors” (Petrongolo and Pissarides, 2001). Therefore, a flexible functional form is of utmost importance, as the shape of the matching function is key for welfare and policy implications as shown in Hosios (1990) and more recently in Brancaccio et al. (2019b). In addition, the results of any quantitative exploration, such as the ones mentioned above, depend crucially on the matching function specification.====Our approach deals with both having data on only one side of the market, as well as allowing for flexibility. We draw from the literature on nonparametric identification (Matzkin, 2003) and non-separable instrumental variable techniques (e.g. Imbens and Newey, 2009). Roughly, the method leverages (i) an invertibility assumption between matches and sellers, (ii) the observed relationship between matches and sellers, (iii) an instrument that shifts the number of sellers, and (iv) a restriction on the matching function that allows us to disentangle monotonic transformations.====It is worth noting that our approach is most suitable when the matching function depends just on the stocks of available sellers and buyers, implying that agents are homogeneous in terms of their matching probabilities. For instance, the number of meetings between taxis and passengers within a NYC block depends only the number of cabs and searching passengers; other characteristics (e.g. car brand of the taxi or passenger gender) do not affect the matching rates (even if they affect the utility of the agents or the price paid). This approach is inspired by the labor literature (e.g. Mortensen and Pissarides, 1994) and is distinct from the matching literature that studies the matching outcomes between heterogeneous agents (e.g. Gale and Shapley, 1962 or Roth and Sotomayor, 1992). For instance, in marriage markets the types of men and women (such as age, income, etc.) may affect marriage rates (Choo and Siow, 2006).====In this note, we present the basic estimation approach. Then, we elaborate on a number of practical issues, such as the different restrictions that can be imposed for identification, the instruments and the possibility of parametric restrictions. Finally, we discuss the estimation method in the context of several empirical applications, including taxis, shipping, bike-sharing and importer-exporter matching.",A guide to estimating matching functions in spatial models,https://www.sciencedirect.com/science/article/pii/S0167718719300554,19 September 2019,2019,Research Article,219.0
"Besanko David,Doraszelski Ulrich,Kryukov Yaroslav","Kellogg School of Management, Northwestern University, Evanston, IL 60208, USA,Wharton School, University of Pennsylvania, Philadelphia, PA 19104, USA,University of Pittsburgh Medical Center, Pittsburgh, PA 15219, USA","Received 1 February 2019, Revised 28 June 2019, Accepted 28 June 2019, Available online 25 July 2019, Version of Record 25 May 2020.",https://doi.org/10.1016/j.ijindorg.2019.102522,Cited by (2),"To detect the presence of ====, antitrust authorities and courts routinely ask whether a firm sacrifices current profit in exchange for the expectation of higher future profit following the exit of its rival. Because predatory pricing is an inherently dynamic phenomenon, we show in this paper how to construct sacrifice tests for predatory pricing in a modern industry-dynamics framework along the lines of Ericson and Pakes (1995). In particular, we adapt the definitions of predation due to Ordover and Willig (1981) and Cabral and Riordan (1997) to this setting and construct the corresponding sacrifice tests.","To detect the presence of predatory pricing, antitrust authorities and courts routinely ask whether a firm sacrifices current profit in exchange for the expectation of higher future profit following the exit of its rival. Because predatory pricing is an inherently dynamic phenomenon, we show in this paper how to construct sacrifice tests for predatory pricing in a modern industry-dynamics framework that endogenizes competitive advantage and industry structure. Due to its presence in a number of high-profile predatory pricing cases, we focus on learning-by-doing.====At the core of predatory pricing is the trade-off between lower profit in the short run due to aggressive pricing and higher profit in the long run due to reduced competition. Determining what constitutes an illegitimate profit sacrifice—and thus predatory pricing—is especially difficult when firms face other intertemporal trade-offs such as learning-by-doing, network effects, or switching costs that can give rise to aggressive pricing with subsequent recoupment. As Farrell and Katz (2005) point out, “[d]istinguishing competition from predation is even harder in network markets than in others. With intertemporal increasing returns, there may innocently be intense initial competition as firms fight to make initial sales and benefit from the increasing returns” (p. 204). Yet, allegations of predation (or, in an international context, dumping) sometimes arise in settings where learning-by-doing is a key feature of the industrial landscape. Examples include the “semiconductor wars” between the U.S. and Japan during the 1970s and 1980s (Flamm, 1993, Flamm, 1996, Dick, 1991), the allegations by U.S. color television producers against Japanese producers during the 1960s and 1970s that are at the core of the ==== predatory pricing case (Yamamura and Vandenberg, 1986), and more recently the debate about Chinese solar panels.====In these and many other industries, a firm has an incentive to price aggressively because its marginal cost of production decreases with its cumulative experience.==== While this makes it difficult to disentangle predatory pricing from mere competition for efficiency on a learning curve, being able to do so is crucial when predation is alleged. In practice, antitrust authorities find a price predatory if there is evidence of an illegitimate profit sacrifice. This, in turn, requires a notion of what constitutes an illegitimate profit sacrifice in the first place. Unfortunately, antitrust authorities and courts have not yet converged on a simple, clear standard.====In this paper, we show how the definitions of predation due to Ordover and Willig (1981) and Cabral and Riordan (1997) can be used to determine what constitutes an illegitimate profit sacrifice. In contrast to antitrust authorities, the economics literature focuses more directly on the impact that a price cut has on reshaping the structure of an industry. According to the definitions of predation due to Ordover and Willig (1981) and Cabral and Riordan (1997), a price is predatory if it had not been worth charging absent its impact on the probability that the rival exits the industry. The definitions differ in that Ordover and Willig (1981) presume that the rival is viable with certainty whereas Cabral and Riordan (1997) presume that its exit probability remains unchanged. While the idea that predatory pricing can be usefully defined by a “but-for” scenario has greatly influenced economists’ thinking, to our knowledge it has rarely been formalized outside simple models such as the one in Cabral and Riordan (1997).==== In this paper, we show how to adapt the definitions of predation due to Ordover and Willig (1981) and Cabral and Riordan (1997) to a Markov-perfect industry-dynamics framework along the lines of Ericson and Pakes (1995). We then show how to construct sacrifice tests from these definitions. The economic definitions of predation in the extant literature therefore amount to particular ways of disentangling an illegitimate profit sacrifice stemming from predatory pricing from a legitimate effort to increase cost efficiency through aggressive pricing.====To construct sacrifice tests in a dynamic pricing model similar to the models of learning-by-doing in Cabral and Riordan (1994) and Besanko et al. (2010), we build on Besanko et al. (2014) and decompose the equilibrium pricing condition. The insight in that paper is that the price set by a firm reflects two goals besides short-run profit. First, by pricing aggressively, the firm may move further down its learning curve and improve its competitive position in the future, giving rise to an ====. Second, the firm may prevent its rival from moving further down its learning curve and becoming a more formidable competitor, giving rise to an ====.====To isolate the probability of rival exit—the linchpin of the definitions of predation due to Ordover and Willig (1981) and Cabral and Riordan (1997)—we go beyond Besanko et al. (2014) and decompose the equilibrium pricing condition with even more granularity. One component of the advantage-building motive is the ====. This is the marginal benefit to the firm from the increase in the probability of rival exit that results if the firm moves further down its learning curve. The ==== is analogously the marginal benefit from preventing the decrease in the probability of rival exit that results if the rival moves further down its learning curve. Other terms in the decomposed equilibrium pricing condition capture the impact of the firm’s pricing decision on its competitive position, its rival’s competitive position, and so on.====Our decomposition highlights the various incentives that a firm faces when it decides on a price. Some of these incentives may be judged to be predatory while others reflect the pursuit of efficiency. In this way, our decomposition mirrors the common practice of antitrust authorities to question the intent behind a business strategy.====We establish formally that certain terms in our decomposition map into the definitions of predation due to Ordover and Willig (1981) and Cabral and Riordan (1997). At the same time, however, our decomposition makes clear that there is much latitude in where exactly to draw the line between predatory pricing and mere competition for efficiency on a learning curve. Indeed, our decomposition lends itself to developing multiple alternative characterizations of a firm’s predatory pricing incentives. Drawing on Edlin and Farrell (2004) and Farrell and Katz (2005), we compare the definitions of predation due to Ordover and Willig (1981) and Cabral and Riordan (1997) with several alternatives that Besanko et al. (2014) have previously formalized in a dynamic pricing model.====For each of these characterizations of a firm’s predatory pricing incentives, we show how to construct the corresponding sacrifice test for predatory pricing. As Edlin and Farrell (2004) point out, one way to test for sacrifice is to determine whether the derivative of a profit function that “incorporate[s] everything except effects on competition” is positive at the price the firm has chosen (p. 510). A different characterization of the firm’s predatory pricing incentives is tantamount to a different operationalization of the everything-except-effects-on-competition profit function.====To further illustrate our decomposition and the multiple alternative sacrifice tests that follow from it, we first link the various terms in the decomposition to key features of the pricing decision. Then we gauge the consequences of applying sacrifice tests for industry structure and dynamics by way of an illustrative example. As antitrust authorities flag and prosecute an illegitimate profit sacrifice, they prevent a firm from pricing to achieve that sacrifice. This amounts to forcing firms to ignore the predatory incentives in setting their prices.====We avoid dealing with out-of-equilibrium adjustment processes and merely delineate what may happen in the counterfactual equilibria once firms are forced to ignore the predatory incentives. Because our goal is to show how to construct sacrifice tests in a modern industry-dynamics framework, and not to run a conclusive “horse race” between antitrust policies that are based on alternative characterizations of a firm’s predatory pricing incentives, we content ourselves with presenting equilibria and counterfactuals for a particular parameterization of the model. At this parameterization, applying sacrifice tests limits competition for the market and thus harms consumers, at least in the short run.====In practice, the impact of forcing firms to ignore the predatory incentives may differ across parameterizations, so that an antitrust authority set to apply a sacrifice test is well advised to first tailor the model to the institutional realities of the industry under study and estimate the underlying primitives. We view our paper as a guide to how to construct sacrifice tests for predatory pricing and assess their implications for industry structure, conduct, and performance in a modern industry-dynamics framework along the lines of Ericson and Pakes (1995).====We characterize potentially predatory incentives in a dynamic pricing model in which firms jostle for competitive advantage as their cost positions evolve over time. Predatory incentives can arise for other reasons. In models of predation based on asymmetric information (Kreps, Wilson, 1982, Milgrom, Roberts, 1982, Fudenberg, Tirole, 1986), an incumbent firm’s incentive to charge a low price is aimed at shaping a potential entrant’s or other rival’s expectations that its future profitability is likely to be low. In models based on capital market imperfections (Bolton, Sharfstein, 1990, Snyder, 1996), it is aimed at exploiting an agency problem and putting the rival in a position where it is unable to obtain financing from outside sources to continue operations. The commonality between these models and our model is that predatory incentives may reside in a firm’s desire to shape future industry structure to its advantage. As in our model, the equilibrium pricing condition impounds goals that may be deemed predatory as well as non-predatory goals (e.g., short-run profit). Whether our decomposition directly generalizes is less clear. For example, models based on asymmetric information often rely on subgame perfect equilibria rather than Markov perfect equilibria.====The remainder of this paper is organized as follows. Section 2 lays out the model. Section 3 develops the decomposition of the equilibrium pricing condition and formalizes its relationship with the definitions of predation due to Ordover and Willig (1981) and Cabral and Riordan (1997). Section 4 uses the decomposition to develop multiple alternative characterizations of a firm’s predatory pricing incentives and construct the corresponding sacrifice tests. Section 5 exemplifies the link between our decomposition and equilibrium behavior and the impact of forcing firms to ignore the predatory incentives in setting their prices. Section 6 concludes.",Sacrifice tests for predation in a dynamic pricing model: Ordover and Willig (1981) and Cabral and Riordan (1997) meet Ericson and Pakes (1995),https://www.sciencedirect.com/science/article/pii/S016771871930044X,25 July 2019,2019,Research Article,220.0
"Herweg Fabian,Schmidt Klaus M.","University of Bayreuth, CESifo, and CEPR, Germany,Faculty of Law, Business and Economics, University of Bayreuth, Universitätsstr. 30, Bayreuth D-95440, Germany,University of Munich, CESifo, and CEPR, Germany,Department of Economics, University of Munich, Ludwigstr. 28 (Rgb.), München D-80539, Germany","Received 12 December 2018, Revised 18 March 2019, Accepted 4 June 2019, Available online 8 July 2019, Version of Record 25 May 2020.",https://doi.org/10.1016/j.ijindorg.2019.06.003,Cited by (2),We consider a multi-dimensional procurement problem in which sellers have ,"In the procurement of a complex good, sellers are often better informed about the optimal design of the good than the buyer. For example, a seller may see that the design proposed by the buyer has a serious flaw or that there is a possibility for a design improvement. The buyer is aware that sellers may have superior information and wants to induce them to contribute their knowledge at the design stage. However, if the good is procured with a price-only auction, a seller who sees a possibility for a design improvement will not reveal this information. It is a weakly dominant strategy to keep the information to himself, bid more aggressively in the auction, and then – after he receives the contract – renegotiate the design with the buyer once he is in a bilateral monopoly position where he can extract some share of the surplus (Herweg and Schmidt, 2017). But if the design is changed late via renegotiation, both parties have to incur inefficient adjustment costs. Therefore, the buyer wants to use a more sophisticated auction that induces sellers to reveal their ideas early.====In this paper we derive the optimal Bayesian incentive compatible mechanism that implements the efficient allocation at minimal cost to the buyer and compare this mechanism to the optimal ex post incentive compatible mechanism (that is informationally less demanding but more expensive) studied in our companion paper Herweg and Schmidt (2019). The mechanism design problem we consider is one with two dimensional private information. Sellers have private information about their costs and they have private information about the existence of a flaw (the possibility of a design improvement). As is standard in the literature we assume that the sellers’ cost types are uncorrelated. However, the information about a design flaw must be correlated, because a seller can observe a flaw only if the flaw exists. Crémer, McLean, 1985, Crémer, McLean, 1988 have shown that if types are correlated, then it is possible to extract all rents from the seller by using penalties in certain states of the world to deter agents from lying. The problem is that these penalties may have to be very large and that it is often impossible to enforce them if agents are protected by limited liability. Our contribution to this literature is to show that the rents that the agents obtain in one dimension (private information about their cost types) can be used to relax the limited liability constraints. Thus, it is possible to exploit the correlation of sellers’ types even if they are protected by limited liability.====The problem that sellers conceal private information about flaws and design improvements in order to renegotiate later seems to be widespread. Bajari and Tadelis (2011) report that sellers read plans and specifications carefully to find flaws that they want to use strategically in order to renegotiate later. Then they bid more aggressively to get the contract. In the construction industry this is called ==== or ==== (Mohamed et al., 2011). Bajari, McMillan, Tadelis, 2009, Bajari, Houghton, Tadelis, 2014 and Iimi (2013) provide additional empirical evidence on this behavior. Moreover, there is significant empirical evidence that contract renegotiation is costly and inefficient (Crocker, Reynolds, 1993, Chakravarty, MacLeod, 2009, Bajari, Houghton, Tadelis, 2014).====Following Herweg and Schmidt (2019) we model this problem as follows. There is one buyer who wants to procure an indivisible good. With some probability the design that she came up with is plagued by a flaw. It is less expensive to fix the flaw early (before the contract is allocated) than late (via renegotiation). There are two sellers with two-dimensional private information. Each seller is privately informed about his production cost. Moreover, if the buyer’s design is flawed, each seller privately observes the flaw with some probability. Thus, a seller’s type consists of a cost type and a flaw type, where the latter is binary. While we assume that sellers’ cost types are independently distributed, sellers’ flaw types – by the nature of the model – are correlated: A seller can observe the flaw only if it exists.====Herweg and Schmidt (2019) point out three inefficiencies that arise if a buyer uses a price-only auction. First, there is the inefficiency that arises if a flaw is fixed with delay because a seller concealed his information at the auction stage and revealed it only later in renegotiation. Second, a seller who spotted a flaw and bids more aggressively may win the auction even though he is not the seller with the lowest production costs. Third, the contract may be awarded to a seller who is not aware of the flaw, while some of his competitors are. In this case the design improvements are never implemented – there is no renegotiation. In our companion paper Herweg and Schmidt (2019) we solve for the ex post incentive compatible mechanism that implements the efficient allocation at lowest cost. Here, we ask whether the buyer can reduce her cost of implementing the efficient outcome if the used mechanism needs to satisfy only Bayesian but not ex post incentive compatibility.====As explained above, we consider a buyer who wants to implement the efficient allocation at the lowest possible transfers to sellers. Efficiency requires that the good is produced by the seller with the lowest cost and that each seller reports the flaw early, if he observed it. The optimal mechanism must be Bayesian incentive compatible to induce sellers to report their types truthfully and it must ensure that sellers do not make losses ex post because they are protected by limited liability.====For the one-dimensional screening problem it is well-known that the buyer can fully extract any information rents of the agents, if types are correlated and agents have unlimited funds (Crémer and McLean, 1988). If agents are protected by limited liability, however, the buyer may not be able to benefit from the (imperfect) correlation of agents’ types. We show that in a multi-dimensional screening problem, the buyer can use the rents the agents obtain in one dimension to reduce the rents they obtain in the dimension where types are correlated, even if the limited liability constraints have to be satisfied.====The optimal mechanism has the following properties: In expectations, each seller obtains the standard rent from a Vickrey auction for revealing his cost type. Moreover, there is an additional fixed payment for revealing the flaw. This payment is higher the lower the correlation of sellers’ flaw types. Note that if only one seller reports the flaw, then the other seller – who did not observe it or decided to conceal it – is punished. The limited liability constraint restricts this punishment to loosing the information rent obtained for reporting the cost type. In order to keep the information rent for a seller who did not observe the flaw in expectations equal to the rent from the Vickrey auction, the rent obtained if both sellers did not observe the flaw has to be higher than the rent from the Vickrey auction.====The optimal Bayesian incentive compatible mechanism does not allow for a separate (i.e. sequential) elicitation of cost and flaw types. Furthermore, the optimal fixed payment obtained for reporting the flaw crucially depends on all distribution functions: the likelihood of the flaw to exist, the probability with which it is detected by sellers, and the distribution of sellers’ production costs. Due to these high informational demands, the practical applicability of the optimal Bayesian mechanism may be limited. This is why we derive the optimal ex post incentive compatible (EPIC) mechanism in Herweg and Schmidt (2019). The EPIC mechanism does not require any ex ante knowledge of the underlying distributions, i.e., the EPIC mechanism is informationally robust. However, it may also be significantly more expensive, because the buyer has to pay a larger information rent to the sellers. In the current paper we derive conditions under which the loss in profits for the buyer is small when using the EPIC instead of the Bayesian mechanism.==== Our paper contributes to three strands of the literature. First, there is a large literature on optimal procurement mechanisms. Important early contributions include McAfee and McMillan (1986) and Laffont and Tirole (1993). Our approach is more closely related to the literature on optimal scoring auctions. In a scoring auction sellers submit bids on price and design. The contract is assigned to the seller who comes up with the best proposal, i.e., the highest score. Scoring auctions outperform price-only auctions, if the buyer can commit not to renegotiate (Dasgupta, Spulber, 1989–1990, Che, 1993).==== The properties and limits of scoring auctions are further analyzed by Asker, Cantillon, 2008, Asker, Cantillon, 2010 and Herweg and Schmidt (2017).==== A crucial difference of our approach to these papers is that in our model the seller who points out the possibility of a design improvement does not have to be the seller who carries out production. This is also the case in Che et al. (2017). Their model focuses on how to give incentives for innovation, while in our model sellers observe the design improvement for free but have to be induced to report it early. Furthermore, in contrast to Che et al. (2017), we study a multidimensional screening problem. We show that the rents obtained in one dimension can be used to reduce rents in another dimension by relaxing the limited liability constraint.====Second, our paper is related to the literature on optimal screening with correlated types. In their seminal contribution, Crémer and McLean (1988) provide necessary and sufficient conditions for full surplus extraction (FSE) with Bayesian and dominant strategy screening mechanisms. They show that FSE is feasible, if (roughly speaking) agents’ types are “correlated”. McAfee and Reny (1992) extended this result to continuous type spaces.==== All of this literature considers one-dimensional screening problems.====Finally, our paper contributes to the extensive and growing literature in economics and computer science on multi-dimensional screening. Armstrong (1996) shows that if the mechanism designer wants to maximize profits, the optimal mechanism (usually) excludes some types. Rochet and Choné (1998) show that it is often optimal to induce “bunching” of types.==== To the best of our knowledge, our paper is the first that points out how the rents an agent obtains from one dimension can be used to reduce the rents from the other dimension by relaxing the limited liability constraint.====The rest of the paper is organized as follows. In the next section we introduce the model and formally state the mechanism design problem. This problem is solved in Section 3, where we proceed in several steps. First, in Subsection 3.1, we show how the optimization problem can be simplified significantly. Then, in Subsection 3.2, we analyze the special cases where flaw types are either perfectly correlated or not correlated at all. Finally, in Subsection 3.3, we derive the optimal direct mechanism. Comparative statics of the optimal mechanism and an equivalent indirect mechanism are discussed in Section 4. The final Section 5 summarizes the main findings and discusses open questions for future research. Proofs that are not presented in the main text are deferred to Appendix A.",Bayesian implementation and rent extraction in a multi-dimensional procurement problem,https://www.sciencedirect.com/science/article/pii/S0167718719300438,8 July 2019,2019,Research Article,221.0
"Montero Juan-Pablo,Spiegel Yossi,Verboven Frank","PUC-Chile,Tel-Aviv University,KU Leuven","Available online 8 March 2020, Version of Record 27 November 2020.",https://doi.org/10.1016/j.ijindorg.2020.102609,Cited by (0),None,None,Introduction to the special issue on advances in the analysis of competition policy and regulation,https://www.sciencedirect.com/science/article/pii/S016771872030031X,8 March 2020,2020,Research Article,227.0
"González Xulia,Moral María J.","Facultad de Ciencias Económicas y Empresariales, Universidade de Vigo, As Lagoas Marcosende, s/n, Vigo 36210, Spain,Facultad de Ciencias Económicas y Empresariales, UNED, Paseo Senda del Rey, 11, Madrid 28040, Spain","Received 25 January 2019, Revised 20 September 2019, Accepted 24 September 2019, Available online 1 October 2019, Version of Record 18 October 2019.",https://doi.org/10.1016/j.ijindorg.2019.102537,Cited by (8),"In February 2015, Spain’s Competition Authority imposed € 32.4 million in fines on five of the country’s largest oil operators as sanctions for price collusion. This paper examines the effect of that antitrust action on retail fuel prices. Our analysis uses a novel data set with detailed information on more than 8000 gas stations throughout Spain. Prices were collected every day from 18 August 2014 to 15 June 2015 (almost 2 million price observations). First we estimate a reduced-form fuel price equation that accounts for wholesale costs and brand affiliation. Then we use a model of gas stations and time fixed effects while adopting a difference-in-differences approach to assessing the fines’ effect on retail fuel prices. Our results indicate that, after publication of the fine, sanctioned firms raise prices slightly, and the additional revenues far exceeded the amount of the fine. We also find substantial heterogeneity, depending on the size of the fine, in the magnitude of this price response. Hence the fine’s burden might well have been borne mainly by consumers, whose welfare was thereby reduced. Our study should be of interest to antitrust authorities as we show that sanctions may not be effective enough in deter price fixing practices, especially when sanctions are weak and the profits from colluding are sufficiently high.","In February 2015, Spain’s competition authority imposed € 32.4  million in fines on five of the country’s largest oil operators (Repsol, Cepsa, Shell, Galp, and Meroil) for colluding on retail fuel prices.==== These firms were found to have violated Article 1 of the Spanish Competition Act 15/2007 and Article 101 of the Treaty on the Functioning of the European Union (EU).==== The sanctioned companies represent more than 60% of all service stations in Spain; Repsol is the leader (accounting for around 40%), followed by Cepsa. These two operators control 92% of the country’s refining capacity.====The Spanish automotive fuel market is of great concern to policymakers and competition authorities. In recent years, the Spanish Competition Authority (====, CNMC) has published several reports (CNC, CNE, CNMC, CNMC) that underscore insufficient competition in the fuel distribution sector as well as retail prices that are higher than those in neighboring countries. In fact, the conclusions of the first two reports motivated the investigation central to this collusion case. That investigation was undertaken by the CNMC, starting in May 2013 with surprise inspections at the targeted companies’ head offices and the Spanish Association of Oil Product Operators (AOP). The investigation concluded in July 2014, and the CNMC published its condemnatory resolution – and the amount of the fines – on 20 February 2015.====Fines are the main instrument used to punish and deter cartels, which motivates our interest in assessing the reaction (if any) of prices to antitrust sanctions. Our identification strategy exploits that, in this particular case, only five brands were sanctioned while others were not. Furthermore, the sanctioned firms knew neither the resolution’s exact date nor the fine amounts. In fact, the sanctions were not announced until eight months after the investigation was closed. In the context of this antitrust prosecution, our research aim is to evaluate the extent to which prices were affected by the competition authority’s infringement decision. More specifically, we observe an increase in the prices set by sanctioned firms that is ==== observed in prices set by non-sanctioned firms (i.e., the control group) after the fine was imposed. This research question is of considerable interest because the Spanish Competition Authority has become more active in promoting competition. In 2015, the CNMC imposed a record number (26) of sanctions for violations of competition regulations; the targets included 14 cartels. Altogether, the fines assessed that year amounted to € 549 million, of which € 506 million were for cases of collusion (OECD, 2016).====Governments and scholars are focusing increased attention on the questions of how retail fuel prices are set and how they change in response to various types of shocks. Fuel purchases represent a significant portion of many households’ budgets and are also a key input for other sectors (e.g., transport).==== Many countries have begun to monitor oil and fuel prices, and competition agencies have thoroughly investigated the sector – leading to numerous reports and to prosecutions for antitrust violations (OECD, OECD).====There is no consensus in the literature on firms’ reaction to an antitrust prosecution. One might logically assume that successful antitrust enforcement drives prices down to competitive levels, at least in the short run. However, there is some evidence that prices do not always decline following termination of an antitrust action (Crandall and Winston, 2003). It is difficult to break a cartel when the market structure is highly concentrated and when the profits from collusion may be high enough to compensate for fines (Harrington, 2014, Harrington, 2018). Hence it is important to develop empirical evaluations of the actual effectiveness of antitrust enforcement – and to do so on a case-by-case basis (Ordóñez-De-Haro and Torres, 2013). Our paper contributes to this line of research by evaluating the ex post effects of a successfully prosecuted case against collusion on retail fuel prices.====To address our research question, we built a novel data set consisting of daily diesel prices obtained from practically all the gas stations that operate in Spain. We focus on diesel fuel because it accounts for about 85% of the fossil automotive fuel used in Spain and also in many other European countries. Data were collected every day from 18 August 2014 to 15 June 2015, a period that ranges from the end of the investigation to five months after the judicial resolution of 20 February 2015. This process yielded nearly 2 million price observations from more than 8000 gas stations.====We then used the resulting panel data to estimate a reduced-form price equation that accounts for cost “shifters” such as oil prices, transport costs (i.e., distance to the nearest refinery), and brand affiliation. In addition, we estimate an alternative, more flexible price equation that accounts for gas station and time fixed effects; this equation explains more than 90% of price variation. We employ a difference-in-differences (DiD) approach, based on this flexible price equation, to identify price differences – between sanctioned and non-sanctioned brands – after publication of the antitrust action and also price differences among sanctioned firms as a function of the fine’s magnitude. Our paper thus contributes to the literature on the effectiveness of antitrust enforcement and to the literature on price-setting behavior in the retail fuel market.====Our results can be summarized as follows. With regard to the ====, we find that prices are strongly affected by oil prices and that “premium” brands set significantly higher prices than do independent brands; moreover, low-cost and supermarket brands set prices that are significantly lower still. As for the ====, our results indicate that, after the fines were made public, prices at stations selling the sanctioned companies’ brands actually increased with respect to their non-sanctioned counterparts. Although the estimated average price increase of about 1.5 euro cents may seem like a small amount, its effect on aggregate consumer welfare would be considerable in light of the extremely large market. In fact, the additional revenues were of such magnitude that they far exceeded the fine imposed. Furthermore, we identify heterogeneous effects across sanctioned brands – namely, price increases were larger for brands with a higher fine (or market share) – and also across markets: price increases were smaller in local markets that were more competitive. In short, our findings suggest that firms could relax their competitive behavior once the verdict was public and they were free of antitrust scrutiny. This possibility should, of course, be taken into account by antitrust authorities when designing anticartel policies.====The rest of our paper is organized as follows. Section 2 summarizes the predictions made in the related literature. In Section 3, we briefly describe the Spanish retail fuel market and the cartel case background. Section 4 describes our data and gives some preliminary evidence of price setting. Section 5 provides evidence based on comparisons between pre- and post-sanction prices, and Section 6 presents our empirical model and estimation strategy. We discuss our results in Section 7, and we conclude in Section 8 with a summary and some suggestions for future research. Additional results and data are documented in the appendices.",Effects of antitrust prosecution on retail fuel prices,https://www.sciencedirect.com/science/article/pii/S0167718719300657,1 October 2019,2019,Research Article,230.0
"Bar Talia,Kalinowski Jesse","Department of Economics, University of Connecticut, 365 Fairfield Way, U-1063, Storrs, CT 06269-1063, United States","Received 29 January 2018, Revised 8 September 2019, Accepted 13 September 2019, Available online 19 September 2019, Version of Record 3 October 2019.",https://doi.org/10.1016/j.ijindorg.2019.102535,Cited by (5),"We study the timing of settlement in patent disputes, accounting for an alleged infringer’s search for evidence to challenge patent validity. Early settlements are more likely reached when the disputed patent has a low quality, and late settlements take place when the alleged infringer finds strong evidence that challenges patent validity. Thus, there is a tendency to settle disputes over patents that would have likely been invalidated in court. Fee shifting induces more early settlements, and when parties did not settle early, a higher expenditure on invalidating the patent. Our model sheds light on disputes involving patent assertion entities.","The United States Patent and Trademark Office (USPTO) receives more than 500,000 patent applications each year. The office examines every application to determine if it satisfies the legal patentability requirements. This process is imperfect and the patent office grants “bad” patents – ones that would not have been granted had the examiner been perfectly informed. Bad patents are believed to impose harm to society (Frakes and Wasserman, 2019), and only some are later invalidated.====We model settlements and litigation in patent disputes. We aim to better understand the timing and size of settlement and the role of search for evidence of invalidity in determining the outcome of negotiation. Settlement timing plays an important financial role in litigation. Settling shortly after a legal demand will often mean avoiding the majority of the costs. But, it is not uncommon for parties to settle just before trial and incur substantial litigation expenses (Gryphon, 2011).====In our symmetric information model, there is a common belief that the patent is good. There are two stages of negotiations. In the first stage, the patentee makes an early settlement demand. If her demand is rejected, then the alleged infringer will expend resources to search for prior art or other evidence that could challenge patent validity. The strength of evidence is revealed to both parties, and it determines the probability that the patent would be found valid in trial. If the alleged infringer finds strong evidence then the probability that the patentee wins is lower than if the alleged infringer only finds weak evidence. The patentee makes her late settlement demand contingent on the strength of evidence. If her demand is rejected again, then the patentee decides whether to drop the suit or to continue to trial.====In many models of pre-trial negotiations, asymmetric information is the reason why parties sometimes fail to settle before trial. In our model, no party has private information, instead trials are driven by asymmetric stakes – the gain to the patentee from winning might be higher than the loss to the alleged infringer. Patent disputes likely exhibit stakes asymmetry; for example, when winning could strengthen a patentee’s position in future disputes.==== This simple modeling approach allows us to combine important characteristics of patent litigation. In particular, in our model, parties can settle early (before the alleged infringer searches for evidence of invalidity), settle late (after evidence is revealed), or go to trial; there is endogenous expenditure on search for evidence; the model addresses both validity and infringement; and it accounts for situations where the patentee would choose to drop the suit if strong evidence of invalidity is found.====When the patentee’s gain from winning a trial is sufficiently higher than the alleged infringer’s loss (a high-returns-to-litigation range), a trial is credible regardless of whether the evidence of invalidity is weak or strong. In this case, we find a range of parameter values for which the parties fail to settle early. Instead, they either settle late, if the alleged infringer finds strong evidence of invalidity, or continue to trial, if evidence is weak. In a low-returns-to-litigation range, a trial is only credible when evidence is weak. In this case, we find a range of parameter values for which the patentee drops the suit if evidence of invalidity is strong and continues to trial if evidence is weak.====Importantly, in our model the alleged infringer chooses the intensity of search for evidence of invalidity. We show that an alleged infringer expends more resources to find evidence of invalidity the lower is the initial probability that the patent is good. The alleged infringer also searches more intensely when finding strong evidence of invalidity would induce the patentee to drop the suit. The patentee chooses the amount of settlement demands. Her early settlement demand increases with the initial probability that the patent is good. When a late settlement is reached, the settlement amount is lower than the highest amount the alleged infringer would have been willing to settle for in the early stage. This is because late settlements take place when the patentee’s position has been weakened by the finding of strong evidence of invalidity and because the pre-trial litigation costs have been incurred earlier.====Farrell and Merges (2004) reason that litigation does not reliably fix patent office errors due to skewed incentives to challenge and defend patents. Complementing their arguments, we claim that the courts’ ability to correct the patent office’s mistakes is limited by the parties’ higher incentive to settle disputes over patents that are more likely to be bad. The reason is that at an early stage, the owner of a lower quality patent makes a lower settlement demand, because she anticipates a higher probability that the alleged infringer would find strong evidence of invalidity if they do not settle early. Moreover, if the parties do not settle early, then after the strength of evidence is revealed, the patentee has a higher incentive to settle late or to drop the suit if the alleged infringer revealed strong evidence of invalidity.====The tendency to settle weak cases before trial suggests that if all disputed patents had their validity challenged in court, the share of invalidated patents would have been higher than in the cases that reach a trial. Still, even under this pattern of selection, empirical findings suggest that a significant share of litigated patents are found to contain invalid claims (Allison et al., 2014). Asymmetric information models similarly predict selection into trial; typically strong cases of the informed party are more likely to reach a trial. For example, in Lee and Bernhardt (2016), where privately informed defendants signal through the timing and the size of settlement offers, weaker defendants attempt to settle pre-discovery.====Our model also highlights a special feature of patent litigation. For the patentee to win an infringement suit, the patent needs to be valid ==== infringed; while for the alleged infringer to win, the patent should either be invalidated ==== not infringed. Extending the base model to consider the patentee’s search for evidence of infringement, we find that if she searches for evidence before filing a suit and files only if she finds strong evidence of infringement, then the patentee will have a stronger incentive to search for evidence of infringement when she has a higher quality patent. This is because the patentee expects a higher payoff from filing a suit when her patent is more likely to be good. In a version of the model with simultaneous search for evidence, we show that the patentee would want to spend more resources to show infringement the less resources the alleged infringer expends to show invalidity. However, the alleged infringer would want to spend more resources to invalidate the patent the more resources the patentee spends to find evidence of infringement.====In patent cases in the US, each party typically pays its own attorney fees. Under a fee shifting rule, the loser may be required to also pay the legal fees of the winning party. US 35 U.S.C. 285 states that, “The court in exceptional cases may award reasonable attorney fees to the prevailing party.” A 2014 Supreme Court ruling tends toward a more supportive environment for fee-shifting in patent litigation.==== The Innovation Act further proposed changes that would make fee shifting the default.==== In our model, if the alleged infringer only reveals weak evidence of invalidity, under a fee shifting rule the threat of trial is credible for a wider range of returns to the patentee compared to the case without fee shifting. This is because when evidence is weak, the patentee is likely to prevail, thus, under fee shifting her expected cost is lower. However, if the alleged infringer finds strong evidence of invalidity, under a fee shifting rule the patentee is more likely to drop the suit, since the cost of losing is higher. This strengthens the alleged infringer’s incentive to search for evidence of invalidity.====We consider the behavior of Patent Assertion Entities (PAE) and the alleged infringers they sue. PAEs—firms that do not research or produce the goods for which they own the intellectual property, but assert a patent to extract rent—are a subject of policy debate. According to Chien (2013), PAEs were responsible for 62% of total US patent litigation cases in 2012, up from 19% in 2006. Mazzeo et al. (2013) suggest that PAEs may be more willing to settle litigation before a court decision and that they have lower success rates in terms of validity and infringement. PAEs specialize in patent litigation, and so they likely have lower litigation costs. Our model predicts that patentees with lower litigation costs file infringement suits on patents that are, on average, of lower quality; and if they fail to settle, they might bring to trial even cases for which there is strong evidence of invalidity.",Patent validity and the timing of settlements,https://www.sciencedirect.com/science/article/pii/S0167718719300633,19 September 2019,2019,Research Article,231.0
"Ambashi Masahito,Régibeau Pierre,Rockett Katharine E.","Economic Research Institute for ASEAN and East Asia (ERIA), and Research Institute of Economy, Trade and Industry (RIETI), Japan,Department of Economics, University of Essex, Wivenhoe Park, Colchester CO4 3SQ, United Kingdom,CEPR, United Kingdom","Received 31 July 2017, Revised 5 September 2019, Accepted 10 September 2019, Available online 14 September 2019, Version of Record 19 October 2019.",https://doi.org/10.1016/j.ijindorg.2019.102534,Cited by (1),We analyse the effect of grantback clauses in licensing contracts. While competition authorities fear that grantback clauses might decrease the licensee's ,"Studies of the prevalence of licensing suggest that when local expertise matters or barriers to trade are significant, it is common for patent-holders to license their technology to a local firm and earn revenues from royalty payments rather than attempt direct entry.==== While the monetary significance of this activity is difficult to pin down, Zuniga and Guellec (2009) find that, in a recent survey of the European Union (EU) and Japan, 20% of firms in Europe and 27% in Japan grant licenses to non-affiliated entities. The Organisation for Economic Cooperation and Development (OECD, 2015a and 2015b) finds that international trade in knowledge assets in 2013 for 32 OECD countries was, on average, 2–3% of GDP or a total of $364,707.7 M for receipts and payments.====Such licensing is generally seen as welfare increasing because it ensures that local production is done efficiently and that technology diffuses across markets and firms. Indeed, such efficiencies underpin the argument for the pro-competitive potential of licensing agreements found in European, US and Japanese policy and regulatory documents.====On the other hand, these authorities recognise that certain contract clauses can diminish licensing's social value. For example, the treatment of improvements to the licensed technology, and in particular whether improvements made by licensees must be “granted back” to the licensor “may reduce a licensee's incentives to engage in research and development and thereby limit rivalry.” (FTC, 2016)====The policy landscape for grantbacks has not crystalised into a single dominant approach. Despite misgivings about their effect on innovation incentives, the landmark ==== case==== in the US established that grantbacks were not ==== illegal. Recent proposed US guidelines echo this by stating that, “[a] non-exclusive grantback …may be necessary to ensure that the licensor is not prevented from effectively competing because it is denied access to improvements developed with the aid of its own technology” (FTC, 2016). A decidedly more negative view can be found in the 2008 Anti-Monopoly Law of the People's Republic of China, which argues that grantbacks are likely “[to be] injurious to proper functioning of normal competition” despite the procompetitive effects of “reducing licensing risks for licensors and…facilitating innovation”. (Ning et al., 2016). Indeed the National Development and Reform Commission in China in a recent decision, rejected Qualcomm's royalty free grantback as restraining innovation and restricting competition in wireless communication technologies as a violation of the Anti-Monopoly Law 17(1).==== Qualcomm was fined 6.088 billion RMB in this decision, and was required to remove the royalty free grantback.====The 2004 EU ==== proposed a different approach, suggesting that the seriousness of anticompetitive and, in particular, innovation concerns depended on the nature of the licensee's innovation. Grantbacks involving severable innovation – innovations that can be used independently or without infringing upon the licensed technology==== – were viewed as more harmful than those applying to non-severable innovations, especially when the grantbacks were exclusive. This approach can also be seen in the Japanese Guidelines, where grantbacks of improved technology that must be used with the licensed technology (ie, non-severable innovation) would generally not be seen as impeding fair competition.==== We allow for this distinction amongst technologies in our modelling and find that, indeed, technology type is an important contributor to both the use of grantbacks and their effects on innovation.====Publicly available legal advice on grantbacks seems unambiguous, as is the argument for why such clauses are necessary. In the words of one of many legal websites, “a properly drafted grantback license can encourage the licensing of technology by removing the fear that the licensor could find itself competing with a licensee who has developed an improvement to its technology.” (McGurk, 2013) Our modelling allows us to test the validity of this “but for…” justification for grantbacks. The same legal advice states that, “an improperly drafted grantback clause risks being viewed as an anticompetitive provision that inhibits innovation.” We also probe this claim.====Given this advice, one would expect that grantbacks would not be uncommon even if they carry some risk of being viewed as anti-competitive. Indeed, Cockburn (2007) finds that 43% of licensing contracts contain such clauses. Hence, while common and available, grantbacks are only accessed in some cases. This paper has multiple purposes, then: to investigate the conditions under which grantbacks would arise, their implications for innovation incentives, and the validity of both the publicly available legal advice for grantbacks and the appropriate policy response.====Others have had the same ambition, and indeed the economics literature has established some important results on grantbacks. Van Dijk (2000) shows that these clauses may decrease innovation incentives. This decrease can improve welfare when the underlying incentives to invent (regardless of grantbacks) are socially excessive. Choi (2002) argues that there may be socially beneficial incentives to use grantbacks in situations of asymmetric information. These contributions leave scope for further work, however. Policy debate regularly calls attention to socially insufficient incentives to innovate, in contrast to the case treated by Van Dijk.==== Furthermore, while asymmetric information certainly may be important in licensing negotiations, Moreira et al. (2012) find that grantbacks tend to be more common amongst firms in the same product market and familiar with the relevant technologies, suggesting that the case of symmetric information remains empirically relevant.====Hence, in contrast to these papers, we consider a framework where there is an underlying socially insufficient incentive to innovate, and where information is symmetric across licensors and licensees. Consistent with our observations of practice, above, our model includes a licensor, a licensee and two markets separated by a transportation cost.==== Also following observed practice, we allow licensing contracts to contain territorial restraints, i.e. clauses that reserve one of the markets to one of the two firms. Importantly, we assume that the licensing agreement enables the licensee to develop a “follow-on” improvement of the licensed technology. In this setting, we examine equilibrium behaviour and innovation incentives under grantbacks and licensing in situations where the follow-on technology is severable and situations where it is not.====We find that, for non-severable innovation, grantbacks are not necessary to ensure that follow-on innovation is undertaken where it is efficient in equilibrium. Indeed, the parties choose not to include grantbacks in the licensing contract in equilibrium precisely because both licensing parties internalize the costs of their negative effect on innovation incentives. Hence, our model suggests that, while we would expect licensing to be prevalent, we would not expect grantbacks to arise for non-severable innovations. As the parties choose not to include them, they have a neutral effect on the final licensing contract if they are permitted. Furthermore, we show that “but for” reasoning does not apply to this case.====On the other hand, we find that grantbacks of severable innovations can be necessary to induce technology diffusion. Severability allows the follow-on innovator to escape the royalty obligations of the original licensing agreement. Indeed, avoiding this royalty may be a spur to innovate. The resultant competitive threat means that the licensor may be better off not licensing in the first place. Furthermore, this outcome takes into account that the equilibrium licensing contract may reflect the cost of optimally blocking or deterring follow-on innovation. Finally, in contrast to the non-severable case, these results show that if we do observe licensing of severable innovations, we should expect the agreement to include a grantback for reasons captured by the “but for” argument.====In short, the severable/non-severable distinction is critical to licensing and innovation behaviour, with a more positive view of grantbacks emerging for severable follow-on innovations and following the reasoning contained in standard legal advice. This has policy implications.====The 2014 revision of the ==== dropped the distinction between non-severable and severable innovations for grantbacks. Our analysis finds that the distinction is justified; however, our model implies the ==== of the 2004 ====. This also applies to the Japanese Guidelines (which share the 2004 approach). Secondly, we find that grantbacks may or may not improve the total innovation incentives of the licensor and licensee, taken together, so that concern about innovation incentives may or may not be well placed. As the US draft Guidelines allow for a rule of reason approach, even though a severable/non-severable distinction is not drawn, this approach may be sufficient to ensure that socially beneficial grantbacks are promoted==== as long as arguments based on the nature of innovation and “but for…” reasoning are allowed.====Our model also contributes to the academic grantback literature in two ways. Firstly, we show that grantback clauses can arise in equilibrium even in the absence of asymmetric information and that their effects depend on their interactions with the type of territorial restraints that are typically found in licensing agreements. This makes our analysis complementary to Choi (2002), who focuses on asymmetric information and abstracts from territorial restraints. We investigate innovation incentives in the absence of underlying socially excessive incentives to innovate, making our analysis complementary to Van Dijk (2000) as well.====Our results also contribute to the literature on sequential innovation, following Green and Scotchmer (1995). We examine an environment where information diffuses through licensing rather than diffusing completely via the initial patent disclosure.==== This means that the initial patent holder controls by means of licensing whether a sequence of innovations develops at all. This distinction in the channel of diffusion turns out to be critical. While Green and Scotchmer (1995) find that basic technologies should receive (infinitely) broad patents, we find in contrast that narrow patents could be preferable.==== The reason is that while licensing is socially efficient under broad patents in the Green and Scotchmer (1995) framework, it is insufficient in ours because the basic technology holder can shut down future innovation completely by refusing to license in the first place. Narrowing the patent breadth tends to make future innovations severable and so “commits” the basic technology holder to share the future gains from the technology stream. A grantback allows sharing whilst returning gains from the follow-on innovation to the basic technology holder. This encourages the basic technology holder to initiate the sequence of innovations in the first place. Put differently, since information diffusion occurs via the licensing contract in our model, patent breadth is a less direct and so inferior tool to manage diffusion than licensing terms.====The remainder of the paper proceeds as follows. Section 2 outlines and motivates in detail the model structure. Section 3 derives our results for the case of non-severable innovation, while Section 4 considers severable innovation. Section 5 links our analysis to the analysis of patent breadth. Section 6 discusses extensions, including relaxing both our assumptions on bargaining and on information about the nature of follow-on innovation before that innovation occurs. Section 7 concludes.","Grantbacks, territorial restraints, and innovation",https://www.sciencedirect.com/science/article/pii/S0167718719300566,14 September 2019,2019,Research Article,232.0
"Ding Yucheng,Zhao Xin","Economics and Management School, Wuhan University, China,School of International Trade and Economics, University of International Business and Economics, China","Received 22 May 2018, Revised 27 August 2019, Accepted 4 September 2019, Available online 13 September 2019, Version of Record 27 September 2019.",https://doi.org/10.1016/j.ijindorg.2019.102532,Cited by (0),"“Pay-for-delay” settlement (P4D), in which the brand patentee reversely pays the generic infringer to delay market entry, is typically criticized for blocking competition but is often excused for its potential to maintain innovation. We present a game-theoretic model to show that when the generic firm’s entry decision is endogenized, P4D can actually increase ex post competition under certain conditions. We further explore the impact of P4D on ex ante innovation and find that the brand’s innovation incentive may increase or decrease, depending on the generic firm’s entry cost and other factors. Our findings contribute to the ongoing P4D debate by identifying conditions under which (1) P4D can improve ==== and (2) the trade-off between competition and innovation can be reconciled.","Balancing innovation and competition is a challenging goal for policy makers worldwide. The issue has been highlighted by the recent debate over “pay-for-delay” settlement (P4D), which involves the brand patentee reversely paying the generic infringer to delay pre-expiration entry. On the one hand, P4D blocks competition and is thus opposed by antitrust authorities in both the United States and the European Union.==== On the other hand, industry observers are concerned that generic entry reduces effective patent life and suggest that allowing P4D may help to maintain innovation incentives and benefit consumers in the long run (Grabowski, Kyle, 2007, Higgins, Graham, 2009). Antitrust and intellectual property policies come into conflict in the case of P4D, and its complexity has given rise to inconsistent judicial findings regarding its legality.==== In reality, P4D arises not only in the multi-trillion dollar pharmaceutical industry but can occur in any market where the patent holder would have greater market power if the entrant were excluded (Elhauge and Krueger, 2012), and its effects are definitely worth exploring.====To examine the overall effects of P4D, we borrow the classic framework from Bebchuk (1984) and establish a game-theoretic model to characterize the firms’ competition and innovation strategies. In contrast to the traditional approach, we treat the generic firm’s entry decision as endogenous. The possibility that P4D induces more generic entry is often overlooked but is supported by recent evidence. Fig. 1 displays the share of new molecules experiencing generic challenges between 1995 and 2013. The number rises as the legal landscape becomes more P4D-friendly and falls if the opposite is true. For example, in 2003, when pharmaceutical firms were required to disclose P4D to the FTC, the share of new molecules facing challenges began to decline after 8 years of growth. Around 2005, with multiple courts having declared that P4D does not violate antitrust rules, we observe an immediate increase in the share of drugs being challenged.==== The decline in 2013 also coincides with the Supreme Court and the EC’s decision to subject P4D to antitrust scrutiny. The co-movement of the prevalence of generic challenges and the legality of P4D provides supportive evidence of endogenous generic entry.====In this paper, we endogenize the generic firm’s entry decision and obtain three key findings. First, P4D may actually increase competition. When P4D is allowed, the generic firm expects higher profit and is more likely to initiate pre-expiration entry. We further assume that there is information asymmetry between the brand and the generic firm such that they sometimes fail to reach an agreement despite having incentives to settle and collude. As a result, part of the induced challenges becomes effective entry and increases competition. Moreover, we find that P4D is more likely to improve competition when the entry cost is at an intermediate level. Otherwise, P4D is either insufficient (when the entry cost is too high) or unnecessary (when the entry cost is low) to encourage generic entry and thus either exerts no influence or purely creates collusion that suppresses competition.====Second, P4D has a heterogeneous effect on the brand-name firm’s innovation incentive, defined as the premium of a high-quality patent over a low-quality patent. The effect also depends substantially on the entry cost. When the entry cost is at intermediate level, P4D increases innovation. This is because, in this region, the entry-inducing effect of P4D is relatively weak, and thus, P4D can only induce entry and reduce profit if the patent quality is low, while it has no impact if the patent quality is high. In other words, P4D raises the relative attractiveness of a high-quality patent. When the entry cost is low, P4D decreases innovation. The reason is that the entry-inducing effect of P4D is strong enough to offset both patents’ ability to foreclose entry and therefore disproportionately reduces the value of a high-quality patent.====Finally, P4D might increase or decrease consumer surplus. When the entry cost is at intermediate level, P4D simultaneously enhances competition and innovation. The two seemingly contradictory goals can be aligned. When the entry cost is relatively low, ex ante innovation is weighed against ex post competition, and the impact of P4D on consumer surplus then further depends on the brand-name firm’s innovation cost. When the entry cost is sufficiently low, P4D reduces both competition and innovation and ultimately consumer surplus. In addition, there is sufficient variation in the entry cost based on estimates from the literature. The average entry cost is $5 million, with a range from $250,000 to $20 million, and the expected profit of a generic challenge is $5.5 million (Morton, 1999, Tang, 2013, Hemphill, Sampat, 2013). Given the numbers, we believe that the fraction of potential entrants prevented by entry costs is non-negligible, and allowing P4D to induce entry is meaningful not only theoretically but practically.====First, this paper is closely related to the economic literature on antitrust implications of patent settlements with reverse payment. While most scholars believe that patent settlement with reverse payments is anti-competitive (Janis, Hovenkamp, Lemley, 2003, Elhauge, Krueger, 2012, Edlin, Hemphill, Hovenkamp, Shapiro, 2013, Drake, Starr, McGuire), some argue that P4D can be pro-competitive through saved litigation costs (Elhauge and Krueger, 2012) or reduced business risks (Yu and Chatterji, 2011). Two papers informally discuss the conditions under which P4D can be pro-competitive. Willig and Bigelow (2004) speculate that P4D can improve welfare even if the reverse payment exceeds the litigation cost. Padilla and Meunier (2016) suggest that P4D may have pro-competitive effect when multiple generic entrants exist or there is information asymmetry. In 2013, the Supreme Court promulgated the “Actavis Inference” in ====. Various papers discuss how to interpret this inference. For example, Edlin et al. (2015) propose a simple rule to evaluate whether P4D is anti-competitive. They believe that the settlement should be illegal if the branded firm has paid the generic firm a large amount of cash or something else of value, which is much higher than the litigation cost, and the generic firm agrees to deter entry. Harris et al. (2014) argue that the P4D judgment is complex and that the simple rule in Edlin et al. (2015) may exclude some pro-competitive P4D settlements.====Our main contribution is to highlight the importance of endogenous entry in evaluating the competitive effect of P4D. Recent studies have begun to lend empirical support for the existence of endogenous entry (Jacobo-Rubio et al., 2017) and consider theoretically the entry-inducing effect of P4D.==== For instance, Böhme et al. (2017) exploit the positive effects of inducing more generic challenges to justify the longer collusion caused by P4D. We contribute by further incorporating innovation and examining how entry-inducing effects influence the tradeoff between ex post competition and ex ante innovation. These considerations allow us to establish a more complete set of welfare determinants and more effectively differentiate welfare-improving P4D from others.====Second, this paper deepens our understanding of the impact of P4D on innovation incentives. The common belief is that P4D promotes innovation, although some scholars argue that P4D undermines the optimal innovation incentives because it endows the brand with supracompetitive profits (Elhauge and Krueger, 2012). Our model shows that the impact of P4D on innovation is heterogeneous and suggests that the observed inconsistency can be explained by the entry-inducing effect of P4D. Specifically, in cases in which P4D induces entry, the supracompetitive profits suggested by Elhauge and Krueger (2012) are not in fact guaranteed. Given that a high-quality patent is more capable of deterring generic entry, P4D increases the value of high-quality patents and promotes innovation. P4D reduces innovation in other cases in which P4D does not induce entry, and the supracompetitive profits are secured.====Recently, several theoretical papers examine why and when P4D settlements occur. Bokhari et al. (2017) find that allowing the branded firm to issue its authentic generic drugs will reinforce P4D deals in equilibrium. Palikot and Pietola (2017) discover that patent strength matters: cases with patents that are too strong or too weak are most likely to be settled. Both papers investigate P4D with multiple generic entrants and involve a settlement externality. In Section 6, we also propose a simple sequential entry model and discuss how the pressure to settle with later generic challengers breaks down the settlement between the branded firm and early challengers. However, the key insight of our paper is that generic challenges are endogenous and that the legality of P4D may influence this incentive, which is not addressed in previous theoretical papers.====The remainder of this paper proceeds as follows. Section 2 provides the institutional background. Section 3 establishes a benchmark model to study the effect of P4D on ex post competition. Section 4 examines the impact of P4D on ex ante innovation incentives. Section 5 studies the welfare implications of P4D, combining ex post competition and ex ante innovation. Section 6 conducts several extensions and robustness checks. Section 7 summarizes and discusses our findings. All proofs are relegated to the Appendix.","Pay-for-delay patent settlement, generic entry and welfare",https://www.sciencedirect.com/science/article/pii/S0167718719300542,13 September 2019,2019,Research Article,233.0
"Loertscher Simon,Marx Leslie M.","Department of Economics, University of Melbourne, Level 4, FBE Building, 111 Barry Street, Victoria 3010, Australia,Fuqua School of Business, Duke University, 100 Fuqua Drive, Durham, NC 27708, USA","Received 21 January 2019, Revised 14 August 2019, Accepted 3 September 2019, Available online 11 September 2019, Version of Record 22 September 2019.",https://doi.org/10.1016/j.ijindorg.2019.102531,Cited by (6),"Buyer power features prominently in antitrust cases and debates, particularly as it relates to the potential for a ","Buyer power features prominently in antitrust debates and cases,==== and merger defenses based on buyer power have been said to sometimes be embraced as if they had ==== power (Steptoe, 1993). Nonetheless, buyer power has proved difficult if not impossible to capture in standard oligopoly models like Cournot or Bertrand simply because these models assume price-taking behaviour on one side of the market. Using a Myersonian mechanism design approach, Loertscher and Marx (2019b) provide a framework for merger review for markets with buyer power, assuming that buyer power is a zero-one variable. In the present paper, we extend this analysis by treating buyer power as a continuous variable (technically, as a Ramsey weight) that ranges from zero to one (Ramsey, 1927). This generalization is relevant, among other reasons, because the Ramsey weight can be interpreted as a conduct parameter that can be estimated.====Moreover, in this paper we adhere to an alternative way of modelling cost synergies,==== thereby showing that the main insights from Loertscher and Marx (2019b) are robust with respect to the specifics of how cost synergies are captured.==== As we show, the main comparative statics and the key insights from Loertscher and Marx (2019b) carry over to the more general setting we study here. Here, of course, the quantitative effects will vary continuously with the Ramsey weight.====Conceptually, the present paper is part of an emerging research agenda to further develop incomplete information models in Industrial Organization. This agenda is inspired by Stigler’s arguments (Stigler, 1961, Stigler, 1964) and uses the mechanism design techniques in the tradition of Myerson (1981) and Myerson and Satterthwaite (1983) to model informational asymmetries. Recent theoretical contributions in this area include the aforementioned paper on buyer power (Loertscher and Marx, 2019b), which defines the designer’s power in the same way as Bulow and Klemperer (1996),==== Loertscher and Marx (2019a), which uses a related setup to define and test for coordinated effects and maverick firms, Loertscher and Marx (2019c), which analyzes the competitive effects of mergers that are combined with mix-and-match divestitures, and Loertscher and Niedermayer (2019), which analyzes optimal transaction fees with incomplete information. Important precursors to this strand of literature are the analyses of mergers and collusion in auction-based markets by McAfee and McMillan (1992), Waehrer (1999), Waehrer and Perry (2003), Blume and Heidhues (2008), Miller (2014), and Froeb et al. (2017). Chae and Heidhues (2004) provide a model in which mergers or alliances among buyers can produce a buyer with enhanced bargaining power. Recent empirical work using Myersonian mechanism design techniques to analyze bargaining include (Loertscher, Niedermayer, Larsen, Larsen, Zhang). Although bargaining power parameters can be estimated in models based on Nash bargaining, see, e.g., Crawford and Yurukoglu (2012), Grennan (2013), Gowrisankaran et al. (2015), Ho and Lee (2017), Ghili (2018), Backus et al. (2018), and Dubois et al. (2018), incomplete information bargaining models offer the ability to capture features such as the tradeoff between surplus extraction and efficiency and the possibility of bargaining breakdown, which do not naturally arise in complete information models.====In Section 2 we describe the setup, and in Section 3 we derive the equilibrium. In Section 4 we present the results. Section 5 provides an extension to a different interpretation of the designer’s objective function. Section 6 concludes.",Merger review with intermediate buyer power,https://www.sciencedirect.com/science/article/pii/S0167718719300530,11 September 2019,2019,Research Article,234.0
"Burguet Roberto,Sákovics József","University of Central Florida, United States,The University of Edinburgh, Scotland,Universitat de les Illes Balears, Spain","Received 8 January 2019, Revised 30 May 2019, Accepted 20 August 2019, Available online 29 August 2019, Version of Record 11 September 2019.",https://doi.org/10.1016/j.ijindorg.2019.102530,Cited by (1),"We analyze personalized pricing by a monopsonist facing a finite number of ==== identical, capacity constrained suppliers with privately known costs. When the distribution of costs is sufficiently smooth and regular, the buyer chooses to make the same offer to all suppliers, leading to a posted price. When demand is sufficiently concave (convex) this price is lower (higher) than the classical monopsony price. In the limit as the seller capacities tend to zero, we obtain the classical monopsony price. Therefore, our model provides a decentralized micro-foundation for monopsony.","In this paper we put forward a new model of decentralized pricing in monopsony==== and identify “mild” conditions under which the buyer’s optimal price vector is a common price offered to all (==== identical) suppliers, independently of their number. This lack of qualification is important, as in our model the buyer faces uncertainty about the realized supply for any given price vector. It is thus reasonable to think that even a risk-neutral buyer would want to manage this uncertainty by using a heterogeneous price vector.====The classical model of monopsony postulates a single buyer who faces a deterministic supply curve resulting from the aggregation of suppliers’ (marginal) costs. It is well-known that the optimal linear (posted) price equates the “mark-down”==== to the reciprocal of this supply’s elasticity. Note that, even if the monopsonist could offer different prices to different – but indistinguishable – suppliers with independent, random costs, setting this common price would still be optimal as long as the suppliers were many, each able to supply only an infinitesimal fraction of the monopsonist’s demand. This is so, because the Law of Large Numbers ensures a deterministic aggregate supply, so that the monopsonist knows with certainty the quantity purchased at each price vector. Thus, in the classic scenario, the assumption of posted prices is without loss of generality.====Instead of this idealized situation, we are interested in the more realistic scenario, where the monopsonist is uncertain about the total supply he receives for a given vector of price offers. Think of an agricultural firm buying the crop of smallholders, not knowing at which price each of them would prefer to consume her crop rather than sell it.==== To manage this uncertainty, there is a potential role for personalized price offers to ex-ante homogeneous suppliers. In addition, we also wish to restrict attention to a selling mechanism that is decentralized, in the sense that the monopsonist’s contract with each supplier is “negotiated” independently.====To address the above, we study the optimal (static) personalized pricing policy for a monopsonist that faces a finite number of ==== identical, unit-capacity suppliers of privately known costs. Thus, the monopsonist may offer personalized (take-it-or-leave-it) prices to each supplier, but he must commit to trade at those prices if accepted. That is, agreements/commitments are bilateral: the terms of trade with each supplier is independent of the terms of trade with other suppliers. As we discuss in the conclusions, this personalized-pricing procedure has proven to be particularly useful for modeling price competition between oligopsonists, where it leads to novel insights.====Restricting the monopsonist’s strategy set to personalized pricing is, of course, not without loss of generality. As argued by Bulow and Roberts (1989), in the optimal mechanism for a monopsonist with full commitment power he announces a demand curve and solicits ask prices by sellers. The resulting aggregate supply schedule together with the announced demand is used to establish the market clearing price at which all the suppliers with ask prices below it trade.==== In such an auction, the terms of trade between the monopsonist and any individual seller depend on the bids of the other suppliers. Putting it differently, for any given supplier, the buyer commits not only to the way in which he will use the information ==== reveals, but also to how he will use the information revealed by all the other suppliers. Such a mechanism could not be implemented, for example, by delegated bargaining, where the monopsonist employs agents to bargain independently with different subsets of suppliers. Also, to verify that the monopsonist has delivered according to his promises to a single supplier, one would have to check the prices offered by all suppliers. Such a verification procedure may be highly impractical, especially when the suppliers would not want to reveal their costs to competitors. To capture situations where some verification might be necessary – say, because of suspicion of corruption – but a full verification is unworkable, we investigate monopsony pricing when the buyer is not able/willing to commit to a centralized mechanism.====Our main result is that, under “mild” conditions on the distribution function of costs, posted prices are constrained optimal. The condition we identify is a strengthening of the traditional “regularity” condition in problems of trade under asymmetric information, which requires that the virtual cost==== of an arbitrary supplier be increasing. Roughly, we need the increase in virtual cost to be higher than the density of the cost distribution. Additionally, we show that the range of possible costs is also relevant. When there is a gap between the lowest possible cost and the buyer’s lowest marginal valuation, the monopsonist may prefer to make fewer (serious) offers than there are traders (and demand). If on the other hand, there is a gap between the highest possible cost and the highest marginal valuation, the monopsonist may prefer to make some offers that are surely accepted, in addition to different offers at lower prices. Notably, however, if the slope condition is met, all interior prices (that is, prices that will be refused and accepted both with positive probability) are the same. It is only when the cost distribution of suppliers is particularly convex that we observe heterogeneous interior prices offered to homogeneous suppliers.====We also show that even though the buyer offers a posted price, this price can be lower or higher than the corresponding classical monopsony price (roughly depending on whether the demand function is concave or convex, respectively). The reason is that the monopsony price is determined by a point elasticity, while the personalized price is optimized by taking expectations over the aggregate uncertainty.====Finally, we establish our convergence result: as aggregate supply is broken up into more and more suppliers, the outcome of our mechanism converges to the textbook monopsony pricing against a continuous supply function and the conditions for posted prices to be optimal are eventually always satisfied. That is, our model is a micro-foundation of the “invisible hand” in monopsony.====Following our concluding remarks, in Appendix A, we introduce heterogeneity in cost distributions and show that, contrary to the classic result, it is not necessarily the case that the less elastic market is offered the lower price. We find conditions for the standard insight to prevail. Just as the ones for the optimality of posted prices, these conditions are a strengthening of the standard condition with a term related to the slope of the monopsonist’s demand function.",Personalized prices and uncertainty in monopsony,https://www.sciencedirect.com/science/article/pii/S0167718719300529,29 August 2019,2019,Research Article,235.0
"Belleflamme Paul,Peitz Martin","Université catholique de Louvain, CORE/LIDAM and Louvain School of Management, Louvain la Neuve, B-1348, Belgium,Department of Economics and MaCCI, University of Mannheim, Mannheim, 68131, Germany","Received 30 August 2018, Revised 9 June 2019, Accepted 18 August 2019, Available online 28 August 2019, Version of Record 13 September 2019.",https://doi.org/10.1016/j.ijindorg.2019.102529,Cited by (15),"We consider two-sided platforms with the feature that some users on one or both sides of the market lack information about the price charged to participants on the other side of the market. With positive cross-group external effects, such lack of price information makes demand less elastic. A monopoly platform does not benefit from opaqueness and optimally reveals price information. By contrast, in a two-sided singlehoming ====, platforms benefit from opaqueness and, thus, do not have an incentive to disclose price information. In competitive bottleneck markets, results are more nuanced: if one side is fully informed (for exogenous reasons), platforms may decide to inform users on the other side either fully, partially or not at all, depending on the strength of cross-group external effects and the degree of horizontal differentiation.","In markets with two-sided platforms, cross-group external effects make the individually optimal participation decision on one side dependent on how many users are active on the other. In markets with a lot of turnover of market participants, this decision has to be based on expected participation on the other side. The level of expected participation depends on market characteristics and, if observable, on the actions of platform providers—in particular, their pricing decisions.====This paper focuses on the disclosure of prices. In contrast to most of the existing literature, we do not impose that users know ==== prices. In particular, we would argue that on many two-sided platforms, information about the price charged to the other side is not universally known. Possibly, platforms can strategically decide whether and to what extent they provide price information to the group of users to whom the respective price does not apply. This is an issue that is specific to two-sided platforms in contrast to networks or platforms with only one group of users.====To illustrate what we have in mind, consider a simplified model of video game markets that abstracts from non-linear pricing and lock-in. Game developers know the fees charged by platforms to end users but the reverse is often not the case. Platforms sometimes inform the market about reductions in the costs of developing games for them. For instance, Sony announced a cut in the price for developers in 2007 and in 2009. In 2009 it released the statement that it “will deploy various measures to further reinforce game development for PS3 and will continue to expand the platform to offer attractive interactive entertainment experiences only available on PS3,” and informed the public that it reduced the price of the development kit from US$ 10,000 to US$ 2000. This announcement was not restricted to the developer community, but spread more widely to users. Thus, Sony’s information policy arguably affected the information available to gamers and, therefore, their expectation about the availability of games on the platform.====This paper formally investigates the incentive of platform operators to disclose price information to the other side of the market. We use standard models of competition between two-sided platforms to obtain equilibrium predictions on the pricing behavior for given disclosure rules and then endogenize the disclosure decision. If some users on one side are not informed about the price charged to users on the other side, they cannot infer the intensity of usage on the other side from the observation of actual prices. Instead, these uninformed users have to form expectations about participation on the other side without knowing the prices that platforms charge on that side. We assume that these expectations are constant (that is, users hold passive beliefs), and are confirmed in perfect Bayesian equilibrium. Clearly, users with constant expectations about participation on the other side do not stop to participate when the price on the other side is increased. This makes market demand less price elastic. Consequently, the decision to widely disclose price information tends to lead to lower prices.====If the platform is in a monopoly position and if users move simultaneously on both sides, we find that the platform fully discloses prices. Why is that so? If users do not observe the price on the other side, the platform has an incentive to raise this price too much for its own good. This is similar to the classic opportunism problem (Hart and Tirole, 1990) and generalizes the result by Hagiu and Halaburda (2014) who allow for no or full disclosure on one side of the market, assuming that the other side is fully informed about prices.====By contrast, with competing platforms (and positive cross-group external effects on both sides), full disclosure does not necessarily obtain, as higher prices may benefit both platforms. In particular, under two-sided singlehoming, we show that at equilibrium, platforms do not disclose any information to users on one side about the price they charge to the other side—the outcome under strategic disclosure here is the same as when platforms coordinate their disclosure decisions. Thus, while a monopoly platform always chooses to disclose information fully on both sides of the market, competing platforms decide not to disclose information whatsoever.====In competitive bottlenecks (i.e., a market in which users on one side singlehome, while users on the other side can multihome) results are more nuanced. Because the analysis becomes more involved in this setting, we focus on situations in which users on one side are fully informed; platforms must then decide the extent to which they want to inform users on the other side. In case singlehomers are fully informed, we find that platforms choose to disclose no information to multihomers (about the price they charge to singlehomers) for a large range of parameters. However, if the horizontal differentiation between the platform is very low (i.e., close to the limit under which only one platform would survive at equilibrium), then full disclosure and no disclosure may coexist at equilibrium, or full disclosure may be the unique equilibrium (if the multihoming side exerts much larger cross-group external effects than the singlehoming side). Here, a firm attracts more users on the singlehoming side through full disclosure, and this increase in market share overcompensates the loss in revenue per singlehoming users (accounting for revenues on both sides of the market). In the other case, in which multihomers are fully informed, the information that platforms give to singlehomers depend again on the parameters of the model: If multihomers exert sufficiently larger cross-group external effects than singlehomers, then platforms find it optimal to inform a fraction of the singlehomers, or even all of them (if multihomers exert even proportionately larger external effects and platforms are not too differentiated); otherwise, platforms do not inform singlehomers whatsoever. In all these models of platform competition, platforms would not inform any users if they could coordinate their disclosure decisions.==== The early literature on network effects has considered alternative specifications about output information. In particular, in their seminal paper, Katz and Shapiro (1985) contrast two models. In the first model (developed in the main text), they assume that users first form expectations about output levels (i.e., network sizes) and next, on the basis of their expectations and observed prices, they make their consumption decisions; to tie down equilibrium predictions, expectations are required to be self-fulfilling. In the second model (sketched in the appendix), the authors assume alternatively that firms can commit to output levels, which allows them to directly influence consumer expectations. In more recent work, Griva and Vettas (2011) and Hurkens and López (2014) further explore the effect of user expectations on market outcomes. In a two-sided market setting, Gabszewicz and Wauthy (2014) consider two versions of expectation formation to show that two ex ante non-differentiated platforms can coexist and make positive profits when the strength of cross-group external effects is heterogeneous across users.====Within the two-sided platform literature, we follow Armstrong (2006) in postulating that users on both sides are heterogeneous with respect to their opportunity cost of joining a platform and suppose that platforms set membership fees.==== While a substantial literature has evolved focusing on pricing implications, several contributions have introduced additional strategic variables including Belleflamme and Peitz (2010) studying sellers’ ex ante investment incentives in two-sided markets. In particular, Jullien and Pavan (2019) analyze the information management of platforms that affects the users’ ability to predict participation decisions on the other side. In this paper, we add to this line of research an alternative strategic variable of platforms, namely the platforms’ decisions whether to disclose prices on the other side of the market, which directly affects the ability of users to predict participation decisions on the other side.====As far as we know, the only other paper to explore the impact of price information on equilibrium outcomes in markets with two-sided platforms is Hagiu and Halaburda (2014). We follow Hagiu and Halaburda (2014) in postulating that users observe the price they have to pay, but possibly not the price users on the other side have to pay and that uninformed users have passive beliefs.==== Our monopoly result shows that the result of Hagiu and Halaburda (2014) is robust to allowing partial disclosure on both sides. Under platform competition, we analyze strategic disclosure, whereas Hagiu and Halaburda (2014) consider coordinated decisions. In their competitive bottleneck model, both or none of the two platforms are assumed to fully disclose or to not disclose at all on the singlehoming side;==== all users on the multihoming side are assumed to be fully informed. They find that platforms jointly decide not to inform users on the singlehoming side. This finding generalizes to the other models of platform competition analyzed in our paper: if platforms can coordinate their disclosure decision they do not inform users. By contrast, we consider the platforms’ strategic disclosure decision (either on the singlehoming or on the multihoming side) and show that, depending on the setting and the parameter constellation, platforms fully, partially or not at all disclose information to users.====While we analyze a static setting, expectations about participation decision also matter in dynamic markets with installed user base. Cabral (2019) provides a theoretical contribution to the dynamics of two-sided market; Tucker and Zhang (2010) provide an empirical investigation into a platform’s ability to affect the expectation about participation decisions on the other side of the market.====Our paper also connects to the literature on price transparency. Furthermore, there is a scant literature on the disclosure of price and product information in oligopoly, which analyzes the competitive effects of informing only a fraction of consumers (see, e.g., Schultz, 2004, Schultz, 2004, Schultz, 2009).====Finally, the disclosure decision can be seen as an instance of informative advertising. The literature has considered oligopoly environments in which firms advertise the existence of a product, its price and product characteristics (including the contributions by Butters, 1977, and Grossman and Shapiro, 1984). In our paper, we presume that product and price are known, but that the price on the other side is not necessarily known. Since the price on the other side affects participation on the other side, and participation on the other side generates an external effect, our model captures a situation in which platforms can disclose information that affects the quality perception.====The rest of the paper is organized as follows. In Sections 2 and 3, we analyze in turn the cases of a monopoly platform and of two competing platforms, distinguishing between environments with singlehoming on both sides or with potential multihoming on one side. In Section 4, we make some concluding remarks.",Price disclosure by two-sided platforms,https://www.sciencedirect.com/science/article/pii/S0167718719300517,28 August 2019,2019,Research Article,236.0
"Bauner Christoph,Wang Emily","University of Massachusetts Amherst, Department of Resource Economics, Stockbridge Hall, 80 Campus Center Way, Amherst, MA 01003","Received 31 December 2017, Revised 1 June 2019, Accepted 21 July 2019, Available online 14 August 2019, Version of Record 26 August 2019.",https://doi.org/10.1016/j.ijindorg.2019.102525,Cited by (15),"This paper empirically examines incumbents’ reactions to market entry along price and non-price dimensions in the example of wholesale warehouse entry into grocery retail markets. Leveraging a detailed retail panel spanning 2001–2011 and a novel dataset documenting opening and closing dates and locations of all Costco warehouse clubs, we classify incumbent retailers’ strategic responses (e.g., pricing, assortment) by the storability of product categories, controlling for persistent systematic differences across retailer–product combinations. We find that retailers are substantially affected by increased competition from wholesale club warehouse openings and in response increase the variability of their prices, consistent with adoption of the Hi–Lo pricing strategy. In addition, incumbent retailers’ strategic responses differ significantly across storability levels: They are more likely to increase prices and reduce assortments for highly storable products and decrease prices and increase assortments for less storable products. We extend our analysis by exploiting the spatial variations in our data and analyzing divergent market effects across geographical areas. We find significant geospatial differences in these strategic responses.","Traditional studies in industrial organization in general and analyses of market entry in particular focus on price effects. However, this is only part of the picture, as producers not only optimize over prices but also over product characteristics. In this paper, we provide empirical evidence for product repositioning by incumbent firms in response to market entry. Our analysis focuses on the example of grocery stores’ responses to entry by Costco, the world’s largest warehouse club and the third largest retailer in the United States.====This paper contributes to the growing literature on endogenous product choice by demonstrating that entry can lead to substantial repositioning by incumbent firms. More specifically, we find that Costco warehouse openings lead to adjustments along the dimensions of assortment, price, and pricing format (“Everyday low price” vs.“Hi–Lo”) in incumbent retailers. Taken together, the moves by incumbents are consistent with a strategy of diversification from Costco.====Several features of the grocery retail industry make it particularly suited for studying the effect of entry. First, grocery stores typically compete in local markets.==== This allows us to treat geographically distinct locations as independent markets. Second, detailed data on prices, sales, and assortments are widely available through IRI and Nielsen. Finally, to our knowledge, no technological or regulatory barriers distort incumbents’ price or non-price reactions to entry.====Additionally, warehouse clubs are a particularly interesting segment of the retail sector. They take the “big box” concept of Walmart and Target a step further by setting a unique business model: products in very large package sizes sold for very low profit margins; limited product variety within a given product category;==== and members-only shopping, with membership fees constituting the majority of store profits (Stone, 2013). They rely on a consumer base able to travel longer distances (as most warehouse clubs are located outside of city centers), with transportation conducive to bulk purchasing (i.e., bigger vehicles), and with sufficient income to cover membership fees. How the warehouse clubs position themselves in the retail sector has important consequences for the strategic behavior of incumbent retailers serving the residual demand. Furthermore, their strategies allow us to distinguish between products that are more or less strongly affected by the competition, thus sharpening our results.====While endogenous product choice has been discussed theoretically for almost 100 years, there is little empirical evidence of firms adjusting their products to altered market conditions. To our knowledge, Hotelling (1929), in his seminal article discussing the linear city model, was the first to discuss, albeit somewhat informally, the optimal choice of product characteristics in a duopoly. More formal theoretical explorations of this topic have been conducted since the 1970s, starting with Spence (1975) influential exploration of quality choice in monopoly. This was soon followed up and added to by Spence (1976), Mussa and Rosen (1975), Maskin and Riley (1984). Efforts to conduct similar analyses in different settings continue today (see, e.g., Bauner et al., 2017).====A number of authors have incorporated endogenous product choice in empirical studies. Often, such papers assume that firms set product characteristics according to some structural model and analyze the welfare consequences of government or firm policies of interest. Articles in this vein include McManus (2007), Fan (2013), Nosko (2010), Eizenberg (2014), Wollmann (2014), and Crawford et al. (2015). However, few papers to date have shown empirical evidence that firms do, in fact, adjust their product characteristics to react to changing market conditions. Berry and Waldfogel (2001) and Sweeting (2010) independently analyze the effect of consolidation in the radio industry. Both show significant repositioning of station format as result of increased merger activity. Matsa (2011) provides evidence that incumbent retailers improve their quality when faced with market entry by Walmart. We add to this literature by providing evidence of repositioning after entry in the supermarket industry along several dimensions: pricing, assortment, and pricing format.====This paper is also related to research on the effect of big-box store entry. Walmart’s venture into grocery retail has been the subject of substantial research, which has found decreased prices overall in incumbent retailers (e.g., Basker and Noel, 2009), increased quality (Matsa, 2011), increased consumer surplus (Hausman and Leibtag, 2007), and limited competition outside of local markets (Ellickson and Grieco, 2012). In contrast, little is known about the impact of warehouse clubs on the strategic behavior of incumbent retailers and the reactions of consumers. Furthermore, unlike existing literature, which typically focuses on the overall effect on incumbent retailers using aggregate price indices or retail-level responses such as employment, we examine in detail variations in strategic behaviors—pricing and assortment—using product specific information across product categories and markets.====We concentrate our analysis on Costco. With a 50% market share among warehouse clubs, Costco is the largest retailer of its kind in the United States (Evans and Satchu, 2010) and the world’s third largest retailer overall (Kantar Retail, 2013). To measure Costco’s effect on incumbent retailers, we take advantage of a detailed retail panel documenting sales volumes and prices for each Universal Product Code (UPC) sold in each week from 2001–2011 for 27 frequently purchased product categories and a novel dataset documenting opening and closing dates and locations of all Costco warehouses. We exploit variations across markets and over time to econometrically control for any systematic differences across retailer–product combinations. Most importantly, this strategy eliminates the effect of persistent local factors that might influence Costco’s strategic decision to enter a local market or the endogenous responses of its competitors.====Our findings suggest that incumbent retailers’ strategic reactions in the face of increased warehouse presence (measured by the number of Costco warehouses in the metro area) differ significantly across product categories. We classify product categories observed in the sample by product storability (e.g., canned soup is highly storable; milk is not). Our classification of storability derives from two definitions: ==== and ====, as defined by Bronnenberg et al. (2008). We further classify incumbent retailers as either small or large. Under both definitions and across small and large stores, we find that incumbent retailers are more likely to increase the prices of more storable products while decreasing the prices of less storable products. Because warehouse clubs’ typically offer products only in large packages, they are in many cases less attractive providers of perishable goods. Hence, the strategy of decreasing prices for less storable items is consistent with an attempt by traditional retailers to differentiate themselves from warehouse clubs.====Our results differ from the only other paper on warehouse clubs of which we are aware, Courtemanche and Carden (2014), which finds that Costco’s entry leads to price increases by non-warehouse supermarkets, while Sam’s Club market entry does not affect local prices. Results from Courtemanche and Carden (2014) run somewhat counter to those of the numerous studies finding that Walmart market entry decreases the prices of products sold by incumbent retailers (Basker, 2005, Volpe, Lavoie, 2008, Basker, Noel, 2009, Lopez, Liu, 2011, Cleary, Lopez, Ellickson, Grieco, 2012). Our study adds significant nuance to Courtemanche and Carden (2014), which relies on quarterly, city-level ACCRA COLI prices and does not break down by storability level. In contrast, our UPC–week–store-level data grant us insight into incumbents’ strategic pricing and assortment adjustments across product categories of different storability levels. This allows us to draw more detailed conclusions and gain a more complete understanding of Costco’s effects on competing retailers.====While we find that price responses differ across product categories, we also find a uniform increase in variation in prices. That is, regardless of price increases or decreases, all product categories for incumbent retailers saw larger intertemporal price variations, consistent with increased use of a Hi–Lo pricing strategy. This is consistent with the theoretical model developed by Glandon and Jaremski (2012), who suggest that entry of a low-cost retailer can lead to more frequent price reductions. Glandon and Jaremski (2012) also find empirical evidence using retail data from Chicago that suggests adoption of Hi–Lo strategy coinciding with Walmart’s expansion in that market. This and our results contrast with those of Ellickson and Misra (2008), who find that frequent price reductions (Hi–Lo) or relatively constant pricing (“Everyday low price” or EDLP) are positively correlated with the pricing format of neighboring stores post Walmart entry.====These results also suggest that Costco entry, and more broadly other actions that are usually considered pro-competitive, can negatively affect substantial numbers of consumers. Hence, policies need to be evaluated at a case-by-case basis rather than by sweeping application of averages.====We further examine how incumbent retailers’ assortment strategies react to entry of Costco’s limited assortment format. Again, incumbent retailers employ different responses across product categories, with differences varying by storability level. More specifically, we find that incumbent retailers generally decrease assortment in more storable categories but increase assortment in less storable ones. This is consistent with retailer efforts to differentiate themselves from Costco and establish a competitive advantage with their product offerings. In reaction to retailers’ changes to pricing and assortment, we find that—not surprisingly—changes in sales also differ across storability levels, with sales volume decreasing for more storable products and increasing for less storable products post Costco entry.====While this apparent strategy of differentiation has clear benefits, it is by no means the only way incumbent grocers could react to increased competition from Costco. In fact, many standard models would suggest that, by increasing competition, warehouse openings lead to lower prices especially in the categories in which Costco is particularly strong. This would intuitively be the case if price elasticities were similar across consumers. On the other hand, if there is substantial variation in price elasticities of demand then it can be rational to cede those with very elastic demand to the warehouse club while earning high margins off the remaining consumers who exhibit less elastic demand. Similar mechanisms are not uncommon. One well-known example is the finding that prices of branded drugs often increase when generic producers enter the market (see Scherer, 1993, and many others).====We extend our analysis and explore how the effects of Costco entry vary across geographically diverse markets with different traffic patterns, population sizes, and incomes. Variation in our data—a total of 485,489,339 UPC–week observations across 33 markets—allows us to analyze the effect of Costco entry across storability levels at the national level and within each geographical market (designated market area, DMA). We find that for certain storability levels, strategic responses are uniform across all markets, while for other storability levels strategic responses are dichotomous between large, coastal markets and smaller, central–U.S. markets. For instance, for least storable products, retailers in larger urban markets (such as New York City) respond to Costco entry by increasing prices, whereas retailers in smaller metropolitan areas such as Des Moines respond by decreasing prices.====The remainder of the paper is structured as follows: We provide some background on the warehouse club industry in general and Costco in particular in Section 2 and introduce the retail data, data on Costco entry and location, definition of storability by perishability and stockpilability, and our empirical strategy in Section 3. Results by product category and geospatial variation are presented and discussed in Section 4, and Section 5 concludes.",The effect of competition on pricing and product positioning: Evidence from wholesale club entry,https://www.sciencedirect.com/science/article/pii/S0167718719300475,14 August 2019,2019,Research Article,237.0
"Bourreau Marc,Grzybowski Lukasz,Hasbi Maude","Telecom ParisTech, Department of Economics and Social Sciences, 46 rue Barrault, Paris 75013, France,Chalmers University of Technology, Department of Technology, Management and Economics, Vera Sandberg Alle 8, Gothenburg 411 33, Sweden,University of Cape Town, School of Economics, Rondebosch, 7701, Cape Town, South Africa","Received 27 October 2018, Revised 29 May 2019, Accepted 23 July 2019, Available online 31 July 2019, Version of Record 8 August 2019.",https://doi.org/10.1016/j.ijindorg.2019.102526,Cited by (1),"In this paper, we study the impact of competition on the legacy copper network on the deployment of high-speed broadband. We first develop a theoretical model, which shows that the relation between the number of competitors and investment in a quality-improving technology can be positive if the quality of the new technology is high enough, and is negative otherwise. We test these theoretical predictions using data on broadband deployments in France in more than 36,000 local municipalities. First, using panel data over the period 2011–2014, we estimate a model of entry into local markets by alternative operators using local loop unbundling (LLU). Second, using cross-sectional data for the year 2015, we estimate how the number of LLU entrants impacts the deployment of high-speed broadband with speed of 30 Mbps or more by means of VDSL, cable and fiber technologies, controlling for the endogeneity of LLU entry. We find that a higher number of LLU competitors in a municipality implies lower incentives to deploy and expand coverage of high-speed broadband with speed of 30 Mbps or more.","The deployment of next-generation broadband access networks, capable of delivering high-speed Internet access, is seen as an important driver of economic and social development. High-speed broadband infrastructures are expected to stimulate growth and job creation, through increased productivity and by stimulating innovation in products and services.====Europe however lags behind other regions, in particular the US, South Korea and Japan, in terms of deployment of next-generation access networks, which has raised concerns from policymakers.==== Some (incumbent) telecommunications operators blame an overly competitive landscape in Europe, which, they argue, has eroded operators’ margins, and as a consequence, their ability to invest in new infrastructures.==== By contrast, alternative operators contend that it is competition that drives investment.====Competition in broadband markets in Europe has developed via liberalization and the introduction of a specific regulatory provision, “local loop unbundling” (LLU), a policy that enables alternative operators to lease wholesale access to the incumbents’ legacy copper networks to offer broadband Internet access services to consumers. LLU aimed at facilitating entry of alternative operators, but is impact on investment has been hotly debated.====While local loop unbundling was abandoned in the US in 2005, in Europe it has been a cornerstone of the regulation of broadband markets over the last ten years. The European local loop unbundling regulation, which was implemented in the early2000’s,==== eventually led to a wave of entry of alternative operators in local markets, offering broadband services to residential and business consumers through the DSL (‘Digital Subscriber Line’) technology.====In this paper, we study how the number of LLU entrants in a local market, which has resulted from this entry wave, affects today the incentives of broadband service providers to roll out next-generation access network infrastructures (‘NGA networks’). The impact of the number of local competitors on investment incentives is ==== ambiguous. On the one hand, a higher number of LLU operators delivering DSL services, and hence a more competitive local broadband market, reduces the expected profits from offering NGA services, and therefore the incentive to invest (a profitability effect). On the other hand, a higher number of LLU competitors implies lower pre-investment profits from existing broadband operations. The opportunity cost of investment (foregone ==== profits), which corresponds to Arrow’s famous ‘replacement effect’ (Arrow, 1962), is thus lower, which increases the incentive to invest. Which of these two effects dominates is ==== unclear.====We start by developing a simple theoretical model to study the impact of the number of competitors using an old technology (in our context, basic broadband) on a firm’s incentive to invest in a quality-improving new technology (the NGA network technology). We show that the relative impact of the number of competitors on the profitability of the investment and on the replacement effect depends on the quality improvement brought by the new technology, compared to the old technology. Using a specific model of quantity competition with quality differentiation, we find that if the quality improvement is sufficiently high, the relation between the number of competitors and the investment incentive is positive, and that otherwise it is negative.====We test these theoretical predictions using a comprehensive data set on the market structure and the deployment of high-speed broadband in local municipalities in France. We adopt a two-step empirical approach, which allows us to estimate the impact of the local market structure on the deployment of high-speed broadband, controlling for the endogeneity of market structure.====In the first step, we build a model of entry of alternative operators in local municipalities via local loop unbundling. We estimate this entry model using panel data on the number of LLU entries and exits in 36,104 municipalities over the period 2011–2014. We find that local market characteristics, such as the size of the market and the density of population, are important determinants of LLU entry. We also find significant sunk costs which represent a barrier to entry, though entry becomes easier over time.====In the second step of our empirical approach, we estimate how the number of LLU operators in a municipality affects the deployment of high-speed broadband. To do so, we use a cross-sectional data set for the second quarter of 2015 on the coverage of different speed tiers in municipalities. We control for the endogeneity of LLU entry by means of a control function approach, using our model of LLU entry estimated at the first step of the analysis. We also take into account local market characteristics such as market size, population density and income, and the heterogeneity in local market conditions. We find that a higher number of LLU competitors in a municipality has a negative impact on the deployment and coverage of fast broadband, delivering speeds of 30 Mbps or more by means of VDSL, cable and fiber technologies.====The remainder of the paper is organized as follows. In Section 2, we review the relevant theoretical and empirical literature and discuss our contribution. In Section 3, we present the theoretical model and results. In Section 4, we provide some background on the broadband industry in France and describe our data sets. In Section 5, we introduce the econometric framework, and in Section 6 we present the estimation results. Section 7 concludes.",Unbundling the incumbent and deployment of high-speed internet: Evidence from France,https://www.sciencedirect.com/science/article/pii/S0167718719300487,31 July 2019,2019,Research Article,238.0
"Spatareanu Mariana,Manole Vlad,Kabiri Ali","Rutgers University, USA,University of Buckingham, UK,FMG, London School of Economics, UK","Received 15 January 2018, Revised 3 June 2019, Accepted 10 June 2019, Available online 26 July 2019, Version of Record 19 August 2019.",https://doi.org/10.1016/j.ijindorg.2019.06.002,Cited by (13),"This paper highlights the importance of bank-based ==== for the innovation activity of UK firms. It identifies both theoretically and empirically how bank shocks affect firms’ innovation. We develop a theoretical model, and test its predictions using a new matched bank-firm-patent dataset for the UK. We find that bank distress during the 2008 and 2011 crises negatively affected firms’ innovation behavior. After carefully controlling for several potential biases in estimation we find that firms whose relationship banks were distressed not only patented less, but those patents were of lower technological value, less original and of lower quality. The negative effect is significantly larger in the case of small and medium size enterprises (SMEs). We also find that banks’ specialization in financing innovation mitigates the impact of bank distress on innovation.","“Here's a weighty fact: In 2007, the Congressional Budget Office published long-term projections of potential G.D.P. that assumed the United States would grow around 2.7% a year for the ensuing decade. It didn't. Growth in both the labor force and worker productivity underperformed those projections. So the reality we're living in underperforms that theoretical potential by $2.2 trillion, or 14%.====One possibility of what went wrong is that the damage of the deep 2008 recession had lasting effects, both pulling some Americans out of the work force and causing businesses to underinvest in innovations.”====The year 2007 may remain in the collective memory as the year the first Apple iPhone==== was introduced and as the start of the Great Recession. The iPhone was made possible by a large number of innovations (Steve Jobs, Apple's former CEO, mentioned over 200 patents associated with the iPhone), while the severe financial crisis hurt banks, affected firms, and possibly reduced their innovation. Could it be that the recent crisis stymied the next iPhone like innovation?====Innovations and technological change are crucial for long term growth and economic development (Aghion and Howitt, 1992, Aghion and Howitt, 1998). Evidence shows that innovations increase productivity through a more efficient use of factors of production. Investments in innovation can, however, be suboptimal. The existence of large positive knowledge spillovers can lead to private underinvestment, while credit frictions due to information asymmetry may impede the optimal flow of finance to innovative areas of the economy, an effect which may worsen considerably during recessions.====Given the importance of innovation to economic growth and the role that finance plays in the efficient allocation of credit, it is critical to understand how a financial system may promote or impede innovation and hence long-term growth.==== The recent global financial crisis offers a useful setting for analyzing the connection between the banking system and the real economy. In this paper we focus on the UK, where the recent crisis severely impacted its banking system, leading to an almost total credit freeze and tremendous uncertainty in the economy. Evidence suggests that “shocks to the availability of credit can constrain resource allocation and severely affect firm performance” (Nanda and Nicholas, 2014). While there is a growing literature linking bank health to firm performance,==== few studies have investigated the effect of bank distress on firms’ innovation, particularly not through the lens of the 2008 and 2011 banking crises emanating in the USA and Eurozone. This paper aims to fill this gap in the literature by examining the impact of bank distress on firms’ innovation using detailed micro data for the UK. We investigate both theoretically and empirically, the broad question of the importance of bank-based debt financing for facilitating the innovation activities of firms. We also identify how bank shocks may disrupt several dimensions of these innovation activities for UK firms. We explore this issue from the perspective of firms’ financing of their innovative behavior, how this interaction behaves under stress and which firm and bank characteristics shape the flow of finance that spurs innovation.====Following a synchronized global housing boom and large mortgage credit expansion in the early 2000s, the UK experienced a severe systemic banking crisis in 2008. The first stirrings of the crisis began as early as 2007 as US Subprime mortgages became the object of investor doubt and the value and liquidity of these securities, held throughout the global financial system, fell precipitously. As this process of reassessment of the value of financial assets held throughout a highly interconnected global financial system, combined with the faltering housing boom in the UK, there was severe pressure exerted on the UK financial system. The apex of the crisis in October 2008 led to a global liquidity squeeze and fears of counterparty insolvency, manifesting as severe pressure on financial systems across the USA and Europe. The ensuing crisis culminated in unprecedented coordinated government and central bank rescues.==== The financial distress ultimately created credit restrictions for many firms in the UK as banks shed risk by selling assets and withdrawing credit==== in an attempt to raise capital-to-asset ratios. The contemporaneous decline in economic activity in the UK during the crisis of 2008 was marked. GDP fell (-) 0.5% and (-) 4.2% in 2008 and 2009, respectively and unemployment rose from 5.4% in 2008 to crest at 8% in 2012.==== Barnett and Thomas (2014) suggest the majority of the credit lending declines were due to bank supply rather than changes in firm risk and that 33–50% of the aggregate GDP fall from 2008 was caused by credit supply restrictions.====The subsequent impact of the ‘Great Recession’ on the UK economy has been long lasting and its long-term effects are still unknown. Given the inability of productivity in the UK to return to its pre-crisis trend (ONS, 2017), the lack of a productivity recovery remains of major concern to Government and firms. As long-term economic growth is of major importance, and innovation a widely acknowledged key to that end, the channels by which innovation may be enhanced or inhibited by the financial system are of acute interest. The dynamics and dimensions of how the financial system affected the innovation performance of the UK are hence the focus of this paper.====Recent literature on the topic of firms’ innovation performance and bank credit restriction demonstrates that there is still limited understanding into how the link operates. It is well known that financing innovation is particularly difficult due to the uncertainty and information asymmetry associated with firms’ research activities (Hall and Lerner, 2010). Despite this, bank finance may play a significant role in financing innovation. Benfratello et al. (2008) uses data on Italian firms to show that local bank development improves the probability of process innovation, but it does not significantly affect product innovation. As some recent evidence from Mann (2018) shows, debt financing is actually common for innovative firms, patents are frequently used as collateral, and bank loans seem to directly finance R&D. Consistent with this view, Chava et al. (2017) find that banks are willing to offer lower-priced loans to firms with more valuable patents, drawing a clearer picture of the role of bank credit mechanisms and firms’ innovation abilities.==== Cornaggia et al. (2015) use US data to show that banking competition promotes innovation by small private firms, who depend more on bank finance for capital. Using intra-state banking deregulation in the US as a negative shock to relationship-based bank lending, Hombert and Matray (2017) find that the shock has a negative effect on small innovative firms, due, partially to the departure of productive inventors from the affected firms. Taken together, these papers suggest that banks’ contribution towards financing innovation is meaningful, and hence that the role that bank distress played in affecting innovation during the Great Recession is an area ripe for investigation.====The few existing studies in this area rely on various ==== measures of firms’ access to external funds, and the results of these studies are mixed. Some recent papers use ==== to investigate the impact of the financial crisis on firms’ innovation. Archibugi et al. (2013) use the UK Community Innovation Surveys to analyze British firms’ innovation performance relative to its pre-2008 crisis behavior. They find that, in general, firms were willing to reduce investment on innovation as a response to crisis, even if a small group of ‘pre-crisis high innovation’ firms continue to invest in innovation at the same rate during the crisis. Kipar (2011) uses survey data from German firms to investigate the impact of restricting bank lending on the probability of discontinuing innovation and finds that such an effect occurs. Paunov (2011) studies firms’ reactions to the recent crisis in eight Latin American countries and finds that financial constraints and negative demand shocks had a negative effect on innovation. Additionally, she identifies young exporting firms or suppliers to multinationals as the most sensitive to these shocks. In contrast, Almeida et al. (2013) find that financial constraints positively affect firms’ innovation, because they improve the efficiency of innovation (measured as the ratio between the number of patents and R&D expenditure). The positive effect is stronger for firms with high excess cash holdings and low investment opportunities, and among firms in less competitive industries.====Although clearly of merit, one major drawback to the above authors’ use of survey data is that it does not allow them to fully correct for endogeneity, which is a major concern in such estimations. Furthermore, all the papers cited above lack a deeper theoretical rationale for the link between firms’ access to external finance and innovation. Importantly, they do not have the much needed and highly revealing firm-bank linkage information.====Our paper aims to improve the preceding literature on the effect of bank shocks on firms’ innovation in several major ways.====Firstly, we use ==== measures of firm-bank relationships using hand-matched data, rather than indirect measures of credit constraints used in previous studies, which inadequately capture a firm's access to external credit. Our data allows us to directly test for the transmission of crises from the banking sector to firms in the real economy. To pave the way for the deeper question of how various dimensions of innovation are affected, we use various patent-based measures of firm innovation. By expanding the scope of our tests to accommodate innovation volume, technological value, quality and originality using patent data matched to firms, we show that bank distress during the 2008 crisis caused firms to innovate less, and to produce innovations of lower technological value, lower quality and less originality.====Secondly, as we know that innovation is inherently risky and uncertain, relations with a bank ==== in financing innovation may be beneficial for innovative firms especially during crises episodes. We therefore shed light on whether banks specialized in financing innovation, which have a better understanding of the value of innovative projects, mitigate the negative impact of bank distress on innovation. We are therefore able to answer broader questions regarding optimal financial intermediation for innovation and open the door to policies that may encourage such specialization.====Thirdly, we provide a theoretical motivation for why bank distress affects firms’ innovation, which is lacking in previous papers. We construct a stylized theoretical model that links banks and firms. In the model, firms’ innovation is financed exclusively with internal funds due to prohibitive information asymmetry, while production activities can be financed either with internal funds or, if internal funds are not available, with more expensive external funds, if the bank is able to loan them. We show that bank distress forces banks to cut loans to firms, which decreases the probability that the firm invests in innovation.====Fourthly, we are also able to overcome endogeneity in estimation in order to produce unbiased estimates of the effect of the banking crisis on UK firms’ innovation. The use of complex datasets in these dynamic settings requires that we carefully use a variety of tests to account for possible endogeneity. We employ propensity score matching techniques (PSM), two instrumental variable tests, and control carefully for demand shocks. Furthermore, the level of detail we apply to our tests provide a stronger basis for answers to a comprehensive set of questions about how financial shocks affected innovation in the UK and the broader role of the financing of innovation via bank debt.====Finally, we account for firm heterogeneity, and focus specifically on SMEs. These companies provide more than half of the employment in the private sector in the UK; this subset of innovative firms are also some of the most dynamic firms in the UK economy – implementing innovations, opening new markets and hiring workers (Lee et al., 2015). Despite their importance SMEs are usually more liquidity constrained and lack alternative sources of outside financing relative to large firms (Wehinger, 2013). Post-crisis, they have been the focus of much UK Government concern==== as they play a vital role in the economy.====Our results highlight important effects. Firms that borrow from banks that become distressed, ultimately patent less and those patents are of lower technological value, less original and of lower quality. The effects are even more pronounced in the case of SMEs, which are widely seen as the most dynamic and innovative firms in the economy. The results are robust to carefully correcting for possible reverse causation in estimation and other biases. These findings are important as we show that an area of economic activity—not traditionally believed to be sensitive to bank related credit frictions—was negatively impacted by the recent banking crises. Our results show that even short-term credit supply disruptions, due to bank distress, may have long-term effects on economic growth by decreasing the quantity and the quality of firms’ innovation. The paper may also go some way to further explain the current productivity stagnation in UK.====The paper is structured as follows: the next section presents a theoretical model highlighting the link between bank distress and firms’ innovation; Section 3 describes the data; Section 4 presents the econometric strategy. Section 5 discusses the basic results (Section 5.1), deals with endogeneity in estimation (Section 5.2), and outlines how the analysis explicitly controls for demand in the regression to prevent bias (Section 5.3). Section 6 analyses the role of bank specialization in financing firms’ innovation. Section 6 examines firm heterogeneity and its effect on innovation. Section 8 discusses other robustness checks. Conclusions follow.",Do bank liquidity shocks hamper firms’ innovation?,https://www.sciencedirect.com/science/article/pii/S0167718719300426,26 July 2019,2019,Research Article,239.0
"Hunold Matthias,Muthers Johannes","University of Siegen, Unteres Schloß 3, 57068 Siegen, Germany,Johannes Kepler Universität Linz, Altenberger Straße 69, 4040 Linz, Austria","Received 17 November 2017, Revised 10 May 2019, Accepted 2 July 2019, Available online 22 July 2019, Version of Record 30 July 2019.",https://doi.org/10.1016/j.ijindorg.2019.102524,Cited by (5),"We characterize mixed-strategy equilibria when capacity-constrained suppliers can charge location-based prices to different customers. We establish an equilibrium with prices that weakly increase in the costs of supplying a customer. Despite prices above costs and excess capacities, each supplier exclusively serves its home market in equilibrium. Competition yields volatile market shares and an inefficient allocation of customers to firms. Even ex-post cross-supplies may restore efficiency only partly. We show that consumers may benefit from price discrimination whereas the firms make the same profits as with uniform pricing. We use our findings to discuss recent competition policy cases and provide hints for a more refined coordinated-effects analysis.","The well-known literature based on Bertrand (1883) and Edgeworth (1925) studies price competition in the case of capacity constraints – but does so mostly for homogeneous products and no spatial differentiation. A recent example is Acemoglu et al. (2009). We contribute at a methodological level with a model of spatial competition where capacity-constrained firms are differentiated in their costs of serving different customers and can charge customer-specific prices. This leads to mixed pricing strategies with different prices for different customers, which is a new and arguably important addition to this strand of literature. One novel aspect of our analysis is that the mixed strategies induce a cost-inefficient supply of goods.====Various competition policy cases feature products with significant transport costs for which location or customer-based price discrimination is common. There are also merger control decisions in relation to such products that use standard Bertrand–Edgeworth models, which unfortunately do not take spatial differentiation and customer-specific pricing into account. For instance, in the assessment of the merger M.7009 HOLCIM/CEMEX WEST the European Commission argued “====” Moreover, it reasoned that “====.” As a supporting argument, the European Commission referred to a Bertrand–Edgeworth model with constant marginal costs and uniform pricing.====Our model makes several predictions that may add to the understanding of presumably anti-competitive practices: Even with overcapacities of 50%, we find that in a competitive equilibrium firms may always serve their closest customers (“home market”), and then at prices above the costs of the closest competitor. Firms set high prices in the home markets of rival firms, although a unilateral undercutting there seems rational in light of their overcapacities. Such a pattern is difficult to reconcile with previous models of competition. To answer the question of whether firms are indeed coordinating or competing, our model – which features spatial differentiation, location-specific pricing, and capacity constraints at the same time – could therefore improve the reliability of competition policy assessments. In addition to the cement industry, these key features can be found in a number of other industries like the production of commodities, chemicals, and building materials. The transportation costs in our model can also be interpreted as costs of adaption.====We find that firms play mixed strategies in prices so that a firm sometimes serves a customer while another firm with lower costs has free capacity. This inefficiency arises in a symmetric setting with efficient rationing, where firms have collectively sufficient capacity to serve all customers and can perfectly price discriminate between customers. There thus seem to be enough instruments to ensure that prices reflect costs and the intensity of competition for each customer. Firms operate under complete information, which means that the allocative inefficiency arises purely due to strategic uncertainty: As one competitor does not know which prices the other competitor will ultimately charge in equilibrium, the less efficient firm sometimes wins a customer. This natural insight that price competition can lead to strategic uncertainty and thereby inefficient outcomes is, to our knowledge, very rarely reflected in formal models.====The allocative inefficiency provides a rationale for cross-supplies between the competing suppliers. There is scope for such subcontracting when one supplier makes the most attractive offer to a customer, while another one has free capacity to serve that customer at lower costs. Cross-supplies can be observed in various industries. For instance, see Marion (2015) for a recent article on subcontracting in highway construction. We show that cross-supplies may restore the efficiency of the market equilibrium. Firms refrain from subcontracting when this frees up the capacity of a constrained firm that has set low prices – as the additional capacity can increase competition on (otherwise) residual demand segments of the market.====Comparing price discrimination with a restriction to uniform prices, we show that price discrimination can increase consumer surplus by reducing the allocative inefficiency. Contrary to established arguments that price discrimination may reduce the firms’ profits by increasing competition, we find that competing firms may obtain the same profits as with uniform pricing.====The remainder of the article is structured as follows. We discuss the related literature in the next section, introduce the model in Section 3, study the case in which each firm has to charge a uniform price in Section 4, and the case in which firms can price discriminate in Section 5. We compare the market outcomes with and without price discrimination in Section 6. In Section 7, we introduce subcontracting. We conclude in Section 8 with further discussion on the inefficiency associated with competition, subcontracting, price discrimination, endogenous capacities, and Bertrand–Edgeworth arguments in competition policy.",Spatial competition and price discrimination with capacity constraints,https://www.sciencedirect.com/science/article/pii/S0167718719300463,22 July 2019,2019,Research Article,240.0
Espinosa Romain,"CREM, CNRS, Université de Rennes I, France","Received 12 August 2018, Revised 7 July 2019, Accepted 10 July 2019, Available online 18 July 2019, Version of Record 3 August 2019.",https://doi.org/10.1016/j.ijindorg.2019.102523,Cited by (21),"In this paper I investigate the role of e-reputation mechanisms on illegal platforms that specialize in drug sales. I ask whether online reputation systems can limit the risk of scamming (i.e. fraud) by dishonest sellers, and thus prevent Akerlof-like market destruction. I do so by analyzing all published offers on the second-largest platform operating on March 18th 2017 (Hansa). Three types of drugs show relatively low scamming risks, with the average probability that a random seller effectively send the ordered good of over 83%. The recent shutdowns of the two leading platforms are likely to increase this probability by 2.7 to 9.7%. Endogeneity may either lead us to overestimate the effect of e-reputation mechanisms (e.g., unobserved heterogeneity in sellers) or underestimate it (e.g., better-functioning markets may attract more scammers).","Economists consider online reputation mechanisms as virtuous tools that can protect markets from low-quality sellers. From this point of view, these mechanisms help reduce the negative externalities resulting from low-quality products and bad sellers (Akerlof, 1970). Honest sellers can thus signal their trustworthiness to imperfectly-informed buyers via their positive e-reputation, with dishonest sellers being punished by negative ratings (MacLeod, 2007). The core question at the heart of this work is the sustainability of unregulated markets in the presence of asymmetric information. In other words: Does e-reputation on its own suffice to discipline the market and sustain high levels of trust and transactions between sellers and buyers? Most research on online platforms (e.g., eBay) confirms this intuition, showing that the overwhelming majority of buyers evaluate their past transactions positively. However, almost all work on the role of e-reputation has considered online legal marketplaces where legal enforcement can also be applied to restrain dishonest sellers. This research is therefore unable to disentangle the disciplinary effect of reputation from the threat of consumer-protection Law. In this paper, I instead investigate the effectiveness of reputation in disciplining the market when legal enforcement is not available by analyzing illicit drug transactions on Darknet Markets.====Illicit online marketplaces, also called Darknet Markets (DNMs), are illegal platforms mainly specialized in drug-selling that are only accessible through secured protocols (TOR, PGP). Transactions on these platforms are fully anonymized via the use of pseudonyms and crypto-currencies (e.g. Bitcoin) and do not benefit from legal protection. To offset the relatively high risks of scamming==== (i.e. fraud), DNMs have made extensive use of technological developments (e.g. the use of multi-signatures for Bitcoin transactions), but have mostly relied on e-reputation mechanisms. As in clearnet marketplaces such as eBay, Amazon and Airbnb, Darknet Markets have reputation mechanisms that allow customers to supply feedback after the completion of their transactions. The very broad use of reputation mechanisms may be the key to the development and longevity of DNMs despite their associated substantial opportunities for scamming and their recurrent shutdowns by public authorities.====I here propose to investigate the role of e-reputation on DNMs and infer the quality of transactions, i.e. the risk of scamming. I do so by analyzing data that I collected on the second-largest platform for drug-dealing operating in March 2017 (the ==== market). I first estimate the relationship between seller reputation and pricing, using about 6000 unique offers for four types of drugs (==== and ====). Second, I derive a model of reputation-building and pricing on Darknet Markets that I estimate for the four types of drugs considered. Finally, I discuss the short- and medium-run impact of the recent shutdown of the Hansa market on future transactions.====My results show that DNM sellers make extensive use of online reputation systems. As in legal marketplaces, I observe a distribution of reputations that is extremely favorable to sellers (a great deal of positive feedback and only little negative feedback). I then show that sellers with more positive reputation profiles charge significantly higher prices for at least three types of drugs (==== and ====). This confirms Shapiro’s well-established prediction of a reputation premium for high-quality products (Shapiro, 1983). Third, eliciting transaction quality from pricing strategies and reputation profiles, I find that sellers provide a surprisingly high quality of service on the ==== Market: the probability that a random seller effectively send the order lies between 83% and 88%. Fourth, I show that consumers welfare varies greatly by drug type: customers retain about 25% of their surplus for Weed and Ecstasy transactions, but only 1.4% for Hash. Fifth, the recent shutdown of the two leading platforms, the ==== and ==== markets, is likely to have reduced the short-run risk of scamming but will also have reduced sellers’ profits. While the Ecstasy market is likely the least affected by the shutdown, with an average price fall of 2.9% to 5%, Weed and Hash sellers are expected to suffer significantly from these shutdowns (with average price falls of 4.4% to 8.9% and 11.3% to 14.2% respectively).====This research contributes to the existing literature in several innovative ways. First, although numerous contributions have considered clearnet platforms, this is one of the first to discuss reputational effects on Darknet Markets. In this respect, I analyze the effect of reputation when it is the most needed, namely when customers have no legal protection. Second, the vast majority of work on clearnet markets has considered auction-based models to structurally estimate the role of reputation. I here propose a new perspective on e-reputation that takes into account the particularities of Darknet Markets. Third, this is the first attempt to estimate the risk of scamming on Darknet Markets. Understanding this risk helps us to better understand the efficiency of environments outside of the Law. The discussion proposes some elements to help predict the effects of the shutdown of illegal platforms; this is central for public authorities who seek to reduce drug addiction and fight against organized crime.====The remainder of the paper is organized as follows. Section 2 reviews the literature on Darknet Markets and previous work on e-reputation mechanisms, and Section 3 discusses the ==== market. Section 4 presents the data collected on the ==== market and Section 5 the use of reputation mechanisms on this platform. Section 6 is devoted to the estimation of a model of reputation-building and pricing for the four drug types. In Section 7, I then discuss the impact of the recent shutdowns of the two leading platforms. Last, Section 8 concludes.",Scamming and the reputation of drug dealers on Darknet Markets,https://www.sciencedirect.com/science/article/pii/S0167718719300451,18 July 2019,2019,Research Article,241.0
"Liu Bin,Lu Jingfeng","School of Management and Economics, CUHK Business School, Shenzhen Finance Institute, The Chinese University of Hong Kong, Shenzhen, China,Department of Economics, National University of Singapore 117570, Singapore","Received 25 September 2017, Revised 15 April 2019, Accepted 24 April 2019, Available online 9 July 2019, Version of Record 26 July 2019.",https://doi.org/10.1016/j.ijindorg.2019.04.005,Cited by (6),"Contestants often need to incur an opportunity cost to participate in the competition. In this paper, we accommodate costly entry and study the effort-maximizing prize allocation rule in a contest environment of all-pay auction with incomplete information as in Moldovanu and Sela (2001). As equilibrium entry can be stochastic, our analysis allows prize allocation rule to be contingent on the number of entrants. With free entry, Moldovanu and Sela establish the optimality of winner-take-all when effort cost function is linear or concave. Costly entry introduces a new trade-off between eliciting effort from entrants and encouraging entry of contestants, which might demand a more lenient optimal prize allocation rule. Surprisingly, we find that the optimality of winner-take-all is robust to costly entry when cost function is linear or concave. On the other hand, we provide examples to show that the new trade-off due to costly entry does make a difference to the optimal design when effort cost function is convex.","Contests are widely employed in practice (e.g., R&D competitions, sports, school admissions, internal labor markets, etc.) to incentivize agents to exert productive effort by awarding prizes. Effort maximization has long been a natural goal for optimal design, and prize allocation rule has been well studied among other devices as a primary instrument to enhance the performance of a contest. Along this line, the seminal work of Moldovanu and Sela (2001) has established the effort-maximizing rule in an all-pay auction environment with incomplete information. A well-known result is that a winner-take-all prize allocation rule is effort-maximizing when the agents’ effort cost function is linear or concave. This result well rationalizes the popularity of winner-take-all as a widely observed prize allocation rule in many contests.====Contestants often have to incur opportunity costs in order to participate in a contest. It is very common that agents must sacrifice other activities and thus would suffer from opportunity costs by attending a particular competition. There are many other situations such as most sports events where the players usually need to travel to a common site and stay there for the whole duration of the event, which could be quite costly. To participate in an R&D contest, the firms need to spend time and resources to collect project-related information, and set up necessary equipment before conducting the core research. For crowdsourcing contests that are typically held online, the contestants need to spend time to figure out how the platform works, and make sure they have access to proper facilities including internet connection and computers.====Consider also the following innovation contest among firms. The principal would like to invite a group of firms to undertake a specific innovative task. Each firm invests in R&D to generate his output (e.g., idea). Higher R&D investment leads to higher output. Each firm’s marginal cost of R&D investment is his private information. In order to participate in the task, each firm needs to incur a fixed cost (or opportunity cost) to prepare for the task. The principal would like to use a fixed budget to award firms based on their output levels to maximize total output. The question is how the principal should design the prize allocation rule, which, in principle, can be contingent on the number of participants.====We would like to emphasize the difference between entry fee and opportunity cost. As is well-known, both entry fee and opportunity cost can be used to screen high-ability contestants. The key difference between them is that entry fee can be used by the organizer to enlarge her prize budget or simply be part of her revenue, while opportunity cost does not. One may wonder whether the role of entry fees in contests is similar to that in auctions, which has been extensively discussed in the literature. As is illustrated and emphasized in Liu et al. (2018) and Hammond et al. (2019), entry fees in revenue-maximizing auctions serve only as a screening device (collecting some revenue at the same time) which would not distort players’ bidding incentives. However, in contests, in addition to screening, entry fees can be collected to top up to the organizer’s budget, which typically would distort contestants’ bidding incentives. Therefore, in contests, entry fees have different effects than opportunity costs.====In this paper, we ask how the existence of opportunity cost would affect the effort-maximizing prize allocation rule. Can winner-take-all remain optimal with costly entry, so that the organizer could ignore costly entry and act as if there were no opportunity cost? To address these issues, we introduce costly entry in the environment of Moldovanu and Sela (2001), and study the optimal prize allocation rule.====The answers to these questions are far from obvious. A new trade-off that does not arise in the analysis of Moldovanu and Sela (2001) looms large: there is a conflict between eliciting effort from entrants ex post and encouraging entry of contestants ex ante. The designer has to strike the optimal balance between ex post effort elicitation and ex ante entry incentive. Winner-take-all is very effective in terms of eliciting effort from entrants, however, at the same time this means it might overly discourage entry of the players. In addition, costly entry essentially leads to stochastic entry endogenously. As a result, prize allocation can be contingent on the number of entrants in general. Given that winner-take-all is clearly not a contingent allocation rule, it seems that there is little reason to expect that it remains optimal when costly entry is introduced in.====To determine the optimal trade-off is a rather complex issue, due to the nature of the bidding behavior that depends on the endogenous entry and the contingent prize structure. To accommodate endogenous and stochastic entry, we name the situation with ==== entrants as ====. The organizer chooses for each scenario the prize structure, which specifies the prize for each effort rank subject to a scenario budget constraint that must be binding. For each scenario, the scenario budget cannot go beyond the initial whole budget, and a higher rank of effort is associated with a (weakly) higher prize. A prize allocation rule is thus fully described by a vector of scenario budgets, and a set of vectors of scenario prizes==== with each vector specifying the prizes in a different scenario.====Our analysis starts with the linear effort cost case. An important step in our analysis is to characterize all the prize allocation rules that induce any given entry threshold,==== and identify the (unique) symmetric bidding function for any such compatible combination. Clearly, it is the set of minimum prizes in all scenarios that fully determines the symmetric entry threshold type as this type for sure wins the minimum prize in each scenario. Then, we identify the optimal allocation rule for any ==== entry threshold by establishing two important principles—the ==== and the ====. The former principle helps us characterize the optimum if the minimum prizes in all scenarios are ==== (and hence the entry threshold is also fixed), whose main idea is to make the first prize in each scenario as large as possible, so that all prizes except the first prize are equal to the minimum prize and the first prize is equal to the budget left; the latter principle then further characterizes the structure of the minimum prizes across all scenarios for a ==== entry threshold (given that other prizes are set optimally by the principle of cross-type transfer), whose main idea is to subsidize entrants as much as possible when the number of entrants is “low”, so that the first prize is as large as possible when the number of entrants is “high”. Finally, equipping with the optimum for any given entry threshold, we vary across all possible entry thresholds to pin down the optimum. The organizer faces the trade-off between extracting higher effort from entrants and inducing more entry. Quite surprisingly, it turns out that she always favors ==== effort extraction such that the optimal entry threshold is induced when it is winner-take-all regardless of the number of entrants—i.e., the minimum prize is zero when the number of entrants is at least two and it is one when there is only one entrant.====We thus establish that the optimum is winner-take-all regardless of the number of entrants when effort cost function is linear. Not surprisingly, the same result extends to the case with concave effort cost function. These results generalize the winner-take-all principle in Moldovanu and Sela (2001) to the environment with opportunity costs. Our results imply that the organizer can design the allocation rule as if there were no opportunity cost at all when the effort cost function is linear or concave. On the other hand, Moldovanu and Sela (2001) show that, with free entry, winner-take-all may not be optimal when the effort cost function is convex. We find that this result also extends to the costly entry environment. However, opportunity cost does make a difference to the optimal contest design when the cost function is convex. We show that when the opportunity cost is below a certain cutoff, there exists some convex effort cost function such that winner-take-all is sub-optimal. We also show by an example that winner-take-all can still be optimal when there is no cost of entry, while it is not when there is a positive opportunity cost. Further simulations of the example reflect the numerical observation that winner-take-all tends to be sub-optimal and the optimum would be farther away from winner-take-all when the magnitude of opportunity cost increases.====Our paper primarily belongs to the literature on optimal prize allocation in all-pay auction with incomplete information.==== Our paper is most closely related to the seminal work of Moldovanu and Sela (2001). They establish that when the effort function is linear or concave, winner-take-all is optimal. Our paper confirms that the principle of winner-take-all is robust to costly entry. Minor (2012) reexamines the same design problem as (Moldovanu and Sela, 2001) when contestants have convex costs of effort and when the contest designer has concave benefit of effort. Moldovanu and Sela (2006) further investigate a two-stage all-pay auction framework, and Moldovanu et al. (2007) study an environment where contestants care about their relative status. Moldovanu et al. (2012) further accommodate carrots and sticks in their analysis.====Our paper is also closely related to the literature on auctions with costly entry, in which bidders are endowed with the information of their private valuations but they have to incur an opportunity cost to participate in the auction. This literature includes (Samuelson, 1985, Stegeman, 1996, Campbell, 1998, Menezes, Monteiro, 2000, Tan, Yilankaya, 2006, Lu, 2009, Cao, Tian, 2010), among many others. There are several similarities as well as differences between this literature and our paper. In this strand of literature, the auctioneer’s objective is often to maximize her revenue from a group of potential bidders by selling an indivisible good, using reserve price and/or entry fees/subsidies as instruments. In our paper, the contest organizer’s objective is to maximize the expected total effort from a group of potential contestants by allocating a divisible monetary budget, using contingent prize-allocation as the instrument. The minimum prize awarded to the lowest-ranked participant in our model can be regarded as an entry subsidy in auctions. Different from this auction literature which often assumes linear bidding costs, we allow the contestants’ bidding costs to be nonlinear.====A handful of papers study contests with a stochastic number of contestants, which include (Higgins, Shughart, II, Tollison, 1985, Kaplan, Sela, 2010, Fu, Jiao, Lu, 2014). All these papers assume that the contestant’s ability (cost of exerting effort) is common knowledge. Our paper differs from these studies by assuming that the contestants’ abilities are their own private information. There are some other papers studying exogenous stochastic entry, including (Myerson, Wärneryd, 2006, Münster, 2006, Lim, Matros, 2009, Fu, Jiao, Lu, 2011). However, the entry is endogenous in our paper.====The rest of this paper is organized as follows. In Section 2, we set up the benchmark model (linear effort cost function). Section 3 analyzes the optimal contest rule. Discussions about concave cost function and convex cost function cases are in Section 4. Section 5 concludes. Technical proofs are relegated to the appendix.",The optimal allocation of prizes in contests with costly entry,https://www.sciencedirect.com/science/article/pii/S0167718719300414,9 July 2019,2019,Research Article,242.0
"Policarpo Garcia Carolina,Furquim de Azevedo Paulo","Sao Paulo School of Economics, FGV. R. Itapeva 474, São Paulo 01332-000, SP, Brazil,Insper Institute, R. Quatá 300, São Paulo 04546-042, SP, Brazil","Received 12 October 2017, Revised 2 June 2019, Accepted 5 June 2019, Available online 12 June 2019, Version of Record 30 June 2019.",https://doi.org/10.1016/j.ijindorg.2019.06.001,Cited by (2),"Mergers and acquisitions may change competition even when they do not affect market structure, a case known as conglomerate mergers. In this paper, we explore a wave of acquisitions of higher education institutions by educational groups in Brazil, which allows us to disentangle the effects of conglomerate mergers and of horizontal mergers on price, quantity, and quality indicators. Our findings show that multiunit organizations are able to increase some quality indicators. As for the effect on price and quantity, results are different. For conglomerate mergers, we estimated an increase in the number of freshmen and tuition fees, whereas for acquisitions that lead to horizontal concentration there is no increase in quantity, just in prices. Also these effects are larger the smaller the market share of the acquired HEI. On the whole, our findings are consistent with the hypothesis that multiunit operations increase efficiency, but only conglomerate mergers tend to pass those gains on to consumers. Results also indicate that greater caution should be taken in market extension mergers when the acquired firm has a clear dominant position.","Standard merger analysis typically assesses the effects on price, on quantity and, more recently, on product characteristics, resulting from changes in market concentration. However, mergers and acquisitions may change competition even when they do not affect market structure, a case known as conglomerate mergers. Competition authorities have adopted an ambiguous view about conglomerate mergers for a long time, in particular about those that are product or market extension mergers (i.e. acquisitions of local firms by a multiunit company that sells the same product in different geographic markets) (Hovenkamp, 2005, p. 559). Such mergers are likely to benefit from economies of scale and of scope, but they also raise competition concerns, due to the loss of potential competition and to the greater likelihood of anticompetitive behaviors (Sullivan et al., 2015).====After a restrictive approach to conglomerate mergers until the mid-1970s, merger policy in the US and elsewhere became more lenient with – or even silent about – mergers with no horizontal or vertical overlapping. More recently, there has been an increasing concern about product extension mergers due to tying and bundling strategies (Chen and Rey, 2015) and about the loss of potential competition in market extension mergers (Sullivan et al., 2015). In the same vein, the European Commission acknowledges the potential threat to competition that can emerge from product or market extension mergers, but it still lacks a clear policy in this area, in particular with regard to appropriate remedies (Neven, 2008). Hovenkamp (2005) even suggests that conglomerate merger effects could be more effectively treated by means of conduct analysis rather than by merger scrutiny. In short, the following question remains: should competition authorities care about conglomerate mergers?====In this paper, we address this question by taking advantage of a wave of acquisitions of higher education institutions by educational groups in Brazil. This should enable us to disentangle the effects on price, quantity, and quality of conglomerate mergers and of horizontal mergers resulting from the same set of acquisitions. Between 2007 and 2014, the major educational groups in Brazil acquired about 120 higher education institutions (HEI) with a total of one million students. In addition to the large number of acquisitions, the Brazilian Ministry of Education uses a number of indicators for all HEIs and respective programs within the country, giving detailed information on HEIs and program characteristics, such as several quality indicators, enrollment and number of freshmen. We also have a unique dataset on tuition fees for most of the programs offered in Brazil from 2007 to 2015.====According to the latest Brazilian higher education census, private institutions currently account for more than 75% of the total number of students in higher education, amounting to 7.5 million. The sector has experienced high growth rates, which is in line with the well-documented high private returns to college education in Latin America (Psacharopoulos, 1994, Psacharopoulos, Patrinos, 2004). Conglomerate mergers in higher education have been an important mechanism for the expansion of multiunit educational groups with potential implications for educational policies.====Our identification strategy is based on a difference-in-differences (DID) estimation with many pre-treatment and post-treatment periods. We exploit the variation in sociodemographic characteristics across markets and in the year of acquisition to provide evidence on the effects of the conglomerate and horizontal mergers and acquisitions. First, we exploit the fact that 20% of the relevant geographic markets affected by the operations led to horizontal concentration. For conglomerate mergers, we estimated an increase in tuition and in the number of freshmen, whereas for acquisitions that lead to horizontal concentration there is no increase in quantity, just in prices, a result that is robust to different geographic boundaries. On the whole, our findings are consistent with the hypothesis that multiunit operations increase efficiency due to economies of scale and scope, but only conglomerate mergers tend to pass those gains on to consumers.====There are also heterogeneous effects regarding educational groups, probably related to brand name positioning, however, this conclusion is not robust. We also found that quality indicators that can be directly controlled by the educational group, such as the percentage of faculty with a Ph.D. degree, increase after the acquisition, suggesting that educational groups are more responsive to regulatory constraints and/or that they use this to increase the perceived quality for the student body/candidates. We find evidence of better student performance in the national exam for some educational groups following these hirings. However, we observed a change in the profiles of students, which makes their performances not comparable over time. Our results also show that educational groups differ in how they profit from market extension mergers and exploit economies of scale and scope. Some provide support for students to access government loans, a service that we interpret as a collective good, whereas others focus more on standardized materials and on the expansion of their brand names. Regressions that investigate heterogeneous effects on a variety of outcomes provide evidence of this variability of strategies across educational groups.====This paper relates to a recent study by Russell (2016), which analyzed 107 mergers between 2001 and 2013 involving public and private non-profit HEIs in the United States. Russell found that the average merger increases tuition and fees by 7% compared to non-merging institutions in the same state, and this result is consistent with the exercise of market power by merging firms. Although this result is qualitatively similar to ours, the effects estimated in both studies are quite different. Russell focuses on horizontal mergers between public and private non-profit institutions, while we focus on market extension mergers. Thus, Russell measures the merger effect among non-profit organizations as a result of the change in competitive pressure, while we measure the effect of becoming part of a multiunit organization through a non-horizontal acquisition.====This paper contributes to a large body of empirical research into industrial organization by evaluating the effects of mergers and acquisitions on prices and quantity, which are primarily assessed by simulation models, such as Peters (2006), Nevo (2000), Hausman et al. (1994) and Werden and Froeb (1994). Weinberg (2011) evaluates the performance of these simulation models by comparing predicted prices with retrospective estimates of the effect of the merger, whereas Budzinski and Ruhmer (2009) present a review of the use of these models in competition analysis. In line with our empirical strategy, Focarelli and Panetta (2003), Hastings (2004), Ashenfelter and Hosken (2010) and Friberg and Romahn (2015) also adopted DID regression in their ex-post evaluations of mergers.====A more recent literature evaluates the effects of mergers and acquisitions on product characteristics, emphasizing the repositioning effects in the horizontal dimension of differentiation, e.g., Mazzeo (2002), Seim (2006), Gandhi et al. (2008), Watson (2009) and Sweeting (2013). Fewer studies assess the effects of mergers on quality, perhaps because it is harder to define them and observe them empirically. Like Fan (2013), we are able to assess merger effects in the vertical dimension of differentiation.====A common characteristic of all these papers is that they investigate the effects of a change in competitive pressure resulting from horizontal concentration. In other words, it is the merger effect from firms operating in the same relevant geographic market. In the cases we focus on in this paper, there is no immediate change in the market structure as a result of the operation. For regulatory reasons that will be detailed later, the operations are mainly characterized by educational groups acquiring an HEI in a relevant geographic market without any branch. As only about 20% of the relevant geographic markets affected by the mergers have led to horizontal concentration, we are also able to explore the difference in the effects of horizontal and non-horizontal acquisitions.====This paper also contributes to the literature by evaluating the entry effects of a chain, focusing on the effects of the WalMart’s entry. Basker (2005b), Hausman and Leibtag (2007) and Basker and Noel (2009) analyze Walmart’s entry effects on retail prices. Matsa (2011) examines the relationship between the competition pressure caused by the entry and the quality of the supermarket stores. Jia (2008) assesses the effects of WalMart’s entry on the profitability and entry/exit decisions of competitors, while Basker (2005a) evaluates the effects on the labor market. Holmes (2011) measures the economies of scale in distribution costs afforded by the dense network of WalMart stores. As pointed out earlier, this literature evaluates the effects of WalMart’s entry in many dimensions, but it is unable to disentangle the entry effect from the multiunit effect of a chain store in the relevant market.====The remainder of the paper is structured as follows. Section 2 provides background information about the Brazilian higher education sector. Section 3 describes the dataset. Section 4 discusses the empirical strategy and presents the results. Section 5 concludes.",Should competition authorities care about conglomerate mergers?,https://www.sciencedirect.com/science/article/pii/S0167718719300384,12 June 2019,2019,Research Article,243.0
"Reisinger Markus,Zenger Hans","Economics Department, Frankfurt School of Finance and Management, Adickesallee 32–34, 60322 Frankfurt am Main, Germany,Economics Department, University of Munich, Ludwigstr. 28, 80539 Munich, Germany","Received 30 August 2018, Revised 15 February 2019, Accepted 20 May 2019, Available online 31 May 2019, Version of Record 28 June 2019.",https://doi.org/10.1016/j.ijindorg.2019.05.002,Cited by (4),"This paper analyzes the impact of interchange fee regulation on the investment incentives of a payment card platform in the presence of full merchant internalization. We distinguish between investment in consumer and retailer services. We find that the optimally regulated interchange fee can be above the privately optimal one to induce the platform to invest more in retailer services. We also demonstrate that the two prominent regulatory benchmarks of a zero interchange fee and regulation according to the “tourist test” tend to set too low investment incentives under a total welfare standard. Instead, “tourist test” regulation can be a reasonable approximation under a total user surplus standard.","Interchange fees are interbank payments between the banks of retailers (acquiring banks) and the banks of cardholders (issuing banks) that are made within open payment card schemes, such as MasterCard or Visa. These fees have received considerable attention of antitrust authorities since they involve an explicit agreement between competing banks. Therefore, interchange fees are regulated in many countries.====The economic literature has extensively studied the role of the interchange fee as a balancing tool in a two-sided market (e.g., Baxter, 1983, Rochet, Tirole, 2002, Rochet, Tirole, 2011, Wright, 2004, Rochet, Wright, 2010). The focus of these studies is to identify differences in the interchange fee to maximize industry profits as compared to social welfare. The literature thereby provides policy implications on the optimal regulation of the fee. These studies take a static perspective by considering the quality of services provided by the card association to consumers and retailers as fixed. In this respect, the regulatory advice does not consider the impact of regulation on investment incentives.====However, the card association can improve the service quality of its card in several dimensions. Among these are credit functionality, contactless payment, and new technologies to prevent and detect fraud offered to consumers, or account verification services, PIN security systems, and faster transaction processes offered to retailers. In fact, the level of services that is provided by different card systems in different countries varies substantially. Providing such services is costly for the association==== but leads to a higher acceptance of cards and an increased transaction volume. As a consequence, the association’s decision which services to provide will depend on the regulatory environment.====The goal of this paper is to determine the impact of interchange fee regulation on such service provision. In particular, we investigate the general influence of the interchange fee on service investments and characterize the optimal regulation of the fee. We specifically analyze the effects of prominent regulatory benchmarks, such as a zero interchange fee or the “tourist test” interchange fee on such investments. We also determine how a prohibition of the no-surcharge rule affects investment incentives.====To study these questions, we set up a standard two-sided market model of a payment card association, in which the association organizes the interaction between issuing banks and acquiring banks. Via the interchange fee, the association reallocates costs from issuers to acquirers, thereby influencing transaction fees to consumers and retailers. The latter must charge the same product price for cash and card payment (no-surcharge rule). Following Rochet and Tirole (2011), we assume that issuers obtain a positive margin while acquirers are perfectly competitive.==== However, in contrast to previous literature, the card association can invest into the quality of card services, which improves the benefits of consumers or retailers in case of card payment.====We explicitly take into account that retailers, when deciding to accept a card or not, consider the full benefit that consumers receive from using the card because this allows retailers to charge higher prices. This property is known as full merchant internalization and is assumed by Rochet and Tirole (2011) and Wright (2012), among others. When patronizing a shop, consumers obtain an extra utility from being able to pay by card, which implies that a retailer who accepts the card obtains a higher demand or can charge a higher price. This implies that cash users are harmed from the possibility to pay with card, as they pay a higher product price. A consequence of this effect is that retailers are willing to accept the card even if the price charged by the acquiring bank is larger than their convenience benefit. As we will show, this property has profound consequences on optimal service provision and affects optimal interchange fee regulation.====Our analysis leads to the following results. We start with the analysis of an unregulated industry, and determine the optimal service investments of the card association. We demonstrate that investment into consumer services leads to a direct increase in consumer demand, whereas investment into retailer services leads to an indirect increase due to the possibility of raising the interchange fee. A higher interchange fee reduces costs of issuing banks, which implies a lower consumer charge. We find that service investments have a “double dividend” due to full merchant internalization. First, they result in a primary demand expansion. This expansion occurs directly in case of consumer service investment and indirectly in case of retailer service investment, as the latter allows the card association to increase the interchange fee, which leads to a fall in the consumer charge. In addition to this effect, investments increase the average consumer benefit. This makes retailers more willing to accept the card, implying that the card platform can increase the interchange fee further. Therefore, demand rises even further. As a consequence, the privately optimal investment in an unregulated industry can be higher than the welfare optimal investment, where the effect of the double dividend is not present. In particular, the card association does not take into account that cash users do not benefit from higher service investment but are harmed through higher product prices.====We then turn to the question of optimal regulation of the interchange fee. Here we find that, under a total welfare standard, the optimally regulated interchange fee can be larger than the one set by the card association without regulation. The intuition is that regulating the interchange fee at a high level implies high costs for acquiring banks. This leads to a high fee charged to retailers. The card platform must then increase its service to retailers to induce them to accept the card. Therefore, a higher regulated interchange fee provides stronger incentives for the association to invest in retailer services. If the privately optimal investment incentive falls short of the socially optimal one, a higher interchange fee raises welfare. As a result, the policy implications emerging from our analysis contrast with those of previous studies, which were based on static models of payment card markets (e.g., Frankel and Shampine, 2006).==== In particular, our results show that, under certain circumstances, a higher interchange fee may provide improved incentives to invest in card systems.====We then consider two prominent regulatory benchmarks. First, an interchange fee of zero and, second, the “tourist test” interchange fee, which leaves retailers indifferent between card and cash payment. We find that an interchange fee of zero induces an investment in retailer services which is below the socially and the privately optimal level. The reason is that the card platform only finds it optimal to invest in retailer services in order to increase the interchange fee and benefit from increased consumer demand. If this channel is closed, investment in retailer services will be inefficiently low. We also show that investment in consumer services tends to be below the privately optimal one. The reason is that the card platform does not benefit from full merchant internalization since it cannot adjust the interchange fee. Interestingly, a regulation of a zero interchange fee leads to a lower investment on the retailer side than on the consumer side, even if costs and benefits are the same for both investments. By contrast, allowing the card platform to set the interchange fee results in equal investments on both sides, which is also the efficient investment structure.====Turning to “tourist test” interchange fee regulation, we find that the efficient investment structure is established with this regulation. However, under a total welfare standard, the resulting investment levels tend to be too low compared to the socially and privately efficient investment levels. The reason is that the card platform cannot adjust the interchange fee, and therefore cannot reap the benefits from full merchant internalization. By contrast, “tourist test” regulation can be a reasonable approximation under a total user surplus standard.====Finally, we consider the investment incentives when retailers can charge a different retail price to consumers for card payment than for cash payment. It is well-known (e.g., from Gans, King, 2003, Rochet, Tirole, 2006) that the interchange fee becomes irrelevant when perfect surcharge is possible. We show that the equilibrium investment incentives are then the same as in case of “tourist test” interchange fee regulation. The intuition is that in both cases the externalities between card users and retailers are internalized as retailers obtain the same profit with cash and card payment.====The existing literature on payment card systems has focused on fixed service quality for consumers and retailers. Most authors (e.g., Baxter, 1983, Rochet, Tirole, 2002, Rochet, Tirole, 2003, Wright, 2003a, Wright, 2003b, Wright, 2004, Guthrie, Wright, 2007) analyzed the difference between the optimal interchange fee of a card platform and the socially optimal interchange fee under different industry structures, i.e., monopoly platform versus platform competition or different forms of heterogeneity of consumers and retailers. Recently, Rochet and Tirole (2011) explicitly introduced the notion of must-take cards or full merchant internalization, which takes into account that retailers consider the average consumer benefits in their decision to accept cards.==== Wright (2012) shows that with full merchant internalization the price structure is biased against retailers, and in favor of consumers. Bedre-Defolie and Calvano (2013) find a similar result due to the fact that consumers are active in deciding which payment instrument to use, whereas retailers only passively accept cards.==== None of these papers considers the investment incentives of the card platform.====As our paper, Verdier (2010) allows for endogenous service investments and distinguishes between investments in consumer and retailer services. However, the focus of her paper and ours is very different. Verdier (2010) focuses on the optimal interchange fee of the card platform without merchant internalization. She shows, for example, that if the contribution of acquirers is more important compared to the one of issuers, a platform lowers its interchange fee to induce higher investments by acquirers. By contrast, the main goal of our study is to determine optimal interchange fee regulation and evaluate prominent regulatory benchmarks with respect to their incentives on service investments. In addition, Verdier (2010) mainly analyzes the situation in which issuer and acquirer are monopolists whereas we focus on perfect acquirer competition and imperfect issuer competition.====Belleflamme and Peitz (2010) analyze investment incentives in two-sided markets in general. In contrast to our paper, they consider investments by sellers and not investments by the platform. They find that sellers may have a larger incentive to invest if platforms charge sellers for access rather than give free access. The reason is that investment increases the buyers’ benefit, which in turn induces the platform to lower the access charge to sellers.====The remainder of the paper is organized as follows: The next section sets out the model. Section 3 determines the optimal unregulated interchange fee and service investments while Section 4 analyzes socially optimal investments. Section 5 determines the optimal regulated interchange fee. Section 6 considers the regulatory benchmarks of a zero interchange fee and the “tourist test” interchange fee. Section 7 analyzes the implications of lifting the no-surcharge rule. Section 8 concludes.",Interchange fee regulation and service investments,https://www.sciencedirect.com/science/article/pii/S0167718719300311,31 May 2019,2019,Research Article,244.0
Kaplow Louis,"Harvard University and National Bureau of Economic Research, United States","Received 1 October 2018, Revised 7 January 2019, Accepted 28 April 2019, Available online 24 May 2019, Version of Record 15 June 2019.",https://doi.org/10.1016/j.ijindorg.2019.04.003,Cited by (5),"Despite decades of research on mechanism design and on many practical aspects of cost-benefit analysis, one of the most basic and ubiquitous features of regulation as actually implemented throughout the world has received little theoretical attention: exemptions for small firms. These firms, although individually small, may generate a disproportionate share of harm due to their being exempt and because exemption induces additional harmful activity to be channeled their way. This article analyzes optimal regulation with exemptions where firms have different productivities that are unobservable to the regulator, regulated and unregulated output each cause harm although at different levels, and the regulatory regime affects entry as well as the output choices of regulated and unregulated firms. In many settings, optimal schemes involve subtle effects and have counterintuitive features: for example, higher regulatory costs need not favor higher exemptions, and the incentives of firms to drop output to become exempt can be too weak as well as too strong. A final section examines the optimal use of output taxation alongside regulation, which illustrates the contrast with the mechanism design approach that analyzes the optimal use of instruments of a type that are not in widespread use.","China recently decided to shut down small businesses that contribute a vastly disproportionate amount to water pollution.==== This action is unsurprising in light of the findings of Jiang et al. (2014) on the importance of pollution from small Chinese manufacturing firms that had been subject to less regulatory intensity, and Qi et al. (2017) on how the elimination of size-dependent distortions would reduce pollution by 20% while raising output 30%. Indeed, in many realms throughout the world, regulation exempts or accords other preferences to small business in a fashion that often has outsized consequences. Garicano et al. (2016) and Gourio and Roys (2014) examine distortions due to French labor regulations that are applicable only to firms with at least fifty employees. Braguinsky et al. (2011) offer evidence suggesting that labor regulations produce substantial firm size distortions in Portugal. Guner et al. (2008) estimate large output and productivity reductions and potentially significant welfare costs due to lower firm sizes in Europe and Japan that arise from size-dependent policies. For the United States, Becker and Henderson (2000) present evidence suggesting that shifts of production to new, small-scale plants (subject to a de facto policy of rare inspections) contributed to air quality degradation.==== Gao et al. (2009) show that firms remained small to avoid more stringent securities law reporting requirements, and Holder et al. (2013) indicate that such firms’ reporting quality suffered relative to firms only moderately larger than the exemption threshold.====Regulation across the globe most often employs a command-and-control approach that exempts or otherwise favors small firms, a treatment often rationalized by economies of scale in regulatory compliance.==== Small firms may cause a disproportionate share of harm precisely because of these preferences and because this favoritism induces additional harmful activity to be channeled their way.==== Note further that a given small business is not exempt from just one or two regulations but from myriads of them – regarding the environment, workplace safety, hiring, employee benefits and other labor regulation, information disclosure, and much more. The aggregate can cause both greater harm and larger production distortions.==== These effects may be exacerbated by tax exemptions (often from the VAT and, in the U.S., from a recent $2000-per-employee health mandate penalty) and other benefits (government contract preferences, subsidized loans).====Despite the ubiquity and practical importance of this phenomenon, it has received little attention in the substantial and often highly refined literatures on cost-benefit analysis and optimal regulatory design. The requisite analysis is not entirely straightforward because exemptions affect entry and exit as well as the output decisions of firms that do operate and also because the production of both regulated and exempt firms typically causes harm, albeit at different levels. Among the counterintuitive implications are that sometimes the incentive of firms to discretely drop output to become exempt is too small rather than too large, and that higher regulatory compliance costs, including greater fixed costs, need not favor higher exemptions. (Indeed, compliance costs do not even appear directly in the pertinent first-order condition, due to an envelope condition, which means that they have no mechanical effect on the optimal exemption.)====Section 2 presents a model of conventional command-and-control regulation like that employed in many realms throughout the world. This regulation applies to firms with different productivities, which differences are unobservable to the government. Regulation consists of imposing a supplemental production technology that entails both fixed and variable costs of compliance, and this technology reduces but does not eliminate the external harm caused by firms’ output.==== The section characterizes the first best, analyzes firms’ behavior under regulation and no regulation, and then compares the two taking into account output effects, including the decision whether to produce at all. Last, the section examines an output tax and compares it to regulation. The brief analysis here serves mainly as a benchmark for what follows.====Section 3 introduces an exemption under which firms are subject to regulation if and only if their output exceeds a quantity threshold.==== In one case, no firm chooses output above the exempt level yet regulation binds in the sense that the most efficient firms would otherwise have produced higher outputs but now produce less in order to be exempt. Such schemes can dominate no regulation because unregulated firms’ output is socially excessive due to the external harm they cause. In another, more interesting case, the most efficient firms have outputs above the exemption threshold, thereby subjecting themselves to regulation; firms of intermediate efficiency have outputs clustered at the exemption threshold; and firms of lower efficiency produce output below the threshold. Raising the exemption, which causes some regulated firms to jump down (discretely reduce their output) to the exempt level, has ambiguous effects on social welfare. Although their output is now unregulated and thus more harmful, their quantity of output is also lower than was their regulated quantity level, and we must keep in mind that regulated output is also harmful (although less so per unit of output). The optimal exemption can be zero (tantamount to simple regulation without an exemption), and this may be so regardless of how high are the costs of regulatory compliance (again, due to output effects). Relatedly, higher fixed or marginal costs of regulatory compliance do not necessarily favor a higher exemption level. In all, cost-benefit analysis of a simple regulation with an exemption – the very sort of regulation in widespread use – is notably more complex than usually imagined.====Although not the core focus of this article, which emphasizes regulation of the type typically employed, the aforementioned entry and output distortions and our general understanding of mechanism design suggest the value of relating the main analysis to results when output taxes can be employed as well.==== Section 4 does this in three steps. It first examines taxation of exempt output (only), motivated by the fact that this output is more harmful and the concern that firms jump down to bunch at the exempt level of output. When the optimal regime involves some firms producing above-exempt quantities, some bunching at the exemption, and some producing less, the optimal tax on exempt output is strictly below the harm caused by such output because the incentive of regulated firms to jump down is otherwise too small, and it is even possible that this tax is optimally set equal to zero. Second, a tax on (only) regulated output is considered. Once again, when the optimal regime involves masses of firms in all three output categories, the optimal tax is strictly less than the harm caused by such output because the incentive of regulated firms to jump down is otherwise too large, and it is possible that this tax is optimally set equal to zero. Third, separate taxes on unregulated and regulated output are considered. It is not surprising that these instruments, not generally in use, do enable implementation of the first best, which highlights the sharp contrast between optimal design of the sort of command-and-control regulation usually employed and the unconstrained mechanism design approach that takes advantage of different, more flexible instruments.",Optimal regulation with exemptions,https://www.sciencedirect.com/science/article/pii/S0167718719300220,24 May 2019,2019,Research Article,245.0
Etro Federico,"Florence School of Economics and Management, University of Florence, Via delle Pandette 32, D6-215, Firenze, Italia 50127, Italy,Charles River Associates, London, UK","Received 3 July 2018, Revised 10 April 2019, Accepted 11 April 2019, Available online 21 May 2019, Version of Record 4 June 2019.",https://doi.org/10.1016/j.ijindorg.2019.04.004,Cited by (8), (even with bundling).,"The literature of industrial organization has recently investigated the impact of mergers in case of endogenous innovation with a focus on antitrust implications (see Shapiro (2012), and Gilbert and Greene (2015)). For instance, Motta and Tarantino (2016) and Federico et al. (2017) have examined horizontal mergers respectively in aggregative games of price competition between firms producing substitute goods and investing in cost reduction, and in symmetric contests with probabilistic innovation.==== In these and other related models a merger can exert a negative effect on consumer welfare driven by either higher prices or lower R&D of the merged firms due to the internalization of business stealing effects.==== However, in other models, as those by Denicolò and Polo (2018), Bourreau et al. (2018) and Bourreau and Jullien (2018), horizontal mergers of firms producing substitute goods can spur innovation and benefit consumers by preventing duplication of efforts or expanding demand.====I explore the alternative case of a merger between firms producing complement goods and investing in R&D, a scenario that is traditionally associated with positive effects on consumer welfare driven by lower prices and strengthened by higher R&D, due to the internalization of Cournot complementarities, except for cases where the merged entity adopts some form of mixed bundling to divert demand from rival producers with an ambiguous impact on consumer welfare (see Economides and Salop (1992), and especially Choi (2008)). In contrast to this, my main result is that a merger of complements can harm consumers even in the absence of bundling and when it allows for the internalization of Cournot complementarities: the reason is that higher R&D by the merging incumbents reduces the incentives of the entrants to invest in R&D generating an increase in expected prices when downstream demand is sufficiently inelastic. Instead, when demand is elastic enough, the merger tends to benefit consumers even when bundling can be adopted and even if this deters entry, due to beneficial effects on cost reduction and pricing of the incumbents.====Conglomerate mergers between producers of complement goods have been the focus of a variety of antitrust cases, as those involving General Electric and Honeywell, Tetrapak and Laval Sidel, Intel and McAfee and few more cases in highly innovative industries. The critical aspects of these mergers concerned complement products that were separately produced by each of the merging firms (for instance jet engines by GE and a portfolio of avionics and non-avionics products by Honeywell)==== and whose combination post-merger could foreclose entry. Bundling issues have been at the core of these cases for their anti-competitive implications. Recently, a proposed merger between Qualcomm and NXP has been cleared by the European antitrust authorities under conditions aimed at avoiding risks of foreclosure for actual and potential rivals. In this case, the merging firms produce complement components for smartphones, respectively baseband chipsets by Qualcomm and near-field communication (NFC) and secure element (SE) chips for contactless payments by NXP, which owns and licenses IP on the Mifare technology, an essential technology for high-end devices used as mobile wallets (for instance to pay for public transport or make other secure payments). The merger has been approved conditionally on ensuring interoperability of Qualcomm’s basebands with NFC and SE products by competitors, excluding pure bundling of the two components, and continuing to license the Mifare technology to other producers for an eight-year period.==== These are appropriate remedies to avoid the negative effects of a conglomerate merger on the incentives of competitors to invest in product development and exert competitive pressure on the merged entity.====At a more general level, conglomerate mergers in high-tech industries often involve “must-have” inputs generating an inelastic demand for a composite good (for instance because there are not substitute producers of essential components protected by IP), while other mergers relate to composite goods with a more elastic demand (for instance because there are rivals producing imperfect substitutes for either the composite goods or their components). I formalize the idea that mergers can raise anti-competitive concerns in the former case and not in the latter.",Mergers of complements and entry in innovative industries,https://www.sciencedirect.com/science/article/pii/S016771871930030X,21 May 2019,2019,Research Article,246.0
Caminal Ramon,"Institute of Economic Analysis, CSIC, Spain,Barcelona GSE, Spain","Received 14 June 2018, Revised 2 May 2019, Accepted 6 May 2019, Available online 14 May 2019, Version of Record 23 May 2019.",https://doi.org/10.1016/j.ijindorg.2019.05.001,Cited by (2),This paper builds a dynamic ,"There is an extensive literature on the effect of the market structure on the provision of product diversity. Most of these studies are built around the idea that a larger number of varieties available at a point in time allows for a better match with the preferences of heterogeneous consumers.==== However, much less attention has been devoted to the temporal dimension of variety. In particular, in many product categories individual consumers have a taste for variety that can only be satisfied over time. This is the case, for example, of leisure goods – such as films, books, music recordings, plays, live musical performances, computer games, etc. In these markets a consumer tends to purchase only one unit of a particular variety at a certain date, but engage in repeat purchases in the same product category as new varieties become available (====). These markets usually exhibit two important characteristics. First, the rate at which consumers can absorb new varieties is clearly limited, and closely related to the amount of time they need to recover from a previous consumption episode (====). Second, there is a high degree of synchronization between commercialization and consumption, as most purchases are typically made immediately following the release of a new variety.====Temporary satiation is relevant in a broad range of markets that includes, but is not restricted to, the leisure goods mentioned above. The rate of recovery may vary quite markedly across consumers. Some people may be willing to attend a live performance of a rock band or go see a romantic comedy film every week, whereas others may prefer to wait much longer until they feel ready for a new experience of that sort. Indeed, observed differences in the frequencies of repeat purchases across consumers tend to reflect a heterogeneous degree of exposure to temporary satiation (See, for instance, Hartmann and Viard, 2008).==== Such ==== has been shown to generate elasticity patterns similar to those observed in storable and durable goods, as higher current consumption implies lower future demand (Hartmann, 2006). Unfortunately, hard empirical evidence on the effect of temporary satiation in the markets of interest (in which new varieties of a leisure good are introduced over time) does not abound. One exception is Einav (2010), who shows that box office revenues in the US would increase if film distributors did not cluster their releases so much. Thus, in a market with essentially fixed consumer prices, such a relationship between aggregate revenues and the timing of releases clearly manifests the presence of temporary satiation.====The high degree of synchronization between commercialization and consumption can also be easily illustrated. For example, many artists and performers often present their new work in a specific location in a single event, or in several events taking place on consecutive days. Thus, consumption happens at the same time the new variety is released. Of course, this is an extreme example. In most industries, such synchronization is not perfect, but still very high. In the film industry, between 40% and 50% of US box-office revenues are taken during a movie’s first week and very few movies generate significant revenue beyond the sixth week.==== In a similar vein, two thirds of the purchases of video games are made during the first three months after release.====A natural question is whether different market structures provide a socially optimal level of dynamic product diversity at reasonable prices. In Caminal (2016) I analyzed a model in which a monopolist provides different varieties of a good over time to a customer base whose preferences are subject to temporary satiation. The welfare results depend on the balance between two opposing effects. On the one hand, if varieties are introduced very frequently then they become imperfect substitutes and the firm has incentives to engage in intertemporal price discrimination: raise prices above the short-run profit maximizing level, and sell each variety only to consumers with very high valuations (better preference matching). On the other hand, higher frequency also generates market expansion. I showed that under strong temporary satiation, better preference matching dominates and the equilibrium frequency of new product introductions is socially excessive.====Clearly, most of the markets for leisure goods, no matter how narrowly the product category is defined, are characterized by intense competition. Indeed, given the high degree of synchronization between commercialization and consumption and the relevance of temporary satiation, firms use the timing of their new product introductions as a crucial strategic variable. It has been observed by the popular media that large Hollywood studios and publishing houses often play around with the timing of their releases, as a response to new information about rivals’ moves.====In the current paper I study the effect of competition on both prices and the timing of new product introductions in markets characterized by temporary satiation and perfect synchronization between commercialization and consumption.==== The model aims at capturing the main features of a specific segment of a leisure good; like horror movies, historical novels, classical music concerts, etc.====More specifically, I consider a dynamic duopoly model in which two symmetric firms sequentially introduce new varieties of a non-durable good. After a consumption episode individual consumers stay out of the market for a random number of periods until they become active again. Hence, current demand depends negatively on past consumption. The model considers both dimensions of product differentiation, static and dynamic. The static dimension is represented by Hotelling’s linear city model. From a dynamic point of view, and because of temporary satiation, two consecutive varieties become imperfect substitutes.====Two important features significantly contribute to the tractability of the model. First, consumers are ex-ante identical but they differ in their valuations of specific varieties. In such a framework, we can study the effect of competition under static product differentiation and intertemporal substitutability, and yet avoid Coasian price dynamics. Second, I focus on parameter values for which all active consumers purchase one unit of the good (covered market). This feature places some limits on the extent of intertemporal price discrimination along the equilibrium path, but nevertheless it is still possible to shed some light on how pricing incentives vary across alternative market structures (monopoly versus duopoly).====As expected, the frequency of new product introductions decreases as the ratio of the rate of demand recovery per period to the fixed cost per variety falls. The value of this ratio is closely related to the length of the period. As time periods get shorter we should expect lower rates of demand recovery per period, as well as a higher discount factor. In one extreme, if this ratio is sufficiently high (the fixed cost is sufficiently low) then both firms may choose to introduce one variety every period, and thus compete head-to-head. If this ratio is not too high then firms will tend to stagger their new product introductions and behave as temporary monopolists.====Because of temporary satiation, demand evolves deterministically over time. As a result, firms use the timing of new product introductions as an additional strategic variable. In fact, when the rate of demand recovery is not too high relative to the fixed cost per variety, firms have incentives to undercut each other in terms of the timing of launching a new variety. As a result, a firm may always be ready to introduce a new product as soon as demand has grown enough to cover costs. Such a behavior is fuelled by the rival firm’s threat to respond immediately to the failure to introduce a new variety by introducing its own in the following period, a magnified business-stealing effect. Thus, in the extreme case that this ratio is sufficiently low (which occurs when time periods are arbitrarily short), profits will be driven down to almost zero. In this case, every cycle looks like a race in which the winner takes (almost) nothing.====Pricing policies crucially depend on whether or not firms can avoid head-to-head competition. If the rate of demand recovery is sufficiently high relative to the fixed cost, then both firms may introduce new varieties every period and prices are equal to those prevailing in the static Hotelling model. In this case, the static competition effect dominates intertemporal substitution. However, even when fixed costs are very low, there may exist another equilibrium in which firms stagger their new product introductions, and behave as temporary monopolists setting the price that maximizes short-run profits. Thus, avoiding head-to-head competition may simply be a coordination problem for firms.====Thus, back to the case of a sufficiently low ratio of the rate of demand recovery to the fixed cost, temporary monopolists set the short-run profit maximizing price, and nevertheless firms intensively compete in the timing of launching new varieties. Thus, the equilibrium outcome may be characterized by the unusual combination of monopolistic prices and (nearly) zero profits. Abusing the language, it could perhaps be argued that the Bertrand and Diamond paradoxes can be reconciled in this type of market. On the one hand, two is enough for competition (zero profits), as in the Bertrand paradox. On the other hand, competition between two (ex-ante) identical firms with constant marginal cost results in monopoly prices, as in the Diamond paradox.====Even when firms are temporary monopolists, their ability to extract surplus from consumers is limited by two factors. First, forward-looking consumers anticipate that current consumption reduces their expected future surplus, since they might be out of the market when future varieties are introduced. Hence, they require a sufficiently high current surplus (a sufficiently low price) in order to compensate for the option value of waiting. More specifically, the short-run profit maximizing price decreases with the intensity of temporary satiation and with the discount factor. Second, competition reduces firms’ incentives to engage in intertemporal price discrimination. As mentioned above, a monopolist may have incentives to raise the price above the short-run profit maximizing level, in order to boost future demand and sell the next variety to these marginal consumers at higher prices. Under duopoly, and provided that firms alternate, it would be the rival firm who would mostly benefit from such an increase in future demand, and hence firms tend to stick to the short-run profit maximizing price.====From a welfare point of view, the timing of new product introductions may or may not be efficient. The welfare analysis of the speed of new product introductions is more transparent if the rate of demand recovery is low relative to the fixed cost, and thus several periods separate two consecutive varieties. In this case, it is easy to find conditions under which a social planner that can dictate the timing of new product introductions as well as prices, under the constraint that firms make non-negative profits (and consumers maximize their utility), can raise total welfare by slowing down the introduction of new products, relative to the equilibrium frequency. In this case, there will be more consumption per variety (more time for demand to recover), but lower number of varieties per unit of time. If the discount rate is sufficiently low relative to the rate of demand recovery, then the first effect dominates and a reduction in the frequency of new product introductions raises total welfare. That is, in equilibrium firms introduce new varieties too quickly compared to this second best benchmark.====From a policy perspective, it is also interesting to examine firms’ incentives to coordinate the timing of their new product introductions, and their effect on consumer and social welfare. If the rate of demand recovery per period is sufficiently low relative to the fixed cost (so that in equilibrium two consecutive new product introductions are separated by more than one period), then firms have incentives to collude and slow down their new product introductions. Such coordination efforts are likely to hurt consumers and they may even reduce total welfare. By increasing the distance between two consecutive varieties, firms can raise consumers’ willingness to pay (by lowering their option value of waiting), as well as the fraction of active consumers who are ready to purchase each variety. Thus, firms would set higher prices and sell larger quantities.==== However, consumers are likely to be harmed. First, total welfare may decrease, and hence consumer surplus would fall even further (as profits increase). Second, even if total surplus increases as a result of less frequent new product introductions, consumers can still lose because of the higher prices (profits may increase more than total welfare). Thus, private and social interests are not necessarily aligned, and hence competition authorities should be concerned about any attempt by rival firms to coordinate the timing of their releases; even more so if the authorities put a higher weight on consumer surplus.====As mentioned above, the present model captures some of the characteristics attributable to durable goods. It would not be hard to rephrase some of the elements of the model and describe every purchase as an investment in a stock that provides a flow of services that decreases over time. However, several features of the model seem more suitable for representing leisure goods rather than standard durable goods. In particular, different varieties introduced over time have independent characteristics in the present model, whereas new varieties of durable goods typically introduce quality improvements. Additionally, in the present model consumers have idiosyncratic preferences over individual varieties whereas consumers’ willingness to pay for quality improvements are likely to be highly correlated over time. Indeed, the literature on durable goods has emphasized how the cumulative nature of innovation affects the frequency of new product introductions (Fishman and Rob, 2000), underinvestment in the durability of the good (Waldman, 1993), compatibility between old and new models (Ellison and Fudenberg, 2000) and, of course, pricing policies in the absence of commitment (Bulow, 1982, Stokey, 1982). In contrast, the present paper focuses on horizontal rather than vertical product differentiation and abstracts from Coasian price dynamics.====The next section presents the model. Section 3 examines the case of a sufficiently low ratio of the rate of demand recovery per period to the fixed cost per variety. The focus is on equilibrium configurations in which both firms introduce a variety every period, firms compete head-to-head and prices are driven by the static competition effect. Section 4 characterizes equilibria in which one variety is introduced every period and firms alternate in its provision. Thus, firms are temporary monopolist that set the short-run profit maximizing price. In this case firms’ incentives to undercut the rival’s timing of new product introductions are very mild because by accelerating the frequency of new product introduction the firm would change the competitive regime by inducing head-to-head competition. Section 5 generalizes Section 4 by building an equilibrium in which firms alternate in the provision of new varieties, but the number of periods between two consecutive new product introductions is higher than one. In this case, firms have a strong incentive to the undercut the rival’s timing of new product introductions. Finally, Section 6 discusses alternative equilibrium configurations.",The dynamic provision of product diversity under duopoly,https://www.sciencedirect.com/science/article/pii/S0167718719300293,14 May 2019,2019,Research Article,247.0
"Huber Martin,Imhof David","Department of Economics, University of Fribourg, Switzerland,University of Bourgogne Franche-Comté, CRESE EA 3190, Besançon F-25000, France,Unidistance, Switzerland,Swiss Competition Commission COMCO","Received 29 March 2018, Revised 25 April 2019, Accepted 25 April 2019, Available online 6 May 2019, Version of Record 24 May 2019.",https://doi.org/10.1016/j.ijindorg.2019.04.002,Cited by (40),"We combine machine learning techniques with statistical screens computed from the distribution of bids in tenders within the Swiss construction sector to predict collusion through bid-rigging cartels. We assess the out of sample performance of this approach and find it to correctly classify more than 84% of the total of bidding processes as collusive or non-collusive. We also discuss tradeoffs in reducing false positive vs. false negative predictions and find that false negative predictions increase much faster in reducing false positive predictions. Finally, we discuss policy implications of our method for competition agencies aiming at detecting bid-rigging cartels.","In competition policy, screens are specific indices derived from the bidding distribution in tenders for distinguishing between competition and collusion as well as for flagging markets and firms likely characterized by collusion. They are thus of interest for competition agencies in order to detect cartels and to enforce competition laws. In the light of the vast variety of screens proposed in the literature (see Harrington, 2008, Jimenez, Perdiguero, 2012, OECD, Froeb, Sibley, Doane, Pinto, 2014), the question arises which detection method some competition agency should choose in practice. However, very few papers, if any, systematically investigate the performance of the screens based on statistical methods.====In this paper, we combine machine learning techniques with several screening methods for predicting collusion. We evaluate the out of sample prediction accuracy in a data set of 584 tenders that are representative for the construction sector in Switzerland. The data cover 4 different bid-rigging cartels and comprise both collusive and competitive (post-collusion) tenders, based on which we define a binary collusion indicator that serves as dependent variable. More concisely, we consider the screens proposed by Imhof (2017b) for detecting bid-rigging cartels and investigate the performance of machine learning techniques when using these screens as predictors. Firstly, we investigate lasso logit regression, (see Tibshirani, 1996), to predict collusion as a function of the screens as well as their interactions and higher order terms. Secondly, we apply an ensemble method that consists of a weighted average of predictions based on bagged regression trees (see Breiman, 1996), random forests (see Ho, 1995, Breiman, 2001), and neural networks (see McCulloch, Pitts, 1943, Ripley, 1996).====We use cross validation to determine the optimal penalization in lasso regression as well as the optimal weighting in the ensemble method. We randomly split the data into training and test samples and estimate the model parameters in the training data, while out of sample performance is assessed in the test data. We repeat these steps 100 times to estimate the average mean squared errors and classification errors. The latter is defined by the mismatch of actual collusion and predicted collusion, which is 1 if the algorithm predicts the collusion probability to be 0.5 or higher and 0 otherwise.====In our analysis, we distinguish between false positive and false negative predictions. A false positive implies that the machine learning algorithm flags a tender as collusive even though no collusion occurs. From the perspective of a competition agency, this might appear to be the worst kind of prediction error, as it could induce an unjustified investigation. In contrast, a false negative implies that the method does not flag a tender as collusive, although collusion occurs. This is undesirable, too, because any method that produces too many false negatives appears not worth being implemented due to a lack of statistical power in detecting collusion. A method that is attractive for competition agencies therefore needs to have an acceptable overall out of sample performance that satisfactorily trades off false positive and false negative prediction rates.====Our results suggest that the combination of machine learning and screening is a powerful tool for detecting bid rigging. Both the lasso and the ensemble method correctly predict 84% of all tenders out of sample or put differently, the overall misclassification rate of either method is 16%. The lasso performs slightly better in collusive than in non-collusive periods with 14% false negative and 18% false positive predictions, while the opposite holds for the ensemble method with 17% false negative and 15% false positive predictions. Therefore, the methods proposed in this paper exhibit a decent performance as the shares of false positives and negatives are low. Furthermore, we also consider reducing the share of false positives by tightening the rule for classifying tenders as collusive, i.e. by predicting collusion only to be 1 if the collusion probability predicted by the algorithm exceeds a specific threshold higher than the default value of 0.5. This exercise appears interesting for competition agencies to find an optimal tradeoff between false positives and false negatives by gauging the choice of the probability threshold. In our data, we not very surprisingly find that reducing false positives induces a considerable increase in false negative predictions.====As lasso is a variable selection method (based on constraining the sum of the absolute values of the estimated slope coefficients) for picking important predictors, it allows determining the most powerful screens. Among a set of 65 predictors that consist of the original screens as well as their interactions and squared terms, we find that two screens play a major role for detecting bid-rigging cartels: the ratio of the price difference between the second and (winning) first lowest bids to the average price difference among all bids and the coefficient of variation of bids in a tender. By far less important predictors are the number and the skewness of bids. However, we also find that discarding the two most powerful screens does not significantly affect the accuracy of the prediction, as other screens “step in” as substitutes.====As a policy recommendation, we propose a two-step procedure to detect bid-rigging cartels. The first step relies on our combination of machine learning and screening. Competition agencies may calculate the screens for each tender from the distribution of submitted bids, an information typically available in procurement processes. They may then apply the model based on screening suggested in our paper to predict collusive and competitive tenders. Concerning classification into collusive and competitive tenders, it seems (at least in our data) advisable to use a decision rule that is based on a predicted probability threshold between 0.5 to 0.7. The second step consists of scrutinizing tenders flagged as collusive by machine learning. Following Imhof et al. (2018), competition agencies should investigate if specific groups of firms or regions can be linked to the suspicious tenders. In particular, agencies can apply the bid rotation screen, see Imhof et al. (2018), which investigates the interaction among suspected firms, to check whether their group-specific interactions match a bid-rigging behavior.====Our paper is related to a small literature on implementing screens to detect bid-rigging cartels (see Feinstein, Block, Nold, 1985, Imhof, Karagoek, Rutz, 2018, Imhof). This literature differs from the majority of studies on detecting bid-rigging cartels that use econometric tests typically not only relying on bidding information, but also on proxies for the costs of the firms (see Porter, Zona, 1993, Porter, Zona, 1999, Pesendorfer, 2000, Bajari, Ye, 2003, Jakobsson, Aryal, Gabrielli, 2013, Chotibhongs, Arditi, 2012, Chotibhongs, Arditi, 2012, Imhof). Finally, our paper is also related to studies on screens in markets not characterized by an auction process (see Abrantes-Metz, Froeb, Geweke, Taylor, 2006, Esposito, Ferrero, Hueschelrath, Veith, Jimenez, Perdiguero, 2012, Abrantes-Metz, Kraten, Metz, Seow, 2012).====The remainder of the paper is organized as follows. Section 2 reviews our data that includes four bid-rigging cartels in the Swiss construction sector and discusses the screens used as predictors for collusion. Section 3 presents the machine learning techniques along with the empirical results. Section 4 discusses several policy recommendations of our method. Section 5 concludes.",Machine learning with screens for detecting bid-rigging cartels,https://www.sciencedirect.com/science/article/pii/S0167718719300219,6 May 2019,2019,Research Article,248.0
"Chao Yong,Tan Guofu,Wong Adam Chi Leung","College of Business, University of Louisville, 2301 S. 3rd Street, Louisville, KY 40292, United States,Department of Economics, University of Southern California, 3620 S. Vermont Avenue, Los Angeles, CA 90089, United States,Department of Economics, Lingnan University, 8 Castle Peak Road, Tuen Mun, Hong Kong","Received 30 June 2018, Revised 10 December 2018, Accepted 15 March 2019, Available online 20 March 2019, Version of Record 2 April 2019.",https://doi.org/10.1016/j.ijindorg.2019.03.001,Cited by (6),"In many abuse of dominance antitrust cases, the dominant firm adopts pricing schemes involving all-units discounts, whereas its smaller competitors often use simple linear pricing. We provide a game-theoretic justification for the observed asymmetry in pricing practices by studying a model in which a firm with full capacity faces a capacity-constrained rival. The asymmetry in capacity between the firms, which gives rise to the captive market, allows the dominant firm to take advantage of the quantity commitment through all-units discounts while the capacity-constrained rival is induced to offer simple linear pricing.","All-units discounts (AUDs) are a common vertical contract. Under AUDs, the per-unit price is cut on all units once the buyer’s purchase order reaches a threshold. The adoption of AUDs by dominant firms has become a prominent antitrust issue, such as in ====,==== and in ====.==== In these cases, the dominant firms’ small rivals often only use linear pricing (LP). The offering of AUDs is deemed as a competition on the merits unless the undertaking is dominant in the market. So when small firms complain about AUDs offered by dominant undertakings as an abuse of dominance, one natural question to ask is: Why don’t small firms offer their own AUDs to compete when they can?====We find that such observed pricing asymmetry in aforementioned cases can be attributed to the small firm’s capacity constraint. The asymmetry in capacity between the firms, which gives rise to the captive market, allows the dominant firm to take advantage of the quantity commitment through all-units discounts while the capacity-constrained rival is induced to offer simple linear pricing.====We consider a duopoly model in which a full-capacity dominant firm competes with a capacity-constrained minor firm for a single buyer. To give the option of using AUDs to both firms and study which pricing policy they would choose in equilibrium, we consider the following four-stage game. In stage 0, each of the two firms simultaneously decides whether to commit itself to use LP or to use AUDs. The next two stages, stages 1 and 2, are pricing stages. In stage 1, each firm can either offer a pricing scheme (from the feasible set determined by its choice in stage 0), or wait until stage 2. In stage 2, any firm who chose to wait in stage 1 offers a pricing scheme (again from the feasible set determined by its choice in stage 0). In stage 3, the buyer chooses the quantities she purchases from the two firms.====Our main result (Theorem 1) is that ====. More precisely, the game has a unique equilibrium outcome, in which only the minor firm commits itself to LP in stage 0, the dominant firm makes an AUDs offer in stage 1, and the minor firm waits in stage 1 and makes a LP offer in stage 2. Chao et al. (2018a, hereafter CTW) focus on a three-stage game in which a dominant firm offers AUDs first and then a capacity-constrained rival responds with LP, followed by a buyer’s purchase decision. The current paper shows that the exogenously assumed pricing and timing asymmetries in CTW will endogenously arise in equilibrium.====Intuitively, the minor firm restricts itself to LP upfront in order to soften the competition between the two firms. Knowing that the minor firm commits to LP, the dominant firm is encouraged to make its offer first (so that the minor firm can enjoy some second-mover benefits) and to make a less aggressive offer. This is the only way that the minor firm can make positive profits given the dominant firm can use AUDs. Such strategic competition-softening commitment can ==== be effectively used by the minor firm, not by the dominant firm. The reason hinges on two important features of our setting: ====. Note that a sequential-move price competition in LP exhibits the features of first-mover disadvantage and second-mover advantage, since the follower can undercut the leader’s price. However, when the leader is allowed to use AUDs, the quantity threshold under AUDs introduces a quantity-strategic instrument into the price competition, so that the model now has some flavor of Stackelberg leadership game and hence can potentially exhibit a first-mover advantage from commitment value. As it turns out, whether a leader can enjoy such a Stackelberg-type first-mover advantage using AUDs depends on whether the leader has a captive demand, i.e., whether the follower is capacity-constrained. If the follower is the capacity-constrained firm, the price-undercutting threat from the follower would be limited and the first-mover advantage exists (Proposition 4). In contrast, if the follower is the unconstrained firm, the undercutting threat from the follower, even if it can only use LP, would be so severe that the first-mover advantage vanishes (Proposition 3). Consequently, by committing to LP, the minor firm can induce the dominant firm to lead, but the dominant firm cannot induce the minor firm to lead by doing the same.====Our main result relies on the assumption that the minor firm is able to make an upfront commitment to use LP. However, the commitment need not be interpreted literally; it can be regarded as a reduced-form modeling that reflects development and maintenance of reputation or industry convention.==== The fundamental point made clear by the analysis of our four-stage game is that: if the minor firm can, by whatever means, commit upfront that it will only use LP, it would have incentives to do so; in contrast, the dominant firm has no such incentive to do so even if it could.====. This paper is related to the oligopoly theory literature of endogenous order of moves. The most related one is Hamilton and Slutsky (1990). They generally proposed two extended games to endogenize the order of moves for a given “basic game,” termed the extended game with ==== and the extended game with ====, respectively.==== Although we formally use “action commitment” to model the endogenous order of making offers, one can easily adapt our analysis to “observable delay” and show that the equilibrium outcome in our paper does not change if we use “observable delay” instead to model the pricing stages.==== However, we do not know of any paper about endogenous order that allows for nonlinear pricing, not to mention choices between linear pricing and nonlinear pricing like in our paper.====The efficiency analysis of our simultaneous-move subgames is related to the literature on competition in nonlinear pricing. When symmetric firms compete simultaneously in nonlinear pricing, Bernheim and Whinston (1986), Bernheim and Whinston (1998), and O’Brien and Shaffer (1997) show that the outcome is efficient under complete information. In our paper, the simultaneous price competition subgames in which the dominant firm can use AUDs (including simultaneous AUDs vs LP and simultaneous AUDs vs AUDs games) also lead to the efficient outcome (Proposition 1). However, other simultaneous price competition subgames in which the dominant firm can only use LP (including simultaneous LP vs LP and simultaneous LP vs AUDs subgames) result in inefficient outcomes (Proposition 2).==== Intuitively, the efficiency under simultaneous price competition relies on a firm who is both capacity-unconstrained and able to use nonlinear pricing to guarantee that no money would be left on the table.====Recently, there have been some studies about competitive effects of AUDs. Feess and Wohlschlegel (2010) show that AUDs can shift the rent from the entrant to the coalition between the incumbent and the buyer. Conlon and Mortimer (2017) study the effects of AUDs used by a dominant chocolate candy manufacturer, and find empirical evidence that AUDs result in upstream foreclosure. Chao et al. (2018a) show that the AUDs adopted by a dominant firm can partially foreclose the capacity-constrained rival and harm the buyer.====The rest of the paper is organized as follows. Section 2 presents the model. Section 3 analyzes the equilibrium and states our results. Section 4 concludes. The proofs that are not provided in the text are in the Appendix.",Asymmetry in capacity and the adoption of all-units discounts,https://www.sciencedirect.com/science/article/pii/S0167718719300189,20 March 2019,2019,Research Article,249.0
"Deneckere Raymond,de Palma André,Leruth Luc","Department of Economics, University of Wisconsin-Madison, Madison, USA,Department of Economics and Management Science, ENS-Paris Saclay, 94230 Cachan, France,Department of Economics, University of Essex, Park, Colchester, CO4 3SQ, United Kingdom, and International Monetary Fund, Washington DC, USA","Received 1 July 2017, Revised 7 October 2018, Accepted 24 January 2019, Available online 9 March 2019, Version of Record 9 April 2019.",https://doi.org/10.1016/j.ijindorg.2019.01.002,Cited by (1),"We introduce bilateral risk aversion into the mixed adverse selection - moral hazard model of Laffont and Tirole (1986). The presence of exogenous risk interacts with the adverse selection problem in interesting ways. In particular, we show that it is never optimal to present the firm with a fixed price contract, that the efficient firm typically bears more risk than the inefficient firm, and that an increase in exogenous risk may bring about a decrease in expected cost of the project. As a by-product, we also establish that the famous ‘no-distortion-on-the top’ result in adverse selection models relies on risk neutrality of the agent.","A substantial volume of goods and services is acquired by both the public and private sector through procurement. For example, while there is substantial variance in the figure across countries, on average government procurement expenditures take up twelve percent of GDP across OECD countries.==== At the same time, firms purchase a large percentage of their inputs through procurement contracts. One such notable example is the construction of factories and office buildings.====Throughout history, procurement has been plagued by substantial cost overruns. Major government infrastructure projects dating back as far as the 1800s have consistently and substantially run over budget.==== Such cost overruns are a global phenomenon,==== and continue unabated to this day. Recent US projects whose cost overruns reached gigantic proportions include the Big Dig,==== a highway artery in Boston, the International Space Station,==== and the rebuilding of San Francisco’s Bay Bridge after the 1989 Loma Prieta earthquake.==== More systematically, in a study of 258 major transport infrastructure projects, Flyvbjerg et. al. found that cost overruns occurred in 86% of the projects, with rail projects incurring an average cost overrun of 46%. Cost overruns are not limited to the public sector, and also occur frequently and in large magnitude in private procurement. For example, the new football stadium for the New York Jets and Giants was initially billed at $998 million, and is reported to have cost over $1.7 billion (LePatner, 2010).====Largely in response to these massive overruns, procurement contracting have shifted from cost-plus contracts, low powered contracts in which the public sector bears all the risk from cost overruns, to fixed price contracts, high powered contracts in which the contractor bears all the risk instead, but gets to pocket any residual profits. The latter type of contracts have variously been labeled as PPP’s in the US, PFI’s in the UK and Contracts de Partenariats in France. Such contracts have been criticized though for leaving too much profits to firms.====To shed light on these issues, the theoretical literature has focused on issues of adverse selection and moral hazard. Specifically, Laffont and Tirole (1986) develop a model in which the regulator can observe realized costs, but cannot perfectly disentangle the underlying inherent efficiency of the firm and its efforts to reduce costs.==== Since the firm knows its own efficiency, hidden information and hidden actions are therefore both present. Laffont and Tirole show that the efficient agent should be given a fixed price contract, in which the transfer is independent of the cost realization. This makes this type of agent a residual claimant for the fruits of his effort reduction, and therefore results in the first best effort selection. This is analogous to the famous “no distortion on the top” result encountered more generally in adverse selection models. Meanwhile, the inefficient agent is made only a partial residual claimant for his effort reduction, resulting in a selection of effort that is below the first best level. To see why, note that inducing the inefficient agent to accept a fixed price contract would require large transfers. Because managers of the efficient firm could always claim that cost conditions are unfavorable, and select effort level required to make that claim credible, and thereby receive the same large transfers as inefficient firms, they would be left with substantial rents. Reducing the effort level elicited from the inefficient firm lowers the required transfers to the inefficient operator, and hence economizes on the rents paid to the efficient one. Laffont and Tirole’s contribution is substantial, as it points out that large project costs resulting from inefficiently low efforts in cost reduction are not necessarily a consequence of poorly designed regulatory schemes.====An important factor leading regulators to move away from cost-plus contracts is the desire to transfer risk to the private party (Iossa and Saussier, 2018). Indeed, a major source of uncertainty facing regulator and firm alike stems from construction risk that is outside the firm’s control. Prominent examples of such uncertainty include geophysical conditions,==== variability in weather, changes labor and material costs, exchange rate fluctuations, or random quality of materials and labor used in construction. Laffont and Tirole’s work has little to say about optimal risk transfer, because both the regulator and the firms are assumed to be risk-neutral.====The current paper introduces exogenous uncertainty into the mixed adverse selection/moral hazard problem of regulation of Laffont and Tirole. Thus the utility functions of the agent and the consumers depend not just on the physical allocation, the monetary transfer and the agent’s type, but also on the realization of a random variable. To make the analysis interesting, we assume that both the agent and the consumers are risk averse.==== In designing an optimal contract, the regulator must then consider its effect on the level and distribution of risk between the contracting parties, as well as on the firm’s incentives to truthfully reveal private information and exert the desired level of effort.====While standard models of regulation generally assume that either the principal, the agent, or both are risk neutral, our assumption on risk aversion on the part of both contracting parties appears to be well grounded in reality. Indeed, internal agency problems such as managerial discretion can lead regulated firms to act in a risk averse manner. Furthermore, if the project represents a significant portion of the regulator’s portfolio, and project risk is not easily be diversified, the principal can be expected to have preferences that are risk averse (Lewis and Sappington, 1995).====Optimal risk sharing arrangements have been studied extensively in the context of moral hazard models. Adverse selection models, on the other hand, have focused almost exclusively on eliciting truthful information from the agents. Most agency relations involve aspects of both moral hazard and adverse selection, yet except for a few notable exceptions,==== the literature has almost universally treated these problems separately. The reason for this dichotomy is entirely technical: without strong simplifying assumptions, models that includes both adverse selection and moral hazard problems have generally proven intractable. For example, Gottlieb and Moreira (2015) require that both the principal and the agent are risk neutral, and assume limited liability. Escobar and Pulgar (2017) assume that the agent is risk neutral, and that his type affects the distribution over signals. While producing interesting conclusions, neither of these models are suitable for studying the issue of optimal risk sharing, as the agent is assumed to be risk neutral. Chade and Swinkels (2016) allows for risk aversion on the part of the agent, and assumes that the type of the agent represents his disutility of effort. The paper presents an algorithm that produces an optimal solution, provided this solution is feasible. Unfortunately, sufficient conditions for the solution to be feasible prove to be rather strong.==== For all these reasons, the extant literature provides little guidance on how exogenous risk should be shared between the principal and the agent.====To maintain tractability, we make the key simplifying assumption that the realization of the exogenous random variable is ex-post observable. This assumption allows us reduce the problem to an equivalent one that has a pure adverse selection structure. Our assumption on ex-post observability of the cost shock is not primarily motivated by a concern for tractability - though this is certainly a bonus - but rather because it represents an empirically important phenomenon. The cost of the realization of a project often contains factors outside of the firms’ control. Fluctuations in exchange rates, or more generally variability in the price of any input, expose both the firm and the consumers to exogenous risk. The realized value of these random variables is generally not known at the effort selection stage, but is easily observed ex-post.====Our analysis produces several interesting results. First, it is optimal to offer a menu of contracts which provide a transfer to the firm that is contingent on the realized cost of the project and the realized value of the exogenous uncertainty. Each contract in the menu leaves the firm with some risk. Thus, unlike in Laffont and Tirole’s model, the efficient firm is not made a residual claimant, but instead shares the exogenous risk with consumers. In other words, no contract in the optimal menu is a fixed price contract. Second, under plausible assumptions, the inefficient firm bears more risk than the efficient one. More precisely, we establish that whenever the firm’s utility function exhibits DARA and the informational asymmetry is not too severe the inefficient firm bears more risk than the efficient firm. Our analysis reveals that the presence of such exogenous risk interacts with the adverse selection problem in interesting ways. In particular, we are able to show that under plausible conditions an increase in uncertainty, effectuated by a mean preserving spread, simultaneously reduces the effort level of the inefficient firm, and raises the effort level of the efficient firm. As a consequence, increased risk may bring about a decrease in the expected cost of the project.====An interesting by-product of our inquiry is that in the absence of exogenous uncertainty, the simultaneous risk aversion of agent and principal distorts all effort levels downward from their first best level. We thus show that the famous ‘no distortion on the top’ result in adverse selection models relies on risk neutrality of the agent.",Risk sharing in procurement,https://www.sciencedirect.com/science/article/pii/S0167718719300050,9 March 2019,2019,Research Article,250.0
"Smrkolj Grega,Wagener Florian","Newcastle University Business School, 5 Barrack Road, Newcastle upon Tyne, NE1 4SE, UK,Amsterdam School of Economics, University of Amsterdam, Amsterdam, The Netherlands","Received 29 September 2017, Revised 2 October 2018, Accepted 28 February 2019, Available online 6 March 2019, Version of Record 16 March 2019.",https://doi.org/10.1016/j.ijindorg.2019.02.002,Cited by (6),"We study a stochastic dynamic game of process innovation in which firms can initiate and terminate R&D efforts and production at different times. We discern the impact of knowledge spillovers on the investments in existing markets, as well as on the likely structure of newly forming markets, for all possible asymmetries in production costs between firms. While an increase in spillovers may improve the likelihood of a competitive market, it may at the same time reduce the level to which a technology is developed. We show that the effects of spillovers on investments and surpluses crucially depend on the stage of technology development considered. In particular, we show that high spillovers are not necessarily pro-competitive as they can make it harder for the laggard to catch up with the technology leader.","Contemporary markets are flooded with imitations – it is hard to find a business model, good, or service that is not a variation or an adaptation of some earlier version. Dell and HP are only two out of many firms that cloned IBM’s Personal Computer. Atari’s video game attracted as many as 75 imitators, led by Nintendo. More recently, Samsung’s lawyers could not tell the difference between Samsung’s Galaxy Tab and Apple’s iPad in court.==== While more latent than product imitations, imitations of business processes abound as well and often even transcend the sector in which they were first introduced. Walmart’s automated supply chain management strategies were imitated by its competitors (e.g., Kmart, Tesco), but also by companies in other sectors, such as Ryanair. South West Airlines’ innovative business model which led to the low cost revolution in air travel was successfully imitated by both Ryanair and easyJet. Henry Ford’s introduction of the moving assembly line did not only reduce the cost of his Model T car, but also revolutionized manufacturing processes across industries worldwide. The pace of imitation seems ever growing. Shenkar (2010) even writes about the ‘imovation challenge’ – companies that want to succeed, need to fuse innovation and imitation as in the future it will not be possible anymore “to rely on innovation or imitation alone to drive competitive advantage.” Moreover, Bloom et al. (2013) recently found large knowledge spillovers for a panel of US firms. Consequently, they estimate that social returns to R&D are between two and three times the private returns.====In this paper, we develop a continuous-time dynamic model in which two competing firms need to decide how much to produce and how much to invest in cost-reducing R&D over an infinite horizon. We then study how information leakages, or spillovers, affect industry dynamics and structure through their impact on innovation incentives of firms at different stages of development.==== Our focus is process innovation, interpreted as any improvement in ‘the way things are done’ that enables a firm to satisfy a given consumer need at lower cost. This focus is motivated by the fact that product innovations often cannot be successfully introduced in the market without accompanying process innovations==== and that over time relative productivity becomes decisive for surviving in the market. In fact, a higher emphasis on process innovation by Japanese companies is believed to be one of the main reasons for their increasing competitiveness over their American counterparts (Bhoovaraghavan et al., 1996).====We solve for a feedback Nash equilibrium of the differential game, which is characterized by a system of highly nonlinear implicit partial differential equations, by a variant of the numerical method of lines (Schiesser, 1991): by discretizing technology levels, but not time, the system of partial differential equations is approximated by a system of ordinary differential equations. The solution to this system is then obtained by standard methods.====The seminal contributions to the analysis of firms’ strategic R&D decisions in the presence of spillovers are d’Aspremont and Jacquemin (1988) and Kamien et al. (1992), which fostered numerous generalizations and extensions. In these two-stage models, firms first invest in cost-reducing R&D and then play a Cournot or Bertrand game in the product market. Surveying early contributions, De Bondt (1997) concludes that “some, but not too high barriers to imitation” seems to be most conducive to innovative activity. Recently, continuous-time dynamic models have emerged.==== Their advantage over static models is that in them firms can smooth their investment over a long time, like usually observed in practice. In the model of Cellini and Lambertini (2009), which is a dynamic version of d’Aspremont and Jacquemin (1988), both firms start with the same level of marginal costs and invest continuously in cost-reducing R&D. This investment gradually reduces initial costs towards the steady-state level. In sharp contrast to its static counterparts, their model leads to a lower level of equilibrium costs for any increase in spillovers. The equilibrium in Cellini and Lambertini (2009) is however not subgame perfect (see Smrkolj and Wagener, 2016), such that the paper effectively discusses only the open-loop situation, in which firms commit to the entire investment schedule at the beginning of the game and therefore cannot respond to each other’s actions over the course of time. Our current paper fills the gap in the literature by providing the feedback solution – which by construction is subgame perfect – to a differential game that shares its fundamental setup with Cellini and Lambertini (2009). It improves on that and other related papers in several dimensions which we discuss below; most importantly, our setup captures the feature that R&D investments in process innovations can be made long before production is viable. This is an alternative to the well-known patent race mechanism.====Recent contributions in the field of dynamic R&D emphasize the importance of initial conditions for long-run outcomes. For instance, in Dawid et al. (2015), a decision of an established incumbent to invest in risky R&D to extend its product range depends on its initial product capacity and knowledge stock. In Hinloopen et al. (2013), the value of the initial marginal cost determines whether a monopolist develops a technology further or exits the market. In particular, it can be optimal for a firm to invest in process innovation when the marginal cost is above the reservation, or ‘choke’, price, that is, before it is profitable for the firm to start production. This extends the current static and dynamic models with spillovers (e.g., Cellini, Lambertini, 2009, Petit, Tolwinski, 1999) which assume that the initial marginal cost is below the choke price, and which thereby focus on how spillovers affect R&D efforts in an existing product market. Such investigations are local in nature, as they discuss dynamics near a steady state and concern themselves with comparative statics questions of the sort ‘What is the influence of changing this parameter on that steady-state quantity?’ We call these ‘questions of degree’.====But for many new technologies the decision of the firm whether or not to develop the product is taken long before production, when the initial production costs of the technology still exceed the highest willingness to pay in the market. This kind of decision is qualitative rather than quantitative. Our global model is capable of analyzing such situations, in which the questions are of the form ‘How does changing this parameter affect in which steady state the system will end up?’ We call these ‘questions of kind’. Clearly, to answer such questions, out-of-steady-state dynamics has to be considered. A distinguishing feature of our approach is that it can model the decision of a firm to never enter a particular market==== or to exit some existing market in due time. Consequently, we are not only able to analyze how spillovers affect R&D investments on existing markets, but also how they influence the likelihood that a new market will be formed, and if so, how its likely structure – monopoly or duopoly – relates to the level of spillovers, and to the distribution of firms’ initial unit costs. Thus, our framework puts conclusions of the previous literature into a broader perspective.====Hinloopen et al. (2013) are the first to provide a global analysis for a continuous-time dynamic model of R&D. The present article extends their dynamic framework in two significant directions. First, rather than a monopoly, we consider a differential game with two competing firms, which are both free to enter the market. Second, given that innovation is inherently uncertain, we introduce stochasticity to the R&D process.====At the outset of the game, each firm has an initial unit cost of production ====(0), corresponding to the initial level of a particular technology. This value may differ between firms, for instance if the imitator lacks the production experience of the innovator. Firms can increase their production efficiency by exerting R&D efforts, which are however subject to imitation. Higher production efficiency makes them stronger competitors on the Cournot product market. Firms’ product market participation constraints are taken into account explicitly. Consequently, R&D and production do not need to coexist at all times and firms can enter or exit the product market and initiate or cease their R&D processes at different times.====In the present set-up, the initial unit production cost of a firm can be above the choke price; in such a situation, the firm has to decide whether development of the product is beneficial in the long run. If so, the firm begins with investing in R&D while postponing production. The investment costs will be recouped in the future, when the technology is advanced to the point that production becomes viable. This key feature our model can capture, while it is inaccessible to static or near-steady-state approaches.====The empirical literature indicates large differences in productivity across firms due to their differences in information technology and management practices (see, e.g., Baily, Hulten, Campbell, Bresnahan, Caves, 1992, Bloom, Van Reenen, 2011). Due to their prominence in practice we put asymmetries in production costs between firms, and their dynamic evolution, in the center of our investigation. For instance, a long-standing conclusion of the literature, confirmed in a deterministic dynamic framework by Petit and Tolwinski (1999), is that higher spillovers can prevent the monopolization of the industry by easing imitation. We show that while intuitive, the above result may not be universally true and that high spillovers can favor market dominance in some asymmetric instances.====The remainder of the paper is organized as follows. Section 2 describes the model specification. Section 3 discusses equilibrium strategies and corresponding industry dynamics. Section 4 considers welfare effects of spillovers. Section 5 compares our results with the related literature. Section 6 summarizes and concludes. Lastly, appendices explain our computational approach to obtaining a feedback Nash equilibrium solution.","Research among copycats: R&D, spillovers, and feedback strategies",https://www.sciencedirect.com/science/article/pii/S0167718719300165,6 March 2019,2019,Research Article,251.0
Vives Xavier,"IESE Business School, Spain","Received 9 January 2018, Revised 4 August 2018, Accepted 7 August 2018, Available online 4 March 2019, Version of Record 8 May 2019.",https://doi.org/10.1016/j.ijindorg.2018.08.011,Cited by (50),"Competition has been suppressed for extended periods in ==== and when it has been unleashed, financial stability has suffered. This paper elucidates the relationship between competition and stability in modern banking, with particular attention to the impact of digital technologies, and derives the policy consequences for regulation and competition policy.","In this paper, I examine the link between competition, stability, and the development of unregulated banking activity. The aftermath of the crisis has revived old issues and posed a host of new questions about the relationship between competition and financial stability, as well as between competition policy and regulation in banking. Competition has an important bearing on all the perceived failures associated with banking and the financial system, such as excessive risk-taking, credit overexpansion, exuberant growth, and bank misconduct. However, we should not forget that competition is also a key driver of efficiency. A recent case in point is the potential disruption of the banking sector by digital technology and fintech competitors. This development has the potential to put pressure on competition and stability in the financial system. New ways of providing financial services to customers could have a negative effect on the profitability of traditional banks. However, they could also lead to a more cost-effective and efficient banking system with better terms for customers.====Banking has been transformed from branches (brick and mortar) to information technology (hard information replacing soft), and highly specialized human capital. Banks and markets have become intertwined, with a higher proportion of intermediary activities becoming market-based. Banks face increased competition from other intermediaries in their core business. Indeed, shadow banking provides a competitive check on banking, but has also induced instability. A question is whether the digital disruption with new players such as ==== and ==== will impair stability. In historical perspective, regulation has not been able to tame financial innovation and regulatory arbitrage, and the regulatory cycle of liberalization, crisis, re-regulation, and regulatory failure has continued. The question is whether we will ever learn to avoid this cycle.====I argue that competition is unequivocally socially beneficial, if regulation is adequate. However, a trade-off exists between competition and financial stability along some dimensions, due to regulatory failure, and it can be alleviated through better regulation. The increase in competition brought by the disruption of digital technologies will be no exception to his trade-off. The upshot is that regulation has to rise up to the challenge.====This article is organized as follows: Section 2 provides a very brief history of financial crises. Section 3 deals with the transformation of banking, focusing on the rise of shadow banking and financial innovation. Section 4 provides a selective overview of competition and stability in modern banking. Section 5 develops the need to coordinate prudential regulation and competition policy. Section 6 provides concluding remarks.",Competition and stability in modern banking: A post-crisis perspective,https://www.sciencedirect.com/science/article/pii/S0167718719300098,May 2019,2019,Research Article,252.0
"Batikas Michail,Claussen Jörg,Peukert Christian","LMU Munich, Germany,UCP – Católica Lisbon School of Business and Economics, Portugal,Copenhagen Business School, Denmark","Received 22 December 2017, Revised 19 February 2019, Accepted 20 February 2019, Available online 28 February 2019, Version of Record 23 March 2019.",https://doi.org/10.1016/j.ijindorg.2019.02.001,Cited by (10),"We study the effects of a self-regulation effort, orchestrated by the European Commission in 2016 and finalized in 2018, that aims to reduce advertising revenues for publishers of copyright infringing content. Data on the third-party HTTP requests made by a large number of piracy websites lets us observe the relations of the piracy and advertising ==== over time. We compare these dynamics to a control group of non-advertising services which are not subject to the self-regulation. Our results suggest that the effort is limited in its effectiveness. On average, the number of piracy websites that make requests to EU-based advertising services does not change significantly. Only when we allow for heterogeneity in the popularity of third-party services, we find that the number of piracy websites that interact with the most popular EU-based advertising services decreases by 42%. We do not find evidence that non-EU-based advertising services react to the self-regulation. This implies that only a small share of the firms in the market comply with self-regulation in a way that is visible in our data. We also do not find evidence that the demand for piracy websites decreases due to this “follow the money” initiative.","Advertising is an important institution in multi-sided markets. On the one hand, it enables firms to inform and persuade consumers about their products. On the other hand, it generates incentives for the provision of services. The classic example is the newspaper market, where advertisers fund journalism by effectively subsidizing the price charged to readers. However, these incentives may be less socially desirable if the provided services are illegal, as it is the case for the provision of unlicensed software, music, film, and book content (“online piracy”). According to industry reports, copyright infringing websites are predominately funded by advertising.==== When advertising makes it profitable to sustain a platform that makes unlicensed software and media products available to consumers, and consumers substitute away from licensed products, this reduces the profits of creators. While close to zero prices increase consumer surplus in the short run, welfare can be reduced in the long run if profits are too low for continued investment in new products (e.g. Bae and Choi, 2006).====Recognizing this trade-off, firms and governmental authorities have used a variety of different approaches to online copyright enforcement. A growing body of literature evaluates these policies and arrives at sobering conclusions. The effects of anti-piracy interventions are often short-lived and have collectively failed to substantially eradicate online piracy (Danaher, Smith, Telang, Chen, 2014, Adermon, Liang, 2014, Reimers, 2016, McKenzie, 2017, Peukert, Claussen, Kretschmer, 2017, Aguiar, Claussen, Peukert, 2018).==== An important reason why new incarnations of unlicensed content platforms keep entering the market could be that the previous initiatives did not put enough emphasis on the economic incentives to supply unlicensed content. A relatively new approach to online copyright enforcement, termed “follow the money”, aims to do so. The largest effort of this kind is an initiative orchestrated by the European Commission (EC), in which the online advertising industry faces pressure to establish a voluntary agreement to reduce the flow of revenues to copyright infringing websites. Following the example of a series of similar smaller initiatives on the national-level, the EC started to invite stakeholders across Europe to participate in a supranational self-regulatory effort. Although participation is voluntary and sanctions for non-compliance are relatively soft in the short-run, the EC has underlined that it has significant leverage in the long run, threatening that soft law measures “[...] could be backed by legislation, if required, to ensure their full effectiveness.”==== The implementation of the self-regulation proceeded in two steps. In October 2016, participants approved a statement of ==== which include commitments to concrete measures. In June 2018, participants declared publicly to adhere to a perhaps stricter and more binding version of these principles by signing a ==== (MoU).====We evaluate the effects of this self-regulation with respect to observable interactions of piracy websites and online advertising intermediaries, and with respect to the demand for piracy websites. Specifically, we compare the involvement of EU-based and non-EU-based advertising firms in the piracy market, before and after the ==== and the ====, to a control group of other third-party services that are not directly part of the advertising industry and are therefore not subject to the EC’s effort to self-regulation.====We collect data on the advertising and non-advertising services associated with 392 piracy websites and 784 comparable legitimate (“placebo”) websites over a period of 3 years, based on the HTTP requests that these websites make to third parties. This lets us follow 7189 third-party services and their interactions with piracy and placebo websites. We add meta information that lets us distinguish between EU-based and non-EU-based, smaller and larger, and different types of third-party services. We estimate parameters of reduced form difference-in-differences models, allowing for group-specific time trends and individual-level fixed effects.====Two main results emerge from our analysis. First, our results suggest that the average effect of the ==== is not distinguishable from zero. Second, once we allow for heterogeneity in size, we find that the number of piracy websites that interact with the largest EU-based advertising services decreases by 42%. We interpret this as evidence that the likelihood of joining the initiative or complying with the self-regulation increases with firm size. Although data issues preclude a clean causal interpretation of the estimates, the ==== seems to have similar effects. These results raise concerns about the overall effectiveness of the self-regulation effort with respect to reducing incentives for publishers to supply unlicensed content. Somewhat consistent with this view, we show that there is no decrease in the number of visits to piracy websites, comparing those that are connected to affected and those that are connected to unaffected third-party services. Being the first academic study that evaluates the “follow the money” approach to online copyright enforcement, our paper contributes to the growing literature on piracy, counterfeiting and the evaluation of IP enforcement policies (Adermon, Liang, 2014, Danaher, Smith, 2014, Reimers, 2016, Peukert, Claussen, Kretschmer, 2017, Aguiar, Claussen, Peukert, 2018). In the sense that we study a potential source of unintended externalities of online advertising, and an regulatory effort that is specifically targeted at that industry, we also contribute to the literature on the economics of online advertising (see Tucker, 2012, Goldfarb, 2014, Peitz, Reisinger, 2015 for summaries), linking directly to papers that study the relationship between advertising and content quality (Sun, Zhu, 2013, Shiller, Waldfogel, Ryan, 2018).====We believe that our results allow to draw some broader implications. Showing that creating incentives for firms to self-regulate and internalize an externality can be intricate when firms are heterogenous, can be helpful for the design of future efforts. Closely related, the EC has announced that it will try to establish similar memoranda in the market for online payment services, and logistics and transport services. Another example is the market for false and misleading information (“fake news”), which seems to share many characteristics with the market for online piracy.",Follow the money: Online piracy and self-regulation in the advertising industry,https://www.sciencedirect.com/science/article/pii/S0167718719300086,28 February 2019,2019,Research Article,253.0
Qi Shi,"Economics Department, College of William and Mary, P.O. Box 8795, Williamsburg, VA 23187, USA","Received 3 October 2017, Revised 7 September 2018, Accepted 18 September 2018, Available online 21 February 2019, Version of Record 4 March 2019.",https://doi.org/10.1016/j.ijindorg.2018.09.006,Cited by (9),"With the advent of the Internet and social media platforms, advertising has become cheaper and more effective in reaching consumers. This paper studies the effect of better informative advertising on innovation and ","Innovation is widely recognized as a driving force behind economic growth. An abundance of economic literature on industry growth, including the seminal work of Grossman and Helpman (1991), has focused on firms’ incentives to innovate.==== One factor that may specifically influence innovation, which has largely been ignored by the literature, is advertising. In many industries, advertising spending exceeds that of innovation. For example, the pharmaceutical industry, which is often considered a highly innovative industry, spends about 20 percent of sales on research and development. Yet, nine out of ten of the largest pharmaceutical companies in 2013 spent more on marketing than they did on R&D.==== With the advent of Internet and social media platforms, advertising has become cheaper and more effective in reaching consumers. It is natural to ask how better advertising will affect innovation and industry growth.====To answer this question, this paper uses a variant of the Grossman and Helpman (1991) quality ladder model, which incorporates informative advertising. The model features discrete-choice in demand, where consumers choose from an existing incumbent product, a new innovative product, and an outside option. A key feature is that consumers become aware of a product through advertising. The more a product is advertised, the higher its consumer awareness level. Because of this feature, firms not only innovate to improve product quality, but advertise to expand product awareness. In addition, the model allows varying degrees of innovation spillover, so new entrants can build upon existing technologies and benefit from incumbent firms’ innovations. Firms invest in innovation in anticipation of possible entry competition in the future.====The model has two driving forces. The first one is the complementarity between informative advertising and innovation. Informative advertising spreads awareness of a newly developed innovation, and expands a new product’s potential consumer base. Therefore, improvements in advertising can spur innovation and industry growth by increasing demand. The second driving force is the entry-inducing effect of innovation. Better advertising makes entry more profitable and raises the threat of competition from potential entrants. This paper argues that firms can deter entry by strategically manipulating their innovation intensity.====The impact of incumbent innovation on entry depends on the interplay of three different effects: the ====, and ====. First, innovation leads to a higher product quality, which makes the incumbent more competitive. Hence, the ==== lowers the entry profitability. Second, a higher quality leads the incumbent to choose a higher price. The ====, similar to the “fat-cat effect” of Fudenberg and Tirole (1984), prompts a higher quality incumbent to extend a price umbrella over the entrant’s product, thus increases entry profitability. In addition, with the ====, a higher incumbent innovation leads to a higher entrant quality, therefore raises entry profitability. The combined effect of incumbent innovation on entry depends on the relative magnitudes of the three effects, which in turn depend on model demand specifications. In this paper, the ==== is the determining effect, while the ==== and the ==== offset each other. Therefore, to prevent the entrant from taking advantage of the spillover, an incumbent undercuts its own innovation to lower entry profit and deter entry.====The paper first uses a simple two-period model to develop theoretical intuitions. Analytical solutions are derived to show that an incumbent firm is not only able, but also willing, to engage in entry deterrence. Then, the two-period model is extended into an overlapping sequential entry dynamic model. Numerical analysis of the model is used to evaluate the impacts of entry deterrence on economic welfare and industry growth, which is inherently dynamic.====This paper finds that improvements in advertising can lead to costly entry deterrence, which slows down the industry innovation rate. As a result, consumer welfare can suffer despite better technology in spreading consumer awareness. Furthermore, the paper investigates the role of innovation spillover on entry deterrence. Interestingly, less spillover can better shield incumbent firms from entry threats, but also can intensify undercutting in innovation when entry deterrence occurs.====The complementarity between advertising and innovation, which plays an important role in this paper, has been explicitly pointed out in the previous literature. Acs and Audretsch (1987) documents that firms in advertising intensive industries have a greater profit potential from innovation. Similar to this paper, Grossmann (2008) provides a theoretical analysis of advertising in a quality-ladder model of endogenous growth. It finds that a firm’s R&D investments are positively related to its incentives to engage in combative advertising. More generally, the advertising and innovation complementarity is a special case of demand-markup complementarity. Measures that increase demand and measures that increase markup are complementary. This principle has been applied to wide array of IO literature, such as those studying the complementarity between product and process innovations (i.e. Utterback and Abernathy, 1975).====This paper also adds to a long strand of IO literature studying entry deterrence. Ample economic literature dating back to Bain (1949) have pointed out that firms engage in costly entry deterrence behavior to protect their market positions. A common thread tying this paper to the previous literature, including Schmalensee (1978), Salop (1979), Dixit (1980), Spulber (1981), and Bagwell (2007), is that improving advertising technology can lead to entry deterrence by under-investment. In Schmalensee (1983) and Fudenberg and Tirole (1984), advertising can expand consumer awareness and incentivize an incumbent to raise its price.==== This increase in the profit margin extends to potential entrants and makes entry more likely. Boyer and Moreaux (1999) provides a static framework of entry deterrence and argues that firms strategically under-invest in advertising to soften pricing competition. Doraszelski and Markovich (2007) provides a more modern approach by incorporating advertising and industry dynamics in the Ericson and Pakes (1995) framework. Following similar logic to Boyer and Moreaux (1999), their numerical simulation results indicate that under-investment in advertising can deter entry. Empirically, Ellison and Ellison (2011) finds that strategic entry deterrence is possible in the pharmaceutical industry by reducing advertising investment. Extending on these papers, this paper shows that better advertising can also lead to entry deterrence by under-investment in a complementary measure, such as innovation.====The main idea of this paper is to show that slower industry growth is a plausible and reasonable outcome of improving advertising technology. The idea that incumbent firms engaging in preemptive entry deterrence inhibits industry innovation growth is supported by empirical findings. Han et al. (2001), using a sample consisting of 127 Korean consumer product firms, finds that efforts by incumbents to discourage entry are associated with ineptitude of industry innovative positions.","Advertising, industry innovation, and entry deterrence",https://www.sciencedirect.com/science/article/pii/S0167718719300074,21 February 2019,2019,Research Article,254.0
"Auriol Emmanuelle,Biancini Sara,Paillacar Rodrigo","Toulouse School of Economics, France,CREM UMR CNRS 6211, France,Université de Cergy-Pontoise, THEMA UMR CNRS 8184, France","Received 7 October 2017, Revised 26 October 2018, Accepted 25 January 2019, Available online 10 February 2019, Version of Record 4 March 2019.",https://doi.org/10.1016/j.ijindorg.2019.01.003,Cited by (9),"Developing countries’ incentives to protect ==== (IPR) are studied in a model of vertical innovation. Enforcing IPR boosts export opportunities to advanced economies but slows down technological transfers and incentives to invest in R&D. Asymmetric protection of IPR, strict in the North and lax in the South, leads in many cases to a higher world level of innovation than universal enforcement. IPR enforcement is U-shaped in the relative size of the export market compared to the domestic one: rich countries and small/poor countries enforce IPR, the former to protect their innovations, the latter to access foreign markets, while large emerging countries free-ride on rich countries’ technology to serve their internal demand.","There has always been an international dimension to debates on intellectual property rights (IPR); with the integration of the world economy, however, IPR debates have become global. The United States, the European Union, Japan, and other developed countries have actively pushed to impose “Western-style” IPR legislation worldwide. Contrary to the Paris and Berne Conventions, which allowed considerable flexibility in their application, the agreement on Trade-Related Aspects of Intellectual Property Rights (TRIPS) imposes a common framework to all World Trade Organization (WTO) members as regards IPR.==== To date, this is the most important international agreement on the design of intellectual property regimes. And it is also the most controversial, having been challenged by many countries, including Korea, Brazil, Thailand, India and the Caribbean states. As a result of these tensions the enforcement of IPR legislation varies considerably around the world. The present paper proposes a simple theoretical framework in which developing countries’ incentive to enforce IPR can be analyzed. The desirability of enforcing IPR equally, everywhere, including in developing countries, can also be assessed.====One source of conflict between developed and developing/emerging countries regarding the TRIPS agreement is that strong IPR limit the possibility of technological learning through imitation, something which has been a key factor in the development of countries such as the US (in the 19th century), Japan, Taiwan, or South Korea (in the 20th century), and more recently China and India (see Sachs, 2003).==== Having copied technology invented by others, these countries have become major innovators: today the top three countries in term of R&D expenditure are the US, China, and Japan.==== It is thus not clear that international agreements such as TRIPS will lead to more innovation at the global level. The costs and benefits of universal enforcement of IPR need to be more thoroughly analyzed.====We study the impact of different IPR regimes (no protection; partial protection where only the rich country enforces IPR; and full protection) on the investment decisions made by private firms in a two-countries – developing and developed – model. We focus on incremental innovation: innovation enhances the quality of a vertically differentiated commodity. This corresponds, for instance, to a new generation of mobile/smart phones, or an improvement of an existing drug. Indeed, many new products are incremental improvements on existing ones (Acemoglu et al., 2016).==== For instance in the pharmaceutical industry, “on average, only about one-third of new-drug applications submitted to the FDA are for new molecular entities. Most of the rest are either for reformulations or incremental modifications of existing drugs...” (see Congressional Budget Office, 2006 pp. 15–16). The cost of the R&D investment depends on the efficiency of the R&D process, which by convention is higher in the advanced economy. By contrast, we assume that imitation is costless. However, it yields a potential indirect cost: a firm that violates IPR cannot legally export the imitated good to a country that enforces them.====If a WTO member is found guilty of violating its IPR obligations, the complaining government obtains the right to impose trade sanctions in the form of punitive tariffs. There have hence been more than 30 TRIPS-relates disputes since the enactment of the agreement. In many cases the simple threat of sanctions was enough for the parties to find a solution.==== In other cases sanctions were implemented.==== In the US, Section 3.1 and Special 301 of the Trade Act include retaliatory trade sanction against countries violating US intellectual property rights. Harris (2008) mentions several countries targeted by this mechanism in Latin-American (Argentina, Brazil, Chile and Mexico) and in Asia (China, India, South Korea and Thailand).====Even in the absence of trade sanctions, advanced economies monitor their importations to block out goods suspected of infringing intellectual property right. The European Union has enacted a new regulation concerning customs enforcement of intellectual property rights, which came into force on 1 January 2014 (see IP/11/630 and MEMO/11/327). This regulation introduced a decisive change to the procedure for destroying suspicious goods: Such goods can now be destroyed by customs control without the need to initiate a legal proceeding to determine the existence of an infringement of intellectual property rights. In the United States, Customs and Borders Protection similarly targets and seizes imports of counterfeit and pirated goods, and enforces exclusion orders on patent-infringing goods.====Consistently with these national and international legislations and practices, in the model below IPR protection shields the domestic firm from the competition of patent infringing foreign competitor. There are thus benefits for a firm originating from a country which enforces IPR in competing with a firm originating from a country that does not enforce them: IPR act as a barrier to its competitor entering into its market, and it can copy its competitor’s innovations, if any. If the developing country chooses to protect IPR to be able to export then the patented products are imperfect substitutes and the domestic and the foreign firms are competing à la Cournot in both markets. The analysis has two steps.====First we establish that the link between protection of IPR and investment in R&D is non-monotonic: full protection of IPR is not always conducive of a higher level of investment than a partial regime. This result arises because, when technological transfer occurs through imitation, innovation by one firm expands the demand of both firms so that the competitor has more incentive to invest in R&D. Technically the R&D investment of the two competing firms are strategic complements under a partial protection regime of IPR and there are strategic substitutes under a full protection regime. Our model then predicts that stricter IPR decreases genuine innovation by the local firm in the developing country, while increasing innovation by the firm in the developed country, without necessarily increasing innovation at the global level. This result is consistent with the empirical literature on pharmaceutical (see Chaudhuri, Goldberg, Jia, 2006, Qian, 2007, Kyle, McGahan, 2012, Williams, 2013).====Second, we establish that the incentives to protect IPR in a developing country are decreasing in the relative size of its domestic market compared to its foreign market. When the size of its national market is large compared to its foreign market, the developing country can afford not to protect IPR to free-ride on advanced economies technologies, even if this precludes some of its firms from legally exporting to rich countries (e.g., generic drugs produced without licence in India). The paper thus predicts that small developing countries should be willing to enforce IPR, since IPR protection enhances export opportunities, while large ones should be more reluctant to do so, as illustrated by the recurrent disputes between the US and China, or the US and Brazil. In other words, our model predicts that the willingness to enforce IPR should be ==== in the relative size of a country’s internal market with respect to its export opportunities. This theoretical result is consistent with existing empirical evidences. Empirically there is a robust U-shaped relation between IPR enforcement and economic development (see Braga, Fink, Sepulveda, 2000, Chen, Puttitanun, 2005, Auriol, Biancini, Paillacar, 2017).",Universal intellectual property rights: Too much of a good thing?,https://www.sciencedirect.com/science/article/pii/S0167718719300062,10 February 2019,2019,Research Article,255.0
"Triebs Thomas P.,Pollitt Michael G.","School of Business and Economics, Loughborough University, LE11 3TU, UK,Judge Business School and Energy Policy Research Group, University of Cambridge, Trumpington Street, Cambridge CB2 1AG, UK","Received 8 March 2018, Revised 17 December 2018, Accepted 19 December 2018, Available online 19 January 2019, Version of Record 15 February 2019.",https://doi.org/10.1016/j.ijindorg.2018.12.003,Cited by (8),"Does privatization increase plant productivity because the private owner’s objective is different, or because they are better able to control management? And, is privatization sufficient to improve productivity, or is it only effective in combination with competition? We answer these questions using the quasi-experiment of Great Britain’s ==== privatization. To separate the effect of a change in objectives from a change in incentives we assume, that the former only affects labor but not fuel productivity. And, assuming that effective competition was only introduced after privatization, we are able to separately identify the effects of privatization and competition. We find that privatization increased labor but not fuel productivity: evidence for the importance of objectives. There is no evidence that the introduction of effective competition after privatization increased labor or fuel productivity: evidence that privatization increases productivity by itself.","Productive efficiency is an important driver of economic welfare (Leibenstein, 1966). Theoretically, both private ownership and competition increase the productive efficiency of firms and plants, but there are alternative explanations for why privatization increases productive efficiency. In this paper we empirically analyze whether privatization increases plant productivity due to changed ==== or changed ====. Related to these alternative causes is a debate about whether privatization itself increases productivity or whether privatization increases productive efficiency only in combination with competition. We analyze whether the ==== effect of privatization and competition is different from the effect of privatization only. Public ownership of firms is still widespread and in some countries, in particular the United Kingdom, there is a debate about reversing privatizations. Knowing why privatization increases productive efficiency, if at all, is important for mitigating any inefficiencies from public ownership, where it has benefits, e.g. lower cost of capital or mitigating externalities. This requires a causal identification of the effects but also of the mechanisms.====To study the effects of privatization and competition on productivity we use the case of Great Britain’s (GB) electric industry reforms: restructuring and privatization (R&P) in 1990/1991 and subsequent competition reforms in the second half of the 1990s. The GB case is of interest, because it was one of the earliest and most pervasive utility industry reforms in the world.==== With the availability of comparable US plants as control group we can identify the effects using standard fixed effects (difference-in-difference) estimation.==== Plant-level productivity is modeled as the intercept, i.e. treatment indicator, in an input demand function derived from a model of constrained cost minimization (Biesebroeck, 2003). Despite the main identification concern being omitted variable bias at the treatment, i.e. country level, the use of micro data has advantages. Micro data allows better comparability by selecting specific plants only. Also, estimation at the plant level increases precision, because we can include relevant control variables for the dependent variable (Angrist and Pischke, 2008, page 23). We collected unique, plant-level, physical input-output data for a sample GB electricity generation plants for the years 1981–2004 and matched it to an extended version of Fabrizio et al. (2007)’s data for US, publicly owned plants. The cross-country comparison minimizes the risk that treatment effects spill over to the control group. Generally, utility industries provide good cases to test predictions for the effects of privatization and competition, because the technology (here electricity generation from coal) is easy to characterize using observable inputs and outputs, has inputs and outputs that are homogeneous and measurable in physical units, is globally available, and has not experienced any technological disruptions over our sample period. Last, most plants in the treatment group continue which allows ruling out that any effects are primarily due to selection (Syverson, 2004).====Besides the causal identification of the effects our main contribution is the identification of mechanisms. To test the relative importance of the ==== and the ==== mechanisms we use the fact that two important inputs for electricity generation, fuel and labor, are complements in a short-run production function. This allows us to analyze productivity changes for labor and fuel separately.==== When we also assume that the fuel input decision (unlike the labor input decision) at the plant level is independent of the owner’s objectives, i.e. burning extra fuel does not benefit any political constituency,==== we can use the contrast between labor and fuel productivity changes as evidence for the relative importance of the incentive and objective mechanisms. To separate the effects of privatization and competition we use the fact that in GB effective, as opposed to formal, competition was only introduced several years after privatization.==== We test whether the combined effect of effective competition and privatization was different from the effect of privatization only, assuming that there was no delayed effect of privatization itself.====The theoretical literature provides detailed explanations for the incentive and objective mechanisms. Assuming that both public and private owners’ objective is profit maximization, private owners might be better able to solve the principal-agent problem where ownership and control are separate.==== Private owners might find it easier than public owners to align their objectives with managers’ objectives, because public ownership is very diffuse (Shleifer, 1998) or because public owners are unable to apply hard budget constraints (Schmidt, 1996). Another strand of the theoretical literature assumes that public and private owners have different objectives for the firm. Public, unlike private owners, want to subsidize employment to maximize votes. If the political cost of subsidizing employment through public firms is lower than through private firms, privatization increases labor productivity (Shleifer, Vishny, 1994, Boycko, Shleifer, Vishny, 1996).====There is prior evidence that ownership affects labor productivity more than fuel productivity. Although they do not discuss this result in their paper==== Fabrizio et al. (2007) show that in the US, during the late 1980s publicly owned power plants had decreasing labor, but constant fuel productivity, compared to investor-owned plants. Atkinson and Halvorsen (1986), comparing regulated private and public utilities, find a similar result using price inefficiency. Similarly, for nuclear generation plants, Pollitt (1996) shows that whereas fuel productivity does not differ across ownership types, labor productivity is much higher for privately owned plants. Although Chinese reforms of the electricity industry are very gradual and somewhat opaque, the empirical evidence is that labor productivity increased more than fuel productivity (Gao, Van Biesebroeck, 2014, Du, Mao, Shi, 2009).====Closest to our work is Gupta (2005) who empirically separates the incentive and objective effects assuming that partial-cash flow-privatization as opposed to full-control right-privatization can only possibly affect productivity via the incentive mechanism. She assumes that after partial privatization, minority shareholders can influence management incentives but not the firm’s objective. Her finding that partial privatizations in India in the 1990s increased firm-level labor productivity, is evidence that the incentive mechanism matters. She also finds some suggestive evidence that full privatization increased labor productivity further, i.e. the objective mechanism is also important, but due to data limitations is not able to test the effect of full privatization formally. Effectively, this data limitation does not allow her to find that only objectives matter. But just like hers our natural experiment has limits, too. For instance, if we found that privatization increased both labor and fuel productivity we could not tell whether this was due to the incentive effect or some combination of both effects.====Incentives based arguments for why privatization increases productivity are often similar to arguments for why competition increases productivity, which gives rise to a debate about whether privatization itself increases productivity (as in Schmidt, 1996) or whether the effect of privatization is conditional on environmental factors, in particular competition. In principal-agent settings the threat of bankruptcy (Schmidt, 1997), better information (Hart, 1983), or increased sensitivity of profits to managerial effort (Willig, 1987) can explain why competition increases productivity. Nickell (1996) summarizes these theoretical arguments. Empirically, there is evidence that competition increases the productive efficiency of firms and plants (Nickell, 1996, Galdón-Sánchez, Schmitz Jr., 2002, Fabrizio, Rose, Wolfram, 2007). There is also evidence that the effect of competition is more important than the effect of ownership (Caves, Christensen, 1980, Bartel, Harrison, 2005).====We find that whereas labor productivity increased substantially after privatization fuel productivity did not. This is evidence, that privatization improves productivity via changes in objectives rather than incentives, and is consistent with some of the cross-sectional evidence in the literature. The introduction of effective competition after privatization did not increase labor or fuel productivity (further). Given the strong prior evidence that competition does increase productivity, we conclude that in our case competition never became effective during the 1990s. In any case we find evidence that privatization is sufficient to increase productivity, which is of course consistent with the finding that privatization increased productivity due to changed objectives. When considering public ownership it is important to guarantee that the firm objective is efficient production.====The outline is as follows. Section 2 provides some background on GB electricity R&P and the identification strategy. Section 3 describes our empirical approach. Section 4 describes the data. Section 5 gives the results and Section 6 concludes.",Objectives and incentives: Evidence from the privatization of Great Britain’s power plants,https://www.sciencedirect.com/science/article/pii/S016771871830122X,19 January 2019,2019,Research Article,256.0
"García-Vega María,Hofmann Patricia,Kneller Richard","School of Economics, GEP, University of Nottingham, University Park, Nottingham NG7 2RD, United Kingdom,Ruhr-University Bochum, Faculty of Management and Economics, Chair of Finance and Economic Policy, 44801 Bochum, Germany,School of Economics, GEP and CESIfo, University of Nottingham, University Park, Nottingham NG7 2RD, United Kingdom","Received 9 May 2018, Revised 14 December 2018, Accepted 4 January 2019, Available online 12 January 2019, Version of Record 29 January 2019.",https://doi.org/10.1016/j.ijindorg.2019.01.001,Cited by (7),"In this paper we consider how the location, organization and output of knowledge production evolve within domestic firms following acquisition-FDI in order to understand the aggregate effect on an index of domestically produced innovations. We find strong differences according to how close the acquiring MNE is to the technologically frontier. Frontier MNEs are more likely to close R&D activities in acquired affiliates, but when they are retained they expand employment of high-skilled R&D workers and transfer R&D knowledge. Non-frontier MNEs make fewer changes to R&D. Overall the effect of acquisition-FDI on the domestic innovation index is positive.","Modern theories of growth and development (Acemoglu, 2007; Bloom et al., 2012, Aghion et al., 2014) emphasise technology and its evolution as an explanation for differences in income levels across countries. This technology stock is usually defined as the sum of that produced domestically, together with the technologies that have diffused internationally in an embodied or disembodied form (Coe and Helpman, 1995, Keller, 2004). Multinational firms (MNEs) are at the heart of discussions around the determinants of this stock as they account for the majority of global R&D expenditures and share their knowledge across their international affiliates (Helpman, 1984, Markusen, 1984, Branstetter et al., 2006, Keller and Yeaple, 2013).====Globally, most foreign direct investment (FDI) takes the form of mergers and acquisitions, often involving R&D active firms (UNCTAD 2005, UNCTAD 2007). Such acquisitions therefore impact domestic technology levels, but how? Guadalupe et al. (2012) show that acquired subsidiaries increase technology adoption as a result of their access to a larger market. But, what are its effects on the domestic innovation capability of acquired firms and what are its aggregate effects?==== What happens with research employment? Do subsidiaries become more reliant on international technology transfers versus domestic production of knowledge, are there differences across types of MNEs, and does this alter the productivity of the innovation process? To answer these questions, we examine the effects of acquisition-FDI on the production of knowledge by acquired firms and then combine these to describe the evolution of an aggregate domestic technology index.====The starting point for our analysis assumes that MNEs have the incentive to generate and to share knowledge across their affiliates as efficiently as possible and will therefore alter R&D structures in response to new acquisitions (Helpman, 1984, Markusen, 1984, Branstetter et al., 2006, Keller and Yeaple, 2013). We capture these changes by following changes in the probability of starting and shutting down research labs, and then conditional of keeping research facilities, we study changes in internal and external R&D spending, R&D employment disaggregated by level of education and patents per euro of expenditure of acquired firms in the time periods following acquisition. We use a difference-in-differences approach with non-acquired domestic firms as a counterfactual. To control for possible selection on observables, we use the propensity score matching previously deployed to study the productivity and employment effects of foreign acquisitions by Chen (2011), Criscuolo and Martin (2009), Greenaway and Kneller (2007) and Guadalupe et al. (2012) amongst others.====To capture the aggregate effects on domestic innovation we calculate the change in an index of domestically produced innovations per euro of expenditure following a period of acquisition-FDI. Within this index, the aggregate effect of FDI depends upon the response of innovation outputs within acquired firms (a within-firm effect measuring the change in innovation output per euro of expenditure keeping constant the innovation spending of the firm), but also between firm reallocation effects (measuring the change in innovation spending keeping innovation output constant), a cross effect (measuring the change in output and spending) and an entry and/or exit effects (measuring the start, or cessation of R&D).==== We also allow these outcomes to differ across acquisitions, with different responses when the acquiring-MNE is closer to the technological frontier. This paper represents the first attempt in the literature to model the effects of acquisition FDI on the evolution of R&D output together with its location and organization within acquired firms in this way.====The data we use are an annualized version of the Spanish Community Innovation Survey (CIS) and contain detailed information on expenditures, employment and innovation outputs. Within the time period we study (2004 to 2009) there are 300 acquisitions of R&D active domestic firms. These represent 3.7% of total Spanish R&D expenditures, indicating the importance of understanding the effect of changes in ownership of this type.==== Consistent with versions of the CIS for other countries, the data include a direct measure of R&D technology transfers within the MNE: imports of knowledge from the business group.==== That Spain represents a country that would not typically being viewed as on the technological frontier provides an important aspect of our investigation as it ensures heterogeneity in these knowledge transfers across MNEs that we then exploit empirically.==== We consider frontier foreign acquisitions if the MNE is headquartered in a technologically intensive country or not. In the baseline regressions we adopt a conservative classification and include only MNEs from Germany Japan, and USA as technological frontier countries. Of the 300 acquisitions, 104 are by frontier MNEs and 196 are MNEs that are behind the technological frontier.====From the analysis we are able to show that the acquisition of R&D active firms by foreign MNEs has a positive effect on the stock of domestically produced knowledge, accounting for 7.14% of the growth in the Spanish technology index over a 5-year period. This is a significant change in the domestic technology production caused by acquisition-FDI, in particular given these firms accounted for a smaller share of expenditures (3.7%). A large part of this increase is accounted for by the ==== effect; the innovation output of acquired firms increases. This increase is driven by changes that occur when acquisition is from a technological frontier MNE. For these firms, we find evidence of skill- upgrading and large increases in technology transfers of scientific knowledge from elsewhere within the MNE. These positive within effects are offset to some extent by negative between effects, as R&D expenditures fall when acquisition is by a non-technologically-leading MNE and by the increased risk the R&D labs are closed by frontier MNEs.====An explanation for our results is that R&D active MNEs face an opportunity cost of choosing a globalized R&D strategy and retaining innovation within the acquired subsidiary, versus the closure of the acquired lab and the relocation of this R&D effort elsewhere. These opportunity costs can be because innovation is produced with increasing returns to scale as in Arkolakis et al. (2018) or because there are localized knowledge spillovers as in the model of Ekholm and Hakkala (2007). These incentives are likely to vary with the technological leadership of the MNE. If the opportunity cost is high, as it might be for technologically frontier MNEs, the acquired R&D lab may be closed and knowledge creation concentrated within a single location. Alternatively the MNE will exploit complementarities in the knowledge base across different locations by sharing its scientific knowledge between its labs.==== The MNE will then retain the affiliate R&D lab and decentralize R&D globally. If the technology transfers that occur in this outcome are complementary to high-skilled personnel, then the R&D process in the acquired lab will also be restructured to take advantage of intrafirm knowledge transfers. Frontier MNEs have more valuable knowledge to share and might undertake the greatest reorganisation, thereby increasing the retained R&D lab's rate of innovation by the greatest amount.====A different mechanism that can also lead to an increase in innovation expenditures following acquisition are due to a market size effect as in Guadalupe et al. (2012) or Denicolò and Polo (2018a). In the model of Denicolò and Polo (2018a), innovation is not firm-specific, and therefore mergers might increase innovation and the sharing of technologies because the scope for the application of new technologies increases.==== Therefore, the effects of acquisition-FDI depend upon the survival of R&D labs, together with the effects on the organization of R&D that might arise from knowledge complementarities or sharing technologies within a globalized R&D structure.====Our findings make novel contributions to the literature on innovation and acquisition-FDI. For Spain, Guadalupe et al. (2012) find that technology adoption increases in acquired affiliates. Stiebale (2016) extends this literature to show that innovation in the merged entity as a whole rises. Of interest, he shows that this occurs largely as a consequence of a rise in innovation in the acquirer, with declines in innovation for the acquired affiliate. In contrast, we find that innovation in the acquired firm rises, but only for those acquired by technologically-frontier MNEs. This rise in innovations combined with declines in expenditures, suggest rising productivity and therefore our findings provide a novel explanation for the ample evidence for productivity improvements that follow from acquisition FDI (Arnold and Javorcik, 2009, Criscuolo and Martin, 2009, Chen, 2011).====We also make a contribution to a literature that explores partial effects of acquisition-FDI on total R&D expenditures. These results appear to depend strongly on the country being considered, with a mix of positive and negative effects found for total R&D spending (Bandick et al., 2014, Stiebale and Reize, 2011) and on the empirical literature of the effect of mergers on R&D (Bertrand and Zuniga, 2006, Szücs, 2014, Haucap et al., 2019). Compared to these articles, we study total expenditures alongside other organisational changes, including closure, and also how acquisition affects innovation output. Moreover, we analyse indirect effects of FDI-acquisition, or in other words, we study the impact of acquisition on R&D for the non-acquired firms.====Finally, we can relate our findings to a number of different avenues of research on MNEs found in the existing literature. Firstly, they are consistent with a number of stylised facts about the R&D behaviour of MNEs. Alongside being major producers of new technologies (Criscuolo et al., 2010; Dunning and Lundan, 2008) MNEs are more globalised in their R&D locations than non-MNEs (Bloom and Griffith, 2001, National Science Foundation (NSF) 2016; Guadalupe et al., 2012).==== They have also been shown to use more intensively knowledge sourced internationally within their R&D production function (Veugelers and Cassiman, 2004, Criscuolo et al., 2010) and transfer knowledge to their affiliates (Branstetter et al., 2006, Bilir and Morales, 2016).====The rest of the paper is organized as follows. Section 2 details the data used and Section 3 the empirical methodology. Section 4 describes our main empirical results and provides an overall quantification of the effect of foreign acquisition on the domestic knowledge production. Section 5 presents some tests for the robustness of our main findings including pre-existing trends, an alternative definition of frontier-MNEs and measures of indirect effects using the methodology of Girma et al. (2015) that controls by Stable Unit Treatment Value Assumption (SUTVA) violation. Finally, we draw the conclusions from the study in Section 6.",Multinationals and the globalization of R&D,https://www.sciencedirect.com/science/article/pii/S0167718719300049,March 2019,2019,Research Article,257.0
"Knittel Christopher R.,Metaxoglou Konstantinos,Trindade André","Sloan School of Management, Center for Energy and Environmental Policy Research, MIT, USA,NBER, USA,Carleton University, Canada,FGV EPGE, Brazil","Received 11 August 2017, Revised 26 November 2018, Accepted 28 December 2018, Available online 8 January 2019, Version of Record 22 January 2019.",https://doi.org/10.1016/j.ijindorg.2018.12.004,Cited by (12),"We examine the environmental implications of market structure using the exogenous variation in the price of natural gas paid by U.S. electric power producers in the aftermath of the Shale Boom. We find that electric power producers were more responsive to fuel prices in vertically integrated markets than in restructured markets, and we explore the underlying factors driving this heterogeneity in responses. Although differences in the capacity of the most efficient gas power plants between the two market structures are the most important factor, we consider others. The heterogeneity in the response of power plant operators to fuel prices has material implications for carbon dioxide emissions.","The restructuring of the U.S. electricity industry in the late 1990s and early 2000s implemented through a wave of regulatory reforms that followed similar efforts in other industries, such as airlines, railroads, and telecommunications, aimed to promote competition as a way to increase efficiency and productivity.==== Given the extent of the reforms and their far reaching implications, a large number of papers studying the effects of restructuring on the industry’s performance on multiple dimensions followed. There was a good reason for this increased interest among researchers. Restructuring created different incentives for firms to respond to their economic and regulatory environment from the traditional market structure of vertically integrated utilities subject to cost-based regulation. Indeed, assessing whether the newly created wholesale markets in some parts of the country (e.g., California) created incentives for firms to exercise market power was of particular interest in some of the earliest papers (Borenstein et al., 2002). Subsequent papers examined whether restructuring delivered on its very basic promises. That is, whether restructuring provided the incentives to enhance the efficiency and productivity of power plants (Fabrizio et al., 2007).====Because the industry is a major polluter, an interest towards understanding the environmental implications of restructuring also developed, creating a new strand in the literature. Much of this interest was due to major changes in the industry’s landscape. The first was the introduction of regional and federal policies aiming to curb emissions of power plants to combat climate change. Electricity generation has accounted for about a third of annual greenhouse gas emissions in the U.S. for the last 30 years or so due to the use of fossil fuels, primarily coal and natural gas (gas). The second was the Shale Boom and its implications for fuel prices, the most important component of power plants’ marginal costs. The literature to date has shown that restructuring can have important, and often ambiguous, environmental implications by changing plant (e.g., fuel efficiency and procurement practices, utilization) and market (e.g., increase in trade across regions and associated shifts in production across plants) operations; see, for example, Davis and Wolfram (2012) and Mansur (2007). Restructuring can also have environmental implications through its effect on firms’ responses to environmental regulations; see Cicala (2015) and Fowlie (2010), among others.====In this paper, we study the environmental implications of restructuring by comparing the responses to fuel prices of power plants in markets that were restructured and those that were not. Our primary interest lies in the response of coal and natural gas consumption to the dramatic exogenous drop in the price of gas in the aftermath of the Shale Boom. On a per unit-of-energy basis ($/MMBtu), the average price of gas was nearly seven times the average price of coal in the beginning of 2006. By the end of 2012, this ratio had decreased to less than two.====We focus on the consumption of coal and gas, as opposed to other fuels, because coal- and gas-fired power plants have accounted for around two thirds of electricity generation in the country since 1990. Although other factors, such as environmental policy, have contributed to the decline in coal consumption in recent years, the coal-to-gas switching post 2008 has been attributed to a large extent to abundant cheap gas because of the Shale Boom (Coglianese et al., 2017). The environmental implications of coal displacement have also been material and well-documented. Although the use of both coal and gas results in carbon dioxide (CO====) and other harmful emissions, the coal-to-gas switching decreases emissions because the CO==== content of gas is 60% less than that of coal per unit of heat input.====However, the changes in fuel-consumption patterns and the resulting emissions need not be homogeneous across markets with different structures. Following the restructuring wave, wholesale electricity markets emerged in a large part of the country highlighted by the formation of independent system operators (ISOs) coordinating the function of some type of a “power pool” for buying and selling electricity. In most cases, restructuring emphasized the unbundling of generation from transmission and distribution, which gave rise to merchant generators, also known as independent power producers. Merchant generators focused exclusively on the production of electricity. The utilities continued to be responsible for the transmission and distribution of electricity to retail customers in their franchise areas, often engaging in generation. The parts of the country that did not opt for the creation of a wholesale market maintained the traditional structure of vertically integrated utilities. Utilities and merchant generators, which are the focus of the paper, accounted for more than 90% of all electricity generated in the country during 2003–2012, the period relevant for our analysis. The same entities also accounted for almost the entirety of coal consumption and more than 80% of gas consumption for electricity generation (EIA, 2015).====Both utilities and merchant generators operate power plants in which they turn heat produced by coal and gas, into electricity. Fuel prices are the largest component of variable costs, about 75%, and electricity generation is best described as a Leontief production function. Hence, non-fuel inputs, such as labor, are not fuel substitutes and fuel consumption may be studied without the concern for any substitution effects with respect to other inputs (Bushnell and Wolfram, 2005). We estimate plant-level own- and cross-price elasticities of fuel consumption and their implications for carbon dioxide emissions.====Our empirical analysis allows for the response of coal and gas consumption to their prices to be non-linear using flexible model specifications (splines) similar to the ones in Cullen and Mansur (2017). Allowing for flexibility in responses is crucial because coal and gas become closer substitutes when the price of coal is high and the price of gas is low due to the technology used in electricity generation. Power plants differ in their ability to transform heat input from coal and gas combustion to electricity. The amount of heat input from coal needed to generate electricity is close to 1.5 times that from gas. Therefore, if the price of gas is sufficiently low, it may be economical for a power plant to switch from coal to gas.====Although the traditional and restructured electricity markets may lead to responses to fuel prices of different magnitude, it is not clear whether the responses in the former should exceed the latter, or vice versa. We show that power plants in traditional markets respond more to changes in the prices of coal and gas consistently across a series of regression specifications and robustness checks when using absolute and relative levels of fuel consumption as our dependent variables. We measure the latter using the share of gas in total fuel consumption. While the fuel consumption regressions allow us to use data from all coal- and gas-fired power plants, the share regressions limit the data to dual-fuel plants that use both coal and gas to generate electricity.====In the case of the fuel-consumption regressions, we find that the elasticity of coal consumption with respect the price of gas is 0.28 for plants in traditional markets and 0.14 for plants in restructured markets, when the price of gas is $5/MMBtu, that is, equal to its average in the post-fracking period January 2009 to December 2012. The difference in responses is statistically and practically significant. The elasticity of gas consumption with respect to the price of gas is −0.41 for plants in traditional markets and −0.02 for plants in restructured markets at the same gas price point. Once again, the difference in responses is statistically and practically significant. The differences in the gas consumption responses are robust and significant across all specifications. While differences in coal consumption responses to the price of gas persist across specifications, the gas price effects in restructured markets fall inside the 95% confidence intervals of their analogs in traditional markets when we control for the environmental regulations that plants are subject to.====In the case of the gas share regressions, which are based on a smaller set of plants compared to the consumption regressions, we offer two micro—within-plant—approaches in modeling coal displacement. In the first, we hold the plant’s electricity generation fixed and in the second, we allow generation to change in response to fuel prices. Holding a dual-fuel plant’s generation fixed, a $1 increase in the price of gas leads up to 3 percentage points (ppt) decrease, on average, in the share of gas for plants in traditional markets when the price of gas is between $3 and $6; these gas prices ensued in the post-fracking period. The same price increase leads to a 1 ppt decrease, on average, in the share of gas for plants in restructured markets. Both are practically significant effects given that the average share of gas for dual-fuel plants is around 9% in traditional markets and 6% in restructured markets. Allowing a dual-fuel plant’s generation to change, the price effect on the share of gas is 3 ppt for traditional markets and 0.7 ppt for restructured markets when the price of gas is $5.====We next consider several factors that may explain this difference in responses to fuel prices between traditional and restructured markets noting that such responses are due to marginal power plants that move to the left and right of the equilibrium point between the electricity supply and demand curves as fuel prices change. In particular, we examine the role of the following five factors: investment in capacity, plant efficiency, fuel procurement practices, wholesale purchases and retail sales of electricity and, finally, the role of market power.====We show that the differences in gas-fired capacity between states that restructured their wholesale electricity markets and those that did not are consistent with our findings. All else equal, market participants respond more to fuel price signals as long as they face no capacity constraints. One way to relax capacity constraints is through investment, which is usually on technology that is no worse than that in the older vintage of plants. Using a difference-in-difference approach to compare gas-fired capacity in both markets before and after restructuring, we find lower capacity in the restructured markets post-restructuring. We suggest that these differences in capacity explain the fact that power plants in restructured markets are less responsive to fuel prices.====We then suggest that a second explanation for our results is due to differences in the efficiency of gas-fired plants in turning heat from fuel to electricity (heat rate) between traditional and restructured markets. In particular, we use the heat rate as our metric to show that gas-fired generation is a closer competitor to coal-fired generation in traditional markets than in restructured markets.====On the other hand, differences in fuel procurement practices, wholesale purchases and retail sales of electricity seem unlikely to explain differences in responses. The last possible explanation we consider is market power, which is plausible and worth pursuing but entails analysis that would be hard to accommodate within this paper.====Finally, we assess the implications of the difference in responses to fuel prices for plant-level carbon dioxide emissions using two back-of-the-envelope calculations. Based on the consumption regressions and a gas price of $5, we find that, following a $1 decrease in the price of gas, plant-level coal-related emissions go down by 37 (31) million lbs. in traditional (restructured) markets and the plant-level gas-related emissions go up by 5 (1) million lbs. in traditional (restructured) markets. Based on the share regressions, and again at a benchmark gas price of $5, we see a reduction in the emissions rate for dual-fuel plants in traditional markets of 3 lbs./MMBtu and 0.6 lbs./MMBtu for their counterparts in the restructured markets.====Overall, the paper contributes to a recent literature on the effects of the Shale Boom on electricity markets. Recent related work – albeit with a different focus and abstracting from the role of restructuring – on the effects of gas prices in electricity markets following the Shale Boom includes Cullen and Mansur (2017), Holladay and LaRiviere (2017), Fell and Kaffine (2018), and Linn and Muehlenbachs (2018). These papers exploit to a large extent the exogenous drop in gas prices due to the Shale Boom using data at different levels of cross-section and time aggregation and often highlight the role of regional heterogeneity in responses due to differences in the existing stock of electricity generators. Cullen and Mansur show that carbon prices are much more effective in reducing CO==== emissions when gas prices are low. Holladay and LaRiviere focus on the impact of cheap gas on the benefits of energy policy aiming to promote renewables and find that the reductions in CO==== emissions due to an increase in renewables can be smaller when gas prices are low. Fell and Kaffine find that low gas prices and increased wind generation both explain the observed reductions in coal-fired generation and CO==== emissions. Moreover, the marginal response to fuel prices is larger at higher levels of wind generation. Linn and Muehlenbachs investigate the effects of gas prices on emissions (CO====, nitrogen oxides, and sulfur dioxide) and electricity prices showing that the regions with larger emissions reduction experience smaller electricity price declines.====The remainder of the paper is organized as follows. We provide a background on gas and coal production, electricity generation technologies, and wholesale electricity markets in Section 2. Section 3 contains a discussion of our data, our baseline results, and robustness checks regarding the heterogeneity in the response of fuel consumption to prices. Section 4 discusses explanations for our findings. Section 5 presents the implications of the difference in responses to fuel prices for carbon dioxide emissions using two simple back-of-the-envelope exercises. We finally conclude. Tables and figures are attached at the end of the paper. An on-line Appendix with information regarding the data, summary statistics, and some robustness checks, is also provided.",Environmental implications of market structure: Shale gas and electricity markets,https://www.sciencedirect.com/science/article/pii/S0167718719300013,March 2019,2019,Research Article,258.0
Sato Susumu,"Graduate School of Economics, The University of Tokyo 7-3-1, Hongo, Bunkyo-ku, Tokyo 113-0033 Japan","Received 1 October 2017, Revised 15 November 2018, Accepted 30 December 2018, Available online 6 January 2019, Version of Record 17 January 2019.",https://doi.org/10.1016/j.ijindorg.2018.12.006,Cited by (11)," services: ad-supported basic service and ad-free premium service. In addition, if the willingness to pay of advertisers is sufficiently high, the basic service is offered for free. This menu pricing is well known as "," is a business model which is coined as a combination of the words ==== and ====. This word describes “a business model in which you give a core product away for free to a large group of users and sell premium products to a smaller fraction of this user base.====” The purpose of this paper is to show that this business model is optimal menu pricing for advertising platforms under certain conditions.====There are many instances of freemium in digital economy. As in Table 1, fair amount of major music- and video-streaming services adopt freemium business models. A prominent example of freemium business is Spotify, a music-streaming service with the largest market share in the world. Spotify offers two services, Free and Premium. In Free service, customers can shuffle several given playlists, with advertising audios interrupting in the time between songs. Customers who pay a monthly fee of $9.99 to subscribe Premium service can play any songs with better sound quality, create their original playlists, download musics, and listen offline, without being interrupted by advertisements. As another example of freemium, YouTube, a well-known ad-supported video-streaming platform, recently started to offer a paid and ad-free membership service, called YouTube Red. YouTube Red also has several additional functionalities such as saving videos on mobile devices or viewing original contents. Users of YouTube who want to avoid advertisements or get richer functionalities can upgrade their accounts to YouTube Red.====This type of business models can be seen as a class of second-degree price discrimination since the firm offers a menu of services (free and premium) and lets customers choose between them. A distinctive feature of this business model is that it uses the amount of advertisements as an instrument to screen customers.==== Customers can choose ad-supported free service or ad-free premium service according to their nuisance from advertisements, and advertisers can show their advertisements only to free customers. Put differently, this is a price discrimination by a two-sided platform using the levels of interactions between agents on both sides as an instrument to price-discriminate.==== This form of price discrimination is relatively new and thus have been subject to few research until recently.====There is a tradeoff when the platform uses the levels of interactions as instruments to price-discriminate. Consumers who want to enjoy contents without being annoyed by advertisements are willing to pay more to reduce the amount of advertisements. Thus, the platform can collect revenues from consumers by introducing a service with a fewer amount of advertisements and charging a higher fee. However, while offering a service with fewer advertisement may successfully collect revenues from consumers, this reduces the revenue from advertisers as the total view of advertisements shrinks. Thus, the platform needs to take this tradeoff into account when they decide how to price-discriminate.====Treating freemium as a price discrimination by two-sided platforms, several questions arise; What is the optimal price discrimination for platforms? Is the freemium optimal price discrimination? Since platforms can potentially consider any nonlinear advertisement-price path to maximize its profit, and freemium is just one special class of such a price discrimination, it is natural to think that there would be better ways to collect revenues for the platforms. On the flip side of the coin, for freemium to be optimal, this must be superior to any other candidate menus platforms can design.====To answer these questions, I construct a model of menu pricing problem of advertising platforms in two-sided markets where consumers are annoyed by advertisements and advertisers benefit from listing advertisements. The platform potentially can offer any menu of services which specify the pairs of amount of advertisements and fixed fees.====My main result (Proposition 1) shows that ==== under certain conditions. More precisely, I show that, under the linear specification, which is commonly adopted in the literature of advertising platforms, the monopolistic platform optimally offers only ==== services: basic service with full advertisements and premium service with no advertisements. This menu pricing segments consumers into two groups: those who contribute to the platform’s revenue by paying premium fees and those who contribute by viewing advertisements, leaving no intermediate segment of consumers. In fact, this simple segmentation is optimal when consumer nuisance from advertisements is linear, and the platform successfully collect revenues from both consumers and advertisers. Furthermore, if the advertisers’ benefit from transaction is sufficiently high relative to the intrinsic value of the platform’s service, then the basic service becomes free (Proposition 2). In this case, the optimal menu is literally freemium.====Then, I analyze several properties of optimal menu pricing. First, I examine welfare properties of binary menu pricing. I show that profit-maximizing price for advertisers is too high and the amount of consumers who view advertisements is too small in terms of social welfare (Proposition 3). As a result, the size of advertising network is too small relative to the social optimum. Next, I compare the binary menu with another business model called single-menu business model. I find that, the platform provides more advertisers and less consumers who view advertisements under the binary menu pricing than under single-menu model (Proposition 4). This difference stems from the difference in the appropriability of surplus from consumers who avoids advertisements. Under single-menu business model, these consumers do not participate in the platform, and the platform collects no revenues. By contrast, a platform which adopts binary menu can collect revenues from these consumers by providing ad-free services. This generates the incentive toward reducing (increasing) the amount of consumers who view (do not view) advertisements. This in turn reduces the average nuisance of consumers who view advertisements, and thus platform increases the amount of advertisers. I discuss how these differences lead to the welfare ranking of these business models (Proposition 5).====I also examine whether the binary structure remains to be valid under different situations. The property that platforms offer only two services remains to be valid under a duopoly situation (Proposition 6). In addition, if the advertisers’ benefit from transaction is sufficiently high relative to the degree of product differentiation (or transportation cost) of the platform’s service, then the basic services become free. In this sense, my main result that freemium is an equilibrium, is robust to the existence of competition.====In summary, the results in my paper provide economic foundations for the prevalence of freemium business models; once we accept the linear environment, offering only two free and premium services is actually the best strategy for platforms among a number of alternatives.====The rest of the paper proceeds as follows. In the next section, I review the related literature. Section 2 presents a model of menu pricing by a monopoly platform and main results. Section 3 presents a doupoly extension, and Section 4 concludes.",Freemium as optimal menu pricing,https://www.sciencedirect.com/science/article/pii/S0167718719300037,March 2019,2019,Research Article,259.0
"Ganuza Juan-José,Penalva Jose","Department of Economics, Universitat Pompeu Fabra and Barcelona GSE, Ramon Trias Fargas 27, Barcelona 08005, Spain,Department of Business, Universidad Carlos III, Madrid 126, Getafe 28903, Spain","Received 6 February 2018, Revised 7 November 2018, Accepted 8 November 2018, Available online 28 December 2018, Version of Record 16 January 2019.",https://doi.org/10.1016/j.ijindorg.2018.11.004,Cited by (2),"This paper analyzes a situation in which the seller controls the accuracy of what potential buyers learn about their valuation of a good to be sold. This setting is related to many real situations such as home sales, antique auctions, and digital platforms such as Google and Facebook selling online advertising slots. Two important questions arise: what is the optimal selling mechanism, and what is the optimal disclosure policy of the seller. Under the assumption of private values, a simple auction with a reserve price is the optimal mechanism. What we show is that the amount of (costly) information provided increases with the number of potential bidders when using the optimal mechanism and is greater than when the object is always sold. Because information changes the distribution of a bidder’s expected valuations, the optimal reserve price also changes, so that the number of bidders (indirectly) affects the reserve price. We show that as the number of bidders increases, the optimal reserve price becomes more restrictive.","We analyze a situation in which the seller of an object controls the accuracy with which ==== risk-neutral bidders learn their private valuations.==== The object is sold using an optimal mechanism, a standard auction with reserve price in this setting. A well-known result in auction theory is that the optimal reserve price depends on the distribution of bidder valuations but not on the number of bidders. In our framework, where the auctioneer chooses how much information to disclose, competition between bidders (captured by the number of bidders, ====) affects the incentives of the auctioneer to disclose information, which in turn affects the distribution of bidder valuations, and hence, the reserve price. What we find is that a greater number of bidders increases the optimal amount of information disclosed, leading to a more restrictive reserve price.====Many situations exist in which the auctioneer can affect bidder information to some extent. Take the sale of goods on the Internet through auctions (e.g., eBay), for example. In most of these auctions, sellers have most of the information about the goods for sale and decide how much information to reveal through electronic images and text descriptions. Similarly, when selling a house, real estate agents control the information disclosed to potential buyers, and when selling antiques at auction, sellers also manage how much detail they provide. Governments soliciting bids for a public project or a company selling a subsidiary have plenty of information about the goods and control how much will reach bidders.====In online advertising, platforms such as Google and Yahoo! sell advertising slots on websites. Each of these advertising slots is sold to potential advertisers, usually via an automatic auction-type mechanism.==== The platform selling the advertising slots has access to a great deal of information from previous interactions about the person viewing the page (obtained directly from the person or extracted from correlating information on observed browsing patterns). However, the potential advertisers are the ones who know the use and benefit of this information (private values). In this interaction, the platform must determine how much information to make accessible to potential buyers of the advertising slot and how to organize the selling mechanism.====The current paper looks at the interaction between access to information the seller gives to potential buyers and the selling mechanism, in particular the reserve price used in the auction. The reserve price can be significant for seller revenue, as shown in Ostrovsky and Schwarz (2016). Their paper analyzes a large-scale field experiment on reserve prices in “sponsored search” auctions conducted by Yahoo! to sell advertisements. In particular, the authors show that when reserve prices are set according to the theory, revenues increase substantially relative to a control group with fixed and suboptimal reserve prices.====The current paper contributes to two branches of the literature. Jullien and Mariotti (2006) and Cai et al. (2007) show that in an affiliated value setting, the auctioneer uses the reserve price to signal the valuation. In that setting, the number of bidders affects the seller’s incentive to signal through the reserve price. In our private value setting, the seller’s valuation is not relevant for bidder decision problems, and the reserve price has no informational content. We also contribute to the literature that analyzes the auctioneer’s incentives to disclose information in private value settings where the assumption is that the object is always sold (see, for example, Board, 2009, Ganuza, 2004, Ganuza, Penalva, 2010; and Hummel and McAfee, 2015====).==== We show that the auctioneer provides more information when using an optimal mechanism, an auction with reserve price. Providing information to bidders has the positive effect of increasing the efficiency of the allocation (and bidders’ willingness to pay). It also, however, has a negative effect, increasing bidders’ informational rents. As the reserve price reduces bidder informational rents, the auctioneer’s incentive to disclose more information increases.==== As the reserve price reduces bidders’ informational rents, it increases the incentives of the auctioneer to disclose more information.====This result sheds light on the targeting problem in the online advertising industry. Platforms that provide highly accurate information about consumers may end up with little competition in the auction. The result would be large market power for firms whose products are a good match with the preferences of those particular consumers. For this reason, platforms may prefer to increase competition by being less precise about consumer characteristics. We show that applying the optimal mechanism alleviates the trade-off between improving the consumer-advertiser match (and advertiser willingness to pay) and competition among advertisers. Thus, the platform is willing to provide a more accurate description of consumer preferences. In addition, we show that for consumers looking for products in niche markets, where less competition for their attention is expected, the optimal choice is to provide less information and use a less restrictive reserve price.====This paper is structured as follows: In the next section, we present the model and known results in the setting where reserve prices are not used. Section 3 solves the model when the seller can set the optimal reserve price, and describes the main results. Section 4 considers alternative signal structures, and Section 5 is the conclusion.",Information disclosure in optimal auctions,https://www.sciencedirect.com/science/article/pii/S0167718718301073,March 2019,2019,Research Article,260.0
Burgdorf Jacob,"Department of Economics, College of Business, Suite 144, University of Louisville, Louisville, KY, 40292, USA","Received 22 December 2017, Revised 30 November 2018, Accepted 2 December 2018, Available online 13 December 2018, Version of Record 3 January 2019.",https://doi.org/10.1016/j.ijindorg.2018.12.001,Cited by (9),I examine the competitive effects of mandated exclusive territories in the US beer ,"The voluntary use of and mandates requiring the use of exclusive territories are common across many industries.==== Theories conflict on the impact of vertical restraints as to whether they increase efficiency along the supply chain or if they are used as means to act anti-competitively. Theoretically, the voluntary use of exclusive territories could have anti-competitive effects, such as reducing competition in prices (Rey and Stiglitz, 1995) or preventing entry (Asker and Bar-Isaac, 2014). Alternatively, exclusive territories may be used to alleviate agency problems, promoting downstream investments in product quality, thus aligning upstream and downstream firms’ incentives (Klein, Murphy, 1988, Mathewson, Winter, 1994, Zanarone, 2009). The impact of a mandate is also theoretically ambiguous—if it prevents large firms from acting anti-competitively, it may foster competition, but it could also raise the cost of inducing downstream product investment, and result in harm to consumers and firms. Further, if this effect is asymmetric across market segments, it could reduce the incentive to compete for the less affected segment and weaken competition between firms.====Since theory gives us ambiguous predictions, empirical work is key (a point made in Cooper et al. (2005)). This paper exploits the implementation in 2006 of a Wisconsin law which required all brewers to designate an exclusive wholesale territory for beer wholesalers. I examine the impact this policy had on prices, quantities, and brands sold in grocery stores.==== Using the timing of the implementation of the law as an identifying strategy, I estimate the impact of the law by comparing Wisconsin to a variety of control groups.====Since the impact of this policy may have different effects on different niches of the market, depending on the type of firm and size, among other factors, I allow the impact to differ across categories of breweries belonging to large breweries with national distribution, other domestic breweries (non-craft), import breweries, and craft breweries. Using a differences-in-differences design I find that mandating exclusive territories increased the price of craft beer, decreased quantities of craft beer, and decreased the number of brands sold overall with the largest reduction coming from craft brands. These findings are consistent across several control groups, robustness checks, and a data driven synthetic control method. The findings of this paper suggest that mandated vertical restraints increase the cost of distribution, and do so especially for craft brewers, thus lending support to inefficient effects of the laws.====These results extend previous findings relating to exclusive territories and the nature of vertical practices. Related to this work, Sass and Saurman (1993) and Rojas (2012) find positive welfare effects from mandated exclusive territories in the brewing industry. These studies span time periods when antitrust treatment of vertical restraints was more restrictive. They argue that their findings are driven by the mandate giving brewers the ability to voluntarily use exclusive territories without attracting antitrust litigation and are thus “indirect tests of the effects of exclusive territories themselves,” and not of the mandate.==== Finding different results from mandated exclusive territories in a more recent time period seems plausible, since the use of exclusive territories is less likely to attract attention from antitrust authorities (see Federal Trade Commission (2005)). In this legal environment, the mandate may remove the threat an upstream brewer could employ of terminating exclusive territories if shirking occurs, thus removing an implicit contract enforcement mechanism. In this case, the law gives protections to downstream firms in markets where downstream investment is important and increases the cost of inducing investment.====Lastly, the brewing industry has changed drastically since the time period of previous studies; in particular, a large number of smaller craft breweries now make up a significant portion of the market. This study is able to estimate different effects by types of brewers and examine impacts of mandates on segments of the market previously unstudied. The bargaining power of small brewers with wholesalers is likely to be less than large, national breweries and thus they may be impacted by the law differently. A small empirical literature across several industries tends to find beneficial welfare impacts of the voluntary use of vertical restraints and negative impacts when they are mandated (see Cooper et al. (2005), Lafontaine and Slade (2008), and Lafontaine and Slade (2014), for relatively recent surveys of the literature). The findings in this study add to this empirical literature.====The rest of the paper proceeds as follows. The next section will discuss the beer industry and the use and mandates of exclusive territories in the industry. Section 3 describes the data used in this study, Section 4 details the empirical research design, Section 5 discusses the results, and Section 6 presents robustness checks on the main results and the synthetic control method. Section 7 concludes and discusses the policy implications of this paper.",Impact of mandated exclusive territories in the US brewing industry: Evidence from scanner level data,https://www.sciencedirect.com/science/article/pii/S0167718718301206,March 2019,2019,Research Article,261.0
"Calcagno Claudio A.,Giardino-Karlinger Liliane","Analysis Group Ltd., London, UK,European Commission, DG COMP, Madou 17/13, Saint-Josse-ten-Node B-1210, Belgium","Received 14 January 2017, Revised 9 November 2018, Accepted 2 December 2018, Available online 11 December 2018, Version of Record 2 January 2019.",https://doi.org/10.1016/j.ijindorg.2018.12.002,Cited by (2),"This paper studies collusion among vertically integrated incumbents who may either delegate output production to a more efficient downstream entrant (“accommodating regime”) or refuse to supply the entrant and produce the final good themselves (“exclusionary regime”). Accommodating agreements yield higher collusive profits, but suffer from contractual frictions: An incumbent may first offer the entrant a high wholesale price for the input, and then undercut the entrant on the final good market, so that the entrant cannot recover its high input costs downstream. When the efficiency gap between the incumbents and the entrant is small, this hold-up effect dominates over the efficiency effect. Depending on modeling choices, exclusionary collusion is then either more profitable than accommodation, or is the only sustainable collusive regime.","Most of the economic literature on exclusion studies this exclusion as a unilateral practice: Typically, it is a monopolist (or a single firm endowed with sufficient market power) which engages in certain business practices to keep actual or potential competitors out of the market. The possibility of exclusionary abuses by a group of firms, rather than a single firm, has received little to no attention.==== Legal scholars instead have long recognized the concept of collective exclusion, and a series of antitrust cases (which we refer to in the next section) have investigated possible abuses by oligopolists, rather than monopolists.====Our paper studies a duopoly which may either exclude or accommodate a more efficient entrant (or rival which has already sunk its entry cost) which competes with the incumbents on the final good market. The incumbents are vertically integrated, meaning that they can produce the necessary input for the production of the final good themselves. The entrant instead cannot integrate backward, i.e. it has to rely on the two incumbents (the competitors on the downstream market) for the supply of the key input. Upstream entry is thus ruled out completely, and downstream entry is limited to one entrant per period.====We study an infinitely repeated two-stage game where wholesale and retail markets open sequentially: On the wholesale market, the two incumbents make independent offers to supply the input to the entrant; on the downstream market, all active firms compete in prices for consumers of the final good. Inputs and outputs of all firms are perfect substitutes. The entrant has a lower cost of transforming the input into the final good than the incumbents, so that productive efficiency requires that all of the final good be produced by the entrant.====We first show that, if this industry operates competitively at both stages of the one-shot game, exclusion can never arise: At all subgame perfect equilibria of the one-shot game, the Bertrand outcome prevails both on the wholesale and on the retail market, and the entrant will be the only firm active on the downstream market. Production is fully efficient, and the entrant earns the efficiency rent, while the upstream firms make zero profits.====We then allow for infinite repetitions of the two-stage game, to study the scope for equilibria in which the two incumbents coordinate their behavior both on the wholesale and on the retail market to raise their profits above the competitive level of zero. Such collusive equilibria may either be exclusionary (i.e. the incumbents refuse to supply the entrant, and serve the downstream market themselves, coordinating retail prices accordingly), or accommodating; in the latter case, the incumbents raise the wholesale price offered to the entrant above the competitive level, and leave the final good market exclusively to the entrant, extracting all the entrant’s rents through the wholesale contracts.====While the accommodating regime is clearly more efficient, it raises an opportunism problem that the incumbents need to solve before they can put such an agreement in place: Suppose that after the entrant accepted the collusive wholesale contracts, it finds out that, contrary to what was promised, one of the incumbents decided to be active on the downstream market as well, undercutting the entrant and stealing all its retail sales. This kind of deviation is specific to our setup comprising both an upstream and a downstream market, and cannot arise in standard duopoly collusion models where only one market - typically a downstream market - is considered. Of course the entrant will be wary of this kind of “supplier hold-up” when contracting with the incumbents, and will only accept upstream contracts which do not expose it to such hold-up.====In other words, opportunism similar in spirit to the one analyzed for the case of upstream monopoly by Hart and Tirole (1990), O’Brien and Shaffer (1992), and McAfee and Schwartz (1994), may undermine joint-profit maximization in a collusive agreement where incumbents accommodate the more efficient entrant. If the incumbents cannot write contingent wholesale contracts that directly avoid the hold-up problem (by allowing the input price paid by the entrant to depend on the entrant’s downstream sales), they are left with two alternatives: (1) modify the collusive agreement (i.e. reduce wholesale and final prices) so as to eliminate the incentives for hold-up: This will allow for efficient production, but will reduce the payoffs for each incumbent below the industry profit maximum; or instead (2) exclude the buyer and retail the good directly, thus compromising on productive efficiency, but facilitating full collusion on retail prices.====The choice between accommodating and exclusionary collusion thus involves a trade-off between lower transformation costs and higher collusive prices. We are particularly interested in the question of whether the contractual frictions may be strong enough to actually outweigh the benefits of accommodating the entrant, thus pushing the incumbents to exclude the entrant and to sacrifice all efficiency rents.====Our results show that in our model, exclusionary and accommodating equilibria coexist, and for a certain range of parameter values (namely when the efficiency gap between incumbents and entrant is sufficiently small), exclusion yields higher payoffs than accommodation, so that if the incumbents can coordinate on which equilibrium to play, they would prefer to exclude. This provides a novel response to the Chicago School argument that it is in the interest of a vertically integrated incumbent to allow a more efficient downstream rival to operate (and then extract its efficiency rent). Our paper is therefore relevant for the antitrust assessment of collective exclusion, and provides a rationale for why such inefficient exclusion may arise.====The paper proceeds as follows: In Section 2, we give an overview of the relevant literature, and briefly discuss various antitrust cases on collective exclusion where our model setup may potentially be relevant. Section 3 introduces the benchmark model and derives its static equilibrium. Section 4 studies two classes of dynamic equilibria, namely exclusionary equilibria where the entrant is not supplied, and accommodating equilibria where the entrant is active downstream. It states our main results in terms of sustainability and profitability of both classes of equilibria. Section 5 examines the robustness of our results to the introduction of Cournot competition with private contracts. Section 6 discusses contractual remedies to the hold-up problem and their implications for consumer and total welfare, and briefly outlines possible extensions of the baseline model. Section 7 concludes.",Collective exclusion,https://www.sciencedirect.com/science/article/pii/S0167718717300449,March 2019,2019,Research Article,262.0
Jäkel Ina C.,"Department of Economics and Business Economics, Aarhus University, Fuglesangs Allé 4, Aarhus V 8210, Denmark","Received 29 November 2016, Revised 2 November 2018, Accepted 6 November 2018, Available online 5 December 2018, Version of Record 11 January 2019.",https://doi.org/10.1016/j.ijindorg.2018.11.002,Cited by (8),This paper studies the relationship between domestic market performance and export performance. We employ data on production and trade in the Danish chocolate and confectionery ,"There is ample evidence that firm-specific attributes that are universal across markets (such as firm productivity) are strongly related to a firm’s success, including survival, growth and export participation.==== Often, however, firms are active on different markets, with varying market conditions: most notably, for an exporting firm, conditions in one destination market may be quite distinct from those on another destination market, or those on the domestic market. Indeed, even for a given firm, the variation in export sales across destinations is tremendous (Eaton et al., 2011). Interactions between characteristics of individual firms and those of the market have received only little attention, even though such market-specific idiosyncratic factors are essential for studying the intensive margin of exporting.====In this paper, we employ data for the Danish chocolate and confectionery industry and a novel measure of differences in tastes to study the cross-country variation in export sales. We argue that demand (and, thus, market performance) is imperfectly correlated across countries due to differences in consumer tastes. Empirically, we show that a product’s appeal on the domestic market is a much poorer predictor of export performance in destinations where tastes differ substantially from domestic tastes.====The data set is unusually rich and provides information on production and trade at the firm-product level, which allows us to infer domestic sales and prices. In a first step, we employ these data to estimate domestic product appeal. Our approach builds on the discrete choice demand framework as put forward in Berry (1994). Since we do not have variety-specific information on product ingredients, we follow Khandelwal (2010) and define product appeal as the sum of all factors (such as product quality, brand recognition, and consumer awareness) which, conditional on price, generate higher market shares. Thus, product appeal measures non-price determinants of domestic market performance. Our estimates reveal that a higher domestic product appeal tends to be associated with higher output prices, higher input prices, a greater likelihood of being exported, and a wider set of penetrated markets.==== However, export performance is not monotonically increasing in domestic product appeal: for example, many varieties with a high domestic product appeal are only sold on the domestic market.====In a second step, we test the hypothesis that domestic product appeal is a poorer predictor of export performance in destinations where consumers have very different tastes from those of domestic consumers. To this aim, we construct a novel measure of differences in tastes across countries, which is based on information on the ==== ingredients (such as cocoa, sweeteners etc.) of chocolate and confectionery sold in different countries. The data is obtained from Euromonitor’s passport database. Differences in average ingredients across countries should to a large extent reflect differences in consumer tastes: for example products will on average have a higher sugar content in countries where consumers prefer a sweet product. While our novel measure of differences in tastes is correlated with proxy variables commonly used in the literature (e.g., linguistic proximity), it also reveals some of the limits of using such proxy variables.====We offer two different estimation strategies to uncover the relation between domestic appeal and export performance across destinations. First, we implement a reduced-form approach, relating destination-specific exports to a variety’s domestic product appeal and its interaction with cross-country differences in tastes. Following Eaton and Kortum (2001) and Crozet et al. (2012), we employ a Tobit estimator with destination-specific censoring point. This estimator solves the selection issue that we only observe exports for the subset of exporting firms. Results reveal that the relation between domestic product appeal and export sales is strong in destinations where tastes of domestic and foreign consumers accord well. In contrast, domestic product appeal has significantly less predictive power for exports to countries where tastes differ substantially from domestic tastes. We also find that the bias from selection into exporting is substantial: based on the subset of observations with positive export sales, we would predict a ==== effect of domestic product appeal on export sales in a considerable number of destinations. The effect turns insignificant or positive across the entire set of destinations once we correct for selection into exporting.====In our second estimation strategy, we model the foreign demand for Danish chocolate and confectionery more formally, and apply the random coefficients logit model of demand in Berry et al. (1995) to our trade data. The Berry et al. (1995) framework takes into account heterogeneity in consumer tastes within a given country, and differences in substitutability across varieties. For example, we allow two high-appeal varieties to be closer substitutes than a high-appeal and a low-appeal variety.==== Moreover, this analysis also explicitly accounts for pricing differences across firms and markets, and thus differences in market power, strategic interactions, etc. We find that the variation in tastes for product appeal across consumers within a given country is at least as large as the variation across countries in average tastes, but that the latter remains substantial even after controlling for the former.====A growing number of studies documents heterogeneity in demand across countries. Eaton et al. (2011) introduce country-specific demand shocks into a model of heterogeneous firms. Estimating the model based on French firm-level export data, they find an enormous idiosyncratic variation in demand across export markets: conditional on entry, firm-destination-specific factors explain much more than 50% of the variation in sales across firms in any given market; see also Crino and Epifani (2012), Nguyen (2012) and Munch and Nguyen (2014). However, these studies remain largely silent on the origin of such demand idiosyncrasies across countries, which is the focus of our work. In order to rationalize the observed variation in demand across countries, Di Comite et al. (2014) propose a quadratic preference model with country-specific taste parameters, where exported varieties that match local tastes better succeed in capturing larger market shares. Our empirical analysis builds on the intuition of their theoretical framework. Empirically, we confirm that cross-country differences in tastes do indeed have an effect on the within-firm variation in sales across export markets.====Our empirical approach builds on recent applications of demand estimation which infer product appeal as the residual from a regression of market shares on prices.==== Foster et al. (2008) and Katayama et al. (2009) apply this approach to the analysis of firm productivity. In the trade literature, Khandelwal (2010) and Hallak and Schott (2011) obtain country-specific estimates of product appeal (interpreted as ‘product quality’) from aggregate trade data. Gervais (2014) infers ‘product quality’ at the plant-level and confirms that it is important in explaining export outcomes. Roberts et al. (2018) study firm-level heterogeneity in product appeal for a sample of Chinese footwear exporters. They find that the heterogeneity across firms in product appeal is more important in explaining differences in export sales than the heterogeneity in marginal costs. We extend this line of work by considering the within-firm variation in sales across export markets.====Our work is also related to empirical studies of the gravity equation which highlight the importance of bilateral affinity for aggregate trade patterns; see, e.g., Felbermayr and Toubal (2010), Disdier et al. (2010) and Melitz and Toubal (2014). We show that bilateral affinity, measured in our framework as the similarity of consumer tastes, has a higher effect on exports for varieties with a high domestic product appeal, and hence affects trade patterns not only across country-pairs but also within country-pairs across varieties. Finally, several authors have provided empirical evidence that high-income countries demand higher quality; see, e.g., Hallak (2006), Verhoogen (2008) and Flach (2016). We also find that high-income countries have a higher demand for varieties of high (domestic) appeal, but that this effect is entirely driven by the positive correlation of income per capita with the similarity in consumer tastes.====The next section discusses the choice of industry, introduces the data for Danish chocolate and confectionery producers, and presents our novel measure of differences in tastes across countries. Section 3 lays out the empirical strategy for estimating domestic product appeal and presents demand estimates. Section 4 discusses reduced-form estimates on the link between domestic product appeal and export sales across destinations, as well as extensions to the benchmark model. In Section 5, we allow for consumer heterogeneity on the export market by estimating the foreign demand for domestic product appeal in a Berry et al. (1995) framework. Section 6 concludes the paper.","Product appeal, differences in tastes, and export performance: Evidence for Danish chocolate and confectionery",https://www.sciencedirect.com/science/article/pii/S0167718716303939,March 2019,2019,Research Article,263.0
"Genc Talat S.,Reynolds Stanley S.","Department of Economics and Finance, University of Guelph, Guelph, ON, N1G2W1, Canada,Department of Economics, Eller College of Management, University of Arizona, Tucson, AZ 85721, USA","Received 4 July 2017, Revised 14 September 2018, Accepted 23 October 2018, Available online 23 November 2018, Version of Record 11 December 2018.",https://doi.org/10.1016/j.ijindorg.2018.10.007,Cited by (16),"We investigate the market implications of ownership of a new low-cost production technology. We relate our theoretical findings to measure the impact of renewable energy penetration into electricity markets and examine how the ownership of renewable capacity changes market outcomes (prices, outputs, emissions). As current public policies influence renewable energy ownership, this research provides useful insights for policy makers. We show how and why ownership of renewable capacity matters when there is market power in energy market. We apply our findings to the wholesale electricity market in Ontario, Canada, to analyze the impact of different ownership structures for wind capacity expansions. Using both simulation analysis and empirical analysis of market data, we show that the price-reducing effects of wind expansion are smaller when a larger strategic firm owns new wind capacity. Lastly, we show that the effect of wind ownership on emissions depends on both the amount of generation displaced by wind output and the emissions rate of displaced generation.","Investments in renewable energy have seen record levels and are expanding at a fast pace almost everywhere in the world. For example, in 2013 about $113.7 billion was invested for solar power, and $80.1 billion invested for wind generation (following the same level in 2012) throughout the world (see Renewables, 2014 Report). In Ontario, Canada, which is a subject of the current paper, due to Ontario’s Green Energy Act the government signed a contract with a consortium (Samsung C&T Corporation and the Korea Electric Power Corporation) in 2010 to construct new green energy facilities which will triple Ontario’s renewable wind and solar energy generation over time. In 2010 Ontario had about 1200 MW installed wind generation capacity and 40 MW of solar capacity. Due to the (price and investment) incentives given to green energy developers and producers, many firms submit their proposed projects to get approval from the energy regulators and/or government. However, the total proposed capacity investments typically exceed the target expansion plans. Therefore, the government/regulator needs to decide which firms get the right to operate wind/solar farms.==== In essence we address the following question: if a fixed amount of renewable energy capacity is added into the system, would it matter whether firm A or firm B owned this new capacity? Specifically, how does the ownership of green technologies impact market outcomes and air quality, in particular wholesale prices, outputs, producer and consumer surplus, NO====, SO====, and CO==== emissions?====Many jurisdictions have implemented “Green Energy Acts” or “Renewable Energy Laws” to promote development of renewable energy supply and increase power production mainly from wind and solar energies so as to diversify generation portfolio, meet environmental targets, and improve air quality. For instance, Germany expects to meet 20% of its electricity demand using renewable energy by 2020 and 65% of its electricity needs by 2050. Numerous states in the U.S. have renewable portfolio standards that set minimum shares of electricity generation from renewable sources. The global electricity supply by wind generation in 2020 is expected to be 8–12% of the total supply.====A number of studies have examined the impact of wind generation on outcomes such as emissions, market prices, outputs of conventional generators, hydropower storage, power trade, and investment incentives; Denny and O’Malley (2006), Benitez et al. (2008), Callaway and Fowlie (2009), Traber and Kemfert (2011), Cullen (2013), Rubin and Babcock (2013), Novan (2015), Genc and Aydemir (2017). Studies by Cullen (2013) and Novan (2015) provide empirical estimates of emissions reductions due to increases in wind generation. Other studies examine the extent to which increases in wind capacity depress wholesale energy prices; this is the so-called merit order effect: Acemoglu et al. (2017), Cludius et al. (2014), Traber and Kemfert (2011). Acemoglu et al. (2017) analyze a theoretical model of a wholesale electricity market with symmetric Cournot suppliers of conventional generation and renewable generation capacity that may be owned by either the Cournot suppliers or ‘outside’ non-strategic firms. They show that the merit order effect holds in their model and that shifting ownership of a fixed amount of renewable capacity toward strategic Cournot suppliers yields higher wholesale prices and lower welfare. The present paper also examines renewable capacity ownership effects, utilizing a different theoretical model and providing quantitative results from an application.====Several studies evaluate how firm ownership structure influences the performance in different market settings. Lucking-Reiley and Spulber (2001) mention the likely impact of ownership structures in the electronic commerce and note that market power can impact the market performance. Yoo et al. (2007) theoretically examine an online marketplace in a two-sided network model to show that prices, market participation, and social welfare can differ under different ownership structures. They find that biased marketplaces (owned and managed by either buyers or sellers) entail higher surplus and lower prices for market participants compared to neutral marketplaces (owned by independent entrepreneurs).====In contrast to these papers, we explain the impact of ownership (i.e., a new production capacity comes online by either firm A or firm B) on market outcomes through the market structure, i.e., cost functions and the degree of competition in the market place. After we develop an ownership theory, as an application we focus on wind generation and examine how its ownership impacts market outcomes and the environment. The analysis could be extended to include solar power because wind and solar generations are assumed to have zero marginal cost of production. In particular, we investigate how wind generation ownership affects CO====, NO====, and SO==== emissions levels, market prices, generation levels of conventional technologies, and aggregate outputs in the Ontario power market. This research has important policy implications because wind generation licenses are often granted by regulatory agencies who decide which wind farms should be approved and provisioned into the network.====Most renewable energy projects (including wind and solar) in Ontario are required to obtain “Renewable Energy Approval” from the Ministry of the Environment (MOE).==== These projects, depending on generator type and location, have to satisfy certain environmental, archaeological, heritage, and locational (proximity to the grid and municipality) requirements, and general public consultations.==== Prior to submitting a renewable project to the MOE, it first goes to the Ministry of Natural Resources, and then the Ministry of Tourism and Culture, where it has to be reviewed and signed-off. In addition, if the project needs to be connected to the grid, it has to pass a Connection Impact Assessment provided by the system operator. Moreover, if new transmission lines are needed to connect renewable energy into the system, approval of Ontario Energy Board (under the Section 92 of the Ontario Energy Board Act, 1998) is required. In other provinces, application procedures are similar.==== Wind project applications are often rejected by the regulators. For instance, 57% of proposed wind projects were rejected in the UK in 2014.====Consequently, regulators and government officials have a direct role in choosing the firms running wind generation facilities. As we show in this paper, ownership matters because the same amount of wind capacity under different firm ownership can lead to different market prices, outputs, and emissions. In particular we aim to answer the following questions: how does the adoption of green technologies affect market prices and outputs? What are the likely effects of increased wind capacity on the environment? How much does an increase in wind capacity reduce greenhouse gas emissions?====We observe that if the market is perfectly competitive then ownership would not affect prices and allocations. This result is called ownership indifference. However, when sellers have market power, ownership of zero cost marginal cost technologies (such as wind/solar farms) matters and is critical to determine market prices, outputs and pollution levels. Using a model with asymmetric Cournot firms and a competitive fringe, we show that when a strategic firm owns new renewable capacity, output and welfare are lower than when competitive fringe firms own the same amount of new renewable capacity. The effect of renewable ownership on emissions is ambiguous, depending on the distribution of emissions rates across power plants. We extend the basic model and apply it to study the Ontario wholesale electricity market, using a dominant sector of three asymmetric strategic (Cournot) firms coupled with a competitive fringe sector. In the Ontario context, we find that market outcomes under the largest firm’s (Ontario Power Generation - OPG) ownership of wind farms are different than those under a smaller firm’s (Brookfield Inc.) ownership. In particular, market prices are higher under OPG’s wind generation ownership scenarios, while air emissions are higher under Brookfield’s ownership. Also the rate of change of emissions (CO====, NO====, and SO====) is non-linear and shows variations over ownership allocations. These findings indicate that market power and the degree of cost asymmetries between firms drive the main results.====The structure of the paper is as follows. Section 2 introduces the theoretical model and shows how the ownership of low marginal cost technologies (such as wind turbines) can impact the market outcomes asymmetrically. Sections 3 and 4 describe the Ontario wholesale electricity market and explain how model parameters are determined. Section 5 reports simulation results. Section 6 extends the model for sensitivity analysis. Section 7 covers an empirical analysis to quantify the impact of renewable energy ownership on prices and emissions in Ontario using a recent data set. The final section discusses the policy implications of the results.",Who should own a renewable technology? Ownership theory and an application,https://www.sciencedirect.com/science/article/pii/S0167718718301097,March 2019,2019,Research Article,264.0
"Casalin Fabrizio,Dia Enzo","IÉSEG School of Management, 3 rue de la Digue, 59000 Lille, France,DEMS, Università degli Studi di Milano-Bicocca, Piazza dell’Ateneo Nuovo 1, Milan 20126, Italy","Received 30 March 2017, Revised 25 October 2018, Accepted 4 November 2018, Available online 22 November 2018, Version of Record 6 December 2018.",https://doi.org/10.1016/j.ijindorg.2018.11.001,Cited by (1),"We investigate how feedback scores and alternative reputation mechanisms can mitigate asymmetric information in auctions of second-hand electronic items. In contrast to previous studies, we exploit the information available across heterogeneous goods and avoid the problems caused by different degrees of wear and tear by studying remanufactured products. We find the presence of relevant scale effects in the value of such reputation mechanisms that become complements or substitutes depending on the value of the transactions. Feedback scores are not valuable when alternative contractual devices are in place, such as refurbishment by a manufacturer-approved vendor and the availability of return policies. However, when we partition the sample according to the value of the auctions, feedback scores become effective for cheap items, whereas return policies are important for expensive goods.","A number of recent articles have used the information available from eBay to investigate how reputation mechanisms can alleviate asymmetric information problems in auctions of second-hand goods. Most of these studies analyze a single class of goods whose features are well defined, such as collectible coins, specific models of toys, microprocessors, and computers==== because differences in the rate of obsolescence influence the price differentials between new and used goods. However, uncertainty about the quality of used items is likely to display substantial variability among goods since the opportunity cost of information-gathering activities changes with the characteristics and value of the items. We therefore work on the hypothesis that the severity of the “Lemons problem” varies with the specific characteristics of the goods. Indeed, differences in the availability of information across heterogeneous goods sold in on-line auctions are relevant, as Lewis (2011) finds that the disclosure of information through the provision of photographs has a significant impact on the auction prices of second-hand cars. The value of the institutional devices in place to reduce this uncertainty – such as indicators of reputation, the presence of specialized intermediaries, and particular contractual features (e.g. return policies and guarantees) – may thus differ across goods, particularly among goods of different value. Moreover, the impacts of reputation mechanisms may be a non-linear function of the value of such items.====In this work, we study a large domain of heterogeneous goods sold on eBay to analyze the role of reputation mechanisms designed to alleviate asymmetric information in on-line auctions for used, but remanufactured, products. Remanufacturing is defined as “returning a used product to at least its original performance with a warranty that is equivalent to – or better than – that of the newly manufactured product”.==== For this class of goods, therefore, the price differentials between the remanufactured item and the corresponding new one should not reflect differences in their expected obsolescence, as long as the information available is reliable.====We compare several alternative mechanisms used by eBay to solve information asymmetry problems, analyzing if they complement each other or if, for some classes of goods, they are alternative tools. Sellers’ reputation, as measured by feedback ratings, is one such mechanism. Sellers’ willingness to handle returns is another mechanism. Certification by original equipment manufacturers is yet another. In fact, although remanufactured products are supposed to be of identical quality, the remanufacturing process may be perceived as having different quality levels when carried out by different vendors. Such certification backs up the information on the quality of remanufacturing processes with the reputation of the original manufacturers. The return policy, instead, is ultimately backed by the seller’s reputation since, for some of the items under scrutiny, legal enforcement could be too costly.====More specifically, we use the dataset produced by Pang et al. (2015) to analyze a large sample of auctions of electronics goods for a wide range of brands, gathering all the transactions occurring over a specified period to avoid potential sample selection bias, which is a challenging issue in the analysis of reputation mechanisms.==== Following Lewis (2011), we supplement our analysis with a set of variables potentially associated with the ability to disclose information to ensure that our results capture the vendor’s reputation rather than its information disclosure skills. Moreover, when we partition the sample to analyze specific information mechanisms, we do so on the basis of the exogenous characteristics of the items. We thus avoid any bias caused by “endogenous stratification”.====We find that, to a large extent, the different mechanisms in place are substitutes, but that at least one of them is always in operation. When we partition the database on the basis of the value of the auctioned items, we find, in line with our fundamental hypothesis, that the impact of the price of the new good on the auction price of the remanufactured one is extremely different across quartiles. Consequently, we also find that the reputation mechanisms operating for goods of different values are different. Our results provide strong empirical support for the basic predictions of the reputation model developed by Klein and Leffler (1981). In particular, we find that the seller’s reputation – as measured by reputation score – is only an effective mechanism for auctions of low value. For transactions of a higher value, the reputation of sellers tends to vanish and it is progressively crowded out by the presence of return policies and certifications by the original manufacturers. When supplementing our analysis with variables potentially associated with the ability to disclose information, following the strategy adopted by Lewis (2011), we find that the main results remain unaltered.====The remainder of the study is organized as follows. Section 2 reviews the relevant literature. Section 3 presents the theoretical framework on which we build our empirical analysis. Section 4 describes the dataset. Section 5 sets out the empirical methodologies. Section 6 discusses the empirical results, while Section 7 concludes.",Information and reputation mechanisms in auctions of remanufactured goods,https://www.sciencedirect.com/science/article/pii/S0167718717302151,March 2019,2019,Research Article,265.0
"Esteves Rosa-Branca,Resende Joana","Universidade do Minho, Escola de Economia e Gestão (EEG) and NIPE, Portugal,Universidade do Porto (FEP) and Cef.Up, Portugal","Received 11 September 2017, Revised 8 June 2018, Accepted 9 November 2018, Available online 22 November 2018, Version of Record 11 December 2018.",https://doi.org/10.1016/j.ijindorg.2018.11.003,Cited by (21)," segment consumers are expected to pay higher average prices under the personalized advertising/pricing strategy. We also show that, in the context of our simultaneous game, targeted advertising with price discrimination might boost firms’ profits in comparison to the case of mass advertising and uniform prices. The overall welfare effects of the personalized strategy are ambiguous. However, even when the personalized strategy boosts overall welfare, consumers might all be worse-off. Thus the paper gives support to concerns that have been raised regarding the firms’ ability to adopt personalized strategies to boost profits at the expense of consumers.","The recent innovation in information technologies has drastically changed firms’ marketing and communication strategies, allowing them to send personalized information (including personalized price offers) to their customers. On the one hand, it is now much easier for firms to gather, store and process consumer-specific data, which increases their ability to segment consumers with different profiles. On the other hand, firms are now able to deliver timely, targeted and local informative/ advertising content in an unprecedented way. A particularly important development allowing for personalized communication between firms and their customers is the generalized use of smartphones and other portable devices (e.g tablets or smartwatches), which allow firms to communicate with their customers anywhere at any time in a very effective way. By 2017, global phone penetration surpassed 5 billion users.====Firms may now easily design real-time pricing strategies in which consumers get “special discounts” and other advantages depending on their ==== For example, in the case of the airline industry, the adoption of personalized pricing strategies has become quite frequent, to an extent that the IATA Resolution 787 ====However, we are far from reaching consensus on the desirability of this type of practices. For instance, some senators in the USA are skeptical about IATA Resolution 787 since ==== This discussion illustrates how the new forms of price discrimination such as behavior-based price discrimination, location-based pricing and other strategies involving personalized prices are challenging conventional wisdom regarding the welfare effects of competitive price discrimination. Both scholars and practitioners are now participating on a lively debate regarding the ==== and==== of price discrimination through new technologies allowing for personalized communication between firms and their customers.====In this respect, the Office of Fair Trading (2010)==== argues that ==== Shiller (2014) also highlights some negative effects of personalized pricing. He simulates counterfactual environments in which Netflix adopts first degree price discrimination and finds out that ====. An article by Lynch (2017) at Financial Times (february 25, 2017) about policing the digital cartels refers that new pricing practices relying on customized algo rithms may raise serious antitrust challenges, which may substantially affect competition authorities ====.====In contrast, Chen (2005) argues that ==== Along the same lines, the Executive Office of the President of the United States (2015)==== argues that ==== [personalized]====In this paper, we contribute to this debate by providing an analytical study on the competitive and welfare effects of personalized pricing through targeted informative advertising. We first analyze a benchmark model in which firms only have access to mass communication technologies and are, therefore, confined to engage in uniform pricing strategies. Then, we shed some light on the competitive effects of personalized ads and pricing by comparing the price and welfare outcomes of this benchmark model to the ones arising in a model of personalized communication (with targeted ads and personalized price offers, as in Esteves and Resende, 2016).==== While economists have long been concerned in understanding the profit and welfare effects of price discrimination and advertising separately, little is known about the competitive and welfare effects of price discrimination enabled by targeted advertising, despite the growing importance of customized advertising/pricing strategies (in light of the recent digital technologies developments).====The paper aims at filling in this literature gap by investigating the following research questions: What are the price and welfare effects of personalized pricing through targeted advertising in comparison to mass advertising and pricing? Can firms sustain higher prices and obtain greater profits by combining personalized ads with price discrimination strategies? Do consumers benefit from targeted ads with personalized price offers?====In order to address these questions, we propose a static game of duopoly competition based on Esteves and Resende (2016). There are two firms launching two ==== differentiated products. As in Stahl (1994), firms need to invest in advertising to generate demand awareness. By investing in advertising, firms endogenously segment the market into captive (partially informed) and selective (fully informed) customers. The model exhibits best-response asymmetry (Corts, 1998): each firm has a strong and a weak market segment (a firm’s strong market includes consumers with a stronger preference for that firm, whereas its weak market includes consumers with a stronger preference for the rival market).==== More precisely, the set of potential buyers is assumed to be composed of two distinct segments of equal mass. As in Shilony (1977), Raju et al. (1990), Esteves (2010) and Esteves and Resende (2016), everything else the same, consumers in segment ==== prefer product ==== over product ==== to a certain extent (measured by a brand preference degree parameter). Accordingly, consumers may end up buying their least preferred product, provided its price is sufficiently low ==== the competing product.====We depart from Esteves and Resende (2016) regarding the features of firms’ advertising and pricing strategies. In our baseline model firms choose an advertising intensity to the entire market and price discrimination is technically unfeasible (since all the ads are identical, they must announce a similar price). We will then analyze the price, profit and welfare effects of personalized pricing through targeted ads by comparing our findings to Esteves and Resende (2016), who consider a model in which firms choose an advertising intensity and a price to be targeted to each segment of the market, without providing any insights on the price and welfare outcomes of personalized communication.====The stylized model addressed in this paper offers new insights to the literature on price discrimination based on customer recognition. First, we find that both firms will compete in both market segments only when the advertising costs are not excessively high and the consumers’ willingness to pay for the products is sufficiently high ==== the brand preference degree parameter. Second, when both firms compete in both market segments, we find that average prices with mass communication (mass advertising and uniform pricing) are ==== their counterparts with personalized communication, regardless of the market segment. Consequently, we show that firms can take advantage of the interplay between price discrimination and targeted advertising to sustain higher prices. Our findings give theoretical support to some recent concerns regarding pricing in online markets, like the example recently referred by Useem (2017) in The Atlantic Magazine, ====As a result, our analysis highlights that, in comparison to mass advertising and uniform pricing, expected profits can be higher when firms employ a strategy of targeted advertising with personalized pricing offers. In particular, when advertising costs are low enough, the ability to price discriminate allows firms to set higher prices and boost their profits.Therefore, our results depart from the classic prisoner dilemma result that arises in models of price discrimination with fully informed consumers (without advertising) and exhibiting best-response asymmetry (e.g. Thisse and Vives, 1988). Our results also depart from Iyer et al. (2005) and Brahim et al. (2011). Iyer et al. (2005) show that the ability to target advertising increases profits. In their paper this result is robust to firms’ ability to price discriminate or not. Brahim et al. (2011) show that profits with mass advertising are always above profits with targeted advertising, provided that firms advertise both to their strong and to their weak markets.====Finally, we find that it is not possible to make general predictions regarding the welfare effects of price discrimination through targeted ads since the expected welfare may either increase or go down when firms move from a set-up with mass communication (mass advertising/uniform pricing) to a set-up with personalized communication (targeted advertising/ price discrimination). However, we show that even if welfare goes up when firms run targeted advertising campaigns with personalized price offers, firms may be the only ones benefiting from this strategy. On the contrary, consumers may end up being worse-off than in the case of mass advertising/ uniform pricing since firms take advantage of the interplay between targeted ads and price discrimination to induce inefficient shopping and relax price competition in the market.====In what concerns our profit and welfare results, it is worth noting that, in our set-up, the driving force for higher profits under personalized price and advertising decisions is completely different from that of the strategic commitment literature, since, in our case, advertising and pricing decisions are taken simultaneously. Instead the rationale for our outcome lies on firms ability to reduce advertising in the strong market to relax the rival’s competitive pressure in this market segment.====In light of our pricing and welfare results, we conclude that the presumption that markets are competitive is not a sufficient condition to avoid anti-trust concerns raised by price discrimination through targeted advertising. Moreover, the recommendations about competition authorities’ decision to block or not price discrimination practices seem to be sensitive to the welfare criterion under consideration. In any case, our findings suggest that competition authorities always need to gather information on aspects like (i) the characteristics the relevant markets under analysis (e.g. advertising technology and costs, consumers’ willingness to pay for the favorite product, degree of product differentiation); and (ii) the strategic variables defining the nature of competition between firms (e.g. prices, advertising,...).",Personalized pricing and advertising: Who are the winners?,https://www.sciencedirect.com/science/article/pii/S0167718718301061,March 2019,2019,Research Article,266.0
"Haucap Justus,Rasch Alexander,Stiebale Joel","DICE, University of Düsseldorf, Düsseldorf, Germany","Received 14 June 2017, Revised 10 September 2018, Accepted 10 October 2018, Available online 14 November 2018, Version of Record 21 December 2018.",https://doi.org/10.1016/j.ijindorg.2018.10.003,Cited by (45),This article analyses how horizontal ,"Following the recent merger wave in high-tech industries, competition authorities in Europe and the US are becoming increasingly concerned about the effects that mergers in these markets can have on innovation. For example, the European Union’s competition commissioner, Margrethe Vestager, recently said “when we look at high-tech mergers, we do not just look at whether they might raise prices. We also assess whether they could be bad for innovation. Last year, we looked at a merger between the drug company Pfizer and its rival, Hospira. We only approved the deal after Pfizer agreed to sell the European rights to an arthritis drug it was developing. One concern was that Hospira already had a competing drug on the market, and we thought Pfizer might stop work on its own drug if the deal went ahead as planned. Which would have meant less of the innovation that we depend on as patients”.==== Similarly, in the acquisition of GlaxoSmithKline’s oncology business by Novartis, the European Commission “identified the risk that Novartis would likely have stopped developing two innovative drugs that showed great promise for the treatment of skin and ovarian cancer” (European Commission, 2016, p. 4). In fact, the European Commission has intervened and, in some prominent merger cases such as most recently Bayer/Monsanto and Dow/DuPont, requested remedies with the explicitly stated goal of preserving innovation.====Interestingly, there appears to be some degree of divergence between US and EU antitrust authorities, as the Federal Trade Commission (FTC) did not request similar remedies in the Pfizer/Hospira merger. However, US Horizontal Merger Guidelines indicate concerns about potential negative effects of mergers on innovation as well. Specifically, section 6.4 of the guidelines specifies that “competition often spurs firms to innovate” and that US competition authorities “may consider whether a merger is likely to diminish innovation competition by encouraging the merged firm to curtail its innovative efforts below the level that would prevail in the absence of the merger.” In this context, it should also be noted that horizontal merger guidelines both in the US and the EU are explicitly allowing for a so-called efficiency defense for otherwise anticompetitive mergers. These efficiency claims often concern research and development (R&D) efficiencies (see OECD, 2012). If a firm can convincingly demonstrate that a merger results in a substantial and timely increase in efficiencies, an otherwise anticompetitive merger can be cleared. Both the European commission and the FTC pursue a consumer welfare standard. Therefore, efficiency gains have to be sufficient to generate net benefits to consumers. While efficiency gains typically take the form of cost savings, innovation incentives can also be affected. A complete analysis of potential efficiencies from mergers should, however, not only analyse how the merged entity’s prices, quantities and innovation incentives change (i.e., the direct effects of a merger), but also how these change for rival firms (indirect effects). While competition authorities sometimes analyse how mergers directly affect the merged firm’s innovation incentives, especially in high-tech industries, impacts on rivals’ innovation incentives have been rarely mentioned in merger guidelines or competition cases. This is unfortunate since the effects of mergers on innovation in the relevant market depend on the reactions of non-merging competitors. Analysing rival firms’ responses to mergers is also essential to understand the mechanisms underlying observed changes in innovation. For instance, a reduction in innovation input and output of merging firms might not only stem from reduced competition but also from organizational integration problems or a rationalization of duplicated research programs. In contrast, adjustments in rival firms’ innovation strategy are likely due to a change in the competitive environment after a merger.====While there is a growing literature on the effects of mergers on the innovation of firms directly involved (see, e.g., Ornaghi, 2009, Guadalupe, Kuzmina, Thomas, 2012, Bena, Li, 2014, Szücs, 2014), evidence on effects of mergers on outsiders’ innovation incentives is scarce. Notable exceptions include recent papers by Valentini (2016) and Uhlenbruck et al. (2017). We contribute to this literature in various ways. First, we improve over recent empirical contributions by using a more accurate definition of rival firms which is based on industry experts involved in the evaluation of merger cases. Further, we take potential endogeneity of merger decisions into account by selecting a control group of unaffected firms with similar pre-merger characteristics and innovation trends using propensity score matching. We also check robustness of our main results towards using an instrumental variable method. Finally, we link our empirical results to a theoretical model of competition in differentiated products among heterogeneous firms.====In our theoretical model, we analyse a three-player oligopoly market in which firms can invest in product innovations. While there are two firms with relatively low innovation costs, there is also one firm which faces higher costs of innovating. We compare profits and innovation levels (i) for the pre-merger oligopoly and (ii) for the post-merger market structure in which one of the firms with higher innovation levels has purchased the less innovative rival firm. The key results from the model are that a merger has (i) a negative effect on the merged entity’s innovation efforts in an industry with high research intensity and (ii) a negative effect on non-merging competitors in an industry with high research intensity provided the target firm conducts relatively little innovation compared to other firms before the merger. Our model also predicts that negative effects of mergers on innovation are more likely to occur when pre-merger competition is intense and less likely when an industry’s R&D intensity is low.====Our empirical analysis is based on a sample of pharmaceutical mergers under scrutiny by the European Commission between 1991 and 2007. The pharmaceutical industry is an interesting case study for the relationship between mergers and innovation for several reasons. First, according to the EU Industrial R&D scoreboard,==== it is among the industries with the highest R&D to sales ratio (above 13%) and pharmaceutical firms account for more than 20% of total R&D spending among top 1000 firms in Europe. Further, the industry has experienced a series of large mergers that raised policy concerns about negative effects on innovation in the industry (e.g., Morgan, 2001, Comanor, Scherer, 2013). For instance, in a recent speech, a former president of global research and development at Pfizer, John LaMattina, claimed that “mergers are bad for science, bad for patients, bad for medicine” since they had reduced drug companies’ R&D budgets and the number of firms engaging in innovative activity.====A unique feature of our data set is that it contains expert market definitions which we collect from merger reports by the European Commission. This enables us to identify competitors for each merger case. Our empirical results are mainly based on the analysis of patent counts as a proxy for innovative activity. We find that mergers are, on average, associated with a large decline in innovative activity of the merged entity and among non-merging competitors. This result is consistent with the theoretical model’s prediction for markets with a high research intensity—which arguably applies to most pharmaceutical markets. It is robust towards using a propensity score matching approach combined with a difference-in-differences (DiD) estimator and applying an instrumental variable (IV) strategy. Our empirical results are also consistent with other implications of the theoretical model. We find that the decline in innovation after mergers is concentrated in markets with high pre-merger innovation intensity, while rival firms sometimes increase their innovation as response to mergers in markets with low research intensity. Further, we find that the effects for merged entities and their competitors mainly stem from technology fields with pre-merger overlap of acquirer, target and rivals indicating that the results are driven by a change in competition after mergers.====The remainder of the article is structured as follows: The next section provides an overview of the related literature, Section 3 presents our theoretical model. The data and empirical strategy are described in Sections 4 and 5, respectively. Results of the empirical analysis are presented in Section 6. Section 7 concludes.",How mergers affect innovation: Theory and evidence,https://www.sciencedirect.com/science/article/pii/S0167718717303685,March 2019,2019,Research Article,267.0
"Lee Jungmin,Park Sangkon","Department of Economics, College of Social Sciences, Seoul National University, Building 16, Room 633, Seoul, Republic of Korea,IZA (Institute for the Study of Labor), Germany,Korea Culture & Tourism Institute, Republic of Korea","Received 8 August 2017, Revised 29 June 2018, Accepted 16 July 2018, Available online 8 November 2018, Version of Record 16 November 2018.",https://doi.org/10.1016/j.ijindorg.2018.07.004,Cited by (3),"To investigate whether foreign shoppers are discriminated against, we conduct a field experiment at a large shopping mall. We employ 56 female foreigners speaking 11 different languages and 7 Korean natives and collect data on total 2267 store visits. To test price discrimination based on differential search costs, we randomly select buyers to send a signal of lower search cost. Results show that negotiation and signaling induce sellers to lower their price offers. Our experimental findings, combined with auxiliary survey data from foreign visitors, suggest that sellers use the language of buyers as a proxy for the distribution of reservation values.","We examine whether foreign tourists are ripped off by domestic sellers and, if so, why. It is not difficult to find such cases at travel review websites, such as TripAdvisor or Expedia. According to the data from the 2015 International Visitor Survey (IVS), about 20% of foreign visitors to Korea, the country in which our field experiment was conducted, reported that they had experienced being ripped off.==== One may think of many reasons why foreign shoppers are treated differently from domestic shoppers. Foreign shoppers are different from domestic shoppers along many important observed and unobserved dimensions, such as income, bargaining skills, and the amount of information, which are all possibly correlated with the willingness to pay.====In this paper, we test the hypothesis that sellers infer the search costs of shoppers from their observable characteristics and charge different prices based on the inferred search costs. For the purpose, we conducted a field experiment which enables us to infer sellers’ reservation price for foreign and domestic buyers, while holding other buyer-side confounders as constant as possible.==== The uniqueness of our experiment is using the ==== that shoppers use as a differentiator for search costs. We employed, as “test buyers”, 56 foreigners from 11 different language countries and 7 domestic (Korean) shoppers. As an additional attempt to control for individual buyer-specific unobservable factors, we recruited two bilingual Koreans who can speak Chinese and Korean fluently as well as two bilingual buyers who are ethnically Korean but speak English.====Besides comparing different groups of foreign shoppers according to their language and domestic shoppers, we also conducted a signaling treatment. We randomly assigned test buyers into the treatment and control groups. Those in the treatment group are instructed to exhibit their intention to “check out other stores”—by saying in their own language while stepping out of the shop with waving hands and so on—at the last stage of bargaining. On the other hand, those in the control group follow the fixed-offer strategy until the end of bargaining. The hypothesis is that a major source of price discrimination against foreign shoppers is their search cost, which is higher than that of domestic buyers as well as differs by their language. It is likely that foreign shoppers are less familiar with the area, do not know how to bargain and have less time for shopping. Then, the signal of a low search cost is expected to reduce price discrimination against foreigners (Gneezy et al., 2010). The particular signal of our experiments was selected via pre-experiment informal interviews with non-participating sellers (similar sellers at different shopping malls from the experimental field).====To sum up our main findings, we find little evidence that foreign shoppers are charged more than domestic shoppers. Surprisingly, the initial price offers are ==== for foreign shoppers. After all haggling according to the fixed-offer strategy and signaling, some foreign shoppers, those speaking English and French, end up with lower price offers. Those speaking Thai and Vietnamese receive a bit higher offers than domestic shoppers, but the differentials are quite small and statistically insignificant. The results from bilingual Koreans also show that sellers give lower price offers to foreign-language buyers. This suggests that when sellers decide price offers, they take into account buyers’ language rather than ethnicity.====Using our experiment data, we structurally estimate the distribution of sellers’ reservation price for each language group. We find that the estimated reservation price distributions are consistent with foreign buyers’ shopping patterns that are found in an independent data source, the IVS. Specifically, sellers who participated in our experiments tend to give higher price offers to those foreign buyers who spend more on shopping while visiting Korea. Further, sellers’ reservation prices are more dispersed when buyers are more heterogeneous within languages in terms of the amount of spending for shopping. The findings suggest that sellers make good inference about the willingness to pay across foreign buyers from the kind of language that they use and that sellers effectively utilize the piece of information to maximize expected profit.====There are a few previous studies that are directly related to our study. Graddy (1995) collected the unique data on the prices paid by individual consumers at a fish market in New York City and found a large degree of price dispersion, implying that there exists imperfect competition in the market which is believed to be highly competitive. In fact, she found the evidence for Cournot behavior that sellers charge less for consumers with a higher price elasticity of demand (Asian buyers compared to white buyers). However, since the data she used are observational, it is impossible to conclude that the price dispersion is solely caused by sellers rather than by buyers or by both. Castillo et al. (2013) conducted a field experiment with taxi drivers and examined gender discrimination. They found that male passengers are unfavorably treated. To distinguish taste-based and statistical discrimination, they conducted an additional signaling experiment where passengers send a signal about their willingness to pay (specifically, showing to the second taxi that they failed to negotiate with the first taxi) and found that the signal eliminates the observed gender difference in bargaining outcomes. Also conducting a field experiment with taxi drivers but in Greece, Balafoutas et al. (2013) examined whether taxi drivers overcharge passengers when they have informational advantages. Similar to our experiment, they compared natives and foreigners with the idea that natives are more familiar with the taxi tariff system as well as the optimal route. They found that foreigners are not only taken on longer routes but also overcharged per mile. In their experiment, all foreigners speak English.====The remainder of the paper proceeds as follows. Section 2 explains the details of our field experiments and presents the summary statistics describing the data that we collected. Section 3 presents our empirical models. Section 4 presents estimation results. In Section 5, we investigate the sources of price discrimination by comparing our experimental findings with foreign visitors’ shopping patterns found in the auxiliary survey data on general foreign visitors. Lastly, Section 6 concludes.",Price discrimination by language: Field experimental evidence from a shopping mall,https://www.sciencedirect.com/science/article/pii/S0167718718300675,March 2019,2019,Research Article,268.0
"Bae Jinsoo,Kagel John H.","Department of Economics, The Ohio State University, United States","Received 1 August 2018, Revised 23 October 2018, Accepted 26 October 2018, Available online 2 November 2018, Version of Record 12 November 2018.",https://doi.org/10.1016/j.ijindorg.2018.10.006,Cited by (5),"We experimentally investigate the Generalized Second Price (GSP) auction used to sell advertising positions in online search engines. Two contrasting click through rates (CTRs) are studied, under both static complete and dynamic incomplete information settings. Subjects consistently bid above the Vikrey–Clarke–Grove's (VCG) like equilibrium favored in the theoretical literature. However, bidding, at least qualitatively, satisfies the contrasting outcomes predicted under the two CTRs. For both CTRs, outcomes under the static complete information environment are similar to those in later rounds of the dynamic incomplete information environment. This supports the theoretical literature that uses the static complete information model as an approximation to the dynamic incomplete information under which advertising positions are allocated in field settings.","Search engines such as Google, Yahoo and Microsoft sell advertisement slots on their search result pages through auctions, among which the ==== (GSP) auction is the most prevalent format. Under GSP auctions, advertisers submit a single per-click bid. These bids are raked from highest to the lowest, with ad-slots assigned according to the ranking, with each advertiser paying a per-click price equal to the bid submitted by the next-highest bidder.====Edelman et al. (2007) and Varian (2007) were the first to characterize the Nash equilibrium of the GSP auction using a ==== model about competitors’ per-click values. They showed that truthful bidding is ==== a dominant strategy, that multiple Nash equilibria exist and that these equilibria need ==== to be efficient. Edelman et al. (2007) proposed a refinement for the Nash equilibrium referred to as ==== equilibria (LEFE), where no bidder would prefer another's slot to her own, given the ad-slot prices.==== The LEFE predicts efficient allocations of ad-slots but still admits multiple equilibria.====Edelman et al. (2007) further proposed that the LEFE with the lowest possible bids would be the most likely equilibrium to emerge as the long-run outcome in GSP auctions, based on an ==== version of the GSP auction. Under this outcome, the allocation of ad-slots and the associated payments coincide with those of the dominant strategy equilibrium in the Vikrey–Clarke–Grove's (VCG) auction. In what follows, this refinement will be referred to as the ==== equilibrium.====Behavior in the GSP auctions is explored here in an experiment with three bidders and two ad-slots under two contrasting CTRs that result in distinctly different bidding behavior. Specifically in one treatment the CTRs are relatively far apart, where the first slots gets 11 clicks and the second slot 3 clicks. In this treatment the VCG-like equilibrium predicts mid-value bidders will employ modest reductions in bids relative to their valuations, while facing minimal competition from the high-value bidder.==== In the second treatment, CTRs are very close to each other, 11 clicks for the first slot and 10 for the second. In this treatment the VCG-like equilibrium predicts that mid-value bidders engage in sharp price cutting due to the first and second positions being close to each other, so that the high-value bidder has an incentive to compete for the second position at a favorable price. The experiment is conducted in a static complete (SC) information environment, and in a dynamic incomplete information (DI) environment, closer in structure to how GSP auctions are conducted in practice.====An additional feature of the experimental design is that while per-click values are assigned randomly across auctions, a fixed ratio is maintained between values, which is required to ensure that the contrasting predictions between the two CTRs are maintained.==== The result is that for the mid-value bidder in the 11–3 treatment the upper bound for an LEFE in undominated strategies (LEFEU) coincides with value bidding, compared to sharp price reductions in the 11–10 treatment. There are also contrasting differences in mid-value bids needed to achieve the VCG-like equilibrium, with bids just above that of the low-value 11–10 treatment, compared to much more modest bid shaving under 11–3.====Behavior is broadly consistent with the predictions of the theory in that there is minimal bid shaving with 11–3 and substantial price shaving with 11–10. The contrast is particularly strong in bidding over rounds in the DI treatment: under 11–3 bids hover around bidders’ values, compared to sharp price cutting over time with 11–10. Efficiency, as traditionally defined in auction experiments, is consistently high averaging over 90% under 11–3 and around 75% under 11–10. Outcomes for mid-value bids under 11–3 lie within the range of LEFEU, but above the upper bound of the LEFEU for 11–10. This is a consequence of the fact that value bidding is at the upper bound of the LEFEU under 11–3, but well above it for 11–10. Bidders consistently bid higher than the VCG prediction under both CTRs, substantially less so under 11–3 than 11–10. Differences between mid-value bids relative to the VCG prediction are the same under SC compared to the last round under DI. This provides support for the idea that SC auctions can serve as a model for bidding in GSP auctions, as actually practiced. However, the predicted VCG-like revenue is not likely to be achieved. Part of this has to do with low-value bidders bidding above value, a common outcome in single unit second-price, private value auctions.==== Median deviations of mid-value bids from the VCG-like equilibrium average around 25% and 150% under 11–3 and 11–10 respectively, under both SC and DI. Reasons for these marked differences are discussed below.====There have been a number of empirical studies of GSP auctions. Borgers et al. (2013) used a revealed preference approach to infer the per-click values of bidders, reporting that in a number of cases the revealed click values do not correspond to a NE. Athey and Nekipelov (2012) developed and estimated a model that allows uncertainty in quality score and bidder entry, and show that efficiency of GSP auctions is slightly less than Vickery auctions, but revenue effects are ambiguous. Varian (2007) showed that if bids are part of an LEFE, the expenditure profile must be increasing and convex, and that the data from Google's auctions are often consistent with this.==== Empirically investigating whether GSP auction outcomes are a VCG-like equilibrium is difficult, since advertisers' click values are not observable. In contrast, in an experiment one can induce click values, providing a clean environment to investigate this question.====There have been several experimental studies comparing outcomes of GPS auctions in relation to the VCG-like equilibrium, with mixed results. Fukuda et al. (2013) and McLaughlin and Friedman (2016) compared revenues of GSP to sealed bid VCG auctions run as a control treatment, concluding that revenue is indistinguishable between the two. However, Noti et al. (2014), in comparing the two found higher revenue in the GSP compared to VCG auctions. In this experiment subjects were instructed to bidding their values in the VCG auctions was optimal.==== All three studies employed a limited set of pre-determined valuations, which restricts the potential variation in outcomes that can be observed with random valuations such as those employed here.====The paper that is closest to ours is Che et al. (2017). They use new random valuations in each auction and two contrasting CTR treatments with characteristics similar to the 11–3 and 11–10 treatments employed here. However, unlike here, they use unrestricted random values which create different equilibrium outcomes between auctions, some of which may deviate from the general characteristics of the CTR treatments under study. In what follows, we employ the same fixed ratio for values that avoids this, while still employing random realization valuations (more on this below). Further, the fixed ratio employed creates very narrow bounds for LEFEU outcomes, and quite demanding VGC-like equilibrium outcomes for mid-value bidders. Nevertheless we view the two papers as complements, with each providing a similar structure for studying GSP auctions, but with some significant differences in experimental design.====The rest of the paper is organized as follows. The theoretical framework for analyzing GSP auctions is reviewed in Section 2. Section 3 describes the experimental design and procedures. Experimental results are reported in Section 4. Section 5 concludes by summarizing the findings.",An experimental study of the generalized second price auction,https://www.sciencedirect.com/science/article/pii/S016771871830105X,March 2019,2019,Research Article,269.0
"Pessoa João Paulo,Rezende Leonardo,Assunção Juliano","Sao Paulo School of Economics - FGV, São Paulo, Brazil,Centre for Economic Performance, London, UK,PUC-Rio, Rio de Janeiro, Brazil,PUC-Rio, Rio de Janeiro, Brazil","Received 7 March 2017, Revised 16 July 2018, Accepted 17 July 2018, Available online 30 October 2018, Version of Record 29 November 2018.",https://doi.org/10.1016/j.ijindorg.2018.07.005,Cited by (5),We study how the diffusion of flex (bi-fuel) cars affected competition on ethanol and gasoline retail markets. We propose a model of price competition in which the two fuels become closer substitutes as flex cars penetration grows. We use a large panel of weekly prices at the station level to show that fuel prices and margins have fallen in response to this change. This finding is evidence of market power in fuel retail and indicates that innovations that increase consumer choice benefit even those who choose not to adopt them.,"The key aspect underlying the identification of conduct in empirical studies in industrial organization is the relationship connecting changes in the price elasticity of demand and firms’ pricing decisions (Bresnahan, 1982). In this paper, we explore two features of the automotive fuel retail market in Brazil that allow us to directly document this relationship. First, Brazil is a dual fuel market: both gasoline and ethanol have been available for automobiles at virtually every fuel station in the country since the 1980s. Second, flex cars have been available in Brazil since 2003 and have allowed consumers to treat these two fuels as nearly perfect substitutes at the pump. Flex, or bi-fuel, vehicles are able to run on any mix of gasoline and ethanol fuel; electronic sensors identify the mix at the fuel tank and adjust the fuel injection accordingly. They have become a commercial success: in 2008, 94% of the new cars registered in the country were flex. This innovation provides a source of change in the cross-price elasticity between two products that allow us to directly identify its effect on pricing.====We build a model of strategic price formation to study the impact of the flex car fleet on equilibrium fuel prices. In the model, stations compete by choosing the price of gasoline and ethanol, and consumers treat fuel from different stations as imperfect substitutes, due to location or other idiosyncratic preferences. The model suggests that, in equilibrium, fuel retailers respond strategically to an increase in flex car penetration (that is, an increase in substitutability between products within its own product line) by reducing markups. There is a clear difference in comparison to other differentiated-goods oligopoly models: in our setting, the law of one price has no bite. Our theory does not predict that the price of the two fuels should move closer as they become perfect substitutes to a growing share of the consumers. Because both prices are set by the same firm, it is generally optimal to keep prices apart to price discriminate for consumers who cannot freely switch.====We empirically assess the model by documenting the causal effect of flex-fuel penetration on fuel prices using reduced-form methods.==== We study the impact of the flex car fleet on retail prices and the spread between gasoline and ethanol. Because the speed of penetration of this technology has been unequal across localities (roughly driven by the pace of car fleet renewal), we have been able to employ panel data methods to control for aggregate time-varying effects and local fixed effects, using a detailed sample of weekly prices at the gas station level.====Our empirical analysis exploits variation in the flex fuel penetration due to local differences in the speed of fleet renewal, which is mostly driven by cross-sectional variation in income and economic activity. (We account for this possible source of omitted variable bias by adding income as an additional control.) In principle, variation in fuel prices (or rather variation in the expectation about future fuel prices) may also have an effect on fleet renewal; several studies have shown that fuel prices affect demand for automobiles (Busse, Knittel, Zettelmeyer, 2013, Goldberg, 1998, Kahn, 1986, Klier, Linn, 2010, Li, Timmins, Haefen, 2009, Pakes, Berry, Levinsohn, 1993), and even specifically consumer choice between diesel and gasoline vehicles in Europe (Verboven, 2002). To account for the potential endogeneity due to reverse causality, we employ an IV strategy that builds on the recent empirical trade literature (Autor, Dorn, Hanson, 2013, Costa, Garred, Pessoa, 2016), using data on the local presence of car dealerships.====Consistent with the predictions of our model, the results show that fuel stations have significantly reduced ethanol and gasoline prices: a 10 percentage point increase in the market share of flex cars reduces ethanol and gasoline prices by approximately 8.1 and 2.8 cents of BRL, respectively. The absolute effects are stronger for ethanol, which is consistent with the fact that ethanol has a smaller market share in the automotive fuel market in Brazil. Our results provide evidence of market power in fuel retail and that innovations that increase consumer choice may benefit even those consumers who choose not to adopt them. We also find (weaker) evidence that the expansion of the flex car fleet has increased the spread (that is, the absolute price difference) between gasoline and ethanol prices.====Moving beyond the model, we study how flex car penetration has affected retail margins and wholesale prices (or retail costs). We find evidence that fuel retail margins have fallen with the increase of flex cars, while the estimated effects of flex cars on wholesale prices are statistically non-significant in most of our specifications. This last result corroborates the interpretation that wholesale prices are more tightly linked to international markets and less influenced by local market effects.====Our paper contributes to an increasing literature on the industrial organization of ethanol as automotive fuel and its relation to the gasoline market. Anderson (2012), Corts (2010) and Shriver (2015) are examples of recent studies that investigate the ethanol market in the US.==== Anderson (2012) studies the demand for the product. Shriver (2015) studies the network effect that arises due to spatially dependent complementarities between the availability of stations supplying ethanol fuel and the local number of flex cars. Corts (2010) also analyzes the decision to supply ethanol by local stations, using as a source of variation purchases of flex cars by government agencies.====This emphasis on the issue of expanding the distribution network reflects the incipient nature of ethanol as automotive fuel in the US. In Brazil, by contrast, the challenge of building an extensive distribution network has been completed in the 1980s with the Pró-álcool program, further discussed in Section 2 below. Brazil provides a setting where it is possible to study a mature dual-fuel market operating.====Most existing studies employing Brazilian data (Ferreira, Prado, Silveira, 2009, Salvo, Huse, 2011, Boff, 2011) use time series of average price data to look for evidence of convergence toward the law of one price between the fuels====. In contrast to this literature, we employ much more detailed data, which allows us to document the importance of price dispersion across stations (an important feature of automotive fuel markets; see, e.g., Lewis, 2008). In addition, we argue in this paper that because of the structure of the retail market for fuel in Brazil, price convergence should not necessarily occur.====The paper is organized as follows: Section 2 provides a brief summary of the general characteristics of the Brazilian fuel market. Section 3 presents a model of oligopolistic competition among fuel stations supplying both types of fuel, allowing us to establish some key predictions on how flex car diffusion affect equilibrium prices. Section 4 describes the data we use and provides descriptive statistics. In Section 5 we employ panel data methods to empirically verify the effect of flex car diffusion on prices and on the absolute price spread between gasoline and ethanol. We provide concluding remarks in Section 6.",Flex cars and competition in fuel retail markets,https://www.sciencedirect.com/science/article/pii/S0167718717301704,March 2019,2019,Research Article,270.0
"Bourguignon Hélène,Gomes Renato,Tirole Jean","MAPP, France,Toulouse School of Economics, CNRS, University of Toulouse Capitole, France,Toulouse School of Economics, IAST, University of Toulouse Capitole, France","Received 17 April 2018, Revised 15 October 2018, Accepted 19 October 2018, Available online 30 October 2018, Version of Record 20 November 2018.",https://doi.org/10.1016/j.ijindorg.2018.10.004,Cited by (13),"The proliferation of new payment methods on the Internet rekindles the old and unsettled debate about merchants’ incentive and ability to differentiate price according to payment choice. This paper develops an imperfect-information framework for the analysis of platform and social regulation of card surcharging and cash discounting. It makes three main contributions. First, it identifies the conditions under which concerns about missed sales induce merchants to perceive that they must take the card. Second, it derives a set of predictions about cash discounts, card surcharges and platform fees that shed light on existing evidence. Finally, it shows that the optimal regulation of surcharging is related to public policy toward merchant fees and substantially differs from current practice.","The payment industry is rapidly evolving. The proliferation of new payment methods on the Internet, such as PayPal and Bitcoin, and payment processing services that piggyback on card networks, such as Android Pay, Apple Pay, and Amazon Pay, challenges the business and regulatory models of traditional card payment systems, such as Visa, MasterCard and American Express. An important dimension of this reconsideration concerns the extent to which merchants should be allowed to price discriminate according to payment method. Absent public regulation on the matter, payment systems have prohibited merchants from surcharging relative to other payments methods, such as cash or other cards.====Merchants have long complained that their inability to pass through high merchant fees to consumers interfered with the proper working of a free market. Over the last decade, most regulators==== sided with the merchants and struck down the uniform-pricing==== mandate imposed by card systems, only to discover a few years later that substantial surcharges may end up being levied on unfortunate consumers. In response to abusive surcharging, the European Union, the UK and Australia all have proposed variations on the idea that surcharges should be limited to some variant of the notion of “cost of acceptance,” which includes the merchant fee plus possible various other costs.====Meanwhile, most regulators have capped the fees that card systems can levy on merchants. For example, the European Commission adopted as its benchmark for the regulation of merchant fees charged by Visa and MasterCard members the “tourist or avoided cost test”,==== according to which the merchant fee should not exceed the merchant’s convenience benefit of a card payment.==== The tourist test induces the cardholder to internalize the merchant’s welfare when choosing the payment method and is thus the incarnation of the Pigouvian precept in the payment context. Because the merchants’ surcharging behavior ultimately depends on the fees they pay on card payments, the merchant fee and surcharging regulations should not be designed separately, as is the case in many jurisdictions.====The purpose of this paper is to clarify the surcharging debate and determine how the regulation of surcharges should be related to public policy toward merchant fees. We develop a micro-founded framework for the study of payment-method-based price discrimination. We assume that consumers are imperfectly informed about the merchant’s “payment policy” (card acceptance, card surcharge, cash discount).==== Accordingly, the transaction costs per sale are shrouded from consumers’ sight (hence the title of the paper).====We obtain three main insights:====Section 2 sets up the model. The payment platform charges a merchant fee and a cardholder fee per card transaction. The merchant’s listed price is just a commitment to sell the good at that price through cash. Customers differ in their relative cost of a cash payment, and choose a payment method as a function of the platform’s cardholder fee and the merchant’s payment policy. A crucial ingredient of our analysis is that consumers are imperfectly informed about the merchant’s cash/card policy (and, in equilibrium, form rational expectations about it). This assumption is natural for retailers in industries characterized by one-time (or infrequent) shopping, such as travel (e.g., airlines and gas stations), tourism, and various retail/service sectors (e.g., durable goods), where surcharging more often occurs.==== More broadly, this assumption is likely to hold whenever consumers have bounded memory or are rationally inattentive regarding past purchasing experiences.====Section 3 considers the benchmark of “uniform pricing” or “price coherence”, where neither cash discounts nor card surcharges are permitted, and develops the notions of missed sales and must-take cards in our imperfect information setting. A missed sale occurs when the customer is in the shop and eager to buy, but has a high inconvenience cost of paying by cash, and is discouraged by either a high card surcharge or an outright rejection of the card. The importance of missed sales is underscored by the analysis of Bolt et al. (2010), who, using survey data from the Netherlands, document that 5% of consumers reported leaving a merchant’s store without purchasing when faced with card refusal or steep card surcharges. Given the relative magnitude of markups and merchant fees, such a fraction is likely to raise a significant concern for merchants.====Section 3 then relates the missed sales concern, and the concomitant low merchant resistance to high merchant fees, to the nature of the demand curve. An inelastic demand for the good calls for a high markup, and therefore makes the merchant particularly wary of missed sales. We characterize the maximum merchant fee that the merchant is willing to accept. Importantly, this ==== violates the tourist test and under weak conditions is selected by the platform.====Section 4 studies the merchant’s optimal payment-method-based price discrimination (cash discount and card surcharge). Because consumers imperfectly observe the merchant’s payment policy, a cash discount is a giveaway to consumers who already are in the shop. In contrast, a card surcharge holds up the consumer, who has made the specific investment to come to the store and inspect wares. While card acceptance is a no-brainer for a merchant whose surcharging behavior is unconstrained, it is not so when only cash discounts are permitted. In this case, steering customers away from using the card is costly for the merchant, and, as a result, merchants may refuse cards if merchant fees are sufficiently high. Yet, because cash discounts improve the merchant’s ability to avoid card fees, the card acceptance threshold is higher than under uniform pricing.====The merchant’s optimal cash discount (card surcharge) balances the marginal revenue from steering customers to pay by cash and the marginal cost (gain) directly generated by the discount (surcharge). A cash discount (card surcharge) is optimal for the merchant if and only if the merchant fee exceeds the ====. The price discrimination threshold is lower for a card surcharge, which brings in additional revenue to the seller, than for a cash discount, which benefits customers. As a result, card surcharges occur even when the tourist test is met, while cash discounts only occur if the merchant fee is sufficiently above the tourist test level.====Price discrimination then leads us to define ==== merchant and cardholder fees, which are the fees that merchants and consumers effectively pay once the merchants’ discount or surcharge are factored in. Because merchants use a card surcharge as a price discrimination device (screening consumers with different costs of cash payments), the merchant always “overshoots” in the surcharge, leading to an effective merchant fee that strictly satisfies the tourist test. In contrast, because a cash discount is a costly gift to consumers, the merchant always “undershoots” in the discount, leading to an effective merchant fee that strictly violates the tourist test. Furthermore, if given the choice, the merchant ==== prefers a card surcharge to a cash discount.====Section 4 also shows that, ==== the platform’s fee structure, allowing cash discounts raises welfare. By contrast, the welfare impact of card surcharges depends on the level of the merchant fee; if the latter satisfies the tourist test or even lies reasonably above the merchant’s convenience benefit, allowing card surcharge reduces welfare by excessively reducing card usage. For very high merchant fees by contrast, allowing card surcharges improves welfare.====Section 5 then studies how the platform optimally adjusts its fee structure in response to laws allowing merchants to practice cash discounts/card surcharges. We show that the ==== of cash discounts/card surcharges reduces the effective merchant fee and increases the cardholder fee (relative to uniform pricing). The magnitude of these effects is strictly higher in the case of surcharging, as a result of the “abusive” surcharges practiced by merchants. In particular, when surcharging is allowed, the effective merchant fee in equilibrium is below the tourist test level, and card usage is inefficiently low.====Interestingly, we show that the platform is able to achieve maximal card usage by selecting ==== merchant and cardholder fees that generate no card surcharges on the equilibrium path. Arguably, such fees should be the ones selected by the platform whenever surcharges involve any extra convenience cost for merchants. This implication is consistent with ample evidence documenting that public regulations authorizing card surcharging do not generate much actual surcharging.====Section 6 derives the optimal regulation of surcharging and shows that a surcharge cap set at or above the merchant fee is too lax. It derives the optimal cap. Section 6 then studies the impact of mandated transparency by the merchant. According to this regulation, the merchant would be required to post the surcharge(s) together with the price. Mandated transparency regulation eliminates hold-ups for attentive customers; however, it (i) may not be feasible (the consumer learns price through a national advertising campaign or a price comparison engine), (ii) may involve transaction costs for the merchant, (iii) does not address the existence of inattentive consumers, and (iv) does not prevent inefficient surcharging if the consumer’s willingness to pay is correlated with his desire to use the card (as we argue is likely to be the case). Thus transparency is not a perfect regulatory response to the inefficiencies attached to surcharging, and the two regulations may well be complementary.====Section 7 summarizes the main insights and policy implications and concludes with evidence consistent with various predictions of the theory. Proofs that are missing in the text can be found in the technical appendix.====Our study of surcharging is related to the literatures on add-ons (Shapiro, 1995, Ellison, 2005) and on shrouded attributes and incomplete contracting (Gabaix, Laibson, 2006, Tirole, 2009). In these one-sided market contributions, sellers benefit ex ante from a regulation of add-on overpricing, but cannot refrain ex post from imposing one, when the consumers rationally anticipate add-ons (perhaps because they have had similar experiences in the past). Sellers may benefit even ex ante from add-on overpricing when consumers are naïve or unaware of the existence of the add-on, that is, either do not think about the possibility of ex-post hold up, or are inattentive in that they fail to take measures (such as tracking their bank balance to avoid overdrafts) that protect them from seller hold ups.====A large literature looks at add-ons or tied aftermarket sales in environments mixing sophisticated and naïve customers. The emphasis in this literature, reviewed by Armstrong and Vickers (2012), is on externalities between the two groups of consumers. Our paper does not investigate redistributive consequences under a mixed population of users. Nor does it analyze the consumers’ incentives to become informed, or forms of regulation that aim at educating naïve consumers.====While the impact of add-ons in one-sided markets is now well-understood, little is known regarding add-ons in two-sided markets. In a two-sided market, the platform intermediating transactions between the buyer and the seller can affect the seller’s behavior, indirectly through the fees charged for using the platform’s services, or directly by regulating the pricing of the add-on. This yields a richer and more complex environment for the study of add-ons, leading to business strategy and regulatory insights that substantially differ from those gleaned from one-sided market theory. Another difference with the earlier literature is that in the latter missed sales concerns do not arise in the following sense: The seller first sells the basic good and then may or may not sell the add-on. In our paper, by contrast, the price of the add-on (the card transaction) is learned before the sale of the basic good is completed. Hence the basic good itself may not be traded because of excessive add-on prices.====The relevance of merchant fees under cash discounts or card surcharges in our paper contrasts with the neutrality result of Carlton and Frankel (1995), Rochet and Tirole (2002) and, in more generality, Gans and King (2003). These papers have shown that, under perfect consumer information, the choice of merchant fee has no impact on the real allocation if surcharging is allowed. Imperfect information generates imperfect passthrough, which explains the difference in results.====Another contribution to the payments literature is to bring life to common policy issues that have eluded previous analysis. A case in point is missed sales, which is a prominent concern for merchants. Because perfectly informed consumers would never come to the shop and not purchase the item, missed sales have not been accounted for in the economics literature.==== Finally, the current literature provides limited guidance to assess the conventional wisdom according to which allowing surcharging is an antidote to abusive merchant fees.====The recurrent lawsuits against card associations are predicated on the “must-take-card” argument, which merchants cannot turn down the card even when the merchant fee exceeds the merchant’s card convenience benefit (Vickers 2005). The standard modeling of the must-take-card argument is based on the idea that card acceptance makes the merchant more attractive to consumers, so that payment platforms can charge a merchant fee beyond the socially efficient level (Rochet and Tirole 2002, and Wright 2012). The attractiveness channel hinges on the consumers being perfectly informed about the merchant’s cash/card policy. By considering the other polar case of uninformed consumers, this paper completely shuts down the attractiveness channel. It therefore contributes to the literature by identifying a novel (and independent) channel for must-take cards: the merchant’s concern about missed sales.====Other papers have investigated the impact of surcharging card transactions.==== Assuming perfect consumer information about the merchant’s payment policy and a perfectly inelastic demand for the good, Wright (2003) concludes that lifting the no-surcharge rule decreases social welfare, and may completely shut down the card payment network. By contrast, among other differences, we assume that the demand for the good is elastic, and that consumers are imperfectly informed about the merchants’ payment policy (what generates the missed sales concerns described above). This leads to different results: We show that allowing for card surcharging is welfare-enhancing if and only if the merchant fees are high. We also show that lifting the no-surcharge rule, while properly regulating the merchants’ behavior through surcharging caps, can lead to substantive welfare improvements relative to uniform pricing.====Schwartz and Vincent (2006) consider a model with fixed populations of cash and card users. Lifting the no-surcharge rule is shown to increase social welfare provided the fraction of cash users is sufficiently small. By assuming that the choice of payment method is exogenous, their model ignores the cash/card steering effects that are central to our analysis. More recently, Edelman and Wright (2015) consider the problem of an intermediary (such as a booking website, a financial brokerage firm, or a card payment platform) that may invest to provide a non-pecuniary benefit to consumers. They show that, under uniform pricing, the intermediary over-invests in the provision of the non-pecuniary benefit, and can actually decrease social welfare. By assuming perfect consumer information, these works abstract from missed sales and the hold-up issues that lie at the core of the present article.====The most closely related paper is Gomes and Tirole (2018). The two papers are complementary. Gomes and Tirole provides a general theory of the pricing of ancillary goods with informed, uninformed-but-rational (as in this paper), and naïve customers, and focuses mostly on one-sided markets. It also contains a detailed study of how a supplier with market power can exploit the merchant’s cost-absorption strategy. It derives regulatory interventions that are robust to the type of clientele and to the industry structure.====Conversely, the current paper makes four contributions relative to Gomes and Tirole (2018), three of which are related to the specificities of the payment card market. First, besides its focus on card platforms, it considers the case of “price coherence” (where card surcharges are ruled out by the payment system). Second, it focuses on an open-system that maximizes the volume of card transactions, while Gomes and Tirole considers a for-profit monopoly platform that maximizes profits. Both paradigms are relevant for policy, depending on the country and market conditions. Third, it derives in this setting a new foundation for the “must-take card” argument, which is applicable to the many jurisdictions where the no-surcharge rule is still in place. Fourth, it studies the merchants’ incentive to price discriminate according to payment method. It demonstrates a fundamental asymmetry between cash discounts and card surcharges (discounts are impractical in many one-sided markets, and are particularly interesting in the card payment application, as payment platforms have traditionally allowed cash discounts while prohibiting surcharges).","Shrouded transaction costs: must-take cards, discounts and surcharges",https://www.sciencedirect.com/science/article/pii/S0167718718301024,March 2019,2019,Research Article,271.0
"Byford Martin C.,Gans Joshua S.","School of Economics, Finance and Marketing, RMIT University, Australia,Rotman School of Management, University of Toronto, Canada,NBER, United States","Received 2 February 2018, Revised 16 October 2018, Accepted 23 October 2018, Available online 30 October 2018, Version of Record 10 November 2018.",https://doi.org/10.1016/j.ijindorg.2018.10.005,Cited by (2),"We provide a new model wherein firms of different productivities survive in an ==== despite the threat of entry by high productivity firms. We demonstrate that an efficient incumbent has a unilateral incentive to establish a relational contract, softening price competition to strengthen its inefficient rival in a war of attrition that emerges post-entry, and raising the price of the inefficient firm in the acquisition market. We show that this equilibrium gives rise to persistent performance differences, market compression, and stability in the identity of firms in the market. Moreover, the relational contracting equilibrium is facilitated by strong anti-trust laws.","A rich literature in antitrust economics—going back to Adam Smith—notes that a firm has every interest in preserving weak competitors within a market, when the alternative is the emergence of a strong rival. A common feature of many prominent oligopoly models is that a firm tends to be more profitable when facing weak (e.g., high marginal cost) competitors. However, an as yet relatively unexplored question in the literature is: Can a firm ensure that it faces a weak rival, instead of a stronger potential adversary, without resorting to behaviors that would violate antitrust laws?====A guiding presumption in much of industrial organization is that, in the absence of prohibitive barriers to entry, firms with low productivity will be replaced by more efficient rivals. A weak incumbent becomes a target for acquisition if an entrant can use its assets more effectively. Likewise, a weak incumbent may be driven out of a market if increased competition, resulting from the de novo entry of a stronger rival, leaves the incumbent unable to recoup its fixed costs.====Mechanisms by which a firm can ‘choose the competition’ have previously been proposed for certain specialised settings: For example, Rockett (1990) develops a model in which a patent holder is effectively able to choose its rival following the expiry of the patent, by selectively licensing the patent to a weak rival during the patent’s lifetime.==== Byford and Gans (2014) show that a strong incumbent can protect a weak rival when entry into the market can only occur via acquisition. By unilaterally raising its price, the strong incumbent directs a stream of profits to its rival, increasing the value of the weak incumbent in the acquisition market beyond an entrant’s willingness-to-pay.====In this paper, we extend the model of Byford and Gans (2014) to an environment in which both entry by acquisition, and de novo entry, are possible. Specifically, we consider a ‘natural duopoly’—a market in which at most two firms can operate profitably—wherein de novo entry by a third firm necessarily initiates a war of attrition. Success in a war of attrition depends both on the relative losses firms endure during the war, and the relative profits firms earn in the event that they emerge victorious.====A key insight of this paper is that the firm profits that determine success in a war of attrition are themselves endogenous—a product of the equilibrium of the game—and can be manipulated via strategic behavior of the market participants. We describe a form of unilateral conduct that can be implemented by an efficient incumbent firm with the purpose of enhancing the ‘type’ of its weak rival in a war of attrition. By softening competition with its weak rival when no other firms are present in the market, the efficient incumbent increases the value to its weak rival of prevailing in war of attrition with an entrant. If the weak firm’s incentive to endure interim losses during a war of attrition can be raised sufficiently, we show that the efficient incumbent’s unilateral conduct will deter de novo entry by stronger potential competitors. Moreover, we show that the conduct has the added benefit of raising the weak firm’s value in the acquisition market, thereby preventing entry by acquisition in the manner of Byford and Gans (2014).====Notably, the unilateral conduct of the efficient incumbent gives rise to persistent performance differences (PPDs) in equilibrium.==== Syverson (2011) shows that initial productivity differences between firms can persist in a market where (a) firms have natural limits on their size (say, due to diseconomies of scale) that prevent expansion, and (b) entry barriers are high.==== The mechanism we describe demonstrates how these factors can emerge ‘strategically’ in markets where they would not otherwise be sufficient to sustain PPDs. Specifically, by improving the ‘type’ of its weak rival in a war of attrition, the efficient incumbent creates a strategic barrier to entry that greatly exceeds the cost of entering the market (which can be arbitrarily small in our model). Similarly, even where it would be myopically optimal for the efficient incumbent to undercut its weak rival, forcing it out of the market, the efficient firm will not do so. Instead, the efficient firm will actually sacrifice market share to its weak rival as it requires the presence of the weak firm to deter entry by a stronger competitor.====The paper proceeds as follows: In Section 2 we set-out our infinite-horizon model of competition in a natural-duopoly market. We describe the operation of the wars of attrition that arise when a third firm enters the market de novo. And we also describe a mechanism whereby an entrant can enter the market via acquisition of an incumbent.====In Section 3, we derive the Markov perfect equilibrium of the model, demonstrating that there exists an equilibrium of that game whereby an inefficient firm is replaced by a more efficient entrant as per the usual intuition for this market structure.====Section 4 presents the main proposition of the paper, and establishes how an efficient incumbent’s unilateral conduct can facilitate its weak rival’s ongoing presence in the market. We provide conditions under which this equilibrium exists. We then conduct comparative statics on firm performance.====In Section 5 we examine three novel implications of the model. We demonstrate that the mechanism we describe is associated with stability in the identities of the firms in the market and with market compression. This second result is consistent with the Collard-Wexler (2011) finding that the dispersion in profitability of concrete plants was less than the dispersion in their measured productivity. Third, we argue that the strength of anti-trust policy matters in a distinct way: A strong activist anti-trust authority facilitates our mechanism by preventing conduct (such as collusion and predatory pricing) that would deliver the efficient incumbent higher profits. A final section concludes.",Strengthening a weak rival for a fight,https://www.sciencedirect.com/science/article/pii/S0167718718301048,March 2019,2019,Research Article,272.0
"Shen Bo,Wright Julian","Economics and Management School, Wuhan University, China,Department of Economics, National University of Singapore, Singapore","Received 2 January 2018, Revised 3 May 2018, Accepted 14 May 2018, Available online 30 October 2018, Version of Record 8 May 2019.",https://doi.org/10.1016/j.ijindorg.2018.05.005,Cited by (9),"When consumers rely on an intermediary’s advice about which firm to buy from but can switch to buying directly after receiving advice, one might expect firms to discount their direct prices to encourage consumers to purchase directly after obtaining advice, thereby avoiding paying commissions. We provide a theory which can explain why firms often do not free ride in this way, as well as when they do. The theory can explain why online marketplaces and hotel booking platforms impose price-parity clauses to prevent such ====, while insurance and financial advisors do not.","Firms that compete to attract uninformed consumers typically pay commissions or kickbacks to information intermediaries with the aim of influencing the intermediaries’ advice to these consumers and so their chance of a sale. A broker for insurance or financial products may advise consumers which insurance or financial product they should purchase. A physician may advise consumers which drug to take. A retailer may provide advice to shoppers for experience or credence goods over which manufacturer’s product they should purchase. An online platform may recommend which seller is best for the consumer. In all these cases, it is conceivable that after receiving the recommendation, consumers could bypass the intermediary and buy directly from the product provider if this allows them to purchase at a lower price. Given this possibility, one might expect firms to free ride on the intermediary’s advice—discounting their direct prices to encourage consumers to purchase directly after obtaining advice, thereby avoiding paying commissions.====This type of free riding seems to arise in online platform settings, and is the main justification given by such platforms for their use of price-parity clauses to prevent firms charging less when selling directly (e.g. Amazon, Booking.com and Expedia, have all used such clauses as detailed in Edelman and Wright, 2015a and Hviid, 2015). However, in the case of insurance, financial and medical products, intermediaries do not impose any such contract restrictions, and yet firms typically do not discount prices for direct sales relative to intermediated sales (see Section 10 of Edelman and Wright, 2015a, for evidence on insurance and financial products).====This paper provides a theory to make sense of these observations. It explains why when firms set commissions, firms will set their direct and intermediated prices to be equal in equilibrium even when intermediaries do not restrict the firms’ pricing in any way. It also explains why when commissions are set by intermediaries, an intermediary may instead need to rely on price-parity clauses to eliminate any discounts for direct sales. Thus, it can potentially reconcile the different observed outcomes across different industries. By doing so it provides policymakers with a more nuanced view of price parity—the phenomenon that direct and intermediated prices are the same. It shows that price parity is not necessarily caused by restrictive price-parity clauses.====The model we use is adapted from Inderst and Ottaviani (2012a). In their framework, firms compete by setting commissions which are paid to an intermediary. The intermediary has a better signal about which of the two firms’ products is more suitable for consumers and can issue a recommendation to consumers about which product to purchase. The intermediary’s advice is assumed crucial for trade—without advice, the expected match value is not sufficient, so consumers will never buy from one of the firms even if the firms price at marginal cost. The intermediary compares the commissions it obtains from each product but also takes into account which product is more suitable due to liability, ethical or reputational concerns. Firms set product prices, taking into account the advice consumers receive. In equilibrium, advice is informative and consumers rationally follow the intermediary’s recommendation. We adapt this model by allowing that after obtaining advice, consumers can switch channels, purchasing directly from one of the firms potentially at a different price (e.g. at a discount). We assume consumers are heterogeneous in the cost they incur from switching channels so that the proportion of consumers switching to purchase directly is increasing in the discount offered.====To understand the logic for why firms do not offer discounts for direct sales in this setting, suppose to the contrary that initially they do. Lowering this discount increases the firm’s margin on the inframarginal consumers who still continue to buy directly. This increase in the amount the firm collects from the inframarginal direct consumers has no bearing on the intermediary or its recommendation, and so is a pure gain for the firm. On the other hand, any loss in the firm’s margin as a result of some additional consumers now purchasing through the intermediary at positive commission fees can be offset with a lower commission fee to leave the firm and the intermediary unaffected, as the expected commission revenue paid by the firm to the intermediary remains the same. Thus, on net the firm is better off reducing its discount on direct sales from any positive level, and lowering commissions by an offsetting amount.====This logic highlights that reducing commissions is better for firms than giving discounts, as a way to reduce the amount paid to the intermediary. This reflects that commissions are better targeted. Discounts involve payments to inframarginal consumers who would anyway buy directly. We show this argument, which we first develop assuming commissions are observable, continues to hold even when consumers do not observe commissions and so there is the potential for a firm to use a discount as a positive signal of the expected quality of its product. We establish this result under two extremes of beliefs: (i) consumers have naive beliefs, so they do not update their beliefs about the quality of the firm’s product implied by the intermediary’s recommendation based on out-of-equilibrium prices, and (ii) consumers are sophisticated in updating their beliefs (in a sense we will make clear later). We also show the argument holds when consumers are naive in that they always assume the recommended firm is more suitable regardless of what prices and/or commissions are observed.====The ability of firms to offset a lower discount with a lower commission, and do better, depends on the assumption that firms set commissions. If instead it is the intermediary that sets commissions, our equivalence argument between commissions and discounts breaks down, and we show firms will always want to offer positive discounts in response to high commissions. In this case, the intermediary will want to impose a price-parity restriction on firms to prevent them discounting for direct sales.==== This allows the intermediary to shift surplus from firms to itself.====Taking these results together, our theory provides predictions on when free riding is more likely to be a problem for advisors and when it is not. Similarly, it predicts when price-parity clauses will be used and when they are redundant. Specifically, to the extent that commissions seem to be set by firms in certain industries (such as the sale of traditional insurance, financial and medical products by brick-and-mortar advisors) and set by intermediaries in other industries (such as the online platforms like Amazon and Expedia)====, our theory explains why price-party clauses are used in the latter online settings but not the former. At the end of the paper, we will conjecture a possible explanation for why commissions might tend to be set by intermediaries in online settings, but not when intermediaries need to provide advice by meeting consumers physically. In brief, the idea is that in contrast to an online setting, in an offline setting each intermediary may only be able to deal with a small number of consumers, in which case there may be many more intermediaries (advisors) than firms, and so it is more natural that the firms rather than the intermediaries set commissions. Combining this explanation with the predictions of our theory, we tie the incidence of free riding and price-parity clauses to the ability of intermediaries to use modern communication technologies to give advice on a mass scale.====The rest of the paper proceeds as follows. Section 2 briefly discusses related literature. Section 3 details the model, and provides some preliminary analysis. Section 4 establishes what happens when firms set commissions, while Section 5 analyzes the setting in which the intermediary sets commissions. Finally, Section 6 concludes.",Why (don’t) firms free ride on an intermediary’s advice?,https://www.sciencedirect.com/science/article/pii/S0167718718301036,May 2019,2019,Research Article,273.0
"Rojas Christian,Briceño Arturo","Department of Resource Economics at the University of Massachusetts Amherst, United States,Business Economics Consulting LLC, MI, United States","Received 18 May 2017, Revised 21 June 2018, Accepted 9 July 2018, Available online 26 October 2018, Version of Record 12 November 2018.",https://doi.org/10.1016/j.ijindorg.2018.07.003,Cited by (3), in the absence of illegal providers.,"While empirical industrial organization and antitrust studies have propagated in many industries and across the world, their use has been limited in developing countries. Developing countries have a myriad of limitations that thwart these endeavors, including data (un)availability, little or no antitrust enforcement, and fragile institutions. In this paper we focus on one important element that is present across industries in much of the developing world and that can have important implications for empirical analyses of markets: the presence of illegal competitors. Illegal provision can be defined in numerous ways. This can range from the provision of an illegal/banned product (e.g. drugs) where all the transactions that occur are outright illegal, to the provision of a legal product by a legally constituted firm that does not fully respect the law (e.g. a car manufacturer that holds all necessary permits for production but manages to evade carbon emissions regulations).====In this paper, we are interested in a market where the service itself is legal (TV subscription), but there are a number of providers that sell it through different illegal means (we later detail the methods through which this occurs). In this paper we refer to this activity as ====, but use the term piracy and illegality interchangeably throughout the text. Piracy in subscription TV in Perú is rampant: 50% of subscription TV users are estimated to obtain their subscription TV signal through an illegal provider (in Latin America this statistic sits at 30%; Alianza, 2015). Data for other countries in the developing world suggest this problem is also widespread. In 2011, an estimated 92% of TV subscriptions in Arab countries were considered to be illegal while it is estimated that cable and satellite operators in China (nearly 50% of TV subscriptions are in Asia; Satellite Markets, 2010) lose nearly $ 2 billion/year due to piracy (Havocscope, 2017).====A central feature of our study is the availability of data on illegal TV subscriptions. Specifically, we make use of a representative household survey administered by the Peruvian telecom regulator in which households provided information that allows us to determine, for a large fraction of households, whether the TV content supplier is illegal. Further, the survey contains information regarding the service, including the price paid as well as several characteristics (e.g. basic v. premium content; cable v. satellite; etc.).====Official statistics (which exclude illegal connections) indicate that 63% of subscription TV users are served by the leading operator (Telefónica). Employing the typical approach of using market share to proxy for market power, antitrust authorities might deem this company as dominant (or to have significant market power). Contrary to this conclusion, if one includes illegal connections as part of the relevant market, Telefonica's market share falls by more than half, to 34%. This stark difference in market share suggests that (ignoring the usual pitfalls of using market share as proxy for market power) antitrust authorities may obtain an erroneous conclusion about the existence of a dominant firm in the market.====This large difference in market shares between two different market definitions (one containing illegal providers, the other one excluding them) motivates us to study the impact of piracy on competition using quantitative antitrust tools (demand estimation, Lerner Indices and diversion ratios). Our estimates suggest that, after controlling for all product characteristics that drive demand (as well as endogeneity), not accounting for piracy leads to erroneous conclusions regarding the determination of substantial market power and the delineation of the relevant (antitrust) market. Specifically, we find that mark-ups of the three leading (legal) operators (which together capture 92% of the legal segment) are relatively modest and similar across firms (between 33.5% and 39.7%); interestingly, the largest firm, Telefónica seems to have limited market power as it registers the lowest Lerner Index (33.5%). While own-price elasticities (and the implied Lerner Indices) are suggestive of the degree of market power in the industry, diversion ratios provide direct evidence regarding the competitive pressure that legal providers (especially Telefónica) face from piracy. To illustrate this point, Telefónica's diversion ratio to illegal providers (13.1%) is of similar magnitude than that observed towards its next two largest legal competitors (11.7% to 14.7%).====To drive our point further, we re-estimate demand and Lerner Indices by removing pirate connections from the choice set (i.e. ignoring their existence) and find that, under this (incorrect) counterfactual, the leading operator would register the highest mark-up of all three leading operators thereby reversing the finding that is obtained when piracy is accounted for in the analysis. The intuition for this result is straightforward: if an important competitor is not accounted for in the analysis (in this case a fringe of illegal providers), the estimation fails to capture the competitive pressure that the removed competitor exerts over other firms.====While the survey information allows us to determine whether some connections are illegal, we cannot do so for all survey respondents.==== This means that our empirical analyses utilize a conservative measure of piracy. This feature implies that our results would only be strengthened if we had more precise information regarding piracy. Specifically, if piracy were completely accounted for in the analysis, legal operators’ market power would be even lower than what we estimate.====While we use the Peruvian subscription TV market as a case study, the widespread prevalence of various variants of illegal provision of goods and services in the developing world (e.g. informal markets, copyright infringement, counterfeiting, etc.) suggests that our findings may have important implications elsewhere. More generally, our results highlight a tension that policy makers in developing countries might encounter. On the one hand, turning a blind eye on the illegal provision of certain goods and services (e.g. copyrighted digital material, reversed-engineered patent-infringing medicines) may result in a (short-term) benefit since the market power of law-abiding firms might be constrained thereby resulting in increased consumer welfare. Conversely, strict enforcement of the law can afford the (long-term) benefit of providing the right environment (incentives) for increased investment and, thus, the delivery of services; this option, however, may come at the cost of a large (short-run) loss in consumer welfare (e.g. Chaudhuri et al., 2006).====We are not aware of empirical studies using structural methods that carry out similar competition analyses in the presence of piracy. A related literature consists of reduced-form empirical studies that investigate the effects of digital piracy (i.e. counterfeit DVDs, illegal downloading, file-sharing) of video and/or audio content on industry performance. A typical study in this literature aims to find the causal relationship between piracy intensity on the music (or motion picture) industry, and sales. Earlier studies focused on hard good piracy (hard copies of the material such as DVDs or CDs). More recently, the focus has been on soft goods piracy (file sharing, illegal downloading). The results of most of these studies favor the hypothesis that piracy has had a negative effect on industry sales (e.g. Rob and Waldfogel, 2006, Zentner, 2006).====Two other studies are also related to our work; both endeavors estimate demand for differentiated products to study the role (or effect) of an informal/illegal fringe. Salvo (2009) studies competition in the Brazilian soft drinks industry with a focus on the emergence of a competitive fringe of informal suppliers characterized by a small-scale operation and low prices. Salvo uses the estimated demand parameters to evaluate whether the low prices offered by the fringe are the result of: (a) tax avoidance by the informal competitors (as claimed by the incumbent large-scale manufacturers), or (b) a larger price sensitivity (and hence lower market power) for the fringe products. Salvo finds evidence for the latter hypothesis; further, and related to our study, Salvo reports evidence that is consistent with the fringe imposing important competitive pressure on the established soft drinks companies. Conversely, Chaudhuri et al. (2006) use the demand estimates to study the welfare effects of banning patent-infringing versions of anti-bacterials in India. The authors find that, because of the low prices offered by the patent-infringing products (and their corresponding competitive pressure on patented versions), consumer welfare would dramatically decrease as a consequence of the ban (prices could increase between 100% and 400%).====Finally, there is a set of papers that have set out to estimate demand models for subscription TV in North America and Europe to address a variety of empirical questions. Chipty (2001) uses demand elasticities in a model aimed to study the effects of vertical integration in the U.S. cable TV industry. Goolsbee and Petrin (2004) use their estimates to study the competition and welfare effects of satellite TV entry in the U.S. market (previously dominated by cable technology).==== Crawford and Yurukoglu (2012) estimate a model for the U.S. to study the welfare effects of bundling in the U.S. subscription TV market. Pereira et al. (2013) carry out demand estimation exercises similar to ours to investigate whether bundled services in the Portuguese telecom industry are part of the same relevant market as stand-alone subscription TV. Finally, Byrne (2015) obtains demand estimates in the Canadian cable TV industry to study whether the marginal costs implied by different models of competition are validated by actual cost data. As we later discuss, our paper shares (and borrows from) methodological features of some of this literature, in particular Goolsbee and Petrin, 2004, Petrin and Train, 2010 and Pereira et al. (2013).====The paper proceeds as follows. Section 2 provides an overview of the subscription TV market in Perú and other important institutional details. Section 3 describes the model and provides a discussion of how we identify illegal connections. Section 4 describes the data and discusses the results. Section 5 concludes.",The effects of piracy on competition: Evidence from subscription TV,https://www.sciencedirect.com/science/article/pii/S0167718717303090,March 2019,2019,Research Article,274.0
"Ciliberto Federico,Watkins Eddie,Williams Jonathan W.","Department of Economics, University of Virginia, USA,Department of Economics, UNC - Chapel Hill, USA","Received 2 August 2017, Revised 23 May 2018, Accepted 26 July 2018, Available online 7 August 2018, Version of Record 11 January 2019.",https://doi.org/10.1016/j.ijindorg.2018.07.008,Cited by (24),"We formulate two empirical tests for collusive behavior based on the theoretical insights of Werden and Froeb (1994) and Athey, Bagwell, and Sanchirico (2004). The first predicts that colluding firms will reduce pair-wise differences in prices within a market if demand satisfies certain properties. The second predicts that colluding firms will sacrifice efficiency in production by increasing price rigidity to avoid informational costs. Using panel data from the US airline industry and fixed-effects estimation, we find that greater multimarket contact between carriers leads to pricing patterns consistent with both theoretical predictions, while code-share agreements are consistent with the second prediction.","Detecting collusion is a central theme of research in empirical industrial organization (Jacquemin, Slade, 1989, Porter, 2005, Harrington, 2008). Collusion can lead oligopolistic firms to achieve monopolistic outcomes, leading to reduced and inefficient equilibrium output, higher prices, and lower consumer welfare.==== In the US, collusion is prohibited under the Sherman Act. Under Section 1 of the Sherman Act, any cartel or cartel-like behavior is “per se” illegal. Tacit collusion is instead evaluated under the rule of reason. Under this standard, the behavior is illegal only if it results in an unreasonable restraint on trade.====Previous empirical work has identified collusive behavior by using variation in costs (Rosse, 1970, Panzar, Rosse, 1987, Baker, Bresnahan, 1988, Weyl, 2009), rotations of demand (Bresnahan, 1982, Lau, 1982), taxes (Ashenfelter and Sullivan, 1987), conduct regimes (Porter, 1983), and product entry and exit (Bresnahan, 1987, Nevo, 2001, Salvo, 2010).==== In this paper, we propose two empirical tests to identify collusive behavior. We then use panel data from the US airline industry to test whether multimarket contact and code-share agreements are associated with the collusive pricing patterns predicted by the theory underlying the tests.====The idea that relates collusion to multimarket contact originates in Bernheim and Whinston (1990): multimarket contact between firms serves to pool the incentive constraints from all the markets served. That is, the more extensive is the overlap in the markets that the two firms serve, the larger are the benefits of collusion and the costs from deviating from a collusive agreement, which makes collusion easier to sustain. If, for example, two firms interact in many markets, they know that if they deviate from collusive behavior in one market, they will be punished by the other firm in all the markets where they interact. Evans and Kessides (1994) were the first to empirically test this hypothesis using data from the US airline industry, an ideal setting given the relatively small number of airlines that interact in a large number of markets. Like many studies that followed this seminal work, Evans and Kessides (1994) find that average prices in a market increase with the degree of multimarket contact among firms.====Like multimarket contact, government regulators have expressed concern that code-share agreements may facilitate collusion (US Department of Transportation, 2003). These agreements allow each airline to sell seats on a partner’s aircraft as if it were effectively their own. To make such an agreement operational requires substantial communication and coordination that could facilitate explicit or tacit collusion among the partner airlines. To date, there is only limited empirical analysis to support this hypothesis. Gayle (2008) studies the Delta-Continental-Northwest code-share agreement but does not find evidence that average prices rise as a result of the agreement.====In the spirit of the methods for distinguishing collusion from competition discussed in Harrington (2008), the two tests we propose focus on more nuanced pricing patterns that can result from collusive behavior. Specifically, the tests examine whether the presence of a potential facilitator of collusion (i.e., greater multimarket contact or a code-share agreement) is associated with smaller price differences between pairs of firms in a market and less variable prices in a market over time, respectively.====The first test is based on the theoretical insight of Werden and Froeb (1994) that mergers will alter the ==== between prices in a market with differentiated products in particular ways. Specifically, for logistic demand, pair-wise differences between products’ prices decrease with the degree that firms internalize the effect of a price change on the profit of competitors (i.e., merge or collude). We show that this pricing pattern associated with collusion extends to other demand models commonly used in academic studies and antitrust enforcement, including variations of the logit model that relax the independence from irrelevant alternatives (IIA) property and linear demand systems. Therefore, if greater multimarket contact between a pair of firms facilitates collusion, one should observe that pair-wise differences in prices decrease in multimarket contact. Similarly, if code-share agreements facilitate collusion, pair-wise differences in prices between partner airlines should be smaller.====The second test is based on the theory developed in Athey et al. (2004), which predicts colluding firms will sacrifice efficiency in production by increasing fare rigidity to avoid informational costs associated with enforcement of a collusive agreement. Specifically, to avoid costly monitoring or price wars, firms’ prices cannot respond to firm-specific shocks, resulting in a misallocation of production across firms. Similar to the Werden and Froeb (1994) test, if multimarket contact facilitates collusion between firms, one should observe a reduction in the variation in two products’ prices as multimarket contact increases. Similarly, code-share agreements should result in less variable prices over time in a market if the extensive cooperation to operationalize the agreement facilitates collusion.====To conduct the empirical analysis, we use data from the Airline Origin and Destination Survey (DB1B) from 1993–2016. The DB1B is a 10% sample of domestic itineraries, which includes information on the fare paid, connections made, and ticketing carrier. We complement this data with information on code-share agreements from Ciliberto et al. (2018). We organize the data so that the fundamental unit of observation is a pair of airlines in a market. Next, we compute pair-wise measures of the differences in average fares, variability in fares (i.e., coefficient of variation), and multi-market contact. We also create an indicator for whether a code-share agreement exists between pairs of carriers. To ensure the robustness of our results, we consider a variety of controls and fixed effects, and also alternative measures of multimarket contact.====In our baseline analysis, the measure of multimarket contact equals the total number of markets that a pair of airlines serve concomitantly in a given period (i.e., year-quarter). For example, if American and Delta serve 200 markets in common, then this variable is equal to 200 for the American-Delta pair. This is consistent with Ciliberto and Williams (2014), which uses pair-wise counts of markets. It is also similar to studies that aggregate the pair-wise measures to construct market-specific measures of average multimarket contact (e.g., Evans and Kessides, 1994). In addition, we explore alternatives like revenue-weighted measures of multimarket contact that account for the relative importance of certain markets for carriers.====Our analysis consists of two sets of regressions, corresponding to the testable theoretical predictions from Werden and Froeb (1994) and Athey et al. (2004). To test the first, we run the differences in pair-specific prices on the corresponding pair-specific measure of multimarket contact and code-share agreement indicator, for which the unit of observation is the market-year-quarter-carrier pair. To test the second, we use a market-carrier pair unit of observation and regress the coefficient of variation of fares over time on the mean of pair-specific multimarket contact and code-share agreement indicators. For both tests, we include a variety of fixed-effects and time-varying market-level controls.====Regarding multimarket contact, we find the theoretical predictions of both Werden and Froeb (1994) and Athey et al. (2004) are consistent with our data. An increase in pairwise multimarket contact is associated with an economically and statistically significant decrease in the difference between fares and the variability of fares. These results are consistent across all specifications for both tests. We also find evidence that code-share agreements are associated with economically and statistically significant reduction in the variability of fares but no effect on the difference between fares. Taken together, the results provide evidence of collusion among airlines with high levels of multimarket contact, and some support for the hypothesis that code-share agreements may facilitate collusion.====Our findings generally support the recent antitrust efforts by the US Department of Justice (DOJ) to investigate the competitive conduct of airlines,==== and class-action lawsuits against US carriers alleging that carriers coordinated to restrict capacity and raise prices.==== Also, the empirical analysis provides a framework for future analyses of industries where a facilitator of collusion, like multimarket contact or code-share agreements, can be identified and data are available to study its relationship with price.====Our work contributes most directly to two literatures. First, we contribute to the empirical literature on collusive behavior in the airline industry, which is fairly sparse, especially when compared to the literature on acquisitions and mergers. Brander and Zhang (1990) estimate a structural model of competition among US airlines that includes a conduct parameter. They find that the cross-sectional data from the 1985 US airline industry are more consistent with a Cournot than either a competitive or cartel model. Brander and Zhang (1993) extend their analysis to a dynamic setting and reach similar conclusions.====More recently, Miller (2010) investigates the effects of the Department of Justice’s lawsuit against eight major domestic airlines and the Airline Tariff Publishing Company to see how prices and output choices change in response to the investigation. Miller (2010) finds that average prices fell in response to the investigation but increased following the settlement, implying that the airlines likely returned to their collusive behavior after the settlement. Zhang and Round (2011) investigate price wars and collusion in China’s airline markets, and find that both tend to occur but are short-lived. Brueckner and Picard (2013) investigate, theoretically, whether antitrust immunity among international airlines facilitates collusive behavior, and demonstrate the dependence of the results on the demand specification and economies of density. Following the DOJ’s recent investigation into airlines’ conduct, Aryal et al. (2017) examine whether airlines use earnings announcements to coordinate capacity reductions in an effort to increase fares.====Second, we contribute to the growing literature that studies the impact of multimarket contact on the strategic decisions of firms. Feinberg (1985) represents an early empirical study of the role of multimarket contact in sustaining supra-competitive prices, while Feinberg (1985) provides supporting experimental evidence. After these two early studies and the theoretical formalization of the idea in Bernheim and Whinston (1990), numerous empirical studies of the relationship between multimarket contact and collusion followed. In the airline industry, Evans and Kessides (1994); Singal (1996); Bilotkach (2011), and Ciliberto and Williams (2014) represent contributions to this literature. These studies focus on average prices, not the more nuanced pricing patterns in our tests, and Ciliberto and Williams (2014) is the only one to examine pair-wise multimarket contact. Related studies in other industries include cement (Jans and Rosenbaum, 1997), mobile telephones (Parker, Roller, 1997, Busse, 2000), banking (Pilloff, 1999), hotels (Fernandez and Marin, 1998), and radio (Waldfogel and Wulf, 2006).====The remainder of the paper is organized as follows. The data are described in Section 2. Section 3 develops the empirical tests, and presents and discusses the results. Section 4 concludes and discusses possible extensions of our research.",Collusive pricing patterns in the US airline industry,https://www.sciencedirect.com/science/article/pii/S0167718717304125,January 2019,2019,Research Article,275.0
"Aravena Olivia,Basso Leonardo J.,Figueroa Nicolás","Civil Engineering Department, Universidad de Chile, Casilla 228-3, Santiago, Chile,Instituto de Economía, Pontificia Universidad Católica de Chile, Av. Vicuña Mackenna 4860, Santiago, Chile","Received 25 August 2017, Revised 24 July 2018, Accepted 26 July 2018, Available online 7 August 2018, Version of Record 11 January 2019.",https://doi.org/10.1016/j.ijindorg.2018.07.007,Cited by (4),"We study and compare three different mechanisms for capacity (slot) allocation in a congested airport when airlines have one-dimensional ====: direct allocation of slots, differentiated tolls and slot auctions. With perfect information, direct allocation is a first best policy which can be implemented through Pigouvian taxes or slot auctions; the mechanisms are equivalent in terms of social welfare. With the introduction of asymmetric information this equivalence is lost: direct allocation is always ex-post inefficient and, in some cases, tolls and subsequent quantity delegation is a better alternative social welfare wise. Auctions may be superior or inferior to tolls. We further show that naïve application of Pigouvian tolls is sub-optimal when imperfect information exists.","In the last decades, the air transport industry has experienced an important growth of demand that has not been accompanied by a similar expansion of airport infrastructure, transforming runaways in a scarce, and therefore congested, resource. An observable consequence is delayed flights that affect passengers and airlines. See e.g. Gillen et al. (2016) for facts and figures.====There are two different kinds of measures that can be implemented by a regulator or benevolent decision maker in order to face congestion problems in airports: (i) increase the infrastructure supply or (ii) manage the demand for that supply. The former – investing in runway capacity – might be too costly, too slow and even unfeasible for space reasons; it might also be politically hard to proceed if there is a suspicion of inefficient use of the existing capacity. For the latter, the regulator could influence the demand in order to use the existing capacity more efficiently.====One alternative for the regulator would be to take administrative measures, that is, to dictate new capacity allocations, based on its best judgement of what is optimal in some sense. This has been, in a nutshell, the approach of the operations research literature (see Gillen et al., 2016; Pellegrini et al., 2017). The economics literature, on the other hand, has looked at tools that can decentralize the desired optimal outcome, focusing mainly on two mechanisms: slot auctions, that is, auctioning a fixed number of permits that allow a firm to use the runaway for a period of time (see Brueckner, 2009, Verhoef, 2010, Basso and Zhang, 2010, Pertuiset and Santos, 2014), and Pigouvian tolls – which may be uniform, differentiated per firm and/or changing over time – and which attempt to dampen or flatten the airlines demand for runways (see Czerny and Zhang, 2012 for a review). To this date, most research in this area has been made under the assumption that all the relevant information is common knowledge to the airlines, the airport and the regulator. With this assumption, any whished assignment of landing rights – including the first best – can be reached either by direct allocation, or using enough economic instruments, such as an auction or an adequate set of (differentiated) tolls. In other words, with perfect and complete information, the efforts have focused on looking for mechanism(s) that could induce the “optimal” allocation (such as the welfare first best), concluding that, if different mechanisms implement this optimal allocation, then they are equivalent. Which one to use should then be decided on grounds other than efficiency, such as political feasibility.====Nevertheless, it seems clear that, in reality, airport authorities have imperfect knowledge about the costs and/or the demand of the airlines and there is no obvious reason why airlines would have incentives to truthfully reveal any of this information. Indeed, most of the modern economic regulation literature considers asymmetric information as a basic ingredient of the modelling, starting with the pioneering work on mechanism design by Baron and Myerson (1982) and Lewis and Sappington (1988); see e.g. Armstrong and Sappington (2007) for a review. Moreover, this imperfect knowledge of information is probably also true between airlines: one airline does not know with certainty the type of opponent it faces.====The key question addressed in this paper is the performance of congestion management mechanisms under asymmetric information; how does the outcome of each mechanism change? How does the information setting affect the decision about which is the best mechanism? We study and compare how asymmetric information changes what we know about three congestion management mechanisms to allocate congestible (and scarce) infrastructure when the agents, i.e., the airlines, are non-atomistic: direct allocation, pricing, and slot auctions.====In our models, one or two airlines use the airport's capacity, producing externalities to the exterior if there is only one airline, or between them if there are two; the single airline case is developed mainly because it helps to build intuition and provide results which are building blocks for the proofs of the duopoly case. The airlines are non-atomistic and offer non-substitute products (flights to different cities) but, for most of the paper, face perfectly elastic demands in terms of their full-prices, which are the sum of fares plus congestion delays; since passengers dislike airport congestion, the actual fares that the airlines charge must be discounted below these fixed full prices. Not having downward sloping demands in terms of the full-prices allows us to focus the research questions because, under perfect information and absence of market power thus defined, both auctions and tolls implement the first-best (Brueckner, 2009). We do discuss the market power case (i.e. downward sloping inverse demands in terms of full prices) in the final sections of the paper. Airlines have private information about their costs and, therefore, the duopoly plays a Bayesian game according to the rules that the regulator, with his imperfect knowledge, defines while seeking to maximize expected social welfare.====Our results show that, with asymmetric information, direct allocation never reproduces the perfect information optimum; it is ex-post inefficient. Nevertheless, the pricing mechanism does reach the perfect information first best when the optimal unit toll (in a perfect information sense) is constant, i.e. does not depend on the production level, something that may occur for a lone airline causing a linear (in output) total external cost. In this case, a direct allocation is obviously an inferior policy than pricing, since the latter elicits the agents’ private information, expressed through their demand. If optimal (in a perfect information sense) tolls depend on production levels, pricing does not reach an efficient ex-post outcome either. This may occur for a lone airline causing a non-linear total external cost and/or exerting market power, but also, and more importantly, when two non-atomistic airlines impose an externality to each other, even if the total external congestion cost caused by one airline on the rival is linear in the airline's output and they have no market power. However, for the relevant case of the duopoly, we show that if the congestion costs per flight increase linearly with total traffic and market power is absent, congestion tolls always perform better than the direct allocation ex-ante, that is to say, in expectation, tolls are a superior mechanism. In more general terms, what all this imply is that the idea of implementing the first-best allocation through tolls is lost; use of tolls may be superior than direct allocation to the planner. We also show that (i) a naïve application of Pigouvian tolls – a toll equal to the marginal external cost evaluated at quantities that maximize expected social welfare – is sub-optimal, that is, there are prices that induce higher expected social welfare; (ii) slot auctions also fail to implement the first-best and its behavior may be ex-ante and ex-post superior or inferior to congestion tolls (iii) manipulable congestion tolls (explained below) cannot circumvent the asymmetric information problem either.",Effects of asymmetric information on airport congestion management mechanisms,https://www.sciencedirect.com/science/article/pii/S016771871730437X,January 2019,2019,Research Article,276.0
Lin Ming Hsin,"Faculty of Economics, Osaka University of Economics 2-2-8 Osumi, Higashiyodogawa-ku, Osaka 533-8533, Japan","Received 1 May 2018, Revised 14 July 2018, Accepted 26 July 2018, Available online 6 August 2018, Version of Record 11 January 2019.",https://doi.org/10.1016/j.ijindorg.2018.07.006,Cited by (15),"This study investigates pricing and capacity investment for a congested airport served by multiple carriers. Removing the symmetric carrier assumption, when airlines’ market shares are potentially ==== the socially non-discriminatory optimal charge rule should include an additional term that corrects the distortion caused by pricing itself. The first-best outcome cannot be achieved, and the airport overinvests under the non-discriminatory optimal charge. However, if the airport levies discriminatory charges respective to carriers, the first-best outcome can be achieved and capacity investment is socially efficient under discriminatory optimal charges. In addition, the discriminatory optimal charges levied on a carrier with a ==== (smaller) market share are ==== (higher).","Air traffic delays have become a major policy issue globally.==== Recent economic studies advocated the use of pricing mechanisms to balance the demand and limited capacity by incorporating the fact that a congested airport is typically dominated by a few major airlines. These contributions include works by Daniel, 1995, Brueckner, 2002, Pels and Verhoef, 2004, Zhang and Zhang, 2006, Basso and Zhang, 2007, Basso, 2008, Brueckner and Dender (2008), Lin (2013), and Lin and Zhang (2016) among others. Based on the “fixed proportion assumption,”==== a common insight provided by these studies is that the socially optimal per-flight or, equivalently, per-passenger charge generally includes two components; congestion externality, which is not internalized by airlines, and rebate, which reflects the airlines’ market power.====Silva and Verhoef (2013) and Lin and Zhang (2017) considered airline scheduling (i.e., removed the fixed proportion assumption) and their main insights are that un-internalized congestion delay costs should be appropriately corrected by airport-level per-flight charges and market power should be corrected by per-passenger charges that subsidize the passengers.====In parallel with the congestion pricing regime, another solution for relaxing runway congestion and delays is to expand runway capacity. Naturally, the debate around airport congestion pricing also raised the new issue of capacity investment. Daniel (1995) was the first to consider congestion pricing and capacity investment by highlighting carriers’ market power. Using a single-airport setting and the fixed proportion assumption, Zhang and Zhang (2006) found that both a profit-maximizing private airport and a budget-constrained public airport tend to overinvest in capacity when carriers have market power, while airline market structure has no impact on a welfare-maximizing public airport.====Using a hub-spoke network setting and removing the fixed proportion assumption, Lin and Zhang (2017) found that first-best charges, consisting of a movement-related per-flight charge and discriminatory per-local and per-connecting passenger charges, can lead the hub airport's capacity investment to be socially efficient. Further, weight-related per-flight charges result in under-investment, whereas marginal cost pricing results in over-investment. Additionally, movement-related per-flight charges lead private airports to overinvest, whereas weight-related per-flight charges lead them to either over- or under-investment.====Regarding the airline competition setting, recent studies explicitly or implicitly assumed that multiple carriers using congested airports are ====. Although the ==== is rather unrealistic, it is popular and acceptable as it simplifies the analysis and does ==== appear to affect the main results. Given this common understanding, the significance of the symmetric airline assumption has received considerably less attention. Therefore, this study examines how relevant are the previously obtained insights if we remove the symmetric airline assumption in airport pricing literature.====This study contributes to the growing literature on airport congestion pricing and capacity investment by adopting the standard setting of a single congested airport served by multiple carriers.==== The first contribution is the consideration of ====. The second contribution is the examination of the effect of ====. Essentially, we consider potentially asymmetric (duopoly) airlines, which may result, for example, from a differential in operating costs. We find that it is significantly important to consider carrier ==== in light of the existing debate. Adopting the fixed proportion assumption, we first find that the socially non-discriminatory optimal charge rule should include three components; an average of congestion delay cost not internalized by the airlines, less an average of the rebate term reflecting the airlines’ market power, and a third term that corrects the distortion caused by the pricing itself if the airline market outputs (shares) are ====. The first two components coincide with the extant results under the symmetric airline assumption. However, the third component is a new finding that highlights the asymmetry of the airlines’ market shares.====Second, from the first finding we derive that the first-best outcome cannot be achieved under a non-discriminatory optimal charge unless the two airlines’ market shares are the same. This occurs because in the symmetric airline case, the comparative static effects on airlines’ market outputs of an increase in airport charges as well as the consequent marginal effects on the welfare of an increase in airlines’ outputs are identical. Therefore, using a single charge can lead to the first-best outcome. However, these effects do not remain identical for the ==== airline case, and thus, the first-best outcome cannot be achieved.====Third, we explicitly examine the effect of discriminatory charges across carriers. We find that, for a congested airport served by asymmetric airlines, discriminatory charges are needed to achieve the first-best outcome. However, this discriminatory regime charges a carrier with a higher (lower) market share less (more), which may raise the issue of unfair airline competition.====In practice, the European Union (EU) adopted a Directive in March 2009 that stated the following:==== “====e.g. ====” Therefore, our examination of discriminatory charges is meaningful, and this third finding provides a good rationale for a congested airport to discriminate among airlines from the perspective of airport runway congestion. However, the associated issue of unfair airline competition should be considered.====Finally, we examine the efficiency of capacity investment for a congested airport. We find that under the non-discriminatory optimal charge, an airport's capacity investment is socially efficient only if both the airlines have the same market shares. If their market shares are different, the airport overinvests. However, this inefficiency can be solved by introducing discriminatory charges with respect to carriers’ market share. This fourth finding provides a significantly stronger rationale for a congested airport to discriminate among airlines from the perspective of socially efficient capacity investment.====The remainder of this paper is organized as follows. Section 2 presents the model setup. Section 3 describes airline competition. Section 4 derives the optimal rules for levying non-discriminatory and discriminatory charges. Section 5 examines the efficiency of capacity investment. Section 6 concludes the paper.",Airport congestion and capacity when carriers are asymmetric,https://www.sciencedirect.com/science/article/pii/S0167718718300390,January 2019,2019,Research Article,277.0
"Forbes Silke J.,Lederman Mara,Wither Michael J.","Tufts University, US,University of Toronto, Rotman School of Management, Canada,Powerlytics, Inc. United States","Received 15 August 2017, Revised 5 April 2018, Accepted 9 April 2018, Available online 22 April 2018, Version of Record 11 January 2019.",https://doi.org/10.1016/j.ijindorg.2018.04.001,Cited by (15),"We investigate how firms adjust their target quality levels when they - or their competitors - become subject to an information disclosure requirement. Our setting is the U.S. airline ====, where all large domestic carriers are required to report their on-time performance (OTP). OTP is measured by comparing a flight’s actual arrival time to its scheduled arrival time, which is chosen by the airline. Therefore, airlines can improve their OTP by simply increasing their scheduled flight times. We study three airlines which become subject to the disclosure requirement and find that they lengthen their schedule times by 1.4 min on average. Moreover, other airlines also increase their schedule times on routes where they compete with newly reporting airlines, by about 2.3 min, while actual flight times remain unchanged. While these numbers are small, the longer schedule times translate into a 15% improvement in OTP for previously reporting airlines. We conclude that newly reporting airlines and their direct competitors adjust their quality targets when they become subject to quality disclosure, which improves their reported quality without improving the actual time that it takes to travel from gate to gate.","Many products are experience goods whose quality is not observable to consumers prior to purchase. In such settings, quality disclosure programs can be valuable as they provide consumers with systematic information about the quality of the products available in the market.==== It is common for disclosure programs to report simplified and summary quality metrics, often based on discrete thresholds - for example, letter grades or progress against a specified quality target.==== A growing literature has shown that the choice of metric is important because organizations often find ways to ‘game’ the programs, improving the reported quality metric without necessarily improving underlying quality. For example, when programs are based on discrete thresholds, firms focus their quality improvement efforts on transactions near the threshold, since these can be brought over to the ‘right’ side of the threshold at the lowest cost.====In this paper, we consider an alternative form of ‘gaming’. We study a disclosure program which measures firms’ quality against a target that the firms themselves set and we investigate whether firms strategically lower the target in order to improve their measured quality. Our setting is the disclosure of airline on-time performance (OTP). In the U.S., the Department of Transportation (DOT) requires all large domestic airlines to submit data on the on-time performance of their flights. The DOT uses this data to issue a monthly ranking of airlines based on the fraction of their flights which arrive less than 15 min delayed. The DOT defines ‘delay’ as the difference between a flight’s actual arrival time at the gate and its scheduled arrival time, which is ====. The disclosure rule imposes no restrictions on how airlines set their schedules. Thus, under this regulation, airlines can improve their reported OTP by simply scheduling more time for each flight.====While lengthening schedules would appear to be an easy way for airlines to improve their on-time performance, there are costs associated with longer schedule times. All else equal, consumers prefer shorter flight times and may avoid flights with long schedule times. Moreover, scheduling more time for one flight means that the aircraft cannot be scheduled for the next flight until later. As a result, aircraft utilization may fall, leading to higher costs and/or lower revenue for the airline. In addition, labor contracts in this industry typically specify that flight crews are compensated based on the maximum of the actual and the scheduled flight times. Finally, computer reservations systems traditionally sorted flights by schedule length and would show the shortest flights on a route before other flights. For all these reasons, airlines contemplating lengthening their schedules would need to trade-off these costs against the benefits of fewer actual and reported delays.==== Furthermore, even if airlines have lengthened flight schedules over time, this may not necessarily be an attempt to ‘game’ the disclosure program.==== Longer schedules may be a rational response to increased congestion, changes in aircraft types or higher fuel prices.====We develop a novel empirical approach to estimate whether the DOT disclosure program, and the particular quality metric used, induces airlines to lengthen their schedules. Airlines are only required to submit their data to the DOT, and only appear in the DOT rankings, if they account for at least 1% of domestic passenger revenues. We exploit the fact that, in 2003, three full-service airlines cross this size threshold and begin reporting their OTP to the government. The new reporters cross this threshold both because they are growing and because other airlines are shrinking in the aftermath of the September 11, 2001 attacks.==== The fact that we observe some airlines before and after they are required to report their OTP allows us to estimate whether airline schedules strategically respond to the disclosure requirement.====Furthermore, because a large set of ==== airlines had been reporting their data throughout this period, we can also estimate whether these airlines respond when the new reporters become subject to the disclosure requirement. This allows us to investigate whether there is strategic interaction in scheduling and efforts to improve an observable dimension of product quality. Airlines offer differentiated products and compete with each other in price and quality. If the airlines’ choices of quality levels are strategic complements, then an incentive for one firm to improve its (reported) quality would induce the firm’s competitors to increase their (reported) quality as well.====We limit our estimation to one year before and after the new reporters qualify and only look at routes that were either already served by new reporters at the beginning of 2002 or were never served by new reporters.==== Thus, we are not considering any routes that are entered by the new reporters at the same time as they begin reporting their OTP. We estimate how the expansion in the set of qualifying airlines impacts relative schedule length on three distinct sets of flights. The first set are flights by new reporters. The second set are flights by existing reporters on routes where they compete with new reporters. The last group of flights are those operated by existing reporters on routes where they do not directly compete with new reporters. Our empirical approach resembles a differences-in-differences strategy where the first two sets of flights could be considered ‘treatment groups’ while flights by existing reporters on routes in which they do not compete with a new reporter could be considered a ‘control group’. However, we recognize that even these latter flights might be affected by the expansion in the set of reporting carriers as there is an indirect incentive for airlines to also improve OTP on these routes because the main OTP metric is reported as the percentage of ==== of an airline’s flights that arrive fewer than 15 min delayed. Therefore, OTP improvements anywhere in an airline’s network can affect its overall OTP.====For the airlines that newly qualify for the disclosure requirement, we find that their scheduled flight times increase in the year after they begin reporting their data.==== We also find increases in scheduled flight time by competitors of the newly reporting airlines on the routes on which they directly compete with one of the newly reporting airlines. Specifically, newly reporting airlines increase their scheduled flight times by about 1.4 min, or 1.2%, on average after they begin reporting their data to the DOT. Existing reporters increase their scheduled flight times on routes where they compete with a new reporter by about 2.3 min, or 2.2%, on average. Both of these changes are relative to any change in flight schedules by existing reporters on routes where they do not compete with a new reporter. Thus, we find evidence of airlines responding to the DOT program by strategically lengthening their schedules. Since our comparison group - flights on routes without new reporters - also has some incentive to lengthen schedules, we believe that our results are a lower bound estimate of the effect of the disclosure program on flight schedules.====After establishing that airlines lengthen flight schedules on routes with new reporters, we then investigate whether this behavior impacts flight delays. We are unable to examine this for the new reporters as data on their OTP is not available until they qualify for the program. However, for existing reporters, we have data on actual flight times and flight delays before and after the new reporters begin reporting. We compare their on-time performance on routes on which they compete with a new reporter and routes on which they do not, before and after the new reporters qualify. We find that their increase in schedule times after the new reporters begin reporting directly translates into shorter arrival delays. In particular, on routes on which an airline competes with a new reporter, they experience a two minute reduction in average arrival delays relative to routes on which they do not compete with a new reporter. Given that we find that they lengthen their schedules by about 2 min, this suggests that a minute of extra schedule time translates almost one-for-one into a minute of reduced arrival delay. Furthermore, this two minute reduction in average arrival delays translates into a 15% reduction in the airlines’ fraction of flights that arrive 15 or more minutes late, the key metric reported by the DOT. At the same time, we find no statistically significant change in departure delays and only a small (about 0.5 min) increase in actual flight times. Thus, the actions airlines take with respect to their schedules improve their reported performance; however, other, unreported, measures of on-time performance are not improved.====This paper makes several important contributions. First, we show that in a setting where firms are subject to a ‘pass-fail’ type disclosure regulation, and the firms themselves can influence the threshold for passing, firms do in fact lower the passing threshold to their advantage. At the same time, the effect is not very large, which suggests that, at least in our setting, firms may be constrained by the potential costs of lowering the threshold, in this case, lower capital utilization, higher labor costs, and potentially lower demand. We also show that the reporting requirement elicits a strategic response in competitors, who lower their own threshold for passing the disclosure requirement. Moreover, for these competitors we can show that lowering the threshold directly translates into better reported performance without a significant improvement in the actual underlying performance. To our knowledge, this is the first study to demonstrate such a strategic response from competitors to a disclosure program. Most disclosure programs are introduced industry-wide, but our setting is unique in that there is a size threshold for reporting and firms pass these thresholds at different points in time.====The remainder of the paper is organized as follows. Section 2 provides relevant institutional background, Section 3 describes our data, Section 4 outlines our empirical approach, Section 5 presents our results, and a final section concludes.",Quality disclosure when firms set their own quality targets,https://www.sciencedirect.com/science/article/pii/S016771871830016X,January 2019,2019,Research Article,278.0
"Bilotkach Volodymyr,Kawata Keisuke,Kim Tae Seung,Park Jaehong,Purwandono Putut,Yoshida Yuichiro","Department of Economics, Newcastle University, NE1 7RU, United Kingdom,Institute of Social Science, the University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan,Graduate School of Logistics, Inha University, 100 Inha-ro Nam-gu, Incheon 22212, South Korea,International Affairs Staff, Yogyakarta City Mayor’s Office, Yogyakarta City Government, Yogyakarta 55165, Indonesia, and Lecturer, Department of Economic Development, Ahmad Dahlan University,Graduate School for International Development and Cooperation, Hiroshima University, 1-5-1 Kagamiyama, Higashi-Hiroshima Hiroshima-ken 739–8529, Japan","Received 15 August 2017, Revised 22 March 2018, Accepted 26 March 2018, Available online 13 April 2018, Version of Record 11 January 2019.",https://doi.org/10.1016/j.ijindorg.2018.03.012,Cited by (7),"Asia is the world’s most rapidly growing aviation market, and the region now cradles the largest number of low-cost carriers (LCCs). This paper identifies the causal effect of LCC entry on international air passenger flows to and from 30 major airports in Asia using reservation-based international air passenger traffic data for years 2010 and 2015 (","Entry on markets with vertical product differentiation can result in either the same or larger market coverage. Theoretically, the outcome depends on the model’s assumptions about the consumer’s outside option. In most if not all “real world” markets, however, we can expect increased market coverage following entry that expands the spectrum of product quality options available to consumers (i.e., if the entrant offers product quality that is either higher or lower than what has been available thus far). A more interesting empirical question in this case is evaluating the extent to which the entrant expands market coverage as opposed to cannibalizing on the incumbents’ customer base. This important and interesting issue received surprisingly little attention in the economics literature. Airline industry presents an excellent place for analysis of the effects of entry with vertical product differentiation, as in this market we have recently seen a widespread entry of firms offering a clearly lower quality product as compared to what has been available from the incumbents.====The emergence and proliferation of the so-called low-cost air carriers (LCCs) represents a very interesting case beyond the disruption the new entrants brought into the established airline markets. While one would struggle to clearly define what a low-cost carrier is, and there is no accepted threshold for the conventional cost metrics (such as cost per available seat mile) that would help us differentiate between low-cost and non-low-cost airlines, one thing is clear from the economics point of view. For economists, LCCs represent an otherwise lower quality product as compared to what has been on offer in the industry prior to these carriers showing up. Generally speaking, incumbent airlines have offered higher flight frequency and better in-flight services and other amenities (such as access to airport lounges) as compared to these new entrants. Rapid development of LCCs has prompted some experts to talk about the threat they are posing for the incumbent carriers’ competitive position or even survival in the longer term. While LCCs did manage to gain significant market share; the incumbent legacy carriers are still around some two decades later (in Europe now, three out of five largest airlines are still legacy carriers, and the new LCC entrants were only able to claim about one third of the total commercial passenger market). We can therefore suggest that at least some of the traffic generated by the new entrants represents expansion of the market coverage rather than cannibalization on the incumbents’ market. Our study is the first one to shed light on these developments on the airline market, and more generally contribute to the literature on the effects of entry on markets with vertical product differentiation.====In this study we identify the causal effect of low-cost carrier (LCC) entry on international air passenger flows by using comprehensive passenger flow data at the airport-pair level between major airports in Asia and those around the globe. We adopt multiple identification strategies we either reduce or bypass the endogeneity arising from self selection due to LCCs’ entry decision, as well as confounding by unobservables. This paper aims to estimate not only the average treatment effect of LCC entry, but also the differences in treatment effects across stage lengths and the number of LCCs on the market. We use annual international air passenger traffic data in 2010 and 2015, available from Official Airline Guide (OAG), and construct a dataset of directional airport origin and destination (OD) pairs to and from 30 international airports in Northeast and Southeast Asia countries. We demonstrate, by using difference-in-differences (DID) and propensity score matching coupled with DID, that LCC entry causes positive change on international air passenger movements. This effect; however, is moderated as distance between OD pair becomes longer. LCC impact is concave with respect to the number of LCCs, with two LCCs maximizing the impact. Moreover, the impact of LCC entry net of increased competition effect on the number of international passengers account for the major part of the overall impact. That is, LCC replacing the incumbent full-service carriers without changing the market concentration will greatly increase the air passenger volume. Strikingly, the magnitude of this impact is constant over the distance.====Extant literature offers several studies of determinants of entry in the airline industry (Reiss, Spiller, 1989, Berry, 1992, Sinclair, 1995, Ciliberto, Tamer, 2009). Dunn (2008) analyzes the issue of the impact of product quality on entry in the US airline industry - that study finds that an airline already offering a “lower quality” one-stop service will be less inclined to enter on the same market with a “higher quality” non-stop flight. Boguslaski et al. (2004) and Mueller et al. (2012) focus on entry decisions of a specific low-cost carrier (Southwest in the former case, and Jetblue in the latter). Studies of the effects of entry (Joskow, Werden, Johnson, 1994, Alderighi, Cento, Nijkamp, Rietveld, 2012, Morrison, 2001) mostly deal with the price effects of new competition. Our study is thus the first to evaluate the effect of an entry of a lower product competitor on market coverage, both total and of the higher product quality incumbent.====In addition to contributing to the general literature on entry on markets with vertical product differentiation and expanding the body of work on airline market structure, our study sheds light on the rather understudied Asian airline industry. Aviation markets in Asia are relatively less liberalized than those in Europe and North America (O’Connell, Williams, 2005, Zhang, Hanaoka, Inamura, Ishikura, 2008 and Hanaoka et al., 2014). Furthermore, previous studies do not provide clear and universal conclusion about the causal effect of LCC on passenger volumes. Some papers revealed that LCCs induce positive and significant impact on the number of passengers or international tourists (Graham, Dennis, 2010, Donzelli, 2010, Chung, Whang, 2011, DiNardo, Fortin, Lemieux., 1996) while few others found the impact is minimum, not significant or limited to a fraction of a particular route (UK Civil Aviation Authority, 2006, Zhang, Findlay, 2014).====The rest of the paper is organized in the following way. The next section discusses the concept of low-cost carriers, and the issue of product differentiation in airline industry more generally. This is followed by the discussion of our identification strategy, data, and estimation results. The last section of the paper offers some concluding comments.",Quantifying the impact of low-cost carriers on international air passenger movements to and from major airports in Asia,https://www.sciencedirect.com/science/article/pii/S0167718717304186,January 2019,2019,Research Article,279.0
"Belleflamme Paul,Peitz Martin","CNRS, EHESS, Aix-Marseille Univ., Centrale Marseille, AMSE, France,KEDGE Business School, France and CESifo, Germany,Department of Economics and MaCCI, University of Mannheim, Mannheim, Germany,CEPR, UK and CESifo and ZEW, Germany","Received 9 January 2018, Revised 29 March 2018, Accepted 30 March 2018, Available online 6 April 2018, Version of Record 8 May 2019.",https://doi.org/10.1016/j.ijindorg.2018.03.014,Cited by (82),"Competition between two-sided platforms is shaped by the possibility of multihoming (i.e., some users joining both platforms). If initially both sides singlehome, each platform provides users on one side exclusive access to its users on the other side. If then one side multihomes, platforms compete on the singlehoming side and exert monopoly power on the multihoming side. This paper explores the allocative effects of such a change from single- to multihoming. Our results challenge the conventional wisdom, according to which the possibility of multihoming hurts the side that can multihome, while benefiting the other side. This in not always true, as the opposite may happen or both sides may benefit.","Two-sided platforms cater to the tastes of two groups of users—in many instances, buyers and sellers. Decisions among these groups are interdependent because of positive cross-group external effects. One of the principle achievements of the literature on two-sided markets has been to characterize the price structure and associated price distortions in alternative market environments. Competing platforms have to take into account the effect of a price change to participation levels not only on the market side directly affected, but also indirect effects arising from altered participation on the other side.====A possible market environment is that both sides singlehome—that is, users are restricted to join a single platform. To reach a particular agent on one side, an agent from the other side has to be on the same platform. If a platform lures an agent from either side away from a competitor onto its site, this platform becomes more attractive to agents on the other side, as more transaction partners become available on the platform’s site and fewer partners are available on the competing site. Another possible market environment is that agents on one side can multihome—that is, they can join several platforms—and agents on the other cannot. This is the so-called competitive bottleneck, which has been described in these terms:====This insight has been appreciated and reproduced in various policy documents. For instance, in a recent report, the German Cartel Office writes:====In this paper, we take a closer look at the price and surplus effects of multihoming. To this end, we compare the competitive bottleneck to the two-sided singlehoming market environment. While such comparison is an interesting exercise, it may appear to be of little practical relevance because singlehoming on both sides is observed in only few market environments. We want to challenge this view. The present comparison informs policy makers about possible effects when taking actions to prevent or enable multihoming for a fraction of users on one side. For instance, platforms may impose exclusivity agreements upon some users, thereby forcing them to singlehome; in contrast, aggregators combining the functionalities and listings by both platforms may make multihoming feasible.====In the two-sided singlehoming market environment, platforms compete on both sides of the market, whereas in the competitive bottleneck environment, they compete on only one. One may therefore be tempted to conclude that the users that obtain the possibility to multihome face higher prices and obtain a lower surplus, while the other users face lower prices and obtain a higher surplus. Also, since in the competitive bottleneck, platforms compete on only one side, one may expect that their profits are higher than in the market environment in which both sides singlehome. Relatedly, Rysman (2009, p. 134) writes:====Yet, the effect of letting one side multihome instead of singlehome is less straightforward than what may in general be perceived. While it is true that platforms exert monopoly power over the multihoming side, participants on this side may actually benefit from multihoming. In addition, platforms may do better under two-sided singlehoming than in the competitive bottleneck.====As Evans and Schmalensee (2012, p. 16) observe,==== “in software platforms, for instance, the price structure appears to be the opposite of what the competitive bottlenecks theory would predict. Most personal computer users rely on a single software platform, while most developers write for multiple platforms. Yet personal computer software providers generally make their platforms available for free, or at low cost, to applications developers and earn profits from the single-homing user side.” As our analysis will reveal, while this observation runs counter the claim that the multihoming side faces high prices, it is perfectly compatible with the competitive bottleneck model.====For the sake of concreteness, we assume that the seller side is the side of the market which potentially can multihome, while buyers always singlehome.==== Our main findings are as follows. When going from singlehoming to multihoming on one side, ==== on both sides of the market always move in opposite directions. It is not necessarily the case that sellers pay a higher fee and buyers, a lower fee; the opposite may occur because sellers may pay a low price to start with in the competitive bottleneck case. ==== prefer to impose exclusivity to sellers (i.e., to prevent them from multihoming) if the sellers’ intrinsic value (the difference between their stand-alone benefit and the marginal cost of accommodating them) is not too large. There exist configurations of parameters for which this condition is always, or never, satisfied. ==== tend to prefer the competitive bottleneck environment when they value a lot the presence of sellers and sellers find it profitable to multihome; they are then more likely to interact with a larger set of buyers and to be charged lower fees. However, it may also happen that platforms charge higher fees to buyers when sellers multihome than when they singlehome, and that this negative price effect outweighs the positive participation effect, leading buyers to prefer the two-sided singlehoming environment. As for ====, if they perceive the two platforms as more differentiated and if they exert weaker cross-group effects on buyers, then they are more likely to be better off in the competitive bottleneck case.====Combining these findings, we obtain three important insights about how the surplus effects play out for the three groups. First, the resulting market outcome may have the feature that buyers, sellers and platforms are all better off when sellers are allowed to multihome. Second, whenever platforms benefit from imposing exclusivity, doing so may benefit or hurt the side that initially multihomes, but definitely hurts the singlehoming side; that is, the situation may arise that the side which is subject to a contractual restriction benefits from this restriction, but that the other side suffers. As a result, in an environment with potential seller multihoming, an agency should prohibit the use of exclusivity of the seller side if its aim is to maximize buyer surplus. Third, whenever buyers suffer from seller multihoming, platforms and sellers benefit from it.====There exists surprisingly little work that studies the competitive effects of multihoming, despite the policy debate about means to encourage multihoming. In the seminal paper by Armstrong (2006) both market environments—that is, two-sided single-homing and competitive bottleneck—are analyzed in detail, but no comparison is undertaken. Armstrong and Wright (2007) endogenize the multihoming decision of buyers and sellers. In their setting, the competitive bottleneck model emerges endogenously as one side decides not to multihome along the equilibrium path. They, as well as the rest of the literature, do not look at the surplus effects of the possibility of multihoming. In general, there exists little work on surplus effects in markets with two-sided platforms. Three exceptions are the following. First, Doganoglu and Wright (2006) examine the incentives for firms to make their one-sided platforms compatible when users are allowed to multihome or not; they extend their analysis to two-sided platforms but only in the special case of symmetric cross-group external effects. Second, Anderson and Peitz (2017) evaluate surplus effects of policy interventions in the competitive bottleneck world. Finally, Brühn and Götz (2018) examine how the competition between shopping malls is affected by ‘radius clauses’, which limit retail chains’ ability to open outlets in neighboring malls. They show that such exclusive agreements are more likely to be detrimental to social welfare the stronger the competition between the shopping malls. The model they use differs from ours in two main respects: platforms move in a sequential way and only set a price on the seller side (as shoppers are assumed to access the malls for free). Their focus is also different, as they consider exclusivity as a means to foreclose the market.====We follow the approach of Armstrong (2006) by considering platforms that are horizontally differentiated on both sides of the market and charge access fees to each side. As an alternative, platforms may charge transaction fees, as analyzed in Rochet and Tirole (2003). For an insightful discussion of the use of different price instruments, see Rochet and Tirole (2006). While it would be interesting, to extend our analysis to other price instruments, we restrict attention to access fees, which is the natural assumption to make when platforms cannot monitor transactions.",Platform competition: Who benefits from multihoming?,https://www.sciencedirect.com/science/article/pii/S0167718718300353,May 2019,2019,Research Article,280.0
Luttmann Alexander,"Department of Economics, University of California-Irvine, 3151 Social Sciences Plaza, Irvine, CA 92697-5100, United States","Received 3 June 2017, Revised 27 March 2018, Accepted 28 March 2018, Available online 30 March 2018, Version of Record 11 January 2019.",https://doi.org/10.1016/j.ijindorg.2018.03.013,Cited by (16),"This paper explores possible determinants that may affect an airline’s decision to charge passengers different roundtrip fares depending on trip origin, a case of directional price discrimination. Such fare differences cannot be the result of differences in cost, as the cost of flying a roundtrip passenger does not significantly differ depending on direction. It is argued that directional fare differences result from airlines recognizing that passenger price elasticities differ between route endpoints. A price discriminating airline will then charge a higher roundtrip fare at the endpoint where the passenger price elasticity of demand is comparatively lower. Evidence is found suggesting that airlines do use differences in income to price discriminate when setting roundtrip fares. Fares are found to be $0.18-$0.43 higher on average for each $1000 difference in average per capita income between origin and destination metro areas. This finding is sensible assuming that higher incomes reduce the price elasticity of demand for air travel, with richer passengers being less sensitive to the cost of travel.","Travelers are all too familiar with the discriminatory pricing behavior of airlines. Two months before departure, a prospective passenger could pay $200 for a seat in economy. If that same passenger waits until one week before departure, the price of an economy seat may have doubled to $400.==== While frustrating for consumers, the ability of an airline to dynamically price seats in order to segment price-elastic leisure travelers from price-inelastic business travelers remains an integral part of an airline’s yield management strategy.====Previous literature on price discrimination in the airline industry has focused on the various mechanisms utilized by airlines to segment passengers with differing price elasticities of demand. Advance-purchase restrictions attached to discount tickets have been shown to be profit-maximizing for airlines wishing to divert demand from peak to off-peak periods since these restrictions are effective in segmenting customers by their value of time (Gale and Holmes, 1993). These restrictions allow airlines to reduce fares for price-elastic leisure travelers, who typically have a low value of time (Dana, 1998). Other restrictions like Saturday night stay, minimum stay, and non-refundable tickets are designed to discourage price-inelastic consumers from buying cheaper tickets (Stavins, 2001).==== Additionally, Puller and Taylor (2012) find fares purchased on weekends to be 5% lower, supporting the conjecture that airlines price discriminate when the mix of purchasing passengers makes demand more price-elastic.====At the route level, Borenstein and Rose (1994) find evidence of significant variation in fares. The expected absolute variation in fares an airline charges to two different passengers on the same route was found to be 36% of the airline’s average ticket price. Borenstein and Rose also observed an increase in the extent of a carrier’s price dispersion as the number of competitors in a market grows, a result consistent with price discrimination in monopolistically competitive markets. The present paper differs from their study by examining price dispersion on a directional basis. For example, instead of treating Los Angeles (LAX) to New York (JFK) roundtrips as the same market regardless of origin, JFK-LAX is treated separately from LAX-JFK. The paper then contributes to the literature by being the first to explore possible determinants that may affect an airline’s decision to charge passengers different roundtrip fares depending on trip origin.==== Directional fare differences cannot be the result of differences in cost, as the cost of flying a roundtrip passenger does not significantly differ depending on direction.====This paper argues that directional fare differences are a result of airlines recognizing that passenger price elasticities of demand differ between route endpoints. Three potential sources impacting the price elasticity are considered. Foremost, if high incomes reduce the price elasticity of demand for air travel, with rich passengers being less sensitive to the cost of travel, airlines may income discriminate by charging passengers beginning roundtrip travel at the higher income endpoint a higher average fare.==== Differences in endpoint populations could also impact the price elasticity if passengers beginning roundtrip travel at the more populous endpoint benefit from greater flight frequency. That is, more frequent service increases the likelihood that passengers are able to book flights with preferred departure times, a benefit passengers are willing to pay more for. Additionally, if the proportion of business and leisure passengers on a route differs depending on trip origin, airlines may set fares directionally, with roundtrip fares being cheaper in the direction where the mix of passengers is more likely to contain a higher proportion of price-elastic leisure travelers.====Given the substantial level of competition in the U.S. airline industry and assuming airlines face constant costs in delivering passengers to a market, it is unlikely that differences in the level of demand (as opposed to the price elasticity) contribute to directional fare differences.==== However, on routes where airlines operate through congested or slot-controlled airports, an airline could face increasing costs of serving passengers. With an upward-sloping cost curve, demand differences could then affect roundtrip fares on a route regardless of which endpoint is costly to serve. Therefore, it is plausible that directional fare differences reflect airlines exploiting differences in passenger price elasticities in addition to differences in the demand for air travel between route endpoints.====A key contribution of this paper is showing that directional fare differences exist and depend on the incomes of the endpoint cities.==== Income discrimination arises if airlines deliberately charge passengers beginning roundtrip travel at the higher income endpoint a higher average fare. A price discriminating airline will price in this manner if passengers originating from the higher income endpoint have a lower price elasticity of demand relative to passengers originating from the lower income endpoint. In an airline’s yield management strategy, the airline does not need to post higher fares at one endpoint for income discrimination to occur. When selling seats on a flight, an airline will typically allocate a certain number of seats to several different fare levels or “buckets.” Once all seats allocated to the cheapest fare bucket are sold, the price rises to the level specified by the next fare bucket. This process continues until all available seats are sold. For simplicity, consider a scenario where an airline operates only a discount and a full fare bucket, charging the same discount and full roundtrip fares no matter what direction is flown. Average roundtrip fares could then directionally differ if the airline allocates fewer seats to the discount fare bucket for passengers beginning roundtrip travel at the endpoint where average income is higher.====After controlling for differences in endpoint populations and differences in the mix of business and leisure passengers between origin and destination markets, the paper finds evidence that airlines use differences in income to price discriminate on a directional basis. The sensitivity of the income discrimination effect is explored by adding several variables that are correlated with income to the base econometric model. These variables control for the level of competition, whether the roundtrip originates at a hub, differences in the volume of business travelers, differences in the proportion of peak vs. off-peak flights, and differences in passenger demographics between origin and destination markets. Additional specifications explore how the magnitude of the income discrimination effect differs by distance, time of year, and carrier type (legacy vs. low-cost). After accounting for these factors, the directional difference in roundtrip fares is found to be $0.18 larger when the difference in average per capita income between the origin cities rises by $1000. For example, the average per capita income for residents of San Francisco is more than $21,000 higher than for Chicago residents, and this finding implies that roundtrip fares are $3.90 more expensive on average for passengers originating in San Francisco.====The rest of the paper is organized as follows. Section 2 describes the fare data used in the empirical analysis and provides summary measures of directional fare differences in the U.S. airline market. Section 3 discusses pricing criteria airlines may utilize in their yield management strategies that potentially contribute to directional roundtrip fare differences. Section 4 outlines the econometric model while Section 5 discusses results of the empirical analysis. Finally, Section 6 concludes.",Evidence of directional price discrimination in the U.S. airline industry,https://www.sciencedirect.com/science/article/pii/S016771871730351X,January 2019,2019,Research Article,281.0
"Chen Yongmin,Gayle Philip G.","Department of Economics, University of Colorado Boulder, Boulder, CO 80309, United States,Department of Economics, Kansas State University, 322 Waters Hall, Manhattan, KS 66506, United States","Received 15 February 2017, Revised 24 February 2018, Accepted 27 February 2018, Available online 21 March 2018, Version of Record 11 January 2019.",https://doi.org/10.1016/j.ijindorg.2018.02.006,Cited by (37),"Retrospective studies of horizontal ==== have focused on their price effects, leaving the important question of how mergers affect product quality largely unanswered. This paper empirically investigates this issue for two recent airline mergers. Consistent with the theory that mergers facilitate coordination but diminish competitive pressure for quality improvement, we find that each merger is associated with a quality decrease (increase) in markets where the merging firms had (had no) pre-merger competition with each other, and the quality change can have a U-shaped relationship with pre-merger competition intensity. Consumer gains/losses associated with quality changes, which we monetize, are substantial.","A long-standing interest in economics and public policy discussions is the competitive effects of horizontal mergers. To evaluate these effects, one natural approach is to study actual mergers retrospectively. Such studies in the economics literature have focused on a merger’s price effects, which are often used to infer relative changes in market power and cost efficiencies associated with the merger (See, for example, Whinston, 2006 for a discussion of this literature).==== However, price increases or decreases associated with a merger could be closely related to product quality changes. Given the importance of product quality to consumers, it is surprising that little attention has been directed to the quality effects of mergers.==== In this paper, we aim to shed light on the relationship between mergers and product quality by empirically investigating two recent airline mergers—the Delta/Northwest (DL/NW) and the Continental/United (CO/UA) merger.====To guide the empirical analysis, we first present a theoretical model that captures what we term as the ==== and ==== effects of a merger on product quality. A horizontal merger allows two firms to share technology information and coordinate production, which can positively affect the quality of their products. On the other hand, the merger also reduces the competitive pressure on the merging firms. This tends to reduce their incentive to improve product quality, but the magnitude of this negative incentive effect may not monotonically increase with the pre-merger competition intensity, because the diminished profit under competition, especially when competition intensity goes beyond a certain point, can weaken the incentive for costly quality provision. Exploring these possibilities, the model generates two predictions. First, a merger will increase the product quality of the merging firms if they had little pre-merger competition with each other, but will likely reduce quality if they had substantial pre-merger competition with each other. Second, the quality change due to the merger may vary non-monotonically as the intensity of pre-merger competition increases, possibly exhibiting a U-shaped relationship.====The Delta/Northwest and the Continental/United mergers offer an interesting opportunity for us to study a merger’s quality effect. In each case, the merging firms produce in multiple markets. In some of the markets, the firms did not have pre-merger competition with each other, whereas in others they competed directly, with varying degrees of competition intensity.==== Therefore, we can examine not only how the overall product quality is affected by a merger, but also how the quality effects differ across markets, in light of our theoretical predictions.====Our specific measure of air travel product quality is what we refer to as ====. (In Section 3, we discuss in detail why we choose this measure in view of alternative measures of quality.) Related to travel convenience of the air travel product itinerary, routing quality is measured by the percentage ratio of nonstop flight distance to the product’s itinerary flight distance used to get passengers from the origin to destination. Since some products have itineraries that require intermediate airport stop(s) that are not on a straight path between the origin and destination, each of these products will have an itinerary flight distance that is longer than the nonstop flight distance. The presumption here is that passengers find a nonstop itinerary most convenient to get to their destination. Therefore, the closer is the product’s itinerary flight distance to the nonstop flight distance, i.e. higher values of our routing quality measure, the more desirable is the travel itinerary to passengers.====Our empirical analysis starts by estimating a discrete choice model of air travel demand. This serves two purposes. First, it verifies that passengers’ choice behavior is consistent with the hypothesis that a higher routing quality measure is associated with a more passenger-desirable travel itinerary. Second, estimates of the pre-merger cross-price elasticities of demand between the two merging firms, in markets where they competed directly, serve as a useful indicator of the competition intensity. We then proceed to use a reduced-form regression equation of routing quality to evaluate effects that each of the two mergers have on product quality of the merged firms.====Consistent with theory, the regression estimates suggest that each merger is associated with an increase in routing quality, on average 0.45% and 5.28% for DL/NW and CO/UA, respectively, in markets where the merging firms did not compete with each other prior to the merger; but with a decline in routing quality, on average 1.35% and 1.05%, respectively, for the two mergers, in markets where they did. Moreover, in the case of the CO/UA merger, the change in product quality appears to exhibit a U-shaped relationship with the two firms’ pre-merger competition intensity.====Our structural demand estimates further allow us to monetize the consumer gains and losses associated with quality changes. Specifically, in markets where the merging firms had no pre-merger competition, due to their quality improvements, a typical consumer is estimated to experience an increase in utility equivalent to $1.00 and $11.77 for the DL/NW and CO/UA mergers, respectively. In contrast, in markets the merging firms competed prior to their merger, due to their quality declines, a typical consumer is estimated to experience a decrease in utility equivalent to $3.01 and $2.34, respectively, for the DL/NW and CO/UA mergers. There are several markets in the sample in which the estimates suggest a typical consumer in these markets experienced a decline in utility greater than $21 due to routing quality declines associated with the merger. These consumer welfare effects are substantial, considering that many of the markets in our sample have populations greater than a million.====Since the deregulation of the US airline industry in 1978, there have been a number of mergers. Empirical studies of these mergers, similar to merger studies in other industries, have largely focused on price effects, and sometimes used these price effects to infer relative changes in market power and cost efficiencies associated with a merger (Werden, Joskow, Johnson, 1989, Borenstein, 1990, Kim, Singal, 1993, Peters, 2006, Luo, 2014). In case of the recent DL/NW and UA/CO mergers, Gayle and Le (2016) estimate marginal, recurrent fixed and sunk entry cost effects associated with these mergers. We are only aware of two concurrent studies in the airline industry that examine the effect of mergers on product quality, Prince and Simon (2017) and Rupp and Tan (2017). Prince and Simon (2017) find that airline mergers have minimal negative impacts on quality, and likely result in long-run improvements. Rupp and Tan (2017) examine how merger-induced de-hubbing impacts product quality. De-hubbing is the phrase used when airlines choose to stop using a particular airport as a hub in their route network structure. Rupp and Tan (2017) find that product quality improvements are associated with merger-induced de-hubbing events, but such quality improvements are not evident when de-hubbing is unrelated to a merger. Unlike our study, neither of these studies consider the possibility that a merger’s impact on product quality depends on intensity of pre-merger competition between the carriers that merge.====Several studies of the airline industry examine the relationship between service quality and market structure/competition. For example, Mazzeo (2003) and Rupp et al. (2006) all find evidence that airlines provide worse on-time performance on less competitive routes. Our paper contributes to this literature, as well as to understanding more generally how mergers affect product quality.====In the rest of the paper, we provide the theoretical motivation in Section 2, describe the mergers and the data in Section 3, and present the empirical model in Section 4. Section 5 contains the empirical results, and Section 6 concludes.",Mergers and product quality: Evidence from the airline industry,https://www.sciencedirect.com/science/article/pii/S0167718717301121,January 2019,2019,Research Article,282.0
"de Jong Gerben,Behrens Christiaan,van Ommeren Jos","School of Business and Economics, Vrije Universiteit Amsterdam, De Boelelaan 1105, Amsterdam 1081 HV, The Netherlands,SEO Amsterdam Economics, Roeterstraat 29, 1018 WB Amsterdam, The Netherlands","Received 2 September 2017, Revised 31 January 2018, Accepted 27 February 2018, Available online 2 March 2018, Version of Record 11 January 2019.",https://doi.org/10.1016/j.ijindorg.2018.02.005,Cited by (8),"We analyze brand loyalty advantages of national airlines in their domestic countries using geocoded data from a major international frequent flier program. We employ a geographic discontinuity design that estimates discontinuities in program activity at the national borders of the program’s sponsoring airlines in the Schengen area of Europe. We document that foreign consumers earn about 60% less miles and are 70% less likely to be a program member. Controlling for self-selection, we also find suggestive evidence for higher purchase frequency and transaction size by domestic members. These results imply that national airlines enjoy a large loyalty advantage in their domestic country, and contribute to an explanation as to why international flights by third country carriers are still a small share of the market.","Brand loyalty can be a significant source of competitive advantage, as it implies a reluctance to switch to competing brands by consumers (Klemperer, 1995, Chen, Rosenthal, 1996, Farrell, Klemperer, 2007, Dubé, Hitsch, Rossi, 2010). One way in which firms build brand loyalty is through the use of loyalty programs, such as frequent flier programs that proliferate in the airline industry.==== These programs create incentives for consumers to concentrate their purchases with a single airline and provide airlines market power over their program membership base.====The literature on frequent flier programs focuses on their role in creating competitive advantages for airlines that dominate an airport (see, e.g., Levine, 1987, Banerjee, Summers, Borenstein, 1996, Lederman, 2007, Lederman, 2008, Escobari, 2011).==== Theoretically, dominant airlines offer the best opportunities to earn and redeem rewards, because they provide the most extensive range of routes. Therefore, local consumers are more likely to become loyal to that airline (e.g., Borenstein, 1996). In two seminal papers, Lederman, 2007, Lederman, 2008 provides empirical evidence of this effect by showing that frequent flier programs have the largest impact on demand and fares on routes from airports at which airlines (and their partners) have a major presence.====The above studies establish how frequent flier programs impact airline competition and market structure in domestic airline industries. These findings are likely more pronounced in the international airline industry, where consumers tend to be biased towards domestic products and services.==== This is particularly important because in most countries, there is one distinct national airline that has a large share of the international market.==== Here, in addition to being attractive in terms of earning and redemption possibilities, consumer loyalty towards their national airline may provide that airline with an additional advantage over its foreign competitors.====In the current paper, we analyze the prevalence of national brand loyalty in international aviation markets by estimating discontinuities in frequent flier program activity at the national borders of the program’s sponsoring airlines. One of the key features is the use of geocoded microdata on program membership and transactions in a ==== that compares program activity by domestic and foreign consumers who reside in close proximity to the border.==== As these consumers belong to practically the same geographic region, all unobserved supply and demand characteristics that vary smoothly over space (e.g., distance to airports and consequently the availability of airline services) are controlled for and cannot cause a local difference in program activity.====We employ a dataset provided by an anonymous international frequent flier program with multiple European sponsoring airlines. Beyond the availability of geocoded membership and transaction data, there are several interesting aspects of these data that fit well with our research design. First, all sponsoring airlines represent the distinct national airline brand in their respective domestic countries. Second, the sponsoring airlines are active in the Schengen area/customs union of Europe. The open borders that exist in this area contribute to the plausibility of the geographic discontinuity design’s key identifying assumption that unobserved supply and demand characteristics vary continuously through the border. Third, the micro level of the data enables us to provide a wider picture of how loyalty towards the sponsoring airlines changes at their national borders. Specifically, we decompose the overall program activity, as measured by the demand for mileage, into an ====, i.e. the probability that consumers become member of the program, and an ====, i.e. the frequency and size (in mileage) of flight activities by program members.====Our empirical analysis provides evidence of large brand loyalty advantages for national airlines in their domestic country. Just outside the national borders of the sponsoring airlines, semiannual mileage earned within the program drops by 11 miles per capita. This implies that the demand for mileage is nearly 60% lower among foreign consumers. On the extensive margin, we find that the program membership rate drops by 0.8% point at the border, which implies that foreign consumers are about 70% less likely to be a program member. Hence, the dramatic decrease in program activity is to a large extent driven by a lower number of program members.====On the intensive margin, discontinuities may either reflect changes in purchase behavior or in membership composition at the border. Ignoring differences in membership composition, we find no discontinuity in purchase frequency and a positive discontinuity of about 400 miles, or 40 per cent, in transaction size. Following the approach of Lee (2009) and Dong (2017) we correct for selection bias by deriving bounds on the intensive margin discontinuities among the subgroup of ‘always participating’ members (i.e., those who are member irrespective of being domestic or foreign). For purchase frequency, the lower bound indicates that foreign members may purchase up to about 3 flights ==== semiannually than domestic members, while the upper bound rules out that foreign members purchase over 0.15 flight ==== than their domestic counterparts. For transaction size, the bounds range from 800 miles ==== to 400 miles ==== per flight for foreign members. Although we cannot statistically rule out that foreign members have a similar (or even higher) purchase frequency and transaction size, the evidence provided by these bounds, especially in terms of purchase frequency, is suggestive of foreign members purchasing less than domestic members, all else equal.====These novel findings contribute to the broad empirical literature on airline competition.==== To our knowledge, we are the first to carry out an empirical analysis of consumer loyalty towards national airlines. One context in which our findings can be illuminating is the ongoing market liberalization of international aviation. Despite the freedom to operate international routes between third countries as offered by the deregulation of Europe’s skies almost 20 years ago, airlines are still predominantly operating routes connected to their domestic countries. For instance, within the European single market in 2016, only 15% of the departing flights was operated by third country carriers, of which the majority were low-cost carrier flights (OAG, 2017). Besides well-known supply-side explanations for this phenomenon (e.g., historical airport slot rights, ongoing bilateral regulation on extra-EU routes), our paper offers a demand-side perspective: airlines might have a hard time competing on foreign grounds where the loyalty of consumers favors local competitors.",Airline loyalty (programs) across borders: A geographic discontinuity approach,https://www.sciencedirect.com/science/article/pii/S0167718717304381,January 2019,2019,Research Article,283.0
"Yan Jia,Fu Xiaowen,Oum Tae Hoon,Wang Kun","School of Economic Sciences, Washington State University, Pullman, WA, USA,College of Economic and Social Development, Nankai University, Tianjin, China,Institute of Transport and Logistics Studies, University of Sydney, Sydney, Australia,Sauder School of Business, The University of British Columbia, Vancouver, BC, Canada,Ningbo NSCIIC and MIT-Global Scale Network China campus, Ningbo, China,School of International Trade and Economics, University of International Business and Economics, Beijing, China","Received 15 August 2017, Revised 13 January 2018, Accepted 16 January 2018, Available online 2 February 2018, Version of Record 11 January 2019.",https://doi.org/10.1016/j.ijindorg.2018.01.004,Cited by (23),"The identification of possible efficiency gains is a core issue in the analysis of mergers. However, empirical studies are generally subject to bias caused by merger endogeneity. In the early 2000s, the Chinese government pursued a strategy of merging small firms in key industries to create large enterprise groups. Mergers created by this policy provide a rare quasi-natural experiment to investigate the effect of mergers. We take the opportunity to apply the difference-in-differences approach to identify the effect of mergers on the efficiency of Chinese airlines. Overall, our analysis suggests that the mergers increased the productivity of Chinese airlines.","This study aims to identify the effect of mergers on airline efficiency using the merger cases of Chinese airlines in the early 2000s. Identifying the possible efficiency gains from a merger is a core issue in merger evaluation. The US Horizontal Merger Guideline (U.S. Department of Justice and the Federal Trade Commission, 2010) noted that the “====.” Scholars have made great efforts to empirically quantify the effect of mergers on productivity for various industries. A comprehensive review of such studies can be found in Kolaric and Schiereck (2014).====Major airline mergers in recent decades have created some of the world's largest airlines. These mergers have generated many policy debates around the world. Past studies (Borenstein, 1990, Kim and Singal, 1993, Prager and Hannan, 1998, Bilotkach, 2010, Kwoka and Shumilkina, 2010) have identified anti-competitive effects of airline mergers. However, as Peters (2006) pointed out, these studies normally omit supply-side factors such as cost. In the airline industry, higher traffic volumes allow the use of larger, more efficient aircraft, and more intensive utilization of aircraft, airport facilities, and ground equipment. Such “economies of density” effects have been found in empirical studies (Caves et al., 1984, Brueckner and Spiller, 1991, Brueckner and Spiller, 1994). Moreover, increasing traffic volume leads to more frequent flights, which reduces schedule delays,==== a major determinant of service quality for airlines (Anderson and Kraus, 1981, Richard, 2003). An increase in service quality will in turn generate positive feedback that adds to the economies of density.==== By aggregating the traffic volumes of the firms involved, airline mergers are expected to bring efficiency gains.====However, although a number of studies directly and indirectly investigated the cost and efficiency implications of airlines mergers, empirical results are mixed. Using TFP analysis, Oum and Yu (1995) noted that Air France improved its TFP substantially since the merger with UTA in 1992. With cost function estimation, Johnston and Ozment (2013) concluded that major US airlines enjoyed increasing returns to scale which was one possible explanation for merger activities. The study on the mergers of Delta-Northwest and United-Continental led Gayle and Le (2013) to conclude that there were marginal and fixed cost savings at the route level. Chow and Fung (2012) estimated a stochastic frontier production function for Chinese airlines using an unbalanced panel data during 1997–2001. They identified efficiency improvements associated with airline mergers, although only one output, revenue-ton-kilometers, was used in their analysis. Positive effects on productivity have also been identified by case studies (Schosser and Wittmer, 2015, Manuela et al., 2016) and investigations on airline alliances (Oum and Zhang, 2001, Goh and Yong, 2006).==== On the other hand, Barros et al. (2013) did not identify any efficiency gains brought by mergers in their technical efficiency analysis of US airlines, partly because most sample carriers were involved in some mergers during the study period. The DEA analysis by Merkert and Morrell (2012) concluded that there is an interval of optimal airline size; thus the economies of scale argument for mergers holds conditionally. Based on cost function estimates for selected international airlines, Gudmundsson et al. (2017) concluded that mergers did not affect unit costs significantly, and might have led to cost increases when merging airlines were of very different sizes.====In addition to geographic and airline specific factors, the mixed findings in the literature may also arise from identification methods and research designs used. The most challenging problem that empirical studies face in attempting to identify the effects of mergers is endogeneity. Mergers are likely to be driven by efficiency concerns, and this endogeneity will bias the estimates of merger effects in empirical approaches that fail to control for it. One approach to address the endogeneity is to use instrument variables that are correlated with the merger decision but not with firm efficiency. However, finding a truly exogenous instrument variable is a daunting task. Another approach to address the endogeneity is to adopt a structural model that incorporates the model of the merger decision directly into the analysis. Examples of this structural approach can be found in Nevo, 2000, Gugler and Siebert, 2007 and Egger and Hahn (2010). The major criticism of this structural approach is that the model normally relies on many assumptions that are difficult to justify,==== and thus it would be useful to check if results are consistent with credible quasi-experimental estimates==== (Angrist and Pischke, 2010).====The mergers of Chinese airlines, the focus of our investigation, were created by the national policy pursued by the Chinese government in the early 2000s. The policy forced small state-owned firms in industries deemed as a “life-line” to the nation, such as the airline, automobile, electricity and steel industries, to merge into large, state-owned enterprise groups. The government's main motivation in pursuing such a strategy was to strengthen its influence over the entire economy (Pearson, 2007). Because the government aimed to create airline groups of similar size, the mergers of Chinese airlines in the early 2000s were not entirely randomly arranged. However, because productive efficiency was not explicitly evaluated in the merger process, such an event can be properly treated as a quasi-natural experiment that allows us to bypass the issue of endogeneity to measure the effects of the merger on airline efficiency. We apply the difference-in-differences (DID) approach to identify the effects of the merger on both the total factor productivity (TFP) and operational costs of Chinese airlines. The control group in the DID estimation includes major airlines in Asia, Europe, and North America. We find that the merger increased the efficiency of the merged airlines and the finding is robust with respect to various identification assumptions.====This study complements the large number of studies reviewed by Kolaric and Schiereck (2014) on identifying the effects of mergers on firm efficiency. In particular, as a case study on airlines, this paper contributes directly to the literature on the effects of mergers and alliances on airlines’ productivity. The findings provide valuable insights for industry practitioners and government regulators at a time when waves of mergers are taking place in the world's major aviation markets.",Airline horizontal mergers and productivity: Empirical evidence from a quasi-natural experiment in China,https://www.sciencedirect.com/science/article/pii/S0167718717304198,January 2019,2019,Research Article,284.0
"Carlton Dennis,Israel Mark,MacSwain Ian,Orlov Eugene","University of Chicago, United States,NBER, United States,Compass Lexecon, United States","Received 6 June 2017, Revised 10 December 2017, Accepted 12 December 2017, Available online 29 December 2017, Version of Record 11 January 2019.",https://doi.org/10.1016/j.ijindorg.2017.12.002,Cited by (21),"Due to a series of recent mergers, the number of legacy airlines in the United States has decreased from six to three. We conduct a comprehensive investigation of the effect on fares and output of these legacy airline mergers to determine whether the mergers have had an overall pro-competitive or anti-competitive effect on consumers. Our difference-in-differences ==== shows that these mergers have been pro-competitive, with no significant adverse effect on nominal fares and with significant increases in passenger traffic as well as capacity. Taken together, the results indicate that the recent legacy mergers were pro-competitive.","Since deregulation in 1978, the U.S. airline industry has experienced waves of entry, mergers and bankruptcies that have caused the industry to become more concentrated than it once was. This pattern of industry consolidation has continued into the 21st century; in the past ten years, six legacy carriers have merged into three: Delta Airlines and Northwest Airways merged to form Delta Airlines; United Airlines and Continental Airlines merged to form United Airlines; and American Airlines and US Airways merged to form American Airlines.====There is an on-going debate whether these recent mergers have had a pro-competitive or anti-competitive effect on consumers. The argument for anti-competitive effects is that the mergers have reduced competition and thereby led to higher airline fares and lower output. The argument for pro-competitive effects is that the mergers have enabled the airlines to take advantage of network effects, with the improved network of service (e.g., more online connections to more airports, better frequent flyer programs) improving product quality, and thereby attracting more passengers leading to greater output.====In this paper, we investigate which of the two effects have prevailed. We are able to differentiate between the two effects because they have very different empirical implications: The anti-competitive explanation predicts that airline mergers lead to higher fares and lower output, particularly on routes where the two merging carriers—and few other carriers—provided service before the merger.==== On the other hand, the pro-competitive explanation predicts that as a result of efficiencies airline mergers lead to lower fares and increased output, particularly on overlap routes since they are generally at the heart of the combined network. In studying these two effects, the effect of the merger on output is critical because it captures the all-in competitive effects reflecting, among other factors, the airlines’ quality of service and the airlines’ non-fare revenues such as baggage fees and early boarding fees.====We focus on legacy airline mergers because they operate the hub and spoke networks that are more likely to get the network effects from mergers. The effect of mergers of “low cost carriers” likely differ in multiple dimensions and are worthy of separate analysis on their own terms.====In our empirical analysis, we employ a difference-in-differences approach, which allows us to compare how the variables of interest changed between pre- and post-merger periods on routes where mergers were expected to have the strongest competitive effects (i.e., overlap routes) relative to other routes. This methodology has been used in previous legacy carrier merger investigations (see, e.g., Heyer et al., (2009)).====Our main conclusion is simple: The recent legacy carrier mergers have been associated with ==== outcomes. We find that, on average across all three mergers combined, nonstop overlap routes (on which both merging parties were present pre-merger) experienced statistically significant output increases and statistically insignificant nominal fare decreases relative to non-overlap routes. This pattern also holds when we study each of the three mergers individually. We find that nonstop overlap routes experienced statistically significant output and capacity increases following all three legacy airline mergers, with statistically significant nominal fare ==== following Delta/Northwest and American/US Airways mergers, and statistically insignificant nominal fare decreases following the United/Continental merger.====For connecting overlaps, while none of the estimated coefficients are statistically significant, the results are suggestive of small decreases in quality-adjusted fares, as the point estimates show increases in output.====The paper proceeds as follows. In Section II, we discuss the related literature. In Section III we provide industry background. In Sections IV and V we describe our estimation methodology and data, and present the estimation results. In Section VI, we conclude.",Are legacy airline mergers pro- or anti-competitive? Evidence from recent U.S. airline mergers,https://www.sciencedirect.com/science/article/pii/S0167718717303533,January 2019,2019,Research Article,285.0
"Picard P.M.,Tampieri A.,Wan Xi","CREA, University of Luxembourg, Luxembourg,CORE, Université Catholique de Louvain, Louvain-la-Neuve, Belgium,Department of Economics, University of Florence, Florence, Italy,AviAlliance, Düsseldorf, Germany","Received 3 April 2017, Revised 6 October 2017, Accepted 10 October 2017, Available online 19 December 2017, Version of Record 11 January 2019.",https://doi.org/10.1016/j.ijindorg.2017.10.003,Cited by (9),"This paper studies the time slot allocation of flight departures when travelers have a preference for departing on peak times and the number of available peak-time slots is constrained by airport capacities. We show that, compared to public airports, private airports may restrain their supply of peak slots strictly below their capacity levels when they serve airlines that compete to the same destinations. Such an inefficiency takes place in airports that have low per-passenger charges and are not too busy. It does not occur in the absence of competition in destination markets.","Over the past decades, airline traffic growth has outpaced capacities at many of the world’s major airports.==== Limited airport capacities are expected to become a more acute and widespread issue in the coming decades because of the expanding travel demand from emerging and developing countries.====The literature on airport capacity presents “congestion pricing” as the appropriate regulatory tool to deal with congestion.==== Accordingly, carriers pay a toll according to their contribution to congestion. However, despite its theoretical attractiveness, congestion pricing has seldom been adopted in the real world. By contrast, slot allocation is the usual approach to management of airport capacity. According to IATA World Scheduling Guidelines, a slot is “the permission given by a managing body for a planned operation to use airport infrastructure that is necessary to arrive or depart at a airport on a specific date and time”.==== Under a slot allocation system, the airport authority distributes the slots according to some allocation rule.==== Because of the prevalence of this system, it is important to investigate the factors underlying slot allocation under airport capacity constraints.====A recent issue in airport management is related to unused or misused slots. Even at congested airports, where capacity cannot satisfy the demand for slots, over 10% of the allocated slots are not eventually utilized (Zografos et al., 2013). ACI Europe (2009) provides some quantitative evidence of this problem by estimating that unused slots account for approximately euro 20 million fewer revenues per season for large, congested European airports. Katsaros and Psaraki (2012) provide evidence of slot allocation misuse in the Greek airport system.====Moreover, there is evidence that the observed slot misuse is magnified by inefficiencies in the slot allocation mechanism per se (Zografos et al., 2012). Irrespective of the reasons, unused or misused slots indicate a poor efficiency of the allocation process itself, and they impact economically on airports, airlines, passengers and the society at large (Zografos et al., 2013). To the best of our knowledge, this recent issue has not been analyzed in the economic literature on airport policies.====The objective of this paper is to understand the airports’ choices of slot allocations, private or public management incentives and airline competition in the presence of limited airport capacity.==== We examine a setting where an airport sorts the time slots according to different departure flights while airlines compete in each market to a destination city. As in Brueckner (2002), we distinguish between peak and offpeak time periods according to the travelers’ preferences for their departure time.==== Because travelers have preferences over the same departure time window, the airport is likely to face a capacity shortage at that time. However, a formal slot allocation that respects the peak time capacity constraint eliminates any peak time congestion. In this sense, congestion does not emerge in the model. We analyze both a private and a public airport: the private airport maximizes its profit from passenger traffic, while the public airport maximizes social welfare.==== We assume that the airport obtains exogenous and uniform revenues from each passenger. Per-passenger revenues are key features of airport revenues, stemming from passenger flows like various travel fees,==== parking tickets, shop revenues and so forth. We consider separately the cases when destination markets are served by competing airlines and when they are not.====Our first finding is that private airports have incentives to allocate different types of slots (peak vs. offpeak) to airlines that compete in same destination markets. Because peak slots occur in the passengers’ preferred time windows, the airport is able to create a quality differential by assigning competing flights to different peak and offpeak slots. As airlines choose their travel offers conditional on this quality differential, the resulting prices, passenger volumes and airport revenues depend on slot assignments. In fact, such quality differentiation induces airlines to target different types of travelers and raises the total number of passengers in the airport while it also helps airlines to soften competition. We show that the first effect dominates so that slot differentiation increases airport revenues.====Our second finding is that private airports have incentives to keep peak slots unused when they are served by competing airlines. Airports therefore do not use their full peak slot capacity. The intuition is as follows. As said above, private airports have incentives to create quality differentiation. They are therefore willing to allocate competing airlines distinct peak and offpeak slots for a maximal number of destinations. When the number of peak time flights to such destinations is smaller than the number of available peak slots, airports fill only a share of the available peak slots. As a result, private airport revenues are higher if they withhold some peak slots and do not fully exploit their capacity. In this paper, we highlight the conditions under which this strategy takes place and show that it is not used by public airports. In this sense, private ownership leads to an allocative inefficiency. Such allocative inefficiency may correspond to airport managements’ practice to declare fewer slots than under their full capacities (Mac Donald, 2007). As De Wit and Burghouwt (2008, p. 153), point out, “an efficient use of the slots at least requires a neutral and transparent determination of the declared capacity”. This result also explains some recent empirical evidence on allocative inefficiency (Zografos, Andreatta, Odoni, 2013, Zografos, Salouras, Madas, 2012, Airport Council International Europe).====Things, however, differ if each destination is served by an airline monopoly. Monopoly airlines have incentives to operate one flight to each destination. We show that they have no incentives to split their demand into different time slots and gather into the same airplane all the passengers departing to a given destination. With one flight to each destination, the airport is unable to apply the above quality differentiation strategy. As a result, private ownership does not lead to allocative inefficiency: private and public airports choose the same slot allocation.====So far, slot allocation has drawn relatively little interest in the economic literature, with few but noteworthy contributions. As in this paper, Barbot (2004) assumes that slots for airline activities are vertically differentiated products with high or low quality, but, unlike this paper, she assumes that carriers choose the number of flights they operate. Verhoef (2008) and Brueckner (2009) compare the pricing and slot policy regimes. They show that the first best congestion pricing and slot trading/auctioning generate the same amount of passenger volume and total surplus. They investigate a single congested period. Their contributions do not distinguish between peak and offpeak hours, or allow the airport to allocate slots without charges. Although this seems a plausible description of some public airports, non-profit behavior is relevant for many other airports. In this respect, Basso and Zhang (2010) generalize Verhoef (2008) and Brueckner’s (2009)) analyses by taking into account airport profits. By contrast, they find that congestion pricing and slot allocation mechanisms lead to different outcomes. Departing from those authors, we consider that passengers have preferences for peak times and each airline operates a single flight, and we discuss the possible inefficiency.====The remainder of the paper is organized as follows. Section 2 introduces the model. Section 3 analyzes the equilibrium in airline markets, where duopoly airlines determine their seat supplies. Section 4 examines the slot allocation equilibrium, the case with monopoly airlines, and shows some stylized facts of airline markets that support our theoretical analysis. Section 5 investigates the case of slot markets. Section 6 concludes.",Airport capacity and inefficiency in slot allocation,https://www.sciencedirect.com/science/article/pii/S0167718717302217,January 2019,2019,Research Article,286.0
"Doi Naoshi,Ohashi Hiroshi","Faculty of Economics, Sapporo Gakuin University, 11 Bunkyo-dai Ebetsu-shi Hokkaido, 069–8555, Japan,Faculty of Economics, University of Tokyo, 7-3-1 Hongo Bunkyo Tokyo, 113-0033, Japan","Received 18 July 2017, Revised 22 November 2017, Accepted 28 November 2017, Available online 19 December 2017, Version of Record 11 January 2019.",https://doi.org/10.1016/j.ijindorg.2017.11.006,Cited by (14),"This study examines the economic consequences of a horizontal merger between Japanese airlines that took place in 2002, with particular emphasis on quality responses to the airline merger. A structural model allows firms to determine not only prices but also flight frequencies. The obtained estimates would reject the hypothesis that the merger facilitated coordinated effects. Efficiency gains from the merger are estimated as not trivial, and are more strongly observed in marginal costs per flight, rather than in marginal costs per passenger. Welfare effects of the merger are positive, but vary by market structure. Neglecting endogenous flight frequency overstates the welfare gains, especially for smaller markets. Finally, remedial measures imposed by the authority did little for the merger outcomes.","As deregulation and liberalization progressed in the airline industry, the number of airline mergers has been on the rise globally. The world’s largest airlines in terms of the number of passengers carried in 2015, including American Airlines, Southwest Airlines, Delta Airlines, and China Southern Airlines, all have experience of mergers and acquisitions this century (Yan et al. (2016)). With significant market shares consolidated under the merged parties, retrospective evaluation of specific cases of airline mergers would provide us with useful information on whether antitrust authorities effectively blocked mergers that reduced economic welfare. Welfare evaluation on horizontal mergers should involve a structural estimation framework for us to assess the economic trade-off between market power and efficiency gain. Up to now, a large body of literature has applied structural techniques to actual airline mergers (see, e.g., Peters, 2006; and Berry and Jia, 2010), and antitrust authorities have increasingly relied on simulations to predict likely impacts of the mergers. However, the extant literature on merger simulation almost exclusively disregards the merger effect on product quality.==== Since airlines offer services over spatial networks that link many origin-destination cities, the merger effects should be evaluated not only on prices, but also naturally on nonprice elements. Research on such a topic would provide new insights for antitrust authorities, who decide whether particular airline mergers should be allowed, prohibited, or cleared subject to certain remedies.====This study quantitatively assesses the welfare trade-off associated with a horizontal merger in an attempt to redress the lack of evidence of nonprice response to the merger. Morrison and Winston (1995) and Brueckner and Luo (2014), for example, pointed out that an important dimension of product quality in the airline industry is flight frequency. The present study concomitantly focuses on flight frequency as product quality, and focuses on a unique merger case that occurred in the Japanese airline industry. In Japan Airlines, Japan Airlines (JAL) and Japan Air System (JAS) consolidated to become a new company with a market share of nearly 50%. Despite the competition authority’s concern over a substantial increase in market power, the Japan Fair Trade Commission (JFTC) approved the merger under several remedial conditions, primarily that the merged Japan Airlines Group (JAG) should cede some slots at the most congested airport in Japan, Haneda Airport, to its competitors.====Casual observations of the dataset presented in Table 1 indicate that the merger was marked by both increased market power and efficiency gain; the merged company steadily increased the average price on those routes that had no competitors, but at the same time, flight frequency of the merged company also increased more than that of nonmerged companies.====To evaluate the economic consequences of the merger, we first conduct a difference-in-differences (DID) analysis, and then turn to structural estimation. In the latter approach, we model major carriers’ behavior in the domestic air passenger market, and estimate parameters of consumer demand and marginal costs for each airline route, followed by simulation exercises of several counterfactual scenarios. The estimates indicate that the merger decreased the marginal costs faced by the merged company by 3–4%. The efficiency gain from the merger was more strongly observed in marginal costs per flight, rather than in marginal costs per passenger. In addition, the efficiency gain was estimated as larger for those routes with more passengers. The obtained estimates reject the hypothesis that the merger facilitated coordinated effects, namely collusion between the merged and non-merged airlines. Simulation exercises based on the obtained structural estimates indicate that neglecting endogenous flight frequencies overstates the welfare gains, especially for smaller markets. Furthermore, simulation exercises show that the merger under study improved both consumer and social welfare, but that remedial measures imposed by the authority did little to the merger outcomes.====This study contributes to the literature on ex-post merger evaluation, with emphasis on quality responses to airline mergers. Several studies have analyzed merger effects on product quality in the airline industry. Prince and Simon (2015) analyzed five major airline mergers that have occurred in the U.S. since 2000, and how these mergers affected on-time performance. The authors found evidence that service quality was improved because of efficiency gains from the merger. Chen and Gayle (2013) studied the mergers of Delta–Northwest and Continental–United. This study examined routing quality, measured by the percentage ratio of nonstop flight distance to the product’s itinerary flight distance used to get passengers from the origin to destination, and found that the mergers worsened the quality for those routes that the merging firms competed on prior to the merger. Similar finding was reported in Werden et al. (1991), and Steven et al. (2016).==== Our paper finds evidence consistent with the findings in the existing literature, in that (1) efficiency gain from the merger improved product quality; and that (2) the merged party internalized the competitive externalities associated with product quality.====Different from the above studies employing reduced-form analyses, Richard (2003) and Lee (2013) used a structural estimation method, the approach that is also employed by this paper. Richard (2003) focused the merger between American Airlines and United Airlines in the market at Chicago O’Hare airport, while Lee (2013) analyzed the Delta-Northwest Airlines merger in the US domestic market. The former work focused solely on merger-to-monopoly routes, that study did not account for strategic interactions between merging and nonmerging parties. This paper, similar to Lee (2013), includes non-merging airlines in the analysis, by using a random-utility discrete-choice model employed by Fan (2013) in her application to U.S. newspaper mergers. Our study contains at least two differences from Lee (2013). The first relates to the scope of our findings. This paper is able to estimate efficiency gain from the merger, and the overall merger effect on economic welfare, whereas Lee (2013) was unable to do so with the limited data covering only the pre-merger period in her estimation. The second difference is that this study examines the implications for merger regulation, including the coordinated effects of the merger and the effectiveness of merger remedies; however, none of these implications were in the scope of Lee (2013).====This paper is related to the growing literature on endogenous product quality in a static structural estimation.==== Mazzeo (2002) and Seim (2006), for example, modified an entry model originally proposed by Bresnahan and Reiss (1991) to endogenize product-choice decisions for heterogeneous competitors. In their applications (of motels and video rental stores, respectively), product choice is typically discretized to take the value of either 0 or 1. This method, however, is not very tractable in our application, in which quality could take a large number. While the method is not structural, Brueckner and Luo (2014) estimated a reduced-form reaction function for flight frequency, using US domestic nonstop duopoly routes. Brueckner and Luo (2014) identified heterogeneous carriers, namely, legacy and low-cost carriers, and examined how the degrees of strategic interaction differed between types. While we observe only legacy carriers during the study period, the present study borrows a motivation from Brueckner and Luo (2014), and builds a structural model to assess the welfare consequences of the airline merger.====The Japanese airline market seems an ideal setting for our application for two reasons. First, neither entry and exit of airlines nor routes were virtually observed,==== except for when a structural remedy was implemented, as discussed in Section 2.1. Thus, it is reasonable for us to assume that network configuration, including entry and exit, is exogenous.==== Second, the merger under study was triggered by the September 11 terror attacks, the occurrence of which was hardly anticipated by the industry. Thus, the merger event of our application is naturally considered exogenous to the development of the Japanese airline market.====The remainder of the study is organized as follows. Section 2 provides an overview of the Japanese airlines industry in the early 2000s with a particular emphasis on the merger between JAL and JAS taking place in 2002. The section also provides a preliminary analysis of the merger effects based on the DID analysis. Section 3 presents a structural model of demand and supply to characterize the Japanese domestic air passenger market. In this section, we also discuss methods for identifying and estimating the structural model in the section. We then present estimation results in Section 4. These results provide a basis for the analysis of Section 5, in which we assess the effects of the merger on market outcomes and economic welfare. The section also assesses the effectiveness of a remedial measure, which includes allocating slots from the merged to nonmerging companies. Section 6 concludes, and it is followed by a data appendix.",Market structure and product quality: A study of the 2002 Japanese airline merger,https://www.sciencedirect.com/science/article/pii/S0167718717304071,January 2019,2019,Research Article,287.0
"Fageda Xavier,Flores-Fillol Ricardo,Theilen Bernd","Department of Applied Economics and GIM, Universitat de Barcelona, Avinguda Diagonal 690, Barcelona 08034, Spain & Public-Private Sector Research Center - IESE Business School, University of Navarra, Spain,Departament d’Economia and CREIP, Universitat Rovira i Virgili, Avinguda de la Universitat 1, Reus 43204, Spain","Received 23 June 2017, Revised 24 October 2017, Accepted 29 October 2017, Available online 12 November 2017, Version of Record 11 January 2019.",https://doi.org/10.1016/j.ijindorg.2017.10.008,Cited by (16),"This paper develops a unified framework to analyze a continuum of hybrid cooperation agreements along two dimensions: their degree of revenue sharing (scope of alliances) and their degree of cost sharing (scope of joint ventures). The analysis focuses on the air transportation industry, distinguishing between interline and interhub markets. As economies of traffic density become stronger, we find that the socially optimal cooperation agreement moves from ==== to ==== in interline markets, whereas it moves from ====to ==== in interhub markets. These results are driven by the tradeoff between a procompetitive effect of alliances in interline markets and an anticompetitive effect in interhub markets, along with the efficiency gains associated with joint ventures in interhub markets. We also develop an empirical application for intercontinental routes for the period 2010–2016 that identifies a positive impact of deeper degrees of airline cooperation (revenue and cost sharing) on traffic, both in interline and interhub markets. Therefore, the potential anticompetitive effect of deeper alliances in interhub markets is not observed in our sample.","The air transportation industry is characterized by a strong competitive pressure that requires airlines to innovate permanently. While innovation affects many dimensions (such as the adoption of new aircraft technologies and business models or the implementation of novel yield management practices), one of its main consequences has been the proliferation of sophisticated and complex hybrid agreements among airlines in recent years.==== This trend has also been reinforced by the existence of regulatory restrictions on foreign ownership of airlines, which has prevented the formation of full mergers at the international level.==== In fact, intercontinental routes are generally operated by airlines integrated in one of the three major global alliances (Oneworld, SkyTeam, and Star Alliance).====Alliance members usually cooperate in various dimensions to exploit ==== such as codesharing agreements, mutual recognition of frequent-flyer programs, and a number of facilities so as to provide a seamless service to interline passengers (e.g., coordination of flight schedules, shared lounge access, and gate proximity). In particular, a standard codesharing agreement allows one carrier to market under its two-letter designator code a certain number of seats on a flight operated by a different carrier. Moreover, some airlines within an alliance may also attain deeper degrees of cooperation on specific international routes when they are granted antitrust immunity (ATI) by competition authorities, allowing partner carriers to cooperatively make scheduling and pricing decisions.==== For instance, ATI has been granted to Star Alliance partner airlines on their flights in the city-pair market between San Francisco and Seoul in 2003.====Another trend in airline cooperation, which also implies a deeper level of revenue sharing than a standard alliance, is the so-called equity alliances. Through these type of agreements, one airline obtains a certain degree of control over another airline through the purchase of equity shares. As long as the proportion of capital acquired does not exceed 49%, this type of agreement escapes from existing national restrictions on foreign ownership. Recent examples of equity alliances are, for instance, the purchase of shares of Air Berlin and Alitalia by Etihad or the purchase of shares of IAG by Qatar Airways.====Finally, hybrid airline cooperation agreements can also take the form of joint ventures, which allow partners to attain a certain degree of ==== on particular routes (Thomas and Catling, 2014). Cooperation in the realm of costs translates into the exploitation of synergies through the joint purchase of fuel, ground handling, catering and other supplies, and the joint use of marketing and global distribution systems (GDS). At the same time, joint ventures may also convey a deeper revenue sharing, allowing airlines to coordinate schedules, prices, and capacities more closely than just cooperating through an alliance.==== The deepest joint ventures that can be attained are based on the principle of metal neutrality, implying that both revenues and costs are shared proportionally no matter which airline actually operates the flights on a route, so that they are akin to merger-like agreements. Some examples of joint ventures are the agreement between Delta and its European SkyTeam partners (Air France-KLM and Alitalia) or the cooperation between Delta and Virgin Atlantic on transatlantic routes.====The impact on consumer welfare of these hybrid cooperation agreements is difficult to assess because most carriers are multi-product firms that operate in several markets characterized by different degrees of competition intensity. Furthermore, airlines may exhibit economies of traffic density on certain routes while they may experience decreasing returns on other routes (typically dense routes affected by airport congestion). As a consequence, the effect of these agreements might be procompetitive in one market but anticompetitive in another.====The objective of this work is to provide a general analysis, both theoretical and empirical, of the existing hybrid forms of cooperation in air transportation markets, from soft alliances to merger-like joint ventures. Taking into account the airlines’ generalized use of hub-and-spoke networks, the analysis has to differentiate two main types of markets: ==== and ====.====In the theoretical part, we develop a unified framework to analyze a continuum of hybrid cooperation agreements along two dimensions: their degree of revenue sharing (scope of alliances) and their degree of cost sharing (scope of joint ventures). We first analyze airline optimal choices in both markets (interline and interhub) under different hybrid cooperation agreements, and then study the differentiated effect of these agreements on consumer welfare.==== There is demand for connecting flights in the interline market, where there are two complementary air services, each of them provided by a different monopoly firm. Differently, there is direct duopoly competition in the interhub market where airline networks overlap.====In the empirical part, we use quarterly data for the period 2010–2016 at airline-route level, focusing on intercontinental routes characterized by a rich variation in the degree of airline cooperation (16,897 observations with complete information for all variables). We estimate a gravity model where passenger traffic for interhub and interline markets is regressed on variables measuring different degrees of airline cooperation (including ATI agreements, equity alliances, and joint ventures). We include as controls the distance of the non-stop route, variables for the economic and demographic size of the endpoints, and different specific fixed effects.====Our theoretical results are summarized as follows. First, under decreasing returns, deeper alliances have a positive effect on traffic in the interline market and a negative effect in the interhub market. Second, under economies of traffic density, the effect of deeper joint ventures on traffic is positive in both markets. Third, the socially optimal cooperation agreement in the interline market is: (i) ==== in the presence of decreasing returns (either weak or strong); (ii) ==== under constant returns; (iii) ==== in the presence of weak economies of traffic density; and (iv) ==== for strong economies of traffic density. The intuition behind this result is that alliances have a positive effect on consumer welfare because they allow internalizing a double marginalization externality but, simultaneously, an adverse cost effect can arise under strong economies of traffic density due to a fall of traffic in the interhub market that increases marginal costs and can result in higher prices and lower traffic volumes in both markets. Fourth, the socially optimal cooperation agreement in the interhub market is: (i) ==== in the presence of strong economies of traffic density, and (ii) ==== otherwise. The reason is that, as economies of traffic density become stronger, the effect of deeper joint ventures on consumer welfare is increasingly positive because it translates into higher efficiency gains. The joint consideration of the interhub and the interline markets elucidates the importance of market-size asymmetry and cost technologies in the assessment of the net welfare effect of cooperation agreements.====The results of our empirical application show that deeper degrees of airline cooperation (revenue and cost sharing) have a positive impact on traffic both in interline and interhub markets. While these results are in general consistent with the existence of economies of traffic density, they also suggest that deeper alliances and joint ventures may also have a positive impact on traffic under decreasing returns. Therefore, the potential anticompetitive effect of deeper alliances in interhub markets is not observed in our sample.====The paper is organized as follows. In Section 2 we review the existing literature, while in Section 3 we set up our theoretical model. Section 4 contains the theoretical results of the paper and, in Section 5, we elucidate the net welfare effect of cooperation agreements by means of a numerical simulation. Section 6 contains our empirical application using data from intercontinental air transportation services. Finally, Section 7 concludes with a discussion on the generality of the results and challenges for future research. All proofs are provided in the Appendix.",Hybrid cooperation agreements in networks: The case of the airline industry,https://www.sciencedirect.com/science/article/pii/S0167718717303855,January 2019,2019,Research Article,288.0
"Brueckner Jan K.,Zhang Anming","University of California, Irvine,University of British Columbia","Available online 31 August 2018, Version of Record 11 January 2019.",https://doi.org/10.1016/j.ijindorg.2018.08.010,Cited by (0),None,None,Introduction to the special issue on the airline industry,https://www.sciencedirect.com/science/article/pii/S016771871830081X,January 2019,2019,Research Article,295.0
