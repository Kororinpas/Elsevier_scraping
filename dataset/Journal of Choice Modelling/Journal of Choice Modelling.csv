name,institution,publish_date,doi,cite,abstract,introduction,Title,Url,Time,Year,Type,Unnamed: 0
"Sokratous Konstantina,Fitch Anderson K.,Kvam Peter D.","University of Florida, United States of America","Received 22 July 2022, Revised 28 April 2023, Accepted 11 May 2023, Available online 19 May 2023, Version of Record 27 May 2023.",https://doi.org/10.1016/j.jocm.2023.100418,Cited by (0),"Subjective value has long been measured using binary choice experiments, yet responses like willingness-to-pay prices can be an effective and efficient way to assess individual differences risk preferences and value. Tony Marley’s work illustrated that dynamic, stochastic models permit meaningful inferences about cognition from process-level data on paradigms beyond binary choice, yet many of these models remain difficult to use because their likelihoods must be approximated from simulation. In this paper, we develop and test an approach that uses deep ","For decades, the formal study of preference was framed around the lens of deterministic utilities and focused on algebraic, axiomatic models of choice (Savage, 1954). Despite the prevalence of choice as a way of measuring subjective value, this is not the only way to understand preferences and their representation. Deterministic theories often fail to adequately account for intra- and inter-individual variability in decision making (Marley and Regenwetter, 2016), and different methods of eliciting preference can even contradict the findings of choice experiments (Lichtenstein and Slovic, 1971, Slovic and Lichtenstein, 1983). Consequently, deterministic utility functions gave way to representations of value defined in probabilistic terms. These random utility models were pivotal because they allowed for modeling responses as a stochastic process, which in turn led to the development of new approaches to modeling subjective value that reconciled choice with other measurement processes like certainty equivalents and pricing schemes (Johnson and Busemeyer, 2005).====The pivot toward understanding value from multiple perspectives has led to a proliferation of models combining utility, risk, and value in choices, prices, and other methods of preference elicitation like demand tasks (Georgescu-Roegen, 1958, Koffarnus et al., 2015). However, the usability of various models is limited by our practical ability to fit and compare them. Probabilistic models are accompanied by a host of practical problems stemming from the fact that many patterns of behavior can arise from stochastic processes. There are many models and even broad classes of theories that have never been explored simply because they do not have convenient probability density functions that specify the likelihood of different patterns of data for various combinations of their parameters. Fortunately, the advent of machine learning and “likelihood free” methods has made it possible to explore new theories and models that were previously impossible (Lueckmann et al., 2019, Gutmann and Corander, 2016, Fengler et al., 2021).====In this paper, we examine how these tools can be brought to bear on models that quantify risk preference from willingness-to-pay prices. These models are conceptually critical because they account for preference reversals as well as distributions and dynamics of pricing behavior (Kvam and Busemeyer, 2020). We show that applying deep learning to the fitting process of complex simulation-based models can reduce the required fitting time by several orders of magnitude, in our applications reducing the time from several months to several minutes. Furthermore, we show that the efficiency of deep neural networks for model fitting allows us to estimate parameters with a minimal amount of behavioral data — making it possible to understand latent processes related to risk aversion, utility, anchoring, and preference dynamics using pricing paradigms with a small number of trials. We conclude by showing that this approach allows for new model-based insights about risk preferences, applying it to fit pricing models to a large volume of participants to grant new insights into pricing behavior across different groups of people.====Throughout his research career, Tony Marley made strides to improve the effectiveness of our models of preference, in large part by extending and enriching random utility models. An impressive volume of Tony’s work centered around the idea that internal representations of stimuli or value can be assigned to responses by comparing them to ====, or exemplar values of stimuli that lie at along or at either end of the range of values a participant might encounter on a particular task (Marley and Cook, 1984, Lacouture and Marley, 1995, Lacouture and Marley, 2004, Brown et al., 2008). In some of his last work (Kvam et al., 2023), we showed that these anchor-based representations could be dynamically mapped onto both discrete and continuous scales. Although these types of models have traditionally been applied to perceptual decision making experiments, anchors and anchoring effects are directly relevant to value-based choice and pricing judgments (Kvam and Busemeyer, 2020, Tversky et al., 1990), connecting Tony’s work on perceptual choice with his work on dynamic models of subjective value (Marley, 1989, Marley and Colonius, 1992, Hawkins et al., 2014). In making these connections, we can create a dynamic, stochastic theory of pricing that accounts for many of the phenomena of random utility models (Corbin and Marley, 1974) while also accounting for process data like response times (Luce, 1986). Specifically, the models we examine here focus on how the competing goals of high-probability and high-payoff prospects combine to determine subjective utility (Swait and Marley, 2013), but use pricing rather than choice as a measure of preference. These approaches resolve apparent preference reversals – where participants select Option A over Option B, but price Option B higher than Option A in willingness-to-pay judgments – in terms of a common currency of utility by attributing differences in preference to response processes rather than “true” differences in subjective value.====There are a number of reasons to examine pricing as a measure of subjective utility if a researcher is interested in assessing risk preferences, whether from a psychological or economic perspective. One is that pricing avoids relative comparisons between options that appear prevalent in binary choice (Busemeyer and Townsend, 1993, Scheibehenne et al., 2009, Gonzalez-Vallejo, 2002, Dai and Busemeyer, 2014), creating context effects based on the differences in payoffs and likelihoods between options. Naturally, there are still context effects from sequences of trials that create effects like contrast, range, and assimilation that are likely to affect pricing (Sherif et al., 1958, Brown et al., 2008) and cause anchoring effects that lead to preference reversals (Tversky et al., 1990). Fortunately, both choice and pricing appear to measure the same underlying utilities, or at least it is not apparent that they measure diverging preferences despite preference reversal phenomena (Johnson and Busemeyer, 2005, Kvam and Busemeyer, 2020). Therefore, pricing can be an effective route to understanding risk preferences as long as we account for the psychological and cognitive processes involved in responding alongside core economic concepts like utility.====Beyond the perspective it adds to complement binary choice, there are information theoretic reasons to favor pricing as a method of eliciting preferences as well. A binary decision provides 1 bit of information per choice problem (0 = choose safe option, 1 = choose risky option). In multi-alternative choice, we theoretically learn which option will be favored over all others in the choice set in a series of binary choices, yielding ==== bits of information for ==== alternatives. Ironically, this may be more information than a participant needs to collect to make their decision, which grows as a logistic rather than linear function (Hick, 1952). Different preference elicitation paradigms can further capitalize on this difference, yielding more and more information about a person’s preferences from fast, simple responses. As Tony Marley’s work suggested (Hawkins et al., 2014, Louviere et al., 2008), ranking and best-worst responses can provide substantially more information than binary choice alone – with ==== different rank orders, they give up to ==== bits of information per problem – but naturally these rankings take quite a long time for participants to complete. By contrast, pricing allows for a selection to be made from a wide range of options in matters of dollars and cents, which can theoretically provide an infinite amount of information by virtue of each response falling along a continuous scale. In practical terms, participants seem unlikely to be much more precise with their responses than 10–25 cents. For a range of $0–20, a precision of 25 cents provides us with ==== bits of information, while a precision of 10 cents provides ==== bits. Previous results suggest that people can obtain a precision of 10 cents within about 5 s (Kvam and Busemeyer, 2020), meaning that they can communicate at least 1 bit of information per second by responding with prices. Given the amount of time to complete a pricing problem is much less than 6 times the amount of time a participant spends on a binary choice – in fact, it is closer to ==== the length of time (Kvam and Busemeyer, 2020) – pricing seems to be an efficient method of eliciting preferences among prospects.====In this paper, we explore how a pricing model based on a double-anchor theory of decision-making can be automated to make inferences about risk preferences from a small number of trials (20). This is made possible by leveraging the rich information provided by prices in combination with machine learning approaches to estimating pricing models, allowing us to maximize our use of response information to make parametric inferences about how people assign value. The procedure is accomplished by taking behavior on a set of pricing problems, using a neural network-based estimation approach (Radev et al., 2020b) to fit a pricing model, and then using the parameter estimates as individual differences to situate a participant in the broader context of their peers. In the next section, we begin by describing the cognitive/economic modeling approach we use to quantify behavior on pricing tasks (behavioral model), followed by a deep learning method (algorithmic model) to analyze the data on the task in terms of the parameters of the behavioral model. The ultimate aim of the paper is to present an alternative method that allows us to make rich inferences about behavior with a pricing model (Kvam and Busemeyer, 2020) and to show how this is enabled by the use of deep learning.",How to ask twenty questions and win: Machine learning tools for assessing preferences from small samples of willingness-to-pay prices,https://www.sciencedirect.com/science/article/pii/S1755534523000192,19 May 2023,2023,Research Article,0.0
"Rose J.M.,Borriello A.,Pellegrini A.","The University of Sydney Business School, Institute of Transport and Logistics Studies, The University of Sydney, NSW, 2006, Australia,European Commission, Joint Research Centre (JRC), Ispra, Italy,The University of Sydney Business School, Institute of Transport and Logistics Studies, The University of Sydney, NSW, 2006, Australia","Received 1 July 2022, Revised 19 April 2023, Accepted 21 April 2023, Available online 14 May 2023, Version of Record 6 June 2023.",https://doi.org/10.1016/j.jocm.2023.100412,Cited by (0),"The inclusion of attitudinal indicator variables within discrete choice models is now largely common practice. Typically, this involves the estimation of multiple indicator multiple cause (MIMIC) type models which are used to construct latent attitudinal variables that are then employed as independent variables within standard discrete choice models. Such models, collectively termed hybrid choice models (HCM) assume a particular causal relationship between the indicator variables, latent construct, and choice. In effect, the underlying assumption of such a model system is that latent variables of interest exist independent of the indicator variables used to measure them, and that the survey items used are reflective in nature insofar as responses to such questions reflect the underlying constructs. In this paper, we describe an alternative form of attitude measure, known as formative measures, where the items themselves are used to create the latent variable rather than the other way around. In addition to making a distinction between formative and reflective attitudinal measures, the paper seeks to describe how the HCM can be adapted to model different types of attitude question formats. Further the paper seeks to act as a catalyst for choice modellers to think more about the quality and validity of attitudinal items capture in survey questionnaires, by placing more emphasis on proper scale development techniques.","Discrete choice models (DCM) have become the dominant method for understanding preferences and behaviour of economic agents observed to make decisions either in real markets or in response to hypothetical scenarios presented to them as part of a broader questionnaire. Estimation of DCMs typically involves defining alternative specific utility functions that are used to describe the role that the attributes of the alternatives being examined and/or the characteristics of the decision makers’ play in the decision-making process. By observing how decisions vary as the attributes of the alternatives or characteristics of the decision makers differ within data, the analyst is able to derive utility weights for each that reflect how the modelled variables influence choice.====As noted over 40 years ago by McFadden (1980) however, “The theory of the economically rational utility-maximizing consumer, interpreted broadly to admit the effects of perception, state of mind, and imperfect discrimination, provides a plausible, logically unified foundation for the development of models of various aspects of market behaviour.” Inspired by the development of the hybrid conjoint model (e.g., Green et al., 1981; Green 1984), McFadden (1986) went on to derive the theoretical foundations for the hybrid choice model (HCM) designed specifically for the purposes of exploring the role that psychological constructs (e.g., attitudes, perceptions) play in discrete choice situations. First operationalized by Train et al. (1987), the model as it currently is utilized assumes that attitudes and perceptions are latent constructs that can be uncovered using indicator variables collected via surveys of the decision makers concurrently with the relevant choice data. Despite early acknowledgement of the importance that psychological factors play when individuals make decisions, it has only been in the last ten to fifteen years that research has really sought to examine how psychological, cognitive, emotional, and social factors, including attitudes and perceptions, influence choice behaviour.====To date, analysts employing the HCM have tended to assume the relationship between indicator variables and latent constructs be reflective, meaning that changes in the unobserved constructs manifest as variations in the corresponding observed indicator variables. As such, these models assume a particular directionality in the relationship between underlying unobserved latent constructs and the responses decision makers provide to survey questions measuring attitudes and perceptions. For example, an individual not concerned about climate change may respond accordingly by selecting values corresponding with such an attitude to a battery of questions dealing with climate change embedded within some survey. If later in life, due to personal circumstances, the same individual changes their underlying attitude towards climate change, then the responses to the same battery of questions would reflect this change. In this way, the latent construct informs the responses to the indicator variables, not the other way around. Such indicators have come to be termed reflective (or effects) indicators, and models to which they are applied to, effects models.====MacCallum and Browne (1993), however, note that in some cases indicators may be viewed as causing, rather than being caused by, the latent variable. Termed formative (or causal) indicators, changes in the values of the indicators result in changes in the underlying latent construct, the exact opposite to reflective indicator variables. Derived from research in psychology and sociology (e.g., Blalock 1964; Bollen 1984; Bollen and Lennox 1991), formative measures of attitudes have been used in marketing (e.g., Diamantopoulos 1999; Diamantopoulos and Winklhofer 2001; Rossiter 2002) and strategy (see e.g., Venaik et al., 2005).====Formative measurement scales differ to reflective scales in that the latent construct being measured is a function of the indicators, and as such, the causal direction between the latent construct and the indicators is reversed. Further, the characteristics of the indicators differ between the two approaches. In reflective models, the direction of concomitant variation is such that changes in the latent construct precede changes in the indicator variables employed to measure the latent construct. This suggests that whilst the indicator variables must be correlated with the latent construct, they may also be somewhat interchangeable in that the specific questions asked, subject to internal validity considerations, have no influence on the underlying latent construct being measured. On the other hand, the latent construct is assumed to be a function of the indicator variables used in formative measurement scales, meaning that the number and specific types of indicator variables adopted will impact on the latent construct. Further, in reflective measures, the expectation is that the indicator variables will be correlated with one another, otherwise they are not measuring the same underlying latent construct. With formative measurement scales, no such correlation is required (see Venaik et al., 2005).====The majority of choice modelling studies have to date employed reflective measures to capture the attitudes of those sampled, then employ the HCM framework to link underlying latent attitudinal constructs to individuals’ choice behaviour. The empirical analysis of such data in this setting is now widely accepted. However, it is worth noting that in some cases, researchers in different fields have inadvertently modelled formative scales as if they were reflective. Within the transportation literature, attitude measures have been defined as being either individual specific or alternative specific in nature (see e.g., Bahamonde-Birke et al., 2017). Under this taxonomy, individual specific measures are assumed to reflect more generally held attitudes of a global nature (e.g., attitudes towards the environment) whereas alternative specific measures relate to perceptions linked to specific goods or services within a market place (e.g., attitudes towards buses). Such a distinction, whilst useful, ignores the fact that the former type of measures is more likely to utilise formative measurement scales whilst the latter are more likely to make use of reflective measures. For example, Daziano (2012) defines appreciation of car features using items measuring eight aspects, these being purchase price, vehicle type, fuel economy, horsepower, safety, seating capacity, reliability, and styling. Such constructs are formative in nature insofar as they represent aspects of the vehicle rather than attempt to measure underlying attitudes towards vehicles. In a more recent study, Guzman et al. (2021) measure the satisfaction of public transport system considering the specific satisfaction for fare, comfort, security, as well as general overall satisfaction towards the public transport system. As with the measurement items formalized in Daziano (2012), these indicator variables appear to measure constructs that contribute to the satisfaction of a public transport system, rather than measure latent satisfaction. In other words, levels of comfort, security, and fare influence satisfaction, rather than being measures of underlying satisfaction. Similarly, Jin et al. (2020) attempt to measure satisfaction with a transport system in a reflective fashion by means of indicators measuring convenience, comfort, timeliness, cost, privacy and safety of the system, with the indicators forming (as opposed to depending on) the latent measure. Again, subjects build the satisfaction with the transport system on different domains, such as convenience, comfort and cost, and it is a change in these indicators that leads to a change in the satisfaction.====Within the Health Economics literature for example, Kløjgaard and Hess (2014) measure pro-surgery attitude via reflective indicators proxying back pain at present and past, leg pain at present and past, influence of pain onto relationships, and physical support needed. Once again, it would be more appropriate to consider the construct as formative, in that changes to the indicators (e.g., higher back pain) will change the subjective attitude towards surgery and not the other way around. In the same field, Santos et al. (2011) incorrectly framed the attitude towards toilets and sewerage connections, which is in fact formed by the indicators measuring prestige, monetary valorisation, comfort, better image, among others.====In a different field of research, Fantechi et al. (2022) explore the effect of three attitudes, namely attitude towards animal welfare, towards hunting and towards wild game meat in a preference study about meat consumption. The authors model all the latent constructs through a reflective measurement setting. This choice seems appropriate for the first two types of attitudes, which “cause” an impact on the corresponding measurement items. For example, the attitude towards animal welfare will influence the rating that the subject assign to the items “It is important that the food I normally eat has been produced in a way that animals have not experienced pain” and “It is important that the food I normally eat has been produced in a way that animals’ rights have been respected”. Opposite is the case of the third latent construct, attitude towards wild game meat, which is composed by, rather than reflects onto, the indicators safety (“It is safe to eat”), taste (“It tastes good”), and convenience (“Its price is fair compared to product quality”).====The purpose of the current paper is two-fold. Firstly, we seek to introduce the concept of reflective versus formative attitudes to the choice modelling community by extending the current HCM framework to include formative attitudinal measures. In doing so, we expose a known issue with the use of reflective attitude measures, that being the difficulty of utilising such measures for forecasting purposes, particularly when applied to cross-sectional survey data (see Chorus and Kroesen 2014). The second purpose of this paper seeks to demonstrate that the limitation of reflective measures in forecasting can be somewhat overcome when using cross sectional data if different respondents are exposed to alternate information states, that can serve as covariates alongside socio-demographic variables. For completeness, we also test the impact of different information states on models incorporating formative measures.====The remainder of the paper is organised as follows. In the following section, we discuss the theory and different modelling approaches that can be applied to formative and reflective attitude measures. Section 3 of the paper then describes an empirical study that is used to demonstrate the various models presented in Section 2. Next, the model results are presented, before section 5 demonstrates how each model can be applied in practice. The paper concludes with sections involving a general discussion and conclusion drawn.",Formative versus reflective attitude measures: Extending the hybrid choice model,https://www.sciencedirect.com/science/article/pii/S1755534523000131,14 May 2023,2023,Research Article,1.0
"Li Zili,Washington Simon P.,Zheng Zuduo,Prato Carlo G.","School of Civil Engineering, The University of Queensland, Brisbane, 4072, Australia,Advanced Mobility Analytics Group Pty Ltd, Australia,The School of Civil Engineering, the University of Leeds, UK","Received 19 December 2021, Revised 19 January 2023, Accepted 12 May 2023, Available online 13 May 2023, Version of Record 18 May 2023.",https://doi.org/10.1016/j.jocm.2023.100419,Cited by (0),"Revealed and stated choice data are fundamental inputs to understanding individuals’ preferences. Owning to the distinctive characteristics and complementary nature of these two types of data, making joint inference based on their combined information content represents an attractive approach to preference studies. However, complications may arise from the different decision protocols under the two distinct choice contexts. In this study, a ","The modelling and inference of individuals' preferences are fundamental in many areas of research. In transport, the analysis of travellers' mode choices provides valuable information for designing policies, evaluating projects and managing infrastructure (e.g., Bhat 1997; Miller et al. 2005; Vij et al. 2013; Ye and Titheridge 2017). In marketing, the investigation of consumers' preferences for brands or products is central to predicting purchase intent and uncovering market dynamics (e.g., Guadagni and Little 1983; Andrews and Srinivasan 1995; Cobb-Walgren et al. 1995; Erdem and Keane 1996; Shin et al. 2012). In labour economics, the study of the factors related to work participation is fundamental for understanding workforce diversity (e.g., Baanders 2002; Broadway et al., 2017; Garcia et al., 2018). In health and environmental economics, the understanding of the public's attitudes and preferences is essential for evaluating health or environmental programs (e.g., Adamowicz et al., 1997; Whitehead et al., 2008; Mentzakis et al. 2011; Andersson et al. 2016).====In the aforementioned contexts, the collected data normally consist of observations of individual choices. Depending on how choice outcomes are obtained, data are typically categorised into two types: Revealed Preference (RP) data contain the observed choices of individuals making actual choices, whereas Stated Preference (SP) data contain the choices made by individuals under hypothetical and controlled situations. Numerous researchers claim that RP data are more reliable than SP data because they represent actual choice situations, whilst the validity of the inferred preferences based on SP data alone may be questionable because of their hypothetical nature (e.g., M. Ben-Akiva et al., 1994; Louviere et al., 2000; Hensher et al. 2005). It is worth noting that RP data can also be imperfect, for example decision-makers may not know the exact attribute values of non-chosen alternatives (e.g. the travel time of the non-chosen bus and the towing capacity of the non-chosen SUV). Despite the known limitations of SP data, they have played and will continue to play an important role in preference studies and are accepted as a valid method for understanding preference behaviour. Their popularity is related to having a complementary nature to RP data, an ability to explicitly vary attribute values across alternatives, an advantage derived from designed experiments typical of SP studies, and being an ideal method for obtaining information on preferences for currently unavailable market offerings.====Due to the complementary nature of RP and SP data, making joint inferences from combined RP and SP data has emerged as an attractive approach to preference studies (M. Ben-Akiva et al., 1994; Brownstone et al. 2000; Bhat and Castelar 2002; Hensher et al. 2008; Cherchi and Juan de Dios Ortúzar, 2011). Combining RP and SP data allows inferences to be made based on all available information and leads to increased sample size and outcome robustness. However, additional challenges arise from making joint inferences, with two widely recognised issues being decision inertia and scale difference (Moshe Ben-Akiva and Morikawa, 1990; Bradley and Daly 1997; Brownstone et al. 2000; Morikawa et al. 2002; Cherchi and Juan de Dios Ortúzar, 2006; Börjesson 2008). Decision inertia — or state dependence —refers to the tendency of individuals to repeat the same choice: for example, SP choices may be influenced by familiarity with previously chosen alternatives or repetition of recent RP choices. Scale differences refer to the magnitude of utility coefficients being different: for example, SP estimated parameters may differ from RP estimated parameters not only because of an actual difference, but because of differences in the number of omitted factors affecting the choice across the RP and SP contexts — thus affecting the scale of the parameter estimates.====The decision inertia of decision-makers is usually captured within the utility function by incorporating an indicator variable that, for example, takes value one if the current choice in the SP data was also chosen by the same individual in the RP data — and essentially enables the testing of significance of the influence of one choice on the other. In contrast, scale difference is usually captured by normalising to the scale of the RP data and estimating de facto the ratio of the scale parameters that captures whether the SP data have less or more variance. While the consideration of decision inertia and scale difference represents an important aspect of preference studies from combining RP and SP data, additional features should also be considered when making joint inference.====Conceptually, the aforementioned issues arise from one fundamental source — the differences in choice behaviour under contrasting situations. For example, decision inertia arises when an individual's decision protocol under hypothetical situations minimises the cost of evaluating unfamiliar information — instead relying more heavily on past choice behaviour. In another example, scale differences arise when different sets of attributes are employed for uncovering preferences. Both of these examples highlight the underpinning nature of preference studies, namely that the heterogeneous behaviour of individuals requires careful capturing and modelling of complex decision making.====However, these two issues do not cover the gamut of issues from combining SP and RP data, and the present study contributes to the literature by delving into the details of how behavioural differences in the two contrasting choice contexts can be better captured in a model. Specifically, the present study combines serval well-established modelling techniques and proposes a modelling framework that accounts not only for the vastly investigated issues of decision inertia and scale differences, but also for the largely overlooked issues of decision-makers ignoring situation constraints, non-attending attributes and misinterpreting attributes. These issues have been long suspected to emerge from the two choice contexts on a conceptual level (Morikawa et al. 2002; Ben-Akiva et al., 2019), but have not been tackled in an integrated statistical model.====The remainder of the paper is organised as follows. Section 2 describes the behavioural constructs and underpinnings that motivate the model. Section 3 introduces the utility functions and the Bayesian estimation procedure of the model. Section 4 illustrates the application of the proposed model using a joint RP and SP travel mode choice dataset. Section 5 draws conclusions from the study and proposes further research avenues.",A Bayesian hierarchical approach to the joint modelling of Revealed and stated choices,https://www.sciencedirect.com/science/article/pii/S1755534523000209,13 May 2023,2023,Research Article,2.0
"Kazagli Evanthia,Lapparent Matthieu de","School of Management and Engineering Vaud (HEIG-VD), University of Applied Sciences and Arts Western Switzerland (HES-SO), Avenue des sports 20 - CP 521 - 1401 Yverdon-les-Bains, Switzerland,HEIG-VD. HES-SO, Avenue des sports 20 - CP 521 - 1401, Yverdon-les-Bains, Switzerland","Received 12 May 2021, Revised 23 January 2023, Accepted 24 April 2023, Available online 10 May 2023.",https://doi.org/10.1016/j.jocm.2023.100413,Cited by (0),"We present a discrete choice modeling framework with heterogeneous decision rules accounting for non-trading behavior. The proposed approach builds upon the state-of-the-art probabilistic finite mixture models and tackles non-trading behavior while accounting for inertia effects and serial correlation in the SP data, and contextual effects on the probability of an individual employing a specific decision rule. The framework involves three subpopulations of decision-makers, referred to respectively as pure utility-maximizers, utility-maximizers with strong preference for one alternative, and non-traders non-utility-maximizers employing a non-trading heuristic. The second subpopulation is expected to exhibit non-trading behavior, despite making trade-offs consistent with utility maximization. Our goal is to disentangle the two types of manifested non-trading behavior. We assume that the manifestation of non-trading behavior—by otherwise utility-maximizing individuals—may be driven by important ====. In order to accommodate this assumption in the modeling framework, we define and add a relative advantage (RA) component in the class-membership model. Finally, we apply the framework to a Swiss stated preferences (SP) mode choice case study, and demonstrate the impact of accounting for non-trading behavior on the value of time estimates."," refers to the case where an individual always chooses the same alternative across choice situations (Hess et al., 2010). This type of behavior is often observed in stated preferences (SP) choice surveys, where respondents are requested to answer several hypothetical choice tasks. Along similar lines, in real life contexts, we encounter ====, with the individual choosing what she chose last time she had to make the same or a similar choice. Opposite to these lies ====, with the individual choosing alternatives that have not been previously chosen. Such decisions-making strategies====—allowing the individual to minimize the cognitive effort—are highly pertinent to routine choices (see e.g. Adamowicz and Swait, 2012 in the context of food choices). They belong to the group of suboptimal decision strategies, commonly referred to as heuristics, connoting the omission of part, or all, of the information by the individual in order to make decisions faster and simpler, as opposed to normative decision strategies that assume a rational individual with almost complete information and sufficient capacity to process it for making ==== that result in an “optimal” choice. A comprehensive review of the decision heuristics within the discrete choice modeling (DCM) framework with SP data is presented by Leong and Hensher (2012a). After discussing the contribution of decision heuristics, as well as this of ====, in explaining choice behavior, the authors suggest that a logical way forward would be to ====The nature of the choice to be made may trigger a specific type of strategy. For instance, mode choices are found to be rather habitual (see e.g. Cantillo et al., 2007, Cherchi and Manca, 2011, Cherchi and Cirillo, 2014, Schmid et al., 2019). Gärling and Axhausen (2003) and Cantillo et al. (2007) define habit or inertia in the context of choice behavior as the “reluctance to change”====. High inertia may be manifested as non-trading behavior. In this context, non-trading can also be described as lexicographic behavior with respect to the alternative====. Indeed, Hess et al. (2010) identify the possible drivers behind non-trading behavior in SP data; these are (i) the strong preference towards a particular alternative, by an otherwise utility-maximizing individual, (ii) the ==== employed by a non-utility maximizing respondent due to fatigue, boredom, irrelevance of the attribute values, and more, and (iii) some sort of political or strategic behavior, such as never choosing a tolled road alternative. The authors argue that respondents in the first category, i.e. utility maximizers with strong preference towards a specific alternative, should not be excluded from a utility maximizing model, while those in the other two categories should ideally be identified and excluded from the model estimation in order to avoid biases in the derivation of policy indicators, such as the willingness to pay. They acknowledge the fact though that, in the majority of cases, it is not possible to discriminate between the two types of non-trading behavior.====The importance of identifying and modeling non-trading behavior in random utility frameworks, as pointed out by Hess et al. (2010), and the significance of ==== in the relevance of a decision rule, as pointed out by Leong and Hensher (2012a) and Hensher (2019), have motivated the modeling framework proposed in this paper. The work concerns a practical application that focuses on a model specification that accounts for contextual effects, which are based on objective and measurable factors, in order to tackle non-trading behavior. The proposed approach involves three subpopulations of decision-makers, referred to respectively as (i) pure utility-maximizers, (ii) utility-maximizers with strong preference for a specific alternative, and (iii) non-traders non-utility-maximizers (pure non-traders), adopting the non-trading heuristic to minimize the effort, for instance. It postulates that the manifestation of non-trading behavior, by otherwise utility-maximizing individuals, may be driven by important context variables, and more specifically by the overall relative advantage with respect to these variables, of ones preferred alternative over the remaining alternatives in the choice task. As discussed by Hess et al. (2010), in the case of extreme preferences, the choice design “====”. In order to test this, we include a relative advantage (RA) component (Leong and Hensher, 2014) in the class-membership model (CMM). Leong and Hensher (2014) propose the relative advantage maximization model (RAM) as an extension to the conventional linear additive random utility model (RUM) in order to account for context dependency in the representation of the choice set, and subsequently the impact of constructed preferences on the choice of an alternative. The use of the RA component here is different from the one in the RAM, in that it adds the RA component in the CMM, rather than in the class-specific choice model (CSM). Yet, its notion and formulation are the same as in Leong and Hensher (2014). Consequently, the RA component is used to describe the probability of an individual belonging to the second class, rather than her probability of choosing an alternative. This model specification aims at disentangling the two types of the manifested non-trading behavior.====Methodologically, the framework is based on the well-established probabilistic decision process modeling (PDPM) approach====. Such approaches have been presented by various papers in the general DCM literature tackling heuristics and non-compensatory choice behaviors. Different mixtures of decision rules have been considered by e.g. Elrod et al. (2004) who proposed an integrated model of disjunctive/conjunctive screening rules; Hensher and Greene (2010) who analyzed attribute non-attendance and dual processing with a latent class specification; Zhu and Timmermans (2010) who assume context-dependent preferences and decision heuristics and incorporate conjunctive, disjunctive and lexicographic decision rules in their modeling framework; Hess et al. (2012) who applied four different mixture of the RUM with other decision rules (lexicography, heterogenous reference points, elimination by aspects and random regret minimization) in four case studies; Leong and Hensher (2012b) who have performed an exploratory analysis of multiple mixtures of heuristics, including the reference point revision and the majority of confirming dimensions; McNair et al. (2012) who presented a PDPM tackling the value learning and the strategic misrepresentation heuristics; Hess and Stathopoulos (2013) who present a mixture of random utility and random regret model and use a latent variable modeling approach in analyzing the probability of each decision rule being followed by a respondent; Hensher et al. (2013) who apply a random parameter PDPM with different levels of attribute attendance (full/non-attendance, as well as aggregation of common attributes) to a SP for car commuters; Boeri et al. (2014) who analyze the performance of a utility maximization-regret minimization mixture in the context of traffic calming schemes, to find that unfamiliar users are more likely to be regret minimizers in the SP, in comparison with familiar users; Balbontin et al. (2017) and Hensher et al. (2018) who tackle the heterogeneity in decision processes in the presence of risk; Dey et al. (2018) apply a utility maximization-regret minimization mixture to bicycle route choice; and more.====This is to our knowledge the first work to integrate contextual effects in the weighting functions that define the class-membership model and the first to tackle non-trading behavior within the random utility framework. We present an application of the approach to a Swiss stated preferences (SP) mode choice dataset and demonstrate the influence of tackling non-trading behavior on the resulting value of time estimates. The model takes the form of a mixed logit. Its estimation treats serial correlation, that is the dependence of the responses provided by the same individual, through the inclusion of respondent-, alternative- and class-specific error components.====The remainder of the paper is organized as follows. Section 2 delineates the modeling framework. Section 3 presents the Swiss mode choice case study. Section 4 presents the results from the application of the proposed modeling framework to the available data. Section 5 summarizes the finding of the work and identifies directions for further research",A discrete choice modeling framework of heterogenous decision rules accounting for non-trading behavior,https://www.sciencedirect.com/science/article/pii/S1755534523000143,Available online 10 May 2023,2023,Research Article,3.0
"Budziński Wiktor,Daziano Ricardo","Faculty of Economic Sciences, University of Warsaw, Poland,School of Civil and Environmental Engineering, Cornell University, United States","Received 15 August 2022, Revised 21 March 2023, Accepted 7 May 2023, Available online 10 May 2023, Version of Record 17 May 2023.",https://doi.org/10.1016/j.jocm.2023.100416,Cited by (0)," for a variety of attributes of online grocery services related to the quality of the stock, delivery characteristics, and the cost of the online order. We characterize consumers in each segment by their observed characteristics as well as fear-related latent variables. On the one hand, we find that individuals who are actively protecting themselves against COVID-19 have a higher willingness to pay for almost all attributes. On the other hand, consumers who avoid crowds have a lower willingness to pay, but they assign relatively higher importance to no-contact delivery.",") as well as affected individuals’ well-being and mental health (====, ====, ====, ====). In this paper, we focus on online shopping. Specifically, we analyze New York City (NYC) residents’ preferences for online grocery shopping at the beginning of the COVID-19 pandemic. To this end, we exploit choice microdata from an online survey conducted at the beginning of May of 2020 in NYC when the “New York State on PAUSE” order was still in place.====The online grocery shopping sector has been steadily growing throughout the last decade (====), making it an important field of study within consumer research. This growth has been rapidly hastened by the pandemic. Although some of the interest in online grocery is likely to fall after the pandemic, some habits developed during this period are likely to continue (====). Furthermore, multiple large retailers invested in online grocery technology, and continue to do so, even after the number of COVID-19 cases dropped and government restrictions were lifted (====). The contribution of the current study is twofold. First, we contribute to the literature regarding online groceries by developing a choice experiment (CE) in which characteristics of the online grocery service related to delivery, quality of stock, and additional costs are each described by several attributes and therefore recognize the multidimensional nature of online purchasing decisions. Second, we study consumers’ behavior during the pandemic. Because of the surrounding global health crisis, there are emergent socio-psychological factors that are likely to affect individuals’ propensity to shop for groceries online, which were not considered before the outbreak. Recent studies analyzing such factors consider constructs such as perceived vulnerability and risk of infection (====), fear of the health-related and economic consequences of the pandemic (====), general uncertainty caused by the pandemic, attitudes towards social distancing guidelines (====), and the perceived risk of formal penalties (e.g., fines) for not complying with the governmental restrictions (====). With the pandemic still active, and having in mind possible future pandemics (====), it is important to understand how these factors affect individuals’ behavior. In this study, we focus on the effect of fear and anxiety that the pandemic has induced (====). Conceptually, we link individuals’ stated grocery choices in the CE with individuals’ responses to the pandemic. Specifically, we operationalize a choice model that integrates two attitudinal constructs. The first attitude measures individuals’ propensity to employ some active protection against COVID-19, such as wearing a mask outdoors or using disinfecting wipes. We hypothesize that individuals who take active actions against COVID-19 will be more likely to use online grocery services, as another measure to limit their exposure to the virus. With the second attitude, we measure individuals’ avoidance of crowds. One of the most direct responses to fear and anxiety is escape or avoidance (====). As social distancing is probably one of the most common non-pharmaceutical interventions introduced by the governments around the world (====, ====), the avoidance of crowds is probably the most straightforward reaction to the pandemic-induced fear. At the same time, offline shopping is often connected to crowding, as one of the main advantages of large retail shops is that they can process a large number of customers in a short time (====). As during the pandemic crowding had basically become a health hazard, we argue that a negative attitude towards crowding may act as a significant factor pulling individuals from offline shopping to online shopping. To combine CE data with measures of attitudes and socio-demographic variables we implement a hybrid choice model (====The rest of the paper is structured as follows. In Section ==== we present a literature review regarding online grocery shopping and coping with fear and anxiety in the context of consumer behavior. We also postulate the hypotheses that will be investigated in the current study. Section ==== provides details about the survey and CE that we use. In Section ==== presents the results of the analysis. The last section provides the discussion of the results and their implication for retailers as well as for public policy.",Preferences for online grocery shopping during the COVID-19 pandemic — the role of fear-related attitudes,https://www.sciencedirect.com/science/article/pii/S1755534523000179,10 May 2023,2023,Research Article,4.0
"Kim Yusun,Reeling Carson,Widmar Nicole J.O.,Lee John G.","Department of Agricultural Economics, Purdue University, West Lafayette, IN, USA","Received 12 October 2022, Revised 11 March 2023, Accepted 25 April 2023, Available online 3 May 2023, Version of Record 5 May 2023.",https://doi.org/10.1016/j.jocm.2023.100414,Cited by (0),"Sales of deer licenses, one of the most important revenue sources for wildlife management at the Indiana Department of ","Discrete choice experiments (DCEs) are widely used to estimate consumer demand for hypothetical goods and their attributes. This approach works by simulating a hypothetical market in which consumers are asked to choose one of several mutually-exclusive alternatives based on their preferences (Holmes et al., 2017). Defining alternatives as a set of attributes with one or more levels, researchers can infer individuals’ preferences by analyzing how they trade off different levels of each attribute when making choices about which goods they would hypothetically buy.====We design a novel DCE that accounts for forward-looking behavior to estimate a choice model for hypothetical lifetime deer hunting licenses among hunters in the U.S. state of Indiana. As the name implies, this license would give hunters the right to harvest a fixed number of deer per season for the rest of their lives in exchange for paying an up-front fee equal to many times the price of existing annual licenses.====The fact that (i) the present value of utility hunters of different ages will receive from a lifetime license depends on their ages—and, hence, the number of remaining years they expect to hunt—and (ii) the decision to purchase a lifetime license obviates the need to purchase future licenses makes this a simple form of optimal stopping problem akin to Rust (1987). Put differently, the choice problem we simulate with our DCE is necessarily dynamic. Prior work commonly uses DCEs to estimate individuals' preferences for other types of long-lived, durable goods like personal vehicles (Costa et al., 2019; Byun et al., 2018; de Jong et al., 2009), home appliances (Lang et al., 2021; Zha et al., 2020) and capital-intensive farm production systems (Howard et al., 2023; Gramig and Widmar 2018; Olynk et al., 2012). However, none of this work explicitly models forward-looking behavior on the part of DCE subjects. The analytical model underpinning this prior work is McFadden's (1974) random utility maximization model which, in its standard form, assumes decisionmakers are myopic—or, at the very least, is not explicitly dynamic.====Ignoring forward-looking behavior in designing a DCE is of no consequence if the values of relevant state variables underlying the simulated decision problem are independent of subjects' current choices (in which case the decision problem is not dynamic in any meaningful sense). Less trivially, if the DCE is designed in such a way that any possible decision leads to one or more absorbing states then, conditional on their contemporaneous choice, the subject's entire future becomes known up to an expectation of the state variables. If neither of these conditions hold—and it is not evident that they do in prior work—then we may expect biased estimates of subjects' preference parameters, which will also impact the levels of any quantities estimated from them (e.g., willingness to pay or market shares).====We design our DCE in accordance with an analytical model that explicitly considers subjects' forward-looking behavior when evaluating lifetime license purchases. Our design is inspired by the one-step-ahead conditional choice probability approach of Hotz and Miller (1993). Specifically, we show hunters a take-it-or-leave-it lifetime license offer. If the hunter chooses to purchase a lifetime license, he or she receives the same utility from that lifetime license each year for the rest of his or her life. If the hunter does not buy a lifetime license, he or she gets the level of utility that corresponds to the ==== combination of licenses he or she buys every year. Our approach effectively turns a dynamic problem into a static one by eliminating subjects' need to consider choices beyond the present period. It also allows us to derive the present value of future utility a hunter receives from either of the choices—and hence hunters’ choice probabilities—in closed form, which is convenient for estimation.====Our empirical approach is informed by the extensive literature on dynamic discrete choice models (see Aguirregabiria and Mira (2010) and Eckstein and Wolpin (1989) for reviews). This prior work primarily relies on observational data; to our knowledge, we are the first to apply these insights in the context of a DCE. This study also makes an empirical contribution by being the first to use DCEs to value lifetime hunting license attributes. DCEs are commonly used to value attributes of big game hunting, including game animal density (Boxall and Macnab 2011; Boxall et al., 1996; Haener et al., 2001; Horne and Petäjistö 2003; Hunt et al., 2005; Kerr and Abell, 2016), the probability of successful harvest (Hussain et al., 2003; Mackenzie 1990), and policy choices like bag limits and season length (Serenari et al., 2019).====In what follows, we first provide background on deer hunting in Indiana to give context to our choice model, which we develop in Section 3. We then describe our DCE and data. Section 4 describes our estimation approach. Section 5 presents parameter estimates from our dynamic model. We compare these to estimates from a myopic model and explain why they generally differ. Section 6 provides an overview and conclusion.",Estimating a model of forward-looking behavior with discrete choice experiments: The case of lifetime hunting license demand,https://www.sciencedirect.com/science/article/pii/S1755534523000155,3 May 2023,2023,Research Article,5.0
"Saxena Shobhit,Bhat Chandra R.,Pinjari Abdul Rawoof","Department of Civil Engineering, Indian Institute of Science (IISc), Bengaluru, 560012, India,Department of Civil, Architectural and Environmental Engineering, The University of Texas at Austin, 301 E. Dean Keeton St. Stop C1761, Austin, TX, 78712, USA,Department of Civil Engineering, Centre for Infrastructure, Sustainable Transportation, and Urban Planning (C,STUP), Indian Institute of Science (IISc), Bengaluru, 560012, India","Received 9 August 2022, Revised 27 January 2023, Accepted 12 March 2023, Available online 17 March 2023, Version of Record 31 March 2023.",https://doi.org/10.1016/j.jocm.2023.100411,Cited by (0),"Many ==== systems involve the estimation of a covariance matrix that must be positive-definite. A common strategy to ensure positive definiteness of the covariance matrix is through the use of a Cholesky parameterization of the covariance matrix. However, several model systems require imposing restrictions on the elements of the covariance elements. For instance, modelling systems may require fixing some (or all) of the diagonal elements in the covariance matrix to unity due to identification considerations. However, imposing such restrictions using the traditional Cholesky decomposition approach is not feasible and requires the additional parameterization of the Cholesky elements.====In this paper, we explore a separation-based strategy with spherical parameterization of the Cholesky matrix to impose restrictions on the covariance matrix. Importantly, using this separation-based parameterization strategy, we also explore the possibility of restricting some covariance (or correlation) terms to zero. The effectiveness of the proposed strategy is assessed through extensive simulation experiments. The results from the simulation experiments highlight better performance of the separation-based strategy in terms of recovery of model parameters – particularly those in the covariance matrix, than the traditional Cholesky parameterization approach. Finally, the proposed strategy is implemented in a joint multivariate binary probit ordered ==== system to analyze the usage (and the extent of use) of non-private modes of transportation in Bengaluru, India. In doing so, the proposed strategy is implemented to restrict several correlations to zero, thus avoiding the estimation of a profligate correlation matrix and substantially easing the estimation process.","The estimation of statistical and econometric models that include multiple outcome variables (that is, a multivariate dependent variable model) has increased over the years, thanks to the ability to generate multivariate distributions through the use of relatively flexible copula-based methods and/or the use of effective factorization techniques for the parsimonious estimation of covariance matrices (see, for example, Rana et al., 2010; Bhat, 2015; Müller and Czado, 2018; Jiryaie and Khodadadi, 2019; Ong et al., 2018). Besides, the alternative of simply ignoring the dependence and estimating separate models, while computationally appealing, is inefficient in estimating covariate effects for each outcome because it fails to borrow information on other outcomes, and is limiting in its ability to answer intrinsically multivariate questions such as the effect of a covariate on a multidimensional outcome (Teixeira-Pinto and Harezlak, 2013). Perhaps, more importantly, when the intention is to consider the potential effect of one outcome (say outcome A) on another outcome (say outcome B), the simple introduction of outcome A as an independent exogenous variable in the modeling of outcome B immediately triggers possible endogeneity bias effects because of the inter-relationship between the two outcomes due to common unobserved effects. On the other hand, the explicit consideration of the possibility that the outcomes may be co-determined controls for this possible endogeneity bias.====An important consideration in multivariate models (or even in univariate models with multiple random coefficients on exogenous variables with correlations across the coefficients) is the estimation of a covariance matrix (sometimes also referred to as a variance-covariance matrix, though we will use the shorter label “covariance matrix” that includes both the diagonal and non-diagonal elements). This covariance matrix, along with other model parameters, is estimated using Bayesian or frequentist methods and will generally involve optimization methods that extensively search over a large parameter space.==== In these estimation methods, a challenge is to maintain positive definiteness of the covariance matrix all through the search process (equivalently, to ensure that the covariance matrix does not become nonpositive definite).==== This can be achieved in one of two ways. The ==== is to impose restrictions on the covariance elements to ensure positive definiteness, which will generally lead to constrained optimization methods. Unfortunately, such constrained optimization methods collapse to multiple unconstrained problem estimations with some trial-and-error or to solving a complex non-linear equation system that is itself difficult to formulate (Dennis and Schanbel, 1989; Pinheiro and Bates, 1996). In particular, each unconstrained estimation typically tends to be undertaken using a simple one-level Cholesky decomposition schemes that write the Cholesky elements in a form conforming to the unit diagonal vector in the correlation matrix. The problem with such an approach (see Srinivasan and Bhat, 2005) is that the estimation can break down, unless the code imposes a steep penalty if any of the diagonal Cholesky elements turn out to be complex (imaginary) during the search process. While a reasonable strategy, such estimations also typically entail the construction of a “nearest” valid correlation matrix when positive definiteness fails (for example, by replacing the negative eigenvalue components in the correlation matrix with a small positive value, or by adding a sufficiently high positive value to the diagonals of a matrix and normalizing to obtain a correlation matrix; see Rebonato and Jäckel, 2000, Higham, 2009, and Schöttle and Werner, 2004 for detailed discussions of these and other adjusting schemes; a review of these techniques is beyond the scope of this paper). And even then, there is no guarantee that the correlation matrix at “convergence” will be positive definite. Thus, it is almost always the case that a ==== that involves a reparameterization of the covariance matrix in a way that renders the resulting estimation process completely unconstrained (while also enforcing the positive definiteness condition) is the preferred method.====Many different reparameterization approaches for the unconstrained estimation of covariance matrices have been proposed in the literature, including a spectral decomposition method, a matrix logarithm method, the typically used Cholesky decomposition approach for the covariance matrix (which only guarantees semi-positive definiteness rather than positive definiteness), modified Cholesky decomposition methods for the covariance matrix (such as the so-called LDLT decomposition and the log-Cholesky decomposition), the Cholesky decomposition of the inverse of the covariance matrix, factor-analytic approaches for the covariance matrix, and a spherical parameterization approach for the covariance matrix that combines the Cholesky decomposition with a specific spherical parameterization of the Cholesky matrix (see Lindstrom and Bates, 1988, Pinheiro and Bates, 1996, Pourahmadi, 2000, Leonard and Hsu, 1992, and McNeish and Bauer, 2022 for details and reviews of these methods). However, these decompositions, except for the spherical parameterization, are not immediately suitable for estimation when correlation matrices are the focus rather than covariance matrices (this is because these decompositions do not adhere to the additional restriction of unit diagonals of a correlation matrix). For instance, correlation matrices are the focus in a multivariate binary choice model system or a multivariate ordered response system (see Bhat et al., 2010; Dias et al., 2020; Bhat and Mondal, 2021), where the scale of the latent variables underlying the limited dependent outcomes have to be normalized.====Even in cases where a covariance matrix is to be estimated, there may be substantial value in breaking down the covariance matrix into a scale (standard deviation) matrix and a correlation matrix and estimating these separately. Barnard et al. (2000) refer to such an approach as a “separation strategy”, while we also see this as a “divide and conquer” strategy. Doing so has both specifications as well as estimation advantages. ====, it is more natural for analysts to think about standard deviations (which are scale-related) and correlations (which are scale-free) in expressing (and imposing) a priori judgements about the relationship among a set of variables, whether in a Frequentist setting or a Bayesian setting. For example, consider a set of random coefficients in an inter-city travel mode choice model on such level-of-service (LOS) explanatory variables as in-vehicle travel time, out-of-vehicle travel time, travel cost, and service frequency. If there is a belief that income and gender play a role in trade-offs, one can specify different mean coefficients on the LOS variables for each of the four segments. But rather than specify the same covariance matrix for these random parameters across the four segments (which could be very restrictive), or allow a separate covariance matrix for each of the segments (which would lead to a profligate model in parameters, potentially not estimable with the sample size for estimation), one could settle for an intermediate specification that specifies the scale (the standard deviation matrix, or spread of values) of the random coefficients to be different across the segments, but maintains the same dependence structure (correlation matrix, or multivariate dependency shape) for the random coefficients across the segments. Similarly, in the case of a multivariate mixed dependent outcome model, the analyst may reasonably assume (in terms of the best balance between parsimony and behavioral realism) that, in a mixed model system of residential choice (say in 3–4 broad categories of downtown, urban, suburban, and rural living), bicycle ownership, car ownership, commute mode choice, and the number of leisure trips per time period, the correlations across the underlying latent variables for the latter four dimensions are the same across residential locations, but the scales are different. Similar specification structures of identical correlation patterns, but different scales, are commonly used for model coefficients or for the relationship across multiple outcomes when a single model is estimated from different sources of data (such as revealed and stated preference data). At the same time, a logical consideration in such model systems can be to restrict correlations across certain specific dimensions. For instance, in a multiple discrete-continuous model system, a behaviorally consistent consideration would be to restrict correlations between the discrete preference of a good ==== and the continuous preference of a good ==== (==== ==== ====) to zero.====, separating out the scale from correlations is helpful in multivariate mixed outcome models (these models have a mix of different types of outcomes, such as continuous, ordinal, grouped, count, and unordered-response outcomes). In such models, the scale of the latent variables underlying the non-continuous and non-grouped outcomes will be fixed to one for identification purposes, while those for the continuous and grouped outcomes will be left free for estimation. Directly estimating a covariance matrix to adhere to these constraints (of some diagonal elements constrained to one and others left free), while also using decomposition techniques to preserve positive-definiteness, leads to a complex and unwieldy situation. In such situations, it is much easier (if not the only way) to use the partitioning strategy into a separate scale matrix and a separate correlation matrix with uniform entries of one on the matrix diagonal. This issue becomes particularly obvious when coding software for the estimation of such models. Besides, de-scaling before estimating interdependencies generally leads to much faster and more stable convergence (see Kohli et al., 2019), and, in a Bayesian inference context, leads to a simple computational strategy for obtaining the posterior distributions of the scale and the correlation matrices (Barnard et al., 2000).====The importance of the separation strategy, while invoked as a potentially effective strategy for Bayesian estimation in Barnard et al. (2000) in many model applications, has particularly started seeing much application in the estimation context of mixed multivariate outcome modeling (see, for example, Jiryaie and Khodadadi, 2019), skew-normal copula models (which are one form of mixed outcome modeling, because the skew can be generated through a latent variable censoring mechanism with the scale of the latent variable set to one; see, for example, Sidharthan and Bhat, 2012), and multinomial probit modeling (see, for example, Bhat and Lavieri, 2018). The precise unconstrained parameterization adopted to ensure that the correlation matrix is positive definite in such applications (as well as in other applications where a correlation matrix is of interest, such as in multivariate ordinal response systems or latent construct-based factorization models; see Bhat, 2015) has varied from simple one-level Cholesky decomposition schemes that write the diagonal Cholesky elements in a form conforming to unit diagonal in the correlation matrix (see, for example, Srinivasan and Bhat, 2005; Bhat and Lavieri, 2018) to specific multi-level Cholesky parameterization schemes. The problem with the first one-level decomposition scheme is that the estimation can break down unless the code imposes a steep penalty for the diagonal elements of the Cholesky if any of these elements turn out to be zero or negative during the search process. While a reasonable strategy, such estimations can require a good bit of handholding during estimation. In the second set of multi-level decomposition schemes, three methods are available, all of which have a common second-level parameterization for the Cholesky elements but differ in the third-level of parameterization (see Bhat and Mondal, 2021 for a detailed discussion): (1) the partial correlations method (Joe, 2006), (2) the spherical parameterization method (Pinheiro and Bates, 1996; Rebonato and Jäckel, 2000), and (3) the radial parametrization method (van Oest, 2021). Of these, the first partial correlations method tends to be relatively complicated in application, and the latter two parameterizations have been shown to be essentially equivalent but for a scaling difference (Bhat and Mondal, 2021).====In this paper, we first examine the value of the separation or “divide and conquer” strategy as it relates to convergence during estimation and the recovery of “true” parameters, and compare this approach with the more traditional Cholesky decomposition approach for covariance matrices. In addition, we also exploit the separation-based strategy to explore the possibility of imposing restrictions on the covariance matrix. Specifically, we explore the possibility of restricting some correlation elements to zero.====Surprisingly, there has been very little exploration of these issues, with most applications using the traditional Cholesky decomposition approach. Of course, as already discussed, the traditional Cholesky decomposition approach is not appropriate for mixed models with some diagonal entries of the covariance matrix normalized to one for the identification or restricting some correlation values to zero. Thus, in the simulation experiments used in the comparison of the two alternative covariance decomposition methods to maintain semi-positive definiteness, we consider a mixed model with unrestricted diagonal entries, except for a top diagonal restriction that can be easily accommodated in both the decomposition approaches. However, we still restrict some correlation values to zero to highlight the efficacy of the proposed strategy. Specifically, we consider the case of a multivariate mixed model with one multinomial unordered-response outcome (with four alternatives), a grouped outcome, and a continuous outcome (leading to a five-dimensional covariance matrix in estimation). Issues of convergence, computation time, as well as accuracy and precision in recovering parameters are examined using both the separation method and the traditional Cholesky method. Further, in the separation method, Bhat and Mondal (2021) indicate that the scale embedded in the logistic function of the spherical (or equivalently, radial)==== parameterization (for the correlation matrix) can impact convergence rates and computational times. Thus, we explore such scale impacts too within the context of the separation approach. Second, we consider a motivating example for the case of a multivariate dependent variable model where some diagonal entries of the covariance matrix are normalized to one for identification. In this case, unless external boundary constraints are placed through a restricted maximum likelihood approach (which, as discussed earlier, is to be avoided because it can lead to very substantial estimation difficulties), one has to employ the separation approach. In this context, we demonstrate the application of the separation strategy through an empirical analysis. In doing so, we also present and demonstrate the use of an approach to estimate model specifications with restricted correlation matrices (that is, some correlation elements are restricted to zero). This is an issue that has received surprisingly no attention (as far as we are aware) in the literature but is important because restrictions on the correlation matrix do not immediately translate to convenient restrictions that can be imposed on the Cholesky elements of the correlation matrix. We develop an algorithm to impose such restrictions, along with an algorithm for the gradient of the correlation elements with respect to active Cholesky elements. The algorithm is implemented in Gauss software and is made available for its use.====The rest of the paper is structured as follows: Section 2 discusses different parameterization approaches to facilitate the estimation of the covariance matrix. Next, building on the separation-based strategy with spherical parametrization, we discuss (and derive) conditions to restrict some correlation parameters to zero. In Section 3, we present simulation experiments to exemplify the benefits of the proposed strategy over the traditional Cholesky parametrization approach. In Section 4, we illustrate the use of the proposed separation-based strategy as well as the procedure developed for restricting specific correlations to zero for an empirical application to analyze the usage and extent of use of non-private modes in Bengaluru, India. Finally, Section 5 concludes the paper with a quick summary.",Separation-based parameterization strategies for estimation of restricted covariance matrices in multivariate model systems,https://www.sciencedirect.com/science/article/pii/S175553452300012X,17 March 2023,2023,Research Article,6.0
"Zong Weiyan,Zhang Junyi,Yang Xiaoguang","School of Rail Transportation, Soochow University, Suzhou, 215131, China,School of Transportation, Southeast University, Nanjing, 211189, China,Mobilities and Urban Policy Lab, Transdisciplinary Science and Engineering Program, Graduate School of Advanced Science and Engineering, Hiroshima University, Higashihiroshima, 739-8529, Japan","Received 27 August 2021, Revised 24 November 2022, Accepted 24 February 2023, Available online 6 March 2023, Version of Record 9 March 2023.",https://doi.org/10.1016/j.jocm.2023.100410,Cited by (0), and local cities are discussed.,"Understanding population migration is crucial to regional and urban planning/policymaking (United Nations , 2016; Murillo, 2017; Sakamoto et al., 2018; Bernt, 2019; OECD, 2019; Wu et al., 2019). Owing to massive out-migration from local cities and rural areas, population concentration in three megacity regions of Japan (i.e., the Capital Area centered around Tokyo, the Chukyo Area centered around Nagoya, and the Kinki Area centered around Osaka) has been a serious problem for several decades. As shown in Fig. 1, population in the three megacity regions accounts for 60.3% of the whole population in 2019 and shows a continuing growth. In particular, the Capital Area accommodated 35.1% of Japanese population in 2019. In contrast, population in local cities and rural areas of Japan peaked in 1998 and since then, a declining trend had been a “new normal”, leading the population in 2019 to decrease to the same size as the mid-1970s. Before the 1970s, population decline was not regarded as a problem by local governments. One phenomenon during this period was the “employment en masse” of junior high school students. Such massive employment mobilities from rural areas to urban areas were actively promoted nationwide and contributed to the country's rapid economic growth (Katase, 2010). However, such migration mobilities caused serious problems for both local and central governments (e.g., public services have been worsened and regional community functions have been on the verge of collapse due to reduced tax revenues caused by the withdrawal of industries====). It is therefore important to better understand the reason and mechanism of population migration into megacity regions of Japan. This argument is also applicable to other countries troubled by similar social issues.====Migration mobility has been investigated using macrolevel, mesolevel and microlevel theories (Zhang et al., 2017). This study focuses on individual migration mobility and associated dynamics over the life course. A review of literature suggests that existing migration decision models cannot capture complicated and dynamic decision-making process over the life course. Defining time-varying utility is especially problematic. Migration is usually triggered by motives related to employment, environment as well as social relations (Lundholm et al., 2004). In this regard, people sometimes need to make a trade-off between own preferences and concerns about other family members. In other words, influences of altruism should not be underestimated in depicting individual migration decisions (Coulter et al., 2016; Burum et al., 2020). Migration may also lead to emotional transfer (Blunt et al., 2012). Namely, emotional linkage with a migration destination may be influential to migration decision-making. Place attachment is usually used to describe people's emotional connection to a place such as home, neighborhood, city, region, country or continent (Lewicka, 2011). Neighborhood is deemed as the proper scale in the research on place attachment (Lewicka, 2010); however, it is also argued that such affection could be stronger at the city level (Casakin et al., 2015). Settling down in a city and recognizing the city as hometown are intertwined (Blunt and Sheringham, 2019), suggesting that place attachment could be formed through accumulation of affection. Furthermore, influences of factors affecting migration decisions change over time and function in a complicated way (Morris et al., 2018). Roles of these factors in migration decision-making over the life course have not been adequately investigated. Filling such a research gap is important to figure out how to resolve various issues caused by the over-concentration of population in megacity regions from the perspective of individual migration decision-making.====This study aims to develop a life-course model for migration mobility, namely migration destination biography model. Biography is a set of mobilities observed over the life course (Zhang et al., 2014). Based on the migration destination biography model, three research questions will be answered.====Answering ==== helps policymakers understand temporal and spatial features of migration. In Japan, a variety of regional revitalization policies have been implemented since the 1980s, and the central government has further established a special department for regional revitalization in 2014. As evidenced in Fig. 1, effects of existing efforts are limited in the sense that population of other areas have declined at a steady rate since the 1990s. Most existing policies emphasize infrastructure development and pay insufficient attention to improving quality of life. Related to this concern, ==== is posited to capture the key behavioral and psychological factors affecting migration decision-making. Understanding the roles of motives, altruism and place attachment allows authorities to make local life-oriented policies. By adopting a centrally-controlled uniform development strategy, regional identity has been seriously damaged and gradually disappear in many places. Loss of regional identity undoubtedly discourages people, especially young people, to reside in local cities and rural areas. Behavior and associated factors change over the life course, motivating us to raise ====. Addressing ==== needs ==== and ==== to be answered taking life-course changes into consideration, and then reflecting the time-varying mechanism underlying migration mobility.====Concretely speaking, this study develops a life-course intertemporal discrete choice model to depict individual migration destination biography. It is a discrete choice model with cross-sectional and longitudinal heterogeneities (DCLH model) and is further improved by introducing quasi-hyperbolic utility based on time preference theory. The resulting model is called DCLH-QHDU model. In DCLH model, parameters of explanatory variables are decomposed into elements connected with different points in time. The DCLH-QHDU model is based on time preference theory (Yu et al., 2017), by distinguishing the influences of present, past and future utility on migration decision-making. Time preference theory argues that the utility of current choice includes retrospective utility (accumulated state dependence), present utility and future utility (expectations). To the best of our knowledge, this is the first attempt in the literature of migration research to incorporate both cross-sectional and longitudinal heterogeneities over the life course and apply time preference theory. These two models are applied to investigate heterogeneous dynamics of migration biography, together with influences of motives, altruism and place attachment.====The remainder is organized as follows: Section 2 reviews existing studies on migration behavior. Section 3 introduces a migration biography survey conducted by the authors in the Capital Area of Japan and presents descriptive analyses. Migration biography models are developed in Section 4, followed by estimation results and empirical analyses in Section 5. Conclusions are elaborated in Section 6, together with policy implications and a discussion on research limitations.",Building a life-course intertemporal discrete choice model to analyze migration biographies,https://www.sciencedirect.com/science/article/pii/S1755534523000118,6 March 2023,2023,Research Article,7.0
"Beeramoole Prithvi Bhat,Arteaga Cristian,Pinz Alban,Haque Md Mazharul,Paz Alexander","School of Civil & Environment Engineering, Queensland University of Technology, Australia,Howard R. Hughes College of Engineering, Civil and Environmental Engineering, University of Nevada-Las Vegas, Las Vegas, NV, USA,Economic Research and Analysis, Queensland Department of Transport and Main Roads, Australia","Received 29 March 2022, Revised 15 February 2023, Accepted 21 February 2023, Available online 27 February 2023, Version of Record 4 March 2023.",https://doi.org/10.1016/j.jocm.2023.100409,Cited by (1),"Estimation of discrete outcome specifications involves significant hypothesis testing, including multiple modelling decisions which could affect results and interpretation. Model development is generally time-bound, and decisions largely rely on experience, knowledge of the problem context and statistics. There is often a risk of adopting restricted specifications, which could preclude important insights and valuable behavioral patterns. This study proposes a framework to assist in testing hypotheses and discovering mixed-Logit specifications that best capture discrete outcome behavior. The proposed framework includes a mathematical programming formulation and a bi-level constrained optimization algorithm to simultaneously test various modelling assumptions and produce meaningful specifications within a reasonable time. The bi-level framework illustrates the integration of a population-based ==== with model estimation procedures. In addition, the optimization algorithm allows the analyst to impose assumptions on the models to test specific hypotheses or to ensure compliance with literature. Numerical experiments are conducted using different datasets and behavioral processes to illustrate the efficacy of the proposed extensive hypothesis testing in terms of interpretability and goodness-of-fit. Results illustrate the ability of the proposed algorithm to reveal important insights that can potentially be overlooked due to limited and/or biased hypothesis testing. In addition, the proposed extensive hypothesis testing generates multiple acceptable solutions, thereby suggesting potential directions for further investigation. The proposed framework can serve as a decision-assistance modelling tool in various applications, involving many variables and outcomes, such as road safety analysis, consumer choice behavior, and integrated land-use and travel choice models.",None,Extensive hypothesis testing for estimation of mixed-Logit models,https://www.sciencedirect.com/science/article/pii/S1755534523000106,27 February 2023,2023,Research Article,8.0
"Ahtiainen Heini,Pouta Eija,Zawadzki Wojciech,Tienhaara Annika","Natural Resources Institute Finland (Luke), Latokartanonkaari 9, FI-00790, Helsinki, Finland,Department of Economic Sciences, University of Warsaw, Długa 44/50, 00-241, Warsaw, Poland","Received 18 March 2021, Revised 13 January 2023, Accepted 20 January 2023, Available online 2 February 2023, Version of Record 3 March 2023.",https://doi.org/10.1016/j.jocm.2023.100401,Cited by (0),"An important component of the design phase of a discrete choice experiment (DCE) is formulating the cost vector, which specifies the costs of the alternatives and enables the calculation of marginal ==== (WTP) estimates. If the cost vector affects choice behaviour, welfare estimates may depend on the choice of the cost vector, which leads to problems with the validity and reliability of DCE results. We employ a split-sample design to examine cost vector effects on choice behaviour and WTP estimates. Our data come from a DCE on agri-environmental policies to a nationally representative sample in Finland. We provide additional insights compared to previous research by including four cost vectors with otherwise identical surveys and experimental designs and a positive cost for the status quo alternative, with cost levels for policy alternatives both below and above the status quo cost. We obtain some evidence that the cost vector affects choice behaviour, as the proportion of status quo choices is larger with higher cost vectors. Both absolute and relative cost levels matter for choices. The marginal WTP estimates are highest in the sub-sample with the largest range cost vector that has cost levels both below and above the status quo cost. We suggest more careful pre-testing of the cost levels compared to current practices to determine a plausible range of cost levels to produce valid welfare estimates.","An important component of the design phase of a discrete choice experiment (DCE) is formulating the attribute levels, including the cost vector, which specifies the costs of the alternatives and allows the calculation of monetary welfare estimates (marginal willingness to pay, WTP) for changes in attributes and their combinations. Most DCE studies have employed a single cost vector, chosen based on pre-testing of attribute levels or insights from previous valuation studies.====In general, the attribute levels in a DCE should be realistic and plausible to respondents, and useful for decision-making (Johnson et al., 2013). For the cost vector, this implies that the levels should be credible to avoid possible biases and protest behaviour (Johnston et al., 2017), thus refraining from presenting implausibly high or low costs for the good. Credibility of the attribute levels can and should be evaluated as part of pre-testing, and this is particularly important for the cost attribute. However, there is no clear guidance on the practical construction of the cost levels, and thus DCE studies may employ different strategies in determining the levels.====This should not be a problem if respondents adhere to the theoretical assumptions of having stable and well-defined preferences, as in that case the cost vector should not affect the observed choices and WTP. However, there is empirical evidence from DCEs that various features of the choice context can have an impact on the subsequent estimates, such as the range of attribute levels and inclusion of a cost attribute (Mørkbak et al., 2010; Luisetti et al., 2011; Pedersen et al., 2011; Glenk et al., 2019). In addition, studies in behavioural economics have observed the provision of arbitrary anchors or value cues to affect preferences (Ariely et al., 2003, Tversky and Kahneman, 1974; Tversky and Kahneman, 1974), which could occur particularly when respondents are uncertain about their valuations (Glenk et al., 2019). For example, in DCEs on less familiar environmental goods and services, respondents may use the cost levels in the valuation task as information on the good's value and anchor their responses to the costs presented (Mitchell and Carson 1989; Kragt 2013). If this is the case, welfare estimates may be affected by the choice of the cost vector.====Cost vector design has been the focus of several contingent valuation studies (e.g., Cooper and Loomis 1992; Kanninen and Kriström 1993; Vossler et al., 2004), and several previous studies have used split samples to investigate the impact of different cost vectors on choice behaviour in the DCE setting (Ryan and Wordsworth 2000; Hanley et al., 2005; Carlsson and Martinsson 2008; Mørkbak et al., 2010; Kragt 2013; Liesivaara and Myyrä 2014; Svenningsen and Jacobsen 2018; Glenk et al., 2019). These studies have examined effects on parameter estimates and marginal WTP, as well as on systematic choice behaviour, such as the proportion of status quo choices and protest responses, the proportion of respondents choosing the lowest or highest cost alternative in all choice sets, and bid acceptance at different cost levels. The findings of these studies have been mixed.====Ryan and Wordsworth (2000) examined the effect of attribute levels on WTP estimates. For five of the six attributes, including cost, there were no significant differences in the parameter estimates across the two split samples, but unit WTP estimates were significantly different for four attributes. However, no difference was found in the WTP for a programme (combination of attributes). Hanley et al. (2005) used a split-sample design to compare two cost vectors, reporting no significant impact on preferences or WTP estimates. Carlsson and Martinsson (2008) observed significantly higher marginal WTP estimates for attributes when a scaled-up cost vector was used. Examining the effect of changes in the maximum level of the cost attribute, Mørkbak et al. (2010) demonstrated significant differences in preferences and WTP estimates, with an increased level of the maximum cost inducing higher WTP estimates for all attributes. Kragt (2013) investigated the effect of different cost vectors on choices, allowing for individual preference and scale heterogeneity, finding, for the most part, no significant difference between the treatments. Liesivaara and Myyrä (2014) used three cost vectors and found WTP to be sensitive to the cost vector employed, with the high vector producing higher welfare estimates. Svenningsen and Jacobsen (2018) tested the impact of different survey designs, including changes in cost range, on hypothetical bias. The results did not allow for isolating the effect of varying the cost vector, but they were able to conclude that the observed hypothetical bias was likely caused by differences in either the elicitation format (hypothetical versus actual payment) or the cost range, instead of changing the payment vehicle. Glenk et al. (2019) examined the effect of cost vectors on choice behaviour, also considering the role of respondents’ income. Their results indicated that in addition to affecting WTP estimates, cost vectors can influence decision strategies via differences in attribute non-attendance and the proportion of respondents always choosing the lowest cost policy alternative. They also found weak evidence that cost vector effects on WTP estimates can be different for lower-versus higher-income respondents. Besides this and studies on the factors affecting the propensity for anchoring (e.g. Furnham and Boo 2011), there are only few investigations how respondent and other characteristics impact systematic choice behaviour.====The effect of the inclusion of a cost attribute on choice behaviour has been examined similarly in health and environmental valuation with mixed findings. In the health care context, several studies have found that including the cost attribute had no significant effect on preferences for the other attributes (Bryan et al., 1998; Essers et al., 2010; Sever et al., 2019; Genie et al., 2021). However, Aravena et al. (2014) observed larger standard errors and lower model fit and Sever et al. (2019) and Genie et al. (2021) an increased response error variance (lower scale) when a cost attribute was included. Both Carlsson et al. (2007) and Van Zanten et al. (2016) found the inclusion of a cost attribute to affect the observed preferences and ranking of the other attributes, and in Pedersen et al. (2011), adding a cost attribute had a significant effect on the parameter estimates in a forced choice context.====A few additional studies have provided information on anchoring effects in DCE. Ladenburg and Olsen (2008) tested two versions of instructional choice sets, presented before the actual choice sets, which differed in the costs of alternatives. They observed differences in preferences and WTP estimates for female respondents and considered this to provide evidence of starting point bias in choice experiments. Similarly, Meyerhoff and Glenk (2015) observed instructional choice sets to affect WTP, particularly when low quality improvements are combined with high prices in the presented sets, and vice versa. In their case, these extreme combinations resulted in lower overall WTP for the attributes, which they argue could potentially be explained by reduced credibility of the entire DCE. In an experiment, Ariely et al. (2003) showed the provision of arbitrary cues of values (i.e., social security numbers) to affect willingness to pay for familiar consumer products.====Choice behaviour may be affected by many (unobservable) factors besides the cost vector and other aspects of the choice context, which could lead to differences in observed choices and WTP across sub-samples. However, as illustrated by the review of empirical evidence, cost vector effects are often observed, and thus it is important to provide more systematic evidence on when and how they may occur. Despite the central status of the cost vector in DCEs and determining welfare impacts, there is yet no consistent evidence on the effect of the cost vector on choice behaviour, resulting in relatively few practical recommendations for cost vector construction that would have wide applicability. If WTP estimates for sub-samples exposed to different cost vectors diverge and there are no other observable characteristics behind this divergence, the scale and range of the costs could be driving the differences. This can have major implications for the validity of DCE results and their use for policy support.====We contribute to the literature by providing additional insights into cost vector effects in the DCE setting using four split samples and otherwise identical surveys and experimental designs. Our data come from a DCE on agri-environmental policies and their environmental benefits, administered to a representative sample of the national population in Finland. The survey elicited preferences for four environmental attributes: traditional rural biotopes and endangered species, agricultural landscape, climate and water quality, with the payment vehicle being a tax paid by all taxpayers. Our aim was to provide welfare estimates to support discussions on future agri-environmental policies, while examining cost vector effects in DCE.====This setting allows us to test the impacts of the scale of the cost vector on choices and WTP. Furthermore, the status quo alternative is associated with a positive cost to reflect the current situation of financing the Finnish agri-environmental policy, and thus cost levels both below and above the reference level costs are included in two of the cost vectors. We contribute to the literature by providing information on cost vector effects on systematic choice behaviour and welfare estimates when the status quo alternative has a positive cost and there are cost levels lower than the status quo cost for the policy alternatives, previously lacking in the literature. We test the differences across the cost vector sub-samples in a systematic and varied way and are able to rule out observable differences in socio-demographic factors across the samples that could lead to observed differences.====The paper is organized as follows. Section 2 describes the cost vector design and hypotheses to be tested. Section 3 presents the econometric models and section 4 the design and implementation of the choice experiment. Results are provided in section 5, and discussion and conclusions in section 6.",Cost vector effects in discrete choice experiments with positive status quo cost,https://www.sciencedirect.com/science/article/pii/S1755534523000027,2 February 2023,2023,Research Article,9.0
"Follett Lendie,Naald Brian Vander","Department of Information Management and Business Analytics, Drake University, United States of America,Department of Economics and Finance, Drake University, United States of America","Received 15 April 2022, Revised 2 December 2022, Accepted 6 December 2022, Available online 4 January 2023, Version of Record 12 January 2023.",https://doi.org/10.1016/j.jocm.2022.100398,Cited by (0),"Discrete mixture (DM) models recognize the presence of heterogeneity across individuals in a given population. In the context of a public land use discrete choice experiment, we use DM models to allow for respondent behavior to probabilistically mix over multiple competing process heuristics. We pairwise combine the Random Utility Model (RUM), Contextual Concavity Model (CCM), and Random Regret Minimization (RRM) heuristic into three DM models, in which the probability of an individual adhering to a particular heuristic is modeled as a function of sociodemographic characteristics. We present a comprehensive ",", ====, ====). Recently, however, a variety of studies have explored modeling DCE data using process heuristics other than RUM (====, ====).====A complicating matter is that of imposing, ====, ====, ====, ====, ====, ====). A second category of studies deal with this issue by nesting multiple process heuristics in a single model. In joint heuristic models, respondents are allowed to weight multiple types of heuristics simultaneously to make a decision (====, ====, ====, ====). A confounding factor in identifying multiple process heuristics is accounting for preference heterogeneity, and several studies have addressed this issue. ====, ====, ==== all tackle this using joint heuristics or latent class-based models, in addition to developing a combined WTP or elasticity measures for the nested model results. ==== suggests the issue is knotty enough that DCE data might not contain enough information to identify heterogeneity in process heuristics, though ==== recently developed a set of necessary conditions to do just that in the presence of preference heterogeneity.====Within the literature examining different process heuristics, there is evidence that respondents may not treat all changes in attribute levels equally when making decisions in a DCE. Extremeness aversion (EA) is a phenomenon in which respondents are observed to be averse to choosing the more “extreme” levels of a particular attribute or set of attributes. Two different models have been employed to account for the EA effect in the data. The first, Random Regret Minimization (RRM), assumes that instead of utility maximizing, respondents seek to minimize future regret. RRM is the most prevalent in the alternative heuristic literature because it is econometrically as parsimonious as the RUM (====, ====, ====, ====). The second, called the Contextual Concavity Model (CCM), assumes respondents are utility maximizing and uses a nonlinear utility functional form to estimate an additional set of parameters that identifies the extent to which extremeness aversion occurs for each attribute in the data. Since the initial effort of ==== examining choice behavior around desktop computers, additional researchers in the marketing literature (====), and the transportation choice literature (====, ====, ====) have also estimated the CCM in an effort to identify extremeness aversion. Despite evidence uncovered in other fields, there remains a paucity of studies estimating the CCM and the RRM in the environmental and natural resource nonmarket valuation literature, with the notable exceptions of ==== and ====.====The rest of the paper proceeds as follows. In Section ==== we review three decision making heuristics, develop a discrete mixture model incorporating two heuristics simultaneously, and also describe the Bayesian methods and model selection techniques. Section ==== describes the survey and resulting data for the empirical application. Section ==== discusses the results of the analysis, and Section ==== concludes.====See ====, ====, ====.",Heterogeneity in choice experiment data: A Bayesian investigation,https://www.sciencedirect.com/science/article/pii/S1755534522000550,4 January 2023,2023,Research Article,10.0
"Danaf Mazen,Guevara C. Angelo,Ben-Akiva Moshe","Uber Freight, 433 W Van Buren St, Chicago, IL, 60607, USA,Departamento de Ingeniería Civil, Universidad de Chile, Blanco Encalada 2002, Santiago, Chile,Instituto Sistemas Complejos de Ingeniería (ISCI), República 695, Santiago, Chile,Edmund K. Turner Professor of Civil and Environmental Engineering, Massachusetts Institute of Technology, 77 Massachusetts Avenue, Cambridge, MA, 02139, USA","Received 29 January 2022, Revised 23 December 2022, Accepted 30 December 2022, Available online 31 December 2022, Version of Record 28 January 2023.",https://doi.org/10.1016/j.jocm.2022.100399,Cited by (0),"Applications of discrete choice models in personalization are becoming increasingly popular among researchers and practitioners. However, in such systems, when users are presented with successive menus (or choice situations), the alternatives and attributes in each menu depend on the choices made by the user in the previous menus. This gives rise to endogeneity which can result in inconsistent estimates. Our companion paper, Danaf et al. (2020), showed that the estimates are only consistent when the entire choice history of each user is included in estimation. However, this might not be feasible because of computational constraints or data availability. In this paper, we present a control-function (CF) correction for the cases where the choice history cannot be included in estimation. Our method uses the attributes of ==== attributes as instruments, and applies the CF correction by including interactions between the explanatory variables and the first stage residuals. Estimation can be done either sequentially or simultaneously, however, the latter is more efficient (if the model reflects the true data generating process). This method is able to recover the population means of the distributed coefficients, especially with a long choice history. The variances are underestimated, because part of the inter-consumer variability is explained by the residuals, which are included in the systematic utility. However, the population variances can be computed from the estimation results. The modified utility equations (which include the residuals) can be used in forecasting and model application, and provide superior fit and predictions.","Applications of discrete choice models in personalization are becoming increasingly popular among researchers and practitioners (====; ====; ====, ====; ====; ====; etc.). These models overcome several limitations associated with traditional recommendation and personalization methods. According to ====, discrete choice models can account for all the available data including user-specific, item-specific, and contextual information (while some traditional methods rely mainly on item and user profiling). In addition, they express utility as a function of attributes, which enables us to recommend new items that have not been chosen or rated before. Finally, while other personalization methods struggle with finding a balance between relevance and diversity, choice models integrate both (relevance and diversity) in a unified utility-based framework (====).====Many of the recent applications of choice models in recommender systems rely on the logit mixture model, primarily because it can explicitly account for unobserved inter-consumer heterogeneity. By conditioning on the observed choices for each user, we can extract user-specific parameters, which can be used in personalization (====). However, our companion paper ====Endogeneity arises in discrete choice models due to several factors including measurement errors, selection bias, omitted variables, and simultaneity, and results in inconsistent estimates of the model parameters (====). The textbook definition of endogeneity is a correlation between the independent/observed variables in the model and the unobserved additive error term. In the presence of taste heterogeneity, endogeneity can also be attributed to correlations between the explanatory variables and the heterogeneity (i.e., the individual-specific parameters) (====).====Several corrections have been proposed for endogeneity in discrete choice models, mainly falling into two categories; the BLP method (====), and the CF method (====; ====). CF methods use extra variables in the utility specification that are obtained using exogenous instruments. These instruments are variables that must exhibit two characteristics. First, they should be ====, meaning that they are correlated with the endogenous variable. Second, they should be ====, which means that they should not be correlated with the unobserved error term.====Different control-functions have been proposed, the most common of which are by ====, ====, ====, ====, ====, and ====. These methods are convenient because they are easier in implementation, and because they apply to cases where BLP cannot be used (====). Most of the proposed CF methods address the issue of correlation between the explanatory variables and the unobserved additive error term. On the other hand, very limited research has addressed the correlation between the explanatory variables and the unobserved heterogeneity, which applies to endogeneity under taste variation (e.g., choice-based recommender systems).====; ====, ====). ==== analyzed several variations of ASP designs and concluded that endogenous SP designs might result in bias in the presence of taste variation in the sample. Similarly, ==== analyzed endogeneity bias in the Leeds Adaptive Stated Preferences (LASP) survey (====; ====), and found significant bias when the models are calibrated over several respondents. ==== and ====, also developed polyhedral methods for survey designs that “reduce the feasible set of parameters as rapidly as possible” but concluded that these methods are susceptible to endogeneity bias.====SP designs that are constructed based on the revealed preferences (RP) choices can also be susceptible to endogeneity bias if estimation is done using the SP data only. ====, ==== propose a full information maximum likelihood (FIML) solution, which overcomes this problem by using a specially designed maximum simulated likelihood method. On the other hand, ==== propose a limited information maximum likelihood (LIML) approach based on a control-function that uses RP attributes as instruments. However, both cases consider correlations between the explanatory variables and the unobserved error term, and not between the explanatory variables and the individual-specific effects.====In this paper, we extend the CF method proposed by ==== and ====, and apply it to the case of choice-based personalization methods. Our CF method uses the attributes of ==== recommendations (i.e., what we would recommend or advertise to an average individual) as instruments to address cases where historical data are excluded from estimation. We apply this method to a Monte Carlo dataset mimicking a choice-based recommender system that recommends the “best” alternatives to users based on their estimated individual-specific preferences, and a real dataset that is modified to introduce endogeneity resulting from personalized advertising. Finally, we extend the analysis of ==== to illustrate how the residuals obtained from the first stage estimation can be used in forecasting and in model application.====Although this paper is primarily motivated by choice-based personalization, the proposed CF correction applies to various applications associated with panel data. Such applications are usually encountered in dynamic choice contexts, where serial correlation (e.g., preference heterogeneity) and the inter-dependence of successive choices give rise to the ====. In such cases, model estimation under the assumption that the initial conditions or the relevant pre-sample history are exogenous might result in biased and inconsistent parameter estimates. Examples are common in transportation, including cases where individuals with a higher time sensitivity tend to relocate closer to their workplace (resulting in shorter travel times), or where individuals with a higher preference towards transit purchase a transit pass (resulting in a lower cost).====The remainder of this paper is organized as follows. Section ==== presents a brief background on endogeneity in recommender systems. Section ==== presents the proposed CF methodology and its application in forecasting. This methodology is applied to simulated and real data in Section ====. Section ==== presents a discussion on estimation considerations, consistency, and simultaneous estimation. Finally, Section ==== concludes the paper by discussing the contributions, limitations, and future research directions.",A control-function correction for endogeneity in random coefficients models: The case of choice-based recommender systems,https://www.sciencedirect.com/science/article/pii/S1755534522000562,31 December 2022,2022,Research Article,11.0
Webster Scott,"W. P. Carey School of Business, Arizona State University, Tempe, AZ, 85287, USA","Received 30 July 2020, Revised 1 October 2022, Accepted 15 November 2022, Available online 26 November 2022, Version of Record 5 December 2022.",https://doi.org/10.1016/j.jocm.2022.100395,Cited by (0),"This paper presents a random utility maximization model for individuals selecting discrete quantities from a set of ==== alternatives. Multiple alternatives with positive quantities may be selected. Diminishing marginal utility to quantity of each alternative is modeled via order statistics of independent Gumbel random variables. The model is parsimonious and tractable, admitting closed-form expressions for choice probabilities. As such, the model is amenable to maximum likelihood estimation of structural parameters from observed choices.====Probability functions recover binary logit probabilities under binary choice and a maximum quantity of one unit, and probability is monotonic in the quantity of each alternative. The monotonic property likely restricts the application of the model to a narrow range of settings. The property is a manifestation of a recursive relationship among Gumbel order statistic probabilities. This relationship and related properties may lead to new models for capturing important complexities in a tractable manner.","The literature on multiple discrete choice (MDC) with continuous quantity decisions is extensive. The literature on MDC with discrete quantity decisions is not. This can be at least partially explained by tractability challenges that arise with the requirement of discrete quantities.====For many real-world applications that exhibit discrete quantity choices, a continuous decision variable can be a reasonable approximation. This is more likely to be case in applications where observed choice quantities tend to be relatively large. For other settings, such as shopping where consumers tend to purchase a few units of several products, the continuous approximation may be problematic.==== propose a tractable formulation for analyzing MDC models with discrete quantities called the threshold utility model (TUM). TUM specifies how alternatives are selected at a choice event. TUM underlies MDC models with continuous quantity decisions and includes all generalized extreme value models (====) as a special case.====In this paper, I present a novel modeling strategy (Section ====) that defines random marginal utilities as differences between Gumbel order statistics. Order statistics capture diminishing marginal utility to consumption. I present properties related to Gumbel order statistics and use these properties to derive choice probability expressions for this model within a TUM framework (Section ====). Included is a theorem showing that a property of the difference of Gumbel random variables extends to a form of conditional Gumbel order statistics. This result is one key to obtaining closed-form expressions for choice probabilities. The characterizations of Gumbel order statistics in this paper may have implications for other application areas. I briefly discuss parameter estimation and normative analysis, then conclude with a summary and reflection. Proofs are in the appendix.",Multiple discrete choice and quantity with order statistic marginal utilities,https://www.sciencedirect.com/science/article/pii/S1755534522000525,26 November 2022,2022,Research Article,12.0
"Gutiérrez-Vargas Álvaro A.,Meulders Michel,Vandebroek Martina","Faculty of Economics and Business, KU Leuven, Leuven, Belgium","Received 15 October 2021, Revised 17 September 2022, Accepted 14 November 2022, Available online 21 November 2022, Version of Record 1 December 2022.",https://doi.org/10.1016/j.jocm.2022.100393,Cited by (0),"This article investigates the usage of a general model-based recursive partitioning algorithm to model preference heterogeneity. We use the algorithm to grow a decision tree based on statistical tests of the stability of individuals’ preference parameters. In particular, we used a Mixed Logit (MIXL) model with alternative-specific attributes at the end leaves of the tree while using individual characteristics as partition variables. This configuration allows us to search for instabilities of the taste parameters across individuals’ characteristics. We conduct a simulation study to investigate the algorithm’s ability to recover different data generating processes with structural breaks in the taste parameters. The results show that the algorithm can correctly recover diverse tree-like data generating processes. Additionally, we applied the algorithm to stated choice data of the preferences for the ","Since the introduction of the Multinomial Logit model (MNL) (====), researchers have developed several model extensions to capture individuals’ heterogeneous preferences. These efforts, mainly motivated by the bias and inconsistency generated when the assumption of homogeneous preferences across individuals does not hold (====), have provided numerous extensions to the MNL model. The extensions are mainly triggered by the fact that including interaction terms between alternative-specific and individual-specific attributes is the only way to capture heterogeneity in MNL models. A popular extension is the Mixed Logit (MIXL) model (====), which captures the heterogeneity of preferences assuming a probability distribution on the model’s parameters. The MIXL has been shown to be a powerful tool with substantial gains in terms of goodness of fit (====Another major contribution to capture preference heterogeneity is the development of the Latent Class (LC) Model (====, ====). Unlike the MIXL==== model, it captures heterogeneity by assigning individuals into different classes. Each of the classes has different taste parameters, meaning that the model provides a discrete distribution of taste parameters across classes. The LC model uses an allocation model that can include characteristics of the individuals to compute the probability of belonging to a class. A common practice among practitioners is to label the resulting classes in terms of the allocation model, such as what observed characteristics would make an individual more likely to belong to a particular class. By doing so, researchers can characterize the taste parameters that an individual with given characteristics will most likely have.====Additionally, further attempts to combine the probabilistic classes created by LC models with the continuous random heterogeneity of MIXL has resulted in what we will refer to as the LC-MIXL model (====). The LC-MIXL is, at its core, a LC model that allows random parameters for each of the classes. ====, using ten different data sets, document that the LC-MIXL has superior model fit compared to LC and MIXL models. The authors attribute this to its capacity to capture a wide range of behavioral types present in the data, from lexicographic/non-compensatory choice behavior to ==== choice behavior, in the sense that the choices are little influenced by the observed attributes. Furthermore, the authors document the difficulty of using the LC-MIXL model in practice, arguing that not only the number of classes is unknown but also the distribution of the random parameters needs to be selected by the modeler, which leads to a large number of models that have to be fitted.====Following the same motivation as the above-mentioned models, this article proposes using the so-called MOdel Based Recursive Partitioning (MOB) algorithm (====) to capture individuals’ preferences heterogeneity. The MOB algorithm generates partitions in the data based on structural tests of parameter stability. The basic idea of the parameter stability tests is to check whether the score functions of the model (i.e., the first derivative of the log-likelihood function) oscillate randomly around zero or if they exhibit systematic deviations generated by some variables, which are referred to as “partition variables”. That is to say, the stability tests analyze the influence of a given partition variable over the score functions of the model. Intuitively, if the scores at different values of the partition variable do not oscillate around zero (i.e. the theoretical mean value when evaluated at the maximum likelihood), the parameters’ estimates are not stable across persons, and a data partition should be introduced. Originally, ==== presented the algorithm with least-squares, logistic, and survival regression models, and later it was extended to models that incorporate random effects, for instance, the so-called generalized linear mixed-effects model tree (GLMM tree) algorithm (====) which uses the same battery of statistical tests proposed by ====.====It is important to notice that the MOB algorithm, when used in a regression context, requires the user to specify which variables will be included in the parametric model at the end leaves and which variables will be used to partition the data. That being said, we will exploit the natural separation between alternative-specific attributes and individual-specific characteristics that arises in discrete choice applications. Concretely, we will use a MIXL model as the parametric model at the end leaves, including all the alternative-specific attributes, and we will use all the individual-specific characteristics to generate partitions on the data based on statistical tests of parameter stability. We will refer to the resulting model as the MOB-MIXL model. In this way, we expect our specification to benefit from the ability to automatically identify which individual-specific variables are relevant to create partitions on the data, which contrasts with the need to select the variables that will be included in latent class’ allocation models. In the following, we refer to the partitions created by the MOB algorithm as ==== because they are deterministic. However, given their probabilistic nature, we refer to the segments created by LC models as ====. Additionally, from all the decision trees applied in the discrete choice literature (see Section ====), this is the first to allow for random coefficients in the parametric model fitted at the end leaves.====To briefly illustrate the potential of the MOB algorithm, ==== shows a hypothetical tree that has a MIXL model with two random coefficients (log-normally distributed). The Figure represents a hypothetical situation in which the algorithm used the individuals’ age as a partition variable, dividing the sample between people older than 25 years old and younger than 25 years old. That is to say, the structural tests (see Section ====)) by estimating a separate model in each partition rather than having one global model fitted to the entire sample. Subsequently, the algorithm performed the same structural tests used in the global model (node ====) at the end leaves (nodes ==== and ====), and it failed to reject the null hypothesis of parameter instability, hence the algorithm stopped. As illustrated, the algorithm can automatically detect different groups of individuals by partitioning the data based on their characteristics; that is, the algorithm does not require a pre-defined number of classes as LC and LC-MIXL models. Finally, since the algorithm is model agnostic, simpler models can be used as well in the end leaves, for instance, an MNL or a Nested Logit model. However, we use a MIXL model because of the considerable improvement in model fit derived from such models .====This article is organized as follows. Section ==== provides a brief description of the discrete choice models we will use in this article. Section ==== introduces the MOB-MIXL algorithm. Section ==== presents two simulation studies. The first one shows that the MOB-MIXL can recover the true DGP from different tree-like with structures hard breaks. The second illustrates how a LC model will behave when in presence of data with hard breaks. Section ==== show a possible way to use the MOB algorithm as a variable selection step for latent classes’ allocation models. Section ==== illustrates the usage of the algorithm on real data. Section ==== present a discussion that compares hard breaks with fuzzy breaks highlighting their advantages and disadvantages and presents the conclusions of the article.====For the sake of illustration of the ==== and its relationship with the score functions, suppose we estimate a hypothetical model with a single attribute (====) in the utility specification. Additionally, let us assume that his parameter is the ==== of a given route and that we estimate the parameter without using random coefficients. That is to say, only one parameter is estimated. Besides, assume we are interested in analyzing whether the estimated parameter is stable across the age of the individuals in our sample. In ==== we plotted the score function (Eq. ====) sorted by individuals’ age (====), and the ==== (Eq. ====) of the variable age (====). The figure shows that the score function does not fluctuate around zero, which serves as a diagnosis of the instability of the estimated parameter. In particular, we can see that it systematically fluctuates below zero for individuals younger than 40 years and above zero for individuals older than 40 years. This fact implies that a model ignoring this parameter instability in our hypothetical situation would overestimate the cost parameter of younger individuals and underestimate it for older ones. Therefore, using this fact, we could get better local models (e.g., with score functions fluctuating randomly around zero) if we fit two separate models; one for people younger than 40 years and one for people older than 40 years.",Modeling preference heterogeneity using model-based decision trees,https://www.sciencedirect.com/science/article/pii/S1755534522000501,21 November 2022,2022,Research Article,13.0
"Abe Makoto,Kaneko Mitsuru","The University of Tokyo, Graduate School of Economics, 7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-0033, Japan,Waseda University, Research Center of Consumer Behavior, 1-104 Totsukamachi, Shinjuku-ku, Tokyo, 169-8050, Japan","Received 17 December 2020, Revised 4 June 2022, Accepted 5 September 2022, Available online 13 September 2022, Version of Record 22 September 2022.",https://doi.org/10.1016/j.jocm.2022.100384,Cited by (0),"According to the behavioral decision theory, time discounting is often used to explain preference reversals. However, the discounting theory fails to explain some types of preference reversals. Furthermore, preference reversals are limited to those along the time axis (i.e., temporal distance). To extend our knowledge of preference reversals in various choice contexts, this study constructs an analytical framework that combines the time discounting notion of behavioral decision theory and construal level theory developed in social psychology. We put forward three propositions for discounting: magnitude effect (the higher the construal level, the smaller the discounting rate), sign effect (the discounting rate is smaller for losses than for gains), and generalization of distance (discounting applies not only to temporal distance but also to psychological distances such as social distance). These propositions were validated in two studies. In Study 1, we conducted a series of three experiments on a lottery choice task using two samples of respondents (i.e., students and a web panel). In Study 2, we estimated the discounting rates of the higher and lower construal levels by employing multiple intertemporal choice tasks. While many choices involve trade-offs among attributes, the effects of changes in psychological distances are not clear. However, by identifying whether these attributes evoke high or low construal levels and whether the aspects are related to gains or losses, our approach greatly facilitates the analysis of how evaluation and preference are affected by psychological distance, and consequently, that of preference reversal behavior.","Here, we describe three familiar examples of preference reversal. The first is procrastination. Almost everyone has procrastinated—for example, in the context of dieting, quitting smoking, or doing homework. Despite the promise of a desirable substantial gain in the distant future, people often tend to prefer earning comparatively minor gains in the immediate future. Such acts of procrastination constitute examples of preference reversal along the time dimension. The second one is “pre-wedding blues,” which is another example of a preference reversal along the time dimension. In the planning stage, a wedding seems to be a tremendously desirable event. However, as the wedding date approaches, there might be some doubts about getting married. The third example involves a preference reversal along social distance. The choice of a product may differ depending on whether it is for a work supervisor, a family member, or oneself (Choi et al., 2018; Hamilton and Thompson, 2007).====In behavioral economics, procrastination is often explained using hyperbolic discounting instead of exponential discounting. However, as will be shown later, this approach fails to explain phenomena, such as pre-wedding blues. Similarly, for preference reversals along dimensions other than time, such as the aforementioned gift example, discounting offers minor insights.====Thus, the objective of this study is to propose a conceptual model for understanding preference reversals over various psychological distances, including time. In particular, it constructs an Analytical Framework for Construal Level Theory (AFCLT) by introducing the concept of discounting, which is applied to general psychological distances (including time). Additionally, we posit two well-known properties of a discounting rate: the magnitude and sign effects.====When making a choice, people distinguish between attributes that they prioritize and those that should be relinquished. Such a process involves the complex task of making a trade-off between the gains and losses brought about by various attributes of varying construal levels. By modeling the input and output of CLT and without explicating the inner mechanism, AFCLT can predict preference shifts and behavioral changes as distance varies with simplicity.====The contributions of this study are two-fold. First, it constructs a common framework for analyzing preference reversals along general psychological distances, such as temporal and social distances. Second, the use of AFCLT can facilitate the analysis of how evaluation, preference, and judgment are affected by psychological distances, thereby providing useful insights for business and policy-making in many fields (e.g., marketing, transportation). For example, in the italicized opening scenario, in which the answer may not be obvious, AFCLT allows us to predict the behavioral outcome with ease. This particular example is explained in Experiment 1c of this study.====The remainder of this paper is organized as follows. Section 2 provides a conceptual background of time discounting and CLT, and highlights their shortcomings with respect to explaining preference reversals. In Section 3, we develop and construct our framework with three propositions. Additionally, we discuss its relationship with CLT. In Sections 4 Experiment 1, 5 Experiment 2: Direct estimation of discounting rates for high and low construals, we conduct statistical tests of these propositions in four studies, using survey data. Finally, Section 6 summarizes the findings and conclusions of the study, discusses the implications, and addresses its limitations.",Preference reversal: Analysis using construal level theory that incorporates discounting,https://www.sciencedirect.com/science/article/pii/S1755534522000410,13 September 2022,2022,Research Article,16.0
"Jiang Qi,Penn Jerrod,Hu Wuyang","Dept. of Agricultural, Environmental, and Development Economics, The Ohio State University, 2120 Fyffe Road, Columbus, OH, 43210, USA,Dept. of Agricultural Economics & Agribusiness, Louisiana State University & LSU AgCenter, 221 Woodin Hall, Baton Rouge, LA, 70803, USA","Received 17 September 2021, Revised 26 May 2022, Accepted 28 August 2022, Available online 10 September 2022, Version of Record 26 September 2022.",https://doi.org/10.1016/j.jocm.2022.100383,Cited by (0)," for a good or service in a hypothetical elicitation. A relatively new method shown to effectively reduce this upward bias is priming. However, these existing priming methods rely on relatively lengthy word or sentence tasks in order to prime respondents. Such tasks are costly in terms of survey time and participant effort, resulting in cognitive overload with benefits limited only to the elicitation. We propose a “real payment priming” method, which takes advantage of a real valuation, where actual payment would occur, prior to a hypothetical valuation. Results show that priming through real payment on one good effectively reduces potential HB in the subsequent hypothetical valuation on another good. Our method enables a wider scope of applications particularly when researchers have multiple valuation tasks, obviating the need for an extra priming task, or that the two goods are identical or similar.","Stated Preference (SP) methods are widely used to measure consumer preference and welfare for both market and non-market goods, most commonly in the form of Willingness to Pay (WTP). However, Hypothetical Bias (HB), the deviation between responses in hypothetical valuation/payment questions and real revealed behaviors with binding payment, is a long-recognized challenge facing SP practitioners (Mitchell and Carson, 1989; List and Gallet, 2001). In response, multiple mitigation techniques such as Cheap Talk (CT) and certainty follow-up aim to reduce the gap between hypothetical values and real values (Loomis, 2011, 2014). While these early HB mitigation methods continue to occur, their inconsistency has prompted further innovation to reduce HB, with methods such as oaths (Stevens et al., 2013; Kemper et al., 2020), real talk (Alfnes et al., 2010), virtual reality (Fang et al., 2021), pivot design (Hensher, 2010), RP-SP estimations (Whitehead et al., 2008), as well as various forms of priming (De-Magistris et al., 2013; Stachtiaris et al., 2011). This study adds to the literature by expanding priming to be potentially useful in a broader number of applications.====The concept of priming originates from social psychology, described as an initial exposure to some external stimulus that may subliminally impact individuals’ subsequent behavior (Bargh et al., 1996; Chartrand et al., 2008; Drouvelis et al., 2015; DeCoster and Claypool, 2004; Schwarz and Bless, 1992). Previous literature has demonstrated that priming respondents to concepts of truthfulness may help reduce HB. For instance, priming focused on honesty or religion has shown some success (De-Magistris et al., 2013; Stachtiaris et al., 2011). However, these priming methods have two main limitations. First, the existing priming tasks are limited to sentence/word tasks. These tasks may create extra cognitive burden with no other use beyond the purpose of priming, or even raise suspicion from respondents since these tasks are often irrelevant to the stated purpose of the survey during respondent recruiting. Second, previous literature shows mixed efficacy of priming at reducing HB, potentially because priming tasks might not be salient, or not receiving due effort from the respondents (De-Magistris et al., 2013; Howard et al., 2017). This paper aims to further explore priming to reduce the noted limitations by taking advantage of another valuation study. We propose a method referred to as “real payment priming” based on the theory of Dissonance Minimization (DM).====According to DM, cognitive dissonance is an unpleasant state which occurs when individuals hold two inconsistent beliefs in two comparable decisions. This stage of discomfort could motivate the individuals to behave consistently to reduce the cognitive discrepancy (Festinger, 1962; Harmon-Jones and Harmon-Jones, 2002). In our study, we harness DM by taking advantage of two valuation scenarios of WTP in one survey, with similar elicitation process instead of similar goods to be valued. In the first valuation question, one group of respondents receives a hypothetical question while the other group makes a real choice with binding payment such that in this context, respondents can be thought to reveal their actual WTP (Carson and Groves, 2007; Mitani and Flores, 2014). Both groups then answer the second valuation question, which is hypothetical and may be subject to HB. Based on the theory of DM, we hypothesize that, since both questions respondents face are elicitations of WTP presented in comparable wording, those who have experienced a non-hypothetical valuation question first may be more likely to reveal their true preferences in the second and hypothetical valuation question just in order to be consistent with their preference-revealing decision process in the first question. This process may reduce HB in the second valuation question.====As such, our priming method takes advantage of another elicitation question occurring within the same survey. This combination of different valuation questions in one survey regularly occurs in previous works and is often necessary following the research objectives or requirements from the funder (Petrolia, 2016; Alfnes et al., 2010; Johannesson et al., 1999; Fox et al., 1998). Another area of application substantiating such combination is when analyzing goods/services with uncertain outcomes, which inherently requires multiple elicitations (e.g., Holt and Laury, 2005; Lusk and Keith, 2005). In such applications, it is common to combine the primary valuation question with a series of non-hypothetical choice questions. The premise is that such real choice questions could elicit measures of risk preference to explain respondents’ behavior in the primary valuation question. Following our notion of real payment priming, such a method can also potentially reduce HB for a succeeding valuation question. Therefore, our priming task, which could be a key component for a study or a standalone valuation question, has its own research purpose beyond the pure cognitive burden and limited usefulness of priming tasks in previous literature.====Our results show that real payment priming works even for two elicitations of two dissimilar goods. We demonstrate that respondent experience in the elicitation process can help reduce HB, without relying on their experience in the products evaluated. This makes our approach more attractive compared to the limited usefulness or impracticality of requiring a pair of similar products. For instance, the real talk method proposed by Alfnes et al. (2010) used two valuations in one study to successfully reduce WTP via DM by disclosing to the respondent that they would participate in a real valuation scenario after a hypothetical elicitation of the same good. This restriction requires real provision for the item, making hypothetical elicitation superfluous. As a result, when researchers have multiple ongoing valuation projects and at least one project can be conducted as a real transaction, combining the two valuations in one survey provides synergy, with one being real to reduce HB for the second and hypothetical question, regardless of their similarity. For comparison, in addition to real payment priming in the first valuation question, we also elicit stated preference in the first valuation question but implement CT, consequentiality, and certainty follow-up to reduce HB. The results show that real payment priming has advantages over the other HB mitigation methods applied to the first valuation question in reducing HB in the second valuation question.",Real payment priming to reduce potential hypothetical bias,https://www.sciencedirect.com/science/article/pii/S1755534522000409,10 September 2022,2022,Research Article,17.0
"Gu Yu,Chen Anthony,Kitthamkesorn Songyot","Department of Civil and Environmental Engineering, The Hong Kong Polytechnic University, Kowloon, Hong Kong,The Hong Kong Polytechnic University Shenzhen Research Institute, Shenzhen, Guangdong, PR China,Excellence Center in Infrastructure Technology and Transportation Engineering, Department of Civil Engineering, School of Engineering Chiang Mai University, 50200, Thailand","Received 4 April 2022, Revised 20 June 2022, Accepted 20 July 2022, Available online 5 August 2022, Version of Record 11 August 2022.",https://doi.org/10.1016/j.jocm.2022.100373,Cited by (2),"This study presents the properties of the weibit travel choice model (which was newly developed in 2008) and its applications in transportation research. The weibit model uses a Weibull-distributed random error term, relaxing the identically distributed assumption to derive the closed-form choice probability expression. The properties and advantages of the weibit model are graphically illustrated by comparisons with the commonly used logit model. The relationship between the weibit and logit models are discussed in the transportation context. The applications of weibit models to transportation research are exemplified based on the mode choice modeling in various transportation networks incorporating emerging technologies. The study reveals that weibit models have attractive features for evaluating and planning future transportation systems.",None,"Weibit choice models: Properties, mode choice application and graphical illustrations",https://www.sciencedirect.com/science/article/pii/S1755534522000306,5 August 2022,2022,Research Article,18.0
"Haghani Milad,de Bekker-Grob Esther W.","Research Centre for Integrated Transport Innovation (rCITI), School of Civil and Environmental Engineering, The University of New South Wales, UNSW Sydney, Australia,Institute of Transport and Logistics Studies (ITLS), The University of Sydney Business School, Sydney, Australia,Erasmus School of Health Policy & Management, Erasmus University Rotterdam, the Netherlands","Received 19 March 2022, Revised 31 May 2022, Accepted 18 July 2022, Available online 21 July 2022, Version of Record 31 July 2022.",https://doi.org/10.1016/j.jocm.2022.100371,Cited by (0),"Published choice experiments linked to various aspects of the COVID-19 pandemic are analysed in a rapid review. The aim is to (i) document the diversity of topics as well as their temporal and geographical patterns of emergence, (ii) compare various elements of design quality across different sectors of applied economics, and (iii) identify potential signs of convergent validity across findings of comparable experiments. Of the N = 43 published choice experiments during the first two years of the pandemic, the majority identifies with health applications (n = 30), followed by transport-related applications (n = 10). Nearly 100,000 people across the world responded to pandemic-related discrete choice surveys. Within health applications, while the dominant theme, up until June 2020, was lockdown relaxation and tracing measures, the focus shifted abruptly to vaccine preference since then. Geographical origins of the health surveys were not diverse. Nearly 50% of all health surveys were conducted in only three countries, namely US, China and The Netherlands. Health applications exhibited stronger pre-testing and larger sample sizes compared to transport applications. Limited signs of convergent validity were identifiable. Within some applications, issues of temporal instability as well as hypothetical bias attributable to social desirability, protest response or policy consequentiality seemed likely to have affected the findings. Nevertheless, very few of the experiments implemented measures of hypothetical bias mitigation and those were limited to ====. Our main conclusion is that swift administration of pandemic-related choice experiments has overall resulted in certain degrees of compromise in study quality, but this has been more so the case in relation to transport topics than health topics.","COVID-19 has had a profound impact on the lives of everyone around the world since early 2020. Due to the rarity of global pandemics, researchers have been keen to analyse the behavioural impact of COVID-19 using choice experiments and choice modelling in various domains. COVID-19 has significant consequences for public health and has strained hospitals and people working in health care. To slow infection rates, many governments have imposed restrictions and lockdowns to reduce mobility. It is, therefore, no surprise that choice modellers have mostly analysed health-related and travel-related choice behaviour. What is surprising is the speed with which choice experiments were designed and implemented, with the first surveys containing choice experiments sent out mere weeks after the start of the pandemic.====In this review, we summarise choice experiments related to COVID-19 that have appeared in the literature with the aim to document the diversity of topics and their emergence, to compare experimental design quality across different applied economics areas, and to identify whether similar choice experiments exhibit convergent validity.",Applications of discrete choice experiments in COVID-19 research: Disparity in survey qualities between health and transport fields,https://www.sciencedirect.com/science/article/pii/S1755534522000288,21 July 2022,2022,Research Article,19.0
"Vass Caroline M.,Boeri Marco,Poulos Christine,Turner Alex J.","RTI Health Solutions, The Pavilion, Towers Business Park, Manchester, M20 2LS, UK,The University of Manchester, Oxford Road, Manchester, M13 9PL, UK,RTI Health Solutions, 23B Forsyth House, Cromac Square, Belfast, BT2 8LA, UK,Queen's University Belfast, University Road, Belfast, BT7 1NN, UK,RTI Health Solutions, 3040 E Cornwallis Rd, Research Triangle, NC, 27709, USA,PHMR Ltd, London, NW1 8XY, UK","Received 30 September 2021, Revised 28 February 2022, Accepted 21 June 2022, Available online 3 July 2022, Version of Record 7 July 2022.",https://doi.org/10.1016/j.jocm.2022.100367,Cited by (0), could be more appropriate than analyses using unmatched preference data when heterogeneity is driven by multiple factors. The paper concludes with a discussion of when matching and weighting may and may not be useful in healthcare decision-making.,"Discrete choice experiments (DCEs) are an increasingly popular method of eliciting stated preferences for health (Soekhai et al., 2019). In a DCE, respondents are asked to select their preferred alternative from a set in a series of hypothetical choices usually presented in a survey. The choices require a tradeoff between different attributes of the good or service. In healthcare, where market data rarely exists, quantifying preferences through DCEs allows decision-makers to understand the value and relative importance of different aspects of the good, service or health state. In the last decade, systematic reviews have shown an increase in both the number of applications (Soekhai et al., 2019) and the number of studies investigating heterogeneity in preferences across individuals with different characteristics (Heidenreich et al., 2021).====There has also been a growing interest in using the tradeoff data to understand the acceptability of various preference sensitive decisions at a policy-level (Vass and Payne, 2017). As both regulatory and health technology assessment often involves an evaluation of whether the benefits of an intervention outweigh its risks or costs, respectively, these data can be a useful tool for those making decisions (Levitan et al., 2017). These regulators and health technology assessment (HTA) bodies may be interested in understanding if and how patients’ values (or willingness to accept treatment) varies based on their personal or disease characteristics (FDA, 2016; Ho et al., 2015; Vass and Payne, 2017). To analyze preference heterogeneity by values of an individual characteristic (such as gender, age, and time since diagnosis), common approaches involve either including interactions between attributes and the characteristic of interest or subgroup analysis through sample-stratification (Heidenreich et al., 2021). Similar approaches are also often used to assess whether aspects of the overall study design, such as the how attributes are framed, recruitment approach, or the device used to complete the DCE, may impact preferences.====However, in many settings differences in preferences across individual characteristics are only informative if these differences have a causal interpretation. If subgroups differ in characteristics other than the characteristic(s) of interest, differences in preferences across subgroups may simply capture differences in the effects of these other characteristics on preferences. For instance, differences between a sample of older and younger patients may be driven by the difference in age, but also by differences in other factors, such as income, employment status, gender ====.====Typically in DCEs, and in stated preference methods in general, researchers do not need to worry as much as with other methods about making causal statements from the data generated in their studies. Indeed, stated preference studies are purposefully designed to capture the causal effect of changing a product characteristic (i.e., an attribute) on individuals’ choices with a set of constraints. Perhaps for this reason, as it has been noted even within this journal, there is a paucity of methods from the causal inference literature within choice modelling (Brathwaite and Walker, 2018).====In this paper, we consider how the pre-processing of preference data can help better examine how preferences differ across individuals with different characteristics. Section 2 provides a brief overview of selected causal inference methods that could be easily implemented alongside popular discrete choice models. We focus on the use of matching and weighting methods to ensure balance on individual characteristics across subgroups of interest. In Section 3, we use two examples (with simulated and empirical data) to illustrate the challenges associated with unweighted subgroup analyses and the erroneous conclusions that may be drawn. In Section 4, we discuss why there may be limited applications of matching and weighting techniques in the preference literature. In this final section, we emphasize why matching and weighting techniques may or may not be useful with examples of research questions potentially of interest to decision makers.",Matching and weighting in stated preferences for health care,https://www.sciencedirect.com/science/article/pii/S1755534522000252,3 July 2022,2022,Research Article,20.0
"Contu Davide,Strazzera Elisabetta","Faculty of Management, Canadian University Dubai, United Arab Emirates,Department of Political and Social Sciences, University of Cagliari, Italy","Received 4 January 2021, Revised 2 June 2022, Accepted 22 June 2022, Available online 24 June 2022, Version of Record 30 June 2022.",https://doi.org/10.1016/j.jocm.2022.100370,Cited by (0),"This work proposes a discrete choice model that jointly accounts for heterogeneity in preferences and in ==== adopted by respondents, as well as for non-linearities in the utility function, allowing for the potential effect of salient attributes in choice experiments. We present an innovative application in the context of preferences towards nuclear energy, with data obtained from a nationwide online survey conducted in Italy. Results show that most of the variation in the choice data is indeed due to heterogeneity in the decision process, where the saliency heuristic plays an important role. Furthermore, the proposed model provides more conservative monetary valuations as opposed to standard models, potentially leading to substantial differences in cost-benefit analysis. Implications for choice modeling practitioners are discussed, emphasizing the need to account for saliency effects when modeling the choice data.","Stated preferences surveys based on choice experiments are extensively employed in transport, health, and environmental economics to value attributes of non-market goods. Research towards improving validity and reliability of results has received widespread attention. Particular focus has been directed at modeling different attribute processing strategies, allowing practitioners to venture beyond the assumption that individuals conform to a fully compensatory decision process all the time, thereby reducing the risk of biased policy implications caused by misspecification of the decision processing strategies (Balbontin et al., 2019; Campbell et al., 2014, 2015; Hensher et al., 2005; Hess et al., 2010, 2012, 2013; Hole 2011).====In this work, we focus on the analysis of choice experiment data relative to the potential implementation of nuclear energy in Italy. Whilst it may contribute to curb climate change, this energy source is generally identified as intrinsically divisive and problematic (Lock et al., 2014). In Italy, public opinion towards nuclear energy has been particularly negative overall (Bersano et al., 2020). In this country there are currently no nuclear plants in operation, but there were until the Chernobyl's accident. Italians rejected further nuclear energy projects via referendum twice, first in 1987 and then in 2011, following the Fukushima's accident. Remarkably, Italy still lacks a complete nuclear waste management program: location of nuclear waste management sites is an unsettled issue between national and local authorities, regardless of economic benefits being offered (La Repubblica 2021). In such a context, it appears important to consider potential decision processing strategies at play when respondents make their choices regarding new nuclear projects. The proposed options may trigger emotions which in turn could affect choice behavior (Araña and León 2008; Araña et al., 2008). People may display non-compensatory choice patterns which may be explained by strategic behavior. If some individuals are radically against nuclear energy, they may completely disregard the attributes presented in the choice experiment. As discussed by Hess et al. (2010) in a different context, “====” (Hess et al., 2010, p.406). So, it may happen that some respondents strongly opposed to nuclear plants may just refuse to move from a ‘No Nuclear option’ (if available). In other cases, apparent non-trading behavior may be caused by different levels of importance attached to specific attributes by some respondents. For example, respondents may always choose the alternative with the better level of one attribute: this behavior is referred to as “attribute dominance” (Johnson et al., 2019). Attribute dominance can be explained by respondents valuing an attribute very highly, with the range of variation in other attributes being too narrow to offer sufficient compensation for moving away from the better level of the preferred attribute (Johnson et al., 2019). Thus, some respondents may accept a trading, but only if some conditions regarding their preferred attributes are satisfied. For example, respondents may not in principle be opposed to nuclear energy, provided that the facility is not developed in their local area: this is often referred to as NIMBY effect, associated with free riding (Carley et al., 2020). Ample literature has shown that acceptance of hazardous facilities is actually influenced by many factors, such as trust in institutions (Bronfman and Vázquez, 2011; Siegrist et al., 2005; Poortinga and Pidgeon 2003), place attachment (Devine-Wright 2011; Van Veelen and Haggett 2017), benefit and risk perception (De Groot et al., 2020; Strazzera et al., 2022), which cannot be simply interpreted as free-riding; yet, as suggested by Uji et al. (2011), these perceptions may trigger a NIMBY attitude, so that in the choice exercises respondents may overly focus on the distance attribute and choose the project option only if this attribute attains a certain level. It may as well happen that other attributes are considered irrelevant, and hence completely ignored in the decision process.====These choice behaviors could be problematic if modelled with standard choice models, e.g. a random utility maximization (RUM) model (Manski 1977), under the assumption that all individuals follow the same utility maximization process. As advocated by Johnston et al. (2017), data analysts should evaluate ==== (Johnston et al., 2017, p.362). Allowing for deviations from rational consumer theory, it can be hypothesized that respondents apply a variety of heuristics (Kahneman 2003), which may have a systematic impact on the estimates resulting from the choice model. Decision heuristics recognize that when humans make decisions, some information is ignored in order to make decisions “====” (Gigerenzer and Gaismaier 2011, p.454). Drawing on research on lexicographic preferences and non-trading (Hess et al., 2010), attribute non-attendance (Scarpa et al., 2010), attribute importance (Balcombe et al., 2014), we allow for decision process heterogeneity employing a finite mixture model. A Latent Class model is fitted to the data, estimating the probability that a respondent uses a specific decision process characterized by attribute saliency, possible attribute non-attendance, and alternative-specific effects, allowing for possible combinations of decision making procedures. Moreover, since individuals who differ in their decision strategies may also have different preferences for the characteristics of the alternatives (Hess et al., 2013), we employ a Latent Class Random Parameters Model (Greene and Hensher 2003), specifying randomly distributed preference parameters over the latent classes. Such a modeling strategy should allow to control for the risk of confounding heterogeneity of taste and decision making (Hess et al., 2012, 2013). Our paper contributes to the literature in showing that accounting for process heterogeneity, jointly with taste heterogeneity, improves goodness of fit; furthermore, in our application, the resulting estimates are more efficient for most parameters of interest, and the point estimates of the compensatory monetary measures are more conservative than those obtained when multiple heuristics are not taken into account. Choice experiment data was collected by means of an online survey aimed at estimating willingness to accept (WTA) measures for the hypothetical building of new nuclear plants in Italy. We opted to run a choice experiment in this context to value changes which are multidimensional, and to infer monetary valuations implicitly (Hanley et al., 2001; Bateman et al., 2002).====The rest of the paper presents the literature review in Section 2, the econometrics of the model in Section 3; Section 4 presents the specific case study; Section 5 describes the results and Section 6 concludes.",Testing for saliency-led choice behavior in discrete choice modeling: An application in the context of preferences towards nuclear energy in Italy,https://www.sciencedirect.com/science/article/pii/S1755534522000276,24 June 2022,2022,Research Article,21.0
"Jourdain Damien,Lairez Juliette,Striffler Bruno,Lundhede Thomas","CIRAD, UMR G-EAU, Hatfield, 0028, South Africa,G-EAU, Univ Montpellier, AgroParisTech, CIRAD, IRD, IRSTEA, Montpellier SupAgro, Montpellier, France,Centre of Environmental Economics and Policy for Africa (CEEPA), Department of Agricultural Economics, Extension and Rural Development, Faculty of Natural and Agricultural Sciences, University of Pretoria, Hatfield, 0028, South Africa,AIDA, Univ Montpellier, CIRAD, Montpellier, France,CIRAD, UPR AIDA, F-34398, Montpellier, France,Agro-solutions, F-75016 Paris, France,Department of Food and Resource Economics (IFRO), Copenhagen University, Copenhagen, Denmark,Centre for Macroecology, Evolution and Climate, University of Copenhagen, Denmark","Received 28 September 2021, Revised 9 April 2022, Accepted 15 June 2022, Available online 18 June 2022, Version of Record 22 June 2022.",https://doi.org/10.1016/j.jocm.2022.100366,Cited by (0),"Sustainable intensification seeks to increase outputs from existing farmland in ways that have a lower ====. An extensive literature has examined the determinants of farmers' adoption of the different agro-ecological cropping systems needed to achieve these goals. However, the farmers' preferences for the attributes of these systems and the decision processes for choosing between available systems is still poorly understood. To fill this gap, this paper proposes a methodology that relies on a discrete choice experiment to analyse farmers’ preferences for cropping systems and estimate the heterogeneity of decision processes among farmers. We modelled three major types of decision processes potentially used by farmers to evaluate the systems that are not consistent with the standard utility maximization framework. These findings offer insights into the behavioural patterns of respondents and should help crop system promoters and developers to better understand how their proposed systems are likely to be evaluated by different types of farmers.","The goal of agricultural sustainable intensification is to increase food production from existing farmland in ways that safeguard its productive capacity and have a lower environmental impact (Garnett et al., 2013). Ideally, sustainable intensification should also increase contributions to natural capital and the flow of environmental services (Pretty, 2014). Many agro-ecological cropping systems==== promoted in recent decades, such as intercropping, improved fallow, crop rotation, conservation tillage, could play a significant role in achieving these goals. Despite efforts to promote the apparent benefits for farmers, the adoption of these agro-ecological systems remains low in the tropical world, especially in South-East Asia (Kassam et al., 2015). Although several studies have looked into the challenges to broad adoption (e.g., Knowler and Bradshaw, 2007), the reasons for the low adoption rates are still not fully understood.====This paper tests farmers’ preferences for different aspects/attributes of cropping systems. We develop a discrete choice experiment (DCE) where farmers are asked to compare and choose between cropping systems that have contrasted impacts on the farm resources and organization. The use of DCE is becoming increasingly popular to evaluate the potential choices to be made by farmers (e.g., Blazy et al., 2011; Useche et al., 2013; Ortega et al., 2016). To add to that literature, we explore the possibility that the decision-making process used in making a choice may in fact vary across farmers. This issue has received some attention in marketing and health economics (Gilbride and Allenby, 2006; Erdem et al., 2014). However, while the technology adoption literature have already used DCE to analyse preferences over alternative agricultural technologies or alternative cropping systems, to the best of our knowledge, most studies ignored the possibilities that farmers may not conform to the standard utility hypotheses used when responding to DCEs. Among the exceptions, Rodríguez-Entrena et al. (2019) who studied the presence of Attribute Non Attendance (ANA) for farmers choosing among agri-environmental schemes, and Owusu et al. (2021) who studied ANA for farmers choosing insurance contracts. However, to the best of our knowledge, this is the first study to assess the possibility of other heuristics such as Elimination by Aspects (Tversky, 1972) in agriculture.",A choice experiment approach to evaluate maize farmers’ decision-making processes in Lao PDR,https://www.sciencedirect.com/science/article/pii/S1755534522000240,18 June 2022,2022,Research Article,22.0
"Pilli Luis,Mazzon José Afonso","School of Economics, Management, Accounting and Actuarial Sciences - FEA - USP, Av. Prof. Luciano Gualberto, 908, Butantã, São Paulo - SP, 05508-010, Brazil,Erasmus School of Health Policy & Management and Erasmus Choice Modelling Centre (Erasmus University Rotterdam), Burgemeester Oudlaan 50, 3062, PA, Rotterdam, Netherlands","Received 31 May 2021, Revised 6 May 2022, Accepted 12 May 2022, Available online 23 May 2022, Version of Record 25 May 2022.",https://doi.org/10.1016/j.jocm.2022.100359,Cited by (0),"Brands develop strategies based on forecasts that allow for individual differences, usually attributed empirically to heterogeneity in consumers' preferences. ","In the paradigmatic representation of competitive markets, product differentiation is a source of competitive advantage arising either from consumers' variety seeking across consumption occasions or from differences in tastes among consumers. In response to consumers' taste heterogeneity, brands choose the most profitable product assortment to offer and the market equilibrium reveals the product variety resulting from agents' behaviors in a given competitive structure (Lancaster, 1990). The identification of consumer response heterogeneity is the foundation for segmentation (where to compete) and positioning (how to compete). This implies that strategic decisions at the marketing level are outcomes of brand managers' understanding of consumers’ preferences that would shape the market structure and competition patterns (Kamakura et al., 1996).====The complex decision environments encapsulated by modern markets are mentally costly for consumers: despite the enormous number of alternatives offered to consumers, they tend to make their choice among a small subset of the available options (i.e., consumers seem to “screen” out large numbers of alternatives from consideration during choice). From 2008 to 2013 the number of TV channels received by the average American household escalated from 129 to 189, but the number of tuned channels stayed constant at 17 (Nielsen, 2014). Unobserved heterogeneity in subjective choice sets is a well-known phenomenon in consumer decision-making, with a major impact on the choice among alternatives (Hauser, 1978; Manski, 1977; Swait and Ben-Akiva, 1987), which arises as an adaptive response to the complexity of the decision-making environment when hierarchically superior psychological processes lead to the selection or elimination of alternatives in the available product assortment (Weber and Johnson, 2009). Hence, it is our argument that other such processes (what we collectively term ====) are at work in consumers’ choice processes conditioning the possible existence and impacts of taste heterogeneity.====This paper therefore studies the correctness of a brand's decision making (hence performance, and ultimately, profitability) in competitive markets in which consumer process heterogeneity occurs, but is empirically misattributed to taste heterogeneity. Our motivation behind targeting this particular directionality of misattribution (consumer process heterogeneity misattributed as taste heterogeneity) arises from (1) the central role that taste heterogeneity has in modern economics and marketing thought, (2) the statistical and computational advances that have greatly facilitated the estimation of demand representations that incorporate sophisticated and complex depictions of process heterogeneity (to illustrate, among others, see Aribarg et al., 2017; Dellaert et al., 2017; Hensher 2014), and (3) the pronounced tendency, among applied choice modelers and market analysts, to represent demand differences based solely on taste heterogeneity. Although the effects of econometric models' misspecification on measures describing demand are self-evident and well known, the magnitude of the resulting biases in model parameters (and derived quantities), and its consequences on brands' profitability have not been properly described and gauged. This paper addresses this gap in the literature from the viewpoint that ignoring the magnitude of such biases and the consequences on brands profitability leads to a lack of incentive to change modeling strategies. By deploying a Monte Carlo simulation (see Web Appendix A), we account simultaneously for variations in a focal brand's equity (Davcik et al., 2015; Erdem and Swait, 1998) and the extent of consumer choice process heterogeneity from a position of full knowledge; such would not be the case if we were to use market demand data from the real world. And given that in competitive markets there are multiple agents making decisions simultaneously, we further depart from extant literature by examining the possible effects of econometric modeling strategies on competitive reaction and market equilibrium. To make this latter extension, we connect the results from the Monte Carlo experiment (demand side) into a game-theoretical analysis (supply side).====In summary, we view markets as characterized by consumers aiming to balance utility maximizing choice while minimizing risks and cognitive effort, while on the supply side brands are searching for marketing strategies that lead to profit maximization. In this decision-making environment, our basic proposition is that when consumers choices are erroneously represented by misattributing choice process heterogeneity solely to preference/taste heterogeneity, it is likely that managers will make decisions leading brands away from profit maximization. We assert this based on the following logic: 1) a misrepresentation of demand will be expressed via the biased location of demand parameters, and through the potential (mis)identification of preference heterogeneity in the demand that may not actually be present in the marketplace, or at least not to the extent inferred by a mis-specified model; 2) biased demand measurements will lead to the adoption of incorrect policy measures due to mispredictions of brands’ market shares and choice elasticities with respect to attributes; 3) consequently, brands will choose wrong quantities to offer in the marketplace from the biased choice probabilities and will operate with a suboptimal marginal cost (and profit) caused by the biased reading of demand price elasticities. Whilst it is likely that real markets are characterized by the co-existence of both choice process and preference heterogeneities, the empirical proclivity of market analysts to assume that only the latter is at work suggests to us that understanding the directional impacts of omitting the former will be most helpful in characterizing how the omission of process heterogeneity will lead to compromised performance on the part of the brand.====To our knowledge, this is the first research in the marketing, management and choice modeling literatures that attempts to investigate the impacts of misdiagnosing choice process heterogeneity as preference heterogeneity on brands' profitability and market equilibrium. Choice process heterogeneity has been a concern among some choice modelers who have been formalizing alternatives to the classical random utility models. However, the consequences of the proposed misspecification on brands’ profitability is still a gap in the literature, perhaps justifying in the minds of many analysts the rare adoption of models that are harder to fit and to understand (as argued by Aribarg et al., 2017). To reinforce the focus of this paper on consequences of overlooking choice process heterogeneity and the importance of making room for it in modeling strategies, we (1) select and emphasize the focal bias directionality (misattribution of process heterogeneity as taste heterogeneity), and (2) extend the study of the consequences of model misspecification from parameters bias to brand financial performance.====To preview the structure of our paper, following this introduction we present the relevant literature about choice process heterogeneity, specifically focusing on two-stage consumer decision-making adopted in our data generation process. After this we offer a brief overview of the design of a Monte Carlo (MC) experiment embedding the true demand representation (i.e., existing choice process heterogeneity but no preference heterogeneity), then describe the experiment's main results. In the next section we describe the effects of the demand misrepresentation (i.e., no choice process heterogeneity, but assumed preference heterogeneity) on brands' profitability; we characterize market equilibria by “plugging in” the results from the MC simulation into the analytical solution to the proposed game. We conclude with a discussion and summary of our study's insights.",Jeopardizing brand profitability by misattributing process heterogeneity to preference heterogeneity,https://www.sciencedirect.com/science/article/pii/S1755534522000173,23 May 2022,2022,Research Article,23.0
"Wang Jiangbo,Yamamoto Toshiyuki,Liu Kai","School of Transportation and Logistics, Dalian University of Technology, No.2 Linggong Road, Ganjingzi District, Dalian, China,Institute of Materials and Systems for Sustainability, Nagoya University, Furo-cho, Chikusa-ku, 464-8603, Nagoya, Japan","Received 31 December 2019, Revised 20 March 2022, Accepted 21 March 2022, Available online 6 April 2022, Version of Record 9 April 2022.",https://doi.org/10.1016/j.jocm.2022.100355,Cited by (2),"Understanding the mechanism of continuous subscribing behavior is vital to the operation and even survival of customized bus (CB) systems. Capturing the differences in subscribing behavior of active and inactive users and developing targeted and differentiated marketing strategies accordingly will help to improve the user retention rate. The results of this study revealed differences in the subscribing behavior between active and inactive users and the influence of a wide range of factors on the subscription behavior between the two groups, such as area-related and individual characteristics. Travel distance presents an inverted “U” shape effect on the propensity to use CB with the vertex at around 16 km for active users and 28 km for inactive users, which is a further finding of the monotonic positive effect indicated by the existing studies. For every RMB 10 cheaper CB trip, the likelihood that an active user holds a higher “propensity” to use CB services will increase by 8%. The cost advantage will attract novelty-seeking travelers to try CB services occasionally, but cannot convert them into high-frequency users. The passengers who started to subscribe to the CB service in winter are more likely to travel by CB frequently no matter in active and inactive user groups. The findings of this empirical study lay a theoretical foundation for ridership retention and marketing orientation of the customized and demand-responsive transit services.","As an innovative demand-responsive transportation (DRT) service, customized bus (CB) has been springing up in some countries in recent years (Abdullah, 2017; Liu and Ceder, 2015). Similar to other DRT systems, CBs collect travelers with similar travel demand, i.e., similar origin-destinations (ODs) as well as expected departure and arrival times, and aim to provide high-quality, point-to-point, customized transit services (Wang et al., 2019). User travel requests are put as a top priority throughout the service design and operation. The CB routes and timetables are customized to the passengers recruited, and the systems, therefore, are suffering from higher operating costs in contrast to the regular bus transit service. In practice, only the routes with high occupancy rates, namely 80% total seat number of a bus, can be successfully launched, and only the launched routes with enough continuous subscriptions have been maintained. Failure to catch the trend of demand evolution leads to cancellation of already launched routes due to that the low occupancy rates made the high-cost operation overwhelmed with notable examples in China, including Guangzhou, Nanjing, Xiamen (Chu, 2015; Ma, 2015; Wang et al., 2019; Yan, 2014). The flexible route design and demand-driven nature make the CB system extremely sensitive to the extreme volatility of demand (Currie and Fournier, 2020). Therefore, investigating the subscribing behaviors of CB users is urgent and imperative for the operation and even the survival of CB systems.====The frequency with which customers repeat their purchases during a period has been frequently analyzed as a behavioral consequence of user loyalty toward a product or service (Aldas-Manzano et al., 2011; Drengner et al., 2010; García Gómez et al., 2006; Tranberg and Hansen, 1986). In the field of transportation, the frequency of riding a mode of transportation is commonly investigated to reveal the mechanism of individual travel behavior towards a specific travel mode (Cao, 2010; Cao et al., 2006; Yang et al., 2019) or namely their propensity to use the mode (Wang et al., 2015). To the best of our knowledge, existing researches rarely investigated how different the travel behavior is between active passengers and inactive users. Obviously, the mechanism of continuous subscribing behavior of novelty-seeking and follow-the-leader users is different from that of loyal passengers. In the public transportation context, the cost of attracting and acquiring a new customer is generally considerably higher than that of retaining an existing one (Webb, 2010). Capturing the differences in subscribing behavior between active users and inactive users and developing targeted and differentiated marketing strategies accordingly will help to improve the passenger retention rate, which is more important for DRT systems. Therefore, this paper pursues the objective of investigating the subscribing behavior of CB users and exploring the differences in effects of a wide range of factors between active and inactive passengers.====The factors in terms of service-related, area-related, and individual characteristics were extracted from the operational data of a successful CB system and additional open-access geospatial data sources. The active passengers and inactive users were distinguished, and differences in their subscribing behavior have been investigated through a comparative analysis.====The remaining parts of this paper are structured as follows: Section 2 focuses on an extended review of studies pertaining to the travel behavior analysis of DRT users; Section 3 focuses on the methodology employed in this study; the research area, data collection, and variable selection are addressed in Section 4; finally, the results and discussions, and the conclusions drawn based on the key findings of this study are presented in Section 5 and Section 6, respectively.",Exploring the subscribing behavior of customized bus passengers: Active users versus inactive users,https://www.sciencedirect.com/science/article/pii/S1755534522000136,6 April 2022,2022,Research Article,24.0
"Gonzalez-Valdes Felipe,Heydecker Benjamin G.,Ortúzar Juan de Dios","Department of Transport Engineering and Logistics, Pontificia Universidad Católica de Chile, Vicuña Mackenna, 4860, Macul, Santiago, Chile,Centre for Transport Studies, University College London, Gower Street, London, WC1E 6BT, England, UK,Department of Transport Engineering and Logistics, Institute in Complex Engineering Systems, BRT + Centre of Excellence, Pontificia Universidad Católica de Chile, Vicuña Mackenna, 4860, Macul, Santiago, Chile","Received 5 July 2021, Revised 21 February 2022, Accepted 30 March 2022, Available online 2 April 2022, Version of Record 26 May 2022.",https://doi.org/10.1016/j.jocm.2022.100356,Cited by (1),"Latent class (LC) models have been used for decades. In some cases, models of this kind have exhibited difficulties in identifying distinct classes. Identifiability is key to determining the presence or absence of the different population cohorts represented by the ====. Theoretical identifiability addresses this issue in general, but no empirical identifiability analysis of this kind of model has been performed previously. Here, we analyse the theoretical properties of LC models to establish necessary conditions on the classes to be identifiable jointly. We then, establish a measure of behavioural difference and relate it to empirical identifiability; this measure highlights factors that are crucial for identifiability. We show how these factors affect identifiability through simulation experiments in which classes are known, and test elements such as the proportion of individuals belonging to each latent class, different correlation structures and sample sizes. In our experiments, each latent class corresponds to a different choice heuristic. We present a graphical diagnostic that supports the measure of behavioural difference that promotes identifiability and provide examples of model non-identifiability, partial identifiability, and strong identifiability. We conclude by discussing how non-identifiability can be detected and understood in ways that will inform survey design and analysis.","Latent class (LC) models can be used to represent finite mixtures of distinct groups of individuals (Kamakura and Russell, 1989). They have been widely applied in recent decades either exclusively with exogenous variables (Swait and Adamowicz, 2001; Rossetti et al., 2018) or in conjunction with latent variables in a MIMIC model (Huang and Bandeen-Roche, 2004; Hess and Stathopoulos, 2013), with diffuse choice sets (Ben-Akiva and Boccara, 1995), and either using only utility maximisation heuristics or adopting a different choice heuristic for each latent class (Hess et al., 2012; Gonzalez-Valdes and Raveau, 2018).====A key issue concerning LC models is their identifiability, which is related to the possibility of drawing inference from observed samples about an underlying theoretical structure that is observationally unique.==== Rothenberg (1971) examined the identifiability of parametric models, concluding that this required the information matrix to be non-singular. Walker and Ben-Akiva (2002) investigated theoretical and empirical identifiability. Here, we focus on the latter, where the model theoretically can be identified, but due to the data and model structure, the Hessian matrix is singular or nearly so (Chiou and Walker, 2007; Cherchi and Ortúzar, 2008), leading to poor estimates of model parameters and impeding empirical identification.====In LC models, identifiability informs about distinguishing different behaviour types and estimating the parameters that govern them, with the behaviour of each individual in the population being described as a linear combination of the theoretical constructs.==== The identifiability of LC models has been studied to varying extents. Huang and Bandeen-Roche (2004) explored theoretical identifiability in LC models specifying conditions of the components of a latent class – latent variable choice model required to achieve it. However, requirements for empirical identifiability of models that have no latent variables have not been addressed thoroughly. Thus, this paper focuses on determining conditions necessary for empirical identifiability in the absence of latent variables.====Among the applications of LC models, the one that motivated the present study is when multiple choice heuristics are considered. Success has been reported in the literature for LC models under a single heuristic with multiple parameter sets (e.g. Greene and Hensher, 2003), but few have successfully presented identifiable multiple heuristic models. Indeed, these LC models have resorted to latent variables (Hess and Stathopoulos, 2013) and normalisations (Leong and Hensher, 2012) for identifiability. Here, after establishing analytical conditions for identifiability, we show how they apply in practice to the challenge of identifying multiple heuristics.====Connecting both of these objectives, this paper investigates the empirical identifiability of LC models when only exogenous variables are used (i.e. without latent variables). To understand this, we first develop a theoretical framework to analyse the interaction of the governing forces of identifiability and show that the ratio of class-conditional probability to the model-wide probability of observed choices is crucial. Then, we investigate the use of this framework by conducting a battery of Monte Carlo simulation experiments in a realistic transport context. In this, we follow the approach proposed by Chiou and Walker (2007) to explore influences on identifiability. The simulation of latent classes is performed in the context of multiple-choice heuristics to investigate identifiability. Each of three distinct choice heuristics is tested against a linear random utility maximisation (RUM) model to assess the identifiability of that combination. We explore drivers for non-identifiability that are exemplified by the scenario of multiple-choice heuristics. Finally, we show how the results of this study provide a framework for practitioners to design surveys and experiments of LC models.====The remainder of this paper is organised as follows. Section 2 develops a theoretical framework to investigate empirical identifiability and provides a metric to explain the reasons for non-identifiability. Section 3 describes the specification and execution of a battery of empirical experiments. Section 4 analyses the results of the experiments and relates them to the drivers of identifiability within the theoretical framework; indeed, this section is helpful for practitioners to understand possible reasons for lack of identifiability. It also shows how to connect reasons beyond those described here to the overarching theoretical framework discussed in Section 2. Finally, Section 5 concludes the paper summarising the main findings and tools.",Quantifying behavioural difference in latent class models to assess empirical identifiability: Analytical development and application to multiple heuristics,https://www.sciencedirect.com/science/article/pii/S1755534522000148,2 April 2022,2022,Research Article,25.0
"Carson Richard T.,Eagle Thomas C.,Islam Towhidul,Louviere Jordan J.","Professor of Economics, University of California, Chief Scientist, ChoiceFlows, Chapel Hill, San Diego, NC, USA,Eagle Analytics of California, Inc., Scotts Valley, CA, 95066, USA,Professor of Marketing and Consumer Studies, Gordon S. Lang School of Business and Economics, University of Guelph, Canada,Emeritus Research Professor of Marketing, University of South Australia, Chief Operating Officer & Co-Founder, Choice Flows, Chapel Hill, NC, USA","Received 17 December 2020, Revised 30 December 2021, Accepted 3 January 2022, Available online 5 January 2022, Version of Record 10 January 2022.",https://doi.org/10.1016/j.jocm.2022.100343,Cited by (5),"Volumetric Choice Experiments (VCEs) are designed to capture purchase quantities rather than a single, discrete choice. They can be seen as an extension of Discrete Choice Experiments (DCEs) where individuals decide how many units of a specific good or service to buy/use rather than deciding whether to buy/use it or not. There is different information in such integer count data than is contained in traditional binary or multinomial discrete choices, which presents new opportunities and interesting challenges. Like DCEs, VCEs have different components ranging from experimental design to modelling and our focus is on the overall process of implementation rather than detailed analysis of components. Our empirical examples come from large-scale VCEs embedded in surveys administered to samples drawn from Information Resources, Inc. (IRI) consumer panel for two product categories: single serve-coffee K-pods and canned tuna. The response for each alternative is a planned purchase count, possibly zero. These counts are fit using a negative binomial regression with a multilevel mixed-effects specification. Our VCE design allows for statistical identification of own- (brand by size) and cross-price elasticities, plus the effects of other attributes and ==== and their interactions with prices. The external validity of our approach is compared to results on actual canned tuna data purchases from the same IRI panelists. Advantages and limitations of VCEs as well as many unresolved research issues are discussed.","This paper provides an overview of the design, implementation, and analysis of Volumetric Choice Experiments (VCEs). As a class of procedures, VCEs extend Discrete Choice Experiments (DCEs) to capture the number of times a consumer undertakes an activity. An obvious example, and the one motivating our empirical application, is the number of units of a specific product that a consumer buys during a single choice occasion. Another example, the number of times an individual plans to visit a local beach during July shows the applicability of VCEs is not limited to commercial products. The dependent variable in both these examples takes the form of integer counts, as such data is referred to in the econometrics and statistics literature. In applied work, and particularly in marketing, it is often referred to as volumetric data.====VCEs are of potential interest to researchers across many fields. Researchers in fields like marketing are likely to be predicting how many units of a specified product are acquired at a single point in time, and how that quantity changes in response to changeable attributes like price. In addition, many choices, which are classically binary and multinomial discrete quantities at the choice occasion level, become count data if one introduces a time element such as an hour, week, month, quarter, or year. It is these counts per time unit that are often of more interest to decision makers. For example, the number of flights a traveller took on a particular route and airline over a specified time period. Product manufacturers may be interested in the number of technical support calls that a new product receives in the first quarter after it has been shipped. Researchers in fields like health and tourism are likely to be interested in quantity usage questions like how many doctor visits a health plan member made during the month and how many vacation trips someone takes in a year, respectively. It is not surprising that a plethora of statistical models for such data have been developed (Cameron and Trivedi, 2013). We show how to use experimental design techniques to collect such volumetric choice data for one or more competing goods. In doing so, we offer one solution to the ubiquitous endogeneity problem and a high degree of multicollinearity between predictors that characterize non-experimental data.====VCEs can be viewed as a special type of DCE where the outcomes are not discrete. Rather, they are integer counts that range between zero and some finite upper bound. For example, consider airline trips between a specific city pair, such as Boston to St. Louis, during a well-defined period like a calendar year. As only one round trip per day is possible, the dependent variable must take the form of an integer between zero and 365. The attributes of those flights matter to airlines starting with the carrier. There are two carriers, American and Southwest, with non-stop flights from Boston to St. Louis. Both generate their own counts of non-stop flights that individuals take on this route during the year. Examining individual flights likely is of interest to these airlines. There is one American flight and two Southwest flights from Boston to St. Louis. These flights differ on attributes like price and departure time. Expanding the set of airlines to include flights with one stop allows the analyst to look at price and time trade-offs. An experimental design can vary price and other flight attributes, in either an actual market context (revealed preference (RP) data), or a survey context (stated preference (SP) data), allowing estimation of the desired trade-off parameters. These parameters are all but impossible to consistently estimate from non-experimental data due to how airline revenue management systems work by endogenously increasing (decreasing) the prices consumers see when tickets for a flight on a specific day sell faster (slower) than a forecasting model had predicted. Not infrequently the price parameter estimate from a simple regression on tickets sold is positive, which is inconsistent with both economic theory and intuition. The focus of a DCE is on which of the three flights from Boston to St. Louis a potential passenger takes, if any, on a particular choice occasion where random assignment of price can overcome the typical endogeneity problem. The focus of a VCE is on how many flights individuals make on each carrier during a period like a year. While related, these two perspectives are quite distinct. Concentrating on how service frequency and pricing practices influence total traffic on a route rather than on filling individual flights is one the management practices that allowed Southwest to surpass all the legacy carriers to become the airline with the highest market capitalization.====VCEs use experimental design techniques to explore how agents choose the number of units of one item (or a relatively small set of related items) in a controlled context. That controlled context experimentally varies factor(s) of interest statistically, thereby statistically identifying at least some of the key parameters in a behavioral model. The controlled context is the ability, which is typically embedded in a survey, field/lab experiment, or test market, to randomly assign different agents to different treatments.====It is useful at the onset of this paper to define the canonical form of a VCE to parallel that of DCE using a single binary discrete choice response (Carson and Hanemann, 2005). With the canonical DCE, agents from the population of interest face a randomly assigned price and the action taken is defined in 0/1 terms (not taken/taken) is recorded. As the number of such agents who are randomly assigned to each specific price used in the DCE and the number of such price points over the relevant range increases, the probability that the action is taken at each price is traced out. In the canonical VCE, the response is the number of times the action is taken so that the expected number of such actions is traced out. It is the random assignment of price to agents that provides statistical identification of the effect of changing price in both DCEs and VCEs. Both canonical forms can be expanded by changing the price attribute to another attribute, adding other attributes, adding multiple actions, and obtaining repeat responses from the same agents under different conditions. What distinguishes between them is the response being measured and the information about preferences embedded in that response. The distinction between the canonical DCE and VCE is that the former is designed to estimate how the propensity to take an action like, buying a particular good, changes with price, while the latter is designed to measure how responsive the quantity purchased is to a change in price.====Changing price to a different attribute, adding additional attributes, considering multiple goods, and observing multiple choice occasions does not alter this fundamental difference between DCEs and VCEs, although they create distinct variants of VCEs. Both DCEs and VCEs are flexible in terms of handling a range of scenarios to allow different types of choice situations. Consider for instance a DCE where an agent, sent shopping for a holiday picnic, is given an instruction to buy only one type of grilling product and then asked whether they would buy hot dogs, hamburgers, or veggie burgers, where each type had a price and specific attributes. The same DCE if the agent did not face the buy only one-type constraint could have offered additional alternatives: hot dogs and hamburgers, hamburgers and veggie burgers, hot dogs and veggie burgers and hot dogs, hamburgers, and veggie burgers. The VCE with the original one-type constraint would have elicited a response in terms of standard quantities, where at most one of the quantity responses is positive. Without this constraint, a VCE allows for multiple positive quantities. Whether the scenario contains the one-type constraint clearly influences the sort of behavior models that one should consider for data from the relevant DCE and VCE. It does not influence, however, that the DCE response is measuring purchase propensities and the VCE is measuring quantity propensities. Throwing away information on whether a positive quantity response is greater than 1, converts VCE data to DCE data.====Before proceeding further, it will be useful to note what this paper is not about because these are all likely to be fruitful areas for future research. First, it is not about identifying which count data model is best for a particular application. While count regression models are useful to our enterprise, there is a large literature on such models. Just as different statistical models have been developed to exploit specific features of DCEs, we expect this to also happen with VCEs and that particular experimental designs will be useful testing competing specifications. Second, this is not a paper on the linkage between count data and consumer demand theory, although this is clearly an area that is underdeveloped relative to that for discrete choice data. Third, this also is not a paper on welfare calculations cast in terms of willingness to pay. While we use a commonly cited variant linking count data to an underlying economic model of demand, our VCE framework is amenable to being used with others. Fourth, this paper is not about experimental design, per se. The literature on experimental design is large in both the biomedical literature world, where clinical trials implemented using random assignment have long been the gold standard and outcomes of interest are often discrete, as well as on the industrial side, where outcomes of interest are more often continuous in nature. Designs for discrete choice experiments, where there is a long history and large literature, are more directly relevant. While the experimental designs used in our empirical examples are more than serviceable, they also are intentionally a bit pedestrian, so they are easy to explain. Finally, the work we report on addresses all the key elements needed to undertake VCEs in a straightforward manner using sets of products in categories people would see on a grocery store shelf. The basic scenario can be adapted to many other contexts. However, we eschew assigning priority as to who did the first VCE. That is probably lost in the ether of academic and commercial research scattered across many fields.====The remainder of the paper is organized as follows. In Section 2, we provide short overviews of some of the related literature that is useful in conceptually thinking about the issues involved in conducting a VCE. Next, in Section 3, we discuss econometric models available for modelling count data from VCEs. Section 4 lays out the specific experimental designs used in our empirical examples. Section 5 provides a description of the data, which was collected as part of a Social Sciences and Humanities Research Council (SSHRC) of Canada grant, as well as the model specification. Section 6 summarizes the model results. In section 7, we examine the external validity of our approach against revealed preference (RP) data from the same VCE panellists. Section 8 closes the paper by discussing advantages and limitations of VCEs and noting some of the research issues that remain.",Volumetric choice experiments (VCEs),https://www.sciencedirect.com/science/article/pii/S175553452200001X,5 January 2022,2022,Research Article,26.0
"Guerrero Thomas E.,Guevara C. Angelo,Cherchi Elisabetta,Ortúzar Juan de Dios","Department of Civil Engineering, Universidad Francisco de Paula Santander Ocaña, Sede el Algodonal Ocaña, Colombia,Department of Transport Engineering and Logistics, Pontificia Universidad Católica de Chile, Santiago, Chile,School of Engineering, Future Mobility Group, Newcastle University, Newcastle upon Tyne, UK,Department of Civil Engineering, Universidad de Chile, Santiago, Chile,BRT+ Centre of Excellence, Santiago, Chile,Instituto Sistemas Complejos de Ingeniería (ISCI), Santiago, Chile","Received 27 January 2021, Revised 28 December 2021, Accepted 29 December 2021, Available online 5 January 2022, Version of Record 8 February 2022.",https://doi.org/10.1016/j.jocm.2021.100342,Cited by (1),") under some conditions, using discrete indicators instead of continuous ones seems not to be a problem, however, (====) there is also evidence that indicates that the correction could fail under not unusual circumstances.",None,Characterizing the impact of discrete indicators to correct for endogeneity in discrete choice models,https://www.sciencedirect.com/science/article/pii/S1755534521000749,5 January 2022,2022,Research Article,27.0
"Astroza Sebastian,Guarda Pablo,Carrasco Juan Antonio","Department of Industrial Engineering, Centro de Desarrollo Urbano Sustentable (CEDEUS), Universidad de Concepción, Chile,Department of Civil Engineering, Centro de Desarrollo Urbano Sustentable (CEDEUS), Universidad de Concepción, Chile","Received 31 August 2020, Revised 8 December 2021, Accepted 15 December 2021, Available online 21 December 2021, Version of Record 26 December 2021.",https://doi.org/10.1016/j.jocm.2021.100341,Cited by (1),"Motivated by the interest in the contextual role of transport in health outcomes, this paper explores the relationship between these two dimensions and food purchasing patterns in the city of Concepción, Chile. The modeling framework corresponds to a Generalized Heterogeneous Data Model, composed of a structural equation and a measurement equation model. Socio-demographic variables and constructs are used to study the relationship between food purchasing patterns (store type choice and frequency), transportation patterns (purchasing usual mode), and health (body mass index, BMI). Individual's healthy lifestyle propensities and time pressure are defined as latent constructs. The results identify how key socio-demographic characteristics, such as age and income, are crucial to understanding store type choice and frequency, controlling the built environment's role in-store supply and distance from home. These variables also influence transportation patterns and health outcomes. The model also captures the endogenous effects among the three dimensions of interest, finding a statistically significant impact of the transportation mode on food purchasing patterns through the interaction with distance. No direct effects were found between BMI and transport patterns when including the latent constructs. This result suggests that promoting active transportation may not necessarily lead to better health. Health-conscious individuals engage in active transport, taking care of their diet, and seeking a healthy daily routine.","Health problems are at the top of public opinion and policy priorities in the world. In Chile, as income levels increase, obesity has become a public health priority problem, both due to sedentary behavior (Sadarangani et al., 2019) and poor healthy food patterns (Crovetto et al., 2018; Cediel et al., 2020). Chile has one of the highest obesity rates globally: 25% of the children between four and six years of age present obesity (Kain et al., 2019), and more than 31% of Chileans 15 years and older are obese (Blanco et al., 2019). However, little attention has been paid to understanding the role of transport and the urban environment on people's activity-travel patterns and their consequent food patterns. A relevant policy question that remains unanswered is people's decision-making processes related to diet, places, and practices with their level of access to food purchase locations (White, 2007). While the connection between mobility, access, and food consumption seems straightforward, multiple factors occur, and complex relationships remain to be understood. Besides, there is an important gap in understanding accessibility to food in Latin America. These transport-related studies still focus mostly on work and study purposes (Vecchio et al., 2020), ignoring the importance of activities as relevant as food purchasing.====Previous studies have mostly concentrated on somewhat limited perspectives, which still heavily rely on the concept of ""food deserts,"" developed in the 1990s, defined as ""poor urban areas, where residents cannot buy affordable, healthy food"" (Cummins, 2014). However, more comprehensive assessments need to look at people's daily mobility strategies in a broader sense that could consider people's context more explicitly (Shannon and Christian, 2016). For example, the multimodal nature of activity-travel patterns needs to be considered. Public transport and walking are essential for specific food shopping destinations for low-income groups and car-sharing and lifts from other people other possible alternatives (Clifton, 2004). This mobility behavior may involve spatial, temporal, monetary, and relational dimensions. For example, buying food at different places on the way to other activities is a strategy that involves space (Cresswell, 2006) and time (e.g., weekly and monthly rhythms, as in Axhausen et al., 2002). However, that strategy also involves money (e.g., combining different stores based on prices as in Shannon, 2016); and relations (e.g., coordinating food shopping activities within and between households, as in Ettema and Schwanen, 2012). Furthermore, as discussed before, these dimensions may be strongly influenced by this study's Latin American context, considering the role that open street markets play in food access compared with Global North cities and the importance of public transport and walking (Rajagopal, 2010).====With this background, this paper aims to study the relationships between people's food purchasing behavior, transport patterns, and health outcomes, by explicitly controlling for the role of the built environment and their socio-demographic characteristics. The approach uses a flexible econometric framework that considers the relationships among three dimensions simultaneously. The work takes advantage of a field survey conducted in Concepcion, Chile, to unveil these relationships. A behavioral model links food consumption patterns – the purchase frequency at various types of food purchase location – to lifestyle and socioeconomic characteristics, accessibility to food purchase locations, mobility, and the travel distances to reach people's most frequent food purchase destinations, among other variables. The data collection process involved intercepting 850 individuals purchasing food in open street markets and supermarkets in Concepción to study the potential relationships between food consumption patterns and eating habits with accessibility to open street markets and supermarkets.====The paper structures as follows. After this introduction, the second section presents a literature review about the three critical relationships studied in this paper: food purchasing and transport, food purchasing and health, and transport and health. With that background, the section ends with the paper's three hypotheses and main contribution. The third section presents the data used for the analysis and some relevant descriptive statistics. The next section presents the modeling framework, followed by the results. Finally, the paper's last section presents some conclusions and future work.","Modeling the relationship between food purchasing, transport, and health outcomes: Evidence from Concepcion, Chile",https://www.sciencedirect.com/science/article/pii/S1755534521000737,21 December 2021,2021,Research Article,28.0
"Tsoleridis Panagiotis,Choudhury Charisma F.,Hess Stephane","University of Leeds, University road, Leeds, LS2 9JT, UK","Received 1 March 2021, Revised 10 November 2021, Accepted 19 November 2021, Available online 10 December 2021, Version of Record 23 December 2021.",https://doi.org/10.1016/j.jocm.2021.100336,Cited by (2),"Choice models estimated on datasets with large numbers of alternatives present significant challenges leading to rapidly expanding computational cost, as well as potential behavioural realism issues. Sampling of alternatives has been a well-established method for overcoming the computational limitations, mostly applied to models of residential location. Nonetheless, destination choice models of discretionary activities require a different type of analysis, since the choice can be governed by time–space constraints and familiarity regarding the alternatives. Observing the general areas of travel for a period of days using high resolution GPS tracking can provide important information of the individuals’ whereabouts. The present study, taking advantage of such a dataset, proposes a more behaviourally realistic sampling protocol to reduce the choice set utilising the geography-based concepts of activity spaces. Differential importance sampling rates are applied depending on the individual’s activity space and trip chain making the resulting sampled choice set a function of person-specific spatial awareness and mode-specific time–space constraints. The performance of the sampling protocol developed is assessed using a model estimated with the full choice set and compared with random sampling and several other importance sampling protocols. The modelling outputs suggest that random sampling should be used with care, since it can result in highly biased estimates, but with low standard errors, as well. The proposed approach incorporates both time–space constraints and individual spatial awareness and is able to produce less biased estimates, achieve higher sampling stability and statistical efficiency, while also avoiding overfitting.","Mathematical models capable of predicting the destinations of travellers are important for forecasting transport demand. First introduced by McFadden (1973) and later expanded by Daly (1982), discrete choice models have emerged as the prominent tool for modelling disaggregate level destination choices. The large number of potential alternatives, however, poses two issues, namely behavioural realism and computational complexity. On one hand, considering the full choice sets has the risk of leading to a behavioural misrepresentation of the individual-level decision making process, since in reality, the decision makers are highly unlikely to equally evaluate all the alternatives in the global choice set. On the other hand, estimating a model using a large number of alternatives in the choice set leads to high estimation times limiting their adoption in practical applications.====The problem of choice set specification and its significance is well documented in the literature (Thill, 1992, Pagliara and Timmermans, 2009). In fact, estimating a model using an inaccurate choice set can be considered a case of model misspecification leading to biased estimates (Swait and Ben-Akiva, 1987). ==== based on the theoretical foundations of Manski’s model (Manski, 1977) has been proposed as an approach of decoupling the choice problem into choice set generation and alternative choice sub-problems (Thill, 1992, Horni et al., 2011). Manski’s formulation requires an exhaustive enumeration of all possible non-empty choice sets, a process that quickly increases exponentially in complexity with the addition of more alternatives. Several variants based on the principles of Manski’s model have been proposed over the years aiming to relax the computational complexity (Swait and Ben-Akiva, 1987, Ben-Akiva and Boccara, 1995, Thill and Horowitz, 1997a, Cascetta and Papola, 2001, Martinez et al., 2009, Haque et al., 2019). Nonetheless, in addition to being critiqued on whether these models are able to replicate Manski’s principles (Bierlaire et al., 2010), in many cases they adversely impact the behavioural realism of choice set generation (e.g. independent availability of alternatives) negating the main purpose of this modelling approach, while the increased number of model parameters and the non-concavity of the log-likelihood function have also hindered their adoption in spatial choice models (Thill, 1992, Pagliara and Timmermans, 2009).====Despite the ongoing efforts to decouple choice set formation from the choice itself (Thill and Horowitz, 1997a), there is the counter-argument that the notion of choice set misspecification only has theoretical grounds (Lerman, 1985, Thill, 1992), since in an empirical setting, the choice probabilities of alternatives that are not in the actual choice set of an individual are likely to be negligible provided the utility function is correctly specified (Thill and Horowitz, 1997b). In that sense, the behaviourally accurate estimates from an unconstrained model using the full choice set could still be considered as a sufficient representation of reality.====Focusing this time on overcoming the computational limitations of models with large choice sets, ==== has been proposed as a way to reduce the choice set size and in turn the estimation times, while still obtaining behaviourally realistic estimates. McFadden (1978) showed that constraining a choice set by sampling of alternatives still yields unbiased estimates, if the true model is an MNL, by adjusting the utility function with the inclusion of an additional term, called the sampling correction term (====). The bias in the estimated parameters, defined as the difference between the sampled estimates and the estimates obtained using the full choice set, will decrease as the size of the sampled choice set keeps increasing (Guevara and Ben-Akiva, 2013b). The specific choice set size beyond which only marginal improvements are observed in the accuracy of the sampled estimates is to be determined as a result of the analysis. As mentioned in Guevara and Ben-Akiva (2013b), the process of identifying the minimum required choice set size to achieve estimation stability is equivalent to the process of finding the required number of draws for the same purpose in a simulated Maximum Likelihood estimation for a mixed Logit modelling framework. The issue of choice set specification is still relevant in the ==== approach, since the inclusion of more relevant alternatives to the choice task/individual will lead to a lower bias with a smaller choice set size, hence in general to a more efficient sampling protocol.====The additional SC term has the purpose of adjusting the utility function to account for the sampling bias, since the spatial distribution of the sampled alternatives will now depend on the sampling protocol developed and it may differ substantially among individuals. The additional term is computed as ====, which is the logarithm of the probability of creating the choice set ==== given that alternative ==== was chosen for individual ====. That can be also considered as a penalty added to the utility, since the ==== will always be between 0 and 1 and its logarithm will always be negative. In other words, the smaller the probability of sampling that choice set ==== given that alternative ==== is selected, the bigger the penalty applied. In that case the choice probabilities are modified as shown in Eq. (1) and the ==== term for stratified importance sampling without replacement is defined in Eq. (2) (Ben-Akiva and Lerman, 1985, Guevara and Ben-Akiva, 2013a). ====
 ====where ==== is the number of alternatives sampled from stratum ==== of alternative ==== and individual ==== and ==== is the total number of alternatives in that stratum. The SC is calculated for each alternative ==== per choice task as if that alternative was chosen. It is clear to see that in cases of random sampling with a uniform probability from the global choice set, where ====, the additional SC term remains the same across alternatives and hence it drops out (Nerella and Bhat, 2004). No correction is thus needed with random sampling, but that is not the case with importance sampling. Guevara and Ben-Akiva, 2013a, Guevara and Ben-Akiva, 2013b extended this theory for stratified importance sampling in GEV and mixed logit models, respectively.====Given the need for corrections when using importance sampling, random sampling provides an easier to implement sampling protocol compared to the former. The limitation of random sampling, however, is that it leads to more deterministic models, since the sampled alternatives can be topologically not relevant to the chosen alternative. Therefore, the model will assign higher choice probabilities to the chosen alternative compared to the rest diminishing the explanatory power of the model. The insufficient number of close substitute alternatives to the chosen one, for small choice set sizes, leads a random sampling protocol to require choice sets of generally larger sizes in order to achieve the same level of estimate accuracy compared to an importance sampling protocol, making the former a less efficient approach. Various importance sampling techniques have been proposed in the literature, as opposed to a pure random sampling, aiming to create a reduced choice set that would best represent the individual’s trip-specific constraints (Li et al., 2005, Scott and He, 2012, Leite Mariante et al., 2018). Examples can be found in empirical studies of mainly residential location choice (McFadden, 1978, Farooq and Miller, 2012, Guevara and Ben-Akiva, 2013a, Guevara and Ben-Akiva, 2013b). The implementation of importance sampling in a destination choice of discretionary activities, however, will require a different type of handling from a residential location choice, since the chosen alternatives will be subject on some degree to travel impedance and time–space constraints (Daly et al., 2014). Evidence also shows that availability-consideration of alternatives depends not only on time–space constraints, but also on the familiarity/awareness of those destinations (Landau et al., 1982, Thill and Horowitz, 1997a).====The current paper focuses on the ==== approach for the purpose of decreasing the computational cost of estimating a spatial choice model with a large number of alternatives. More specifically, the aim is to propose a sampling protocol that utilises concepts of Activity Spaces (AS) from the time–space and behavioural geography literature, namely (1) Potential Path Areas based on detour factors around a previous origin ==== and a following destination ====; and (2) Ellipses incorporating a notion of the individuals’ awareness/knowledge of their surrounding space. The geography-derived notion of Activity Spaces is a tool capable of capturing individual spatial awareness and time–space constraints, and we utilise them in order to create person- and trip-specific spaces, respectively, for importance sampling of mode-destination alternatives.====We rely on the notions of ====, ==== and ====, concepts that are looked at in detail in Section 2. To the best of our knowledge, SDEs and FBs have never been used before, on their own or in combination with DEs, for the purpose of delineating a choice set in a destination choice model, despite their extensive use in studies focusing on exploratory analysis of individual travel-activity behaviour. It is hypothesised that including an additional stratum delineated by SDEs and FBs would result in more accurate sampled choice set models (less biased estimates). That sampling protocol will result in constrained/sampled choice sets with most alternatives adhering to time–space constraints (within DEs) and also being familiar to the individual (within SDEs/FBs).====The remainder of the paper is as follows. In the following section, we give an overview of the relevant literature on time–space geography before expanding this to the context of sampling of destinations. In the third section, the modelling framework developed and the data utilised for the ensued practical application are presented. The results are presented next followed by a concluding section summarising the findings and setting the direction for future research.",Utilising activity space concepts to sampling of alternatives for mode and destination choice modelling of discretionary activities,https://www.sciencedirect.com/science/article/pii/S1755534521000695,10 December 2021,2021,Research Article,29.0
"Bose Arundhati Sarkar,Sarkar Sumit","XLRI – Xavier School of Management, Jamshedpur, 831001, India","Received 25 November 2020, Revised 2 November 2021, Accepted 3 November 2021, Available online 8 November 2021, Version of Record 11 November 2021.",https://doi.org/10.1016/j.jocm.2021.100327,Cited by (0),"This paper develops a discrete choice model to address the problem of an other-pleasing decision-maker (DM), who makes a choice for another individual without knowing her/his preferences and expectation. The DM's choice may delight or disappoint the other, depending on her/his expectation from the DM. The psychological utility of the other is a function of the emotion triggered by the DM's choice. The objective of the DM is to maximise the expected psychological utility of the other. In the absence of complete information, a risky choice is made by the DM, based on signals about the other's expectation that s/he receives. In terms of ====, the DM may be a delight-seeker, or disappointment-averse, or neutral. While a delight-seeker overweighs the other's psychological utility from delight, a disappointment-averse DM overweighs the disutility of disappointment. Based on the model, hypotheses have been constructed and tested in a gamified decision-making scenario that simulated the model conditions. In this serious game experiment, delight-seeking, disappointment-averse, and neutral behaviours were induced by use of appropriate incentivisation of the subjects in the corresponding treatment groups. Unique choice architecture has been given to subjects in each treatment group to signal the expectation of a hypothetical other, and choice decisions were observed under elicited belief. Compared to the control (neutral) group, those in delight-seeking treatment were observed to be less likely, and those in disappointment-averse more likely, to make a choice in conformity to the signal they received. Statistically significant results were obtained only when the signals were strong.","In terms of personality types (Adler, 2013) a ‘pleaser’ loves to serve others (Mosac, 1971), and her/his thoughts are ‘other-centred’ (Boldt, 2007). Personality typology is a ‘conceptual device to make more understandable the similarities of individuals’ (Adler, 1929), and hence, a person who displays pleaser behaviour in specific circumstances is not always a pleaser (Kefir and Corsini, 1974). There is documented evidence in the academic literature, across multiple disciplines, of pleaser behaviour in making choice decisions. Many of the consumption choices, for example, buying presents, going to restaurants or movies, choosing recreational sites, or buying food at the grocery store, can be made to please others, like a spouse, a parent, a son/daughter, a friend, a family member who is a significant other, or even a professional relation, rather than the self (Ariely, 2000; Beagan and Chapman, 2004; Morey and Kritzberg, 2012). Such choices are referred to as other-pleasing in this paper.====When individuals are concerned about pleasing another individual, they attempt to make decisions that are consistent with the perspectives of the other if they have complete information about the other's perspectives (Kardes et al., 2014). Gift selection is possibly the most common example of other-pleasing choice when the giver wants to convey to the recipient that the relationship is valued and selects a gift that matches the recipient's preferences (Belk and Coon, 1993; Belk, 1996; Steffel and Le Boeuf, 2014). However, the pleasers sometimes must make other-pleasing choices under incomplete information about the other's preferences. Choosing a gift to initiate a relationship and to achieve relational proximity to the recipient (Belk and Coon, 1993) is a classic example of other-pleasing choice made under incomplete information. The decision-maker (DM) may however mispredict the other's preferences and expectations (Zhang and Epley, 2012; Galak et al., 2016), while attempting to make an other-pleasing choice. In the absence of complete information, the DM is required to anticipate the other's preferences.====There are choice models where the DM's utility or willingness to pay depends on the well-being of others along with her/his own (Hensher et al., 2011; Ozdemir et al., 2016; Svenningsen and Jacobsen, 2018; Elsenbroich and Payette, 2020). Other-pleasing DMs, in contrast, care only about fulfilling the expectation of the other. Surprisingly, there does not exist a discrete choice model of other-pleasing decisions made under incomplete information. The emerging literature on decision-making for others is closely related. This strand of behavioural economics research focusses primarily on decisions involving risk-taking for others, like making an investment choice, or choosing a medical intervention or career option for another individual. The dominant experimental query has been whether risk-behaviour or ambiguity attitude significantly differs while deciding for others vis-à-vis while making the same decision for self (Zikmund-Fisher et al., 2006; Füllbrunn and Luhan, 2017; Vlaev et al., 2017; Zhang et al., 2017; Xu et al., 2018). The difference (if any) may be the consequence of incomplete information regarding the other's risk-preference or ambiguity attitude.====While deciding for others, the DM takes risk due to incomplete information about the other's preferences. It might be impossible to obtain complete information about another individual even when the DM knows him/her for a long time and the relationship is a close one. Studies in social psychology (Dunning et al., 1990; Swann and Gill, 1997) and consumer behaviour (Lerouge and Warlop, 2006) observed that people overestimate what they know about their close friends and significant others. Likewise, the literature on proxy choice (Allen and Shuster, 2002; Cai et al., 2015; Turnbull et al., 2019) and proxy response (Beck et al., 2012; Seebauer et al., 2017; Maruyama et al., 2021) highlights the fact that people do not know the preferences of their close relations and/or household members. Hence, when they must choose for a family member, they encounter a situation of decision-making for another individual without complete information about the other's preference and expectation. For decisions like medical intervention or school/career choice, the other may not be able to express her/his preference and expectation when the decision is made, but ex-post there might be a mismatch. For incapacitated patients, familial proxies make decisions based on inaccurate information about the patient's wishes (Allen-Burge and Haley, 1997). Incompleteness of information may prevail even when a joint consumption decision (like choosing a restaurant or holiday destination) is made through a discussion, wherein the partner who first proposes a choice takes the risk of decision-making under incomplete information about the other's preference and expectation. This is so because the other might accept the proposal to avoid disappointing the proposer, even if the proposed choice is not her/his most preferred one. This paper develops a model of other-pleasing choice under incomplete information about the other's preferences, accounting for the possibility of expectation mismatch.====Ex-post, the choice made by the DM (who will be referred to by the pronoun ‘she’) may or may not match with the preferred alternative of the other (referred by the pronoun ‘he’). If the choice made by the DM does not match with the other's preference, then it not only results in loss of material utility derived by the other from consuming the good/service, but also affects his psychological utility, which is a function of the emotional state (like delight or disappointment). According to ‘decision affect theory’ (Mellers et al., 1997), emotional response is triggered by unexpected outcome. If the other expects the DM to choose the alternative that is not preferred by him, but she makes the choice in accordance with his preference, then the other gets delighted by the surprise and thereby receives a positive psychological utility. However, if the other expects the DM to make the choice in accordance with his preference, but she chooses the alternative that is not preferred by him, then the other receives a negative psychological utility due to disappointment. The expectation of the other depends on his belief about what the DM may choose. The other-pleasing DM is strategic as her sole objective is to maximise the expected psychological utility of the other. Since the DM does not know the other's belief, she is required to form a second-order belief about the belief of the other. The DM is delight-seeking if she cares more about the other's utility gain because of the delight and is disappointment-averse if she cares more about the other's utility loss due to a disappointment. This paper models a choice decision based on the DM's belief about the other's expectation. The framework of ‘psychological game theory’ (Geanakoplos et al., 1989; Battigalli & Dufwenberg, 2007, 2009) has been utilised to incorporate the second-order belief of the DM in the analysis. The second-order belief may be formed based on an exogenous signal (Khalmetski et al., 2015). The DM may derive signals about the other's expectation from her past interactions with him, and/or from her past interactions with other individuals for whom she has made choice decisions. Notably, past interactions help the DM in obtaining only noisy signals, and not definite information. The strength of the signal depends on the frequency of disappointment/delight observed by the DM in her past interactions. Considering that the second-order belief is formed based on an exogenous signal, the analytical model develops the hypotheses of this paper that a delight-seeker (disappointment-averse) is less (more) likely to conform to the signals. This is an important result in choice modelling, as it tells us how a DM might use available signals while choosing for others under incomplete information about the other's preferences, and in absence of any self-interest. The general nature of the model makes it applicable in several areas, which is discussed in Section 5.====A serious game (SG) experiment (Villalobos, 2007; Vos, 2015; Katsaliaki and Mustafee, 2015; Rumore et al., 2016; Gordon and Yiannakoulias, 2020) has been designed to test the hypotheses. In the context of moral choice behaviour, Chorus (2015) noted the usefulness of SG experiments in studying how an individual's behaviour influences and is influenced by others, as this approach helps the researcher in controlling the behaviours, perceptions, and expectations of the participants to some extent. The purpose of the SG experiment is to study the choice behaviour of the DM under incomplete information, and to observe if there is any significant difference between the behaviour of delight-seeking and disappointment-averse DMs, in response to the same set of signals. However, it must be noted that this study does not intend to identify the prevalence of delight-seeking and disappointment-averse personality traits in the sample of subjects. Identifying the determinants of these personality traits is not within the scope of this study. The SG approach is conducive for inducing the subjects to emulate the other-pleasing DM's behaviour under incomplete information, through use of appropriate incentives. Delight-seeking behaviour was induced on one treatment group, disappointment-aversion on another group, and neutrality (that is, neither delight-seeking nor disappointment-averse) was induced in the control group. The subjects were given vignettes about a hypothetical ‘other’ for whom they were required to make a decision. Within a treatment group, each subject was given a unique choice architecture that signalled to them the other's expectation. Signals received by different participants varied in strength, which was controlled through the choice architecture. The mechanism has been elaborated in Section 3.1. Experimental observations reveal that the participants in the delight-seeking (disappointment-averse) treatment were significantly less (more) likely to conform to the signal, in comparison to the control group, only when the signals were strong.====The rest of the paper is organised as follows. Section 2 develops the analytical model. The experiment design is discussed in Section 3, followed by data analysis and result presentation in Section 4. The methods and results are discussed with respect to the literature in Section 5, and the final section is dedicated to drawing conclusions.",Delight or disappointment? A model of signal-based other-pleasing choice,https://www.sciencedirect.com/science/article/pii/S1755534521000609,8 November 2021,2021,Research Article,30.0
"Arteaga Cristian,Park JeeWoong,Beeramoole Prithvi Bhat,Paz Alexander","Department of Civil and Environmental Engineering, University of Nevada Las Vegas, Las Vegas, NV 89154, United States of America,Department of Civil Engineering and Built Environment, Queensland University of Technology, Brisbane, Queensland 4000, Australia","Received 17 April 2021, Revised 24 September 2021, Accepted 29 November 2021, Available online 20 December 2021, Version of Record 29 December 2021.",https://doi.org/10.1016/j.jocm.2021.100339,Cited by (1),"Mixed Logit is an advanced and flexible tool for the study of discrete choice problems. However, this flexibility involves computationally intensive calculations, as the estimation of Mixed Logit models requires the simulation of integrals. In addition, the specification of Mixed Logit models requires decisions such as potential explanatory variables to be included in the model as well as their mixing distributions. This specification process involves testing and estimation of different combinations of variables and mixing distributions, which is time consuming and computationally intensive. In response, this paper introduces ====, an open-source ==== package that leverages the performance of graphic processing units (GPU) for an efficient estimation of Mixed Logit models. For benchmarking, the performance of ==== was compared against the ==== and ==== ==== packages as well as the ====, ====, ====, and ==== ==== packages. Artificially generated as well as actual data were used to evaluate the performance gains provided by ====. Results suggest that using a mid-range graphics card and a regular desktop computer, ==== is in average 55x faster than ====, 43x faster than ====, 74x faster than ====, 39x faster than ====, 16x faster than ====, and 27x faster than ====, with an additional advantage of efficient memory management. The performance gains provided by ==== facilitate an efficient modeling process, as it enables the testing of a large number of model specifications more efficiently relative to existing software packages. ====’s open source code, documentation, and usage examples are publicly available in the package’s GitHub repository.","Discrete choice models are widely applied in many contexts, including analysis of consumer preferences, travel behavior, and traffic crash severity (Train, 2003, Ben-Akiva and Lerman, 1985). Mixed Logit is one of the most prominent techniques for discrete choice modeling because of its flexibility and ability to approximate any random utility specification (McFadden and Train, 2000). They allow a flexible error structure and unrestricted substitution patterns. Unlike Probit models, Mixed Logit models are not constrained to normal parameter distributions, which enables a wider range of applications by providing extra flexibility. Various applications of Mixed Logit models include taste heterogeneity in mode choice behavior (Vij and Krueger, 2017), health study preferences (Raspa et al., 2020), neighborhood choice (Wang et al., 2020), and road crash analyzes (Intini et al., 2020). Despite the advantages of Mixed Logit models, there are significant difficulties in effectively developing and applying these models in a time-efficient manner. In the process of specifying these models, a series of assumptions must be made and subsequently verified. Hence, it is important to develop tools that can help analysts to efficiently test a large number of model specifications.====Several proprietary and open-source tools are available for the estimation of Mixed Logit models (Mariel et al., 2021). Popular proprietary tools include ==== (StataCorp, 2019) and ==== (Greene, 2012), and well-known open-source tools include the ==== ==== package (Brathwaite and Walker, 2018), the ==== ==== package (Bierlaire, 2020), the ==== ==== package (Croissant, 2020), the ==== ==== package (Hess and Palma, 2019), the ==== ==== package (Molloy et al., 2021), and the ==== ==== package (Sarrias and Daziano, 2017). The discussion and benchmark conducted in this paper focuses on open-source tools, given that the proprietary tools have licensing requirements for usage. Existing open-source tools offer a rich set of features for specification of Mixed Logit models, such as the ability to work with panel data, handling of unbalanced panels, and inclusion of individual and alternative specific variables. In addition, these open-source tools allow the incorporation of several types of mixing distributions, such as normal, log-normal, triangular, and uniform, except for ====, which only allows normal mixing distributions.====Although the existing open-source tools for the estimation of Mixed Logit models have significantly contributed to the research and practice in statistical analyzes, these tools are limited by their running times, as Mixed Logit models are computationally intensive. This can be a strong limiting factor in several scenarios. First, slow estimation times can be inconvenient when analysts need to test several model specifications before deciding on a final specification. Second, the amount of data available for analysis have significantly increased due to the extensive adoption of technology and telecommunications. Therefore, data analysis tools need to evolve to keep up with the increasing demand in volume processing capacity. Third, despite the widespread practice of estimating models using only a few hundred random draws, past studies have highlighted the importance of using a larger number of draws (Czajkowski and Budziński, 2019, Chiou and Walker, 2007), as failing to do so may hide serious identification problems. Given that the use of many random draws can be important in model estimation, estimation efficiency becomes a crucial factor, which is one of the aspects the developed ==== package seeks to enhance. Finally, slow estimation times limit the leveraging of recently proposed approaches for the assisted specification of Mixed Logit models, such as the ones proposed by Paz et al., 2019, Ortelli et al., 2021 and Rodrigues et al. (2020), that require iterative testing of a large number of model specifications, which can be excessively time consuming using existing estimation tools.====The existing ====, ====, and ==== packages lack a built-in capability to perform parallel processing to reduce estimation time. On the other hand, the ====, ====, and ==== packages can leverage multiple processor threads for parallel processing, which has been shown to help reduce the estimation time (Hess and Palma, 2019). However, despite the multi-thread processing capabilities of ====, ====, and ====, their estimation times are still high, even when using many processor threads, as it will be shown later in the benchmark section. In view of the performance limitations of existing estimation packages, this paper introduces ==== as an alternative estimation tool that leverages GPU processing to significantly speed-up the estimation of Mixed Logit models and escalate to estimations using hundreds of thousands of random draws.",: An open-source ,https://www.sciencedirect.com/science/article/pii/S1755534521000713,20 December 2021,2021,Research Article,35.0
"Maaya Leonard,Meulders Michel,Vandebroek Martina","Faculty of Economics and Business, KU Leuven, Naamsestraat 69, 3000 Leuven, Belgium","Received 23 November 2020, Revised 7 May 2021, Accepted 29 July 2021, Available online 7 September 2021, Version of Record 21 September 2021.",https://doi.org/10.1016/j.jocm.2021.100308,Cited by (0),"Choice data appear together with drop out data indicating if respondents completed the exercise. In case of non-completion, the choice sequences end at the tasks where the respondents exited the study. In the analysis of choice data, the focus is always on choices made while the drop out behavior is completely ignored. However, the choice making and the drop out process could be latently related. For instance, respondents who are more likely to drop out of the exercise could give less consistent choices throughout or just before they exit. In such cases, ignoring the drop out dimension could lead to biased or inefficient results. In this paper, we use shared random effects and covariate effects to model the association between a scaled multinomial logit model for the choices and two different models for the drop out component. Through simulations, we show that a joint model provides less biased and more precise estimates and its 95% credible intervals have better coverage for true parameter values.","A Discrete Choice Experiment (DCE) is an attribute-based method used to investigate individuals’ preferences among alternatives. DCEs involve hypothetical scenarios (choice sets or choice tasks) composed of several competing alternatives. The alternatives in a choice set are designed to vary along their attribute levels (see Rose and Bliemer, 2009 for a review of design of choice experiments). Respondents are then asked to choose a preferred alternative in each scenario. The choice experiments can be done offline through mailing or by providing choice cards to respondents. With the near-universal availability of technology however, more and more studies are being conducted online. Often, respondents are presented with several choice scenarios and therefore provide a sequence of choices. The sequence of scenarios can additionally be considered as having a discrete ==== feature arising from the serialization of choice tasks. The repeated nature of choice data therefore calls for a special analysis to handle the correlation in responses and the underlying ==== dimension.====Most of the focus in the literature has been directed to accounting for the correlation arising from respondents providing repeated choices. In this regard, the mixed multinomial logit (MMNL) (Train, 2009, Hess and Train, 2017) has been popular and is a highly flexible model that allows parameters to be person-specific and correlated. Restrictions to the MMNL model have also been proposed in cases where estimating a full covariance matrix is uninteresting or to minimize the computational burden. For example, the scaled multinomial logit (SMNL) (Fiebig et al., 2010, Sarrias and Daziano, 2017) model introduces a person-specific scale that multiplies the fixed utility coefficients. This multiplication ensures that there is a proportional change to the affected coefficients for each respondent. Whereas ways to handle the correlation feature in the analysis of choice data have largely been addressed, research into the time-like feature has only been limited to respondents’ fatigue and learning effects (Hess et al., 2012, Czajkowski et al., 2014). However, as we note below, long and complex choice experiments amplify fatigue and learning effects which can lead to respondents dropping out.====Long surveys can be tiring or could be associated with respondent learning. The influence of these effects on responses has been assumed to be limited to the model scale (Hess et al., 2012, Czajkowski et al., 2014). First, if respondents become fatigued, their involvement in the survey reduces and laxity creeps in. As a result, precision (scale) in their responses decreases (increases). Second, learning implies that as respondents go through the choice exercise, their understanding improves and they therefore provide more precise choices. Studies have also shown that precision in responses can vary with the complexity of choice tasks (Danthurebandara et al., 2015). This is because more complex tasks require more effort and time to make a choice, which can compromise the precision in the responses, compared to simple tasks. Of more relevance for this research, however, is that the effects of lengthy and complex surveys are not limited to the precision in responses. Such surveys can also result in non-completion of choice exercises by some participants. This article focusses on the effects of the time-like feature on the respondents’ drop out behavior.====To investigate the time feature in choice data, we juxtapose it with longitudinal data with which it shares some important characteristics. First, responses (choices in choice data) from the same respondent are more likely to be correlated compared to those from other respondents. Second, there is usually a pre-specified number of responses that a respondent is expected to provide by design. Third, while all respondents are expected to provide a complete sequence of responses, it is common that some respondents do not provide responses in some instances or drop out entirely before the end of the study. Sequences for which no choice is observed after the first missing response are said to exhibit monotone missingness. Any respondent so affected is called a dropout (Molenberghs and Verbeke, 2005). Some sequences may also have a missing response preceding or sandwiched between observed choices. These are usually termed as intermittent missing responses and they are said to exhibit non-monotone missingness. The presence of missing responses gives rise to unbalanced data which needs to be incorporated in analyses to provide reliable estimates for decision making (Molenberghs and Verbeke, 2005).====In DCE analyses, interest in missingness has mostly centered on covariates (Qian and Xie, 2011, Sanko et al., 2014, Varotto et al., 2017, Irannezhad et al., 2019).  Qian and Xie (2011) proposed a Bayesian approach to include respondents with missing covariate values in analyses. The Bayesian method samples missing values from their conditional distributions through a data augmentation technique. Missing covariate values have also been modeled as latent variables in hybrid models (Sanko et al., 2014, Varotto et al., 2017, Irannezhad et al., 2019). The latent approach assumes that stated values are subject to measurement errors and attempts to correct the errors while replacing missing values. The twofold correction is theoretically better compared to an imputation approach which assumes that stated values are error-free measures of real values (Sanko et al., 2014, Molenberghs and Verbeke, 2005). We add to these studies by modeling a specific form of missingness in responses.====We restrict our investigation to surveys where missingness in choices is monotone. We view that the drop out patterns provide extra insights on choice making behavior which can enrich available information for estimation of parameters. To explore this, we model the ==== to a drop out event as an additional component to the usual choice behavior. Here, ==== is a borrowed term from survival analysis and it is used to refer to the number of tasks respondents attempt before dropping out. In a typical sense, however, time would refer to the time spent by a respondent in a survey as a whole or the time spent at a choice task to make a choice if such data is recorded. Jointly modeling the choices and the risk of stopping the experiment can be advantageous in correcting biases introduced by the occurrence of the drop out event (Tsiatis and Davidian, 2004, Wu et al., 2012, Proust-Lima et al., 2014). The joint model is also useful when there is interest in investigating if the choice making process and the risk to stop the experiment are governed by the same latent process. This could be the case where a latent factor that leads to a respondent stopping to provide choices also compromises their evaluation of tasks leading to less precise choices. Further, incorporating the drop out model can help in investigating reasons for respondents not finishing the choice experiment. Possible reasons may include lengthy surveys, increasing choice complexity, fatigue and boredom or respondents’ protests to a combination of the mentioned reasons.====Choice and drop out data appear together frequently in surveys. Fig. 1 shows a sequence of choices for some respondents in an online study on university choices. For this study, 14 choice scenarios were presented to participants. Each scenario comprised of three alternatives (A, B and C) and an optout option (O). The figure shows that some participants completed the exercise (e.g., ID 5, 40 and 165 on the y-axis). These are completers. Some other participants did not provide responses in all the choice sets (e.g., IDs 55, 90, 186, 202 etc.). They are the non-completers. We observe that once a missing value occurs, all succeeding choice sets also have missing responses. Therefore, all non-completers in this case were dropouts.====Analyses on choice data similar to that in Fig. 1 often take one of two forms. First, some analyses exclusively focus on the completers and discard all information from dropouts. This means that out of the fifteen sampled profiles in Fig. 1, only five would be analyzed. This approach can have far-reaching consequences because it reduces the sample size, can result in datasets that do not reflect original populations and can be endogenous due to self-selection (Qian and Xie, 2011, Sanko et al., 2014). Results from analyzing completers can also be biased and inefficient if the drop out mechanism is non-random or if the drop out mechanism is random but the proportion of completers is low.====Second, some analyses include all the choices up to when respondents drop out of the exercise. In this case, all the choices from the fifteen profiles in Fig. 1 are included in the analysis. This approach uses a richer dataset and thus mitigates some of the shortcomings of relying on completers. However, it is possible that the responses provided are associated with the risk of dropping out. For instance, respondents could exhibit fatigue effects before dropping out. In such cases, dropouts are likely to provide unreliable responses towards the end of the study or prior to stopping. For example, respondent ID 77 in Fig. 1 selects the optout option in all the last eight choice tasks. Choosing the optout at this rate could imply that the respondent was no longer trading alternatives in these choice sets. This was particularly possible in this survey since a respondent had to make a choice in a task before accessing subsequent tasks. Respondent 15 could similarly be considered as no longer making choices after the twelfth choice task. More dropouts may also be observed around complex choice sets, as the choice complexity builds up or in lengthy surveys. In these cases, analyzing choice data without examining the drop out dimension could bias or lead to inefficient results since it masks the existing behavioral intricacies. Conversely, jointly analyzing preferences and the risk to drop out incorporates all the information at once and provides valid and efficient estimates. Further, no extra effort is needed on the part of the respondents to observe dropouts since they are by-products of the available data. Depending on how extensive the drop out behavior is to be investigated, a researcher may need to record data on time spent by a respondent in the whole survey or at respective choice tasks. This can be done by specifying timestamps at the beginning and at the end of the survey or on every click made at each choice task.====Typically, a joint model combines two or more submodels (or parts). With respect to discrete choice experiments in this article, a joint model will comprise of two submodels: one for the choice data and another for the drop out data. The two submodels are then linked by sharing some random effects or variables. Joint models are commonly estimated using either a two-stage or a unified likelihood maximization approach (Tsiatis and Davidian, 2004, Wu et al., 2012). In a nutshell involving shared random effects, a simple two-stage approach involves first fitting one of the submodels with random effects. Then, the second submodel is fit separately with the shared random effects replaced by their estimates from the first step as if they were observed values. The advantage of the two-stage approach is that it is simple and can be easily estimated using available software. However, this approach can lead to biased estimates since it does not incorporate the uncertainty in the first step into the second step. This can lead to underestimation of standard errors for the submodel estimated in the second step. Further, the approach does not utilize the combined information from the choice and drop out data to provide the most efficient estimates. The unified likelihood maximization approach on the other hand attempts to overcome these shortcomings by simultaneously estimating both submodels. We discuss a standard formulation of a joint model tailored for choice experiments and its estimation in Section 2. We then illustrate and evaluate the method in Section 3 using simulation studies. Section 4 concludes the article with a discussion.",Joint analysis of preferences and drop out data in discrete choice experiments,https://www.sciencedirect.com/science/article/pii/S1755534521000415,7 September 2021,2021,Research Article,36.0
"Sfeir Georges,Abou-Zeid Maya,Rodrigues Filipe,Pereira Francisco Camara,Kaysi Isam","Civil and Environmental Engineering, American University of Beirut, Beirut, Riad el Solh, 1107 2020, Lebanon,DTU Management Engineering, Transport DTU, Technical University of Denmark, Kgs, Lyngby, 2800, Denmark","Received 8 July 2020, Revised 5 July 2021, Accepted 12 August 2021, Available online 23 August 2021, Version of Record 1 September 2021.",https://doi.org/10.1016/j.jocm.2021.100320,Cited by (4), accuracy in addition to better representations of heterogeneity without weakening the behavioral and economic interpretability of the choice models.,"Modeling and forecasting people's choices (e.g. demand for travel modes) are usually done using discrete choice models (DCM), such as the multinomial logit model (MNL) and its variants, which are rooted in the traditional microeconomic theory of consumer behavior. These models assume that each decision-maker associates a utility to each available alternative and selects the alternative with the highest utility. The utility of an alternative is usually specified as a linear-in-the-parameters function of the alternative attributes and socio-economic characteristics of the decision-maker, in addition to a random term that represents the effect of unobserved variables. Over the years, several advanced discrete choice models have been developed to overcome different problems such as the limitations of MNL, taste heterogeneity, and endogeneity, to name a few. However, the representation of unobserved heterogeneity is still a key concern for discrete choice modelers while the family of Mixed logit models remains the predominant approach for dealing with random heterogeneity. The family of Mixed logit consists of two main categories, continuous and discrete distributions. The continuous Mixed logit category assumes continuous distribution(s) with predefined forms (e.g. normal or lognormal) for the random parameters and can approximate any choice situation to a high degree of accuracy (McFadden and Train, 2000). However, such models are constrained by the predefined forms and the choice of a proper distribution which can be a complicated and computationally expensive task (Train, 2016; Vij and Krueger, 2017). On the other hand, latent class formulation (e.g. Latent Class Choice Models) of unobserved random heterogeneity offers an alternative perspective by making fewer statistical assumptions concerning the distributions' forms and eliminating the problematic and time-consuming task of choosing the right parameters' distributions. However, the linear-in-the-parameters utility form of the latent classes and assuming a small number of classes might oversimplify the heterogeneity representation.====This research develops a new discrete choice model that formulates the class membership component of Latent Class Choice Models (LCCM) as a mixture model, a method commonly used as an unsupervised technique in the machine learning community, to allow for more complex/flexible representation of unobserved heterogeneity and consequently for better estimation and prediction of the decision-making process of people when faced with different choice alternatives. LCCM is a random-utility model used to identify behavioral heterogeneity by allocating individuals probabilistically to a set of homogenous latent classes. On the other hand, mixture models are model-based clustering techniques that have been widely used in the last decades in several areas such as machine learning, data mining, pattern recognition, image analysis and several other clustering and classification problems (Viroli and McLachlan, 2019). Using two different mode choice applications, the proposed model is compared to traditional discrete choice models such as LCCM, MNL, and continuous mixed logit models, on the basis of parameter estimates’ signs, values of time, statistical goodness-of-fit measures, and cross-validation tests.====The remainder of this paper is organized as follows. First, we review the literature on discrete choice models and machine learning. Second, we present the mathematical formulation of the proposed hybrid model. Third, we present the two choice case studies. Next, we present and compare the estimation results. Finally, we summarize our findings and discuss future extensions of this work.",Latent class choice model with a flexible class membership component: A mixture model approach,https://www.sciencedirect.com/science/article/pii/S1755534521000531,23 August 2021,2021,Research Article,37.0
"Haghani Milad,Rose John M.,Oppewal Harmen,Lancsar Emily","School of Civil and Environmental Engineering, The University of New South Wales, UNSW Sydney, Australia,Institute of Transport and Logistics Studies, The University of Sydney Business School, The University of Sydney, Australia,Centre for Business Intelligence and Data Analytics, UTS Business School, University of Technology Sydney, Australia,Department of Marketing, Monash Business School, Monash University, Australia,Department of Health Services Research and Policy, Research School of Population Health, Australian National University, Australia","Received 21 December 2020, Revised 12 August 2021, Accepted 13 August 2021, Available online 21 August 2021, Version of Record 11 October 2021.",https://doi.org/10.1016/j.jocm.2021.100322,Cited by (27),"This paper follows the review of empirical evidence on the existence of ==== (HB) in choice experiments (CEs) presented in Part I of this study. It observes how the variation in operational definitions of HB has prohibited consistent measurement of HB in CE. It offers a unifying definition of HB and presents an integrative framework of how HB relates to but is also distinct from ==== (EV), with HB representing one component of the wider concept of EV. The paper further identifies major sources of HB and discusses explanations as well as possible moderating factors of HB. The paper reviews methods of HB mitigation identified in the literature and the empirical evidence of their effectiveness. The review includes both ex-ante and ex-post bias mitigation methods. Ex-ante bias mitigation methods include ====, ====, ====, ====, ====, ====, ====, ====, ====, ==== and ====. Ex-post methods include ====, ====, and ====. It is observed that the mitigation methods and their preferred use vary markedly across different sectors of applied economics. The existing empirical evidence points to the overall effectiveness of mitigation strategies in reducing HB, although there is some variation. The paper further discusses how each mitigation method can counter a certain subset of HB sources. Considering the prevalence of HB in CEs and the effectiveness of bias mitigation methods, it is recommended that implementation of at least one bias mitigation method (or a suitable combination where possible) becomes standard practice in conducting CEs to ensure that inferences and subsequent policy decisions are as much as possible free of HB.","Do responses to hypothetical choice scenarios allow measuring preferences and predicting choices in real-world settings? This question—which is commonly referred to as the problem of ==== (HB)— arguably is the most fundamental question regarding the legitimacy of ==== (CEs) and the usefulness of CEs in policy making and cost-benefit analysis. A particular issue regarding CEs—also referred to as ==== (SCEs), ==== (DCEs), or ==== (CBC) in the literature (Haghani et al., 2021b)—is the estimation of effects that represent measures such as ==== (WTP) or ==== (WTA) and the extent to which the effects in hypothetical settings correspond to their values in real-world settings. If there is no behavioural realism in hypothetical choice data, the use of CEs would seem fruitless, regardless of any methodological advancements in capturing econometric phenomena using sophisticated model structures or the improvements in statistical efficiency of choice surveys. The question represents the broader issue of generalisability which for decades has been debated by social scientists, including experimental economists (Charness and Fehr, 2015; Herbst and Mas, 2015; Levitt and List, 2005, 2007), but is particularly relevant for choice modellers given the nature of CEs.====CE's are typically administered as part of a questionnaire in a lab or in a survey. Lab settings allow maximising experimental control but as Levitt and List (2007) point out, behaviour in the lab can be influenced not just by monetary incentives but by a range of other factors including social and ethical considerations (Kahneman and Knetsch, 1992). Surveys are low-cost and widely applicable instruments to study human preferences and decision-making and are often essential in policy making (Hainmueller et al., 2015). While there are many reasons why behaviour exhibited in a lab or survey may differ from real-world behaviour, their versatility in terms of representing choice settings under highly controlled conditions at low cost have justifiably prompted scholars to try to better understand the nuance surrounding the issue of generalisability beyond lab and survey settings and to identify ways to improve it (Falk and Heckman, 2009).====The ==== of a study concerns the extent to which the study's results represent and apply to the true state of some observed phenomenon. It is often discussed in terms of internal and external validity (Cook and Campbell, 1979). ==== refers to whether observed effects represent causal effects. CEs generally have high internal validity because the analyst can control for many aspects in the data collection including random allocation of participants to conditions. ==== (EV) refers to the generalisability of results beyond the study setting. As will be further defined later, HB is the bias in choice model estimates that results when data are collected in a hypothetical setting instead of in a more realistic setting. HB, as argued in more nuance in the following sections, relates to EV and in fact is one aspect of EV.====With respect to the usefulness of CEs, over the years much emphasis has been placed on enhancing their ====, to obtain more reliable estimates (than, for example, those obtained from conventional orthogonal designs) from a given sample size (Rose and Bliemer, 2009; Rose et al., 2008), as well as avoiding dominant alternatives (Bliemer et al., 2017). Efforts to design efficient CEs, however, will be only meaningful if there is sufficient ==== in the underlying data (Hensher, 2010, 2015). In many situations involving non-market or non-existent goods, there is no plausible alternative for CEs. However, we argue that issues surrounding behavioural realism of CEs should not be viewed as whether analytical advantages of CEs justify the lack of realism or whether there is any better alternative. Rather, actions need to be taken to improve behavioural realism and address such issues during both design and analysis phases of such surveys. It is important to consider practical implications of HB in high-stake CE-based cost-benefit analyses where an uncontrolled bias could amount to the make-or-break of major national projects. Therefore, rather than simply accepting the existence of HB as an inherent feature of CEs and hoping for the best, we argue that it is essential that those who employ CEs have a nuanced appreciation of the HB problem including its likely sources, its likely direction and magnitude and also of ways to effectively counter/minimise bias during the experiment design/administration and/or analysis phases.====For developing this detailed understanding of HB, it is critical to maximally and rigorously test the problem in as many contexts of choice as possible and form a database that informs us about the likely prevalence of the problem and its underlying causes and drivers. Given inherent structural differences of CEs from other forms of SP (e.g., CVs), such inferences should be obtained from the evidence on choice methods rather than borrowing evidence from the CV domain. In a comprehensive synthesis of such empirical evidence in Part I of this study (Haghani et al., 2021a), we established that although there is variability in findings on the existence of HB in CEs, when one considers the entirety of empirical evidence, the role of HB in CEs is undeniable. Here, we focus on the effectiveness of bias mitigation strategies as well as on formulating a nuanced and unifying definition and conceptualisation of HB. We investigate the variation in definitions of HB/EV across existing studies and propose a definition that is consistent across these domains and that can be operationalised. A list of the most relevant explanations and sources of HB in CEs is also presented, along with the moderating factors that could influence the magnitude of HB in CEs. Finally, recognising the variety of terminologies and dimensions of validity (Bishop and Boyle, 2019; Khan, 2011; Kimberlin and Winterstein, 2008; McQuarrie, 2004), we provide a unifying definition of HB for CEs and clarify the distinction between HB and EV by suggesting that HB may be best characterised as a component of the broader notion of EV. We also show how recognition of the sources/explanations of HB can be instrumental in determining best bias-mitigation strategies in a CE application.====In the remainder of this paper, Section 2 presents a conceptualisation of HB and its relation to EV. Section 3 discusses sources/explanations of HB as well as the moderating factors. Section 4 reviews empirical evidence on the effectiveness of HB mitigation methods. Section 5 shows how mitigation strategies can be linked to sources of HB and how by recognising the likely sources, tailored mitigation strategies may be adopted for each CE application to minimise HB. Section 6 discusses the prevalence of employing HB mitigation methods across various domains of applied economics. Section 7 presents a summary and draws final conclusions.","Hypothetical bias in stated choice experiments: Part II. Conceptualisation of external validity, sources and explanations of bias and effectiveness of mitigation methods",https://www.sciencedirect.com/science/article/pii/S1755534521000555,21 August 2021,2021,Research Article,38.0
"Haghani Milad,Rose John M.,Oppewal Harmen,Lancsar Emily","School of Civil and Environmental Engineering, The University of New South Wales, Sydney, Australia,Institute of Transport and Logistics Studies, The University of Sydney Business School, The University of Sydney, Australia,Centre for Business Intelligence and Data Analytics, UTS Business School, University of Technology Sydney, Australia,Department of Marketing, Monash Business School, Monash University, Australia,Department of Health Services Research and Policy, Research School of Population Health, Australian National University, Australia","Received 21 December 2020, Revised 26 July 2021, Accepted 29 July 2021, Available online 1 August 2021, Version of Record 12 October 2021.",https://doi.org/10.1016/j.jocm.2021.100309,Cited by (29),The notion of ,"The debate on the relevance of experimental data in economic valuation dates back at least a century (Ben-Akiva et al., 2019; Fisher, 1893). From its inception, the notion of ==== (SP) (Thurstone, 1931), has sparked debate as to whether consumer experiments should go beyond the mere testing of axioms of economic choice and whether SP data collected outside of real-world markets can be utilised for demand estimation (Ben-Akiva et al., 2019). It was not until the 1960's (Luce, 1965; Luce and Tukey, 1964) that sustained application of SP methods in the form of ratings- and ==== measurement began to emerge in the academic literature (Green, 1974; Green and Srinivasan, 1978; Johnson, 1974; Louviere and Woodworth, 1983). Since then, choice-based SP methods have increasingly become established as a well-founded and preferred approach of studying choice behaviour (Louviere et al., 2000). They are also increasingly preferred over ==== (CV) (Ciriacy-Wantrup, 1947; Davis, 1963), a method that environmental economists originally proposed to determine monetary values of environmental damage/protection (Bergstrom, 1990; Thayer, 1981). ==== (CEs), also known by a variety of other terms, including ==== and ====, represent an indirect approach of inferring preferences. CEs present subjects with hypothetical profiles of multiple alternatives described by their attribute levels. The subject states their preferred option and subsequent analysis reveals the subject's implied utilities of the alternatives and their attribute levels. A subject may be a potential commuter (Devarasetty et al., 2012), food consumer (Alfnes et al., 2006), energy consumer (Mamkhezri et al., 2020), patient (de Bekker-Grob et al., 2019), physician (Kulik and Carlino, 1987), farmer (Espinosa-Goded et al., 2010), air traveller (Hensher and Louviere, 1983), tourist (Oppewal et al., 2015; van Cranenburgh et al., 2014) or pollution/climate conservation tax payer (Liu et al., 2015; Ščasný et al., 2017), to name only a few of the many applications.====CE observations are usually analysed using discrete choice methods founded in the theory of random utility maximization (McFadden, 1981, 1986; McFadden et al., 1986). This provides the analyst an array of econometric measures for placing a value on non-traded goods, market shares of novel products, or price sensitivities and elasticities. The use of CEs also allows the analyst to circumvent the potential issues linked to the analysis of choice observations from real markets, known as ==== (RP) data, namely, ambiguity in the delineation of the choice set and in the attributes of non-chosen alternatives and the absence of adequate attribute variability due to forces of market equilibrium. However, despite its appealing features, the lack of realism inherent to the hypothetical nature of CEs, i.e., the issue of ====, has always loomed over the use of CEs, meaning that their results have been viewed with scepticism (Mitani and Flores, 2014). As such, the potential presence of HB limits the acceptance and use of these methods in economic valuation studies.====Gaining a nuanced understanding of HB in SP survey outcomes requires empirical investigations into the issue. While ample empirical investigations exist with respect to CV surveys, auctions, or referenda (Loomis, 2014; Shogren, 2006; Vossler and Evans, 2009), research in relation to CEs has been much more limited (Ben-Akiva et al., 2019; Hensher, 2010; Lancsar and Swait, 2014; Quaife et al., 2018; Rakotonarivo et al., 2016). This seems partly attributable to CE constituting a younger branch of SP methods, as most of the CE literature has accumulated only in more recent years and within a shorter time than, for example, CV (Ben-Akiva et al., 2019; Janssen et al., 2017; Lancsar and Louviere, 2008; Mandeville et al., 2014; Mühlbacher and Johnson, 2016). The extensive set of empirical evidence on HB in CV experiments has enabled economists to conduct meta-analytical studies, estimate the likely magnitude of HB and better understand what experimental factors and protocols aggravate HB (Carson et al., 1996; List and Gallet, 2001; Little and Berrens, 2004; Little et al., 2012; Murphy et al., 2005; Penn and Hu, 2018, 2019; Schmidt and Bijmolt, 2019). While these meta-analyses adopted different methods of calibration and different sets of variables, their findings suggest that HB can affect the estimates of ==== (WTP) or ==== (WTA) by a factor between 1.2 (Schmidt and Bijmolt, 2019) and 3.13 (Little and Berrens, 2004). A range of factors has been recognised to potentially affect the magnitude of the bias, including the type of good (public or private), the type of design (within- or between-subject), the elicitation method (e.g., choice versus open-ended CV) or the measurement type (WTP versus WTA (Guzman and Kolstad, 2007)====). These meta-studies suggest that indirect elicitation methods, in particular CEs, may be less susceptible to HB than direct hypothetical valuation questions (Murphy et al., 2005; Penn and Hu, 2018). However, like the influence of most of the abovementioned factors, the effect of the choice elicitation format on HB has not been unanimously supported by all meta data. Little et al. (2012) suggest that statistical evidence does not support the argument that CEs are less prone to HB than other typical CV approaches and Schmidt and Bijmolt (2019) find that indirect methods overestimate WTP more than direct methods.====The CE domain offers relatively few empirical investigations of HB because real-world counterparts of hypothetical choice data is often challenging or even impossible to obtain. A CE can include novel alternatives or attribute levels that do not (yet) exist in current markets, hence can represent choice settings for which no RP data may exist as a benchmark for evaluating HB. Studies that have reported such investigations appeared across different disciplines in which SP applications are common (Liebe and Meyerhoff, 2021). In addition, different terminologies have been used to characterise such efforts. Many studies refer to ==== (EV) (Herriges et al., 1999; Krucien et al., 2015; Lancsar and Swait, 2014; Louviere, 1988; Louviere and Timmermans, 1992; Quaife et al., 2018; Rogers and Soopramanien, 2009), while others directly refer to HB (Buckell and Hess, 2019; Hensher, 2010). This has further hindered the acquisition of a holistic understanding of the extent to which HB occurs.====Furthermore, while HB in CV studies could be consistently operationalised—often by directly comparing the hypothetically stated WTP with actual payments in the form of a ratio—such direct comparisons are not possible for CEs, and as such, the operationalisation of HB in CE studies has been variable, using a multitude of estimates and prediction measures that can be inferred from CEs. Depending on the main purpose of a CE, HB may relate to differences in marginal rates of substitution, opt-in rates, total WTP (TWTP), marginal WTP (MWTP), market shares or other measures. Also, cost is not always an attribute in CEs, essentially rendering meaningless the notion of WTP in terms of money (as a focal point for defining/measuring HB). This lack of universality in characterising, measuring and reporting HB in CEs, as well as the lack of clarity about what qualifies as a basis for evaluating HB (e.g., whether to use non-experimental revealed preference (RP) data or incentivised SP (i.e., incentive compatible designs)), in conjunction with the relatively small number of empirical investigations, has so far hindered a comprehensive analysis and synthesis of the evidence on this topic.====To address the lack of an integrative multidisciplinary investigation specific to CEs, we provide a comprehensive detailed review of the empirical findings on HB in CEs in each of the environmental, consumer, health, and transport economic domains, as well as experimental psychology, in a two-part study. The current paper presents Part I, a macro-scale overview of the literature of CE. It focuses in particular on the ==== of HB in CEs. In addition to empirical evidence from various sectors of applied economics, it also visits the fields of psychology and cognitive neuroscience for insights regarding HB in CEs. We discuss how recently emerged evidence from experimental psychology and brain imaging studies can help delineate HB as a phenomenon and help explain some of the unexpected or counterintuitive findings in the CE literature.====In the current paper, we initially only use a broad definition of HB given that there is no universal agreement on its definition. However, based on our findings, and in Part II of this study, we provide an exact and inclusive definition, as one of the outputs (not inputs) of our study. Therefore, following the current study, the second paper (Part II, Haghani et al., 2021) is dedicated to providing a more nuanced conceptualization of EV and HB in CEs and offering a unified definition. Sources and explanations of HB as well as the moderating factors are also discussed. The second paper also focuses on mitigation strategies of HB in CEs. A comprehensive catalogue of such methods is devised and empirical evidence on their effectiveness are assembled and reviewed. It also shows how establishing links between likely sources of HB in individual CE application may help identify effective and customised ways of mitigating potential HB.====The outline of the present paper is as follows. In Section 2, we describe the search strategies and resulting datasets. Section 3 provides a macro-scale overview of the CE literature based on bibliometric indicators of CE studies. Sections 4 Empirical evidence on hypothetical bias in choice experiments, 5 Empirical evidence on hypothetical bias from psychology and neuroimaging review empirical evidence on HB in CEs across various disciplines of applied economics and psychology and neuroimaging studies, respectively. Section 6 presents a summary and discussion and final conclusions.","Hypothetical bias in stated choice experiments: Part I. Macro-scale analysis of literature and integrative synthesis of empirical evidence from applied economics, experimental psychology and neuroimaging",https://www.sciencedirect.com/science/article/pii/S1755534521000427,1 August 2021,2021,Research Article,39.0
"Ozonder Gozde,Miller Eric J.","Department of Civil and Mineral Engineering, University of Toronto, Toronto, M5S 1A4, Canada","Received 21 July 2020, Revised 17 March 2021, Accepted 7 July 2021, Available online 15 July 2021, Version of Record 22 July 2021.",https://doi.org/10.1016/j.jocm.2021.100306,Cited by (1),"This study proposes a joint modelling approach to simulate episode timing decisions of skeletal activities -- activities that typically take precedence in the scheduling process -- using ====. The analyses focus on examining the correlation between activity start time and duration while exploring the timing behaviour over a 20-year period between 1996 and 2016 in the Greater Toronto and Hamilton Area. For three types of agent classes (“Worker”, “Student”, “Both”), timing decisions associated with three skeletal activities are investigated: “Work”, “Work Business” and “School”. To obtain mutually exclusive homogeneous subsamples, further segmentation of agents is undertaken considering employment and/or student status, and occupation of the individuals. Parametric models are fitted independently to marginal start time and duration distributions using finite mixture models before estimating copula models for their joint distributions. In the “Worker” class, considerable stability is observed in “Work” activity timing models in all occupation sectors for full-time out-of-home employees, except for the “retail sales/service” sector. “Student” class agents’ “School” timing behaviour is also found to be stable in general for full-time students. More noise is observed in the timing behaviour of “Both” class agents. Given the correlation levels observed, it is concluded that start time and duration variables should be modelled jointly, not independently, within an agent-based, activity-based travel demand modelling framework. This paper demonstrates that it is possible to capture complex behaviour in timing decisions through parsimonious models that only utilize a small number of parameters and recommends exploiting behavioural information accrued over time while developing models.","As indicated in the above aphorism ascribed to Heraclitus, change is part of everyday life. Changes in social structures, economy, technology, and built environment are undeniable. These changes ultimately affect urban activity and transportation systems, and their interactions in terms of land development, location choice, activity schedules and patterns and the corresponding travel demand, network flows, etc. (Manheim, 1979; Meyer and Miller, 2001). The world's population, and hence population density, is increasing; existing infrastructure is expanding; the means of transportation are evolving with new concepts and technologies such as the “mobility as a service” (MaaS) concept (e.g., ridehailing, sharing cars, bicycles, electric scooters, etc.), electric vehicles, and autonomous vehicles. By the same token, one might anticipate changes in activity patterns. But does the underlying “behaviour”, the fundamental principle of activity participation (which triggers travel), change markedly? Considering the theory of needs (Maslow, 1943), individuals have wants/needs such as physiological needs, safety, love, esteem, etc., and the desire to fulfil these needs motivates their behaviour. That is why they go out and participate in activities; that is why they travel. How much they travel inevitably depends on income, the availability of establishments that address individuals' needs, ease and affordability of travel, etc. Yet, they still go out, socialize, and interact. Hence, perhaps, it is the conditions in which decision-making processes take place that are changing notably. Nowadays, it is possible to go to work with Uber or with a shared electric scooter, instead of driving your private vehicle or taking the bus. But an employee still has to go to work under “regular”/“normal” circumstances. Thus, it is probable that the new alternatives are complements to the previously available options, rather than being substitutes -- online gatherings are possible, but individuals still have face-to-face meetings; people watch Netflix, but still go to the movies. Evidently, the conceptual arguments presented herein are not based on extreme conditions such as the current global pandemic. Activity and travel patterns have exhibited significant changes during the pandemic (e.g., De Vos, 2020; Gao et al., 2020; Müller et al., 2020; Palm et al., 2020; Park, 2020). Due to the restrictions and precautions taken to prevent the spread of the Coronavirus disease, those who can started working from home, education continued through virtual platforms, individuals started shopping online more often, attending social gatherings online, exercising at home instead of going to the gym, etc. Residual impacts of the pandemic are inevitable considering these reshaped/evolved versions of activity and travel patterns. Yet, there is considerable uncertainty associated with the “recovery period” and whether work, school, shopping, leisure, etc. activity participation styles will return to pre-pandemic patterns when the effects of COVID-19 have subsided remains to be seen.====From a modelling standpoint, changes in activity/travel patterns over time can be investigated through agent-based, activity-based microsimulation models focussing on their most fundamental component: “activity generation”. Within an activity generation component, in addition to modelling the activity participation decisions -- i.e., modelling activity frequencies, which answers the “how many times does the individual engage in a given activity type?” or the “does the individual engage in a given type of activity?” question -- a few other attributes of the activity episodes are modelled. These attributes generally include==== the ==== of the activity (answering the “when does the activity begin?” question), the ==== of the activity (answering the “how long does the activity take?” question), the ==== of the activity (answering the “where does the activity take place” question), and the ==== of the activity (answering the “with whom the activity is conducted” question). An ==== is fully defined only when all these attributes are determined, where an ==== can be described as a single instance of participation in a given type of activity (Miller, 2005). Hence, an activity generation component embodies decision-making structures for these episode attributes. Activity generation plays a significant role in an activity-based modelling framework since the overall activity/travel levels are determined based on the outputs of this component, which are then utilized for answering various transportation-, land use- and environment-related research/policy questions that might capture the impacts of changes in lifestyles, mobility, technology, etc.====In this paper, we focus on the skeletal activity episodes, i.e., the episodes of work-related and school-related activities. Activity frequency generation is studied in Ozonder and Miller (2020; 2021), which explore the changes/stabilities in activity participation within a longitudinal analysis framework. For skeletal activities, we assume that the activity location is “known/given”,==== hence, this attribute is not modelled herein. This follows from the individuals' work and school locations' usually being considered as a relatively long-term decision, not a daily decision to be made/modelled (Arentze and Timmermans, 2004; Miller, 2018; Miller and Roorda, 2003). Further, considering the ==== attribute, skeletal activities are assumed to be individual-based, not household-based, i.e., they are not joint activities. Thus, we investigate the remaining attributes of an activity episode in this paper, the two activity episode timing decisions: start time and duration, which are essential for scheduling processes that usually come after activity generation within activity-based models, because a generated episode cannot be added to an individual's agenda or schedule without checking for and resolving potential conflicts. This can only be done after the aforementioned episode attributes are modelled/set.====We propose modelling activity start time and duration, both of which are likely to be dominantly determined/shaped by the job/education requirements for the skeletal activities, jointly using a simultaneous modelling approach. This research focusses on examining the dependence between the timing decisions of skeletal activities while analyzing the behaviour over a 20-year period between 1996 and 2016, where applicable,==== in the Greater Toronto and Hamilton Area (GTHA). Following the agent class and activity type structure in Ozonder and Miller (2021), the analyses study the behaviour of three agent classes, “Worker”, “Student” and “Both” considering three activity types: “Work”, “Work Business” and “School”. Additional segmentation is based on the employment/student status of the individuals: part-time versus full-time, due to the anticipation of variance in behaviour. For agent classes “Worker” and “Both”, further partitioning is undertaken taking into account agents’ occupation sectors and their usual workplace. To identify and conserve the correlation structure between the two decisions, copula models are adopted in this research where decisions are modelled through statistical parametric models that are purely functions of the underlying distributions, i.e., the models do not require any sociodemographic, or other, attributes of decision-makers as inputs. Models are designed to reflect the conditionality in hierarchical or simultaneous activity generation structures established in Ozonder and Miller (2021), which is explained in detail in the following sections of the paper.====Within the core of our regional operational agent-based activity-based travel demand model GTAModel V4.1 (Travel Modelling Group, 2019) lies TASHA (Travel/Activity Scheduler for Household Agents). TASHA is responsible for generating and scheduling out-of-home activities along with conducting tour-based mode choice analysis while handling intra-household interrelationships such as utility-based vehicle allocation, within-household ridesharing, serving dependents (e.g., taking children to/from daycare) (Miller, 2007; Miller and Roorda, 2003; Miller et al., 2005; Roorda, 2005). In the current version of TASHA, start time and duration values are drawn from joint probability distributions observed in the most recent cross-sectional travel survey data set using Monte Carlo simulation. One disadvantage of this approach is not benefiting from the full information collected over time (travel surveys in the GTHA are conducted every five years). Another disadvantage is overlooking the direct correlation between the choices. Moreover, two computational disadvantages of this method are adding to the amount of data needed to be stored and the requirement of building conditional distributions after data are read into the model. Thus, one additional purpose of this study is to replace the aforementioned structure with the parametric models estimated in this study which maintain the correlation patterns inherent in data to improve the dependency structure embedded in the operational model while modelling timing decisions.====The rest of the paper is organized as follows. An overview of literature on activity start time and duration modelling is provided in the next section. Section 3 presents the data used in this study. Methods are described in Section 4. The results of the study are detailed in Section 5. Finally, Section 6 concludes the paper.",Longitudinal investigation of skeletal activity episode timing decisions – A copula approach,https://www.sciencedirect.com/science/article/pii/S1755534521000397,15 July 2021,2021,Research Article,40.0
"Webb Edward J.D.,Hess Stephane","Academic Unit of Health Economics, Leeds Institute of Health Sciences, University of Leeds, UK,Choice Modelling Centre, University of Leeds, UK","Received 7 August 2020, Revised 18 June 2021, Accepted 28 June 2021, Available online 3 July 2021, Version of Record 10 July 2021.",https://doi.org/10.1016/j.jocm.2021.100304,Cited by (1),"In many cases, ordinal data, for example rating objects on a scale from 1 to 5, is observed only for those objects that have been chosen from a set of discrete alternatives, with no ratings for unchosen objects. An example is customer ratings of goods sold by online retailers. The joint modelling of choice and rating is made difficult by the missing ratings for unchosen alternatives. A method of jointly modelling choice and rating data termed a choice-ordered logit (COL) model is presented. Two types of COL model are defined: two-step, which places a positive probability on the chosen alternative not having the highest rating, and one-step, where the highest rated alternative is always chosen. Three case studies exemplifying the use of COL models are given. One uses simulated data and two use data from discrete choice experiments. It is shown that COL models can produce robust estimates. Two-step models provided a better fit than one-step, and most participants seemed to use two-step decision-making. However, a sizeable minority used one-step decision-making in one case study. It is argued that COL models have benefits over standard approaches, in particular adding information on strength-of-preference to discrete choices.","There are many sources of data on individuals rating objects on an ordinal scale. Often though, rating occurs only after individuals have made a discrete choice between several alternatives. This means that ratings of unchosen alternatives are not observed. An example of this arises in the case of retail websites such as Amazon, where ratings of between one and five stars are only observed for the goods consumers chose to purchase. Other non-retail websites allow ratings of goods purchased elsewhere, such as IMDB for films or Yelp for restaurants and other services. Here, again, cinema goers must first choose what film they want to see, and diners must choose which restaurant to eat at, and their ratings for non-chosen alternatives are not observed. Another example is that of extending stated choice (SC) surveys otherwise known as discrete choice experiments (DCEs), so participants additionally rate their preferred option after each choice.====When decision-makers only rate objects that were previously chosen, the full decision-making process is a joint one. Thus ordered logit (OL) models, which use only rating data, do not fully model the decision-making environment, nor do they make use of the fact that individuals clearly like the objects they are rating enough to select them over others. Likewise, modelling only discrete choices neglects additional information on individuals’ preferences provided by the ratings.====This paper introduces a framework for jointly modelling both discrete choices and ratings, which we refer to as choice-ordered logit (COL). It thus captures both components of the decision-making process in a single model, which it is hoped will aid a greater understanding of the whole decision-making environment, both theoretically and empirically. At present, the modelling framework is mostly applicable to stated preference data, although there is a discussion of how it might be extended to revealed preference data in section 4.====This work draws on an extensive literature on modelling both discrete choices and ordered data using both revealed and stated preferences (see Train (2009) and Hess and Daly (2014) for a partial overview). However, we are not aware of any previous studies which combine choices and ratings in the way proposed here. For example, while some studies have examined how and why individuals leave online ratings (e.g. Moe and Schweidel, 2012) or how consumers react to them (e.g. Sun, 2012), these studies do not explicitly consider the prior discrete choice.====There are several studies in which a DCE is expanded with a Likert scale question, for example Regier, Watson, Burnett, and Ungar (2014) in health, Beck et al. (2016) in transport, and Mattmann et al. (2019) in environmental economics. Yet the Likert scales in these cases measure the uncertainty respondents have over their decisions. Thus they measure something fundamentally different than the strength of preferences for the choice objects, meaning approaches to jointly modelling this data are also conceptually different to the approach detailed here. Here, it is assumed that both discrete choices and ratings are reflective of intrinsic preferences for choice objects.====Gutknecht et al. (2018) and Wijnen et al. (2015) conduct DCEs while also measuring how important each attribute was using Likert scales. However, in neither study was the data from both exercises modelled jointly, instead the results of separate models were compared.====Some similarities to the current approach are seen in Rose et al. (2015), Beck et al. (2013) and Hensher and Rose (2012). In the survey in those studies, DCE participants were asked whether each item was acceptable or not, which could be thought of as a binary rating which reflects their preferences for choice objects. However, in that case the additional question was asked for each choice object, whereas the focus here is on situations in which only ratings for the chosen object are observed. The additional data was also used to estimate participants’ consideration sets, whereas this heuristic is not modelled here.====The remainder of this paper proceeds as follows: Section 2 gives the mathematical framework for the model approach. Section 3 then gives three case studies of applying joint choice-ordered logit models. The first uses simulated data, and the other two use data from discrete choice experiment (DCE) surveys. Section 4 provides a general discussion of the results from each case study, and section 5 concludes.",Joint modelling of choice and rating data: Theory and examples,https://www.sciencedirect.com/science/article/pii/S1755534521000373,3 July 2021,2021,Research Article,41.0
"Haghani Milad,Hensher David A.","Institute of Transport and Logistics Studies, The University of Sydney Business School, The University of Sydney, Australia,School of Civil and Environmental Engineering, The University of New South Wales, Sydney, Australia","Received 5 January 2021, Revised 15 April 2021, Accepted 18 June 2021, Available online 24 June 2021, Version of Record 3 July 2021.",https://doi.org/10.1016/j.jocm.2021.100303,Cited by (21)," ==== are steadily on the rise at a rate comparable to that of health. Activities in the methodological cluster of this field have rather notably slowed down during the recent years although not extinct. Also, despite slowing down of choice modelling applications in transportation compared to the previous decades, such applications have not disappeared from the transportation sector. A particular area of transportation research where applications of choice modelling methods are still notably trending is electric and automated mobility. Pioneering studies, most influential studies and various streams of choice modelling research along with their time of emergence and duration of trendiness are also objectively determined using a document co-citation analysis. Further analyses are also conducted on patterns of collaboration in this field. These outcomes document the history of development of choice modelling literature at a macro scale and provide a holistic understanding of various divisions of this field along with its influential entities.","It is nearly 100 years since Thurstone's classic 1927 paper on ==== (Thurstone, 1927), in which he assumed the response of an individual to a pair of alternatives, ====, ====, is determined by the discriminal processes ==== and ====. The terms ==== and ==== represent a single-valued function of unknown parameters ==== and ====, characteristics of the ‘objects participating in the ==== pair’ and were referred to by Thurstone as ‘affective values’ of the corresponding objects (or alternatives); with ==== and ==== defined as the elements of the discriminal processes specific to the randomly selected individual, assumed by Thurstone to obey a normal bivariate distribution function. The difference process ==== is distributed normally with mean ==== and variance ==== where ==== is the correlation between the alternatives. The individual was assumed to judge alternative ==== preferable to alternative ==== when ====. Thus, the probability that a randomly sampled individual will be observed to judge ==== is ====. This response function was referred to as a statement of the law of comparative judgment. McFadden (2001a) described the Thurstone contribution as a model of imperfect discrimination in which alternative ==== with true stimulus level ==== is perceived with a normal error as ==== and Thurstone showed that the probability ==== that alternative ==== is chosen over alternative ==== has a form that we now call binomial probit. The emphasis on probabilistic choice theory can be credited to both Thurstone (1945) and a lesser known author, Hull (1943).====Luce (1959) proposed a model for the probability that ==== is ranked above ==== in the pair ====, as ====, ====. ==== and ==== are positive parameters characteristic of alternatives ==== and ====. Luce (1959) developed the theoretical foundations in a precise form, in which ==== can be interpreted as the probability that ==== will be ranked first among all ==== alternatives. The probability that ==== will be ranked first in any subset of alternatives, and in particular in the subset ==== in any subset of alternatives follows from Luce's principles of ====, which states that the ratio is constant regardless of what other alternatives are in the subset. This became known as the IIA rule or constant.====There were a number of developments that refined the Luce Model through the 1950s and early 1960s, but it was Daniel McFadden in the late 1960s who developed an empirical model from Luce's choice axiom (centred on IIA as described above) that had formal links with economic theory. Letting ==== denote the probability that a subject confronted with a set of mutually exclusive and exhaustive alternatives ==== will choose alternative ====, given the IIA property, Luce showed that if his axiom holds, then one can associate with each alternative a positive “strict utility” ==== such that ====. Taking the strict utility for alternative ==== to be a parametric exponential function of its attributes ====, ==== gave a practical statistical model for individual choice data. McFadden called this the ==== because it reduced to a logistic in the two-alternative case, and had a ratio form analogous to the form for conditional probabilities (McFadden, 1968, 1973).====The development of interest in discrete choice models as a theoretically sound and practical tool to explore choice behaviour and especially behavioural outputs such as the willingness to pay, developed at a fast pace, initially in the transportation context which was the application setting of McFadden's early contributions. Hensher et al. (2015) provide an historical overview of the contributions through the 1970s and up to the early 1990s. Since then we have seen a burgeoning of research in many areas of the choice literature, including behaviourally more realistic discrete choice model forms (e.g., closed form models such as nested logit, and open forms such as mixed logit, latent class, generalized mixed logit), new data paradigms, including stated preference and choice experiments and combined data methods, and accommodating process heuristics into choice models such as attribute non-attendance, and accounting for endogeneity of attributes through hybrid logit.====The current work analyses and synthesises these developments over the last fifty years of research in the field of discrete choice modelling through the lenses of bibliometric indicators of its underlying articles. The objectives of the study are multi-fold and can be outlined as follows: (i) to estimate the size of the scholarly literature in this field and identify its main components/divisions, (ii) to identify current and past temporal trends in this field of research, (iii) to determine the most influential studies of the field, in general, as well as those within various streams of this research, based on a multitude of indicators. It is expected that such analyses provide a fresh level of insight into the structural composition of this field as well as the history of its development.",The landscape of econometric discrete choice modelling research,https://www.sciencedirect.com/science/article/pii/S1755534521000361,24 June 2021,2021,Research Article,42.0
"Coote Leonard V.,Adamowicz Wiktor","UQ Business School the University of QueenslandBrisbane QLD, 4072, Australia,Erasmus Choice Modelling Centre and Erasmus School of Health Policy & Management Erasmus University Rotterdam Burgemeester Oudlaan, 50 3062, PA, Rotterdam, the Netherlands,Distinguished University Professor Department of Resource Economics and Environmental Sociology Faculty of Agricultural, Life and Environmental Sciences the University of Alberta Edmonton, Alberta, T6G 2H1, Canada","Received 21 April 2020, Revised 29 April 2021, Accepted 10 June 2021, Available online 18 June 2021, Version of Record 30 June 2021.",https://doi.org/10.1016/j.jocm.2021.100302,Cited by (2),"Preference heterogeneity is one of the central behavioral concepts in applied ====. Its centrality is particularly evident in the choice modeling literature, notably in its widespread application to environmental and health economics, marketing, and transport. Despite conceptual and empirical advances in modeling preference heterogeneity, the generalizability of preference heterogeneity to different decision contexts and different data generation processes remains an open question. The basic premise of this paper is that latent sources of preference heterogeneity can be decomposed into components general to decision contexts and others specific to them. We study the structure of preference heterogeneity in different data generation processes with the goal of reliably identifying common (presumably generalizable) and specific (presumably not generalizable) sources of preference heterogeneity. The contribution of the paper is both conceptual and methodological, leading to the testing of five rival model specifications which together elucidate the heterogeneity structure present in two preference data sources of the same choice behavior. In the empirical application, we find that the multitrait-multimethod model of preference heterogeneity has the best fit and most sensible interpretations, indicating that while each data source contributes uniquely to certain heterogeneity components, both data sources contribute also to common (generalizable) preference heterogeneity. Recognition of the separability of the common versus source-specific preference heterogeneity will lead to more reliable and accurate demand model forecasts and assessments of welfare impacts.","Preference heterogeneity is one of the central concepts in applied econometrics. The centrality of preference heterogeneity is particularly evident in the literature on choice modeling and its applications to environmental and health economics, as well as in marketing and transport. Advanced by McFadden (1974), conditional (multinomial) logit was the first tractable choice model for studying the preferences of samples (populations) of decision makers. It is widely applied today and estimates preference parameters (marginal utilities) that inform policymaking, yet first generation models did not incorporate preference heterogeneity except though systematic sources (e.g., socio-demographics and contextual). Second-generation models including McFadden and Train's mixed logit (McFadden and Train 2000) extend first generation models by incorporating latent sources of preference heterogeneity. The random coefficient specification of mixed logit, for example, estimates preference mean and variance parameters with the variance parameters representing unobserved (latent) sources of preference heterogeneity. With its focus on the correlation structure of unobserved sources of heterogeneity, mixed logit can approximate any choice model (Hess and Train 2017) and therefore represents a significant development in the econometrics of choice modeling. Third generation choice models such as the latent class specification of conditional logit and recent extensions of the latent class framework (see, e.g., Greene and Hensher 2003; Kamakura and Russell 1989; Swait et al. 2018) can give a particular structure to the unobserved sources of preference heterogeneity. Modeling the structure of the heterogeneity has the potential advantages of improving model interpretations and model fits but requires researchers to extend a theory-testing mindset to specifying and testing the patterns of heterogeneity.====Despite both conceptual and empirical advances in modeling preference heterogeneity, the generalization of preference heterogeneity to different decision contexts and different data generation processes remains an open question. This paper addresses the issue by applying a factor-analytic choice model to study the generalizability of preference heterogeneity across different data generation processes. The model form used is the so-called “structural choice model,” a very general and flexible form of mixed logit (Rungie et al. 2011). The model is specifically designed to incorporate latent variables representing (or equivalently, giving structure to) unobserved sources of preference heterogeneity. These latent variables jointly specify the structure of utility weights (preferences) and are separate from any latent variables such as commonly included in hybrid choice models (see, e.g., Ashok et al. 2002; Bhat and Dubey 2014; Walker and Ben-Akiva 2002). The researcher gives structure to the preference heterogeneity by specifying a set of simultaneous equations systematizing the interrelationships among the latent variables. The preference parameters and the parameters of the structural equations are estimated simultaneously. Importantly, the latent variables can be specific to the data generation processes (dgp's) studied or general across multiple dgp's. The application of structural choice modeling illustrated in this paper is based on a classic recreational hunting site choice study (Adamowicz et al. 1997). For a complete hunting season, recreational (moose) hunters in Canada kept a log of their actual hunting trips and completed a stated preference hunting site choice experiment. The model specifications advanced in the present paper represent rival explanations (theories) of the preference heterogeneity and the behavioral process(es) they imply for describing hunters' choices in the field (RP – revealed preferences) and in the experiment (SP – stated preferences).====The contribution of the paper is both conceptual and methodological. Conceptual because it is, to our best knowledge, the first research to suggest that fusion of preference data (see Hensher et al., 1999 and von Haefen et al., 2008 for reviews) should explicitly reflect that inferences about preference heterogeneity be built on the recognition that choice data may reflect common sources (e.g., cost-driven tradeoffs in multiple dgp's) and unique sources (e.g., actual expenditures in one dgp, hypothetical expenditures in another dgp) of differentiation. Other examples may also arise. For example, in SP there may be elements outside the current RP attribute space. Heterogeneity here may not generalise to RP or common preferences. This distinction is important because the analysis goal is to improve real-world policy with improved estimates of behavioral response and welfare impacts. We suggest that this goal will be better achieved by joining the preference data sources in ways that recognize and differentiate between those dgp features that are general and those that are specific. In this way, we may be able to make forecasts and welfare analyses that are not dependent on data source-specific idiosyncrasies. To illustrate, the hypothetical nature of SP tasks, particularly arising from the weakening or even delinking of the behavior from the operative income constraint in the RP context, raises the question of whether the price response measured in the SP task is transferable to the RP context. Or, as another example, does the private goods task used to elicit SP choices suffer from strategic behavior, such as price or provision bias (Johnston et al., 2017), thus resulting in a different structure of price response than RP data? It is to this endeavor that we focus our efforts, and we suggest that it is through the separation of common and source-specific preference heterogeneity that we can improve policy recommendations.====Second, our contribution is methodological because we offer a specific modeling approach to address the question about common and source-specific preference heterogeneity. As we will subsequently elucidate, we suggest the use of structural choice models (SCMs), an extension to the basic mixed logit framework, to clarify the structure of preference heterogeneity across data sources. In its most general form examined here, we arrive at a multitrait-multimethod specification, which stipulates sources of preference heterogeneity (latent variables) that are general to the dgp's and unique to them. Further, the latent variables general to the dgp's are regressed on a higher-order factor. Thus, the structure of the preference heterogeneity is retrieved independent of the dgp's. The other four models examined represent plausible behavioral restrictions to the common and unique structure of preference heterogeneity; these restricted models are rejected in the empirical application of this paper but need not have been.====The multitrait-multimethod model has the best fit and clear interpretations about common versus source-specific heterogeneity. This important result has broad implications and suggests a theory of the structure of individual differences. Firstly, the finding establishes that unobserved sources of preference heterogeneity can be general to dgp's and specific to them. Second, it establishes that the structure of the preference heterogeneity can be represented independent of the dgp's. To be more specific, there is firstly evidence that some sources of preference heterogeneity are general to the revealed and stated preferences – notably, variation in preferences for travel costs, the nature of the hunting trails, and logging activity. Hunters sensitive to variation in travel cost in the field were also sensitive to travel cost in the experiment, whereas hunters sensitive to logging activity in the field were not so in the experiment (and ====). This may reflect limited variability of logging in the field. Second, there is evidence that preference heterogeneity can be specific to the dgp's after controlling for the common sources of preference heterogeneity. Hunters in the field differed greatly in their sensitivity to travel costs, the quality of roads, and the presence of other hunters. In the experiment, hunters were more sensitive to access to four-wheel drive trails than to road quality, and were more alike in their response to the presence of other hunters. Independent of the dgp, the structure of the heterogeneity is defined by variation in preferences for travel costs and the number of moose encountered on a hunt, with hunters sensitive to travel costs placing much less emphasis on encountering three or more moose (and ====). Thus, these results give structure to a theory of individual differences in hunters' behavior and preferences.====The paper is organized as follows. The next section provides a brief overview of relevant literature with an initial focus on data fusion. The following section presents the general form of SCM and the econometric specifications of the five models of the model catalogue. The case study, empirical results, and a policy simulation follow. Implications for choice modelers and policymakers conclude the paper with an emphasis on potential applications in which it may be useful to establish the generality of preference heterogeneity across dgp's.",Separating generalizable from source-specific preference heterogeneity in the fusion of revealed and stated preferences,https://www.sciencedirect.com/science/article/pii/S175553452100035X,18 June 2021,2021,Research Article,43.0
Sarrias Mauricio,"Facultad de Economía y Negocios, Universidad de Talca, Talca, Chile","Received 11 June 2020, Revised 23 March 2021, Accepted 6 June 2021, Available online 16 June 2021, Version of Record 18 June 2021.",https://doi.org/10.1016/j.jocm.2021.100301,Cited by (2),". Concretely, it is assumed that there exists an endogenous and continuous variable defined as a predictor, while unobserved heterogeneity is conceptualized as a vector of parameters that varies across individuals following a discrete distribution. A Maximum Likelihood Estimator is provided to estimate the model parameters based on normally distributed random terms and a free code in R software is provided to carry out the estimation procedure. A small Monte Carlo experiment is carried out to analyze the properties of the estimator. Finally, the estimator is applied to analyze the heterogeneous effects of weight on mental well-being.","The Latent Class (LC) model or Mixture Model has been an important econometric tool for disentangling unobserved heterogeneity in the sample. Under this approach, the coefficients for each explanatory variable are allowed to vary across the sample assuming that there exist segments or classes of individuals that have different parameters, but within each class the coefficients are homogeneous.==== Disentangling the coefficients for different segments of individuals in the sample not only adds more realism to the modeling approach by detecting important features and insights about heterogeneous relationships, but also increases the explanatory power and reduces the bias of the estimated coefficients (Hess, 2014) if the true data generating process incorporates unobserved heterogeneity. Given this flexibility, LC models have been applied for modeling unobserved heterogeneity on the determinants of healthcare demand (Deb and Trivedi, 2002, Atella et al., 2004, d’Uva and Jones, 2009), subjective well-being measures (Clark et al., 2005, Palomino and Sarrias, 2019) travel demand (Gopimatj, 1997, Greene and Hensher, 2003), R&D and patent (Wang et al., 1998), the effect of direct marketing on purchases (Wedel and DeSarbo, 1995), and criminology (Nagin and Land, 1993) to mention a few.====Although modeling unobserved heterogeneity using a LC approach is an expanding field both in theoretical and applied grounds, another important empirical concern is the potential endogeneity due to omitted variables, measurement error, simultaneity, and/or self-selection: if the error term is correlated with the explanatory variables (or the treatment), then the coefficients representing the degree of heterogeneity across classes will be inconsistent even using large samples. In such a case, the instrumental variables method can be a solution to eliminate the potential endogeneity of the treatment variable, either continuous or discrete. For example, some researchers have already proposed models that incorporate both econometric issues for linear models. Heckman et al. (2006) examine the properties of instrumental variables applied to models with essential heterogeneity, that is, when the treatment effect varies with the treatment due to unobserved confounders. In this scenario, individuals might respond differently to the same treatment yielding substantial differences in the average treatment effect. Similarly, Florens et al. (2008) propose a control function approach to identify the average treatment effect and the effect of the treatment on the treated in models with a continuous endogenous regressor whose impact is heterogeneous, whereas Moffitt (2008) proposes a nonparametric method of estimating marginal treatment effects in heterogeneous populations when the treatment is a dummy variable by assuming that the outcomes are a nonlinear function of participation probabilities.====More in line with this article, Bhat et al. (2014) provide a methodological innovation that allows to control for potential endogenous effects using instrumental variables techniques (for different type of endogenous variables, such as continuous and count variables) and parametric distribution for multi-dimensional choice systems using a Maximum Approximate Composite Marginal Likelihood approach for the estimation of the parameters (Bhat, 2011). In particular, they applied this joint model to analyze household-level decision on residential location, motorized vehicle ownership, and activity-travel patterns and found that these choice dimensions are inter-related, both through direct observed structural relationships and through correlation across the error term, and that the endogeneity coming from self-selection might overestimate the treatment effects.====This article proposes a fully parametric method to deal with both unobserved heterogeneity at the individual level (represented by different coefficients for each individuals) and the potential endogeneity of a continuous variable in models where the dependent variable is dichotomous. Specifically, an instrumental variable latent class approach for the binary Probit (IVLC-Probit) model is derived by incorporating the characteristics of the traditional recursive two-equation models proposed by Heckman, 1978, Amemiya, 1978, Amemiya, 1979 and Rivers and Vuong (1988) to the LC-Probit model (see Greene, 2004 for a review of the LC-Probit model).==== It is assumed that there exists an endogenous and continuous regressor defined as a predictor, which is partly determined by predetermined factors and instrumental variables, while unobserved heterogeneity is conceptualized as a vector of parameters that varies across individuals following a discrete distribution. Thus, individual heterogeneity is accommodated by making use of a discrete and fixed number, say ====, of separate classes (support points) with different coefficients in each class, while the degree of endogeneity might vary across classes. The proposed IVLC-Probit model extends the traditional IV-Probit by including unobserved heterogeneity using latent classes, and the LC-Probit by including the possibility that a continuous variable is endogenous in some or all classes. Therefore, the treatment, the strength of instrument and the degree of endogeneity are allowed to vary across groups of individuals. A conditional Maximum Likelihood Estimator (MLE) is provided to estimate the model parameters based on normally distributed random terms and a free code in R software is provided in Annex A to carry out the estimation procedure.==== To assess the properties of the proposed estimator under small and large sample a small Monte Carlo experiment is performed. Finally, the advantages and disadvantages of the proposed method are analyzed through an empirical example where the potential heterogeneous effect of weight on mental well-being is analyzed.====The remainder of the paper is organized as follows. Section 2 presents the IVLC-Probit model along with the assumptions and Maximum Likelihood Estimator. Some issues regarding estimation, testing and post-estimation measures are provided in Section 3. Section 4 shows the results for the Monte Carlo experiment, while Section 5 provides the empirical application. Finally, Section 6 concludes.",A two recursive equation model to correct for endogeneity in latent class binary probit models,https://www.sciencedirect.com/science/article/pii/S1755534521000348,16 June 2021,2021,Research Article,44.0
"Souza Flavio,Duff Gordon","Afterpay, Australia,National Disability Services, Australia,Erasmus University Rotterdam, Netherlands","Received 4 May 2020, Revised 15 May 2021, Accepted 27 May 2021, Available online 4 June 2021, Version of Record 12 June 2021.",https://doi.org/10.1016/j.jocm.2021.100300,Cited by (1),"The National Disability Insurance Scheme (NDIS) is an ambitious Australian ====, with the goal of supporting 460,000 people living with a disability. To promote individual choices, the NDIS allocates personalised budgets based on the goals of Participants (or their nominated Carers). This paper examines (a) the heterogeneity of personal preferences for support services in the disability sector, (b) whether the program is subject to dissonance in the evaluation of supports for allowing different ==== as plan managers and (c) how the central role of personal goals impacts the choice outcomes of eligible NDIS Participants (and their nominated Carers). Our findings reveal the adoption of two distinct mindsets: (i) one that favours ","The National Disability Insurance Scheme (NDIS or ‘the Scheme’) is a new Australian model of funding and social support for people with disability, and in 2020 (at full rollout) approximately 460,000 Australians are expected to be eligible (Department of Social Services 2014). The NDIS reconciles many progressive policy-making themes that have gained traction around the globe. It is particularly innovative because (a) it places choice and control for care-related decision making in the hands of people with disabilities (and/or their families and unpaid Carers), and (b) its individualised funding and support model, based on participants' goals and aspirations, accounts for the variety of supports that might be required throughout a person's life. Operationally, the big picture view of how the Scheme works is that participants, with the aid of a National Disability Insurance Agency (NDIA) representative, determine their preferred plan management solution, including who will manage their choice of supports, services, therapies and interventions.====There is little understanding about how people with disabilities (or beneficiaries of voucher-type schemes within public sector or quasi-markets) make choices about services and supports. Since its trial launch in 2016, the implementation of the NDIS has been marked by a number of contentious issues, ranging from procedural matters (e.g., delays in participant enrolment, inadequate funding allocation) to policy outcomes (e.g., reduced employment of people with a disability, alienation of remote communities). Many such issues derive from the need to understand the choices that individuals with a disability make when pursuing their personal goals. In particular, there is little empirical data on the influence of personal goals in the choices of Participants and Carers in choosing amongst different service providers.====Moreover, the desired shift from disability-based to goal-based funding allocation could further entrench inequities in the disability sector. The NDIS allows Participants to nominate Carers, who are responsible for reporting goals and manage approved NDIS plans. However, evidence from decision making studies show that people often introduce systematic biases when choosing for others (Atanasov 2010; Polman 2012), including in the context of healthcare choices (Lockenhoff and Cartensen 2008). The shift from disability-specific to goal-based funding might result in further Scheme-derived inequities depending on whether the NDIS decision maker is not the Participant.====The principal objective of this paper is to characterize the heterogeneity of preferences of eligible NDIS Participants (and their nominated Carers) in the disability sector, considering NDIS program parameters and operational structure. Additionally, we examine whether by allowing for different NDIS decision makers in the Scheme Plan (i.e., Participants themselves or nominated Carers), the program is subject to dissonance in the evaluation of services, supports, therapies and interventions among decision makers. Finally, considering the central role of personal goals in NDIS funding allocation, we also investigate the possibility that the pursuit of personal goals by Participants or Carers results in distinct choice patterns: that is, the pursuit of different goals and differential importance associated with these goals by Participants and Carers may be the basis for future program problems and potential failure.====Following this introduction, Section 2 provides a background to the NDIS, including a rationale for the program switch from disability-to goals-focused basis for personal support. This is followed by a discussion on the use of choice-based segmentation in Section 3, which provides a link between goals and choices, as well capturing preference heterogeneity between decision makers. In Section 4 we describe the methodology used for our study, including data collection and analysis strategy. Given the prominent role of personal goals and their pursuit in our theoretical framework, in Section 5 we examine their importance for Participants and Carers and how they are associated to NDIS support services. The DCE results are presented in Section 6 and discussed within broader sector implications in Section 7. Section 8 concludes our paper.",“Whose plan is it?” understanding how the goal pursuit of consumers and carers influence choices in the australian disability sector,https://www.sciencedirect.com/science/article/pii/S1755534521000336,4 June 2021,2021,Research Article,45.0
"Ladenburg Jacob,Larsen Britt Ø.,Berger Nichlas P.,Olsen Leif","Technical University of Denmark (DTU), Department of Technology, Management and Economics, Produktionstorvet 424, 024, 2800 Kgs, Lyngby, Denmark,VIVE - The Danish Center for Social Science Research, Herluf Trolles Gade 11, 1052, Copenhagen, Denmark","Received 8 August 2018, Revised 10 May 2021, Accepted 17 May 2021, Available online 3 June 2021, Version of Record 8 June 2021.",https://doi.org/10.1016/j.jocm.2021.100291,Cited by (0)," (FoC) has social and economic consequences and can impact both physical and mental health. Consequently, strategies to reduce FoC are ==== to national and local policy makers. We use the ==== method choice experiments to elicit preferences for multiple FoC mitigation attributes in a nightlife setting among adolescents and their parents in 10 Danish cities. We find the strongest FoC mitigation preferences for higher number of friends accompanying the adolescent and the atmosphere in the nightlife, followed by police, limiting the number of strangers and the presence of volunteers. Significant differences in preferences between adolescents and their parents are evident.","Research has consistently shown that personal fear of crime (FoC) is common and associated with increased levels of anxiety, withdrawal from social activities, decline in social integration and changes in daily personal behaviours (Zhao et al., 2015). Ferraroʹs definition of FoC (1995, p. 4) describes the content very well: ‘an emotional response of dread or anxiety to crime or symbols that a person associates with crime’. In other words, FoC is an affective reaction to crime (or signs of crime) that is based on perceptions rather than objective measures and therefore is often unrelated to crime rates and the actual risk of victimisation (Hale, 1996).====FoC can have detrimental consequences for both individuals and neighbourhoods created by behavioural, social, physical, psychological and economic effects (Lane et al., 2014). High levels of FoC can influence individuals beyond the personal feeling of anxiety and determine behaviours and social interactions (Gibson et al., 2002). The experience of fear can contribute to avoidance of strangers, situations and places as well as increased distrust, disengagement and withdrawal from the neighbourhood (Ferraro, 1995; Skogan, 1986). Furthermore, studies have found adverse health outcomes associated with FoC as it can affect life quality, mental health, physical health, and even mortality (Grinshteyn et al., 2019; Jackson and Stafford, 2009; Stafford et al., 2007). Finally, FoC can be an influential factor in for example residential and school choices (Gibbons, 2004), and research on house prices has shown negative effects to prices of nearby houses after sex offenders move into a neighbourhood (e.g. Pope, 2008).====FoC has long been an important research area for social scientists and a key topic in national and local crime policies (Brunton-Smith and Sturgis, 2011). The traditional response to high levels of FoC has been surveillance and policing strategies (Gill et al., 2014; Ratcliffe et al., 2015; Weisburd and Eck, 2004), increasing the collective efficacy in target neighbourhoods (Brunton-Smith et al., 2014), improving physical environments (Blakely and Snyder, 1997) and reducing signs of crime (Pate et al., 1986), to mention but a few. It has also been suggested that FoC can be successfully reduced through various environmental or order-related improvements. Investigating such a perspective, Painter (1996) identifies darkness and disorder as the pivotal environmental cues that increase fear in pedestrians and argues that good quality street lighting can make a substantial contribution. Similarly, as a FoC reduction strategy there is increasing evidence that focused types of policing, such as community policing and hotspot-policing, reduces FoC, though without reducing crime and disorder to a comparable degree (Weisburd and Eck, 2004).====It is often argued that traditional measures of FoC fail to capture the complexity of FoC, and as stated by Vauclair and Bratanova (2017, p. 222) ==== Knowledge about the prevalence, frequency and intensity of fear is often largely absent in these studies. Previous research has shown that the traditional questions such as “How safe do you feel or would you feel being out alone in your neighbourhood at night” often overestimate the everyday experience of fear (Farrall, 2004; Farrall and Gadd, 2004; Gray et al., 2008), e.g. because respondents are not asked directly about experienced FoC episodes (Jackson, 2004).====Despite this criticism, many studies of FoC still use such measures with an ordinal response scale, which does not give any insight into the trade-offs regarding what can mitigate everyday experiences of FoC.==== The complexity of the concept of FoC and the traditional methods used to measure the impact of FoC-mitigating attributes calls for solutions that deal with the measurement challenges and make it possible to obtain FoC estimates in a cardinal space. The latter requirement can be met using experimental survey designs in which respondents repeatedly choose between scenarios with different attributes combinations, which will enable us to make comparisons between individuals in terms of how they value the influence of different factors on FoC. Applying methods that can obtain cardinal measures of FoC will also allow policy makers to compare mitigating measures and make more informed decisions as to which FoC reduction strategies to implement. An example to illustrate this could be the intensity of police patrolling and neighbourhood watch in relation to reducing the fear of becoming a victim of burglary. Using the traditional methodology, we could ask how often people would worry about their home being burgled if the police drove through the neighbourhood at night or if they lived in an area with neighbourhood watch. Respondents could then state their level of worry on a four-point scale, such as 1 = ‘all or most of the time’, 4 = ‘never’. The results might show that mean FoC in the case of police patrolling was 3 and 2 in the case of neighbourhood watch. However, this traditional design would still lack a quantification of the relative importance of these two FoC reduction strategies. With cardinal measures of the relationship between policing, neighbourhood watch and the fear of burglary, we would be able to obtain this information. The stated preference method choice experiments (CE) (Adamowicz et al., 1994; Louviere et al., 2000; Louviere and Woodworth, 1983) is specifically designed to disentangle the importance of different choice determinants and investigate the relative strength differences between mitigating factors.====To the authors’ knowledge, no studies have elicited cardinal preferences and choice trade-offs for multiple FoC reduction strategies. Using stated preferences from parents and their adolescent children, we report the results from a CE to elicit preferences for FoC attributes in a nightlife setting. The nightlife setting was investigated, as FoC related to Danish adolescents going out is central topic that receives a lot of attention from parents, but also from local and national policy makers. This attention is driven by, among other things, the high intake of alcohol among Danish adolescents (Ahlström and Österberg, 2005; Engels and Knibbe, 2000) and the fact that adolescents start partying in the age of 14–16 years in Denmark and go out in the nightlife at an early age (Demant and Østergaard, 2007). Our application of CE in relation to FoC is case specific. Clearly, the CE methodology would also be most relevant to apply to other areas of the FoC framework for example FoC reduction strategies in neighbourhoods or related to victimisation of violence or burglary.",Who is watching out for me? Quantifying fear of crime mitigation attributes using a choice experiment approach among adolescents and their parents,https://www.sciencedirect.com/science/article/pii/S1755534521000245,3 June 2021,2021,Research Article,46.0
"Di Gangi Massimo,Vitetta Antonino","Università degli Studi di Messina, Dipartimento di Ingegneria, 98166, Messina, Italy,Università degli Studi Mediterranea di Reggio Calabria, DIIES, 89124, Reggio Calabria, Italy","Received 18 July 2020, Revised 5 May 2021, Accepted 12 May 2021, Available online 27 May 2021, Version of Record 3 June 2021.",https://doi.org/10.1016/j.jocm.2021.100290,Cited by (9),"In assignment models a key role is played by the path choice model, which computes the set of decisions made by the user before (and during) the routing on the network according to perceived paths and related costs. The path choice is a component of the assignment models.====This paper considers the effects of an ====The path choice model, embedded inside the assignment procedure, is specified, calibrated, and validated using traffic counts in two real systems. The numerical results obtained in the analysed systems, in terms of reproduction of the counted flows, demonstrate that the new term can explain aspects that are not represented by consolidated random utility models. Results confirm the theoretical hypotheses stated in the paper.","This paper concerns the modelling of user behaviour in the path choice process. Path choice gives, as output, choice probabilities for the paths belonging to the choice set. Path choice, in conjunction with the other demand levels and with supply, is a component of the assignment models. Starting from the consolidated Random Utility Model (RUM) approach, a new interference term derived from the Quantum Utility Model (QUM) is introduced in path choice process, and a Quantum Utility Model integrated with a Random Utility one (QUARUM) is proposed.====Two levels in the decision-making process can be identified in path choice: i) the generation of the perceived alternatives; ii) the choice among alternatives.====The generation step gives the perceived alternatives as output (Prato and Bekhor, 2006). It can follow an exhaustive or a selective approach. In the first one all loop-less paths are generated. In the second one it is considered a sub-set of all loop-less paths, satisfying one (mono-criterion) or more (multi-criteria) defined rules and/or behavioural aspects. In the state of the art, several procedures are proposed and compared (Bekhor and Prato, 2009) in the generation approach to better understand the size and the composition of the choice set with respect to the alternatives perceived by the users. Bovy et al. (2009) and Frejinger et al. (2009) describe the method to sample alternatives for path generation.====The number of paths to be considered has been defined in several different ways. In some papers, all the loop-less paths are considered (e.g., Sheffi and Powel, 1982); in others only efficient paths are considered (e.g. Dial, 1971); in others papers they are enumerated with respect to some selection rules (e.g. Ben Akiva et al., 1984). It should be noted that users, before making their choice, consider a limited number of alternatives. It is unthinkable that a user, before making a choice, should consider too many alternatives, each of which may consist of a large number of links. Procedures that consider a large number of alternatives may be valid numerically but may not be significant in modelling actual user behaviour.====As regards the path choice models, user behaviour can be modelled considering:====In path choice models, the choice of the user's path is modelled after defining the origin and destination of the trip, the departure time, and the transport mode. An accurate and general overview of path choice models can be found in Prato (2009). It is also the main component of assignment models integrating path choice and other levels of demand.====The assignment models can be classified according to the characteristics of:====Regarding the time at which the decision is made, that is usually modelled inside the demand levels including the path choice, two levels of decision can be considered:====In the reported path choice models, it is assumed that: users have a clear perception of the alternatives in the pre-trip decision-making process, associate a utility value to each alternative, choose the alternative with maximum utility; the modeller computes the choice probability associated with each alternative by considering that the utility derives from a random (in RUM) or fuzzy (in FUM) distribution.====In real conditions, before choosing an alternative, at generation level users can consider that some alternatives might interfere with each other and cannot be perceived as clearly separate. In this condition, users can choose, at the pre-trip decision, more than one alternative considered with maximum utility. To take into consideration this interference between alternatives as well, an interference term is introduced that computes the unclear perception of alternatives that users have at pre-trip decision making. New models, called QUM, must be considered to consider this interference (Baker, 1999; Busemeyer and Bruza, 2012; Lipovetsky, 2018).====In choosing alternatives, two effects must be modelled: perception of utility and overlap between alternatives, assessed with the probabilistic distribution of utility and the term of covariance in RUM (or similar in FUM); the unclear perception of alternatives, assessed with the term of interference in QUM. Considering all the combinations, it is possible to generate four different cases: modelling, or not, of perception and overlapping of utility; modelling, or not, of the interference. To model the two effects, the joint RUM and QUM specifications must be considered. Without interference terms, the joint RUM and QUM specifications are equivalent to the RUM specifications (Busemeyer et al., 2006; Vitetta, 2016).====In the following, the new common model (called QUARUM) is specified in the case of path choice. QUARUMs for path choice are proposed in Vitetta (2016) and Hancock et al. (2020). The model is calibrated and validated by means of traffic counts, using an aggregate method, in two real networks to have different cases and settings. The obtained results confirm the validity of the joint formulation and the modelling effects that are not represented with the RUM models. In the specific experimented case, the QUM is adopted for the perception of the choice set, the RUM is adopted to choose among alternatives belonging to the choice set.====In the case of applications to real networks, the calibration of QUM parameters is of fundamental importance.====In this paper the following improvements have been introduced:====Considering the structure of this document, after a brief summary of the assignment models described in section 2, it is shown how quantum path choice models (subsection 2.1) can be developed, through the integration of an interference term within the choice models of random paths, in the context of assignment models (subsection 2.2). In section 3 the aggregate calibration procedure of path choice model from traffic counts is reported. A test performed in a real case is reported in section 4, in which obtained results are shown and commented; some final results are compared using another real system. Finally, section 5 contains a summary of the results achieved and some indications for further developments.",Quantum utility and random utility model for path choice modelling: Specification and aggregate calibration from traffic counts,https://www.sciencedirect.com/science/article/pii/S1755534521000233,27 May 2021,2021,Research Article,47.0
"Hassanpour Sajjad,Rassafi Amir Abbas,González Vicente A.,Liu Jiamou","Faculty of Engineering, Imam Khomeini International University, Qazvin, Iran,Department of Civil and Environmental Engineering, The University of Auckland, Auckland, New Zealand,Department of Computer Science, The University of Auckland, Auckland, New Zealand","Received 23 December 2019, Revised 9 February 2021, Accepted 8 April 2021, Available online 22 April 2021, Version of Record 29 April 2021.",https://doi.org/10.1016/j.jocm.2021.100288,Cited by (9),Simulation models are an undeniable tool to help researchers and designers forecast effects of definite policies regarding ==== from a dynamic network of the environment using ,None,A hierarchical agent-based approach to simulate a dynamic decision-making process of evacuees using reinforcement learning,https://www.sciencedirect.com/science/article/pii/S175553452100021X,22 April 2021,2021,Research Article,48.0
"Pizzol Bruna,Strambi Orlando,Giannotti Mariana,Arbex Renato Oliveira,Alves Bianca Bianchi","Department of Transportation Engineering, Polytechnic School, University of São Paulo, São Paulo, Brazil,Center for Metropolitan Studies, São Paulo, Brazil,World Bank Group, 1818 H Street NW, Washington, D.C., 20433, United States","Received 30 August 2020, Revised 26 January 2021, Accepted 1 April 2021, Available online 19 April 2021, Version of Record 28 April 2021.",https://doi.org/10.1016/j.jocm.2021.100287,Cited by (0),"This paper investigates the activity behavior of residents of Paraisópolis, the second largest slum of São Paulo (Brazil). The study used data from a survey and one week of GPS traces of a sample of residents. Location data was the basis to infer individual stays and points of interest. Stays were clustered into 6 classes, based on spatial, temporal, repetition and sequence variables characterizing each stay. These stays classes were used to describe individual weekly activity patterns. Individuals were then clustered into 7 categories, based on the similarity of their activity patterns, as described by measures of intensity, variation and repetition. Finally, each group was analyzed in terms of its ==== and socioeconomic composition. Results reveal considerable coherence, confirming expected relationships between the weekly activity patterns and individuals’ attributes. It should be highlighted that more than half of sampled residents were classified into groups with diversified behavior. This result, considering the high density and mixed land use of the Paraisópolis area, reinforces the idea that modelling efforts, even in poorer areas, need to consider activity patterns beyond the more usual simple commute. The article also demonstrates how new multiday data collection methods can contribute to improving the access to hard-to-reach groups, like slums residents.","Studies seeking to understand how individuals move in time and space can be directly relevant for decision-makers when developing and implementing short and long-term transportation policies. The analysis of activity and travel patterns is an important topic in this regard, given the importance of understanding individuals’ behavior to craft transportation solutions that can adequately meet daily demands, now and in the future.====Most models and behavior analyses concerning travel aim to explain variation in travel behavior between individuals – the so-called ==== – based on their personal characteristics and attitudes, and the environment surrounding them (Dharmowijoyo et al., 2020). Besides, the analyses are typically based on the records of activity and travel on a single day (Zhang et al., 2018).====Single-day data, however, only reveal limited aspects of travel behavior (Huff and Hanson, 1986; Pas and Koppelman, 1987), given that people do not repeat the same pattern every day. Although the importance of this ==== has been conceptually recognized for a long time, assessing its magnitude requires data from multiple days of the same individual (Li et al., 2017). Until recently, the collection of travel behavior data for multiple days was relatively rare. Smartphones with GPS receivers, though, are rapidly changing this scenario (Huang and Levinson, 2017; Montoliu et al., 2013; Zhou et al., 2019; Zong et al., 2019).====Investigating data from multiple days allows the assessment of intrapersonal variability but increases the complexity of the analysis. To reduce complexity, it is important to classify individuals in different groups, in an attempt to identify similar behaviors along multiple days (Goulet Langlois et al., 2016; Jiang et al., 2012; Ortega-Tong, 2013). These categories may help to improve models and to better portray activity and travel behavior.====In many cities of less developed countries, a sizeable proportion of the citizens live in informal settlements or slums. Slums are urban areas characterized by poverty and substandard living conditions, while informal settlements are areas developed outside of planning regulations and legally sanctioned housing and land markets (Abramo, 2009). While we acknowledge these differences, we will refer to the study area as a slum (although it presents characteristics related to both concepts), or use the Portuguese word ====.====Individuals living in these types of settlements face geographical, social, and economic conditions that may considerably influence the activities and travel they conduct (Lindau and Linhares, 2011). Nevertheless, studies on travel behavior of residents in slums or informal settlements, in general, refer to places in Africa and Asia, with limited efforts directed to Latin America (Koch et al., 2013; Lucas, 2012; Maia et al., 2016; Mark and Heinrichs, 2019) or particularly to Brazil, where ==== are house to many, more than 11 million of people, according to 2010 Census (Rodrigues et al., 2021).====Given the scarce literature on the use of smartphones for collecting travel data in Brazil, as well as on activity and travel patterns of the population living in ====, this paper investigates the behavior of residents in Paraisópolis, a large ==== in the city of São Paulo, using location data collected over multiple days with a smartphone application, associated with a face-to-face interview collecting individual's socioeconomic characteristics and transportation habits.====The paper is organized into five sections, following this introduction. Section 2 brings an overview of surveying hard-to-reach groups and the collection of location data with GPS-enabled smartphones, including details on data collection effort. Section 3 describes several data processing tasks, including data cleaning and the selection of individuals for the analysis, the method for identifying stays and ==== (POIs), as well as the clustering techniques used to classify stays and individuals. Section 4 presents and analyzes the results and, lastly, Section 5 provides some conclusions and recommendations for future work.",Activity behavior of residents of Paraisópolis slum: Analysis of multiday activity patterns using data collected with smartphones,https://www.sciencedirect.com/science/article/pii/S1755534521000208,19 April 2021,2021,Research Article,49.0
"Pinjari Abdul Rawoof,Bhat Chandra","Department of Civil Engineering, Centre for Infrastructure, Sustainable Transportation and Urban Planning (CiSTUP), Indian Institute of Science, Sir C.V. Raman Road, Bangalore, 560012, India,Dept of Civil, Architectural & Environmental Engineering, The University of Texas at Austin1 University Station C1761, Austin, TX, 78712-0278, USA","Received 10 October 2020, Revised 1 January 2021, Accepted 4 March 2021, Available online 24 March 2021, Version of Record 17 May 2021.",https://doi.org/10.1016/j.jocm.2021.100283,Cited by (13),"This paper proposes simple and computationally efficient forecasting algorithms for random utility maximization-based multiple discrete-continuous (MDC) choice models with additively separable utility functions, such as the Multiple Discrete-Continuous Extreme Value (MDCEV) model. The algorithms build on simple yet insightful, analytical explorations with the Karush Kuhn-Tucker (KKT) conditions of optimality that shed new light on the properties of the models. The MDCEV model and the forecasting algorithms proposed in this paper are applied to a household-level energy consumption dataset to analyze residential energy consumption patterns in the United States. Further, simulation experiments are undertaken to assess the computational performance of the proposed and existing KT demand forecasting algorithms for a range of choice situations with small and large choice sets.","Most choice modeling literature has focused on analyzing consumer demand situations that involve the choice of a single, discrete alternative from a set of choice alternatives that are mutually exclusive to (or perfect substitutes of) each other. In such situations, the consumer's choice of one alternative precludes the choice of other alternatives. However, in numerous situations, consumer choices may be characterized by “multiple discreteness,” where consumers can potentially choose multiple alternatives, as opposed to a single alternative, because the choice alternatives are imperfect substitutes. Further, along with the discrete-choice decisions of which alternative(s) to choose, consumers typically make continuous quantity decisions on how much of each chosen alternative to consume. Such multiple discrete-continuous choice (MDC) situations are being increasingly recognized and modeled in the recent literature in transportation, marketing, and economics.====A variety of modeling frameworks have been used to analyze MDC choice situations. Among these, a particularly attractive approach is based on the classical microeconomic consumer theory of random utility maximization (RUM). Specifically, consumers are assumed to optimize a direct utility function ==== over a bundle of non-negative consumption quantities ==== subject to a linear budget constraint, as:====In the above equation is a quasi-concave, increasing and continuously differentiable utility function with respect to the consumption quantity vector ====, ==== is the vector of unit prices (====) for all goods, and ==== is the budget available for total expenditure. The consumption vector ==== may or may not include an outside good, which, when included, is a Hicksian composite good that represents all goods other than the ==== inside goods of interest to the analyst. Typically, the outside good is treated as a numeraire with unit price, implying that the prices of all goods clubbed into this category do not influence the expenditure allocation among the inside goods (Deaton and Muelbauer, 1980). The form of the utility function governs the characteristics of the optimal consumption bundle resulting from the above utility maximization problem. A typical assumption made on the functional form is that the contribution to the utility ==== from the different goods is additively separable. That is, ====. Another typical assumption is that the Hicksian composite good is considered ==== in that it is always consumed by all the decision-makers.====An increasingly popular approach for deriving the demand functions from the utility maximization problem in (1), especially in the context of modeling MDC choices, is based on the familiar Karush-Kuhn-Tucker (KKT) conditions of optimality. When the utility function is assumed to be randomly distributed over the population (from the analyst's perspective), the resulting KKT conditions are also stochastic in nature. Such randomly distributed KKT conditions provide a basis for deriving the likelihood expressions for observed consumption patterns. Due to the central role played by the KKT conditions, consumer demand models based on this approach are also called Kuhn-Tucker (KT) demand models.====The KT demand systems have been known for quite some time, dating back at least to the works of Hanemann (1978) and Wales and Woodland (1983). However, it is only in the past two decades that practical formulations of the KT demand system have appeared in the literature. Among these, the multiple discrete-continuous extreme value (MDCEV) model structure proposed by Bhat (2005, 2008) has gained substantial attention in the literature. The model is based on a translated constant elasticity of substitution (CES) utility function, whose random utility terms ==== enter the utility function as ====, where ==== is the deterministic component of the utility from consuming good ==== and ==== are specified as independent and identically distributed type-1 extreme value. Due to a careful specification of the utility function and its stochastic terms, the MDCEV model offers an intuitive and clear interpretation of the utility function parameters, closed-form likelihood expressions, and subsumes several other KT demand systems in the literature as special cases.====Over the past decade, the basic MDCEV framework has been expanded in several directions, including (a) the incorporation of general error structures to allow flexible inter-alternative correlations (Pinjari and Bhat, 2010; Pinjari, 2011; Bhat et al., 2013), (b) incorporation of multiple budget constraints (Castro et al., 2012; Satomura et al., 2011; Pellegrini et al., 2021) and lower and upper bounds on consumption (Van Nostrand et al., 2013; Saxena et al., 2020), and (c) accommodation of flexible utility forms (Vasqez-Lavín and Hanemann, 2009; Bhat et al., 2015, 2020; Bhat, 2018; Palma and Hess, 2020), and the use of the MDC framework within an integrated choice and latent variable framework (Enam et al., 2017). The model and its variants have been applied in many empirical contexts, including individual activity participation and time-use (Bhat, 2005, 2008; Habib and Miller, 2009; Rajagopalan et al., 2009; Pinjari and Bhat 2010; Sikder and Pinjari, 2013; Calastri et al., 2017), household travel expenditures (Rajagopalan and Srinivasan, 2008; Ferdous et al., 2010), household vehicle ownership and usage forecasting (Ahn et al., 2007; Fang, 2008; Bhat et al., 2008; Pinjari et al., 2016), shippers’ mode and port choices for freight infrastructure planning (Tapia et al., 2020), outdoor recreational demand (Phaneuf et al., 2000; von Haefen et al., 2004; von Haefen and Phaneuf, 2005), and grocery purchases (Kim et al., 2002).====Despite the many developments and applications, a simple and very quick forecasting procedure has not been available for the MDCEV and other KT demand model systems. On the other hand, since the end-goal of most model development and estimation is forecasting, policy evaluation, and/or welfare analysis, the development of a simple and easily applicable forecasting procedure is a critical issue in the application of KT demand model systems. The forecasting methods available prior to this work are either enumerative or iterative in nature, are not very accurate, and require long computation times.====In this paper, we propose computationally efficient==== forecasting algorithms for random utility maximization-based multiple discrete-continuous choice models with additively separable utility functions, such as the MDCEV model. The algorithms build on simple yet insightful, analytic explorations with the KKT conditions that shed new light on the properties of such models. For specific utility functional forms used in many MDCEV model applications, the proposed approach is non-iterative and results in analytically expressible consumption quantities once the chosen alternatives are known. Even with more general utility forms that fall within the class of additively separable utility functions, we employ the model properties presented in this paper to design efficient (albeit iterative) forecasting algorithms. Also, we formulate variants to the proposed algorithms that remain computationally efficient even in situations with large choice sets. Further, the insights gained from the analysis of the KKT conditions can be used to develop efficient forecasting procedures for other KT demand systems with additively separable utility functions.====As a demonstration of the effectiveness of the proposed algorithms, we present an application to analyze residential energy consumption patterns in the U.S., using household-level energy consumption data from the 2005 Residential Energy Consumption Survey (RECS) conducted by the Energy Information Administration (EIA). This application provides insights into the influence of household, house-related, and climatic factors on households’ consumption patterns of different types of energy, including electricity, natural gas, fuel oil, and liquefied petroleum gas (LPG). Prediction exercises with the proposed algorithms and currently used algorithms highlight the significant computational efficiency of the proposed algorithms. In addition, we present simulation experiments to assess the computational performance of the proposed algorithms vis-à-vis existing algorithms in situations with large choice sets.====The remainder of the paper is organized as follows. The next section presents the challenge associated with forecasting with MDC choice models and describes the forecasting procedures used in the literature. Section 3 highlights some new properties of the MDC choice models with additively separable utility functions. Building on these properties, Section 4 presents new forecasting algorithms tailored for different types of utility specifications under different situations, ranging from small to large choice sets. In addition, a discussion is provided on how similar forecasting algorithms can be developed for other KT demand system models. Section 5 presents an application of the MDCEV model to analyze residential energy consumption patterns in the U.S., using the 2005 RECS data. Section 6 presents several prediction experiments with the RECS data as well as other simulated data to assess the computational performance of the proposed forecasting algorithms vis-à-vis existing approaches. In addition, this section includes hypothetical policy simulations to predict the impact of different climate change-related scenarios on residential energy consumption patterns. Section 7 summarizes and concludes the paper.",Computationally efficient forecasting procedures for Kuhn-Tucker consumer demand model systems: Application to residential energy consumption analysis,https://www.sciencedirect.com/science/article/pii/S1755534521000166,24 March 2021,2021,Research Article,50.0
Jimenez Mori Raul,"IDB Invest, Development Effectiveness Division, Colombia","Received 13 April 2020, Revised 15 November 2020, Accepted 6 January 2021, Available online 18 February 2021, Version of Record 27 February 2021.",https://doi.org/10.1016/j.jocm.2021.100269,Cited by (0),"Over the last decade, immigration flows among developing countries have grown at a faster pace than immigration to advanced economies. Despite the documented welfare benefits of immigration, adverse policies and negative ==== toward immigrants appear to be as common in developing countries as in developed ones. In such a context, understanding natives' attitudes toward immigrants may help inform measures to smooth free human mobility. This paper applies choice experiments to elicit individual preferences with regard to immigrants' attributes, the first such application in a developing country. Aligned with previous literature, the results indicate that younger immigrants with greater levels of education, higher-skilled professions, and fluency in the local language are preferred by natives. An interesting result is that foreigners from developed countries tend to receive greater support for admission. Also, there is significant heterogeneity in the estimations that is not explained by observable characteristics of the respondent, suggesting that most heterogeneity is idiosyncratic. Lastly, the experimental designs compare choice settings with a ‘neither’ option versus those with a forced choice, indicating that the former, arguably a more realistic setting, returns a lower probability of admission.","There is an increasing interest on migration in developing countries. Over 35% of the migrant stock resides in those nations and over the last decade immigration flows among emerging economies have grown at a faster pace than those from emerging to advanced economies.==== In the near future, this trend is likely to increase even more, exposing poor countries to a population influx for which they are unprepared, and risking political and social turbulence (Hanson and McIntosh 2016; Manning and Roy 2010). Examples of this turbulence have already been registered in countries such as Côte d’Ivoire and the Dominican Republic, where immigrants face adverse policies for staying in the host country. Together with recent political events in Europe and the United States, this situation shows not only that immigration is a relevant and sensitive topic in national and foreign policy, but also appears to reflect a generally unfavorable view toward free human mobility across borders.====The working hypotheses to explain such negative attitudes are that immigration tends to be perceived as an economic, and cultural threat (Facchini et al., 2008; Newman 2014; Blinder 2015; Bertoli et al., 2016). Broadly, the factors that drive opinions about immigrants may be divided into economic and noneconomic. Economic factors include labor market and fiscal concerns, while noneconomic factors include norm adherence, national identity (i.e. religious beliefs, language) and ethnicity. Hypotheses based on labor market factors assume that citizens tend to reject immigrants if they represent direct competition for jobs. Similarly, it is postulated that fiscal considerations may explain why natives tend to reject foreigners if they represent a greater tax burden through greater use of public services and social programs. For the noneconomic drivers, it is assumed that immigrants with greater norm adherence, greater identification with the national culture, and a similar ethnicity to natives are viewed more favorably.====Consensus among those hypotheses is still elusive to the empirical literature. One stream of research, based on public opinion surveys, focuses on natives’ attitudes toward immigration. This stream finds support for the above hypotheses and emphasize the role of cultural factors in explaining opinions on immigration but with substantial heterogeneities across citizens with different characteristics (i.e., education, gender, income) (e.g., Mayda 2006; Dustman and Preston 2007; Malchow-Møller et al., 2008; Facchini and Mayda 2012; Card et al., 2012; Ortega and Polavieja 2012). However, because this research is based on general opinion surveys—not on evaluations of individual immigrants—, such studies capture and examine the effects of impersonal attitudes. Thus, although they have the advantage of evaluating cross-country samples, they restrict researchers to a few dimensions of analysis. On the other hand, there is increasing evidence indicating that regardless of labor market, fiscal, and cultural concerns, migration has sizeable welfare net benefits (Alesina et al., 2016; Akay et al., 2014; Chassamboulli and Palivos 2013; Peri 2012).====To investigate the apparent disconnect between public opinion and the net contribution of foreigners, a growing literature focuses on identifying which factors shape natives' individual attitudes (Hainmueller and Hiscox 2010; Aalberg et al., 2012; Iyengar et al., 2013; Hartman et al., 2014; Hainmueller and Hopkins 2015; Wright et al., 2016). These studies are largely based on surveys and choice experiments, which allow for the manipulation of immigrants' individual attributes in order to characterize the corresponding citizens’ preferences. Iyengar et al. (2013) stress the necessary distinction between studies focusing on attitudes toward individual immigrants and group-level analyses of views on immigration, as the two approaches may potentially lead to different conclusions with different implications for public policy. Further, the authors document a lack of correspondence between evaluations of individual immigrants and immigration policy preferences, finding that this gap is explained by individual immigrant characteristics.====Therefore, the literature on individual attitudes has provided important insights into the policy and theories behind the immigration debate. On the one hand, some results indicate that more educated foreigners with skilled occupations are, in fact, supported. These findings align with evidence about the positive effects of immigration on overall welfare, appearing to reject the labor market competition hypothesis. On the other hand, immigrants who violate norms or have little cultural resemblance to natives tend to face opposition. In general, these findings are roughly homogeneous across different groups of natives, suggesting that noneconomic factors and the composition of the immigrant stock in a country drive public opinion and immigration policies.====Within this literature, however, three gaps can be identified. First, previous studies do not explicitly incorporate natives' heterogeneity when evaluating their preferences.==== That is, previous experimental designs allow for heterogeneity in immigrant characteristics, but do not incorporate the heterogeneity of natives’ characteristics in the estimation model. Second, previous experimental designs only apply one type of decision setting within the population sample, restricting our understanding of how they may influence responses. For example, decision settings with the “neither” option may represent a short-cut to respondents, reducing the credibility of the stated preference experiment. On the other hand, forced choice experiments may be not realistic for cases such as immigration in which officials are typically allowed to deny access to candidates. Third, despite the growing immigrant flows among developing countries, to date the literature studying drivers of attitudes toward immigrants has been carried out in developed countries.==== Therefore, there is no evidence that previous findings extend to different institutional, cultural, and ethnic contexts such as those in developing countries. For example, in developing countries, immigration control tends to be lax, and assimilation policies are scarce or nonexistent, while social assistance, provision of public goods, and fiscal contributions are also markedly weaker; a context which could have serious implications for attitude formation towards foreigners (OECD 2011).====This paper attempts to address these gaps by studying attitude formation toward individual immigrants in a developing country. To that end, I designed choice experiments (CEs) to evaluate the effect of 10 immigrant attributes on the probability of obtaining citizens' support for admission. In the CE design, I randomly and efficiently construct immigrant profiles, generating exogenous variability to identify citizens’ preferences. As estimation framework, I use a mixed binary logit model, that allows for heterogeneous preferences among respondents, as well as the examination of drivers of such variance. Further, to examine the effects of different decision settings, I designed two types of CE: one with a forced choice, and one with a “neither” option.====The study took place in the Dominican Republic, a middle-income country where the migrant situation has been receiving significant attention from the authorities, the public, and the international community. Local (and international) media widely cover immigration topics, including a governmental plan, launched in 2014, to regularize illegal immigrants by requiring them to register in the hopes of eventually receiving identity documents.==== On the other hand, since the Dominican economy relies heavily on tourism, foreigners are a common part of daily life.==== These conditions may help to enhance the realism of the choice experiment among respondents.====I designed a survey and administered it to approximately 2500 citizens to collect stated preference data on immigrants’ profiles. The choice experiment applied repeated choice situations to each respondent, returning a panel data structure on a representative urban population sample. The survey also collected detailed information on a range of demographic, social, and economic characteristics of the respondents, as well as time spent per CE, which allowed for the control of potential confounding factors.====Overall, and aligned with the literature, this paper results' indicate that foreigners with higher education, skilled jobs, norm adherence, and national resemblance are preferred for admission. The gender of the immigrant is not relevant. An interesting finding is that foreigners from developed countries are more likely to be accepted. Such a premium is not observed for immigrants from other Latin American countries; moreover, a penalty is detected for immigrants from the only neighboring country; Haitians (though, this is not surprising since the relationship between both nations have been conflictive in the past). The estimated preference weights among natives are shown to be heterogeneous, but they do not seem to be systematically driven by the education, income level, or gender of the respondent, suggesting that such variability is mostly based on natives’ tastes or idiosyncratic preferences. Although these results are qualitatively similar across decision settings (with or without a “neither” option), the latter returns a lower probability of acceptance. Another difference between the experiments is that labor experience is only statistically significant when the neither option is available, suggesting that in a more stringent decision setting, more information about the candidate becomes relevant.====To my knowledge, this is the first stated preference experiment extending to developing countries the study of attitudes toward individual immigrants. In contrast to previous studies, this work also contributes to the literature by applying experiments that include a broader set of immigrant attributes, as well as comparing findings between choice settings with a neither option versus those with a forced choice. An additional contribution is that the analytical framework and estimation methodology allow for heterogeneity in preferences, and for testing whether such heterogeneity is driven by observable characteristics of the respondent.====The next section presents background on the case study. Section 3 outlines the empirical approach to estimating the preference weights for immigrant attributes. Section 4 describes the sampling frame and data collection. Section 5 presents the main results and a set of robustness regressions. Section 6 provides the conclusion and discussion.",Eliciting individual preferences for immigrants in the Dominican Republic. Results from two choice experiments,https://www.sciencedirect.com/science/article/pii/S1755534521000038,18 February 2021,2021,Research Article,51.0
"Kassahun Habtamu Tilahun,Jacobsen Jette Bredahl","Australian Rivers Institute, Griffith University, Nathan, Australia,School of Health Policy & Management and Erasmus Choice Modelling Centre, Erasmus University Rotterdam, Netherlands,Department of Food and Resource Economics and Centre for Macroecology, Evolution and Climate, University of Copenhagen, Denmark","Received 14 January 2020, Revised 4 December 2020, Accepted 17 January 2021, Available online 29 January 2021, Version of Record 6 February 2021.",https://doi.org/10.1016/j.jocm.2021.100271,Cited by (1),"In this paper, we analyze the implications for the economic valuation of the provision of public goods, considering respondents’ perceptions of the institution(s) that provide the service. The specific behavioral mechanism whereby institutional distrust (ID) shows itself is through the activation of screening of choice options (choice set formation). However, ID-induced choice set formation might be confounded with the consumer budget constraint, especially in a developing country context, leading to biased welfare estimates for service improvement. We formulate a semi-compensatory hybrid choice set formation (SC-HCSF) model that enables us to 1) discriminate the effect of a budget constraint from that of ID-induced choice set formation and 2) characterize their separate impacts on welfare estimates using a spatial framework. We compare our model results to those from a standard Random Parameters Logit (RPL) Model. The RPL underestimates (overestimates) welfare when individuals have a low (high) ID. Based on our empirical model results, we demonstrate that the impacts of ignoring institutional trust issues can be highly deleterious to project appraisals, particularly in settings where legislative and regulatory institutions are perceived to be endemically corrupt.","For a stated preference study to be considered valid, a set of assumptions needs to be fulfilled, salient among them is the issue of consequentiality (Carson and Groves, 2007; Vossler et al., 2012): that respondents are assumed to expect that their choice can affect the policy being investigated. More concretely, it often implies using setups with institutions that would also, in reality, carry out such policies. However, if these institutions are known to be prone to corruption, the elicited values (i.e., preferences, willingness to pay, elasticities) may be biased. This becomes increasingly important as stated preferences studies gain importance in developing country research and policy development. Analyzing the impact of consumer and citizen (dis)trust in institutions is a focal interest of the current paper in the context of improving electricity service (reducing power outages) in a developing country setting.====Institutional distrust (ID) has been largely ignored in the literature of non-market valuation involving stated preference (SP)==== studies. Such distrust can lead to the perception that the likelihood of implementation of a described project is small. In the worst-case scenario, such distrust can become dysfunctional, the perceived negative expectations of the agents outweigh the benefits of the activity, thus leading to the elimination of all value-creating opportunities (Tomlinson and Lewicki, 2006; Vedantam, 2014). In this circumstance, conducting and getting reliable information from SP studies is impossible. Individuals may not be willing to participate in the SP survey at all or provide a protest response. This issue can be identified during a focus group discussion/pre-test, and adjustments to the survey can be made accordingly as a standard approach to mitigate protest response in SP survey (Johnston et al., 2017; Kassahun et al., 2020a; Mariel et al., 2021; Meyerhoff and Liebe, 2010).==== However, if a corrupt institutional structure is in place, it may be accepted in focus groups as realistic, and the result we obtain from such a study is just being adjusted for the unrecognized effects of ID. If a screening or elimination of alternatives (choice set formation) exists in the data generation process by the decision-maker arising from ID, yet goes unrecognized by the analyst, a bias in preference inferences and welfare estimates will occur (Li et al., 2015; Peters et al., 1995; Swait, 2001b; Swait and Ben-Akiva, 1987a; 1987b; Truong et al., 2015).====The objective of this article is to test whether such choice set formation processes based on ID takes place in a policy setting of provisioning a public good (reducing power outage) in Ethiopia. More specifically, we 1) formulate an (implicit) choice set formation econometric model that incorporates budget and institutional trust constraints within the framework of standard microeconomic choice theory; and 2) empirically demonstrate the importance of accounting for welfare estimate bias that dwells in the SP data when the ID effect goes unrecognized. A recent study supported by the World Bank and the government of Ethiopia showed that there is a perception amongst Ethiopian citizens that there is widespread corruption in their government in various sectors of the economy, including education, judicial, construction, and water provision and management (Plummer, 2012). This makes it interesting to conduct a case study in Ethiopia in the context of welfare estimation. Data collected from two regional capital cities in Ethiopia that differ in terms of political administration and ethnicity were used to estimate the marginal willingness to pay (WTP) for power outage reduction and watershed management attributes.====The remainder of this paper develops as follows: First, we review relevant literature on endemic institutional distrust and project evaluations; Second, we present theoretical and empirical model specifications; Third, we report and discuss empirical findings; Finally, we summarize findings and provide concluding remarks.",Distortions in willingness-to-pay for public goods induced by endemic distrust in institutions,https://www.sciencedirect.com/science/article/pii/S1755534521000051,29 January 2021,2021,Research Article,52.0
"Chanel Olivier,Luchini Stéphane,Shogren Jason F.","Aix-Marseille Univ, CNRS, AMSE, 5 boulevard Maurice Bourdet CS50498, 13205, Marseille Cedex 01, France,Aix-Marseille Univ, CNRS, AMSE, Marseille, France,Department of Economics, University of Wyoming, Laramie, USA","Received 26 September 2019, Revised 21 December 2020, Accepted 6 January 2021, Available online 21 January 2021, Version of Record 25 January 2021.",https://doi.org/10.1016/j.jocm.2021.100268,Cited by (2),We propose a structural ,"Over the last few decades altruism has become a more important theme in economic theory. Experimental evidence has supported the idea that altruism is compatible with rational behavior (e.g., Andreoni, 2002). Moreover, altruism has been demonstrated to play a significant role in how people cooperate and contribute to public goods (see Fehr and Fischbacher, 2003; Goeree et al., 2002) and the way they deal with global externalities like climate change (see Daube, 2019). Given its impact on behavior and choice, many argue that altruism should be addressed within cost-benefit analysis (CBA) to obtain more accurate public decision making (Bergstrom, 2006; Jacobsson et al., 2007; Quiggin, 1997); in particular for those people we are closest to (e.g. our family) when safety and health are at risk (Kimball-Stanley, 2018). A key question, however, is which type of altruism is driving behavior – pure forms of altruism==== or specific-good-oriented altruism, e.g. wealth, health, safety, education. This distinction matters because accounting for the utility change of others in the expression of one's utility may or may not affect the desirability of a public project (Bergstrom, 1982, 2006; Jones-Lee, 1991, 1992).====With pure altruism, the utility of others is built into one's utility function and the change in the utility of others may be zero if they pay an amount exactly equals to the change in utility (i.e. their Hicksian compensating variation). As shown in Flores (2002) and Bergstrom (2006), a sufficient condition for a marginal change in a public good to be potentially Pareto-improving is that it satisfies a cost-benefit test based on private values only (if the initial allocation is distributionally efficient). To get this sufficient condition, one also needs additional technical assumptions, unrelated to altruism itself, to show that the private value cost-benefit test is a necessary condition (see Bergstrom, 2006). Moreover, the Pareto-improvement is qualified as potential since compensation schemes are rarely implemented once the public project realized. As one cares for both the benefits and the costs others' face for the public project, and one's assume others to properly maximize their own welfare, the predominant recommendation is that pure altruism should be excluded in the benefit estimate to avoid double-counting. Note this supposes, however, that others pay an amount exactly equal to their private benefit for the project, and not a lump sum (Gyrd-Hansen et al., 2016).====In contrast, with non-pure forms of altruism (i.e. health or wealth of others enter one's utility function directly), the private value criterion is insufficient to detect a potentially Pareto-improving project. One's utility gains from other people's increases in health or wealth could exceed the Hicksian compensating variation paid by the others. This translates in additional benefits one's would like to add to other's health- or wealth-related benefits (disregarding their preferences) which would make the project Pareto-improving. Because non-pure forms of altruism are self-motivated, the impact of greater health or wealth of others in one's utility function should be accounted for in CBA estimates (Jones-Lee, 1976; Johansson-Stenman, 2005; Jacobsson et al., 2007; Zhang et al., 2013; Andersson et al., 2019).====In this paper, we test for pure forms versus non-pure forms of altruism for the provision of a public good within the family, using stated preference data. One obvious way to explore altruism in CBA is to design a valuation survey explicitly eliciting altruistic preferences. This would have people directly state the extent of their altruism, i.e. the value of personal well-being they are willing to forgo to increase the well-being of someone else.====Some studies explore altruism in the family in this way on the basis of family relationship. Dickie and Gerking (2007, 2009) found that parents are altruistic towards their young children over reducing skin cancer risks, and Evans et al. (2011) found higher Willingness To Pay (WTP) for changes in asthma care-giving time affecting the respondent's child(ren) under 18 than for changes affecting the respondent him/herself, and no difference for other adults. Most studies assessing adults' WTP for children's health (see the review by Agee and Crocker, 2004; or Gerking and Dickie, 2013; Williams, 2013) found a degree of parental concern==== greater than one. For the avoidance of acute illnesses, there was a degree of about two for Liu et al. (2000), Dickie and Messman (2004) or Guerriero et al. (2018), of 1.5 for Blomquist et al. (2011) or Balmford et al. (2019), and between 1.4 and 2 for Dickie and Salois (2014). For the reduction of more severe illnesses or mortality risk, Hunt and Ortiz (2006)'s review reported degrees of concern in the range 0.6–2.3, almost all being greater than 1; Blomquist et al. (2011) of 1.7 and Gerking et al. (2014) or Balmford et al. (2019) reported degrees of concern greater than 2.====However, the design of these studies makes altruism towards children explicit in the elicitation questions, either through WTP or changes in protective actions taken to reduce the risk of death/illness. This leaves them open to the criticism that if asked whether one is altruistic, it costs nothing to say ‘yes’. We propose to fill this gap by designing a survey in such a way that, instead of directly stating the extent of their altruism, people unwittingly reveal their altruism. Moreover, our design makes private the action on a household public good to eliminate the out-of-family altruistic dimension. Overall, respondents state their preferences over certain household trade-offs, following Viscusi et al. (1988), and econometric tools then allow us to estimate the value of their intra familial altruism from their choices.====We make two principal contributions to the existing literature. First, our analysis contributes to the methodological literature by proposing a structural econometric model to measure degrees of altruism within the family regarding a change in the level of a household public good. Second, our analysis contributes to the empirical literature by proposing an appropriate experimental design and applying it to a French contingent valuation (CV) survey related to air quality improvements.====We find evidence of a strict form of air quality focused altruism - parents reveal significant concern only towards their own children under 18. All other forms of pure or air quality focused altruism within the family are insignificant, including for children over 18, siblings, spouses, and parents. This result implies benefit estimates that do not consider altruism towards children under 18 could be undervaluing the public good.====The paper proceeds as follows. Section 2 presents a structural model of altruism within the family. Section 3 analyses the experimental design and the data. Section 4 discusses the results and the final section provides conclusions.",Does charity begin at home for air pollution reductions? Unraveling intra familial altruism,https://www.sciencedirect.com/science/article/pii/S1755534521000026,21 January 2021,2021,Research Article,53.0
"Verma Ashish,Verma Meghna,Sarangi Punyabeet,Yadav Vivek,M Manoj","Department of Civil Engineering and Robert Bosch Centre for Cyber-Physical Systems, Indian Institute of Science (IISc), Bangalore, 560012, Karnataka, India,Ramaiah Institute of Management, Bangalore, 560012, Karnataka, India,Department of Civil Engineering, Indian Institute of Science, Bangalore, 560012, Karnataka, India,Department of Civil Engineering, Indian Institute of Technology, Delhi, 110016, Hauz Khas New Delhi, India","Received 3 August 2019, Revised 13 November 2020, Accepted 5 January 2021, Available online 11 January 2021, Version of Record 20 January 2021.",https://doi.org/10.1016/j.jocm.2021.100267,Cited by (5),"Activity travel pattern of pilgrims in a religious setting is a complex process. Extant literature on religious tourism has taken minimal efforts in addressing such complexity, which has led to a paucity of information on preferred activity participation destinations and trip chain sequences of pilgrims. So, the present research objective is two-fold. First, to examine the causal effects of socio-demographics and daily local temperature on activity participation, trip chain type, and time allocation of individuals using ==== (SEM) that can help identify the dominant activity patterns. Second is to explore the impact of socio-demographic variables and activity patterns on the propensity of stop-making behavior using an ordered logit (OL) framework to better plan and manage the influx of flows. The primary data was collected using an activity-travel diary by taking the case study of the Kumbh Mela event, which is considered as the world's largest mass religious gathering, held at Ujjain, India, in 2016. From the results, it is observed that Males have a lower tendency to take multiple stops for primary religious activities and have simple trip chains. An increase in the members of a family visiting Kumbh decreases their overall time spent across various activities. As the mercury (temperature) rises, it reduces tourist's participation in recreational and discretionary activities. Individuals who participate in primary and secondary religious activities tend to spend more time at Kumbh as compared to individuals who primarily visit for recreational purposes. These empirical findings provide meaningful insights for managing large religious events.","Religious tourism can be viewed as any travel that involves religious experience (Puşcaşu, 2015; Raj and Griffin, 2015). Given this definition, it can be said that all trips taken to religious sites and the activities that individuals engage themselves at these sites broadly define the religious tourism context from a transportation perspective. In fact, over the past few years, many researchers within the transportation field have explored the challenges faced by this sector. Firstly, any form of tourism (religious or recreational) is a perishable economic product due to the observed Spatio-temporal variances in the preference of travelers (Witt and Witt, 1995). These shifts in the behavior of travelers==== may have detrimental effects on the economic vitality of a country. Secondly, policymakers and transport planners face an arduous task of planning as well as maintaining a hassle-free religious event for visiting travelers as religious events are associated with traffic congestions, freight movements, etc. (Ahmed and Memish, 2019). Knowing the socio-demographic profiling of travelers and their motivation to visit and participate in religious activities can help planners improve and expand services for a steady flow of travelers. To address these challenges, as well as to have a better understanding of the tourism behavior, researchers have applied the well-defined activity-travel demand paradigm into the religious tourism sector. The popularity of these activity-based modeling frameworks can be attributed to the ease with which they incorporate the temporal and spatial constraints at the macro (household) and micro (individual) level of activity participation while providing a framework that is more viable and implementable in forecasting travel demand (Arentze and Timmermans, 2004; Bhat and Koppelman, 1999). Besides, these well-defined activity mechanisms provide acceptable inferences and a better understanding of the travel behavior of people that helps transport planners in framing policies to regulate the travel demand management smoothly.====Moreover, the interdependency between various keywords, namely joint travel, task and time allocation, episode frequency, trips, and tours, is well established in the extant literature on activity-travel demand modeling (Golob, 2003; Simma and Axhausen, 2001). This stream of literature is further enriched by various activity-based studies (Cheng et al., 2019; Liu et al., 2018; Manoj and Verma, 2015a) conducted in developing countries. These past studies based on activity paradigm have explored the interdependence between household demographics, individual attributes, in-home, and out-of-home activity participation, and travel behavior. However, only a handful of past studies in the activity domain have attempted to explicitly model the inherent relationship between pilgrim's activity participation and time allocation behavior in a massive conglomeration of pilgrims at a religious destination. This has led to a paucity of information on destination choices & preferences; crowd movement & management; and prioritizing & planning activity participation due to temporal and spatial constraints in the context of events, festivals, and religious gatherings, etc. that are of global importance (Smallwood et al., 2012; Xia et al., 2011; Zoltan and McKercher, 2015). Notably, in a religious event, walking is the primary mode of movement for most of the between and within destination trips that are undertaken to participate in various activities inside the cordoned destination region==== for various religious fairs or festivals worldwide (Verma et al., 2018). With walking being a slow mode of travel, it induces both acceptable trip time and distance constraints. As a consequence, the choices and preferences of travelers in allocating time to various religious activities have to be carefully interpreted.====Furthermore, the propensity of taking stops during a home-based or non-home based tour by an individual has also been extensively researched in the past decade to understand the trip patterns and frequency of episodes undertaken by an individual (Chu, 2003; Timmermans et al., 2003). The findings of these studies reveal that travel patterns are mainly independent of spatial settings though some exceptions exist. However, unlike the traditional activity travel-based analysis where home-to-home is considered as the anchor point for a trip chain, in case of festivals and religious events, they have designated entry and exit points that make the trip chaining pattern more interesting and exciting to study but has garnered limited attention. In a religious event setting, pilgrims/visitors desire to participate in several activities in a cordoned destination region forces them to link their activities, resulting in complex trip chains. This interlinking of trips exhibited by individuals is stemmed due to a variety of external and internal factors (Smallwood et al., 2012; Xia et al., 2011). The external factors include socio-demographic characteristics (sex, age, income), weather condition (temperature, humidity etc.), socio-cultural factors (language, religion, culture) and service quality attributes (access to transport facilities, atm, etc.) while the internal factors include psychological aspects, behavioral traits like motivation, level of satisfaction, and perception towards individual safety and security (Alejziak, 2013; LaMondia et al., 2008). Thus, it can be seen that unlike the traditional home-based activity setting, a multitude of variables impacts the activity participation, time spent, and stop-making behavior of travelers visiting religious destinations. However, the current work is mainly concerned with the causal effects of socio-demographic variables on activity participation, time spent, and the stop-making behavior of travelers visiting a religious site.====Based on the above discussions, the present study has tried to address the existing gaps in the religious tourism sector with the following objectives: i) examine the interdependencies between socio-demographics, climatic conditions, religious activity participation, and travel pattern (that includes total time spent and trip chain type) of individuals using structural equation model (SEM). ii) exploring the impact of socio-demographic variables, climatic conditions, and religious activity participation on the propensity of stop-making behavior using an ordered logit (OL) model to identify the dominant activity patterns of tourists/pilgrims at a religious event. Here, both the proposed empirical approaches are estimated independently by taking the case study of Kumbh Mela, which was held at Ujjain, India, from April 22, 2016 to May 21, 2016. The reason behind carrying out the study at Kumbh Mela is that it is considered as the largest human congregation on Earth and experiences footfall from all over the world. The event represents a fascinating amalgam of religious and social activities, and recreational elements assembled in a short span with limited space (Maclean, 2008).====The remaining paper is divided into six sections. Section 2 briefly describes the extant studies on activity participation, time allocation, and trip chaining behavior from a methodological perspective. Section 3 Model specification, 4 Methodology respectively discuss the specification and theoretical foundation behind the empirical frameworks undertaken in the present paper. This is followed by section 5, which contains a description of the study area and data (exploratory analysis of exogenous and endogenous variables). The penultimate section (Section 6) elaborately interprets the causal effects and empirical findings of the models. The final section concludes with the salient findings and scope of future research.","Activity participation, episode duration and stop-making behavior of pilgrims in a religious event: An exploratory analysis",https://www.sciencedirect.com/science/article/pii/S1755534521000014,11 January 2021,2021,Research Article,54.0
"Lu Hui,Burge Peter,Sussex Jon","RAND Europe, Westbrook Centre, Milton Road, Cambridge, CB4 1YG, UK","Received 16 November 2019, Revised 16 November 2020, Accepted 18 November 2020, Available online 5 December 2020, Version of Record 11 December 2020.",https://doi.org/10.1016/j.jocm.2020.100266,Cited by (4),"Additional funding will be needed to meet the growing demand for ==== in the UK. What is the most acceptable way to raise it? Options range from taxation to mandatory insurance, voluntary insurance and user charges. We sought to analyse the preferences of the UK general public.====An online quantitative survey embedded within a DCE was undertaken with a representative sample of 2,756 members of the public in England, Northern Ireland, Scotland and Wales, recruited from a survey panel. The DCE was designed on the basis of detailed background research including focus groups and cognitive interviews. The survey also collected information on respondents', age, health state, experience of ====, income, employment status and education; and tested respondents’ knowledge and awareness of National Health Service (NHS) and social care funding levels and sources. From the DCE data we developed models to understand the influence that differences in attributes had on the propensity to choose funding mechanisms.====From the scaled MNL model results, all sections of the public – across age groups, income groups, employment status, health status and countries of the UK – would like additional funding for adult social care to be raised in the same way as additional NHS funding. Specifically, the public prefer a collective rather than individualistic approach to raising additional funds; and preferably a progressive system. All age groups prefer that contributions should not differ by age ====. Raising additional funds should for preference be by a public, not a private, organisation. There is support for earmarking the funds raised to only be used for health care or social care. Preferences are very similar across the four UK countries, once age and socioeconomic characteristics are controlled for.====This research provides novel evidence to help policy makers understand the relative public acceptability of different options for raising additional funds for health and adult social care in the UK. We find uniformity of preferences across the UK countries and across sub-groups of the population.",None,Measuring public preferences between health and social care funding options,https://www.sciencedirect.com/science/article/pii/S1755534520300634,5 December 2020,2020,Research Article,55.0
"Parsons George,Yan Lingxiao","University of Delaware, United States,South China Agricultural University, China","Received 1 December 2019, Revised 1 November 2020, Accepted 14 November 2020, Available online 3 December 2020, Version of Record 11 December 2020.",https://doi.org/10.1016/j.jocm.2020.100264,Cited by (6),"We consider anchoring on visual cues in a contingent-behavior study of the effects of offshore wind power projects on beach use on the East Coast of the United States. In an internet-based survey of beachgoers, we show respondents visual simulations of wind power projects at three offshore distances and vary the order in which respondents see the visuals -- so some see near visuals first and some see far visuals first. Respondents are asked how their trip-taking behavior may be affected by the projects. In parametric and non-parametric analyses, we find strong anchoring in the far-to-near ordering of the visuals and weak anchoring in the near-to-far ordering. We also find greater dependence on the first-shown visual versus the most-recent-shown visual. Finally, we find some effects of having viewed wind turbines in real life before entering the survey. The size of the ","In classical economic models, decision makers’ preferences are regarded as predefined, consistent, and stable. In contrast, Tversky and Kahneman (1974) describe a heuristic wherein individuals use prior information, such as initial values shown in a survey or advertisement, to guide preferences. This phenomenon is referred to alternatively as anchoring, reference-dependence, or ordering effect (Ariely et al., 2003).====In this paper, we consider anchoring on visual cues in a stated preference survey. We conducted a large-scale revealed-preference/contingent-behavior study using an internet survey. The purpose of the study was to understand the effect of offshore wind power projects on beach recreation on the East Coast of the United States (Parsons et al., 2020). In the survey, respondents (beachgoers) were shown visual computer simulations of offshore wind power projects and asked how it might affect their behavior. Each respondent was shown a simulation at a near, mid, and far distance, and then for each distance asked to report how it might affect their trip taking behavior. The order in which a respondent saw the visual simulations varied. For example, they might see near-mid-far, far-mid-near, or some other combination. The respondents know in advance that they will see the wind turbines at different distances offshore.====Our research question then is: Does the order in which respondents see visual simulations affect their stated change in trip taking? Or, put differently, do respondents anchor in some way on the first distance they are shown? We also consider the potential for anchoring on information respondents have as they enter the survey, since some respondents have prior experience viewing wind power projects and others not.====At the outset, we were uncertain whether visuals would serve as an anchor or not and, if so, how the anchor might manifest itself. We had two competing hypotheses: assimilation and contrast. Our null hypothesis is no anchoring – perhaps the clarity that visuals bring to the choice setting obviates anchoring effects. In the assimilation hypothesis, a respondent's initial level of satisfaction with a wind power project is carried forward in the survey. If, for example, a respondent happened to see a non-intrusive field of turbines at a far distance first, they would be prone throughout the survey to have a positive view of turbines even if the positioning worsened. A tone would be set by the initial view, carried forward by the respondent with perhaps some adjustments as new views are shown, and ultimately assimilated into later responses. These responses are pulled toward the preference held in the initial view. The assimilation theory is like the psychometric descriptions of behavior discussed by Green et al. (1998). In the contrast hypothesis respondents see the current viewing of turbines relative to or in contrast to the earlier views. In the previous example, the non-intrusive first view of turbines would be carried forward as a comparator for current views. If current views are worsening, then respondents are less favorable than they would have been otherwise. They see the current view in contrast to what might be possible – a less intrusive view. The contrast hypothesis is like the reference-effect hypothesis discussed by Day et al. (2012). Dillman et al. (2014, p. 234) discusses these competing anchoring effects in surveys.====We find anchoring in our results and they align best with the contrast hypothesis. Respondents appear to treat their early visual as a “possibility” or “deal” and go on to compare their later visuals relative to that first view. So, if the sequence of visuals is such that circumstances are improving, respondents tend to see a relative “gain” and react more favorably toward to later visuals versus a case without an earlier view. If, on the other hand, the sequence is such that circumstances are worsening, respondents tend to see a relative “loss” and react less favorably toward later visuals versus a case without the earlier view. While both effects are observed in our data, the loss is larger in absolute value. We interpret this as loss aversion, which we discuss later. We begin with a brief review of the literature and then discuss our study design, results, robustness, and overall findings.",Anchoring on visual cues in a stated preference survey: The case of siting offshore wind power projects,https://www.sciencedirect.com/science/article/pii/S1755534520300610,3 December 2020,2020,Research Article,56.0
"Fukushi Mitsuyoshi,Guevara C. Angelo","Department of Transport Engineering and Logistics, Pontificia Universidad Católica de Chile, Chile,Department of Civil Engineering, University of Chile, Chile,Department of Management Control and Information Systems, University of Chile, Chile,Instituto Sistemas Complejos de Ingeniería (ISCI), Chile","Received 31 March 2020, Revised 12 October 2020, Accepted 14 October 2020, Available online 21 November 2020, Version of Record 28 November 2020.",https://doi.org/10.1016/j.jocm.2020.100256,Cited by (2),", or between one fourth and one half of the travel cost displayed to the interviewees.","Modeling human choices is essential for many areas of knowledge including, among others, transportation science, marketing, energy, health and public policy in general. The canonical tools for modeling human choices are based on the random utility maximization (RUM) principle (Marschak, 1959; Marschak, 1974; McFadden, 1974), which relies on the regularity assumption that establishes that the incorporation of a new alternative cannot increase the preference of an option already present in the choice-set. However, the violation of the regularity assumption has been often detected in various areas of choice behavior (see, e.g. Huber et al., 1982; Frederick et al., 2014). This phenomenon was originally termed as the attraction effect by Huber et al. (1982) and later denominated as the decoy effect by Wedell (1991).====The existence of the decoy effect calls for a need of understanding its susceptibility and subjective valuation, that is, to learn what type of individuals may be prone to be influenced by a decoy effect and to what degree. This goal has been scarcely pursued before and this paper proposes, illustrates and assesses two methodologies to achieve this goal: systematic taste variations and the latent classes approach.====The systematic taste variation method is based on a multinomial logit choice model that accounts for the decoy effect using an emergent value term with a coefficient that varies among groups of individuals that are defined by exogenous variables. The latent classes’ method uses a similar scheme, based on a logit model, but accounts for heterogeneity on the decoy susceptibility and subjective valuation by considering latent classes that are defined by a probabilistic model.====The proposed methods are illustrated and assessed using a case study on route choice data collected from a stated preference (SP) survey depicting a hypothetical shopping trip performed by car on a Saturday afternoon, lasting an average of 24 min. The survey consisted on eight choice scenarios, each one with three alternatives defined by their travel time and cost. The first two scenarios were used to approximate the value of time of each individual in the sample. Subsequent scenarios depicted two alternatives over the trade-off line defined by the inferred value of time, and a third alternative that was built as an asymmetrically dominated decoy that favored one (i.e the objective) of the latter alternatives over the other (i.e. the competitor). To achieve the asymmetrically dominated status the decoy was worse on time or cost, compared to the objective, but had a mixed performance when compared to the competitor. Further details of the design are provided in Section 4. We also collected response time and attitudinal information of the sample using four questions, together with socioeconomic information such as gender, age, home location, number of members of the household, and motorization rate.====Results show first that both methods are feasible for analyzing the decoy effect and that only respondent's age and response time seem to play a clear role in decoy's susceptibility in the case study. On the contrary, no noticeable impact on susceptibility is detected for gender, imputed income, motorization rate, household size and whether the survey was conducted in person or online. Finally, regarding decoy's subjective valuation, evidence from this case study suggests that presenting the decoy was on average as large as reducing about one fourth of the travel time, or between one fourth and one half of the travel cost in the hypothetical shopping route choice experiment developed for the case study.====The article is structured as follows. After this introduction, Section 2 summarizes the state of the art regarding the study of the decoy effect. Section 3 describes the methods proposed in this research to investigate the susceptibility and subjective valuation of the decoy effect. Section 4 describes the case study on route choice used to assess the methods and Section 5 reports the main results attained with the methodologies presented. The article finishes summarizing the main findings of this investigation and proposing future lines of research in this area.","A discrete choice modeling approach to measure susceptibility and subjective valuation of the decoy effect, with an application to route choice",https://www.sciencedirect.com/science/article/pii/S1755534520300531,21 November 2020,2020,Research Article,57.0
"Smith Brett,Goods Caleb,Barratt Tom,Veen Alex","Centre for Business Data Analytics, University of Western Australia, Australia,UWA Business School, University of Western Australia, Australia,School of Business and Law, Edith Cowan University, Australia,The University of Sydney, Discipline of Work and Organisational Studies, Australia","Received 29 November 2019, Revised 27 September 2020, Accepted 30 September 2020, Available online 30 October 2020, Version of Record 11 November 2020.",https://doi.org/10.1016/j.jocm.2020.100254,Cited by (18),The emergence of the ‘gig’ economy is disrupting ,"The digitally-enabled ‘gig’ economy is a contemporary, technology-driven transformation of work organisation (De Stefano, 2016) and is linked to wider labour market trends including a rise in precarity, the separation of paid work from employment, and the increasing atomisation of tasks and responsibilities within both supply-chains and jobs (Stewart and Stanford, 2017; Wood et al., 2018). Platforms create digital marketplaces where both in-person and online services are intermediated and sold, with work performed and paid as separate (although often near-identical) ‘gigs’. The ‘gig’ economy has become synonymous with organisations such as Uber, Uptasker, Deliveroo and Amazon's Mechanical Turk. Despite marked differences between these organisations, the pervasive role that online technology plays in facilitating and enabling work and the contractual arrangements through which these organisations engage workers unites them and distinguishes ‘gig’ work from other forms of work organisation (De Stefano, 2016). In Australia, a recent labour market survey (McDonald et al., 2019) (n = 14,000) found that 7.1% of respondents worked via a digital platform in the previous 12 months, with food-delivery platforms – the focus and context of this research – amongst the most frequently used by consumers and workers.====Different typologies have been developed to capture the variety within ‘gig’ work, seeking to capture the diversity within the ‘gig’ economy (e.g. Howcroft and Bergvall-Kåreborn, 2018). One sub-category within these typologies is app-based ‘gig’ work, characterised by a platform's mobile phone application facilitating the provision of ‘on demand’ services, commonly involving labour by workers. App-based platforms, including Uber, intermediate and actively facilitate the provision of services through their digital eco-systems, offering highly standardised experiences for consumers. One way in which platforms standardise this experience is through algorithmic control, where algorithms, in effect, manage workers (Lee et al., 2015; Rosenblat and Stark, 2016; Veen et al., 2019). Platform firms usually structure relations with workers through arm's length arrangements, seeking to classify workers as independent contractors whose interactions with the platform are expressions of enterprise. In Australia, the use of the independent contractor classifications enables platforms to evade many of the social protections for employees, such as minimum wages and occupational health and safety insurance (Cherry and Aloisi, 2017; Stewart and Stanford, 2017). Although contested, ‘gig’ work represents a significant departure from traditional legal and social norms around work and employment, with transport and food-delivery platforms worker classification, for instance, subject to legal challenges in many countries (Meijerink and Keegan, 2019). The classifications approaches adopted by these types of platforms have, however, contributed to a situation where market forces and consumer moral choices can play a greater role in determining the pay and conditions of ‘gig’ workers than would be the case under conditions of employment.====As an emergent phenomenon, the ‘gig’ economy has garnered increasing scholarly attention. The first wave of research unpacked the nature of ‘gig’ work and contractual arrangements used by platform-organisations (De Stefano, 2016; Cherry, 2016); raising important questions about regulatory responses to these work arrangements (Stewart and Stanford, 2017; Thelen, 2018). The second wave charted the implications of this emergent phenomenon for the experience of workers (e.g. Goods et al., 2019; Peticca-Harris et al., 2018; Rosenblat, 2019). Despite this, scarce attention has been directed to another critical actor, consumers, who are simultaneously the initiator, end-user and consumer of these services - a notable exception is the work by Healy et al. (2020). Indeed, Rahman and Thelen (2019) pose that consumers are pivotal to the existence and functioning of platforms. Through the technology, platforms effectively co-opt consumers into co-management; evaluating and disciplining workers based on consumer-inputs such as performance ratings (e.g. Uber's five-star rating for its ride-share partners). Rosenblat (2019) describes how consumer ratings affect workers' ongoing ability to access Uber's platform, and thus earn income. End-users further occupy another position vis-à-vis the ‘gig’ economy, as civil actors who have the capacity to shape both product demand and the regulatory environment under which platforms operate (Culpepper and Thelen, 2020, Mayes, 2015). Unhappy consumers have the ability to do damage to company reputation and profit, for example by boycotting the platform as with the #deleteuber actions (Cresci, 2017). As a result, workers and labour activists increasingly position the consumer's role in the ‘gig’ economy as a potential pathway to improving working conditions. In contrast, platforms can also engage consumers as civil actors. Uber, for instance, has leveraged the power of consumers to pressure local policymakers to change taxi regulations (Rosenblat, 2019).====‘Gig’ work, as a non-standard form of work, represents a fundamental departure from standard employment. Platforms firms commonly deliberately evade worker protections, making ‘gig’ work contentious from a moral standpoint (De Stefano, 2016; Rosenblat, 2019). It raises questions around how work within contemporary societies is constructed, including how workers are treated within labour markets. Paid work is often viewed as an opportunity for individuals to participate as citizens in society, whereby there has been a long-standing expectation in developed nations like Australia that those who actively participate in the labour market should be able to sustain themselves and their families. In Australia, the national discourse around ‘fairness’ and ‘egalitarianism’ (Sheldon, 1971) reflect a social contract (Locke, 2016) premised around “fair and reasonable wages” for a “human being in a civilized community”==== (Higgins, 1907) – i.e. a form of so-called ‘decent work’. The International Labour Organization (2020) defines ‘decent work’ as:====Indeed, where past governments have violated or eroded these standards by enacting laws that undermined these standards, the Australian electorate – who are also consumers – have shown a willingness to change the government to restore these protections (Wilson and Spies‐Butcher, 2011). Platform firms have created work arrangements that circumvent labour protections and, therefore, could be construed to challenge notions of ‘fairness’ and ‘decent’ work. In similar vein, it can be argued that those who consume these services, deliberately or unintentionally, engage in behaviour which contribute to the erosion of working conditions; a behaviour which, theoretically, can be considered either amoral or immoral.====To date, little is understood about how much end-users know and understand about work conditions and entitlements in the ‘gig’ economy, nor how their knowledge, accurate or otherwise, shape their consumption behaviours. Employing a survey and a choice experiment to assess moral decision-making of consumers, this study explores the attitudes and understanding of Australian consumers towards work conditions and entitlements in app-based food-delivery services. Following the work of Chorus (2015, p. 70), we recognise that what constitutes a moral decision is complex. For the purposes of this paper, however, moral decisions are understood to be choice conditions whereby the decision maker is likely to view their choices as having ‘a moral dimension’. To limit the focus, and in light of the choice context presented herein, we further refine our focus to moral decisions within consumption. Within this narrow confine, moral consumption decisions are actions that have material or psychological consequences for others (Schwartz, 1968) and, at least, pose some form of trade-off between self-interest and consideration of others (Nisan, 1990). Thus, a moral consumption action is to consider others rather than pursue self-interest. To gauge whether respondents consider the outcomes of others, the survey instrument elicits attribute attendance scales for all attributes presented in the stated choice tasks. However, the analysis is limited to the attribute attendance information concerning the earnings of the gig-worker and a limited number of workplace protections.====The purpose of this paper is to interrogate whether consumers are willing to pay to ensure worker entitlements and whether an awareness-raising treatment influences their moral decision-making. In this context, awareness refers to raising worker entitlements as a possible moral dimension of the choice task. The utilised research design helps to discern, firstly, the willingness of consumers to prioritise and pay for worker entitlements as part of their consumption choices. Secondly, it examines whether these priorities are influenced by a priming effect that asks respondents to consider work conditions and entitlements of ‘gig’ workers. The paper first reviews the literature on the potential for consumers to influence workplace practices through their consumption or their citizenship action. It then presents the rationale informing the survey method, experimental design, and sample. Section 4 details the impact of our treatment effect. A profile of consumers is then achieved via a latent-class choice model as detailed in Section 5 and a discussion of results along with our conclusions are presented in Section 6.",Consumer ‘app-etite’ for workers' rights in the Australian ‘gig’ economy,https://www.sciencedirect.com/science/article/pii/S1755534520300518,30 October 2020,2020,Research Article,58.0
Delle Site Paolo,"University Niccolò Cusano, Rome, Italy","Received 25 February 2020, Revised 5 October 2020, Accepted 5 October 2020, Available online 29 October 2020, Version of Record 5 November 2020.",https://doi.org/10.1016/j.jocm.2020.100253,Cited by (1),"In linear-in-income logit, a case with no income effect, the expectation ofthe compensating variation and the expectation of the equivalent variation are identical and provided by the logsum formula. The statistical properties of the estimator of this measure of welfare change are investigated. Under regularity conditions, the estimator of the measure obtained from maximum likelihood estimators of the utility coefficients is consistent, asymptotically normal and asymptotically efficient. Estimates are provided of the variance of the estimator of the measure, useful to derive large sample confidence bounds, and of the covariance of estimators of the measure in distinct scenarios, useful for large sample hypothesis testing.","Among discrete choice random utility models, logit has the longest record of theoretical research and applications. The case of linear-in-income specification of the utility functions, with a coefficient that does not change with income or alternative, a case of absence of income effect, was the first tackled in the development of welfare change measures. The expectation of the compensating variation, which since McFadden (1999) has been established as mainstream measure of welfare change, is given by the logsum formula. This is used, in particular, in transportation and recreation demand applications.====The estimator of this measure is a function of the maximum likelihood estimators of the coefficients of the utility functions. Investigation of the statistical properties of this derived estimator is of relevance to applied work. In the assessment of a scenario of change of price and quality attributes of the alternatives, the computation of confidence bounds of the welfare change measure is of interest. In the comparison of two scenarios, that exhibit different changes of price and quality attributes of the alternatives, a test of hypothesis on the relative magnitude of the welfare change measures associated with the scenarios is also of interest.====Investigation of derived estimators in logit models has been restricted to measures that exhibit a one-to-one correspondence with the utility coefficients, as is the case of ratios of coefficients representing willingness-to-pay measures (Daly et al., 2012). The case of invertible, i.e. one-to-one, estimators derived from maximum likelihood estimators is conventional in mathematical statistics. Indeed, maximization of the likelihood is invariant to one-to-one transformations of the parameters to be estimated (see, e.g., Ruud, 2000).====The case of non-invertible derived estimators is more intricate. Nevertheless, theorems have been formulated that provide the regularity conditions under which the properties of the maximum likelihood estimators transfer to the derived estimators. These invariance theorems use the definition of an induced likelihood function, introduced by Zhena (1966). The logsum measure is a non-invertible derived estimator. The statistical properties of this measure are not yet investigated. The present note aims to fill this gap.====The results in the note are stronger than the delta method, which provides the covariance matrix based on a theorem that states the normality of the asymptotic distribution of a derived vector estimator (see, among the others, Mittelhammer, 2013, Theorem 5.40). The note provides a proposition which states, in addition, consistency and asymptotic efficiency.====The note has the following organization. Section 2 reviews logit theory and welfare change measurement. The statistical properties of the welfare change measure are presented in Section 3. Section 4 includes a numerical illustration. Section 5 extends treatment to the case of logit with income effect. Section 6 concludes.",A note on the large sample properties of the welfare change estimator in linear-in-income logit,https://www.sciencedirect.com/science/article/pii/S1755534520300506,29 October 2020,2020,Research Article,59.0
"Hess Stephane,Meads David,Twiddy Maureen,Mason Sam,Czoski-Murray Carolyn,Minton Jane","Choice Modelling Centre & Institute for Transport Studies, University of Leeds, Leeds, UK,Leeds Institute for Health Sciences, University of Leeds, Leeds, UK,Institute of Clinical and Applied Health Research, Hull York Medical School, University of Hull, Hull, UK,University of Sheffield, Sheffield, UK,St James's University Hospital, Leeds, UK","Available online 14 October 2020, Version of Record 28 December 2020.",https://doi.org/10.1016/j.jocm.2020.100252,Cited by (0),"Choice modelling techniques have established themselves as a key analysis tool in health economics and have been used to understand patient and practitioner preferences across a wide variety of settings. A key interest in recent years has been the incorporation of ever more flexible levels of heterogeneity in preferences across individual ====, and in particular a growing interest in the potential role that attitudes and perceptions might play in healthcare choices. At the same time however, many applications simply apply these new tools without then investigating the resulting richness in the results. This paper not only presents a novel application of hybrid choice modelling in health, by looking at preferences for outpatient parenteral intravenous antimicrobial therapy (OPAT), but also carefully explores the findings in terms of sources of heterogeneity, disentangling the role of attitudes from other heterogeneity. We find that a large share of the heterogeneity can be attributed to two key underlying attitudinal constructs, related to the general attitude towards hospitals and whether responsibility for healthcare should lie with the patient or the practitioner. Especially the latter accounts for more than 60% of the overall heterogeneity in preferences for the type of treatment. These results may help design services that are suitable and appealing for a wide variety of patients as well as providing some insights into how nudging of attitudes and perceptions could help drive patients towards safer and more cost-effective treatment options.","The use of choice modelling in health research is growing in popularity, with an increasing breadth of application areas and growing sophistication of the models used (see e.g. de Bekker-Grob et al., 2012; Clark et al., 2014; Hole, 2018; Soekhai et al., 2019). A key focus in choice modelling, and increasingly also in health research, has been the recognition that preferences vary across individual decision makers. Such work recognises that understanding average preferences for service or treatment attributes and levels is not sufficient alone, and heterogeneity in preferences should be explored fully. Indeed, optimal healthcare decision making requires an understanding of, and the ability to account for, heterogeneity in preferences in any one population. Given that preference data could be used to inform service provision, ignoring heterogeneity in preferences may result in sub-optimal services. These may have a negative impact on patient satisfaction, patient uptake and adherence, which in turn will likely impact on health outcomes and costs. Even in those cases where customisation of services is not possible and only a uniform treatment can be delivered, insights from a model recognising heterogeneity in preferences are still valuable. Indeed, given likely asymmetries in the distribution of preferences in a population, the results from a model not recognising the presence of such heterogeneity are likely to give a biased estimate of the mean sensitivities, a point well rehearsed in other fields (cf. Hess et al., 2005). Understanding the sources of heterogeneity is also important as this can highlight a route through which a health care professional may be able to better communicate the advantages or disadvantages of specific treatments to patients, and identify types of patients most likely to find a particular model of healthcare challenging, so tailored support can be offered.====The assessment of heterogeneity in preferences is a key topic in choice modelling across disciplines (see Hess and Daly, 2014 for an overview of choice modelling). In recent years, the has been increasing focus on incorporating the role of attitudes and perceptions as key drivers of preference heterogeneity. While the interest in attitudes for choice modelling goes back at least to McFadden (1986), it is only thanks to more recent improvements in estimation capability that analysts have been able to include such approaches in their analyses. A key recognition in that more recent stream of research has been that attitudes cannot be observed by an analyst, and that the treatment of answers to attitudinal questions as if they were error free measures of attitudes will thus likely lead to biased results. This has led to the development of hybrid choice models. These structures treat the attitudes themselves as latent (i.e. unobserved) variables, and use them to help explain the heterogeneity in preferences. The models are “hybrid” structures as the latent attitudes not only act as an explanatory of the choice behaviour but are also used in measurement models that explain the answers to attitudinal questions. For an overview of the development of such models, see Abou-Zeid and Ben-Akiva (2014).====The easy access to powerful software has led to a growing number of health applications using hybrid choice models across areas, including in health (going back to Kløjgaard and Hess 2014). However, such work has often been characterised by two shortcomings. First, there has been a tendency to apply the models without then exploiting their full potential when it comes to the analysis of the results and the additional insights that can be gained into behaviour. Second, many studies (which out of respect for the authors we do not cite here) have failed the heed the warnings in Vij and Walker (2016) when it comes to the specification of the models. Vij & Walker show that if an analyst allows for random heterogeneity only through the latent attitudes, then this will invariably lead to improvements in fit over a model without these latent constructs. This comparison however is misguided as it is based on the assumption that all random heterogeneity can indeed be linked to the attitudinal constructs. In reality, there will almost surely be a partial misattribution of the source of the heterogeneity, with heterogeneity not related to attitudes being attributed to attitudes. This point can be understood most easily by noting that, if heterogeneity across individuals exists in the data, then a model will use any mechanism available to capture that heterogeneity. This issue can be avoided by allowing at the same time for heterogeneity not linked to attitudes. As Vij and Walker (2016) show, any advantages in terms of prediction capability over a model without attitudinal constructs then disappear - a hybrid choice model ==== offer a better fit to the choice data than a model with a correspondingly flexible specification that only explains the choices (and not the answers to attitudinal questions). At first, this may seem a disappointing finding and reduce the interest in applying hybrid choice models. On the contrary however, it opens the possibility to use a hybrid choice model to dissect the sources of heterogeneity and allow us to understand what part of the variation in preferences can be linked back to attitudes. This is the aim of the present paper.====Work in a health context has long highlighted the possible role of attitudes in driving patients' preferences (Mosnier-Pudar et al., 2010; Russo et al., 2019). The specific focus of our application is to look at the preferences of patients who require intravenous antimicrobials (IVA) delivered by outpatient or community-based services – termed outpatient parenteral antimicrobial therapy (OPAT). OPAT services are used to treat short-term skin and soft tissue infections as well as chronic or longer-term infections such as joint and bone infections, bacteraemia, osteomyelitis, diabetic foot and tuberculosis. There is a wide range of OPAT service configurations but the most common are hospital/clinic outpatient appointments, nurse provision in the patient's home (general or specialist nurse), or patient self-administration after the receipt of training. Each has advantages and disadvantages; for example, being treated at home is convenient and has a lower risk of acquiring another infection than being treated in clinic but may involve the use of once or twice-a-day broader spectrum antibiotics rather than targeted antibiotics. While OPAT service provision is growing in the UK, there is considerable variation in what is available geographically and a paucity of evidence to commend one particular service type over another, with care pathway decisions often made on the basis of funding or clinician preference. This motivates the focus on understanding patients' preferences for the different services, how these preferences vary across patients, and how this heterogeneity links back to underlying attitudes. These findings can be used to inform the development of new, tailored and patient-centred OPAT services (e.g. ====; Chapman et al., 2019).====We collected data via a stated choice (SC) survey (also regularly called DCE in health) in the UK, sampling (current or previous) patients who had either short or long term infections. The SC component looked at choices between the key three delivery methods outlined above (outpatient, nurse visit at home, self-administration), while the survey also collected extensive socio-demographic and other clinical data in addition to answers to attitudinal questions. We then specified a flexible hybrid choice model which allowed for two main sources of heterogeneity, one linked to underlying attitudes and one independent of those attitudes. For both sources, we incorporate both deterministic variation (i.e. linked to patient characteristics) as well as remaining random variation. Our findings show a substantial role for two underlying attitudes, one that captures a broad attitude towards hospital while the other relates to the attitude towards responsibility of healthcare. We see that these two attitudes together account for the vast majority of the heterogeneity in preferences towards the three types of treatment. This work thus highlights that a carefully specified model can disentangle the sources of heterogeneity and provide insights into the role of psychological constructs in driving preferences. This opens up substantial scope for policy interventions aimed at changing behaviour through nudging of attitudes (cf Voyer, 2015).====The remainder of this paper is organised as follows. Section 2 presents the survey design, sampling, and preliminary analysis of the attitudinal data. This is followed in Section 3 by the modelling methodology. Section 4 presents the results of the analysis, with conclusions in Section 5.",Characterising heterogeneity and the role of attitudes in patient preferences: A case study in preferences for outpatient parenteral intravenous antimicrobial therapy (OPAT) services,https://www.sciencedirect.com/science/article/pii/S175553452030049X,14 October 2020,2020,Research Article,60.0
"Lederrey Gael,Lurkin Virginie,Hillel Tim,Bierlaire Michel","École Polytechnique Fédérale de Lausanne (EPFL), School of Architecture, Civil and Environmental Engineering (ENAC), Transport and Mobility Laboratory, Switzerland,Eindhoven University of Technology, Department of Industrial Engineering & Innovation Science, the Netherlands","Received 11 December 2019, Revised 20 March 2020, Accepted 19 May 2020, Available online 22 August 2020, Version of Record 11 November 2020.",https://doi.org/10.1016/j.jocm.2020.100226,Cited by (4),"The emergence of Big Data has enabled new research perspectives in the discrete choice ====. While the techniques to estimate Machine Learning models on a massive amount of data are well established, these have not yet been fully explored for the estimation of statistical Discrete Choice Models based on the random utility framework. In this article, we provide new ways of dealing with large datasets in the context of Discrete Choice Models. We achieve this by proposing new efficient stochastic optimization algorithms and extensively testing them alongside existing approaches. We develop these algorithms based on three main contributions: the use of a stochastic Hessian, the modification of the batch size, and a change of optimization algorithm depending on the batch size. A comprehensive experimental comparison of fifteen optimization algorithms is conducted across ten benchmark Discrete Choice Model cases. The results indicate that the HAMABS algorithm, a hybrid adaptive batch size stochastic method, is the best performing algorithm across the optimization benchmarks. This algorithm speeds up the optimization time by a factor of 23 on the largest model compared to existing algorithms used in practice. The integration of the new algorithms in Discrete Choice Models estimation software will significantly reduce the time required for model estimation and therefore enable researchers and practitioners to explore new approaches for the specification of choice models.","The availability of more and more data for choice analysis is both a blessing and a curse. On the one hand, this data provides analysts with a great wealth of behavioral information. On the other hand, processing the increasingly large and complex datasets to estimate choice models presents new computational challenges. The Machine Learning community has been thriving in dealing with vast amounts of data. It, therefore, seems natural to investigate Machine Learning optimization algorithms to estimate choice models.====The predominant approach of these algorithms is the ====. It consists in approximating the gradient of the log likelihood function (or any goodness of fit measure) using only a small subset of the data set. The gradient is said to be stochastic because the subset of data used to calculate it is drawn randomly from the full data set. A version of the steepest descent algorithm using this stochastic gradient is then applied to maximize the log likelihood function. Many variants have been proposed around this primary principle.====To illustrate the stochastic gradient, we consider applying stochastic gradient descent to a choice model with ==== alternatives, and a data set of ==== observations, each of them containing the vector of exploratory variables ==== and the observed choice ====. The choice model====provides the probability that individual ==== chooses alternative ==== in the context specified by ====, where ==== is a vector of ==== unknown parameters, to be estimated from data. Typically, this is done using maximum likelihood estimation, where the log likelihood function ==== is maximized:====In the optimization literature, it is custom to define the algorithms for minimization problems. We follow the same convention, and consider the equivalent minimization problem:====The gradient of the log likelihood function is====To obtain its stochastic version, we draw randomly, without replacement, a subset of ==== observations from the data that we call a ====, and calculate:====As shown in the above analysis, the variants of the stochastic gradient methods that are successful in Machine Learning could be used as such to estimate the parameters of choice models. This could be used to decrease the time and computational cost of estimating choice models on large datasets. However, there are three critical differences between the two contexts which must be considered. Firstly, the parameters in choice models are a substantial output, and are used to estimate behavioral indicators such as Values of Time (VoT) and elasticities for the population. Conversely, parameters in Machine Learning models typically have no behavioral interpretation, and only the model predictions are treated as a modeling output. It is, therefore, typical to allow choice model parameter estimates to converge during estimation to obtain the highest accuracy and precision of each individual parameter estimate. Conversely, in Machine Learning, it is typical to restrict the model from converging fully on the training data to prevent overfitting. This can be achieved, for example, using early stopping, by stopping the optimization process when the model achieves highest performance on an out of sample validation set.====Secondly, choice models tend to have far fewer parameters than used in Machine Learning. Complex choice models have hundreds of parameters, while the neural networks used in deep learning can involve millions of unknown parameters. There have been recent efforts to reduce the number of parameters in Machine Learning models. For example, Wu (2019) investigates simplifying Convolutional Neural Networks (CNNs). The author can reduce the number of parameters using matrix decompositions from three million to just above three thousand, with only a small loss of precision. Nonetheless, these models are still complex and exceed the usual number of parameters used in choice models.====Finally, choice data is typically collected using specific sampling strategies and designs of experiments. The objective is to obtain a representative sample while avoiding redundancies. Furthermore, the analyst usually has a model in mind when designing the data collection. In contrast, Machine Learning techniques are often applied to datasets collected automatically or that were originally collected for other purposes (e.g., trip records from contactless payment cards), to detect patterns which have not previously been considered. It is, therefore, common to have a great deal of redundancy in the data.====In this paper, we introduce a new algorithmic framework for the estimation of choice models, which addresses the key differences between the choice modeling and Machine Learning contexts. Our framework includes three primary contributions:====The rest of the paper is laid out as follows. In the next section, we describe optimization algorithms used both for the estimation of Discrete Choice Models (DCMs) and in Machine Learning. Then, in Section 3, we present in detail the three ideas mentioned above and propose a catalog of variants of optimization algorithms for the estimation of choice models. In Section 4, we evaluate the performance of a series of algorithms on various choice models with large data sets. Finally, in Section 5, we conclude this article and mention some further ideas that will be investigated in the future.",Estimation of discrete choice models with hybrid stochastic adaptive batch size algorithms,https://www.sciencedirect.com/science/article/pii/S1755534520300257,22 August 2020,2020,Research Article,61.0
"Hillel Tim,Bierlaire Michel,Elshafie Mohammed Z.E.B.,Jin Ying","Department of Engineering, University of Cambridge, UK,School of Architecture, Civil and Environmental Engineering, Ecole Polytechnique Fédérale de Lausanne, Switzerland,College of Engineering, Qatar University, Doha, Qatar,Department of Architecture, University of Cambridge, UK","Received 27 October 2019, Revised 3 March 2020, Accepted 10 May 2020, Available online 6 August 2020, Version of Record 9 December 2020.",https://doi.org/10.1016/j.jocm.2020.100221,Cited by (28),"Machine Learning (ML) approaches are increasingly being investigated as an alternative to Random Utility Models (RUMs) for modelling passenger mode choice. These approaches have the potential to provide valuable insights into choice modelling research questions. However, the research and the methodologies used are fragmented. Whilst ==== on RUMs for mode choice prediction have long existed and the methods have been well scrutinised for mode choice prediction, the same is not true for ML models. To address this need, this paper conducts a systematic review of ML methodologies for modelling passenger mode choice. The review analyses the methodologies employed within each study to (a) establish the state-of-research frameworks for ML mode choice modelling and (b) identify and quantify the prevalence of methodological limitations in previous studies.====A comprehensive search methodology across the three largest online publication databases is used to identify 574 unique records. These are screened for relevance, leaving 70 peer-reviewed articles containing 73 primary studies for data extraction. The studies are reviewed in detail to extract 17 attributes covering five research questions, concerning (i) classification techniques, (ii) datasets, (iii) performance estimation, (iv) hyper-parameter selection, and (v) model analysis.====The review identifies ten common methodological limitations. Five are determined to be methodological pitfalls, which are likely to introduce bias in the estimation of model performance. The remaining five are identified as areas for improvement, which may limit the achieved performance of the models considered. A further six general limitations are identified, which highlight gaps in knowledge for future work.","Solutions used both in industry and academic research for modelling passenger mode choice have traditionally relied almost exclusively on econometric Discrete Choice Models (DCMs) based on the random utility framework (McFadden, 1981). However, there have been two key recent drivers which have resulted in researchers exploring alternative approaches. Firstly, the adoption of new transportation-related technologies has driven a step change in the availability of data on passenger movements of several orders of magnitude. Secondly, there have recently been significant breakthroughs in Machine Learning (ML) research, which have resulted in numerous success stories of ML applications in other similar tasks.====These drivers have resulted in a number of recent research applications of ML techniques to the mode choice problem. The application of ML has the potential to provide valuable new insights into mode choice modelling research questions. However, the existing research is fragmented, and there have been few studies which comprehensively compare ML techniques with each other and with Random Utility Models (RUMs). Additionally, whilst systematic reviews on RUMs for mode choice prediction have long existed, and the methods have been well scrutinised, the same is not at all true for ML models.====To address these limitations, this paper conducts a systematic review of ML machine learning approaches for modelling passenger mode choice. The paper specifically focuses on classification approaches, where a labelled dataset is used to estimate an individual mode choice model. The review focuses on the methodologies employed within each study, including the classification algorithms, datasets, model validation, model optimisation, and model analysis. The review aims to (a) establish the state-of-research frameworks for ML mode choice modelling and (b) identify and quantify the prevalence of methodological limitations in previous studies. Whilst this review is focused on passenger mode choice literature, the findings are relevant to other choice modelling domains.",A systematic review of machine learning classification methodologies for modelling passenger mode choice,https://www.sciencedirect.com/science/article/pii/S1755534520300208,6 August 2020,2020,Research Article,62.0
White Mark H.,"National Coalition of Independent Scholars, United States","Received 26 December 2019, Revised 14 April 2021, Accepted 21 April 2021, Available online 7 May 2021, Version of Record 11 May 2021.",https://doi.org/10.1016/j.jocm.2021.100289,Cited by (6),"Case 1 best-worst scaling, also known as best-worst scaling or MaxDiff, is a popular method for examining the relative ratings and ranks of a series of items in various disciplines in academia and industry. The method involves a survey respondent indicating the “best” and “worst” from a sample of items across a series of trials. Many methods exist for calculating scores at the individual and aggregate levels. I introduce the bwsTools package, a free and open-source set of tools for the R statistical programming language, to aid researchers and practitioners in the construction and analysis of best-worst scaling designs. This package is designed to work seamlessly with tidy data, does not require design matrices, and employs various published individual- and aggregate-level scoring methods that have yet to be employed in free software.","Important social and psychological processes require people to choose between alternatives. A high school, for example, might need new chemistry equipment and updated books—but the budget only supports one or the other. In politics, people say they are highly supportive of equality and freedom—but what about when these values come into conflict? Affirmative action policies, for example, have been framed as promoting racial ==== in academic institutions, while others have said these policies necessarily limit the ==== for universities to accept who they would like (Lakoff, 2014).====Likert-type scales—such as seven-point scales anchored at ==== and ====—may not be appropriate measurement tools in these common situations. On a seven-point scale, a respondent can indicate that they “strongly agree” that races should be equal on one item, then also “strongly agree” that universities should be free to accept any students they want on another. The tension between the two, such as in the case of affirmative action, is obscured. Ceiling (plurality responding at the highest point of the scale) and floor (plurality responding at the lowest) effects are common in studying certain important issues like prejudice, values, and political ideology. In the abstract, every one might agree—or at least follow a social norm—that racial inequality is bad and that freedom is good.====A different method to measure attitudes in these domains is to have respondents choose between a series of alternatives. Case 1 best-worst scaling is one such method. This article introduces an R package for designing and analyzing data using this method. It is meant as a tutorial and introduction; this article does not explore detailed mathematical proofs for different analysis options, but offers suggested readings for those interested.",bwsTools: An R package for case 1 best-worst scaling,https://www.sciencedirect.com/science/article/pii/S1755534521000221,7 May 2021,2021,Research Article,66.0
"Chorus Caspar G.,Liebe Ulf,Meyerhoff Jürgen","Faculty of Technology, Policy and Management, Delft University of Technology, the Netherlands,Department of Sociology, University of Warwick, Coventry, United Kingdom,Institute for Landscape Architecture and Environmental Planning, Technische Universität Berlin, Berlin, Germany","Available online 17 February 2021, Version of Record 23 February 2021.",https://doi.org/10.1016/j.jocm.2021.100280,Cited by (1),None,None,Models of moral decision making: Theory and empirical applications in various domains,https://www.sciencedirect.com/science/article/pii/S1755534521000142,17 February 2021,2021,Research Article,67.0
"Garzón L.,Estrada J.,Cantillo V.","Department of Civil and Environmental Engineering, Universidad del Norte, Colombia","Received 28 March 2020, Revised 19 September 2020, Accepted 28 September 2020, Available online 1 October 2020, Version of Record 7 October 2020.",https://doi.org/10.1016/j.jocm.2020.100251,Cited by (9),"Modelling people's behaviour is a complex task, not only because of their intrinsic complexity but also because of their interaction with the environment and other individuals. The traditional format for discrete choice experiments involves the use of text and images. However, there is a growing tendency for using tools that offer a more realistic representation of complex and dynamic attributes in discrete choice experiments. The ","Modelling of people's behaviour is complex. For instance, due to the fact that pedestrian behaviour depends on the context and the situation, several factors intervene in the choice process such as the interactions with the surrounding environment, the influence of other individuals, the presence of obstacles, and vehicle traffic, among others. The study of pedestrian behaviour while crossing urban roads or during an evacuation emergency exemplify the complexity of the choice process.====On the one hand, considering the rates of death and injury crashes involving pedestrians, the study of pedestrian behaviour when crossing roads is highly relevant. Globally, pedestrians represent 23% of traffic deaths (World Health Organization, 2018). The understanding of the fundamental features of walking behaviour has significant implications for pedestrian infrastructure planning, safety studies, and policy formulation.====On the other hand, pedestrian behaviour during an evacuation emergency is another remarkable context of analysis. Understanding pedestrian decisions in this context is relevant for developing safe evacuation plans and the risk-analysis on pedestrian crowd situations (Rendón et al., 2019). The absence of a safe and efficient evacuation plan can affect a large number of evacuees during emergencies.====Discrete Choice Experiments (DCEs) are a useful approach to represent and understand pedestrian behaviour in these and other contexts involving pedestrians. DCEs are used to evaluate individual behavioural responses when facing hypothetical choice scenarios and to study the influence of attributes in the choice process. Several researchers have used DCEs to study the pedestrian crossing behaviour (Cantillo et al., 2015; Anciaes et al., 2018; Anciaes and Jones, 2018; Dada et al., 2019), and also for assessing the route and multi-exit pedestrian decisions during evacuation situations (Lovreglio et al., 2016; Haghani and Sarvi, 2017; Aleksandrov et al., 2018; Rendón et al., 2019).====Despite their widespread use in the pedestrian behaviour literature, DCEs have been criticised for their lack of realism. This issue is particularly visible when the scenarios not only do not have a good representation of attributes that are involved in the analysis but are also inherently complex. Nevertheless, traditional text-only attribute representation continues to be the mainstream when performing DCEs due to "" … their higher test-retest reliability, their lower cost, and their ease of implementation … "" as suggested by Birenboim et al. (2019). An alternative for developing highly realistic, immersive, and interactive choice scenarios is using Virtual Immersive Reality Environment (VIRE). VIRE has been used in diverse research fields such as urban development (Rid et al., 2018), neighbourhood choice (Patterson et al., 2017), environmental preferences (Birenboim et al., 2019), human behaviour during emergencies (Lovreglio et al., 2016), pedestrian behaviour (Tapiro et al., 2016; Sobhani and Farooq, 2018), and the study of autonomous vehicles (Farooq et al., 2018; Velasco et al., 2019), among others.====Despite its advantages, using the VIRE technique has limitations. Its design is more expensive than paper or laptop surveys as it requires experts in graphic design and virtual reality. It also needs specialised equipment that is not always available, particularly when taking information in uncontrolled environments or the field. In this sense, the question arises about whether the effort is worthwhile, particularly in the context of complex decisions, such as the study of pedestrian behaviour. This article addresses this question and compares the results of both techniques (i.e. VIRE and traditional DCEs) in two specific contexts: the case of pedestrians crossing a road, and during an evacuation from an enclosed sports arena.",On the use of virtual immersive reality for discrete choice experiments to modelling pedestrian behaviour,https://www.sciencedirect.com/science/article/pii/S1755534520300488,1 October 2020,2020,Research Article,69.0
"Lee Ungki,Kang Namwoo,Lee Ikjin","Department of Mechanical Engineering, Korea Advanced Institute of Science and Technology, Daejeon 34141, South Korea,Department of Mechanical Systems Engineering, Sookmyung Women's University, Seoul 04310, South Korea","Received 23 July 2019, Revised 13 August 2020, Accepted 31 August 2020, Available online 6 September 2020, Version of Record 13 September 2020.",https://doi.org/10.1016/j.jocm.2020.100250,Cited by (3)," is a popular method of estimating heterogeneous customer preferences. Although model accuracy can be increased by including more choice data, this option is untenable when the obtaining of choice data from target customers is costly and time-consuming.. We thus propose a method for choice data generation for commercial products whose expected money value is a key factor in consumer choice (e.g., commercial vehicles and financial product). Using an individual usage scenario, we generate a ","Consumers tend to consider a product's aesthetic, functionality, and economic value when assessing it with the intent to purchase. This decision-making process has attracted tremendous interest from scholars of consumer research (Bettman et al., 1998), operations management (Karniouchina et al., 2009), market system design (Frischknecht et al., 2010; Kang, 2014), the environment (Alriksson and Öberg, 2008), food studies (Annunziata and Vecchio, 2013; Caputo et al., 2018; Darby et al., 2008; Halbrendt et al., 1991), and healthcare (Bridges et al., 2008, 2011; Marshall et al., 2010). Discrete choice analysis is one of the techniques used to explore this decision-making process. This statistical technique uses choice data, which can be regarded as observed choice sets for respondents, to measure consumers' tradeoffs among products or services with assorted attributes (Green and Srinivasan, 1990).====One of the possible decision rules in discrete choice analysis is for it to use a utility model and assume that the consumer chooses a product with a greater utility, which is the sum of the ==== the consumer assigns to the product attributes (Kang et al., 2019; Lewis et al., 2006; Michalek et al., 2005; Raghavarao et al., 2011; Wong et al., 2019).====The choice data needed to estimate partworths can be collected through a survey completed by potential customers (Kang et al., 2016; MacKerron, 2011). A discrete choice experiment presents a series of choice sets on a set of products with different combinations of attributes. The respondent is asked to choose the preferred product option among the alternatives (DeSarbo et al., 1995; Halme and Kallio, 2011; Louviere and Woodworth, 1983). The individual partworths can then be estimated via a hierarchical Bayesian (HB) approach, which determines the probability of a respondent choosing one product over the others as a function of its attributes (Hein et al., 2019; Orme, 2009; Rossi et al., 2005; Train, 2001). The HB approach has found especially wide use for estimating customer preferences for vehicles under various market conditions (Kang et al., 2015, 2018). Along this line, we consider a commercial vehicle case study.====The precision of the discrete choice model's parameter estimates depends strongly on the sample size. Obtaining choice data can be costly and time-consuming when the target customers are specified to a certain group. It may not even be possible to obtain enough data to perform discrete choice analysis. Therefore, this study seeks to improve the precision of the discrete choice model's parameter estimates even when the sample size is not adequately large. To this end, the discounted cash flow (DCF) model based on usage scenarios and DCF analysis is applied to discrete choice analysis. The usage scenarios that can be obtained from the simulated individual consist of individual demographic data related to product use and concern the gains or costs the product will bring to the customers during the period of use.",Choice data generation using usage scenarios and discounted cash flow analysis,https://www.sciencedirect.com/science/article/pii/S1755534520300476,6 September 2020,2020,Research Article,70.0
"Kamakura Wagner A.,Kwak Kyuseop","Jesse H. Jones Graduate School of Business, Rice University, Houston, TX, USA,Marketing Discipline Group, Business School, University of Technology Sydney","Received 2 October 2019, Revised 27 April 2020, Accepted 28 April 2020, Available online 25 August 2020, Version of Record 15 September 2020.",https://doi.org/10.1016/j.jocm.2020.100214,Cited by (0),"This study focuses on the menus typically found in the marketplace (e.g., restaurants and Internet vendors), where the consumer may choose one or more from dozens of options or menu items, each at a posted price or fee. We show that modeling choices out of the typical menu leads to the “curse of dimensionality,” which transpires in two ways. First, the choice set (all possible menu selections) grows geometrically with the number of items in the menu. Second, the number of interactions among menu items also grows disproportionately to the number of items in the menu. We propose a menu choice model that circumvents these two problems in a feasible and flexible, but parsimonious way.====We test the proposed model on synthetic data from Monte-Carlo simulations and find that the proposed estimation approach produces consistent parameter estimates while significantly reducing the dimensionality of the problem. We then apply the proposed approach to an actual choice experiment where a sample of consumers made multiple choices from eight different menus, each combining a base system with selections from 25 optional features. Our empirical results show that menu items do interact (positively or negatively) and the proposed approach produces a graphical representation of these interactions. We also perform an optimal pricing policy experiment to further illustrate the practical features of the proposed menu-choice modeling approach.","Menu choice, where the consumer makes multiple decisions at the same occasion, is a prevalent phenomenon in the marketplace. For example, as s/he walks down restaurant-row, a tourist studies the menus posted in front of the restaurants, rejecting a few until s/he finds an attractive menu. Once s/he decides to choose from that particular menu, she makes multiple concurrent and connected choices: will s/he order the churrasco along with the Mendonza Malbec to pair with it? Or the almond-encrusted trout with a glass of Sauvignon Blanc from Marlborough Valley? Here, the value of a particular selection (red or white wine) depends on other selections (steak or seafood), something that happens at one particular choice occasion, and therefore cannot be ascribed to correlated tastes as often assumed, because correlations are only observed across occasions.====Most importantly, this choice phenomenon is not confined to restaurant menus. It occurs in many situations where the consumer must make multiple inter-related choices at the same occasion. For example, on a typical shopping trip to the local grocer, the sight of pumpkins at the produce section might evoke the desire for pumpkin pie, leading to the purchase of a pumpkin, but also condensed milk, baking flour, vegetable shortening and spices. These items are purchased at the same shopping occasion not because preferences for these items are correlated (across consumers and/or shopping trips), but because the marginal value of baking flour increases if a pumpkin is also included in the basket (and vice-versa). One could also argue that a catalog is nothing more than a large menu from which multiple possibly inter-dependent items can be ordered. Nowadays, menu choice is becoming ubiquitous in the marketplace because many services (cable TV, wireless telecom, banking, etc) are sold in flexible bundles, where the customer selects the desired options beyond a basic plan. One empirical evidence for the importance of menu-choice to marketers is the fact that a major provider of software and services for measuring consumer preferences (Orme, 2010a) already offers a commercially successful “advanced discrete choice modeling software for multi-check menu studies.”====What makes menu choice distinctive from other choice decisions traditionally modeled in the marketing literature is that choice alternatives are not mutually-exclusive, leading to multivariate and potentially polytomous interdependent choice outcomes. Similar to traditional choice models, preferences for menu items might be correlated. In other words, some people might have a strong preference for sports programs, while others prefer movies. Such preference heterogeneity leads to the segmentation of people with similar preferences. Or, a person may buy similar items across shopping occasions (e.g., purchase of Forbes magazine in one week, followed by purchase of Businessweek in next week).====The fact that preference for menu items might be correlated across consumers and possibly over choice occasions has already been studied in the literature (Manchanda et al., 1999; Liechty et al., 2001) and is the approach current used in commercial applications (Orme, 2010a). However, in addition to heterogeneous correlated preferences, the items in a menu are interdependent so that the value assigned to one item at a given choice occasion depends on which other items are also selected at that same occasion. For example, the value of buying a subscription for Time magazine depends on whether the consumer already subscribes to Newsweek. It is likely that the value of the Times subscription decreases because of its overlap with Newsweek. This second form of item dependence happens at one choice occasion and therefore cannot be ascribed to heterogeneous correlated preferences as commonly modeled in the literature (e.g., Ainslie and Rossi, 1998; Singh et al., 2005) which, by definition, can only be observed across consumers or choice occasions. The fact that items in a menu can be complements or substitutes ==== is important to marketers because it affects the profit firms can extract from the market by combining items into pre-defined bundles (Venkatesh and Kamakura, 2003).====Given its prevalence and importance, multivariate cross-category choice has already been widely studied in the literature, which we will review later. Basically, the problem has been addressed with two main approaches. The first (heretofore labeled ====) approach is to treat the purchase of multiple items as outcomes from a multivariate binomial process, attempting to account for the complementarity or substitution among items with the correlation of preferences across consumers (Manchanda et al., 1999; Liechty et al., 2001). This multivariate binomial modeling approach is commonly associated with menu-choice modeling, because it is widely utilized in commercial applications.====Ben-Akiva and Gershenfeld (1998) developed a multinomial Logit model based upon same principle, but converting the multivariate binary Logit problem into a multinomial Logit choice model for all possible bundles (which we will discuss in more detail later). This approach carries the implicit assumption that items are “locally” independent at any choice occasion, which simplifies the problem considerably, allowing for the modeling of menus of realistic size (more than 10 items), but ignoring the possibility that the value of an item depends on the other chosen items ====. As demonstrated by Schweidel et al. (2011) the multinomial Probit choice model for a given bundle or portfolio out of all possible bundles can be derived by multiplying independent multinomial Probit probabilities. For the Multinomial Logit model, Ben-Akiva and Gershenfeld (1998) demonstrated the reverse: multiple independent binary choices of items in a menu can be translated into a single multinomial Logit choice model for one bundle out of all possible combinations of selected items.====The second main (heretofore labeled the ====) approach used to model multiple simultaneous choices involves different variants of the auto-logistic model, either applied directly as originally developed in spatial statistics (Russell and Petersen, 2000; Niraj et al., 2008) or derived from utility theory (Song and Chintagunta, 2006, 2007; Mehta, 2007). An important property of the model is that the full joint choice probability is ==== the product of individual conditional choice probabilities (Besag, 1974) because it incorporates the inter-dependencies among items at a given choice occasion, thereby capturing cannibalization/synergetic effects in a menu or portfolio choice. However, while the auto-logistic approach does take the inter-dependencies among items into account, it is still limited to relatively small menus where all possible item combinations or bundles can be fully enumerated.====The model we propose in this study follows the second (====) approach. We apply the same relationship between conditional and joint choice probabilities utilized by researchers in this literature, and model menu choices directly within the context of a random-utility model, taking into account the fact that valuation of each menu item depends on all other items selected from the menu. However, in contrast to the literature on cross-category choices using this ==== framework, which study few product categories in isolation (due to infeasibility of these models for problems of more realistic proportions), we incorporate three important features in our menu-choice model, which allow us to consider larger menus while still maintaining some parsimony:====With these three improvements over existing cross-category choice models, we obtain a menu-choice model that can handle more realistic menus like those typically found in the marketplace, containing more than a few items, while accounting for heterogeneous correlated preferences and for item interdependencies within each single choice occasion.====In the next section, we briefly review approaches previously applied to model choice across multiple interdependent items. We then develop our proposed model, starting with intuitive assumptions about the necessary conditions for observed choices under utility maximization, arriving at a reasonably straightforward bundle choice model that is only feasible for very simplistic problems containing no more than a few product categories. We then discuss, test and implement strategies for making the menu-choice model feasible for problems of realistic proportions, while accounting for unobserved preference heterogeneity and for first-order interdependencies among all items in the menu in a relatively parsimonious way.====We illustrate and test our menu-choice modeling framework with extensive Monte-Carlo experiments. We then apply our menu-choice model to an actual choice experiment conducted by a business-to-business information service to study customer preferences for price plans consisting of a basic service plus equipment and up to 25 value-added services. This empirical illustration is followed by a final discussion and directions for future research.",Menu-choice modeling with interactions and heterogeneous correlated preferences,https://www.sciencedirect.com/science/article/pii/S1755534520300130,25 August 2020,2020,Research Article,71.0
"Kuriyama Koichi,Shoji Yasushi,Tsuge Takahiro","Graduate School of Agriculture, Kyoto University, Oiwake-cho, Kitashirakawa, Sakyo-ku, Kyoto, 606-8502, Japan,Research Faculty of Agriculture, Hokkaido University, Kita 9 Nishi 9 Kita-ku, Sapporo, Hokkaido, 060-8589, Japan,Graduate School of Global Environmental Studies, Sophia University, Kioicho 7-1, Chiyoda-ku, Tokyo, 102-8554, Japan","Received 23 June 2019, Revised 15 April 2020, Accepted 18 July 2020, Available online 19 August 2020, Version of Record 2 September 2020.",https://doi.org/10.1016/j.jocm.2020.100238,Cited by (7),"In this study, we apply the multiple discrete–continuous extreme value (MDCEV) model with triple constraints to identify the value of leisure time during weekends and long holidays. Our approach models the economic behavior of leisure trips with the triple constraints of budget, duration of weekend, and duration of holiday. The ","Time is one of the fundamental constraints for leisure trips. It is difficult for most workers to take a leisure trip while working, so the possibility of a leisure trip is constrained to non-working time such as a weekend or a long holiday. For a weekend trip, the maximum length of the trip is two days, Saturday and Sunday. When a leisure trip to a faraway destination requires more than two days, a long holiday has to be available. Another basic constraint is money. Consumers have to consider the expenditure allocation between leisure trips and other goods. Namely, the consumer's utility maximization problem includes at least the triple constraints: the budget, time of weekend, and time of long holiday. This study develops a behavioral and econometric model of the leisure trip decision with these triple constraints.====As leisure time is a scarce resource, people value it highly. After Bekcer's (1965) seminal work, the value of time is a primary interest in the transport economics literature.==== Jara-Díaz (2007) reviews theoretical models of the value of travel time savings. Hensher and Hotchkiss (1974) introduce the discrete choice model to estimate the value of time. Train and McFadden (1978) propose a discrete choice approach to model good and leisure allocation. Small and Verhoef (2007) review the empirical studies of the value of time in transportation. Kitamura (1984) considers the time allocation model between in-home and out-of-home activities. Bhat and Misra (1999) propose a time allocation model for weekdays and weekends.====Environmental economists have also discussed the value of leisure time and recreation demand.==== Bockstael et al. (1989) review early studies on the role of time in the recreation demand model. Feather and Shaw (1999) estimate the value of time using stated preference data of labor choice and apply the results to estimate the demand for river recreation. Larson and Shaikh (2004) analyze the relationship between the value of time and the individual's wage using trip data of whale watching. Lew and Larson (2005) estimate the value of leisure time and beach recreation demand. Larson and Lew (2013) analyze the opportunity cost of travel time in relation to salmon fishing. Fezzi et al. (2014) estimate the value of travel time using the driving route choice between open access and toll roads. However, most empirical studies of recreation demand assume that the value of leisure time is a fixed fraction of wages, following Cesario (1976), who suggests a rate of one-third of wages.====The consumer's leisure decision has multiple discrete-continuous choice characteristics. Namely, the individual may visit more than one site (multiple discrete choice), and the sites may be visited once or many times (continuous choice). In other words, leisure trips are characterized by a mixture of corner and interior solutions. For a given individual, some sites are unvisited (corner solution), whereas other sites are visited once or more (interior solution). To deal with this property of leisure trips, the Kuhn-Tucker (KT) demand model==== and the multiple discrete–continuous extreme value (MDCEV) choice model (Bhat, 2005, 2008) were proposed. Both models use the Karush-Kuhn-Tucker utility maximization condition to estimate the parameters of the utility function. The difference between these two models is the assumption that the outside good is deterministic or random (Bhat, 2008). For the KT model, the utility of the outside good (the Hicks composite good) is assumed to be deterministic. For the MDCEV model, the utility of all goods are assumed to have error components with an extreme value distribution. As Bhat (2008) shows, the MDCEV is a general model that includes a conditional logit model and a KT model for special cases. LaMondia et al. (2008) apply the MDCEV model to estimate the value of leisure time. Castro et al. (2012) propose the MDCEV model with budget and time constraints to estimate the value of time.====This study proposes the MDCEV model with triple constraints to identify the value of leisure times for weekends and long holidays. Our proposed approach has two methodological contributions to the literature. First, we develop a conceptual and econometric model of the leisure trip decision with the triple constraints of budget, duration of weekend, and duration of long holiday. We extend the MDCEV model to construct an estimation using the observed budget allocation of goods and time allocation between a weekend and a long holiday. Our proposed approach models endogenous time allocation and analyzes the substitution effect between weekends and long holidays. Second, we propose a numerical method of solving a system of nonlinear equations using the Karush-Kuhn-Tucker condition and the Markov Chain Monte Carlo (MCMC) method, which constructs the value of leisure time, demand prediction, and welfare analysis. To illustrate our proposed model, we apply it to recreation demand for national parks. The results suggest that a significantly large difference in the value of leisure time between weekends and long holidays.",The value of leisure time of weekends and long holidays: The multiple discrete–continuous extreme value (MDCEV) choice model with triple constraints,https://www.sciencedirect.com/science/article/pii/S1755534520300361,19 August 2020,2020,Research Article,72.0
"Ozhegova Alina,Ozhegov Evgeniy M.","National Research University Higher School of Economics, Research Group for Applied Markets and Enterprises Studies, Russian Federation, Perm, Studencheskaya st. 38, Russia,National Research University Higher School of Economics, Department of Economics and Finance, Center for Market Studies and Spatial Economics, Russia","Received 11 October 2019, Revised 3 July 2020, Accepted 14 July 2020, Available online 9 August 2020, Version of Record 26 August 2020.",https://doi.org/10.1016/j.jocm.2020.100237,Cited by (1)," instruments to manage theatre revenue. Since development of detailed price discrimination strategy requires data on consumer's purchase history, her behavioural and socio-demographic characteristics, we collect and combine two data sources: data on ticket purchases and data obtained from discrete choice experiment. We modify ==== logit model for joint revealed and ","Although performing art is usually considered as a non-profit industry, there are some prominent examples of self-supporting theatres like Broadway. These theatres usually produce one or two productions and run it during the season or even longer. They tend to choose more popular types of performance like musical, drama or comedy. In contrast, the non-profit theatres usually provide an extensive repertoire with a variety of performances, such as ballet, opera, musical, drama. The repertoire is set for a season and reviewed every year. Some performances drop out of the repertoire or changed by the new ones, the others keep running.====Regardless of the organisational form theatres are interested in earning revenue. The former theatres are allowed more tools to earn a profit. First, they more extensively use touring to sell their performances that allows to set higher prices for the audience. These institutions are more flexible in choosing time and place to show their performance since they are usually not tied to a particular stage. Ultimately, the process of pricing is also quite different from that for non-profit theatres. Since they run few performances at a time, the price for the spectator varies basically from the choice of seat (Towse, 2011; Zieba, 2009). Non-profit theatres are usually based on a particular stage where they run the majority of their performances. The variety of performances in a repertoire complicates the pricing mechanism since people with different preferences and income attend different performances (Towse, 1997). Thus, a flexible price strategy that would allow theatre administration to set prices based on performance characteristics and spectators’ willingness-to-pay would be a highly useful tool for both types of theatre institutions.====Theatrical productions are often characterized as perishable goods (Choi et al., 2015; Ozhegova and Ozhegov, 2017). According to Hetrakul and Cirillo (2014) the definition of perishable good includes some special aspects. The key aspect of a perishable good is that tickets for a particular play cannot be inventoried and sold after a time of play. Inflexible capacity is implicated by the limited number of seats in a house. Variable and uncertain demand assumes that the attendance on a particular performance depends on the day of the week, the time of day and the season as well as on the characteristics of production. The cost of production creation is high due to significant fixed costs on decorations, costumes, director reward, etc. Whereas the marginal cost of a particular performance is much lower as well as the marginal cost of additional attendee. These properties allow deploying the pricing strategies typically used for perishable goods.====In the revenue management perishable goods markets, the methods of price discrimination are widely used. By virtue of the fact that theatre audience is heterogeneous in terms of visit purpose, ability to perceive quality, willingness-to-pay for performance and seat, price discrimination strategy should take into account a presence of various consumer segments. Furthermore, theatrical production is a highly differentiated product, that possesses several performance and play characteristics. The seats in the house also vary by the distance to the stage, the quality of view and sound and, finally, by price. Considering these features of product and consumers, the theatre with heterogeneous consumers and differentiated performances and seats has an ability for ticket price discrimination.====The empirical literature on theatre demand modelling has evolved since the 1960s. In terms of data collection, the papers are based on either revealed or stated preferences approach. Revealed preferences approach is based on what consumers actually do and employs sales data. The majority of earlier papers utilizing purchase data use information aggregated to year or season level, region, company or venue level (Houthakker and Taylor, 1970; Touchstone, 1980; Gapinski, 1984; Bonato et al., 1990). The development of discrete choice models forwarded the issue of demand estimation making the demand modelling using individual data possible (Grisolía and Willis, 2012). The key property of demand for performing arts is that a consumer purchases a bundle of a performance and a seat in the house that has its own value depending on the type of performance. There are very few studies where the authors consider the demand for a particular seat in a house (Schimmelpfennig, 1997). In this research detailed data allow us to study the demand in great depth considering the attendance of a particular seat in a house for a particular performance. Stated preferences approach is used when the survey is the only way to address the research question. Based on survey data previous research has done extensive work concerning revealing customer segments among theatre audience (Baumol et al., 1993; Colbert and Nantel, 1989) and estimation of customer's willingness-to-pay for different attributes (Garboua and Montmarquette, 1996; Hansen, 1997; Schulze and Rose, 1998; Grisolía and Willis, 2012).====Revealed and stated preferences approaches have both strengths and weaknesses. Method of revealed preferences is based on real purchasing behaviour of a consumer, though it could present a challenge if the attributes of a product are not separable. Insufficient variation in data may lead to parameters underidentification. Stated preferences approach can solve this problem using discrete choice experiments, so that a small volume of sample may ensure sufficient variation in data. Grisolía and Willis (2016) show that combining RP and SP choices one can overcome the mutual shortcomings of the approaches. Choice experiments address the issue of insufficient variation in RP attributes, the data on real behaviour induce realism into the model. Grisolía and Willis (2016) also demonstrate that the combining RP data with SP allows to get more efficient estimates and identify the parameters for the attributes that are not observed in purchase data.====In this article, we focus on the model of individual choice in the theatre using the joint RP and SP data approach proposed by Grisolía and Willis (2016). RP data permit to account for real behaviour of a consumer. The data from the sales system do not have information on consumers apart from their behaviour in past. The inclusion of SP data provides socio-demographic information on consumers and induces necessary variation in attributes. Having rich data on customers we employ the latent class model (LCM) to reveal customer segments. Furthermore, we modify the LCM for the case of combined data where the consumer data for segments description may be observed from either revealed preferences (RP), stated preferences (SP) or both (combined RP-SP) data sources. Detection of consumer segments in such a manner leads to a fine-tuning of the pricing strategy with respect to theatre revenue. This tool would be of much use for theatrical institutions especially for those that need a flexible pricing mechanism that would factor for highly differentiated nature of product and audience. Thus, the aim of this study is to reveal theatre segments and develop pricing and marketing tools for various theatre segments.====This paper develops the idea of optimal price discrimination for various segments of the theatre audience introduced in (Baldin et al., 2018), where the authors employ revealed preferences data to optimize prices for observed segments. We extend the (Baldin et al., 2018) approach by combining RP and SP data and describing segments not only in terms of utility parameters estimates but with a wide range of demographic characteristics. This allows us to provide detailed recommendations for working with segments given more information about preferences, cultural participation and demographic features. We also develop the idea proposed in the paper of Grisolía and Willis (2016). We use a latent class model for combined RP and SP data rather than a mixed model that allows describing segments in terms of audience characteristics. Furthermore, we contribute to the literature on combination of RP and SP data (Morikawa et al., 2002; Grisolía and Willis, 2016) by developing an algorithm which utilizes partially observed data on consumer characteristics while we have a certain portion of consumers who did not participate in the SP experiment and those who do not have real purchases.====In this paper, we study consumers of Perm Opera and Ballet Theatre, one of the best regional musical theatre in Russia. The theatre is a non-commercial organization and as such is loss-making. The main source of funding is a state budget. As a noncommercial organization, the theatre is interested in keeping prices affordable for Perm residents, though it has an obligation to cover at least 10% of its expenses by the revenue from tickets sale. All the tickets in the theatre could be purchased either physically in the ticket office or online through the theatre's website. In the study, we focus on online ticket purchases since these data keep information about the history of purchases and provide extensive possibilities for analysis of consumer behaviour. Online purchases allow to follow up the history of spectator's attendance including frequency, price of bought tickets, location of bought seats in a house and attended performances. Furthermore, the online purchases constitute about 70% of all the tickets purchases and this share is constantly increasing. Having the detailed data on consumers who purchase online and average characteristics of audience who attend the theatre mostly obtained from a number of surveys conducted in the theatre we may conclude that on the average the characteristics of online buyer do not differ from the average characteristics of theatregoer. It allows to generalize the results for the whole theatre audience.====Since the current pricing policy of the theatre assumes simultaneous proportional price change for all seats in the house, the actual data do not allow to correctly identify substitutional patterns between seats in the theatre. Data on internet purchases also lack socio-demographic consumer characteristics. Absence of consumer characteristics does not allow to describe consumer segments. Along with the real purchasing behaviour, we collect data from a discrete choice experiment that allows to induce variation in the tickets price and makes the dataset richer adding the consumer characteristics. Combined RP and SP dataset allows to obtain insights about consumer segments and its preferences towards seats in a house and performance characteristics.====The rest of the paper is organized as follows. The next part outlines the methodology. It is followed by the section explaining the data. Then we describe obtained empirical results and its managerial implication. The last section concludes.",Segmentation of theatre audiences: A latent class approach for combined data,https://www.sciencedirect.com/science/article/pii/S175553452030035X,9 August 2020,2020,Research Article,73.0
"Wang Shenhao,Wang Qingyi","Massachusetts Institute of Technology, 77 Mass Ave, Cambridge, MA, USA","Received 30 August 2019, Revised 4 July 2020, Accepted 8 July 2020, Available online 8 August 2020, Version of Record 20 August 2020.",https://doi.org/10.1016/j.jocm.2020.100236,Cited by (20)," (MTLDNNs) as an alternative framework, and discusses its theoretical foundation, empirical performance, and behavioral intuition. We first demonstrate that the MTLDNNs are theoretically more general than the NL models because of MTLDNNs’ automatic feature learning, flexible regularizations, and diverse architectures. By analyzing the adoption of autonomous vehicles (AVs), we illustrate that the MTLDNNs outperform the NL models in terms of prediction accuracy but underperform in terms of cross-entropy losses. To interpret the MTLDNNs, we compute the elasticities and visualize the relationship between choice probabilities and input variables. The MTLDNNs reveal that AVs mainly substitute driving and ride hailing, and that the variables specific to AVs are more important than the socio-economic variables in determining AV adoption. Overall, this work demonstrates that MTLDNNs are theoretically appealing in leveraging the information shared by RP and SP and capable of revealing meaningful behavioral patterns, although its performance gain over the classical NL model is still limited. To improve upon this work, future studies can investigate the inconsistency between prediction accuracy and cross-entropy losses, novel MTLDNN architectures, regularization design for the RP-SP question, MTLDNN applications to other choice scenarios, and deeper theoretical connections between choice models and the MTLDNN framework.","For decades, researchers have been combining revealed preference (RP) and stated preference (SP) data to analyze individual behavior, owing to their complementary properties. RP data are thought to have stronger external validity but often lack the variation in attributes or alternatives, while SP data often incorporate new attributes or alternatives but lack strong external validity. As a classical method, the nested logit (NL) model has been commonly used to combine RP and SP by assigning their alternatives to two nests with different utility scale factors (Hensher and Bradley, 1993; Bradley and Daly, 1997; Ben-Akiva and Morikawa, 1990; Ben-Akivaet al., 1994).==== However, in the NL model, researchers need to analyze RP and SP by handcrafting the model structure, which can be too restrictive to capture the complex data generating process. This handcrafted feature engineering is different from the mechanism in deep neural networks (DNNs) (LeCun et al., 2015; Bengio et al., 2013; Collobert and Weston, 2008), which can automatically learn generalizable features to achieve outstanding predictive performance across disciplines (Fernández-Delgadoet al., 2014; Krizhevsky et al., 2012; LeCun et al., 2015). The recent innovations in DNNs prompt us to investigate the possibility of using a DNN framework to address the classical problem of combining RP and SP, as an alternative to the traditional NL method.====This study presents a framework of multitask learning deep neural networks (MTLDNNs) to jointly analyze RP and SP, demonstrating MTLDNNs' theoretical flexibility, empirical performance, and behavioral intuition. A MTLDNN architecture starts with shared layers capturing the similarities between RP and SP, and ends with task-specific layers capturing their differences (Fig. 1) (Caruana, 1997). We first demonstrate that MTLDNNs are theoretically more general than NL owing to their automatic feature learning, soft constraints, and diverse architectures. Then we apply the MTLDNN framework to a data set collected in Singapore, which was designed to analyze the adoption of autonomous vehicles (AVs). In the empirical experiments, we compare the MTLDNNs to two NL benchmarks using prediction accuracy and cross-entropy loss.==== To understand the determinants of AV adoption, we visualize the relationship between choice probabilities and input variables and compute the elasticity values using MTLDNNs’ gradients information (Baehrenset al., 2010; Simonyan et al., 2013). Overall, our analysis demonstrates that the MTLDNNs are theoretically appealing in leveraging the shared information between RP and SP, and are capable of revealing meaningful behavioral patterns, although the gain in empirical performance, particularly measured by cross-entropy losses, is still limited.====This study contributes to the choice modeling community by being the first to present the MTLDNN framework in the important context of combining RP and SP. Future studies can investigate deeper theoretical and empirical questions revolving around this topic. Particularly, future researchers should investigate the inconsistency between prediction accuracy and cross-entropy losses, because the two metrics represent the different perspectives from machine learning and classical choice modeling. Researchers can also investigate the classical theoretical question (e.g. modeling the structure of random utility terms of RP and SP) under this MTLDNN framework and improve the empirical performance of this study by using advanced MTLDNN architectures (Long and Wang, 2015; Hashimotoet al., 2016; Misraet al., 2016; Ruder12et al., 2017). Future studies can also apply the MTLDNN framework to other choice scenarios, such as jointly analyzing car ownership and travel mode choice (Train, 1980; Zegras, 2010), activity patterns and trip chain choices (Kitamuraet al., 1992; Golob and McNally, 1997), and many others that are traditionally analyzed by structural equation models (SEM). For future researchers to replicate and improve upon our work, we have uploaded the project to Github.====This paper is organized as following. Section 2 reviews the MTLDNN and NL models. Section 3 presents the MTLDNN framework and compares its theoretical properties to the NL models. Section 4 presents data and methods, and Section 5 analyzes model performance and presents the economic information in MTLDNNs. Section 6 concludes our findings and discusses future research directions.",Multitask learning deep neural networks to combine revealed and stated preference data,https://www.sciencedirect.com/science/article/pii/S1755534520300348,8 August 2020,2020,Research Article,74.0
Park Minjung,"Department of Economics, Ewha Womans University, 52 Ewhayeodae-gil Seodaemun-gu Seoul, Republic of Korea","Received 27 March 2020, Revised 1 July 2020, Accepted 29 July 2020, Available online 7 August 2020, Version of Record 12 August 2020.",https://doi.org/10.1016/j.jocm.2020.100247,Cited by (2),"Estimating installed-base effects for product adoption in the presence of unobserved heterogeneity is challenging since the typical solution of including ==== leads to inconsistent estimates in models with installed base. Narayanan and Nair (2013) highlight this problem and propose a bias correction method as a solution to the problem. This research note proposes an alternative solution: Borrowing IVs from the dynamic panel data literature. As lags and lagged differences of the installed base are used as instruments after first-differencing, this approach does not require external instruments and therefore has the key advantage of being easily accessible in many settings. I present Monte Carlo results to demonstrate the performance of the proposed approach.","How an individual’s product adoption is influenced by the behavior among the individual’s reference group is an important topic. A specification with installed base is widely used to empirically answer such a question, where peer influence is measured as the impact of the size of past adopters (i.e., installed base) in the reference group on the individual’s choice (Iyengar et al., 2011, Bollinger and Gillingham, 2012, Narayanan and Nair, 2013). A key challenge in estimation of causal installed-base effects is homophily, a phenomenon that group members share similar preferences and thus tend to behave similarly.====Narayanan and Nair (2013) make an insightful observation that including group fixed effects to account for homophily leads to inconsistent estimates in models with installed base, and propose an IV approach and a bias correction approach as possible solutions. While they examine both, their main focus is on the bias correction approach, as they view the IV approach as infeasible in many contexts due to “the difficulty of finding exogenous variation that shifts the installed base over time but holds current adoption fixed”, a perception that seems fairly widespread in the literature (e.g., Bollinger and Gillingham (2012)).====This research note argues that borrowing insights from the dynamic panel data literature provides us with instruments that would be available in many settings. In dynamic panel data models, including the lagged dependent variable as a regressor leads to inconsistency of fixed effects estimator, and researchers have proposed to use lags and lagged differences of the lagged dependent variable as instruments after first-differencing (Anderson and Hsiao, 1981, Arellano and Bond, 1991). I argue that similar types of instruments can be used in the considered model. First-differencing is used to remove homophily, and the ensuing endogeneity is addressed by using lags and lagged differences of the installed base as IV. This approach has the key advantage of relying on only internal variables to generate instruments.====Narayanan and Nair (2013) note the close similarity between the considered model and dynamic panel data models, but suggest that finding IVs would be in general difficult in the considered setting. The contribution of this paper is to highlight an IV approach borrowed from the dynamic panel data literature as a readily available and feasible solution, thereby offering a nice complement to the bias correction approach.",Estimating installed-base effects in product adoption: Borrowing IVs from the dynamic panel data literature,https://www.sciencedirect.com/science/article/pii/S1755534520300440,7 August 2020,2020,Research Article,75.0
"Scott Anthony,Witt Julia","Melbourne Institute, Applied Economics and Social Research, The University of Melbourne, Parkville, 3010, VIC, Australia,Department of Economics, University of Manitoba, Winnipeg, MB, R3T 5V5, Canada","Received 11 March 2019, Revised 18 June 2020, Accepted 25 June 2020, Available online 30 July 2020, Version of Record 17 August 2020.",https://doi.org/10.1016/j.jocm.2020.100230,Cited by (0),"This paper tests for the existence of loss aversion, reference dependence and diminishing sensitivity in a discrete choice experiment (DCE). A status quo alternative is introduced in a DCE of nurses' job choices and modeled as an individual-specific third alternative representing the respondent's current job. This provides a feasible method for including a status quo, which changes the reference point for each respondent. The increased salience of the status quo changes the size of any losses or gains when comparing Job A or Job B with their current situation, and since losses are valued more than gains, affects the marginal utility of each attribute. Models that differentially incorporate loss aversion, reference dependence and changes in sensitivity yield varying estimates of marginal rates of substitution, suggesting that consideration of these effects is important, particularly when policy implications are sought.","The design of policies and interventions to influence decisions requires an understanding of how decisions are made. For example, attempting to move people from their current set of choices to a new set of choices depends, according to standard economic theory, on the changes in costs and benefits. Economic theory assumes an attribute of a good or service is valued independently of context and that individuals have well-formed preferences (Hess et al., 2018; Shiell et al., 2000). Thus, choice tasks should generate attribute rankings that are consistent regardless of how they were elicited, but this is not observed in practice. Prospect theory (Kahneman and Tversky, 1979) recognises that reference points influence choices, and movements away from the reference point are evaluated in terms of gains and losses relative to this point. In a series of experiments, individuals were found to be more sensitive to losses than to gains when evaluating their options, and were generally risk-seeking when outcomes were losses, and risk-averse when outcomes were gains. This implies an s-shaped value function, where losses lead to larger reductions in value than equivalent sized gains. Using prospect theory, Thaler (1980) introduced the concept of the endowment effect that also helped explain the importance of the difference in value between losses and gains. If individuals view losses as out-of-pocket costs, and gains as forgone opportunity costs, then a good taken away from an individual's endowment will be weighted more heavily than the same good added to their existing endowment. Samuelson and Zeckhauser (1988) discuss status quo bias: decision makers choose the alternative that was their current situation more frequently than predicted by a rational choice model. They cite a range of examples from observations and experiments that support that status quo biases are ubiquitous and significant, and that the bias increases with the number of choice alternatives. The design of their experiments allows them to conclude that status quo bias is generally present, not just as a result of loss aversion (Samuelson and Zeckhauser, 1988). De Borger and Fosgerau (De Borger and Fosgerau, 2008) introduced a theoretical framework that maps choice pairs to different welfare measures and used this to compare four commonly used valuation measures that represent gains and losses in two dimensions from a reference point. They find loss aversion, reference dependence and diminishing sensitivity in an empirical application using data from a survey of binary choices where respondents trade off money and time. They also find that “mistakes”, defined as choosing a dominated alternative, are much more common when the reference scenario is dominated. In other words, respondents are more likely to choose their status quo, even if it is a dominated scenario.====The aim of this paper is to examine the effect of the status quo in the context of a discrete choice experiment (DCE). We present an application that integrates a scenario describing each respondent's current situation (their status quo) consisting largely of qualitative attributes into a DCE. We use data from a DCE of job preferences among nurses, where respondents were asked to choose their preferred alternative from a set of hypothetical scenarios, then asked to choose again from the same hypothetical scenarios and their current situation. The ‘current situation’ is an additional alternative reconstructed for each respondent from answers to questions that were phrased with the same description of attributes and levels used in the DCE, which offers a simple way to integrate any reference scenario into discrete choice experiments without loss of data.","Loss aversion, reference dependence and diminishing sensitivity in choice experiments",https://www.sciencedirect.com/science/article/pii/S1755534520300294,30 July 2020,2020,Research Article,76.0
"Lu Hui,Rohr Charlene,Howarth David,Pollitt Alexandra,Grant Jonathan","RAND Europe, Westbrook Centre, Milton Road, Cambridge, CB4 1YG, United Kingdom,Department of Land Economy, University of Cambridge, Room 5 House 18, Silver Street, Cambridge, CB3 9EP, United Kingdom,Policy Institute, King's College London, 22 Kingsway, London, London, WC2B 6NR, United Kingdom","Received 15 December 2018, Revised 23 June 2020, Accepted 2 July 2020, Available online 24 July 2020, Version of Record 14 August 2020.",https://doi.org/10.1016/j.jocm.2020.100233,Cited by (1), negotiation positions.,None,What sort of Brexit do the British people want? A longitudinal study examining the ‘trade-offs’ people would be willing to make in reaching a Brexit deal,https://www.sciencedirect.com/science/article/pii/S1755534520300324,24 July 2020,2020,Research Article,77.0
"Ntuli Herbert,Muchapondwa Edwin,Okumu Boscow","School of Economics, University of Cape Town, Private Bag, Rondebosch 7701, Cape Town, South Africa,The National Treasury and Planning, Treasury Building, Harambee Avenue, P. O. Box 30007-00100, Nairobi, Kenya,WWF South Africa, South Africa,Department of Business Administration, Technology and Social Sciences, Luleå University of Technology, Sweden,Environment for Development (EfD) Centre, School of Economics, University of Nairobi, Kenya","Received 24 September 2018, Revised 28 April 2020, Accepted 28 June 2020, Available online 23 July 2020, Version of Record 18 August 2020.",https://doi.org/10.1016/j.jocm.2020.100231,Cited by (4),"Wildlife is widely becoming an important vehicle for rural development in most third-world countries across the globe. With wildlife, as with other conservation and ","Wildlife conservation is increasingly becoming important for the livelihoods of poor rural households living adjacent to national parks in Southern Africa (Ntuli and Muchapondwa, 2018). There is great potential for rural economies in the region to grow at a faster pace than current rates through both consumptive and non-consumptive tourism. Despite this fact, most rural economies in the region are struggling to cope with high unemployment, incessant poverty and inequality, while poaching continues unabated. Studies have also found a strong linkage between rural-urban migration and limited opportunities in the region and communities located near protected areas are not an exception to this phenomenon (Moro et al., 2013). These challenges are exacerbated by the marginalization of indigenous communities through laws and policies inherited from the colonial epoch, and as such deriving substantial benefits from conservation becomes extremely difficult. As a result, there is increasing pressure from interest groups to involve local communities in wildlife conservation. In fact, scholars such as Muchapondwa (2003) and Dikgang and Muchapondwa (2017) argue that harnessing the tourism potential and involving indigenous people in wildlife conservation might address some of these social problems. Although there has been an effort to integrate local communities in wildlife management, the outcome has not been favourable in some parts of the region. Several studies done in the region documented either limited success or total failure in community wildlife conservation (e.g., Benjaminsen et al., 2013; Balint and Mashinya, 2008, 2006; Goldman, 2003).====Scholars argue that problems associated with community wildlife conservation are inherent in the design of most Integrated Conservation and Development Programmes (ICDP) in developing countries (Du et al., 2015; Atela et al., 2015; Bauch et al., 2014; Romero et al., 2012; Garnett et al., 2007; Hughes and Flintan, 2001). Furthermore, policies are fervently designed and endorsed without consulting other important stakeholders such as local communities. In addition to nonalignment of incentives,==== government and donor funded projects frequently fail because of lack of ownership, which occurs when local communities are side-lined during project conception, design and implementation (Campbell and Vainio-Mattila, 2003). Quite often, policymakers and development practitioners are concerned with supply side interventions, yet communities have their own way of perceiving costs and benefits associated with such type of initiatives. The assumption is that rural communities will embrace any project simply because they are poor, and their options are limited, but evidence proves otherwise.====The Communal Areas Management Programme for Indigenous Resources (CAMPFIRE) is a good example of a supply side intervention initiated by the government of Zimbabwe, whose aim was to strike a balance between development and conservation objectives. CAMPFIRE allows local communities living adjacent to national parks to contribute meaningfully towards wildlife conservation, while at the same time deriving income under benefit-sharing arrangements with their Rural District Councils (RDCs). Unlike most countries in the region, the government handed over appropriation and management rights to local communities through their respective RDCs in a partial devolution exercise, which started during the mid-1980s (Murombedzi, 1999). The government seems reluctant to take the devolution exercise to a next step by transferring appropriation authority from RDC to the community level as previously envisioned by CAMPFIRE. As a result, CAMPFIRE has failed to generate adequate incentives for indigenous communities to take conservation work seriously since local people are viewed as a threat rather than important stakeholders with an interest in wildlife conservation. Compared to the private game farming communities in neighbouring conservancies, CAMPFIRE offers local communities in the study area lesser autonomy and fewer management rights to wildlife.====A system that does not involve state arms such as RDCs does not only level the playing field by transferring power to local communities, but also ensures that revenue is not dissipated through corruption and mismanagement of resources. Transferring appropriation rights to producer communities means that they will earn more income from selling hunting licences directly to safari operators rather than going through RDCs. Interesting developments are happening in this regard in Mahenye ward==== located in Chipinge RDC, which has recently approached a court of law seeking to run CAMPFIRE activities independently from the RDC and to be declared the Appropriate Authority==== for wildlife in the area because of the slow pace of the process and unwillingness by the state to hand over power over the area's natural resources. This shows the extent to which some local communities are willing to go in order to enjoy autonomy in managing their natural resources.====As with many other interventions, it is uncertain what sort of CAMPFIRE institutions==== the local communities really want. Hence, the main focus of the study is to investigate local communities’ preferences over key institutional attributes that can be modulated to produce alternative CAMPFIRE regimes. Specifically, we ask, do producer communities care about increased devolution and authority that will allow them to manage wildlife under CAMPFIRE on their own with minimal state interference? If they do and reforms would come at a cost, how much would they be willing to pay for different institutional attributes which can be varied to configure a CAMPFIRE regime they desire? In addition, what are the specific values of the key institutional attributes that would generate a Community-Based Natural Resource Management (CBNRM) model that producer communities would prefer?====It is important that communities get the configuration of CAMPFIRE they want because it has potential to enhance wildlife conservation and improve rural livelihoods (Muchapondwa, 2003). CAMPFIRE's positive impact on wildlife conservation largely comes through discouraging poaching by communities and others they might assist, particularly in the buffer zone between the protected area and communal land. Subsistence poaching is attributed to local people, while illegal trophy hunting is linked to sophisticated poachers from distant places, but with inside help from local communities. Local communities can only reduce/prevent poaching when the benefit associated with wildlife conservation is greater than the cost of living with it.==== Thus, the incentives associated with the design of the programme (cost-benefit structure) act as an important linkage between the uptake of CAMPFIRE and reduction in illegal hunting activities. Muchapondwa (2003) reported that CAMPFIRE initially seemed to achieve the desired conservation outcomes, as poaching initially subsided. However, poaching later gained momentum as expected incentives slowly diminished due to inadequacies in the design of the scheme, lack of adherence to planned activities or lack of full implementation of the programme.====In order to gain insight into local communities’ preferences over key institutional attributes, we designed and conducted a choice experiment (CE) survey in communities around the Gonarezhou National Park in Zimbabwe paying particular attention to the guidelines for best practice in stated preference (SP) surveys (Johnston et al., 2017). CEs are used extensively in different strands of literature to investigate preferences and demand (Moro et al., 2013; Hanley et al., 2009). The attribute-based technique is appropriate for this study because it extracts information about household preferences for various institutional attributes associated with CAMPFIRE (Johnston et al., 2017).====To differentiate our study from previous work, we focused our attention on the wildlife sector to study household preferences for various institutional attributes of a wildlife management scheme in the context of a developing country. The relevant trade-off under evaluation in this choice experiment is greater devolution across institutional attributes such as appropriation rights, monitoring and enforcement, collective choice arrangements and governance leadership to promote integrated conservation and development in exchange for local communities' willingness to pay (WTP) to support the devolution policy, where alleviation of poaching is a positive by-product. It is not clear how CAMPFIRE communities would react when facing an option with more rights==== compared to status quo, but at a cost. To investigate this, the study also uses a Contingent Valuation scenario to assign economic values on the current CAMPFIRE regime and a futuristic fully devolved CAMPFIRE regime that resembles the conservancy community which is known to achieve superior outcomes (Ntuli and Muchapondwa, 2018). These results are used to assess the producer communities’ willingness to take full control over wildlife conservation. Our results provide pragmatic evidence to policymakers and rural development practitioners alike about the preferences of local communities and feed into future policy reforms in the wildlife sector. Our study builds on the work of previous studies that contributed to the improvement of the design of CAMPFIRE (Ntuli, 2015; Frost and Bond, 2008; Muchapondwa, 2003) and ICDP schemes in Africa (Johannesen, 2007; Johannesen and Skonhoft, 2005).====The rest of the paper is structured as follows. Section 2 describes the design of the CE and attributes. Section 3 presents the econometric framework. The results are discussed in section 4, and Section 5 concludes the paper.",Can local communities afford full control over wildlife conservation? The case of Zimbabwe,https://www.sciencedirect.com/science/article/pii/S1755534520300300,23 July 2020,2020,Research Article,78.0
"Dudinskaya Emilia Cubero,Naspetti Simona","Department of Agricultural, Food and Environmental Sciences (D3A), Università Politecnica delle Marche, Via Brecce Bianche, 60131, Ancona, Italy,Department of Materials, Environmental Sciences and Urban Planning (SIMAU), Università Politecnica delle Marche, Via Brecce Bianche, 60131, Ancona, Italy","Received 31 December 2019, Revised 27 May 2020, Accepted 2 July 2020, Available online 15 July 2020, Version of Record 18 July 2020.",https://doi.org/10.1016/j.jocm.2020.100232,Cited by (6),"Researchers using discrete choice experiments (DCE) are often faced with the difficult decision of selecting which are the key attributes that must be included into their analysis. Previous literature on methods for attribute selection is not particularly well documented, frequently leaving researchers with a wide choice of attributes that could lead to complex choice tasks. Moreover, selecting attributes that might be ignored by the respondents might generate biased results, especially if attribute non-attendance is not taken into consideration. In this paper, we offer a framework for the selection of key attributes using eye-tracking software. Our main objective is to investigate if eye movements during the completion of a choice experiment can provide additional information to select the attributes in designing a DCE. Pretesting the DCE by an on-screen survey tool and eye-tracking, we implemented three multinomial logit models (MNL) to compare the ==== of the respondents, the self-reported statements on non-attendance and their visual attention to each attribute. The eye-tracking data revealed that all respondents looked at most attributes for most of the time. However, attention is different from attendance. Results show that eye-tracking can be a complementary method to self-reported statements by providing key information for reducing task complexity and potential attribute non-attendance in designing a DCE (one from seven attributes was dropped).","Previous studies in diverse fields have recognized that only a subset of the attributes is considered by the respondents when making discrete choices (Campbell et al., 2011; Heidenreich et al., 2018; Scarpa et al., 2013). As the complexity of the choice task increases (more attributes or levels), the greater is the cognitive burden of the respondent (Hensher, 2006). To simplify such complex decisions, respondents may follow a variety of decision rules or “heuristics” (Ares et al., 2013; Glaholt et al., 2010; Mawad et al., 2015; Orquin and Mueller Loose, 2013), from which a potential strategy might be to ignore certain attributes when selecting between alternatives (Balcombe et al., 2015). This is what is known in discrete choice experiments (DCE) as attribute non-attendance (ANA) (Scarpa et al., 2013), a key problem in which respondents do not attend to all the attributes. As a result, respondents are not able to make the assumed trade-offs between all those attributes, resulting in a violation of the continuity axion (Lancaster, 1966; McFadden, 1974) and leading to biased results (Caputo et al., 2016; Van Loo et al., 2018).====Although there are no restrictions on the number of attributes that can be included in a DCE, researchers are encouraged to keep the number of attributes low (Meiβner and Decker (2010) suggest no more than six attributes) to avoid cognitive biases in respondents, which could generate impaired predictivity validity on the DCE. However, the process of attribute selection for DCE is often unclear or poorly reported (Coast et al., 2012). The most common methods to identify attributes include: literature review (Agnoli et al., 2016), previous reports (Apostolidis and McLeay, 2016; Green and Gerard, 2009), qualitative studies as focus groups and interviews (Childs and Drake, 2009; Mühlbacher et al., 2017), professional and experts recommendations (Adanacioglu and Albayram, 2012; Janssen et al., 2016), among others. Nevertheless, these approaches frequently result in a long list of attributes from which researchers still have to choose which and how many of those attributes are truly of interest for the respondent. This is a critical aspect of the design, as the validity of the study depends on the correct specification of relevant attributes (Mangham et al., 2009).====Our main objective was to investigate if, during the completion of a choice experiment, the study of eye movements can provide additional information in identifying the attributes that consumers are interested in. Using eye-tracking technology, during a DCE we compared the stated preferences of the respondents in the DCE, the self-reported statements on non-attendance (Scarpa et al., 2013) and their visual attention to each attribute (Van Loo et al., 2018).====Eye-tracking, via the recording of what a respondent is looking at, has been widely used in marketing and psychology literature to understand decision processes (i.e. Chandon et al., 2009; Glaholt et al., 2010). Eye-tracking offers important advantages in obtaining additional information from consumer research (Spinks and Mortimer, 2015). It does not requires the researcher to ask the participants to convey their judgements deduced from their visual stimulus (i.e. looking at the product). Individuals tend to have different personal skills (introspection, oral expression) which might affect the way the participants express their ideas or their objectivity. Using eye-tracking eliminates these problems, as it does not depends on the participants sensory capabilities, memory or communication skills (Rebollar et al., 2015). However, results must be interpreted carefully, as eye-tracking technology has also some limitations, as the lack of realism (the experiment takes place in a laboratory on a computer screen) and the use of a limited sample.====Given the advantages that the eye-tracking technology offers, many researchers have started to implement eye-tracking in many consumer studies. Krajbich et al. (2012) used eye-tracking to study consumer purchasing decisions within the attentional drift-diffusion model. Van Loo et al. (2015) investigated respondents visual attention, while Uggeldahl et al. (2016) analysed respondents choice certainty. Rasch et al. (2015) focused on the role of affect, while Meiβner et al. (2016) identified a small but consistent attribute focus on positive aspects of the item chosen and negative aspects of the items not chosen.====Notwithstanding the advantages that eye-tracking offers, only a few studies have used this technology in designing and pretesting DCEs. This is unfortunate, as using eye-tracking could be particularly insightful in the selection of key attributes and the identification of attribute non-attendance (Spinks and Mortimer, 2015). In particular, eye-tracking allows the researcher to monitor the way in which the participants engage with the DCE survey instrument while supplementing the role of ex-post debriefing questions about ANA with more objective measures, as ex-post questions might not always represent an accurate response of actual behaviour (Balcombe et al., 2015). In brief, eye-tracking offers researchers an additional tool to study and prevent ANA in DCEs.====To the best of our knowledge, Balcombe et al. (2015) were the first to use eye-tracking technology to investigate ANA in DCE's. The authors addressed the concordance between visual attention and stated attribute non-attendance by including visual and stated ANA information into the estimation model. Van Loo et al. (2018) later extended their work, including alternative fixation count cut-offs, alternative definitions of visual ANA, and two alternative modelling approaches (including coefficients of ignored attributes and coefficients of variation).====We further extended the work of Balcombe et al. (2015) and Van Loo et al. (2018) in three main aspects. First, following the recommendations of Balcombe et al. (2017), we proposed the use of eye-tracking during the DCE's design and piloting phase. The objective of the study was to optimize the experimental design by offering and additional tool that could assist the researcher in the selection of the attributes that the respondents would actually be interested in. Second, and based on Van Loo et al. (2018) suggestion, we compared visual ANA to self-stated ANA and increased the complexity of the choice experiment by adding nutritional labels and graphics that compete for the respondents attention and attendance, making it more difficult for the participants to attend to all the information. It is important to distinguish attention from attendance, as respondents may have paid attention to an attribute but this does not means that they actually attended the attribute, either because the attribute plays no role on the respondents choices or because only a subset of the information was relevant to the respondent (Balcombe et al., 2015). This issue is key, as in real life there are plenty of labels that consumers are exposed to, and given that this choice experiment was at the piloting stage, including so many attributes allowed us to actually understand which were the attributes that respondents were really interested in.====Third, we extended the model proposed by Balcombe et al. (2015) by including attribute specific shrinkage parameters. We provide a novel, different approach to pretesting DCEs than those offered in the literature (De Brún et al., 2018) and our results offer an insight into the use of various types of ANA data in early stages of a DCE design, as well as different ways to analyse it.",Using eye-tracking as an aid to design on-screen choice experiments,https://www.sciencedirect.com/science/article/pii/S1755534520300312,15 July 2020,2020,Research Article,79.0
"Hancock Thomas O.,Broekaert Jan,Hess Stephane,Choudhury Charisma F.","Choice Modelling Centre and Institute for Transport Studies, University of Leeds, United Kingdom of Great Britain and Northern Ireland","Received 27 September 2019, Accepted 1 July 2020, Available online 15 July 2020, Version of Record 22 July 2020.",https://doi.org/10.1016/j.jocm.2020.100235,Cited by (9)," ‘changing perspectives’ in choice contexts with salient moral attributes. We apply these models to two distinctly different case-studies. In the first, respondents have to make choices between route alternatives with variable ‘concrete’ and ‘moral’ attributes — Chorus et al. (2018)’s ‘taboo trade-off’ between time-cost and deaths-injuries. The second study investigates how an individual weighs wages and commuting times for themselves relative to the wages and commuting times for their partner. Under both scenarios, we find that the flexibility provided by quantum choice models allows them to accurately capture and formally explain choices across the differing contexts.","Moral choice scenarios can be summarised as those where the choices or actions a decision-maker takes could negatively impact other individuals. Thus, to the decision-maker, the choice alternatives may to some extent be categorised as ‘right’ or ‘wrong’, depending on how serious (and possibly how likely) the consequences are. As a result, the associated choices can perhaps be more complex as they do not involve straightforward trade-offs between rather concrete attributes of alternatives. For example, a decision-maker may not choose the alternative that they would choose based on more attractive concrete features as they believe it to be an overall morally contentious option. Alternatively, a set of options may all have negative features, where different schools of moral thought suggest different actions should be taken (for example, Awad et al., 2020 discuss country-level variations in decision-making in ‘moral machine’ choice tasks).====While moral choice behaviour has received much attention in economics and psychology, it is rarely considered in the choice modelling literature (see Chorus 2015 for a detailed discussion). This is despite the fact that many typical experiments conducted to understand or interpret an individual’s preferences in moral choice scenarios use paradigms such as variations of the well-known trolley problem (where a ‘runaway trolley’ has two possible paths, both of which will result in the death of some individual(s), and the decision-maker must choose who to save), for which a precise understanding of the trade-offs that are being made could be obtained using choice models. This is perhaps due to the fact that an individual’s moral preferences are difficult to investigate outside of the laboratory, with typical experimental methods for examining moral choice scenarios often suffering from low external validity (Bauman et al., 2014). However, more recently, moral choice behaviour has become more prominent in the travel behaviour modelling community through, for example, the reinvention of the trolley problem as a self-driving car problem (Awad et al., 2018). Thus far, there has not been much consideration given to the types of choice models used for the modelling of such scenarios, despite the wide range of theoretical explanations for moral behaviour that have been proposed (Chorus, 2015). However, some steps towards the development of choice models specifically for moral choice contexts have been made (Chorus et al., 2018).====In this paper, we specifically look at models based on quantum probability theory. These have not yet been applied to moral choice scenarios, despite the adoption of such methods ‘allowing for a re-examination of the challenge of formalising psychological concepts of conflict, ambiguity, and uncertainty’ (Wang et al., 2013). Quantum probability theory has recently made a significant impact in cognitive psychology (Bruza et al., 2015). This impact is in part due to the underlying logic of quantum probability theory which revealed a fundamental lack of distributivity of propositions concerning non-compatible features of an observed system (Birkhoff and Von Neumann, 1936). This key difference between classical and quantum logic reveals that under quantum theory, the law of probability following the distributivity of ‘and’ and ‘or’ of propositions – ==== – may fail to hold (for a detailed example, see Hancock et al. 2020). Another essential difference follows from the description of a system by using state vectors with complex-valued components which entail the occurrence of interference effects when such states are superposed, famously leading to the paradoxical state of Schrödinger’s cat being both dead and alive at the same time in a historical thought experiment devised to point out the consequences of the entanglement of the quantum system and its observer (Schrödinger, 1935). In effect, the measurement of a property of a system occurs differently, namely by applying projection operators on the state vector of a system which inherently ‘changes the system by making an observation’ — as opposed to simply reading of the value of a pre-existent property of the system. Crucially, these features mean that the adoption of quantum probability theory allows for a powerful and elegant framework for modelling and understanding many ‘paradoxical’ findings which become ‘intuitive’ (Wang et al., 2013), such as probability judgement errors (Busemeyer et al., 2011), question ordering effects (Trueblood and Busemeyer, 2011) and violations of the ‘sure thing principle’ (Pothos and Busemeyer, 2009, Broekaert et al., 2020). A classic example of a probability judgement error is given by Tversky and Kahneman (1983), who found that participants, after reading ‘Linda was a philosophy major. She is bright and concerned with issues of discrimination and social justice’, were more likely to agree with the statement ‘Linda is a feminist bank teller’ than the statement ‘Linda is a bank teller’. This subjective assessment clearly contradicts logical set theory in which the category “feminist bank teller” is a subset of the category “bank teller”, and hence on probabilistic grounds of set membership, this should lead to a lower association of Linda with the former category.====With, for example, ordering effects also frequently observed in choice modelling applications, it is unsurprising that quantum models have also since made the transition into choice modelling (Lipovetsky, 2018). Furthermore, quantum models can be used to accurately capture the ‘change of decision context and mental state’ when moving between choices made under revealed preference and stated preference settings (Yu and Jayakrishnan, 2018). Additionally, it has been demonstrated that quantum probability theory can be implemented into choice models to accurately understand route choice problems as well as best–worst choice behaviour in the context of alternative routes (Hancock et al., 2020). Thus there appears to be ample scope for further developments of quantum choice models, with our previous development of the notion of a ‘quantum rotation’ within a choice model providing useful transitions across choice contexts. The aim of this paper is to build on work presented in Hancock et al. (2020), which focussed solely on typical travel behaviour data, by testing these models on more complex choice scenarios. We specifically test whether these rotations and other quantum choice model features can equivalently be used to accurately capture changes in choice context within moral choice scenarios.==== ====We apply the models to two very different datasets. The first allows us to test whether quantum choice models can be used to capture the impact of the presence of a ‘taboo trade-off’ (Chorus et al., 2018) involving trade-offs between ‘moral’ and ‘concrete’ features.==== ==== The moral attributes of the route alternatives appeal to the personal sense of ==== versus ==== grounded in the decision-maker’s socio-cultural and philosophical or religious association — like the personal answerability or blame for opting for a route alternative with a higher expected number of deaths or severely injured travellers. The concrete attributes on the other hand call for a more pragmatic material utility which a priori does not ponder rightness or wrongness of the choice — like for instance the additional time on a route alternative. It goes without saying that these categories may well be perceived as intertwined; a faster route alternative with implicit detrimental environmental effects can appeal to the decision-maker’s ethical principles. Vice versa, a utilitarian based ethical approach held by a decision-maker could result in equating moral attributes with pragmatic features of the alternative.====The second dataset tests whether quantum choice models can be used to capture differences between how an individual weighs wage and commuting times for themselves relative to considering the wages and commuting times for both their partner and themselves, developed by Swärdh and Algers (2009), with descriptions also in Beck and Hess (2016). An aspect of morality is again appealed to in this experimental paradigm. The consideration of the partner’s situation may appeal to the decision-maker’s empathy or selfishness with respect to the partner, or, a particular balanced choice may result from an evaluation of the pragmatic joint utility for the couple.====The remainder of this paper is organised as follows. Section 2 gives an introduction to quantum probability, discusses how it has provided useful explanations for choices with a moral component in cognitive psychology, and shows how we mathematically build our quantum choice models. Section 3 shows the empirical application to our two moral choice datasets. We finish with some conclusions and directions for future research.",Quantum choice models: A flexible new approach for understanding moral decision-making,https://www.sciencedirect.com/science/article/pii/S1755534520300336,15 July 2020,2020,Research Article,80.0
"Krueger Rico,Rashidi Taha H.,Vij Akshay","Transport and Mobility Laboratory, School of Architecture, Civil and Environmental Engineering, Ecole Polytechnique Fédérale de Lausanne, Station 18, Lausanne, 1015, Switzerland,Research Centre for Integrated Transport Innovation, School of Civil and Environmental Engineering, UNSW Australia, Sydney, NSW, 2052, Australia,Institute for Choice, University of South Australia, 140 Arthur Street, North Sydney, NSW, 2060, Australia","Received 19 November 2019, Revised 2 April 2020, Accepted 26 May 2020, Available online 11 July 2020, Version of Record 14 August 2020.",https://doi.org/10.1016/j.jocm.2020.100229,Cited by (5),This paper i) compares parametric and semi-parametric representations of unobserved heterogeneity in hierarchical ==== logit models and ii) applies these methods to infer distributions of ==== for features of shared ,"The representation of inter-individual taste heterogeneity is a key concern of discrete choice analysis, as information on the distribution of tastes is critical for demand forecasting, market segmentation and welfare analysis. In many empirical settings, the analyst cannot perfectly explain taste heterogeneity in terms of observed individual characteristics, and taste heterogeneity remains to a substantial extent random from the analyst's point-of-view. Mixed random utility models such as mixed logit or probit can accommodate any empirical random heterogeneity distribution by marginalising the discrete choice kernel over some mixing distribution, which describes the unobserved distribution of tastes in the sample (McFadden and Train, 2000; Train, 2009).====However, the ability of mixed random utility models to recover any true heterogeneity distribution is only predicated on an existence proof (see McFadden and Train, 2000) and therefore, the analyst is required to select an appropriate mixing distribution in a given empirical setting. There are three principle ways in which unobserved taste heterogeneity can be incorporated into mixed random utility models (for a review, see Vij and Krueger, 2017): ==== mixing distributions such as the multivariate normal (MVN) distribution are described through a finite set of parameters, have well-defined functional forms, but are limited in the shapes they can assume. By contrast, ==== mixing distributions such as the categorical distribution, which underlies the formulation of latent class models (e.g. Greene and Hensher, 2003), are not described through a finite set of parameters and do not have well-defined functional forms. Accordingly, their complexity can adapt to the available information. Finally, ==== mixing distributions such as the finite mixture-of-normals (F-MON) distribution (e.g. Rossi et al., 2012) aim to combine the benefits of the previous two approaches by convolving well-defined parametric kernels with flexible nonparametric mixing distributions.====With recent advances in technical computing soft- and hardware, Bayesian methods are re-emerging as a viable alternative to frequentist methods for the estimation of mixed random utility models (Bansal et al., 2019b; Ben-Akiva et al., 2019). The key difference between Bayesian and frequentist procedures is that the Bayesian approach entails the specification of a full probability model for both the observed data and all unknown model parameters so that the posterior distribution of the unknown model parameters can be learnt by conditioning prior knowledge on observed data (Gelman et al., 2013). Aside from a few exceptions (Burda et al., 2008; Daziano, 2013; De Blasi et al., 2010; Kim et al., 2004; Li and Ansari, 2013), Bayesian methods have been primarily used for the estimation of mixed random utility models with parametric mixing distributions (e.g. Bansal et al., 2019b; Ben-Akiva et al., 2019; Scarpa et al., 2008; Train and Weeks, 2005; Train, 2009). This is in spite of the fact that some Bayesian procedures such as Markov Chain Monte Carlo (MCMC) methods lend themselves well to the estimation of complex hierarchical models by facilitating the approximation of multi-dimensional integrals in models with many unknown parameters (see Gelman et al., 2013).====The Dirichlet process mixture of normals (DP-MON) distribution (Antoniak, 1974; Escobar and West, 1995) is a semi-parametric distribution, which is well-grounded in the hierarchical Bayesian modelling paradigm and promises to be particularly flexible in terms of the distributional shapes it can assume. The DP-MON distribution results from the convolution of a multivariate normal kernel with a Dirichlet process prior (Ferguson, 1973), a flexible nonparametric mixing distribution which unlike the categorical distribution does not require that the number of mixture components is fixed prior to estimation. Rather, the complexity of the nonparametric mixing distribution is inferred from the evidence, and the number of mixture components is effectively treated as a model parameter. Accordingly, the DP-MON distribution can be viewed as an infinite-dimensional generalisation of the finite mixture-of-normals (F-MON) distribution. Notwithstanding that the DP-MON distribution has been incorporated into mixed random utility models to admit flexible representations of unobserved heterogeneity (in particular Burda et al., 2008; De Blasi et al., 2010; Li and Ansari, 2013), its performance relative to simpler mixing distributions such as the MVN or the F-MON distributions is not well understood. Importantly, the predictive ability of the DP-MON mixing distribution on external data is unknown, because existing studies solely rely on measures of in-sample fit or on visual inspections of the estimated heterogeneity distributions for model comparison (see in particular Burda et al., 2008; De Blasi et al., 2010; Li and Ansari, 2013).====Accounting for preference heterogeneity is centrally important in numerous empirical domains including, but not limited to, general consumer behaviour modelling (e.g. Kamakura and Russell, 1989), travel demand analysis (e.g. Vij, 2013), residential location choice modelling (e.g. Walker and Li, 2007) as well as environmental (e.g. Boxall and Adamowicz, 2002), health (e.g. Zhou et al., 2018) and food (e.g. Caputo et al., 2018) economics. Another such domain is the analysis of demand for emerging transportation services and technologies such as shared autonomous vehicles (SAVs Burns, 2013; Chen et al., 2016; Fagnant and Kockelman, 2014; Fagnant et al., 2016). In this context, various stakeholders are interested in understanding the distribution of individual preferences to facilitate a user-centric design and operational management of this emerging transport system (Krueger et al., 2016; Milakis et al., 2017).====Consequently, this paper has two objectives. In a first step, we seek to assess the practical implications of different flexible representations of unobserved heterogeneity on the ability of mixed logit to capture and predict preferences. To this end, we systematically evaluate the in- and out-of-sample performance of the MVN, F-MON and DP-MON mixing distributions for mixed logit in a simulation study and in a case study, which uses data from a stated choice survey (Bansal and Daziano, 2018; Liu et al., 2018) investigating preferences for SAV services in New York City. The case study also serves the second aim of the paper, which is to leverage the considered semi-parametric distributions to infer flexible estimates of WTP for different features of SAV services (out-of-vehicle travel time, in-vehicle travel time, vehicle automation and vehicle electrification).====The remainder of this paper is organised as follows: First, we review semi-parametric methods for incorporating unobserved heterogeneity into mixed random utility models (Section 2). Next, we explain the modelling and estimation methodology (Section 3). Then, we present the simulation and case studies (Sections 4 Simulation study, 5 Case study). Finally, we conclude (Section 6).",A Dirichlet process mixture model of discrete choice: Comparisons and a case study on preferences for shared automated vehicles,https://www.sciencedirect.com/science/article/pii/S1755534520300282,11 July 2020,2020,Research Article,81.0
Peyhardi Dr Jean,"Institut Montpellierain Alexander Grothendieck, 34000, Montpellier, France","Received 6 November 2019, Revised 18 May 2020, Accepted 22 May 2020, Available online 7 July 2020, Version of Record 14 August 2020.",https://doi.org/10.1016/j.jocm.2020.100228,Cited by (2),"The Student distribution has already been used to obtain robust maximum likelihood estimator (MLE) in the framework of binary choice models. But, until recently, only the logit and probit binary models were extended to the case of multinomial choices, resulting in the multinomial logit (MNL) and the multinomial probit (MNP). The recently introduced family of reference models, well defines a multivariate extension of any binary choice model, i.e. for any link function. In particular, this is the first extension of the binary robit to the case of multinomial choices. These models define the choice probability for category j relative to an (interchangeable) reference category. This paper highlights the robustness of reference models with Student link function, by showing that the influence function is bounded. Inference of the MLE is detailed through the Fisher's scoring algorithm, which is appropriated since reference models belong to the family of ==== (GLMs). These models are compared to the MNL on the benchmark dataset of travel mode choice between Sydney and Melbourne. The results obtained on this dataset with reference models are completely different compared with those usually obtained with MNL, nested logit (NL) or MNP that failed to select relevant attributes. It will be shown that the travel mode choice is totally deterministic according to the transfer time. In fact, the use of Student link function allow us to detect the total artificial aspect of this famous dataset.","The use of Student distribution to obtain robust estimations has been introduced by Lange et al., (1989) in the framework of linear regression. For a binary response variable, the use of Student distribution was suggested by Albert and Chib (1993) as an alternative to logit or probit regression. Liu 2004 called this model the robit regression model and demonstrated the robustness of the MLE. Koenker and Yoon (2009) studied the importance of the link function in binary choice models with a focus on Student link function. But the variety of link functions defined in the literature, is considerably decreasing when the number of alternative choices ==== is more than two. Only the logit and probit binary models were extended to the case of multinomial choices, resulting in the MNL and the MNP. Contrarily to the MNL, the probabilities of alternative choices have no analytic forms with the MNP. Their computation has to be made through approximations and this complexity often leads the practitioner to neglect the MNP when ====. Despite these difficulties, the extension of the MNP with multivariate Student error distribution in the context of random utility maximisation (RUM), has recently been introduced by Dubey et al., (2020). But this multinomial choice model (====) is not the natural extension of the binary robit model since the difference of two Student independent errors does not follow a Student distribution. This is the case only with MNL and MNP since the difference of two independent Gumbel errors follows a logistic distribution and the difference of two Gaussian errors follows a Gaussian distribution.====An alternative way to extend link functions when ====, has been recently introduced by Peyhardi et al., (2015) in the context of GLMs. The family of reference models was thereby introduced, for which all alternatives are compared to a reference alternative. Since each of these ==== comparisons is binary, the link function through the linear predictor can be made with a cumulative distribution function (cdf). All usual econometric outputs (willingness-to-pay, elasticities, …) of reference models have been determined by Bouscasse et al., (2019). The goal of the present paper is to demonstrate the robustness of reference models defined with Student link function. A usual way to study the robustness is to study the influence function (Hampel, 1974). This approach is well established in the GLM framework for the class of ==== estimators (Künsch et al., 1989). We propose to study the influence function for a reference model based on the score ====. We extend the results on robustness of Student link function, shown by Liu (2004), to the case of ==== alternatives. More precisely the influence function for a reference model is shown to be unbounded for the logistic and normal link functions and bounded for the Student link function.====Student and logistic link function are then compared on the well known dataset of travel mode choice between Sydney and Melbourne. This is certainly the most used benchmark dataset to compare different families of discrete choice models (Greene, 2003; Hensher and Greene, 2002). The MNL, the NL and the MNP models have been extensively studied among this dataset, principally to highlight the limitation of the independence of irrelevant alternatives (IIA) property. This paper presents the IIA property for reference models and relates it to invariance property under permutations of alternatives. It will be shown that, even if the NL or the MNP model do not share this property, they fail, as the MNL, to select relevant attributes. The model selection is clearly in favour of the Student link function against the MNL, NL and MNP. It reveals that the travel mode choice is driven only by the transfer time. Plotting the observed choices according to the transfer time, revealed that all individuals made the same choice for each given transfer time value. Moreover, using a three dimensional plot, it is noticed that the design experiment takes a geometric form of a cross. In fact, the use of Student link function allow us to highlight the total artificial aspect of this famous dataset and thus reconsider classical results obtained in the usual literature (Greene, 2003; Hensher and Greene, 2002).====The present paper is organized as follows. The second section presents the family of reference models as an extension of the MNL. The third section describes the invariance property of such models under permutations of the alternatives and relates it to the IIA property. It is highlighted that a reference model is depending on the reference alternative. Otherwise, the reference model is also depending on the degree of freedom parameter of the Student distribution. In Section 4, the Fisher's scoring algorithm is therefore firstly detailed for a given reference alternative and a given degree of freedom. Then the inference procedure is described when these two parameters are unknown. The influence function of a reference model is described according to the chosen link function, i.e. the chosen cdf. It is thus easily seen that the influence function is bounded with Student cdf and unbounded with logistic or normal cdf. Section 5 briefly presents the dataset. The model selection is then detailed, leading to a simple model with only the transfer time as attribute. This section then backs to the dataset and highlights its artificial aspect, using a well chosen plot of the dataset. It makes this dataset the perfect candidate to study the sensitivity of a model to different kind of noise. Indeed the general cost, for instance, can be considered as a noise in attribute (i.e., the corresponding parameter is null) and some observations can be considered as outliers (i.e., prediction and observation are the opposite). Section 6 presents a simulation that firstly studies the inference accuracy of the degree of freedom estimator and then the sensitivity of the inference to the noise in attributes.",Robustness of Student link function in multinomial choice models,https://www.sciencedirect.com/science/article/pii/S1755534520300270,7 July 2020,2020,Research Article,82.0
Sarrias Mauricio,"IDEAR, Department of Economics, Universidad Católica del Norte, Antofagasta, Avenida Angamos 0610, Chile","Received 31 May 2019, Revised 9 March 2020, Accepted 14 May 2020, Available online 23 June 2020, Version of Record 2 July 2020.",https://doi.org/10.1016/j.jocm.2020.100224,Cited by (11),"Individual-specific posterior distributions are an attractive tool for disentangling the tastes for each person in the sample. However, there exists some risks and certain limitations regarding their use. This study reviews and summarizes the theoretical literature about the individual-specific posterior distributions derived from the Mixed Logit model, focusing on their properties, limitations and common pitfalls. It also reviews and analyzes the behavior of some diagnostic checks proposed in the literature for the reliability of such estimates in applied works using Monte Carlo experiments. Finally, this article provides reasonable guidelines for the correct use of individual-specific posterior distributions.","Since the Revelt and Train (2000)'s work and Train (2009)'s book, the use of conditional estimators derived from conditional posterior distributions has been an attractive tool for disentangling the tastes for each individual in the sample. Conditional estimates, put simply, allow us to know something about respondents' tastes based on their previous choices providing their most likely location on the population distribution.====Due to the attractiveness of eliciting the tastes for each person, individual-specific estimates (or conditional estimates) derived from the Mixed Logit (MIXL) Model have received a lot of attention from the applied literature in different fields. For example, they have been used to compute willingness-to-pay (WTP) measures at the individual level (Sillano and de Dios Ortúzar, 2005; Greene et al., 2005; Hensher et al., 2003, 2006; Hess, 2007; Sandorf et al., 2017; Dumont et al., 2015), to analyze the spatial dependence of tastes (Campbell et al., 2009; Budziński et al., 2018; Abildtrup et al., 2013), to retrieve individual-specific attribute processing strategies (Hess and Hensher, 2010), to create clusters or segments of individuals (Richter and Pollitt, 2018; Huber and Train, 2001) and for predicting the future behavior of the individuals (Train, 2009; Dumont et al., 2015).====Unfortunately, there exists some risks and certain limitations regarding the use of conditional estimates. For example, Revelt and Train (2000) have already shown that individual-specific estimates are consistent if and only if, given a fixed number of individuals, the number of choice situations increases without bound.==== That is, we need several choice situations per individual to learn something about the preferences of each respondent: if we could observe infinitely how individuals react to changes in attributes and their choices in terms of these changes, in theory, we could elucidate the specific tastes of individuals. However, there exist published articles using individual-specific estimates with a very low number of choice situations without reporting any type of diagnostic check to analyze whether the estimates are reliable to be used in a second-step procedure.====In addition, it seems that there is still some confusion regarding the relationship between the variance of the individual-specific estimates and the variance of the population distribution of the random parameters. Some researchers use individual-specific estimates arguing that they show more plausible results in terms of their domain. For example, it is common to find in the applied literature claims such that ==== or comments such as “====”, especially when computing individual-specific WTP. But, as briefly mentioned by Daly et al. (2012), such claims ignore the fact that the variance of the conditional mean will be lower than the variance of the population distribution of the random parameters (or unconditional population), specially when the number of choice situations per individual is low. In other words, the apparent better fit of the individual estimates in terms of sign and values might be an artifact of the statistical behavior of the conditional estimates when the number of choice situations is not large enough.====Another problem not yet analyzed in depth, and pointed out by Hess (2010), is the potential impact of the misspecification of the parametric population distribution on the individual estimates. As argued by Hess (2010), researchers should analyze the impact of assumptions made for the unconditional distributions on the shape of the conditional distributions. Failing to choose the most adequate distribution for the random parameters might invalidate the use of conditional estimates and lead to misleading conclusions.====Thus, the first objective of this paper is to review and summarize the properties, limitations and common pitfalls when using individual-specific estimates in the context of the MIXL model and reinforce their understanding. Relying on the previous work of Revelt and Train (2000), Daly et al. (2012) and Hess (2010), this study revisits the theoretical properties of individual-specific estimator regarding to their consistency and the relationship between the conditional and unconditional distribution of tastes. The second objective is to provide reasonable guidelines for the correct use of individual-specific tastes under a well-specified specification and analyze the potential problems under misspecification, extending the previous work done by Hess (2010). To achieve this goal, the study extends the Monte Carlo experiment carried out by Revelt and Train (2000) by analyzing the behavior of the conditional estimates under misspecification and by including new measures as diagnostic tools.====The rest of the paper is organized as follows. Section 2 briefly reviews the MIXL model. Section 3 summarizes the main theoretical properties of the individual-specific estimates. Section 4 explains the Monte Carlo setup, whereas Section 5 presents the results. Finally, Section 6 discusses the results and concludes.","Individual-specific posterior distributions from Mixed Logit models: Properties, limitations and diagnostic checks",https://www.sciencedirect.com/science/article/pii/S1755534520300233,23 June 2020,2020,Research Article,83.0
"Chavez Daniel E.,Palma Marco A.,Nayga Rodolfo M.,Mjelde James W.","Department of Marketing and Supply Chain, University of Kentucky, 550 South Limestone, Lexington, KY, 40506-0034, USA,Department of Agricultural Economics, Texas A&M University, 2124 TAMU, College Station, TX, 778432124, USA,Agricultural Economics and Agribusiness, 217 Agriculture Building University of Arkansas, University of Arkansas, Fayetteville, AR, 72701, USA","Received 15 May 2019, Revised 11 May 2020, Accepted 15 May 2020, Available online 29 May 2020, Version of Record 15 June 2020.",https://doi.org/10.1016/j.jocm.2020.100225,Cited by (4),"The proneness of ==== methods to different biases has increased the popularity of incentivized choice experiments (ICE). ICEs, however, are not free from challenges. One such challenge when using ICEs in market valuation is that some product alternatives may not be available to incentivize experiments. In this context, withholding information about product availability could be considered deception while disclosing information raises questions regarding influences on participants’ choice behavior. The present study explores this issue by changing the number of available product alternatives in an induced value choice experiment. Participant engagement is captured using an eye-tracking and measurement scales. The results suggest, while incentives matter, engagement matters more. A random-effects logit estimation reveals that engagement is positively correlated with profit-maximizing behavior, while the number of available alternatives is not. A similar result of engagement holds after accounting for differences in payouts between alternatives. In contrast, the same thing cannot be said for number of available alternatives. When the difference between potential payouts is small, no number of available alternatives has a significant effect on profit-maximizing behavior. When the difference between payouts is larger, any number of available alternatives increases the likelihood of choosing the profit-maximizing choice. The findings suggest that researchers could successfully conduct ICE with trustworthy results without having all the product alternatives being available as long as participants are engaged in the choice task.","Stated preference methods such as choice experiments are widely used for market and non-market valuations. The potential exists for various biases in stated preference studies including response bias (Diamond and Hausman, 1994), justification and policy bias (Bonsall, 1985; Bates, 1988), aggregation bias (Morrison, 2000), strategic bias (Lu et al., 2008), and hypothetical bias (List and Gallet, 2001; Murphy et al., 2005; Penn and Hu, 2018). Researchers are gravitating towards methods that help mitigate any or all of the potential biases. One such method for market valuations is the incentivized choice experiment (ICE) (Ding et al., 2005; Rakotonarivo et al., 2016; Penn and Hu, 2018). In ICEs, participants make choices, and some or all of these choices are acted upon. There are individual consequences to participants stating their preferences in an ICE.====ICEs are widely used in private good valuations to determine how consumers respond to new products, new features of existing products, or new production technologies (Hensher, 1994). New products or features not available in real markets, however, pose a challenge in designing ICEs for market valuation because some alternatives in the choice sets cannot be used for incentivization simply because they are not available. Given that deception is generally not allowed in economic experiments (Colson et al., 2016), researchers conducting ICE have to inform participants that some of the alternatives in the choice sets are not available. Consequently, the information about alternative availability may modify participants' choice behavior, especially when it is uncertain whether participants fully understand the procedures (Cason and Plott, 2014). Furthermore, knowing not all decisions are consequential may change the perceptions of participants and how much effort they exert not only to understand the procedure, but also to reveal their true preferences (Yang et al., 2018). The main question addressed is whether the participant's behavior in ICEs differs when the number of product alternatives physically available for incentivization differs? This is an important empirical question, as participants may be unwilling to exert the necessary effort to understand and follow the experimental procedures (Bardsley, 2005). Such a lack of engagement may be exacerbated by the fact that ICEs are a novel task for most participants, and hence, potentially limiting the reliability of information obtained.====The objective is to determine whether and how the number of alternatives with consequences has an effect on choice behavior in valuation studies for private goods. To achieve this objective, an ICE is designed to gauge whether the number of alternatives with consequences changes participants’ profit-maximizing behavior. The study uses the induced values (IV) framework to assign value to the options presented in the ICE. Because there is a unique profit-maximizing alternative in each choice set, the performance of participants is measured objectively by evaluating the deviations from the profit-maximizing alternative. The effect of the number of alternatives with consequences to participants is captured by randomly assigning participants into one of four treatments differing in the number of alternatives with consequences. The ICE is presented to participants on a computer screen using an eye-tracking device that allows for the measurement of the search behavior of participants, information that could help understand the decision-making process better.",Product availability in discrete choice experiments with private goods,https://www.sciencedirect.com/science/article/pii/S1755534520300245,29 May 2020,2020,Research Article,84.0
"Tran Yen,Yamamoto Toshiyuki,Sato Hitomi,Miwa Tomio,Morikawa Takayuki","Graduate School of Engineering, Nagoya University, Japan,Institute of Materials and Systems for Sustainability, Nagoya University, Japan,Institutes of Innovation for Future Society, Nagoya University, Japan","Received 25 October 2019, Revised 16 April 2020, Accepted 20 May 2020, Available online 22 May 2020, Version of Record 14 June 2020.",https://doi.org/10.1016/j.jocm.2020.100227,Cited by (4),". As such, we integrated attitudes towards physical activity with two specific attitudes towards bus and car use in a binary logit mode choice model between car and bus, looking at the area of Asuke, Japan. As we found data separation due to highly unbalanced mode shares in the input data, the choice models were estimated with the likelihood function penalized. While only a few parameters were found to be significant, arguably as a result of the unbalanced mode share pattern, attitude variables were almost unaffected by the data separation phenomenon. Whereas maximum likelihood estimates do not exist in the presence of data separation, the employed penalized maximum likelihood estimator was demonstrated to be a solution to this problem. Thus, we suggest that checking for data separation in the case of highly unbalanced mode share patterns is important and if data separation exits, penalizing the likelihood functions can be a solution rather than excluding out some irrelevant variables to avoid data separation. Overall, we found that attitude towards physical activity had a significant effect on bus utility, suggesting that policymakers could use this factor to connect transport and health policies.","Physical activity is important for modern life as inactivity has been identified increasingly as a potential health risk (González et al., 2017). Thus, the promotion of increasing physical activity has been called for worldwide (Abu-Omar et al., 2017; Conn et al., 2011). In this study, we examine the influence of attitudes towards physical activity (APA) on a transport mode utility, specifically a bus utility.====The need to analyze such attitudes remains a strong motivation for the application of integrated choice and latent variable models. Conventionally, mode choice models are based on mode attributes, such as travel time and cost, and socio-demographic characteristics, such as age and sex, as the determinants of mode utilities. Integrated choice and latent variable (ICLV) models, or, simply, hybrid models, enable the integration of more abstract factors into the mode choice explanation. For instance, comfort and convenience can be considered “latent” attributes of transport modes (Morikawa et al., 2002) that cannot be directly observed and, hence, are not viable in conventional mode choice models. Similarly, personal traits, social norms, and attitudes can only be modeled through the framework of choice models with latent variables. Among common abstract factors, attitude is the most frequently studied object. This may be because attitude is a central subject in psychology (Allport, 1935), and the inclusion of attitudes in ICLV models can bridge the gap between psychological and econometrical models (McFadden, 2001). Nevertheless, attitudes toward physical activity have appeared infrequently in mode choice models, as we found only one case where APA was considered in the context of a school travel mode choice among teenagers (Kamargianni et al., 2015).====Addressing this gap in the literature, we consider APA, a type of general attitudes which are more exogenous to behavior than specific attitudes==== (Kroesen and Chorus, 2018), in the context of transport mode choice. General attitudes are also believed to be more stable than specific attitudes in various situations and allow policymakers to observe their effects on various relevant behaviors simultaneously (Bamberg, 2003).====In our study, this gives rise to the idea of a linked transport and health policy; if, for example, APA can be proven to have a positive effect on bus utility and, hence, bus choices, then promotional campaigns for more daily physical activities initiated from health policies could have an additional influence on transport mode choice by increasing the mode share of buses. The additional benefit this offers for transport policy support could be considered a supportive argument for the implementation of such campaigns.====There is also a theoretical basis for postulating an effect of APA on the choice of the mode that contains some amount of physical activity. For instance, bus travelers have to spend some time in cycling or walking to/from bus stations and, thus, making it intuitive to hypothesize an effect of APA on bus utility. Evidence for the association between physical activity and public transport use can be found in Rissel et al. (2012).====Here, we investigate the influence of APA on mode choice in the context of a rural area. Our interest in the rural context stems from the fact that there appears to be a disparity between rural and urban areas in both level of physical activity and opportunities for physical activity. For instance, compared with urban residents, those living in rural areas exhibit lower levels of physical activity (Cleland et al., 2017; Ozemek et al., 2019) and higher obesity prevalence (Befort et al., 2012). Rural areas are also considered as physical activity disadvantaged and the need to promote physical activity among rural adults is urgent (Mitchell et al., 2019). One possible reason for this is that residents in rural areas are highly dependent on car use (Gray et al., 2001; Marr, 2015; Wiersma et al., 2015) and more time spent in the car is associated with an increase in obesity (Wener and Evans, 2007). Thus, the realization of physical activity promotional campaigns in rural areas should be given a higher priority and, as argued above, the establishment of the effect of APA on mode choice provides supportive arguments for such campaigns.====In addition, we noticed that the situation of high car dependence in rural areas, which implies a resultant highly unbalanced mode share pattern, needs special attention in mode choice models. Specifically, when mode share patterns are strongly skewed towards certain modes, such as car, the observed choices for other modes, such as bus, become “rarer.” The very low mode shares of some alternatives relative to the others can cause data to be separated==== at certain levels (Frischknecht et al., 2014). In such cases, choice models are prone to bias in the estimates and convergence problems (Bull et al., 2007; Heinze, 2006; Rainey, 2016).====For this reason, the determinants of mode choice in cases of highly unbalanced mode share patterns, such as in rural areas where car use dominates other modes, can differ from the common determinants in general cases. As a result, general practice in mode choice models may not be applicable to rural areas with high car dependence. Thus, the rural context in our study may prove valuable for transport policies in rural areas. In sum, we examine the potential effect of APA on mode choice in this study. Our main purpose is to support transport policies, particularly in rural areas.====The rest of the article is organized as follows. In the next section, we describe the data set used for analysis. The model framework is presented in Section 3, followed by the results of model estimation in Section 4. Finally, discussion and conclusions are presented in Sections 5 Discussion, 6 Conclusion.",The analysis of influences of attitudes on mode choice under highly unbalanced mode share patterns.,https://www.sciencedirect.com/science/article/pii/S1755534520300269,22 May 2020,2020,Research Article,85.0
"Isihara Paul,Shi Chaojun,Ward Jonathan,O'Malley Leo,Laney Skyler,Diedrichs Danilo,Flores Gabriel","Department of Mathematics, Wheaton College (IL), Wheaton, IL, 60187, USA","Received 20 December 2018, Revised 16 December 2019, Accepted 18 January 2020, Available online 30 January 2020, Version of Record 13 May 2020.",https://doi.org/10.1016/j.jocm.2020.100204,Cited by (3),This paper proposes the use of Most Typical (MT) and Most Ideal (MI) levels when an adaptive choice-based conjoint (ACBC) survey can only obtain a small sample size ==== from a small population size ,"Surveys with a small population size ==== who are expert decision-makers may arise in a wide variety of important contexts such as a survey of executives of very large companies (Equilar, 2019; Russell Reynolds Associates, 2010) or cohort of political leaders such as big city mayors (Douglas, 2017). There are a number of practical problems inherent in doing statistical analysis of small populations. For example, there is the question of wide margins of error when the sample size ==== is only a fraction of the population size ====. A 30% margin of error (==== 15% of the true value or proportion) may arise in a business to business survey sample of ==== companies out of a small known list of ==== companies (Henning, 2014). The margin of error reduces to ==== 10% with a sample size ====, and ==== 7.5% with a sample size ====. This being the case, a recommended approach is to elicit increased survey response by various methods (phone, e-mail, etc.). A different type of small population challenge is encountered in the medical profession (Kirkendall and White, 2018). A small “hidden” population with acute health care needs may involve a much larger, unknown value for ==== than in a business to business survey population. Members of the population may be largely unknown. Here, small population means relative to the mainstream population, and the difficulty to gauge, locate, and obtain a sufficiently large sample for statistical inference. In this case, centralized electronic data collection from multiple sources helps to increase sample sizes (Devers et al., 2014).====In this study we consider two small populations of disaster relief organizations, one faith based, and the other non faith-based. Effectiveness of a disaster response may depend on the quality of collaboration between organizations with a broad diversity of religious and ideological perspectives (Gingerich et al., 2017). For effective coordination of relief, it is important that humanitarian organizations understand the unique traits and characteristics that shape their disaster response decisions (Zakour and Harrell, 2003).====Faith-based organizations (FBOs) play an increasing role in responding to humanitarian crises, collaborating with larger non-governmental organizations and receiving funding from government programs. Although many FBO projects have been evaluated for their outcomes, such as the quantity of aid delivered, this survey was part of a larger study to explore how Christian values and perspectives influence the planning and implementation of disaster relief (CCCU n.d.). Our study provides a data point for the specific question whether and how Christian values influence the decisions made by FBOs when responding to a particular disaster.====We designed an ACBC survey (Orme, 2014; Orme and Chrzan, 2017; Rao, 2014) to identify similarities and differences between faith based disaster relief organizations (hereafter abbreviated FBOs) and non-faith based disaster relief organizations (hereafter abbreviated NFBOs) in their “Go/No-Go decision-making”. Following the first use of adaptive choice-based conjoint (ACBC) surveys to study priorities in disaster relief logistics (Gralla et al., 2014), our survey focused on four attributes pertinent to the “Go/No-Go” decision by disaster relief managers: Available Funding, Disaster Response Type, Assessment of Need, and Access to the Affected Community. We also sought to distinguish Most Typical (MT) attribute levels from Most Ideal (MI) levels for both groups of respondents (FBO and NFBOs). For example, a large disparity between MT and MI levels was observed in FBO Funding, with all respondents reporting their MT level was 50% or less, and more than half estimated to have an MI funding level of at least 75%. We used the Build Your Own (BYO) question (Orme and Chrzan, 2017; Cunningham et al., 2010; Orme and Johnson, 2008) to determine MT levels and binary choice task data (Thurstone, 1927; Maydeu-Olivares and Böckenholt, 2005) to determine MI levels.====The two small populations which we surveyed both had a known size ====. We identified for our FBO population a nearly exhaustive list of ==== international FBOs headquartered in the U.S. (one was in Canada). Our NFBO population of size ==== consisted of the non faith-based members of National Voluntary Organizations Active in Disaster who engage in international disaster relief. Our sample size ==== for both populations was small (==== for FBOs and ==== for NFBOs), even though we used phone calls, electronic data collection, and follow-up e-mails in an effort to increase sample sizes.====The main problem addressed in this paper is how to validate MI levels obtained using estimated part-worth utilities for a small sample size ====. Questionable use of part-worths is exemplified by an ACBC survey with a sample size ==== which reported a hierarchical Bayesian estimated part-worth choice task hit rate of 53.5% compared to 52.8% for part-worths estimated by Monotone Regression (Johnson, 1975; Lighthouse Studio Help n.d.). The solution we propose is to validate part-worth utilities using a basic PAPRIKA (Potentially All Pairwise RanKings of all possible Alternatives) method (Hansen and Ombler, 2009) whose judgments (choice tasks) are structured as and limited to a single-elimination round of 16 tournament. PAPRIKA is particularly appropriate for small samples whose respondents are all expert decision makers whose choice task data is non self-contradictory. In our case, 16 out of 19 respondents each had over 15 years experience as disaster relief managers. The probability that PAPRIKA correctly identifies an expert respondent's MI levels depends on the structure of the survey and choice task tournament, but not on the sample size ====. Thus, PAPRIKA is an effective validation tool for small samples. In our survey, PAPRIKA confirmed part-worth estimated similarity between FBOs and NFBOs for MI Need Assessment, with “Clear Need” for the organization's services being the MI level for almost all respondents. Major disagreement between PAPRIKA and part-worth utility based MI levels identified specific areas where further investigation of an attribute or level is necessary. In our case, our analysis revealed two such areas: (i) the most ideal disaster response type attribute and (ii) the MT/MI local partner level of the Community Access attribute for both FBOs and NFBOs.====Observed differences in MT and MI levels between FBOs and NFBOs suggest possible areas for further research into how Christian values may influence FBO decision-making. For example, a much larger proportion of FBOs (96%) than NFBOs (50%) is estimated to respond to “non-headline news” type disasters, meaning those which are not declared “Level 3” by the United Nations' Inter-agency Standing Committee (i.e. “IASC 3”). This may reflect an FBO value to avoid the limelight in doing good works as was expressed by one of the FBO disaster relief managers. Another observed difference between FBOs and NFBOs was in the PAPRIKA MI funding level. Whereas 1/3 of the FBO's MI level was 25% or less, this level was a zero count for the NFBOs. This agreed with a preliminary study (Veatch and Reimel, 2017) which suggested for FBOs (i) an emphasis on reaching the most vulnerable or remote people, and a de-emphasis of cost or efficiency considerations; and (ii) a desire to respond to disasters according to the needs, even when donors are not interested in a particular disaster.====The layout of the paper is as follows: Section 2 describes the ACBC survey instrument (2.1), data collection and processing (2.2), and PAPRIKA validation of partworth utilities (2.3); Section 3 summarizes results for both FBOs and NFBOs, including comparison of MT and MI levels, comparison of partworth MI and PAPRIKA MI levels, and estimation of MT level proportions for the entire population (Table 3); Section 4 describes a Monte Carlo simulation to estimate the probability that PAPRIKA correctly determines MI levels; and finally, Section 5 summarizes the MT/MI estimation method as presented, and how the method might be improved. As there are a number of acronyms used in the sequel, we compile them alphabetically in Table 1 for quick reference.",Identifying most typical and most ideal attribute levels in small populations of expert decision makers: Studying the Go/No Go decision of disaster relief organizations,https://www.sciencedirect.com/science/article/pii/S1755534520300038,30 January 2020,2020,Research Article,86.0
"Elsenbroich Corinna,Payette Nicolas","Centre for Research in Social Simulation, University of Surrey, United Kingdom,Complex Human-Environmental Systems Simulation Laboratory, University of Oxford, United Kingdom","Received 19 March 2019, Revised 3 January 2020, Accepted 9 January 2020, Available online 21 January 2020, Version of Record 28 January 2020.",https://doi.org/10.1016/j.jocm.2020.100203,Cited by (10),This paper presents an agent-based model of ,"In this paper we will look in more detail at how agent-based modelling (ABM) can contribute to two aspects of choice modelling, the question of modelling how individuals actually make choices and of exploring macro outcomes of aggregate individual choices under social influence. The ABM presented in this paper models the importance of an agent's social context on the choice to cooperate or defect in a public goods game (PGG). The model implements team reasoning akin to Bacharach's theory of ==== (Bacharach, 1999, 2006). Experimental studies strongly support the existence of team reasoning strategies (Colman et al., 2008a, b).====The model departs from the individualist profit maximisation perspective of traditional game theory that sees cooperation as resulting from social order mechanisms such as social norms, reputation or sanctioning. Instead of maximising ones individual payoffs, (some) agents interpret a game or situation as collective, believing others do the same (Elsenbroich and Verhagen, 2016). The model presented here is a model of a PGG but rather than modelling an individual's focus on utility or payoff the focus is on behaving as part of a team. It explores the influence of preference convergence on levels of individual wealth and social inequality.====When it comes to modelling how individuals actually make choices, the number of theories is almost endless. Different theories can be mapped on a spectrum of universality. Some theories are very specific, taking into account personality differences or an individual's life course (e.g. Kassarjian, 1971; Ham et al., 2009), focus on detailed contexts, looking at socio economic variables such as gender, class and ethnicity (e.g. Dekkers et al., 2000) or the influence of social networks (e.g. Easley and Kleinberg, 2010). On the other end of the spectrum are universal theories that capture choices with a single principle, e.g. utility maximisation (e.g. Hooff et al., 2001).====The increase in universality goes hand in hand with a loss of heterogeneity of individuals. For formal modelling at scale this universality used to be essential as increasing heterogeneity automatically results in models too complex to solve analytically. The loss of heterogeneity, however, means that potentially salient information is lost. This is a particular problem for modelling the macro or societal consequences of individuals’ choices. When it comes to predicting the future state of a system such as a society, small differences in the individual components, the agents, can lead to big differences in overall system behaviour (Ormerod, 2011).====The question of the consequences at the social, societal or macro level resulting from the aggregation of individual choices is the second dimension we are looking at in this paper. At the more abstract level these are questions about the sustainability, equality and equity of societies (e.g. Luebker, 2014). At a more concrete level there might be questions about how choices, such as education, influence the life course (e.g. Cardak, 2004) or what might lead to migration decisions (Klabunde and Willekens, 2016). Understanding the societal dimensions of individual choices is at the heart of many social sciences, nowhere more so than in economics. Economics built on the choice model of rationality has had to recognise that irrationality and bias are integral parts of human decision making (Kahneman, 2011). This heterogeneity detracts from the simplicity of the models but is paramount for understanding how individual choices aggregate.====Agent-based modelling (see social_simulation) is a method that allows for formal and systematic modelling of interactions of heterogeneous individuals. Whilst ABM cannot find out how people actually make choices (this is a question reserved for empirical research) ABM is a method that can implement heterogeneity, allowing for modelling of less abstract, more context dependent and concrete ways of making choices than traditional mathematical approaches. Through this ABM has the ability to model macro social outcomes of social influences and interactions. Klabunde and Willekens argue strongly for the use of ABM, as “Agent-based modelling is the only method that allows for the explicit modelling of social interaction and the social networks that result from it.” (Klabunde and Willekens, 2016, p.75). By modelling many agents over time, interacting with each other and the environment in which they are situated, ABM can simulate macro phenomena emergent from the micro interactions. Furthermore, through modelling individual agents over time interacting with the macro level, ABM can also model how the macro level influences the individuals (====, cf. Conte et al. (2014)).====The ABM discussed in this paper models individuals’ choices to cooperate or defect in a social dilemma setting, depending on the group context of the agent. It implements a particular theory of choice, team reasoning (Bacharach, 1999, 2006), as an explanation for high levels of cooperation and models the macro consequences of team reasoning in different social contexts. In the model the idea of team reasoning is operationalised by an agent observing the cooperation levels within their group or team and adjusting their personal likelihood of cooperation in the next iteration.====The purpose of this model is theoretical exploration of team reasoning as an alternative theory of choice to traditional individual utility maximisation. The model contributes to the literature on cooperation in collective dilemmas by looking at cooperation dynamics where individuals make the decision whether to cooperate or not in relation to their membership of a collective or team. Rather than focussing on utility calculations it focusses on convergence dynamics of cooperation probabilities in a team. The questions the model is trying to tackle are outcomes of individual wealth and population inequality for different initial group compositions, as is done in traditional social dilemma models. The dynamics driving the model are, however, interaction convergence dynamics similar to traditional opinion dynamics models.====The paper is structured as follows. In Section 2 we discuss some solutions to the problem of cooperation in social dilemmas, in particular team reasoning. In Section 3 we introduce agent-based modelling as a method for modelling the choice to cooperate or defect. In Section 4 we describe the model, presenting the analysis and results in Section 5. Section 6 draws conclusions and discusses future work.",Choosing to cooperate: Modelling public goods games with team reasoning,https://www.sciencedirect.com/science/article/pii/S1755534520300026,21 January 2020,2020,Research Article,87.0
"Pink Sebastian,Kretschmer David,Leszczensky Lars","Mannheim Centre for European Social Research (MZES), University of Mannheim, Mannheim, Germany","Received 31 March 2019, Revised 13 December 2019, Accepted 9 January 2020, Available online 14 January 2020, Version of Record 28 January 2020.",https://doi.org/10.1016/j.jocm.2020.100202,Cited by (4),"Combining choice modelling with ====, we show how the stochastic actor-oriented model for the co-evolution of networks and behavior (SAOM) can be used as a powerful statistical framework to empirically analyze network-related choices. We discuss the underlying assumptions of SAOMs and show that they can be interpreted to represent a random utility maximization model (RUM). Network-related choices pertain both to decisions to engage in (or disengage from) specific ==== and decisions to adapt behavior to that of social contacts. We demonstrate the usefulness of SAOM for the choice modelling ====. We further illustrate how SAOM can be used to study network-related choices by providing an exemplary empirical analysis.","People make choices concerning a wide array of subjects, such as where to live, which transportation mode to use, or which wine to buy. Conceptually, these choices are based on two principles. First, different alternatives satisfy people's preferences to different extents, thus providing different utility, and people choose the alternative with the highest utility (McFadden, 1973). Second, utility is not tied to the alternatives as such but derives from the combination of their characteristics, attributes, or properties (Lancaster, 1966). If people, for example, think about moving to a different neighborhood, they may consider that this potential new place of residence differs from the current one in terms of commuting time, local facilities, or social composition of its residents, and then choose to move or to stay. Thus, understanding how people weigh these different characteristics is crucial. Stated choice experiments (SCE) constitute a popular means to disentangle the importance of different characteristics (Louviere et al., 2000), and the random utility maximization model (RUM; McFadden, 1973) is a key tool to specify how choice situations can be modelled statistically to estimate the probabilities of choosing each alternative in a given choice set.====However, people do not only make decisions about goods, such as transportation modes or wine, but also about their ====. That is, people choose who to befriend, who to collaborate with, or who to gossip about. These relations result in ====, which can be defined as the web of specific relationships among a group of people, such as friendships in school, co-authors in a scientific field, or gossipers in a firm. From a choice modelling perspective, social networks are interesting for two reasons. First, social networks rarely emerge randomly but, as mentioned above, arise from people's decisions regarding their network partners. In the literature on social networks, these decisions are referred to as the ==== of interaction partners (Steglich et al., 2010). The structure of networks, then, is the outcome of individuals' choices of who to connect to. Thus, although the choice to select interaction partners reflects only one specific choice process among many others, it is particularly interesting and challenging due to the inherent interdependency of choices in social networks. The reason is that social networks do not suddenly pop up such that a group of people selects their interaction partners at the same time. Rather, social networks emerge over time through a sequence of people selecting their interaction partners. This renders people's choices inherently dependent on the previous choices of other people because these choices change how attractive social relationships to particular people become. Thus, understanding choices regarding the selection of interaction partners, on which we focus in the following, is inextricably linked to the idea of interdependency of choices and, consequentially, to the evolution of social networks. One outcome of these selection processes is, for example, the size of personal networks, which has been assessed previously based on hybrid choice models using the relationship strength to predict retention and loss of social contacts (Calastri et al., 2018). Second, irrespective of how social networks formed and evolved, they are often important for choice modelling processes because people are embedded in networks which ==== their non-network related individual behavior and decisions. For example, people have been shown to be influenced by their social contacts in their political voting choices (Reed, 2015). These effects of network partners on individual outcomes are referred to as influence, and understanding how and why people's choice behavior is influenced by their network partners is another key task for both network researchers and choice modellers.====Studying network-related choices, however, is complicated by the fact that both selection and influence result in the same structural outcome of similar people being connected to each other. For example, do adolescents start drinking alcohol because their friends do so, or do drinkers befriend other drinkers (Osgood et al., 2013)? Answering such questions requires separating selection (i.e., choosing network partners based on their characteristics) from influence (i.e., choosing to change one's own behavior because of network partners). One statistical approach to study choices in a social network context and to separate selection from influence is the so-called ==== (SAOM, Snijders et al., 2010; Steglich et al., 2010). Though SAOMs are specifically suited to studying choice processes in network-related settings, the literature on choice modelling and social networks have so far been largely decoupled (but see Arentze et al., 2013; Calastri et al., 2018; Reed, 2015). In fact, a core critique of research on decision-making is that it often fails to take social context into account (Bruch and Feinberg, 2017; Maness, 2020), an important part of which is people's embeddedness in social networks. Furthermore, this decoupling is surprising because rational choice arguments might be a common basis of modelling decision-making processes (Hess et al., 2018) as they feature in both literatures. Several authors have pointed to the conceptual alignment of the SAOM modelling strategy with rational choice considerations (Amati et al., 2015; Snijders, 1996; Van de Bunt et al., 1999) and SAOMs have been applied to substantive questions related to rational choice arguments (Leszczensky and Pink, 2015).====In this paper, we aim to increase the mutual appeal between the fields of social network analysis and choice modelling. In order to do so, we focus on the SAOM, a well-established method of longitudinal social network analysis (for the main introduction, see Snijders et al., 2010). We show that a SAOM can be interpreted as a type of choice model, particularly as a ==== model (RUM; McFadden, 1973, McFadden, 1986), in which network features constitute the characteristics that shape people's utility and people make network-related choices by utility maximization. To prepare this comparison, we first provide a short description of the random utility maximization model. Afterwards, we provide a basic intuitive understanding of how SAOMs work. Thereafter, we engage in a more formal discussion of how SAOMs model choices and show how the model's foundations relate to random utility maximization models. Then, we illustrate the usefulness of SAOMs for choice modelling in a network context by discussing an exemplary application that shows how academic achievement affects friendship formation among adolescents and how adolescents' friendships influence their academic achievement. We close with a short summary of the benefits of SAOMs for choice modelling and an outlook on the further possibilities using SAOMs.",Choice modelling in social networks using stochastic actor-oriented models,https://www.sciencedirect.com/science/article/pii/S1755534520300014,14 January 2020,2020,Research Article,88.0
Herger Nils,"Study Center Gerzensee, Dorfstrasse 2, P.O. Box 21, 3115, Gerzensee, Switzerland","Received 14 January 2019, Revised 23 September 2019, Accepted 29 December 2019, Available online 9 January 2020, Version of Record 21 January 2020.",https://doi.org/10.1016/j.jocm.2019.100201,Cited by (2),"In linear regressions, the ecological fallacy—the erroneous belief that aggregate-level coefficients coincide with individual-level coefficients—arises when individual outcomes depend on the group environment. This paper suggest that such “group effects” determine also the circumstances under which the ecological fallacy vanishes from basic discrete-choice models. In particular, when controlling for group effects, it is shown that the same coefficients arise from a conditional logit model, which is a popular framework to analyse individual choices, and a ====, which provides a framework to analyse the aggregate number (or count) of such choices across groups.","The ecological fallacy highlights the danger of making statistical inferences about individual behaviour from aggregate, or group, data. Despite the well-known pitfalls associated with the ecological fallacy, the statistical analysis of individual decisions has often to resort to an ecological study design, because only aggregate-level data are available. Of course, to what extent this approach is valid depends on the magnitude of the ecological bias.====To appreciate the paradox nature of the ecological fallacy, the literature has often invoked the study of Durkheim (1897), which found a positive correlation between the suicide rate and the share of protestants across Prussian provinces. However, this does not automatically mean that protestants are more likely to kill themselves. At the individual level, the converse relationship could hold if many catholics commit suicide in predominantly protestant areas (see e.g. Greenland and Robins, 1994, p.756). Attempts to infer individual voting decisions by comparing election results across districts provide another classical example of the thorny issues from making the seemingly harmless assumption that statistical relationships obtained at the group level hold automatically at the individual level (see e.g. Kramer, 1983). Other classical examples include the potential bias of inferring individual literacy rates from the demographic structure across geographical areas (Robinson, 1950), individual propensities to commit a crime from aggregate crime data (Firebaugh, 1978, p.558), or the demand function of individual households from aggregate demand conditions across all households of a country (Theil, 1971, pp.560ff.).====Robinson (1950) provided the path-breaking analysis of the difference between an ecological and an individual correlation coefficient. Goodman (1953, 1959) pioneered the ecological regression technique, where the explanatory variables and the dependent (or explained) variable are expressed in terms of averages within groups (typically across geographical areas). In general, coefficient estimates from aggregate data cannot replace an analysis of the underlying individuals; the ecological fallacy vanishes only under “very special circumstances” (Goodman, 1953, p.663). However, it was not until Firebaugh (1978) and Piantadosi et al. (1988), when these circumstances were clearly defined. In particular, Firebaugh (1978, pp.559ff.) suggested that the ecological fallacy vanishes from linear regressions, when the group environment has no effect on the dependent variable at the individual level, after controlling for individual-level explanatory variables. In a similar vein, Piantadosi et al. (1988, pp.895ff.) have found that the ecological fallacy vanishes from linear regressions in the absence of “group effects”, due to which statistical relationships could differ across groups. Taken together, it is perhaps intuitive that the distinction between individual and group-level coefficient estimates matters, when the group environment affects individual behaviour.====Hitherto, the literature on the ecological fallacy has, by and large, been confined to linear correlations and regressions. This is perhaps problematic, because social sciences regularly contemplate choices over a finite number of alternatives (or options). Often, the possibility that a discrete-choice analysis at various levels of aggregation can suffer from an ecological fallacy is simply ignored. Notable exceptions include Bernasco (2010), who modelled the location choice to commit a crime across small and large geographical areas, and Gnaldi et al. (2018), who analysed voting decisions at the individual level and with aggregate data per polling station. However, since choices are inherently discrete, it is well known that they do not lend themselves to a linear regression analysis with a continuous dependent variable, but rather warrant a special class of nonlinear models. In particular, models of the logit class provide the established framework to empirically analyse individual choices (see Train, 2009; Hensher et al., 2005). Aside from the classical example of voting decisions, these types of models have also been used to study the selection of a recreation site (see e.g. Creel and Loomis, 1992), or the corporate-tax effect on multinational firm location decisions (Barrios et al., 2012). Then again, rather than contemplating individual choices, it is, in principle, possible to summarise them into an aggregate variable. For example, such things as voting decisions can be coded into the number (or count) of votes for a given party within an electoral district (Brown and Payne, 1986). Furthermore, in close concurrence with the just-mentioned examples, the selection of a recreational site has also been analysed through the number of recreational trips (see e.g. Creel and Loomis, 1992), and the corporate-tax effect on the choice to establish a firm in a given country through the number of investment decisions (Herger et al., 2016). The statistical analysis of aggregate choices requires, in turn, a specific class of nonlinear models, such as Poisson regressions for count data (see Cameron and Trivedi, 1998; Winkelmann, 2008), or aggregate logit models like Kelejian (1995), which has e.g. been used by Heckelman (1997) to study voting turnout rates.====In principle, there is no reason to believe that the ecological fallacy cannot arise within a discrete-choice setting. Why should a regression of individual choices yield the same coefficient estimates than a regression onto the corresponding aggregate count variable? Moreover, what are the circumstances under which a regression of individual choices yields the same coefficient estimates than a regression onto the corresponding count variable? Are these circumstances comparable to those that have been found for linear regressions? What are the dangers of comparing empirical choice results obtained at the individual level with those employing an aggregate variable of these choices as the dependent variable? This paper endeavours to answer these types of questions.====Although discrete-choice and count regressions have been developed independently, it has been recognised that the underlying logit and Poisson distributions are intertwined (see e.g. Kingman, 1993, pp.5ff.). Thanks to this, the log-likelihood functions of a logit model and a Poisson regression can be sufficiently similar, such that identical coefficient estimates arise (Baker, 1994; Guimarães et al., 2003; Herger and McCorriston, 2013). By exploiting these connections, this paper suggests that the ecological bias vanishes from a discrete-choice environment only under very special circumstances. Similar to the case of linear regressions, it turns out that the presence of group effects with respect to the choice environment matters. In particular, only when controlling for such group effects, identical coefficient estimates arise—e.g. the ecological bias vanishes—from a standard conditional logit model involving individual choices, and a standard Poisson regression involving the count of such choices within a specific group.====The paper is organised as follows. The next section defines the discrete-choice setting. For the sake of comparison, Section 3 reviews the ecological fallacy in linear regressions. Section 4 turns to the case of discrete choices and derives the corresponding connection between discrete-choice models of the conditional logit class and a corresponding Poisson regression. Section 5 illustrates these connections by means of an example. Section 6 summarises and concludes.",On the ecological fallacy in discrete-choice models,https://www.sciencedirect.com/science/article/pii/S175553451930106X,9 January 2020,2020,Research Article,89.0
"Danaf Mazen,Guevara Angelo,Atasoy Bilge,Ben-Akiva Moshe","Massachusetts Institute of Technology, 77 Massachusetts Avenue, Cambridge, MA, 02139, USA,Civil Engineering Department and Instituto Sistemas Complejos de Ingeniería (ISCI), Universidad de Chile, Blanco Encalada 2002, Santiago, Chile,Department of Maritime and Transport Technology, Delft University of Technology, Mekelweg 2, Delft, 2628 CD, the Netherlands,Edmund K. Turner Professor of Civil and Environmental Engineering, Massachusetts Institute of Technology, 77 Massachusetts Avenue, Cambridge, MA, 02139, USA,Instituto Sistemas Complejos de Ingeniería (ISCI), República 695, Santiago, Chile","Received 27 February 2019, Revised 26 September 2019, Accepted 29 December 2019, Available online 2 January 2020, Version of Record 22 January 2020.",https://doi.org/10.1016/j.jocm.2019.100200,Cited by (7),"Endogeneity arises in discrete choice models due to several factors and results in inconsistent estimates of the model parameters. In adaptive choice contexts such as choice-based recommender systems and adaptive ==== (ASP) surveys, endogeneity is expected because the attributes presented to an individual in a specific menu (or choice situation) depend on the previous choices of the same individual (as well as the alternative attributes in the previous menus). Nevertheless, the literature is indecisive on whether the parameter estimates in such cases are consistent or not. In this paper, we discuss cases where the estimates are consistent and those where they are not. We provide a theoretical explanation for this discrepancy and discuss the implications on the design of these systems and on model estimation. We conclude that endogeneity is not a concern when the likelihood function properly accounts for the data generation process. This can be achieved when the system is initialized exogenously and all the data are used in the estimation. In line with previous literature, Monte Carlo results suggest that, even when exogenous initialization is missing, empirical bias decreases with the number of choices per individual. We conclude by discussing the practical implications and extensions of this research.","Endogeneity can arise in discrete choice models due to several factors including measurement errors, selection bias, omitted variables, and simultaneity, and results in inconsistent estimates of the model parameters (Guevara, 2015). The textbook definition of endogeneity is a correlation between the independent/observed variables in the model and the unobserved error term. A broader definition of endogeneity has been provided by Louviere et al. (2005), in which they defined “endogenous” as “all effects that are not exogenous”. In the latter work, the authors attribute endogeneity to model misspecification.====In linear regression models, different methods have been proposed to address endogeneity, the most common of which are instrumental variables (IV) and Two-Stage Least Squares (TSLS). These methods use instruments that are correlated with the endogenous variables, but not with the error term. Other corrections include Heckman correction for selection bias (Heckman, 1979), and the estimation of simultaneous equation models.====In discrete choice models, several corrections have been proposed that mainly fall into two categories; the BLP method (Berry et al., 1995), and the control-function method (Heckman, 1977; Hausman, 1978). The BLP method (also known as the “product-market” control approach) suggests aggregating disaggregate consumer preferences obtained from discrete choice models into an aggregate market-level system, allowing for the application of standard instrumental variable methods. On the other hand, control-function methods use extra variables in the utility specification that are obtained using exogenous instruments. Different control-functions have been proposed, the most common of which are by Petrin and Train (2010), Villas-Boas and Winer (1999), Blundell and Powell (2004), Guevara and Ben-Akiva (2006, 2012), and Guevara and Polanco (2016).====These methods are convenient when a few endogenous variables are identified and relevant instruments are available. However, the cases considered in this paper are of a different nature; we consider adaptive choice contexts such as adaptive stated preferences (ASP) surveys and choice-based recommender systems. When individuals are presented with successive menus (or choice situations), the alternatives and attributes in each menu depend on the attributes and choices in the previous menus. Recommender systems recommend the “best” alternatives to a user (e.g. alternatives that are most likely to be chosen), while ASP surveys usually do the opposite; the attributes of the chosen alternative are deteriorated in order to test whether a respondent will switch.====In the presence of taste variation (e.g. logit mixture with random parameters), the distributions of heterogeneity are assumed to be uncorrelated with the covariates (see Wooldridge, 2010). To illustrate the source of the endogeneity that may arise in adaptive choice contexts, we consider the simple logit mixture model shown in equation (1), where individual ==== utility of alternative ==== includes an alternative specific constant (====) and one attribute with a random parameter ====:====where ====, and ==== is a standard normal random variable independently and identically distributed across individuals.====Endogeneity arises as a result of the correlation between the unobserved heterogeneity (====) and the independent variable ====. In recommender systems, individuals with a (supposedly) higher preference for the attribute ==== (i.e., positive and high value of ====) are recommended alternatives having higher values of this attribute. In turn, in ASP surveys, individuals with a (supposedly) higher preference for ==== are usually presented with a lower value of that attribute in their following choice tasks. If recommendations/choice tasks are generated based on multiple attributes, the above reasoning can be extended to conclude that endogeneity arises in all attributes (as ==== and ==== will be multidimensional).====Under the context described before for recommender systems or ASP surveys, endogeneity can also be explained by model misspecification. The values of ==== depend on the user's previous choices (and thus on this user's preferences ====). A correctly specified model should account for the joint likelihood of the choices (====) and the observed attributes ==== as shown in equation (2). On the other hand, models that do not account for this dependency ==== will be misspecified and might result in biased estimates.====In this paper, we extend the theoretical analyses of Liu et al. (2007) on “adaptive metric utility balance” to choice contexts to demonstrate how endogeneity can cause inconsistent estimation results, and how this inconsistency can be avoided. We show that when the system is initialized with exogenous attributes, and when all menus (or choice tasks) are included in the estimation, the estimates are consistent. On the other hand, excluding data from the estimation leads to inconsistent estimates. We note that this paper only addresses endogeneity that arises as a result of the adaptive nature of these contexts, however, there might be other sources of endogeneity that are outside the scope of this paper, such as measurement errors, omitted variables, and self-selection.====The remainder of this paper is organized as follows. Section 2 presents an overview of adaptive contexts such as recommender systems and ASP surveys. Section 3 presents the methodology used in adaptive choice scenarios and a theoretical analysis of endogeneity. A Monte Carlo experiment mimicking a dynamic recommender system is presented in Section 4. Section 5 discusses the practical implications on recommender systems, ASP surveys, and RP/SP estimation. Finally, Section 6 concludes the paper.",Endogeneity in adaptive choice contexts: Choice-based recommender systems and adaptive stated preferences surveys,https://www.sciencedirect.com/science/article/pii/S1755534519301058,2 January 2020,2020,Research Article,90.0
"Pedersen Line Bjørnskov,Mørkbak Morten Raun,Scarpa Riccardo","DaCHE –Danish Centre for Health Economics, Department of Public Health, University of Southern Denmark, J.B. Winsløwsvej 9B, 5000, Odense C, Denmark,Research Unit for General Practice, University of Southern Denmark, J.B. Winsløwsvej 9A, 5000, Odense C, Denmark,Danish Economic Councils, Emil Møllers Gade 41, 8700, Horsens, Denmark,Durham University Business School, Durham University, Mill Hill Lane, Durham, DH1 3LB, United Kingdom,Department of Business Administration, University of Verona, Via Cantarane, 24 - 37129, Verona, Italy,School of Accounting, Economics & Finance, University of Waikato, Te Whare Wananga o Waikato, Gate 1 Knighton Road, Private Bag 3105, Hamilton, 3240, New Zealand","Received 27 February 2019, Revised 4 November 2019, Accepted 18 November 2019, Available online 25 November 2019, Version of Record 29 November 2019.",https://doi.org/10.1016/j.jocm.2019.100199,Cited by (2),"Health economists often use discrete choice experiments (DCEs) to predict behavior, as actual market data is often unavailable. Manski (1990) argues that due to the incompleteness of the hypothetical scenarios used in DCEs, substantial uncertainty surrounds stated choice. Uncertainty can be decomposed into “resolvable” and “unresolvable”; the former is expected to become resolved in actual choice, as individuals collect further information. To enable its identification, Manski suggests eliciting subjective choice probabilities (ECPs) rather than discrete choices. We introduce the ECP approach in health economics and explore its convergent validity. The context is future physicians’ stated choices of job in rural general practice in Denmark. Our results are mixed, but show remarkable similarities in forecasting abilities, despite the ECP models being less econometrically demanding and relying on different preference distributional assumptions.","Forecasting choice behavior in health economics is challenging because actual data is often unavailable. In order to derive estimates of health care demand and/or supply, health economists often resort to data derived from hypothetical choice scenarios. An increasingly popular way of doing this is by means of discrete choice experiments (DCEs) in which respondents select their favorite alternative between two or more hypothetical scenarios describing the available treatments, services or, in this case, general practices. Data collected in this manner are then used to estimate random utility models, based on specific assumptions on behavior, from which expectations of real choice behavior are forecast (see e.g. Brown et al., 2015; Meenakshi et al., 2012; Sivey et al., 2012; Lancsar et al., 2011). Manski (1990) argues that statements of choice intentions might not be good predictors of future behavior. This because respondents taking part in a hypothetical DCE are likely to be provided with only a subset of the information deemed subjectively relevant or even necessary to make a real-life choice. Manski (1999) referred to this limitation as DCEs suffering from “incomplete scenarios” and repeated what Fischhoff et al. (1999) wrote “If needed detail is missing, then people may make it up.”====Uncertainty generates a systematic divergence between hypothetical and actual choice. The divergence is caused by the necessarily incomplete information base provided to respondents when eliciting their choice intentions. This information gap causes what was termed “resolvable uncertainty” (Manski, 1999) where the term “resolvable” indicates that to make real life choices respondents expect (and are expected) to acquire further information in order to reduce the overall uncertainty. In our study, we deal with the preferences future physicians hold over types of employment as general practitioners (GPs) in a general practice in rural locations in Denmark. This is currently an important area of investigation as there is a severe shortage of GPs (Marchand and Peckham, 2017). Also, it is likely that when junior doctors make actual job choices, they would further seek and process details about working conditions in different general practices. Hence, it is plausible to expect that once faced with a real choice scenario, respondents would have resolved some of the uncertainty surrounding the hypothetical choice context. Cognizant of this fact, analysts are faced by an extrapolation problem in which assumptions are likely to play a crucial role. However, the consequences of such information discrepancy and their impact on standard assumptions in the analysis of choice data have rarely been explored in the empirical literature.====In the standard DCE framework, the issue of incomplete scenarios is typically handled by assuming that what remains undescribed in the characterization of alternatives has no systematic effect on utilities of alternatives as evaluated by respondents, except perhaps by inflating error variance. Some studies report asking respondents to score how certain they are about their stated choice and they use this score in heteroskedastic choice models. Because of limitations in both cognitive effort in information processing and in experimental design dimensions, it is of course impossible to include all potentially salient characteristics of alternatives in a DCE setting. However, eliciting subjective choice probabilities could potentially overcome this issue, by allowing respondents to be explicitly uncertain about their stated subjective choice. This approach was first proposed by Manski (1999) and later applied by Blass et al. (2010) who also demonstrated its additional advantage in giving rise to specifications with estimations that are substantively less econometrically demanding, thereby giving rise to a claim of a more robust inference.====The elicitation of subjective choice probabilities has only been applied in a few stated choice studies, and never in the area of health economics. Blass et al. (2010) show how the approach can empirically be applied to data on consumers’ preferences for the reliability of electricity services in Israel. Shoyama et al. (2013) used this approach for eliciting public preference for land-use scenarios in Kushiro watershed in northern Japan and find some divergence between willingness to pay (WTP) estimates obtained from standard DCE data and the alternative elicited choice probability (ECP) approach. In a working paper on lake recreation, Herriges et al. (2011) use the 2009 Iowa Lake Survey to administer a split treatment in terms of information provision (low and high) and preference elicitation method (DCE versus ECP). They find significant differences between the two formats in terms of implied preferences for two hypothetical lake scenarios. More generally, subjective choice probabilities have recently been used within labor economics – studying e.g. the choices of major subjects by college students and income expectations in American households, although not in a choice experiment framework (Dominitz and Manski, 1997; Arcidiacono et al., 2012; Wiswall and Zafar, 2015).====In this study we contribute to the sparse literature on this issue by examining the convergent validity of the DCE and the ECP approaches. We focus on the hypothesis that, when some resolvable uncertainty is allowed to be expressed by using subjective probabilities, the structure of utility differ from that underlying discrete choices where this uncertainty cannot be expressed.====Unlike previous studies using this approach, we sample a population of future medical doctors. Compared to the populations sampled in other studies, our respondents are extremely well-educated and have all been exposed to academic training in the field of probabilities. We first analyze the data from the DCE using conventional approaches based on random parameter mixed logit (RPL) models of discrete choice. Then from the ECP using both RPL models and, following Blass et al. (2010) the more robust least absolute deviations (LAD) regressions.",Handling resolvable uncertainty from incomplete scenarios in future doctors' job choice – Probabilities vs discrete choices,https://www.sciencedirect.com/science/article/pii/S1755534519301046,25 November 2019,2019,Research Article,91.0
"Rossetti Tomás,Hurtubia Ricardo","Department of Transport Engineering and Logistics, Pontificia Universidad Católica de Chile, Vicuña Mackenna, 4860, Macul, Santiago, Chile,Centre for Sustainable Urban Development (CEDEUS), Vicuña Mackenna, 4860, Macul, Santiago, Chile,Systems Science and Engineering, Cornell University, Vicuña Mackenna, 4860, Macul, Santiago, Chile,School of Architecture, Pontificia Universidad Católica de Chile, Vicuña Mackenna, 4860, Macul, Santiago, Chile,Instituto de Sistemas Complejos de Ingeniería (ISCI), Vicuña Mackenna, 4860, Macul, Santiago, Chile","Received 17 April 2019, Revised 16 October 2019, Accepted 6 November 2019, Available online 12 November 2019, Version of Record 20 November 2019.",https://doi.org/10.1016/j.jocm.2019.100198,Cited by (24),"Images, videos, and virtual reality have been widely used in the literature to portray complex attributes to survey respondents. It is reasonable to expect immersive videos will be increasingly used in the future due to their decreasing costs and potentially more accurate representation of reality. Nevertheless, the literature has not sufficiently tested their ecological validity, which can be defined as the extent to which the results they produce in a laboratory setting, such as in choice experiments, are close enough to the results that would have been obtained in a real-life setting. The following work presents a comparison of two representation formats, images and immersive videos, to verify if they can elicit the same perceptual responses of ==== as real environments. To do this, a survey was carried out using these two formats as well as on-site interviews. Using a MIMIC approach, and after controlling for all relevant sociodemographic variables, results show that perceptions elicited through immersive videos were not different from those elicited in reality in one qualitative variable (perception of safety and security) out of three relevant ones identified. Furthermore, results also show immersive videos can induce a smaller distortion than photographs.","Images and videos have been widely used in stated preference (SP) experiments to portray complex constructs to respondents such as qualitative variables or attributes that are hard to communicate through text, like style or aesthetic design. Images are also able to provide abundant information to respondents while being able to be read more quickly than text. Nevertheless, there are concerns over their ability to properly portray the alternatives in question.====Indeed, all representation methods used in stated preference surveys run the potential risk of misrepresenting reality, and therefore being unable to reproduce the choices respondents would have made in a real-life setting. This specific validity problem, understanding validity as “the extent to which a research procedure measures what it is supposed to measure” (Bronfenbrenner, 1977), has been referred to in the environmental psychology literature as ====. Ecological validity can be defined as “the applicability of the results of laboratory analogues to non-laboratory, real-life settings” (McKechnie, 1977), which in other words means that an ecologically valid experiment is one that is able to produce the same results in a simulated setting (e.g., a stated preference survey) and in a real scenario (a real choice setting).==== Some studies from the fields of psychology and landscape studies have measured the ecological validity of several representation formats, including text, images, and virtual reality, arriving at mixed results (see, for example, Higuera-Trujillo et al., 2017; Farooq et al., 2018). In other words, there is no consensus around a representation format or visual stimuli that is ecologically valid.====We believe it is very likely that immersive videos will be used significantly more in the future to collect SP data due to its reducing costs and presumably better representation of reality. To anticipate this trend, it is important to verify if this representation format is better than already existing ones to portray complex attributes. If, for example, immersive videos are as ecologically invalid as conventional photographs, then it would not make sense to use this technology since it implies a much more complex construction of the scenarios and data collection process. On the other hand, if there is an improvement, then researchers should prefer it over other formats.====In this study, we compare two representation formats, photographs and immersive videos, and verify their ecological validity. To do this, a survey concerning pedestrians’ perceptions of public spaces was deployed in three formats: (1) at three preselected locations by interviewing pedestrians; (2) by showing immersive videos of these locations to respondents; (3) by showing photographs of these locations to respondents. A multiple indicators, multiple causes approach (MIMIC; Jöreskog and Goldberger, 1975) was used to measure differences in perceptions according to sociodemographic characteristics and representation formats. After controlling for all observable variables available, we found a perceptual variable in which immersive videos could accurately mimic reality (perceived safety and security), while in two others both representation formats induced biases in responses. We also found that immersive videos induced smaller or equal biases than images, leading us to believe this representation format should generally be preferred over photographs.====The following sections detail how we arrived at this conclusion. Section 2 reviews the literature and shows when different representation methods have been used, as well as which studies have compared them. Section 3 describes this study's data collection process, and Section 4 details the methodology used to obtain the results shown in Section 5. Section 6 provides some concluding remarks and future research paths.",An assessment of the ecological validity of immersive videos in stated preference surveys,https://www.sciencedirect.com/science/article/pii/S1755534519301034,12 November 2019,2019,Research Article,92.0
"Danaf Mazen,Atasoy Bilge,Ben-Akiva Moshe","Massachusetts Institute of Technology, 77 Massachusetts Avenue, Cambridge, MA, 02139, USA,Department of Maritime and Transport Technology, Delft University of Technology, Mekelweg 2, Delft, 2628 CD, the Netherlands,Civil and Environmental Engineering, Massachusetts Institute of Technology, 77 Massachusetts Avenue, Cambridge, MA, 02139, USA","Received 9 April 2019, Revised 29 August 2019, Accepted 20 October 2019, Available online 23 October 2019, Version of Record 13 May 2020.",https://doi.org/10.1016/j.jocm.2019.100188,Cited by (5),"Logit mixture models have gained increasing interest among researchers and practitioners because of their ability to capture unobserved taste heterogeneity. Becker et al. (2018) proposed a Hierarchical Bayes (HB) estimator for logit mixtures with inter- and intra-consumer heterogeneity (defined as taste variations among different individuals and among different choices made by the same individual respectively). However, the underlying model relies on strong assumptions on the inter- and intra-consumer mixing distributions; these distributions are assumed to be normal (or log-normal), and the intra-consumer covariance matrix is assumed to be the same for all individuals. This paper presents a ==== extension to the model and the estimator proposed by Becker et al. (2018) to account for flexible, semi-parametric mixing distributions. This relaxes the normality assumptions and allows different individuals to have different intra-consumer covariance matrices. The proposed model and the HB estimator are validated using real and synthetic data sets, and the models are evaluated using goodness-of-fit statistics and out-of-sample validation. Our results show that when the data comes from two or more distinct classes (with different population means and inter- and intra-consumer covariance matrices), this model results in a better fit and predictions compared to the single class model.","Because of their ability to capture unobserved taste heterogeneity, logit mixture models have gained increasing interest among researchers and practitioners. These models can be estimated using classical and Bayesian methods, the most common of which is Maximum Simulated Likelihood (MSL). In the Bayesian context, the Hierarchical Bayes (HB) estimator for logit mixture models has been widely applied and documented (Allenby, 1997; Allenby and Rossi, 1998; Train, 2009; etc.). This estimator uses three Gibbs layers, drawing from the population means, covariance matrix, and individual-specific parameters respectively.====Becker et al. (2018) extended this estimator to account for inter- and intra-consumer heterogeneity (defined as taste variations among different individuals and among different choices made by the same individual respectively). However, the underlying model relies on strong assumptions on the inter- and intra-consumer mixing distributions of preferences.====The first assumption is that the mixing distributions are normal (or log-normal), i.e. the choice-specific parameters are normally distributed around the individual-specific means, which are in turn normally distributed around the population means. The key limitation of these parametric distributions is the assumption of uni-modality (Hess, 2014). Several studies have shown that nonparametric mixture models (i.e. latent class) can sometimes outperform continuous mixture models (e.g. Vij and Krueger, 2017), especially when the number of observations per individual is small (Andrews et al., 2002). According to Vij and Krueger (2017), semi-parametric distributions allow for modeling complex patterns of heterogeneity that cannot be captured using uni-modal distributions. These distributions can asymptotically mimic any shape (however, this is not always possible due to data and computational limitations).====The second assumption is that the intra-consumer covariance matrix is assumed to be the same for all individuals, which means that the level of heterogeneity across different choices of a given individual is the same across the population. Becker et al. (2018) justified this assumption because it is not possible to estimate individual-specific covariance matrices given the small number of observations per individual in typical datasets.====This paper extends the logit double mixture model and the estimator proposed by Becker et al. (2018) to account for flexible, semi-parametric mixing distributions of unobserved taste heterogeneity. This extension is an important contribution as it overcomes the abovementioned limitations:====Semi-parametric distributions have not been used before in logit mixture models with inter- and intra-consumer heterogeneity, even though they have proven useful in models with inter-consumer heterogeneity only (Rossi et al., 2005; Bujosa et al., 2010; Greene and Hensher, 2013; Keane and Wasi, 2013; and Krueger et al., 2018). In this paper, we propose a Hierarchical Bayes (HB) estimator for these models based on Gibbs sampling with embedded Metropolis-Hastings (MH) algorithms, and demonstrate that they can outperform logit mixture models with unimodal (normal or log-normal) distributions of inter- and intra-consumer heterogeneity. Finally, we show that the optimal number of classes can be determined empirically using out-of-sample validation.====The remainder of the paper is divided as follows. Section 2 presents a brief background on flexible mixing distributions and models with intra-consumer heterogeneity. Section 3 presents the proposed extension and the Hierarchical Bayes (HB) estimator. Section 4 presents an application with synthetic data to validate the estimator presented in section 3. Another application with real data is presented in Section 5. Finally, Section 6 presents a discussion of the applications, advantages, and limitations of the model, and Section 7 concludes the paper.",Logit mixture with inter and intra-consumer heterogeneity and flexible mixing distributions,https://www.sciencedirect.com/science/article/pii/S1755534519300934,23 October 2019,2019,Research Article,93.0
Frith Michael J.,"Department of Security and Crime Science, University College London, 35 Tavistock Square, WC1H 9EZ, UK","Received 19 September 2018, Revised 25 September 2019, Accepted 3 October 2019, Available online 7 October 2019, Version of Record 15 October 2019.",https://doi.org/10.1016/j.jocm.2019.100187,Cited by (9),"One of the central topics in crime research, and one in which ","One of the central topics in criminological research, and one in which discrete choice modelling [DCM] has been introduced relatively recently (Bernasco and Nieuwbeerta, 2003, 2005) but has since become increasingly popular, is the study of offence location choices. Or, more simply, the study of where criminals choose to offend and why they choose those locations. Prior to the introduction of DCM, these types of analyses generally employed one of two approaches: the target-based approach (e.g. Sampson, 1985), which analyses the relationship between the characteristics of locations (such as the demographic makeup of the residents) and the number of offences. Alternatively, the offender-based approach (e.g. Wiles and Costello, 2000) analyses the relationship between distance from each offender's home and the number of offences that offender has committed. While both approaches are not unreasonable, various research including qualitative studies supports that offending decisions are generally influenced by ==== types of factors (e.g. Bennett et al., 1984; Rengert and Wasilchick, 1985; Wright and Decker, 1996), and so separate approaches do not account for their joint influence. More specifically, target-based analyses cannot incorporate proximity (for the exceptions, see Bernasco and Luykx, 2003; Bernasco and Block, 2011) and so do not address it directly as a crucial factor. Likewise, offender-based analyses do not incorporate the characteristics of the potential offence locations.====In contrast, DCM in these types of analyses takes the form for each offence of the offender choosing from a set of locations to offend in and can simultaneously include both types of variables. The models can include proximity (and other similar variables) as an individual-alternative-specific variable as it will vary for each individual (offender) based on the locations of where they live and each alternative. The models can include the characteristics of each location as alternative-specific variables as they will vary for each possible offence location but will remain identical for all offenders. In addition, they can also incorporate individual-specific variables, such as age or ethnicity, to examine their effects on offending decisions.====In 2003 and 2005, Bernasco and Nieuwbeerta introduced this approach to the study of offence location choices by using it to analyse residential burglaries in The Hague (Netherlands) and found that the offenders preferred shorter crime trips and offending in locations with more targets, with more accessible targets and ethnically heterogeneous residents that are expected to be less likely to act as guardians for each other with the latter being especially the case for non-native offenders. Since then the approach has been used in over 25 published studies of offence location choices (Frith, 2019; see also Ruiter, 2017 for a recent review) and has been extended, for example, to find that other sources of residential heterogeneity can impact crime trips (Johnson and Summers, 2015; Frith et al., 2017) and that the residential (e.g. Bernasco, 2010; Bernasco and Kooistra, 2010) and offending (Lammers et al., 2015) history of offenders also impacts their crime location choices. Although many of these studies have investigated acquisitive crimes such as residential burglary (e.g. Bernasco and Nieuwbeerta, 2005; Bernasco, 2006), robbery (e.g. Bernasco and Block, 2009; Bernasco et al., 2013) and theft from vehicles (Bernasco, 2010; Johnson and Summers, 2015), others have analysed offences such as violence (Summers, 2012) and terrorism (Marchment and Gill, 2018) and found the offenders generally behaviour similarly. Some studies have also analysed groups of offences altogether were the offenders are assumed and generally found to share similar crime location choice criteria (e.g. Bernasco, 2010; Lammers et al., 2015). These studies have also been conducted, with relatively similar results, in a range of study areas from the Netherlands (e.g. Bernasco and Nieuwbeerta, 2005; Bernasco, 2006) and England (Summers, 2012; Baudains et al., 2013) to USA (e.g. Bernasco and Block, 2009; Bernasco et al., 2013) and Australia (Clare et al., 2009; Townsley et al., 2015).====In all but three of these analyses the conditional logit [CL] (McFadden, 1974) has been used. This model assumes that offender preferences are homogenous, or that they only systematically differ between broad sub-groups, for example between juvenile and adult offenders. Despite this, various criminological research including qualitative (e.g. Bennett et al., 1984; Rengert and Wasilchick, 1985; Wright and Decker, 1996) and other non-DCM quantitative studies (e.g. Townsley and Sidebottom, 2010; Bouhana et al., 2016) find offenders vary in their decision-making criteria. As such, three more recent studies (Townsley et al., 2016; Frith et al., 2017; Long et al., 2018) acknowledge that offender preferences are likely to vary including in subtle or unobservable ways. These analyses model this unobserved heterogeneity using the mixed logit [ML] model (McFadden and Train, 2000; Hensher and Greene, 2003), which assumes the preferences belong to some continuous distribution. Townsley et al. (2016) and Frith et al. (2017) both investigated individual-level heterogeneity and found significant amounts of variation in the effects of the observed variables across their samples and that their ML models fit the data better than the equivalent CL models. Long et al. (2018) instead investigated alternative-level heterogeneity but reported not detecting any significant unobserved heterogeneity specific to the alternatives. Based on these first analyses of heterogeneity in offence location choices there are several open research questions. In particular regarding taste heterogeneity and its scale across (other samples of) offenders, its underlying distributions, and any potentially observable sources.====Based on this, and whilst so-far unused in this literature, there is a second model that is popular in the wider literature that also deals with unobserved heterogeneity, the latent class logit [LCL] (Lazarsfeld and Henry, 1968; McLachlan and Peel, 2000). As an alternative to the continuous distributions in ML, LCL allows for unobserved heterogeneity by estimating joint discrete distributions of preferences (classes) and assigning decision-makers (offenders) to each class on a probability basis. The probabilistic class allocations can also be related to individual-level characteristics. One key advantage of the LCL, compared to ML, is that the distribution of preferences does not need to be specified; though the number of classes do. In fact, in many of comparisons of the two models (Greene and Hensher, 2003; Hess et al., 2011), neither is to be assumed superior - although the extent to which this will be true for (each sample of) offenders will depend on the actual underlying distributions of preferences.====This article furthers this growing area of research through the introduction and testing of the LCL and comparing it to the equivalent CL and ML models. This is through an analysis of the offence location decisions of serious acquisitive crime [SAC] offenders which includes residential burglars, robbers and theft of and from vehicle offenders in York (UK). This study therefore contributes to the criminological literature by:====The remainder of this article is organised as follows. In the next section, the DCM analytical framework is described in relation to offence location choice research and the models that will be used in this analysis. The second section describes the data and the analytical strategy employed. The third and fourth sections presents and discusses the results respectively.",Modelling taste heterogeneity regarding offence location choices,https://www.sciencedirect.com/science/article/pii/S1755534519300922,7 October 2019,2019,Research Article,98.0
"Schmidt Alejandro,Ortúzar Juan de Dios,Paredes Ricardo D.","Departamento de Ingeniería de Transporte y Logística, Pontificia Universidad Católica de Chile, Vicuña Mackenna 4860, Macul, Santiago, Chile,Departamento de Ingeniería de Transporte y Logística, Instituto Sistemas Complejos de Ingeniería (ISCI), Pontificia Universidad Católica de Chile, Vicuña Mackenna 4860, Macul, Santiago, Chile,Escuela de Ingeniería, Pontificia Universidad Católica de Chile & Duoc UC, Eleodoro Yañez 1595, Providencia, Santiago, Chile","Received 29 January 2018, Revised 21 July 2019, Accepted 9 September 2019, Available online 18 September 2019, Version of Record 24 September 2019.",https://doi.org/10.1016/j.jocm.2019.100185,Cited by (2),"The huge increase in higher education coverage in many developing countries has gone hand-in-hand with an additional supply of private colleges and with the enrolment of low to middle-class students, previously excluded from a historically elitist education segment. The larger diversity of both “suppliers and consumers”, unseen a few years ago, calls for methodological approaches that recognize heterogenous tastes and eventually, to classify individuals into mutually exclusive groups, something that can improve the design of public policy.====The importance of college choice in educational systems using voucher schemes, makes it relevant to know what are the main variables determining such choice and whether they differ among different groups. Chile, one of the countries with the most extensive voucher system in education, experienced a significant increase in higher education enrolment (over 250% over the last 15 years), and has faced fierce political controversy due to the high heterogeneity in college quality.====, and that those groups are not only differentiated by their income level but mainly, by how they performed at high school. From the different sensitivities to college characteristics such as cost, quality, and location, identifying these groups allows us to derive different policy prescriptions.","The growth rates of “middle-class” economies in several parts of the world have seen the emergence of different classes of individuals, consumers and potentially target groups. Commercial effectiveness and better tailor policies require identifying and characterizing the way these groups decide. Differentiating groups has become more relevant in many sectors, particularly those with high income elasticity, such as cultural goods and higher education.====Until not long ago, the elitism associated with higher education in less developed countries meant that only a small number of traditional colleges served a quite homogeneous group. This way, high income, mostly male students, coming from selective schools, attended to a fairly small number of colleges. Most countries however, including the less developed ones, have experienced lately a strong raise in higher education coverage, particularly because students from low and middle-income sectors are now completing schooling. Thus, college choice is becoming a relevant issue in countries that have managed to lower the barriers to entry into the higher education sector and where massive access policies have led to high heterogeneity of institutions and students.====The Chilean case is particularly interesting in this sense, because the country experienced one of the most substantial changes in higher education worldwide, in terms of coverage and diversity of institutions (Espinoza and Urzúa, 2018). Chile's data has also certain unique advantages. For instance, tuition fees are largely determined by the historical setting of the so called ==== (reference fees), which are values set by the Ministry of Education in the mid 1990s and therefore bear no relationship with demand or supply excesses (avoiding endogeneity). On the other hand, the state subsidies behind housing policies, for example, imply that the distance between colleges and homes is largely exogenous, allowing a reasonable identification of residence location as determinants of college choice. Finally, the extended use of a voucher system (i.e. the government subsidizes the schools chosen by parents in direct proportion to the size of enrollment) at all levels of the Chilean education system, makes the analysis of college choice especially relevant in this case.====The main purpose of this paper is to provide evidence about the potential differences in college choice among different groups of individuals and to identify those groups. Using administrative data from admissions to college in Chile, we examine choice patterns using Latent Class Logit (LCL) models. In fixed parameter models, such as the multinomial logit (MNL) or nested logit (NL) models, the way to identify systematic differences in preferences does not allow identifying classes, something that LCL models do, and that is highly useful from a policy perspective. The alternative to use models with random parameters, such as Mixed Logit (ML), needs the researcher to deal with complex identification and interpretation problems (Train, 2009; Ortúzar and Willumsen, 2011).====The LCL model, progressively used in psychology and economics (Bauer, 2007; Kohn et al., 1976; Hess, 2014), allows to detect attribute valuations that may differ among groups. Specifically, we found that allowing for heterogeneous preferences without a predefined structure improved the model fit in comparison to simpler models; in particular, we found three clearly defined classes, characterized in terms of their income, parent's education and schooling performance, which could not had been defined ====. These shed light on the perception of college quality as revealed by individual preferences and allowed us to focus on policies for the most needed, a prescription particularly relevant in less developed countries.====The rest of the paper is organized as follows. The second section presents a brief review of the literature and describes the Chilean higher education context. The third section outlines the methodology and describes the data available. The fourth section presents our main results, and in the fifth we discuss some policy conclusions.",Heterogeneity and college choice: Latent class modelling for improved policy making,https://www.sciencedirect.com/science/article/pii/S1755534519300909,18 September 2019,2019,Research Article,99.0
"Loría Luis Enrique,Watson Verity,Kiso Takahiko,Phimister Euan","Health Economics Research Unit. University of Aberdeen, AB25 2ZD, United Kingdom,Business School. University of Aberdeen, AB24 3FX, United Kingdom","Received 1 June 2017, Revised 2 May 2019, Accepted 9 May 2019, Available online 21 May 2019, Version of Record 28 May 2019.",https://doi.org/10.1016/j.jocm.2019.05.001,Cited by (7),None,"Public transport plays a vital role in mobilising people to their workplaces and leisure centres across urban settings. In the United Kingdom (UK), buses are the most common form of public transport (Department for Transport, 2018). The majority of buses are powered using diesel fuel, which emits different types of emissions when combusted. These emissions contribute to climate change and create pollution hotspots in city and town centres==== (Potter, 2003). Given the diversity of effects, emissions from diesel fuel buses can be categorised into those having predominantly global (i.e. climate change) and local effects (i.e. increased health problems in communities). There is limited evidence about which of the two types of emission is valued more highly, or whether individuals are willing to trade-off between them.====The introduction of Low Emission Buses (LEB) could reduce the amount of emissions generated from bus travel. LEBs are defined as a bus that “operates using efficient technology or alternative fuels rather than just a traditional diesel engine. They are defined by the UK Government as producing 15% less Well-to-Wheel==== emissions compared with an equivalent Euro V standard diesel bus” (Low Carbon Vehicle Partnership, 2018). Existing LEBs in Europe are all part of small-scale pilot fleets, implemented in isolation from one another, typically without secured long-term financing. Two recurrent barriers to the introduction of new LEB fleets or the expansion of existing current ones are high upfront costs and uncertainty about whether bus users value reduced emissions.====This research presents the findings of a Discrete Choice Experiment (DCE) administered amongst bus users to examine their preferences for the different characteristics of bus travel, including different types of emissions. This DCE was administered in Aberdeen (Scotland), which is home to Europe's largest hydrogen bus fleet - the Aberdeen Hydrogen Bus Project (AHBP). The AHBP introduced 10 hydrogen fuel cell buses on two local routes that were previously serviced using diesel engine buses. The way in which the AHBP was introduced created a natural experiment in which bus users differ in their experience using a zero emission bus. This allows us to test how bus users' direct experience affects their preferences and explore the role of experience and familiarity on individuals' stated preferences. In doing so, we contribute to the limited number of studies in this area (Cherchi and Hensher, 2015).====We find that Aberdeen bus users place a higher value on reductions of local pollutant emissions over those emissions that relate to global climate change. This questions the common practice in stated preference studies of framing emission attributes in terms of carbon or greenhouse gas emissions only. We also find evidence of the effect of direct experience in preference formation. Specifically, bus users who regularly use hydrogen buses display different preferences for the comfort inside the bus and emissions, compared to users who occasionally or never use hydrogen buses.====The outline for this paper as follows: first, we present an overview of previous stated preference studies that value emissions from road transport, and research on the role of experience in preferences for environmental goods. Second, we describe the DCE survey and data collection methods. Third, we present the results obtained across several model specifications in terms of the values for the different bus attributes. We test the role of experience on preferences by separating the sample according to the level of experience using a hydrogen bus. Fourthly, we discuss the policy implications of our results.",Investigating users' preferences for Low Emission Buses: Experiences from Europe's largest hydrogen bus fleet,https://www.sciencedirect.com/science/article/pii/S1755534517301094,21 May 2019,2019,Research Article,100.0
"Bansal Prateek,Hurtubia Ricardo,Tirachini Alejandro,Daziano Ricardo A.","Cornell University, United States,Pontificia Universidad Católica de Chile, Chile,Universidad de Chile, Chile","Received 16 June 2018, Revised 10 April 2019, Accepted 25 April 2019, Available online 3 May 2019, Version of Record 14 May 2019.",https://doi.org/10.1016/j.jocm.2019.04.004,Cited by (23), an important avenue for future research.,"Methods for very flexible representations of preference heterogeneity in discrete choice models are evolving rather quickly with computationally efficient software implementations; however, practical consequences of using these models are less clear. In this paper, we highlight the implications of using differing flexible mixing distributions by estimating heterogeneity in the valuation of subway crowding. We discuss empirical and modeling details of this work below.====Subway demand in New York City has been on the rise, reaching 1.8 billion trips by 2014 in a system that has not grown in size. As demand approaches capacity of the system, subway riders experience overcrowding conditions – which is the equivalent of passenger congestion. From an economic perspective, overcrowding is an externality that provokes a welfare loss on passengers due to travel time disruptions in terms of delays and discomfort experienced while traveling. In the New York subway, overcrowding delays have extended to non-peak hours and weekends; weekend overcrowding delays grew 141.2% in 2014–2015 (weekday overcrowding delays increased 65.3% during the same period; NY Daily News, 2015). Even though overcrowding has been recognized as a major and ever-growing problem of the NYC subway, valuation of the nonmonetary costs of crowding for the specific case of New York, unlike many other major cities, has not been performed.====In transportation economics, authors usually aim at estimating a crowding multiplier (CM) (Basu and Hunt, 2012; Tirachini et al., 2013) – the ratio of value-of-time (VOT) estimates under crowded and uncrowded conditions. CM can be estimated by quantifying the trade-off made by a traveller when accepting a marginal increase in travel time for lesser crowding. The relevance of estimating users’ valuation of crowding reductions goes well beyond academic studies into policy and supply decisions. Quantification of the crowding disutility allows policymakers to estimate the welfare gain due to reducing crowding levels in public transportation, as done by Kroes et al. (2014) when assessing a rail extension project in the Paris region (the authors estimated a social benefit of 23 million euros per year due to reducing crowding on the existing rail lines). Thus, not including crowding effects in the cost-benefit analysis may lead to a sustained underinvestment in the capacity expansion of mass transit. The optimal design of a public transportation system is also affected by crowding discomfort – an increased value of travel time savings due to crowding induces a larger optimal service frequency and vehicle size (Jara-Díaz and Gschwender, 2003). Regarding pricing, studies like (Tirachini et al., 2014) have shown that crowding externalities increase the optimal public transportation fare, which is usually introduced as a demand management measure.====Therefore, monetizing the crowding discomfort that users experience is of paramount relevance to support investment, design and pricing decisions in public transportation systems. Countries such as Australia, Sweden, France and the United Kingdom have already included the valuation of crowding on their official guidelines for public transportation project appraisal (Wardman, 2014). Thus, having a sound econometric approach to support the monetization of crowding externalities is certainly desirable from a public policy perspective.====Whereas Wardman and Whelan (2011) compared crowding multiplier estimates of 17 studies in Great Britain, Tirachini et al. (2017) reviewed crowding valuation estimates across the globe including new evidence for the subway system of Santiago in Chile. Most of these studies have derived crowding multipliers using a conditional (or multinomial) logit model (MNL), perhaps due to the ease of incorporation of MNL estimates into supply-side models (Tirachini et al., 2014). A few studies have modelled unobserved preference heterogeneity using the parametric mixed multinomial logit (MMNL) model (Whelan and Crockett, 2009; Basu and Hunt, 2012; Tirachini et al., 2017). Whereas Whelan and Crockett (2009) adopted the standard assumption of normally distributed parameters, Basu and Hunt (2012) used a constrained triangular distribution to avoid extreme values, and Tirachini et al. (2017) considered an analytically tractable lognormal distribution. From an econometric perspective, the current crowding multiplier estimators succumb to two problems: ====, parametric assumptions on unobserved preference heterogeneity can misspecify the shape of the underlying mixing distribution (Caputo et al., 2018; Bansal et al., 2018a; Bazzani et al., 2018). ==== because crowding valuation involves estimates of marginal rates of substitution, the analyst working with random parameter choice models faces the many known issues that have been described for inference on parameter ratios (Train and Weeks, 2005; Hole and Kolstad, 2012; Owusu Coffie et al., 2016).====This study addresses the parametric constraints and working with ratios in the estimation of crowding multipliers and makes a two-fold contribution to the crowding valuation literature. ====, we propose and explore estimation using a ‘crowding-multiplier (CM) space’ specification, which being analog to willingness-to-pay space (Train and Weeks, 2005) allows researchers to directly specify random heterogeneity in crowding multipliers (instead of working with the ratio of two random parameters). ====, we illustrate applications of state-of-the-art semi-nonparametric logit models, namely the logit-mixed logit (LML) (Train, 2016; Bansal et al., 2018b) and mixture-of-normals MNL (MON-MNL) (Rossi et al., 2005; Keane and Nada, 2013),==== which offer the flexibility of letting the data reveal how preferences vary in the population. We not only compare estimates of these models, but also test and succinctly discuss the sensitivity of the crowding multiplier estimates relative to the estimation procedure (Bayesian and classical) and parameter space (preference and CM). To the best of our knowledge, we are not aware of applications of such flexible, semi-nonparametric models for the valuation of crowding externalities in public transportation.====The remaining of the paper is organized as follows. Section 2 reviews the literature on flexible specifications of mixing distribution in discrete choice modeling, and describes the actual specification and estimation of the models under analysis. Section 3 focuses on the stated preference survey and our proposed CM-space specification. Section 4 outlines estimation results and discusses practical insights from an empirical perspective. Conclusions and future work are presented in section 5.",Flexible estimates of heterogeneity in crowding valuation in the New York City subway,https://www.sciencedirect.com/science/article/pii/S1755534518300666,3 May 2019,2019,Research Article,101.0
"Bansal Prateek,Daziano Ricardo A.,Sunder Naveen","Cornell University, United States","Received 4 June 2018, Revised 24 March 2019, Accepted 1 April 2019, Available online 20 April 2019, Version of Record 3 May 2019.",https://doi.org/10.1016/j.jocm.2019.04.001,Cited by (3),"The Multinomial Logit (MNL) model is popular, but a semi-parametric specification of its link/utility function has seldom been used in empirical applications. This is primarily because of the resource intensive nature of semi-parametric estimation. In this paper we propose and implement a parallel computation algorithm to estimate the semi-parametric kernel MNL model. This algorithm reduces model estimation time by a factor of 2–10, depending on the size of the dataset and the available resources for computation. These computational gains make the estimation of this model feasible for large datasets. Additionally, using a Monte Carlo study we show that the kernel MNL outperforms the traditional linear MNL model in terms of fit and predicted choice probabilities. We demonstrate how kernel-based specification can unearth important heterogeneities in the effect of covariates through an empirical exercise. We use data from a nationally representative household survey (N = 157,804) to analyze the factors associated with institutional births (as opposed to home births) in India. Our revealed-preference results indicate that maternal education, household assets, distance to formal health facility, and birth order play an essential role in determining birth location choice. Although the directions of impact are similar across both the linear and the kernel MNL specifications, there are significant differences in the marginal effects of different factors across the two models. These differences, which arise due to the flexibility afforded by the semi-parametric specification, potentially bring additional nuance to policy discussions.","In this paper we analyze healthcare demand in the context of India. This is important since a bulk of health interventions have focused on the supply side, i.e. these interventions are aimed at improving the quality of healthcare provided to consumers. Since health outcomes are a result of the interaction between health care demand and supply, a comprehensive understanding of the demand is equally critical for policy analysis. In particular, in this research we examine the factors associated with the decision of pregnant women (and their families) to give birth in an institutional birth facility.====Maternal mortality rates are high, and the various complications that happen during child birth are among its major causes (Bartlett III et al., 1993; Chalumeau et al., 2000; Kusiako et al., 2000). Due to a high mortality concentration around the first 24 h after birth, the proponents of institutional birth argue that appropriate care during this critical time is necessary to minimize the risk of mortality (Organization, 2006). Filippi et al. (2006) posit the role of timely and qualified care in reducing maternal mortality and in enhancing well-being of both mother and child.====Institutional birth has several effects on the mother's and child's well-being. First, child birth in a medical facility greatly reduces the chances of infant and/or maternal mortality (Darmstadt et al., 2005; Campbell et al., 2006, 2016). This effect is important especially because of the high rates of mortality due to preventable causes related to pregnancy and child birth – estimates from UNICEF (2018) indicate that at the global level these account for nearly 15,000 deaths per day. Second, previous studies have shown that giving birth in an institutional facility is positively associated with better post-natal care, such as vaccination and breast-feeding (Biks et al., 2015; Setegn et al., 2011; Odiit and Amuge, 2003), especially in developing country contexts. These in turn have been shown to have long term impacts on child outcomes (Kramer et al., 2008; Oddy et al., 2010; Quigley et al., 2012; Belfield and Kelly, 2012). Third, early childhood (0–5 years) is a key period of life – events and circumstances in this period have a large bearing on the physical and cognitive development of children, and have effects that persist into adulthood, in the form of education and labor market outcomes (Currie and Vogl, 2013, provides a detailed review).====From 1990 to 2013, there have been tremendous improvements in the global rates of institutional birth, but several regions of the world are still lagging behind. The proportion of births attended by a skilled attendant has increased from 57% to 74%, while the percentage of births with at least one ante natal visit has risen from 65% to 83% (WHO, 2015; UN, 2014). Despite these advances, maternal mortality in developing countries is more than 14 times the rate in developed countries (Alkema et al., 2016). Although large parts of Northern Africa and Southern Asia have seen rapid declines in maternal mortality, the absolute levels still remain high.====In this paper, we study the factors affecting demand for institutional birth among households in India. In particular, we focus on the role of four critical factors in shaping institutional birth choice in India – maternal education, distance, wealth and birth order. We choose India because it is the second most populous country in the world and still performs poorly on these indicators. For example, maternal mortality in India has fallen from 556 per 100,000 live births in the early 1990s to around 174 per 100,000 live births in 2015, but is still more than twice the target level of 70 deaths per 100,000 live births set in the Sustainable Development Goals (SDG).====In our analysis, we use a Multinomial Logit (MNL) model, which is a limited dependent variable regression that characterizes the association between various covariates and an unordered categorical response variable (McFadden, 1974). The conventional MNL model can be interpreted as a linear indirect utility specification if alternative-specific attributes are present in the context of choice between differentiated goods. In this paper, we explore non-linear relationships, and show that this “linear MNL” specification may not capture all the complexities of modeling birth location preferences in India.====In applications across different disciplines, previous studies have introduced non-linearity in different ways, for example a logarithmic transformation (Krishnamurthi and Raj, 1988), and a piece-wise linear function (Kalyanaram and Little, 1994), among others. Only a handful of studies have addressed non-linearity by using semi- or non-parametric utility specifications that do not require apriori assumptions on the functional form of the logit link.====Abe (1999) first introduced a spline-based utility specification in a semi-parametric MNL. Kneib et al. (2007) extended this model using penalized B-spline in a Bayesian estimation framework, while (Fukuda and Yai, 2010) used cubic spline functions. Further, Müller (2001) introduced kernel methods for binary logit models in which the likelihood is localized by kernel smoothing, whereas Langrock et al. (2014) generalized this estimator to multinomial unordered response variables. In this study, we build upon Langrock et al. (2014)'s work and estimate birth location preferences using a kernel-based MNL approach.====This study makes several contributions to the literature. First, this is one of the first studies to implement semi-parametric logit specifications of the utility function (as opposed to heterogeneity distributions), which are still uncommon in practice, especially in contexts with large datasets (>150,000 observations). Their usage has been restricted due to their resource-intensive sequential estimation – which arises because of an associated increase in the number of identified parameters. We disentangle the estimation of the kernel MNL into independent tasks, and implement a parallel estimation strategy which quickens the estimation process by a factor of 2–10, depending on the sample size and available computational resources. Since the semi-parametric specification improves on an extensively used parametric MNL approach, this parallelization technique broadens the use of kernel-based MNL models.====Second, we propose an empirical procedure to analyze the semi-parametric results in comparison with the linear MNL model results. Our results indicate that the kernel MNL model is able to provide insights that the linear MNL model cannot, especially in terms of identifying “non-linearity” in the underlying relationship between the covariates and the outcome (health care facility choice). Third, using a Monte Carlo study, we illustrate the superiority of the kernel MNL model based on different statistical criteria (e.g., model fit and predictive analysis). Fourth, we come full circle and provide a framework to compare the semi-parametric specification with the “nearest” linear specification – the underlying parametric model that most closely mimics the semi-parametric results. In a case study, we back-out an empirical procedure to select an appropriate parametric specification that is “closest” to the results of the kernel MNL model. We also provide a graphical framework to analyze the results from the non-parametric analysis.====The paper is organized as follows: section 2 explores the strands of literature that this study contributes to, while section 3 details the data used in the analysis, section 4 describes our empirical strategy in detail. We outline the simulation strategy and results in section 5, and discuss results of the empirical study in section 6. We end with a summary of our findings, their implication in the policy realm and the potential wider use of this technique to similar research questions.",Arriving at a decision: A semi-parametric approach to institutional birth choice in India,https://www.sciencedirect.com/science/article/pii/S175553451830054X,20 April 2019,2019,Research Article,102.0
Budziński Wiktor,"University of Warsaw, Department of Economics, Dluga 44/50, 00-241, Warsaw, Poland","Received 15 December 2017, Revised 2 April 2019, Accepted 2 April 2019, Available online 12 April 2019, Version of Record 22 April 2019.",https://doi.org/10.1016/j.jocm.2019.04.003,Cited by (54)," draws required for the desired precision exceeds 2 000 in some of the 5-attribute settings, and 20,000 in the case of some 10-attribute settings considered.","Discrete choice models are widely used in many applications, with modelling of consumers’ preferences probably being the most prominent (Ben-Akiva and Lerman, 1985; Train, 2009). Mixed logit (Revelt and Train, 1998) is the model of choice for most of these applications, and arguably also the state-of-the-art, considering its ability to approximate any random utility based choice model to any degree of accuracy (McFadden and Train, 2000).====Most applications estimate the model using the simulated maximum likelihood method, as it is relatively straightforward and readily implemented in most statistical software packages. Simulating the value of the log-likelihood function is necessarily associated with the simulation error that depends on the number and type of draws used. By using a different set of draws or even changing the order of explanatory variables, a researcher will arrive at somewhat different estimation results, in terms of the value of the log-likelihood function, parameter estimates, and their estimated standard errors (and hence the associated z-statistics).====Several studies have demonstrated the advantages of using quasi Monte Carlo (QMC) methods in terms of reducing simulation-driven variation of the results (e.g., using Halton rather than pseudo-random draws), and this has led to their wide proliferation. Unfortunately, examples of 100 Halton draws leading to smaller bias than 1 000 pseudo-random draws (e.g., Bhat, 2001) have led some to actually use very few draws for simulations, when in fact not much is known about the extent of the possible bias resulting from using different numbers of different types of draws in various conditions (datasets). Our study aims at filling this gap.====In what follows, we present the results of a systematic comparison of pseudo-random, modified Latin hypercube sampling, Halton, and Sobol draws under a wide set of experimental conditions in terms of experimental designs, the number of individuals (400–1 200), the number of choice tasks per individual (4–12), and the number of attributes (5, 10). Based on a Monte Carlo simulation, we demonstrate the extent of the simulation error resulting from using 100 up to 1,000,000 draws.==== This allows us to offer recommendations in terms of the QMC method: which performs best and its relative efficiency.====Using more draws is always better than using fewer – not only will the estimates become more precise (lower simulation error) but this can also lead to uncovering identification problems. However, we propose guidelines regarding how many draws are “enough” for a required precision level. Our measure is based on limiting the probability of making an error (to, e.g., 5%) when comparing otherwise identical models that can differ in simulated values only.====Overall, we find that the scrambled Sobol sequence performs the best in these simulations. As expected, scrambled Halton draws and modified Latin hypercube sampling also perform substantially better than pseudo-random draws. Importantly, our results indicate that, for common setups, thousands or, in some cases, tens of thousands of draws are required to attain desired levels of precision. While this result suggests more draws than have been common in previous applications, advances in computer speed now permit far more draws with the same runtimes as previously.====The rest of the paper is structured as follows. Section 2 provides an overview of earlier studies devoted to measuring simulation error and comparing the performance of various QMC methods. Section 3 presents the set-up of our Monte Carlo study. Section 4 introduces the methodology of comparisons and describes the framework used for recommending a “sufficient” number of draws. Results are presented in section 5 – we first compare the performance of various QMC simulation methods and then address the question of how many draws are “enough.” The last section offers discussion and conclusions.",Simulation error in maximum likelihood estimation of discrete choice models,https://www.sciencedirect.com/science/article/pii/S1755534517302002,12 April 2019,2019,Research Article,103.0
"Danaf Mazen,Atasoy Bilge,de Azevedo Carlos Lima,Ding-Mastera Jing,Abou-Zeid Maya,Cox Nathaniel,Zhao Fang,Ben-Akiva Moshe","Massachusetts Institute of Technology, 77 Massachusetts Avenue, Cambridge, MA, 02139, USA,Department of Maritime and Transport Technology, Delft University of Technology, Mekelweg 2, Delft, 2628, CD, the Netherlands,Technical University of Denmark, Anker Engelunds Vej 1 Bygning 101A, 2800 Kgs. Lyngby, Denmark,American University of Beirut, Bechtel Engineering Building 527, Lebanon,Singapore-MIT Alliance for Research and Technology, 1 CREATE Way, #10-01 CREATE Tower, 138602, Singapore,Edmund K. Turner Professor of Civil and Environmental Engineering, Massachusetts Institute of Technology, 77 Massachusetts Avenue, Cambridge, MA, 02139, USA","Received 15 April 2018, Revised 28 February 2019, Accepted 6 March 2019, Available online 11 March 2019, Version of Record 28 March 2019.",https://doi.org/10.1016/j.jocm.2019.03.001,Cited by (22)," surveys are most commonly used to provide behavioral insights on hypothetical travel scenarios such as new transportation services or attribute ranges beyond those observed in existing conditions. When designing SP surveys, considerable care is needed to balance the statistical objectives with the realism of the experiment. This paper presents an innovative method for smartphone-based stated preferences (SP) surveys leveraging state-of-the-art smartphone-based survey platforms and their revealed preferences sensing capabilities. A random experimental design generates context-aware SP profiles using user specific socioeconomic characteristics and past travel data along with relevant web data for scenario generation. The generated choice tasks are automatically validated to reduce the number of dominant or inferior alternatives in real-time, then validated using Monte-Carlo simulations offline. In this paper we focus our attention on mode choice and design an experiment that considers a wide range of possible existing mode alternatives along with a new alternative on-demand mobility service that does not exist in real life. This experiment is then used to collect SP data or a sample of 224 respondents in the Greater Boston Area. A discrete mode choice model is estimated to illustrate the benefit of the proposed method in capturing current context-specific preferences in response to the new scenario.","The pervasiveness of smartphones coupled with developments in information and communication technologies (ICT) and enhanced computing performance has paved the way for shifting towards “smart mobility”. Smart mobility is one of the main pillars of smart cities (Giffinger et al., 2007). It is defined as a combination of improved accessibility, availability of ICT, exploration of new data sources and analytics, and modern sustainable and efficient transportation systems. Indeed, technology has not only been changing the transportation systems in our cities, but also influencing the way transportation surveys are conducted. The popularity of location-enabled devices has greatly expanded transportation data collection options (Susilo et al., 2016). Along with location-based big-data collection for aggregate mobility patterns (Cottrill and Derrible, 2015, Jiang et al., 2017), smartphones have also been used to collect detailed travel diaries (Cottrill et al., 2013, Susilo et al., 2016, Zhao et al., 2015b) in a cheap and non-intrusive manner (Prelipcean et al., 2015). While these technologies have been well established in collecting trip diaries - or revealed preferences (RP) data, they have not yet been utilized in the collection of mobility related stated preferences (SP) data.====SP data are hypothetically created choice situations in which the researcher has the freedom to define the tradeoffs faced by the respondent (Ben-Akiva et al., 2019; Walker et al., 2015). SP methods were first introduced by Thurstone (1931), who proposed experiments of the form “eight hats and eight pairs of shoes versus six hats and ___ pairs of shoes”. In the context of transportation, SP surveys are used to analyze hypothetical scenarios, such as testing attribute ranges beyond those observed in RP data or to infer preferences towards new modes and services.====When designing SP surveys, considerable care is needed to balance the statistical objectives with the realism of the experiment (Ben-Akiva et al., 2019). The statistical objectives involve identifying parameter estimates consistently and with low standard errors. The realism of experiments involves accounting for market, personal, or contextual constraints, and presenting alternatives in the same way as their market framing. These objectives can be met by designing context-aware SP surveys, which pertain to a specific context already faced by the respondent. For example, a transportation mode SP survey would refer to a trip performed by the respondent, but present different alternatives and attributes from those originally experienced by this respondent.====This paper presents a generic method for context-aware SP surveys leveraging state-of-the-art smartphone-based RP methods, and presents its application to mode choice of future smart mobility solutions. The context is coming from the observed RP data, e.g., weekly activity pattern or a selected trip for a given day, together with user specific information, e.g., vehicle ownership, usage of car/bike sharing services, etc. In addition to the direct information obtained from the user, we collect external contextual data such as the available activity or transportation alternatives for the user through online sources. The experimental design uses this context in order to generate SP choice experiments with reasonable alternatives and attributes. By accounting for the trip context (e.g. user-specific considerations, and trip-specific constraints) and using smartphone data, this method overcomes several limitations associated with traditional SP surveys related to data quality, realism, and user experience. This methodology can also be used to estimate preferences towards new mobility solutions which then can be used for the design and operation of these solutions.====The remainder of this paper is organized as follows: section 2 presents a brief literature review on prompted recall surveys, context-aware SP surveys, and survey design. Section 3 provides an overview of our proposed system architecture and outlines the methodology of SP data collection. Section 4 presents its application to a case study on FMOD (Future Mobility on Demand). Section 5 presents a discussion of the contributions and limitations of the proposed method. Finally, Section 6 presents future research directions and concludes the paper.",Context-aware stated preferences with smartphone-based travel surveys,https://www.sciencedirect.com/science/article/pii/S1755534518300381,11 March 2019,2019,Research Article,104.0
"Hasnat Md Mehedi,Faghih-Imani Ahmadreza,Hasan Samiul","Department of Civil, Environmental and Construction Engineering; University of Central Florida; 12800 Pegasus Drive, Orlando, FL 32816, USA,Department of Civil Engineering, University of Toronto, 35 St. George Street, Toronto, Ontario, M5S 1A4, Canada","Received 2 April 2018, Revised 28 February 2019, Accepted 6 March 2019, Available online 11 March 2019, Version of Record 22 March 2019.",https://doi.org/10.1016/j.jocm.2019.03.002,Cited by (9),None,"Travel surveys complemented by additional land use and socio-economic data have served as primary inputs for travel demand models. A complete household survey with all the required travel information costs about $200 per household (Zhang and Mohammadian, 2008). Although access to such individual level travel information is crucial for developing advanced travel behavior models, conducting such a survey is costly and time consuming (Flyvbjerg et al., 2005). With increased use of pervasive technologies, alternative approaches can potentially be used to collect/augment this information in a cost-effective way. Web-based surveys (including trip planning apps), social networking applications, smart phones, and personal health sensors have been explored to collect individual travel information. To gather travel behavior information, organizations have started using global positioning system (GPS) log data (NYMTC and NJTPA, 2014), smart phone based travel surveys (Greene et al., 2015) and web based surveys (“North Florida Travel Survey, 2017”). Different countries around the world such as Singapore (Cottrill et al., 2013), New Zealand (Safi et al., 2015), Australia (Greaves et al., 2015), Netherlands (Geurs et al., 2015) etc. have resorted to smart phone based GPS travel data as a complementary approach to traditional travel surveys. These studies have found GPS based travel surveys through wearable devices as promising alternative or addition to traditional trip diaries. However, researchers are yet to fully explore their potential as well as identify all the limitations of these emerging technology-based data collection methods (Abbasi et al., 2015, Geurs et al., 2015).",Destination choice modeling using location-based social media data,https://www.sciencedirect.com/science/article/pii/S1755534518300332,11 March 2019,2019,Research Article,105.0
"Hein Maren,Kurz Peter,Steiner Winfried J.","Department of Marketing, Clausthal University of Technology, 38678, Clausthal-Zellerfeld, Germany,bms Marketing Research + Strategy, Landsberger Straße 487, 81241, Munich, Germany","Received 8 August 2017, Revised 26 January 2019, Accepted 27 February 2019, Available online 5 March 2019, Version of Record 11 April 2019.",https://doi.org/10.1016/j.jocm.2019.02.001,Cited by (5),"The authors conduct an extensive simulation study to substantially contribute to the question how HB prior parameter settings (i.e. the prior variance and the prior degrees of freedom) affect the performance of HB-CBC models. The statistical performance of HB is evaluated under experimentally varying conditions based on six experimental factors using criteria for goodness-of-fit, parameter recovery and predictive accuracy. The results indicate that the prior degrees of freedom play a negligible role as there is not any noticeable impact on the performance of HB when varying that factor. For increasing prior variance levels overfitting problems occur that markedly affect parameter recovery and model fit, and a number of related interaction effects with regard to the settings for the prior variance can be observed both at the upper and lower level of the HB model. Perhaps the most striking finding however is that the predictive performance of HB-CBC is hardly affected by an increase of the prior variance. Many of our findings regarding the parameter settings of the inverse Wishart prior contrast those reported in previously proposed empirical studies.","Since the seminal papers of Allenby et al. (1995), Allenby and Ginter (1995), and Lenk et al. (1996) Hierarchical Bayes (HB) has become the standard technique to estimate individual part-worth utilities for Choice-Based Conjoint (CBC) data. HB models belong to the class of sample level models that account for (at least a certain level of) heterogeneity of individual respondents while using the complete sample of respondents for parameter estimation (e.g., Train, 2003). Early approaches using separate models per respondent to address heterogeneity in individual respondents' choice behavior were proposed by Beggs et al. (1981), and Chapman (1984). These early approaches rely on the ranking of choice data from a small number of choice tasks but may suffer from convergence problems. Later, Louviere et al. (2008) presented a weighted least squares regression approach for sequences of best-worst choices from multiple choice tasks to fit models to individuals’ choice data (for more details on these approaches, see Dumont et al., 2015). Because unordered (first) choice data, as collected in most CBC studies, provide still less information compared to ranked or best-worst choice data, CBC models were traditionally estimated at aggregate respondent level, either by pooling across all individuals or by estimating part-worths at the group (segment) level.====With the use of HB it is nowadays possible to recover respondents' heterogeneous preferences from unordered choice data, thereby solving the limited data problem at the individual level. The key strength of HB is its ability to provide individual part-worth estimates given only relatively few observations per respondent. Even when there are more parameters than observations, HB stably estimates individual part-worths (Lenk et al., 1996; Lenk and Orme, 2009). However, HB inference requires prior beliefs about the unknown part-worths (e.g. based on logic, theories, experts, or past analyses). These prior beliefs are represented by a probability distribution, also called prior distribution. According to the Bayes theorem, Bayesian estimators combine the prior information about the part-worths with information contained in the choice data to arrive at the posterior distributions of the parameters (Allenby et al., 1995; Lenk et al., 1996; Train, 2003; Rossi and Allenby, 2003; Rossi et al., 2005; Rao, 2014). In the Hierarchical Bayes model, the prior is specified in a two-stages process. The multivariate normal distribution is typically used as probability distribution (first-stage prior) with a vector of means (population mean) and a covariance matrix that captures the extent of heterogeneity (as well as the correlation between the part-worths) across individuals. In a second step, HB also captures uncertainty on the parameters of the prior distribution by treating them as random variables. Therefore, to estimate the population mean and the covariance matrix of the prior distribution, hyperprior distributions (second-stage priors) have to be specified. In the frequently used “normal model” it is common to assume that the prior on the population mean is normal and the prior on the covariance matrix is inverse Wishart distributed as these are conditionally conjugate priors that render some mathematical simplicity (e.g. the posterior distribution of the covariance matrix is inverse Wishart as well) (Allenby and Rossi, 1999; Barnard et al., 2000; Rossi and Allenby, 2003; Rossi et al., 2005). Normally, noninformative second-stage priors are chosen that are very diffuse and nearly flat. For the inverse Wishart distribution two parameters have to be specified: the number of degrees of freedom and the scale matrix also referred to as prior covariance matrix. The prior covariance matrix can be interpreted as to determine the position of the inverse Wishart distribution in the parameter space, and the prior degrees of freedom determine the strength (confidence level) of the prior beliefs for the prior covariance matrix (Train, 2003; Rossi et al., 2005; Bouriga and Féron, 2013; Hsu et al., 2012; Chung et al., 2015). The tightness of the inverse Wishart prior plays an important role in determining the amount of shrinkage of the individual part-worths toward the population mean. The inverse Wishart prior becomes more diffuse with smaller values of the degrees of freedom and larger elements of the covariance matrix which results in less shrinkage. On the other hand, if the prior is set to be more informative by increasing the degrees of freedom and decreasing the elements of the covariance matrix, there will be more shrinkage permitting less heterogeneity across individuals (Barnard et al., 2000; Train, 2003; Rossi et al., 2005; Dumont et al., 2015). Hence, how much the individual part-worth estimates are influenced by the aggregate (population) level strongly depends on the settings of the HB prior degrees of freedom and the prior covariance matrix. For example, with sparse data sets (i.e. only few observations per respondent and many parameters to estimate), it may be reasonable to have a higher degree of shrinkage permitting less heterogeneity across individuals in order to get more robust individual part-worth estimates (Orme, 2003; Lenk and Orme, 2009). To the extent that a lot of individual information is available with only few parameters to estimate, more weight can be given on fitting each individual's choice data. However, allowing respondents to be very heterogeneous also increases the risk of overfitting at the individual level (Orme, 2003; Dumont et al., 2015). Thus, the choice of the HB prior settings should be well considered as the prior specification can have a large influence on Bayesian estimates.====In this paper, the objective is to investigate the impact of different prior settings for the inverse Wishart distribution (i.e. the prior degrees of freedom and the prior covariance matrix) on the performance of HB-CBC models. Since practitioners often use the default settings in standard software, we designed a simulation study using synthetic choice-based conjoint data to examine how changing the prior settings up or down from the default affects the results. Especially, we investigate the influence of the HB priors by systematically varying experimental factors, such as the number of respondents, the number of choice tasks, and the number of parameters to be estimated. It has been shown that the inverse Wishart prior has some undesirable properties (e.g. Akinc and Vandebroek, 2018; Alvarez et al., 2014; Hsu et al., 2012; Lenk and Orme, 2009): There is only a single degrees of freedom parameter for all components of the covariance matrix. Thus, there is no flexibility to use different prior distributions for each covariance matrix element. Second, there is a strong dependency among the components of the covariance matrix, e.g. small variances are associated with correlations near zero and larger variances are associated with extremely large correlations. Further, since the prior for the variances (which is a scaled inverse χ==== distribution) has a “dead zone” near zero where the density becomes very small, the inverse Wishart prior will cause a bias toward larger variances if the true variance (sample heterogeneity) is quite small. Nevertheless, although alternative prior specifications have been proposed in the literature (e.g. Barnard et al., 2000; Hsu et al., 2012; Bouriga and Féron, 2013; Alvarez et al., 2014) the inverse Wishart distribution is the dominant prior specification for the covariance matrix in practice, which can be attributed to its conditionally conjugate property (Barnard et al., 2000). Here, as outlined next, most previous research has concentrated on prior settings in the context of sparse CBC data sets. A comparison of the inverse Wishart distribution to other prior distributions for the covariance matrix is left for future research.====Pinnell and Fridley (2001) conducted a meta-analysis of nine partial profil CBC data sets in order to compare the predictive validity of HB to the aggregate logit model in terms of holdout hit rates. Results showed that for some data sets HB performed poorly and was outperformed by the aggregate logit model. Orme (2003) re-analyzed the partial-profile CBC data sets examined by Pinnell and Fridley (2001) and found that the default prior settings in the software used for HB estimation were suboptimal for sparse partial-profile CBC data sets leading to overfitting at the individual level (and thereby worsening the predictive performance in holdouts). Orme (2003) could show that results improved when adjusting the prior variance and the prior degrees of freedom parameters in a way that there was a stronger HB smoothing toward the population mean. Thus, the author pointed out that under extreme conditions, e.g. estimating many parameters from sparse CBC data, the performance of HB is dependent on the proper specification of the prior settings.====Further, Lenk and Orme (2009) discussed the effect of priors for variances and covariance matrices when data are sparse. The authors showed that informative priors can improve the estimation for sparse data sets, while the use of noninformative priors can result in unexpected and misleading results. In particular, the authors discussed methods to specify an informative prior in the case of the inverse gamma distribution that is commonly used as a prior for variances. Further, they proposed a more flexible alternative to the inverse Wishart distribution for the covariance matrix of the multivariate normal distribution allowing separate degrees of freedom parameters for the variances. With regard to CBC studies the authors showed that when using the default prior covariance matrix for the inverse Wishart distribution (a constant times the identity matrix, zero covariances) the estimation of the omitted (reference) attribute level under effects coding is biased, especially for sparse data. The effects coding of categorical attributes leads to a larger prior variance for the omitted attribute levels which results in a greater variation in the Bayesian estimates. Therefore, the authors proposed a non-diagonal prior covariance matrix with an appropriate amount of correlation of the levels within an attribute.==== This “effects prior” leads to a symmetrical treatment of all levels within an attribute so that the results are invariant to the choice of the omitted (reference) level.====McCullough (2009) focused on comparisons between latent class CBC and HB choice-based conjoint models applied to sparse real-world data sets. In addition, the aggregate logit model served as a benchmark. In particular, three data sets were analyzed (not sparse, sparse with a large sample, sparse with a small sample). The performance of default as well as advanced forms of HB and latent class was compared by using the hit rate and mean absolute error (MAE) with regard to random and holdout tasks as well as in terms of the average holdout variance ratio. For utility estimation with HB, default prior settings as well as adjusted priors were used. The priors were adjusted to either maximize the hit rate or minimize the MAE of holdout tasks. A grid search was used to find the optimal combination of prior variance and prior degrees of freedom. The results showed that both latent class and HB models performed similar well in default and more advanced forms. Further, McCullough (2009) could show that for sparse data sets holdout hit rates improved when adjusted HB priors were used instead of default HB prior settings.====Recently, Orme and Williams (2016) conducted a meta-analysis of about 50 commercial CBC data sets (as well as data sets based on best-worst scaling) in order to find the optimal HB prior settings that on the one hand avoid overfitting and on the other hand capture proper heterogeneity for the sample. The authors used a grid search approach to estimate HB utilities under different settings for the prior variance and the prior degrees of freedom. Optimal priors for different choice data sets were assessed based on holdout hit rates. Further, the hit rate at the default priors was compared to the hit rate at the optimal prior settings. Results showed that the optimal prior variance for CBC data sets ranges from 0.1 to 1.6 and depends on the data set characteristics (e.g. number of attributes). Further, results indicated that there is a general tendency that as the number of attributes in CBC studies increases, the optimal variance tends to decrease and vice versa. In addition, the authors suggested making the degrees of freedom depending on the sample size, i.e. increasing the degrees of freedom with a larger sample size.====Table 1 summarizes the main characteristics and findings of the studies discussed above. Nearly all studies concentrated on how priors should be set in the context of sparse CBC data sets. Sparse data sets refer to partial profil designs where only a subset of the total number of attributes within each choice task is shown, or to a small number of choice tasks per respondent relatively to the number of parameters to estimate. Only one study was conducted to find optimal HB prior settings and except for Lenk and Orme (2009) who proposed a non-diagonal covariance matrix for the inverse Wishart distribution to correct for estimation biases with respect to omitted attribute levels all studies used empirical data. To the best of our knowledge no study has yet explored the effects of both the prior degrees of freedom and the prior covariance matrix settings (as well as related interaction effects concerning the two settings) on the posterior Bayesian estimates on the basis of simulated CBC data. Following previous studies we also generate sparse data sets in our simulation study. For example, we consider experimental conditions that contain a rather large number of parameters to be estimated (up to 56 individual-level parameters) with a minimum number of only 8 choice tasks per respondent (as can be seen in Table 2). Further, the previous studies focused on predictive validity measures. In our simulation study we additionally assess the performance of HB-CBC using goodness-of-fit (as indicator of model overfitting) and parameter recovery measures. Parameter recovery is important as parameters relate to values of product attributes and management wants to find good or optimal attribute levels for their products.====The objective of the present study is to substantially contribute to the question how HB prior parameter settings affect the performance of HB-CBC models. We therefore provide an extensive simulation study based on synthetic choice-based conjoint data. The statistical performance of HB is evaluated under experimentally varying conditions (based on 6 experimental factors) using criteria for goodness-of-fit, parameter recovery and predictive accuracy. Analyses of variance (ANOVAs) are conducted to assess the impact of the experimental factors on the measures of performance. The article proceeds as follows. In the next section, we describe the Hierarchical Bayesian approach focusing in particular on the parameters of the inverse Wishart prior. Then, we describe the design of the simulation study. In particular, we introduce the factors that were experimentally varied, explain the data generation process, and propose the statistical criteria used for evaluating the performance of HB-CBC models. Subsequently, we present the results of our simulation study. Finally, we discuss implications and provide an outlook on future research perspectives.",On the effect of HB covariance matrix prior settings: A simulation study,https://www.sciencedirect.com/science/article/pii/S1755534517301343,5 March 2019,2019,Research Article,106.0
"Chrzan Keith,Peitz Megan","Sawtooth Software, Inc., 996N 250E, Chesterton, IN, 46304, USA,Sawtooth Software, 3210 N. Canyon Road, Provo, UT, 84604, USA","Received 4 August 2017, Revised 1 January 2019, Accepted 6 January 2019, Available online 11 January 2019, Version of Record 18 January 2019.",https://doi.org/10.1016/j.jocm.2019.01.002,Cited by (7),"Best-worst scaling (BWS) has become so useful that practitioners feel pressure to include ever more items in their experiments. Researchers wanting more items and enough observations of each item by each respondent to support individual respondent-level utility models may greatly increase the burden on respondents, resulting in respondent fatigue and potentially in lower quality responses. Wirth and Wolfrath (2012) proposed two methods for creating BWS designs that allow for large numbers of items and respondent-level utility estimation, Sparse and Express BWS. This study aims to uncover the recommended approach when the goal is recovering individual respondent-level utilities and intends to do so by comparing the relative ability of Sparse and Express BWS to capture the utilities that would have resulted from a full BWS experiment, one with at least three observations of each item by each respondent. The current study repeats previous comparisons of Sparse and Express BWS using a new empirical data set. It also extends previous findings by collecting enough observations from each respondent for both a full experiment and one of the proposed methods, Express BWS and Sparse BWS. The results replicate and extend previous findings regarding the superior ability of the Sparse BWS methodology, relative to Express, to reproduce “known” utilities or utilities that result from a full BWS design.",None,Best-Worst Scaling with many items,https://www.sciencedirect.com/science/article/pii/S1755534517301355,March 2019,2019,Research Article,107.0
"Paz Alexander,Arteaga Cristian,Cobos Carlos","Queensland University of Technology, 2 George Street, Brisbane, Queensland 4000, Australia,Transportation Research Center, University of Nevada, 4505 Maryland Parkway, PO Box 454007, NV, 89154-4007, Las Vegas, USA,Information Technology Research Group (GTI), Universidad del Cauca, Sector Tulcán Office 422, Popayán, Colombia","Received 14 April 2018, Revised 26 December 2018, Accepted 4 January 2019, Available online 7 January 2019, Version of Record 15 January 2019.",https://doi.org/10.1016/j.jocm.2019.01.001,Cited by (21),"Mixed logit is a widely used discrete outcome model that requires for the analyst to make three important decisions that affect the quality of the model specification. These decisions are: 1) what variables are considered in the analysis, 2) which variables are to be modeled with random parameters, and 3) what density function do these parameters follow. The literature provides guidance; however, a strong statistical background and an ad hoc search process are required to obtain an adequate model specification. Knowledge and data about the problem context are required; also, the process is time consuming, and there is no certainty that the specified model is the best available. This paper proposes an algorithm to assist analysts in the search of an appropriate specification in terms of explanatory power and goodness of fit for mixed logit models. The specification includes the variables that should be considered as well as the random and deterministic parameters and their corresponding distributions. Three experiments were performed to test the effectiveness of the proposed algorithm. Comparison with existing model specifications for the same datasets were performed. The results suggest that the proposed algorithm can find adequate model specifications, thereby supporting the analyst in the modeling process.","The modeling and prediction of discrete outcomes is a common problem in many areas, including economics, engineering, and medicine. Some examples of discrete outcome problems include (i) analysis of transportation modes (i.e., car, transit, or walking) based on observed socioeconomic characteristics and mode attributes; (ii) estimation of the presence of a pathology based on attributes of a patient; and (iii) estimation of how many cars will be owned based on observed characteristics of a household.====In general, a categorical variable associated or explained by a set of attributes and/or characteristics can be considered a discrete outcome problem. Several statistical and machine-learning approaches have been proposed in the literature to model discrete outcome problems (Hensher and Ton, 2000; Luo, 2015; Omrani, 2015). Regardless of the proposed approach, the analyst needs to decide which variables need to be considered in the model specification. This model specification process relies greatly on human judgment to include context-specific knowledge in the model and to accommodate interpretation needs. However, this process is time consuming, and is subject to expert knowledge and ad hoc trial-and-error approaches. Due to the high dimensionality of the problem, at the end of the process, the analyst is not entirely certain about whether the specified model is the best available. Therefore, approaches that support the search for model specifications can help the analyst and decrease the time and effort required for this process. Despite human dependency of the model-specification process, several optimization approaches have been applied successfully to this problem (Brusco, 2014; Thornton et al., 2013; Vinterbo and Ohno-Machado, 1999). These approaches usually use measures such as goodness of fit and predictive performance as optimization targets.====The variables included in a model affect its predictive performance significantly. Models with a proper subset, and the smallest subset, of explanatory variables allow greater influence of the included variables, eliminate redundancy, reduce costs of data acquisition, and are computationally efficient; they provide a better understanding of the final model (Fouskakis and Draper, 2008). Variable selection, also referred to as subset selection or model specification, aims to find a model with the highest explanatory power while selecting the smallest possible number of variables. One challenge is that the number of possible combinations of variables that could be considered grows exponentially as the number of potential explanatory variables increases (Sato et al., 2016; Vinterbo and Ohno-Machado, 1999). For example, for a model with 30 variables, the number of different possible specifications is 2==== = 1,073,741,824. This approach is computationally intensive when solving by using an exhaustive search. Various approaches used to address this problem are described in this paper.====Discrete outcome problems can be viewed as discrete choice processes, where a decision maker chooses an alternative from a finite set. Theoretically, it is assumed that the chosen alternative maximizes the utility of the decision maker, known as random utility maximization, where observed factors and distributions of the unobserved factors are used to estimate choice probabilities (Ben-Akiva and Lerman, 1985; Train, 2003). Application of discrete choice models include, among others, the estimation of route, mode, residential location, and the number of vehicles or bikes to own (Paz and Peeta, 2009a, Paz and Peeta, 2009b; Pinjari et al., 2011).====Multinomial logit and probit are common choice models that have been applied successfully to model discrete outcome problems. It is known that logit models suffer such limitations as Independence of Irrelevant Alternatives (IIA), restrictive substitution patterns, and the inability to model random taste variations. Probit models have addressed these limitations; however, they are restricted to modeling random taste variations by using only a normal distribution, which is not always convenient.====In view of these limitations, mixed logit models have been proposed (Train, 2003) as one of the most prominent techniques for modeling discrete outcome problems. Mixed logit models address the limitations of logit and probit by allowing modeling of variables with random coefficients. Such variables can follow any statistical distribution specified by the analyst as well as a general random term that follows an extreme value distribution. The predictive power and quality of a mixed logit depends greatly on an appropriate definition of the distribution of the random coefficients (Hensher and Greene, 2003). The modeling of coefficients as random variables provided by mixed logit allows capturing the heterogeneity in preferences among the decision makers. The output of a mixed logit with random coefficients includes the mean and standard deviation of the parameters treated as random terms. The mean represents the average preference about the variable, and the standard deviation has valuable information about the heterogeneity of that preference (McFadden and Train, 2000).====Given a mixed logit estimation problem, several assumptions are required to determine the best model specification. In general, the distribution of the random coefficients and potential explanatory variables need to be assumed before a model is estimated (Dekker, 2016; Hensher and Greene, 2003; Hess and Train, 2017; Train, 2016). When analysts start a mixed logit specification process, they use their knowledge about the problem context and data, to test several specifications based on assumptions, expectations and hypothesis. This process is involved, and the analysts are limited by the time they can dedicate to test several specifications. This paper proposes an optimization framework to assist the search of an adequate model specification in terms of explanatory power and goodness of fit, including the variables considered as well as the distribution and associated parameters for the corresponding coefficients. In this study, a solution algorithm was implemented and tested with three datasets.====The remaining of this paper is organized as follows. Section 2 summarizes a set of studies related to variable selection. Section 3 provides details about the proposed solution including the mathematical programming formulation of the problem. Section 4 describes the experiments and then the analysis of the results is presented in Section 5. Finally, Section 6 provides conclusions and recommends future research directions.",Specification of mixed logit models assisted by an optimization framework,https://www.sciencedirect.com/science/article/pii/S175553451830037X,March 2019,2019,Research Article,108.0
"Kessels Roselinde,Jones Bradley,Goos Peter","University of Antwerp, Faculty of Business and Economics & StatUa Center for Statistics, Prinsstraat 13, 2000 Antwerp, Belgium,University of Amsterdam, School of Economics, PO Box 15867, 1001 NJ Amsterdam, the Netherlands,JMP Division of SAS Institute, SAS Campus Drive, Cary, NC 27513, USA,KU Leuven, Faculty of Bioscience Engineering & Leuven Statistics Research Centre, Kasteelpark Arenberg 30, 3001 Heverlee, Belgium","Received 24 July 2018, Revised 14 November 2018, Accepted 17 December 2018, Available online 22 December 2018, Version of Record 22 February 2019.",https://doi.org/10.1016/j.jocm.2018.12.002,Cited by (9), population distribution and to effectively predict people's choices and detect market segments. Segment recovery may even be better when individual-level estimates are obtained using Firth's method instead of hierarchical Bayes estimation under a normal prior. We base all findings on data from a stated choice study on various forms of employee compensation.,"Discrete choice models relate respondents' choices of one of two or more alternatives or profiles to the attributes of the respondents and the attributes of the alternatives. Data for discrete choice models are either collected via stated or discrete choice experiments (DCEs), where respondents state their choices in hypothetical situations, or via observational studies, where respondents reveal their actual choices made. Stated choice data have been used to predict preferences for prospective goods in marketing, innovative health programs in health economics, new transportation systems in transport planning, and various other applications often involving new developments. Revealed choice data have been used to study actual choices of, for example, which car to buy, where to go to college and which mode of transport (car, bus, rail) to use for commuting to work.====Individual-level choice data often exhibit separation. In general, separation occurs in discrete choice data if the responses can be perfectly classified by a linear combination of the attributes of the alternatives (see, for studies on logistic regression, Albert and Anderson (1984), Santner and Duffy (1986), Lesaffre and Albert (1989) and Allison (2008)). Complete separation occurs when a combination of the attributes classifies responses without error according to a strict inequality. Quasi-complete separation occurs when a combination of the attributes classifies responses without error up to a non-strict inequality.====A commonly used procedure to fit discrete choice models is maximum likelihood (ML) estimation, which guarantees that the estimator is unbiased in the event of an infinite sample size. However, for many applications, the sample data collected are small. One consequence of finite samples is that the probability of separation is always strictly positive. In the event of separation, the ML estimator does not exist. Therefore, for finite samples and logistic models such as discrete choice models, the expectation of the ML estimator does not exist (Le Cam, 1990). That is, the integral defining their expected value does not converge. This is because the probability of data separation is never nonzero.====In practical applications, data may or may not exhibit separation. When data separation occurs, computer implementations of ML estimation often show the likelihood estimates converging while at least one parameter gets large without bound. The actual parameter estimate reported is then a function of the convergence criterion for the likelihood rather than having any practical meaning. When attempting ML estimation for individuals, data separation occurs so frequently as to make such an approach infeasible. For small numbers of respondents, a lesser problem with ML estimation is that it tends to over-estimate the utility of strongly preferred attribute levels. Similarly, undesirable attribute levels are modeled as being even less desirable than their true utilities would indicate. This bias is often far from negligible and can have practical implications in the decisions that practitioners make.====To overcome these two weaknesses of ML estimation, we introduce the penalized ML method of Firth (1993, 1995) for estimating the multinomial logit (MNL) model in the literature on the analysis of choice data. As we show in this paper, a major advantage of the method is that it allows fitting a MNL model to individual-level data, and subsequently, exploring the heterogeneity in the respondents' preferences and segmenting the market. Bull et al. (2002) were the first to propose Firth's method to estimate the MNL model, but they applied it to small sample clinical trials outside a choice modeling context, and did not consider individual-level data.====Firth's method was originally developed as a general bias reducing technique in the context of ML estimation, but it was also shown to provide finite parameter estimates in the case of separation (see, for binomial and trinomial logistic regression on clinical data, Bull et al. (2002), Heinze and Schemper (2002) and Heinze (2006)). Separation occurs more often in small samples and in larger experiments where a design is used in which the success probability of every observation is near 0 or 1 (Woods and van de Ven, 2011; Goos and Gilmour, 2012). In a DCE context, Kessels et al. (2011a,b) describe an example of an orthogonal design involving eight choice sets of two alternatives. Such design is also called a utility-neutral design because it relies on the assumption that people are ambivalent about any of the attribute levels, and thus also about any of the alternatives. The utility-neutral design of the example is poor since it leads to separation 20% of the time when there are 100 respondents and 4% of the time when there are 200 respondents. The authors also show that Bayesian designs for DCEs usually do not lead to data separation. That is because the Bayesian design methodology incorporates the available information about people's preferences for various attributes in the choice design (Sándor and Wedel, 2001; Bliemer and Rose, 2010; Kessels et al., 2011a). A key feature of many DCEs is that they involve a small set of levels for the attributes, which makes them more vulnerable to data separation than studies with explanatory variables that take many different levels.====Recently, to overcome the data separation challenge and estimate choice models for each individual separately, Frischknecht et al. (2014) presented a penalized ML method where the penalty function corresponds to a Bayesian approach that augments the limited data with prior beliefs about the behavior of the data (Geweke, 2005). The authors built on penalty methods proposed by Clogg et al. (1991) and Cardell (1993). Typical of any penalized ML approach, including Firth's method, is that it shrinks the estimates toward zero. Another approach proposed by Dumont et al. (2015) to estimate individual-level choice models uses a Bayesian framework that draws priors from a sample-level model in which the data are pooled. The approaches of Frischknecht et al. (2014) and Dumont et al. (2015) require weaker prior distributional assumptions than hierarchical Bayes (HB) estimation, thereby attempting to simplify the computations.====Nevertheless, a large body of literature focuses on obtaining individual-level preference estimates from sample-level models such as mixed logit models (Train, 2009), HB models (Lenk et al., 1996), latent class models (Andrews et al., 2002) and convex optimization techniques (Evgeniou et al., 2007). These individual-level estimates are subsequently used for market segmentation (see, for a case study, Allenby and Ginter (1995)). All these techniques involve making distributional assumptions about the respondents, usually treating them as coming from a single multivariate distribution. In the common case of segmented markets, the assumption of a single population is impractical. Moreover, it is impossible ==== to rely on guesses about the number or multivariate location and variability of market segments.====Along the lines of the penalized ML method proposed by Frischknecht et al. (2014), Firth's method fits a model to each individual's choices separately with no prior distributional assumptions imposed on the parameters. We can therefore use it to construct an empirical distribution of the respondents' preferences. Firth's method differs from the method of Frischknecht et al. (2014) in that it uses a prior that depends on the estimated model itself rather than on artificial data augmentation, making it much simpler to implement.====Inspired by Louviere et al. (2008), we classify Firth's method to obtain individual-level parameters for the empirical distribution of sample preferences as a “bottom-up” approach. We call an approach that makes use of prior distributional assumptions a “top-down” approach. Also, Louviere et al. (2008) state that, in theory, if one specifies correct preference distributions, and the number of choices per person is sufficiently large, top-down and bottom-up approaches should give the same results. In contrast, if assumptions about preference distributions are incorrect, the inferences from top-down models will be biased.====The remainder of this paper is organized as follows. Section 2 reviews the MNL model and explains the ML and Firth's estimation techniques for this model. In Section 3, we illustrate Firth's method for individual-level preference estimation using an application in employee compensation and compare its performance to HB estimation of the panel mixed logit model. To provide an overview of the situations in which Firth's method proves most effective for aggregate and individual-level estimation, we describe the results of a simulation study in Section 4. In another simulation study presented in Section 5, we compare the performance of Firth individual-level estimates for market segmentation to that of HB individual-level estimates. Finally, in Section 6, we summarize and discuss the results.",Using Firth's method for model estimation and market segmentation based on choice data,https://www.sciencedirect.com/science/article/pii/S1755534518300836,22 December 2018,2018,Research Article,109.0
"Delgado-Lindeman Maira,Cantillo Víctor","Department of Civil and Environmental Engineering, Fundación Universidad del Norte, Km 5 Vía a Puerto Colombia, Barranquilla, Colombia","Received 28 June 2018, Revised 1 November 2018, Accepted 5 December 2018, Available online 10 December 2018, Version of Record 14 December 2018.",https://doi.org/10.1016/j.jocm.2018.12.001,Cited by (8),"In the case of a medical emergency, a failure to provide timely care could have serious health consequences and even cause death. When the waiting time increases, the patient perceives the loss of well-being associated with the lack of primary care. Considering the ==== (WTP) for avoiding human suffering in the context of emergency medical services (EMS) within the impact assessment of prehospital care is a relatively new research topic. This paper evaluates, through discrete choice models, the WTP for avoiding the externality associated with the human suffering experienced by the patient who does not receive immediate attention in a medical emergency.====The cost of the service, the waiting time of the service, the time elapsed at the time of making the decision, the patient's condition and the time to aggravate are relevant to define the WTP in the context of EMS. Heterogeneity in the WTP was captured by the inclusion of some socioeconomic attributes and proxy variables to evaluate several levels of patient severity. The results are intended to demonstrate the relevance that this externality could have in the planning, operation, management of the emergency system and in the coordination of public policies aimed at providing efficient services with a better distribution of resources.","Emergency medical services (EMS) are responsible for providing primary medical care to patients outside of a hospital (e.g. in the place of study or work, at home or in the public space). Some statistics reveal the importance of EMS within a territory and the importance of its planning, operation, and management. For example, the two leading causes of death in the world (ischemic heart disease and stroke), which represent 15 million deaths in 2015, are related to emergency episodes (WHO, 2017). About 1.2 million deaths are recorded in the year worldwide due to traffic crashes. In such cases, the EMS is usually requested, which means that at least 1.2 million requests are received without taking into account the crashes involving injuries(WHO, 2016). In middle-income and low-income countries, 24 million deaths per year are related to emergency medical conditions (Hsia et al., 2015). Only in the United States, 46.8 million medical 911 calls are made annually. In Bogotá, Colombia, an average of 2,135 calls are received per day, and around 759 ambulance dispatches are generated for emergency care (Jiménez, 2016).====Response time to an emergency medical situation has been a topic of interest in recent years (Pell et al., 2001; Blackwell and Kaufman, 2002; Pons et al., 2005; Jiménez Fabrega and Espila, 2010; Sánchez-Mangas et al., 2010; Keeffe et al., 2011; Blanchard et al., 2012). The importance and interest in the response times are because the pathologies associated with emergencies are time-dependent. In other words, the delay of care has a negative impact on the patient's health. On the other hand, adequate care and at the right time can modify their medical prognosis (Jiménez Fabrega and Espila, 2010). Furthermore, when the waiting time to receive the EMS grows, not only affects the patient's health condition but also the patient perceives a loss of well-being associated with the lack of primary care. The loss of well-being can be related to the human suffering and will increase until receiving the attention from the EMS.====Valuation of the human suffering had been recently studied in the context of humanitarian logistics in natural disaster situations (Holguín-Veras et al., 2013, 2016, Cantillo et al., 2017, 2018; Cotes and Cantillo, 2018; Macea et al., 2018a, Macea et al., 2018b; Macea et al., 2018a, Macea et al., 2018b) (Holguín-Veras et al., 2013, 2016; Cantillo et al., 2018a,b; Cantillo et al., 2018; Cotes and Cantillo, 2018; Macea et al., 2018a, Macea et al., 2018b; Macea et al., 2018). In the field of humanitarian logistics, Deprivation Cost Functions (DCF) have been developed to quantify the cost of deprivation, defined as the economic value of the human suffering that individuals perceive due to the lack of a good or service (Holguín-Veras et al., 2013).====For DCF, three main characteristics have been defined: a non-linear form, a convex form and a monotonically increasing form with respect to the time of deprivation (Holguín-Veras et al., 2013). These functions have been estimated using contingent valuation and, more recently, discrete choice models, using stated choice surveys. From discrete choice models, it has been possible to obtain welfare measures using the marginal substitution rate and the change in consumer surplus (Cantillo et al., 2017). The cost of deprivation mathematically approximates a DCF explained through parameters describing the good or service, the time of lack experienced, and individual characteristics.====In the context of EMS, the impact valuation of pre-hospital care is relatively new as a research topic (Lerner et al., 2006). From our best knowledge, any study has estimated the Willingness to Pay (WTP) for avoiding human suffering in this context. There is a need to propose new approaches to optimize the resources available in health care. Estimating the costs inherent to changes of welfare in this regard would change the focus or enhance the criteria currently used in the planning, operation and management stages of EMS.====The purpose of this paper is to assess, through discrete choice models, the WTP for avoiding the externality associated with the human suffering experienced by the patient who does not receive immediate attention in a medical emergency. Data used to estimate the model comes from a stated choice experiment, which was applied in Barranquilla, Colombia.",Willingness to pay functions for emergency ambulance services,https://www.sciencedirect.com/science/article/pii/S1755534518300733,March 2019,2019,Research Article,110.0
"Zhou Mo,Bridges John F.P.","Johns Hopkins Bloomberg School of Public Health, Department of Health Policy and Management, USA","Received 1 June 2017, Revised 27 October 2018, Accepted 25 November 2018, Available online 26 November 2018, Version of Record 22 December 2018.",https://doi.org/10.1016/j.jocm.2018.11.002,Cited by (10),"There has been an increasing interest in studying patient preference heterogeneity to support regulatory decision-making. While the traditional mixed logit (MXL) and the ====Significant preference heterogeneity was found in all models. The 2-class RELCL has the lowest ==== (8350.64) and prediction error (11.6%) compared to MXL (BIC = 9345.40; pred. err. = 13.0%) and the 5-class LCL (BIC = 8440.30; pred. err. = 16.4%), indicating improved model fit. Allowing random effects also reduces the number of classes from five in LCL to two, both having significant policy and clinical implications. RELCL provides the flexibility of LCL and the parsimony of MXL. Both our empirical results and sensitivity analysis shows that when there is significant preference heterogeneity among patients that cannot be captured by a small number of clusters, RELCL may be used to generate more accurate predictions and more parsimonious results that are policy-relevant.","There has been an increasing interest in studying patient preferences since the United States Food and Drug Administration (FDA) issued guidance in 2015 to incorporate patient preferences into their regulatory decision-making (CDRH, 2015) and used patient preference studies to support the market approval of new treatments (Ho et al., 2015, Peay et al., 2014). A key regulatory change from the past is that the agency is now willing to approve treatments even if the benefit-risk profile is only acceptable to a subset of risk-tolerant patients (CDRH, 2015). Studying preference heterogeneity among patients is therefore meaningful and has significant policy implications. Appropriate analytical methods should be used to ensure unbiased results be generated to support regulatory decision-making.====Mixed logit (also termed random parameter logit) and latent class logit (also termed finite mixture logit) are the most commonly used models to study unobserved preference heterogeneity in stated-preference literature (McFadden, 1986, Swait, 1994). Mixed logit (MXL) allows some or all of the preference parameters to be random and vary across individuals according to a continuous probability distribution (Bhat, 1998, Revelt and Train, 1998, Brownstone and Train, 1998). Latent class logit (LCL) assumes that preferences are discretely distributed. Preferences are similar among individuals within clusters but vary between clusters (McFadden, 1986).====Many health-related stated-preference studies prefer to use LCL to analyze preference heterogeneity because the segmentation results have more straightforward interpretation than MXL. Despite weaker assumptions on the distributions of parameters, LCL often fits the data at least as well as MXL (Louviere, 2006). However, one limitation of LCL model is that it uses a set of distinct values to capture preference heterogeneity. When the heterogeneity in a population is substantial and there is significant overlap between clusters, LCL often leads to too many classes (Lenk and DeSarbo, 2000). As MXL can accommodate more extensive heterogeneity with fewer parameters, it may not be sufficient when there are sizeable subgroups.====Lenk and DeSarbo (2000) proposed using random effects in LCL models as a remedy, which provides the flexibility of LCL and the parsimony of MXL. Keane and Wasi (2013) empirically compared the random effect LCL (RELCL) model (also termed mixture-of-normals logit or mixed-mixed logit) to the traditional MXL and LCL as well as other scale-adjusted logit models, and found RELCL was preferred when the structure of heterogeneity was complex (e.g., there were more than five classes, segments of individuals with lexicographic preferences, or segments of individuals whose choices were not influenced much by observed attributes). Given the complication and wide variation of preferences among patients, RELCL may be a useful tool to model preference heterogeneity in health. However, while RELCL has been tested in transportation and marketing (Greene and Hensher, 2013), only Keane and Wasi (2013) applied RELCL in health-related choice data. This paper sought to apply RELCL to analyze unobserved preference heterogeneity for treatments among patients. We empirically compare RELCL to the traditional MXL and LCL and examine circumstances where RELCL outperforms LCL through simulation. We also demonstrate the approaches to testing the underlying distribution of preferences and selecting the appropriate model to analyze preference heterogeneity for researchers who are interested in studying patient preferences.====The data is from a discrete-choice experiment (DCE) survey on preferences for diabetes medications among patients with Type 2 diabetes. Patient preferences for Type 2 diabetes treatments are selected as a case study because diabetes is a prevalent chronic condition that affects nearly 30 million individuals in the US. As patient population is diverse, preference heterogeneity among patients can be significant. In addition, the complications of diabetes result in significant morbidity and costs (CDC, 2014). While treatment options have expanded (Tran et al., 2015), poor adherence to known effective interventions persists. Understanding patient preferences and incorporating them in drug development and clinical guidelines could improve adherence, satisfaction, and quality of life, and have significant public health impact.",Explore preference heterogeneity for treatment among people with Type 2 diabetes: A comparison of random-parameters and latent-class estimation techniques,https://www.sciencedirect.com/science/article/pii/S1755534517301069,March 2019,2019,Research Article,111.0
Park Minjung,"Department of Economics, Ewha Womans University, 52 Ewhayeodae-gil Seodaemun-gu, Seoul, South Korea","Received 15 February 2018, Revised 24 October 2018, Accepted 2 November 2018, Available online 8 November 2018, Version of Record 24 November 2018.",https://doi.org/10.1016/j.jocm.2018.11.001,Cited by (2),It is well recognized that consistent estimation of ,"The propensity of an individual to behave in some way could be influenced by the prevalence of that behavior in the individual’s reference group, and such an influence is called a peer effect. Correlated behavior among members of the same group is observed in various contexts, including product adoption, and peer effects are frequently discussed as a possible mechanism that generates such correlated behavior. Prior literature, such as Manski (1993), Moffitt (2001), and Brock and Durlauf (2001), has long recognized challenges in identifying peer effects separately from other forces that also lead to correlated choices in the data, such as endogenous group formation,==== correlated unobservables,==== and simultaneity.==== When researchers do not have access to field experiments and instead have to rely on observational data, correctly accounting for other confounding factors is crucial in order to obtain consistent estimates of peer effects.====In this paper, I discuss another source of bias that could arise when a researcher examines peer effects in product adoption: a selection bias due to endogenous sample attrition. When people differ in their valuations of the product in ways that are not observable to the researcher, people with higher valuations will become early adopters and people with lower valuations will become late adopters, if they adopt at all. When the product under consideration is a durable good for which repeat purchase is insignificant, the set of people who remain in the market to make an adoption choice keeps changing over time, and in particular, only those with increasingly lower valuations will remain in the market as time progresses. This phenomenon of ‘a changing composition of potential buyers’ has attracted lots of attention in the literature on durable goods adoption and inter-temporal pricing for durable goods (e.g., Stokey, 1979; Song and Chintagunta, 2003; Nair, 2007),==== but its implication has not been explicitly explored in the context of peer effects estimation.====To determine the likely direction of the resulting bias, this paper considers a commonly used specification where the size of the installed base is used to capture the effects of peers’ past adoption on own adoption decision. In such a setting, there is a negative correlation between the size of the installed base (regressor of interest), which grows over time, and the error term that captures the unobserved valuations of remaining consumers, which become lower over time, and this could lead to a downward bias in the estimation of peer effects. This is in contrast to the upward bias typically generated by homophily, correlated unobservables, and simultaneity.====When people with higher (unobserved) valuations tend to adopt earlier, valuations for the remaining individuals come from a truncated distribution where the top of the distribution has been removed, and the truncation point becomes increasingly lower over time. Thus, consistent estimation of peer effects would require that a researcher account for the truncation of the value distribution during estimation. My proposed remedy for the bias is based on this idea. The proposed remedy can be easily implemented using built-in commands for panel data estimation with random effects, widely available in standard statistical packages, ==== the unobserved valuation is independent of the regressors in the underlying population and the researcher observes data from the initial period when all groups have zero adoption.==== I also examine how the proposed remedy can be applied in case the unobserved valuations are correlated within a group, in the spirit of correlated random effects in Mundlak (1978) and Chamberlain (1982, 1984). I present Monte Carlo simulations to numerically demonstrate the presence and extent of the selection bias as well as the remedy under various scenarios.====This paper falls into the intersection between the literature on peer effects and the literature on product adoption, two literatures that have evolved without significant overlap. While the literature on peer effects is vast, much of it has been in settings other than product adoption, such as peer effects in education (Coleman et al., 1966; Sacerdote, 2001). Similarly, while the literature on product adoption is vast, the focus has been mostly on aspects other than peer effects, such as dynamics of product adoption (Gowrisankaran and Rysman, 2012). The intersection between peer effects and product adoption comprises a very small segment of each literature and, while growing, is still thin. Papers that have examined peer effects in product adoption using observational data include Bell and Song (2007), Manchanda et al. (2008), Tucker (2008), Nam et al. (2010), Katona et al. (2011), Iyengar et al. (2011), Bollinger and Gillingham (2012), Narayanan and Nair (2013), Hu and Van den Bulte (2014), and Gardete (2015). To my knowledge, the issue of selection bias in peer effects estimation due to endogenous sample attrition has not been explicitly considered in the prior literature. This paper fills the gap and demonstrates how the combination of unobserved heterogeneity and departure from the sample after adoption leads to a selection bias in the considered setting.====In the discrete choice literature, it has become standard to control for unobserved preference heterogeneity, so the paper's emphasis that one needs to account for unobserved heterogeneity is not new. The value added of this paper is that it demonstrates the mechanism through which failure to control for unobserved heterogeneity systematically leads to a very specific type of bias for estimation of peer effects in case of durable goods adoption. The selection bias highlighted in this paper could be relevant for many observational studies on peer effects in product adoption, so this paper’s key contribution is to show the mechanism behind and bring attention to the potential selection bias for researchers who work on this small but important and growing class of problems.",Selection bias in estimation of peer effects in product adoption,https://www.sciencedirect.com/science/article/pii/S1755534518300162,March 2019,2019,Research Article,112.0
"Batram Manuel,Bauer Dietmar","Department of Business Administration and Economics, Bielefeld University, Postfach 10 01 31, D-33501, Bielefeld, Germany","Received 27 September 2017, Revised 28 September 2018, Accepted 27 October 2018, Available online 31 October 2018, Version of Record 20 November 2018.",https://doi.org/10.1016/j.jocm.2018.10.001,Cited by (1),"In this paper the properties of the maximum approximate composite marginal likelihood (MACML) approach to the estimation of multinomial ==== (MNP) proposed by Chandra Bhat and coworkers is investigated with respect to asymptotic properties. It is shown that, if the choice proportions are normalized to sum to one, a variant of the method provides consistent estimates of the choice proportions for a number of approximation methods.====Furthermore it is argued that each approximation method leads to a particular mapping of regressors to choice proportions which is close - but not identical - to the map induced by the probit model. If the data are in fact generated according to this mapping then standard asymptotics, that is consistency and asymptotic normality of the estimators, hold. If the data are, however, generated by a probit model and the approximations are used for estimation, then the corresponding estimators are not guaranteed to be consistent.====Different approximation methods are subsequently analyzed with respect to their asymptotic biases and additionally with respect to finite sample properties. It is shown that normalization of the choice proportions is essential for obtaining consistent estimates of the choice proportions. Normalization also decreases biases in parameter estimates.","Discrete choice models are routinely used for modelling mode choice, destination choice, choice of travel time, route choice, vehicle purchase decision, activity choice and many other areas (see e.g. Cascetta (2009); Train (2009)). They are used for example for the evaluation of new mode options, the design of pricing schemes for public transport, the adoption of new vehicle technologies, the impact of the provision of travel time information as well as in the simulation of transportation systems. Discrete choice models have been used in cross sectional data sets based on revealed preferences as well as panel data sets combining revealed and stated preference data (see Hess and Daly (2011)).====Most commonly discrete choice models are formulated using the random utility model (RUM) paradigm based either on the multinomial logit (MNL) or the random utility model (MNP) model. While different variants of the MNL models (including mixed models) have enjoyed a strong popularity, MNP models suffer from high computational costs. A major reason for the high computational costs is that the probit likelihood by definition involves the evaluation of the multivariate normal cumulative distribution function (MVNCDF) which is analytically intractable. Hence it is necessary to rely on approximation methods in order to evaluate the likelihood. For standard quadrature methods the relationship between the dimension of the integral and the computational complexity is of exponential order which renders those methods too time consuming for all but the smallest choice sets.====Therefore, the integral is usually approximated by Monte Carlo simulations, which depending on the number of repetitions used for simulations in comparison can lead to less accurate estimators than alternatives in some situations (see (Bhat, 2018) and (Fu and Juan, 2017)). When combined with maximum likelihood estimation those methods are known as maximum simulated likelihood (MSL) approach. The most widely used method is the Geweke-Hajivassiliou-Keane (GHK) algorithm (see e.g. (Train, 2009, p. 115) and (Bolduc, 1999)). The assessment of integrals by simulation is computationally appealing because the computational complexity of the simulation is an approximately linear function of the dimension of the integral (see Hajivassliliou, 2000, p. 88f). Like Monte Carlo methods in general MSL is justified asymptotically, which induces the need to rely on many simulation runs and, leads to biased estimates whenever the number of Monte Carlo (MC) replications is not increased fast enough as a function of the sample size (see Train, 2009, p. 250ff). Moreover the guidelines for choosing the number of replications derived from asymptotic results are only of limited help for a given sample size as they only restrict the rate of increase of the number of replications.====As an alternative, several authors have suggested that analytic approximations might have the potential to offer a faster way to estimate MNP models. In fact, the use of analytic approximations – namely the Clark approximation – to estimate MNP models predated the introduction of MSL (Daganzo et al., 1977). However, with the advent of MSL this approximation was deemed to be too imprecise and, therefore, its importance vanished (Horowitz et al., 1982). In general analytic approximations of the MVNCDF are known to be less accurate than quadrature and simulation methods (Joe, 1995) but they share neither the infeasibility of quadrature methods nor the long computation times of MSL for large choice sets.====Utilizing an analytic approximation, Bhat (2011) introduced the maximum approximate composite marginal likelihood (MACML) approach for simulation-free estimation of MNP models combining the Solow-Joe (SJ)-approximation for the MVNCDF and composite marginal likelihood (CML) estimation with the specific aim to speed up the estimation of complex MNP models. In a first simulation study MACML is reported to be up to 350 times faster than MSL estimation while the accuracy of parameter recovery was at least at par with the latter (Bhat and Sidhartan, 2011). A later simulation study revealed that the difference in computation times shrinks once MSL estimation is also performed within the CML framework but that there is still a reasonable performance gain which is then fully attributable to the analytic approximation (Patil et al., 2017). Furthermore (Patil et al., 2017), report that MACML in some settings is faster and more accurate than several competing estimation procedures including two GHK variants as well as Bayesian Markov Chain Monte Carlo (MCMC) estimation. Similar findings are also reported in (Fu and Juan, 2017).====However, the theory underlying the MACML approach still lacks an important piece as we are not aware of any results addressing the consistency properties of the estimator. While the consistency of CML methods is rather well understood (see Varin et al., 2011), it is unclear how the MVNCDF approximation interferes with estimation.====The MACML method as proposed in Bhat (2011) in based on the SJ-approximation but simulations in Connors et al. (2014) provide evidence that the Mendell-Elston (ME)-approximation is superior to the SJ-approximation with regard to accuracy of MVNCDF approximation and with respect to computation time. Furthermore (Trinh and Genz, 2015), and (Bhat, 2018) have recently presented improved variants of the ME-approximation which are also considered in this paper.====In order to enhance the understanding of the properties of MACML estimation we study the large sample properties of various MACML estimators using different approximations to the MVNCDF. We demonstrate using a simple cross sectional model that by normalizing approximated probabilities to sum to one the MACML estimators of the choice proportions for a given particular value of the regressors are consistent and asymptotically normal and hence follow standard asymptotics. In the example we also show that the estimators for the corresponding underlying parameters are potentially biased. Additionally there is no guarantee that the unnormalized choice proportions estimators are consistent.====The example subsequently is extended to investigate the estimation of the map linking regressor values to choice proportions. It is demonstrated that each approximation concept implements one particular such mapping that is close but not identical to the one implied by the MNP. The MACML estimators based on normalized proportions are shown to be consistent estimators of the mapping implied by the approximation concept used while it is potentially a biased estimator of the mapping implied by the MNP model. By way of contrast, a nonparametric estimator of the mapping will lead to a consistent estimation of the MNP model choice proportions while still leading to asymptotically biased estimators of the corresponding parameters.====Building on those insights a comparison in finite sample as well as with respect to asymptotic biases for all previously analyzed approximation concepts including MSL is given. The evidence given in this respect leads to the conclusion that – at least in the investigated examples – methods using normalized proportions provide superior estimators while between the various approximation concepts no method dominates all other methods with some methods clearly being worse than others. Even though we specifically focus on the MACML method our results have important implications whenever an analytic approximation is used in the context of MNP estimation (see e.g. Ochi and Prentice (1984), Kamakura (1989), Waddington and Thompson (2004) and Martinetti and Geniaux (2017)).====The outline of the paper is as follows: In section 2 the model used for demonstration purposes is introduced and the main underlying estimation and specification results are provided. Section 3 describes the various approximation concepts used, derives the properties of the corresponding maps connecting regressors to choice probabilities. Section 4 then presents and discusses our simulation results and section 5 concludes the paper.",On consistency of the MACML approach to discrete choice modelling,https://www.sciencedirect.com/science/article/pii/S1755534517301628,March 2019,2019,Research Article,113.0
"Liebe Ulf,von Meyer-Höfer Marie,Spiller Achim","University of Warwick, Department of Sociology, Social Science Building, Coventry, CV4 7AL, UK,Scotland's Rural College (SRUC) Land Economy, Environment and Society Group, West Mains Road, Edinburgh, EH9 3JG, UK,Georg-August-Universität Göttingen, Department of Agricultural Economics and Rural Development, Chair of Marketing for Food and Agricultural Products, Platz der Göttinger Sieben 5, 37073, Göttingen, Germany","Received 14 July 2017, Revised 25 July 2018, Accepted 30 July 2018, Available online 31 July 2018, Version of Record 11 November 2019.",https://doi.org/10.1016/j.jocm.2018.07.003,Cited by (14),"This research note presents the first study to implement a real choice experiment in a web survey. In a case study on ethical food consumption, we find statistically significant lower willingness-to-pay values for the attributes “organic production” and “fair trade” in a choice experiment involving real payments compared to a choice experiment without real payments. This holds only true for respondents who are prepared to provide their personal details in order to deliver the product (83% of the sample), providing further evidence that lack of consequentiality can be an important source of validity problems. The implementation of a real choice experiment online proves useful and can form the baseline for future tests of the effectiveness of ==== approaches such as cheap talk or honesty priming as well as consequentiality scripts in web-based choice experiments.","Over the past decades, the use of stated choice experiments (SCEs), here defined as multifactorial survey experiments with repeated binary or multinomial choice questions, has considerably increased in economics and other social sciences. Originally developed in marketing and transportation economics (see for example: Louviere and Hensher, 1982, Louviere and Woodworth, 1983, Louviere, 1988, Louviere, 1992), it has become a popular method in environmental, health and agricultural economics for eliciting preferences and estimating willingness-to-pay (WTP) for private and public goods that are currently not traded in markets (Louviere et al., 2000). This includes environmental public goods (Adamowicz et al., 2014), health services (Ryan et al., 2008), and novel food and consumer products (Alfnes and Rickertsen, 2011). In the case of consumer products, choice experiments are often used to investigate the market potential of products that have not yet been introduced, and to estimate consumers’ WTP for novel product characteristics, including health, environmental and ethical attributes (Lusk and Schroeder, 2004). In the context of ethical consumption, choice experiments are also used to investigate and explain differences in preferences and WTP values for existing consumer products such as organic and fair trade coffee (Andorfer and Liebe, 2013, Rousseau, 2015).====Following general trends in social research, SCEs are increasingly being conducted online (Liebe et al., 2015, Menegaki et al., 2016), and there is an interest in investigating the differences between web-based SCEs and those using other survey modes, for example regarding sample characteristics, response rates, and preferences (Olsen, 2009, Determann et al., 2017). We see a similar trend in behavioral economics where experiments are increasingly being conducted online, for example using Amazon's MTurk, and results are compared to those from laboratory experiments (Paolacci et al., 2010, Hauser and Schwarz, 2016). When compared with other research settings and survey modes, web surveys have the advantage that a larger sample representing a wider population with respect to socio-demographic characteristics such as age, education, and income, can be realized in short time and in a cost effective manner.====Irrespective of whether conducting a survey online or by another mode, the divergence of hypothetically stated and actual preferences and willingness to pay has been a major concern (e.g., List and Gallet, 2001, Harrison, 2006, Chang et al., 2009, Hensher, 2010, Fifer et al., 2014). Meta-analyses of the extent of hypothetical bias in stated preference methods that largely draw on contingent valuation studies find a mean ratio of hypothetical to actual willingness to pay of about 3 (List and Gallet, 2001) and median ratios of 1.35 (Murphy et al., 2005) and 1.39 (Penn and Hu, 2018); they further show that the hypothetical bias tends to be lower if private instead of public goods are valued, and if a choice-based format rather than a market-based elicitation mechanism (various types of auction and open-ended formats) is used to elicit WTP, possibly contributing to a persistent belief that SCEs may be less susceptible to hypothetical bias compared to other stated preference elicitation formats (Hoyos, 2010), although actual findings are limited and mixed (e.g., Carlsson and Martinsson, 2001, Lusk and Schroeder, 2004).====In this research note we demonstrate that choice experiments can be conducted with real payments in an online survey and compare results with a purely hypothetical SCE. We argue that, complementing other approaches, web-based real choice experiments can be used to study the validity of SCE results. By real choice experiment we refer to “[…] a straightforward extension of a stated choice experiment by including real economic incentives. As in stated choice experiments, participants are asked to make choices in a series of choice scenarios. […] To induce real economic incentives, one of the choice scenarios is randomly drawn as binding, and the choices made in that scenario are implemented. The participants pay the price and receive the product chosen in the binding scenario” (Alfnes and Rickertsen, 2011: 222). Using the terminology of Collins and Vossler (2009), this (making one choice binding) represents a typical provision rule aimed at increasing the consequentiality of the experiment.====Historically, many stated preference studies had been set up as purely hypothetical exercises. This was criticized for lacking incentive compatibility. As a consequence, it is now becoming best practice to emphasize consequentiality of SCE studies as a necessary condition of incentive compatibility (Johnston et al., 2017). Consequentiality implies that there is a non-zero probability that responses are perceived to affect (policy) outcomes and that respondents will be asked to pay for the implemented outcome. This is expected to increase the external validity of study results. The real-payment approach can be seen useful, because an actual exchange between money and the good at hand is still the best way of ensuring consequentiality in a stated preference survey: “Given equivocal evidence supporting any one (or combination) of these methods [==== approaches], we believe that the most promising ==== approach remains a consequential design with a binding payment” (Johnston et al., 2017: 356).====SCEs are most useful to assess demand for private goods with novel (product) characteristics that cannot (yet) be purchased in the market place, and to elicit preferences for policies and projects with public good character. However, ensuring payment consequentiality is difficult or impossible in such contexts. Nevertheless, we argue that researchers can learn from demonstrating how results differ between incentivized (involving actual monetary costs) and non-incentivized (not involving actual monetary costs) SCEs and can use results of real choice experiments as a benchmark for testing the effectiveness of devices aimed at increasing perceived outcome consequentiality and ==== procedures to enhance validity such as cheap talk (Cummings and Taylor, 1999, Carlsson et al., 2005, Tonsor and Shupp, 2011, Ladenburg and Olsen, 2014) or honesty-based methods (de-Magistris et al., 2013, Carlsson et al., 2013, de-Magistris and Pascucci, 2014, Howard et al., 2017). A contribution of this note is to demonstrate how such comparisons between incentivized and non-incentivized SCEs can be conducted online.",A web survey application of real choice experiments,https://www.sciencedirect.com/science/article/pii/S1755534517301276,31 July 2018,2018,Research Article,114.0
"Kim Hyun No,Boxall Peter C.,Adamowicz W.L.(Vic)","Department of Sustainable Development Research, Korea Environment Institute, Bldg B, 370 Sicheong-daero, Sejong, 30147, South Korea,Department of Resource Economics and Environmental Sociology, University of Alberta, Edmonton, Alberta, T6G2H1, Canada","Received 5 March 2018, Revised 11 July 2018, Accepted 11 July 2018, Available online 12 July 2018, Version of Record 11 November 2019.",https://doi.org/10.1016/j.jocm.2018.07.001,Cited by (5),. A comparison of models across different temporal windows to define an individual's choice set shows that model parameters are sensitive to the assumptions used to define the choice sets. The models that approximate choice set formation improve the efficiency of estimation and influence estimated welfare measures suggesting the importance of choice set formation in the context of discrete housing choice models.,"Most empirical studies have employed hedonic property analysis to investigate the economic value of environmental attributes associated with property values. Even though the value of changes in environmental attributes can be evaluated by marginal implicit prices obtained from a first-stage and quasi-experimental hedonic property analysis, this value would be valid only for marginal changes in attributes (Grafton et al., 2004). As well, it will be a good approximation of welfare change only if the assumption regarding a localized impact of change in environmental attributes is met.====For validating non-marginal changes in attributes as well as identifying more precise welfare measures, two different methodologies using revealed preference data (market data) can be considered; a two-stage hedonic approach and a discrete residential choice model. The two-stage hedonic method can be used to recover an inverse demand function but this approach is challenging to apply since it must deal with issues such as data availability and econometric-related issues (Phaneuf and Requate, 2017, Bockstael and McConnell, 2007).====The discrete residential choice model, initially developed by McFadden (1978),==== can also be used to value changes in housing attributes. The main distinction between the hedonic model and the discrete choice model are the different approaches used to uncover preference parameters (Bockstael and McConnell, 2007). While the hedonic approach depends on the precise gradient of the estimated hedonic function since it requires marginal implicit prices of attributes in order to incorporate them into a second-stage hedonic model, the discrete choice model does not require this step since preference parameters can be directly recovered from the utility function identified from parameter estimates. This advantage has appeal for the application of a discrete choice model in the environmental valuation literature.====In this study we develop discrete residential choice models based on random utility theory to investigate non-marginal changes in major environmental attributes (water resources) in a local area. Prior use of discrete choice models to examine residential choice in the context of environmental amenities include Palmquist and Israngkura (1999) and Banzhaf and Smith (2007). We extend this literature by examining the issue of the choice set, which approximates home buyers' consideration set in their housing purchase behavior. While choice set formation has been addressed in other literatures such as transportation mode choice and recreational site choice, it has not been well-researched in the residential choice literature. Examples include Manski, 1977, Swait and Ben-Akiva, 1987a, Swait and Ben-Akiva, 1987b, Horowitz and Louviere, 1995, Ben-Akiva and Boccara, 1995, Peters et al., 1995, Haab and Hicks, 1997, Cascetta and Papola, 2001, Martinez et al., 2009, and Truong (2013). In these studies the motivation for examining choice set formation relates to the decision makers' costs of acquiring information. While they might want to be fully informed about all choice alternatives, they typically do not have time or energy to characterize their full choice set and therefore use rules of thumb or heuristics to pare down the number of alternatives. In residential choice cases, however, while buying a house is an important decision and thus home buyers could expend considerable effort on discovering complete choice alternatives, they don't necessarily need to when defining their consideration set. For example, they could provide a realtor with a list of priority residential attributes; then the realtor will find a set of houses which meet their requirements. So, even for such an important decision, home buyers may not necessarily be willing to incur the costs of developing a fully informed choice set.====Therefore, in the context of residential choice analysis, the choice set issue will be important because an analyst must make assumptions about the size and extent of the set of choices from which a home buyer will make a purchase. As discussed by Banzhaf and Smith (2007), the model parameter estimates of attributes will be affected by the assumptions regarding the choice set and thus the assessment of welfare measures associated with environmental attributes will vary depending on these assumptions.====Our approach to construct sets of choice alternatives is different from previous studies in terms of relaxing some of the assumptions to be made. First, unlike previous studies that assumed all households face the same number of alternatives, we allow for the number of choice alternatives to vary among each household depending on the temporal windows used to define the choice set. We think this better represents the purchasing environment faced by prospective buyers. Second, given the choice sets defined by spatial and temporal extent, we relax the assumption that decision makers equally consider all alternatives in their choice set. That is, while some alternatives in the choice set are available to the decision maker, the degree of their consideration could differ based on particular attributes of those alternatives. To allow for varying degrees of availability of properties, we take into consideration some important housing attributes (e.g. house prices and perceived need for renovation) that home buyers could consider for their final purchase decision.====Our application involves housing in the Town of Chestermere and nearby Glenmore reservoir in Calgary, Alberta from 2000 to 2010. Our particular focus involves the use of a discrete residential choice model to assess the economic value of increases in the supply of aquatic ecosystem services provided by a major local water body - Chestermere Lake used for irrigation water storage.==== We focus on investigating welfare measures associated with the implementation of the 2005 Water Management Agreement signed by the Town and the Western Irrigation District==== who is in charge of managing/operating Chestermere Lake to control water level and quality in the lake for irrigators. In the next section, we provide background information in more detail regarding the water management policy in the study area.====We compare various choice models defined using different choice sets. For the comparisons of models defined by different temporal windows, we apply statistical tests discussed by Ben-Akiva and Lerman (1985) and examine consistency with theory to determine the models of interest. Given the choice sets defined with different temporal dimensions, we estimated models with/without the choice set formation using one sample of choice sets that we chose as a preferred model and compare welfare measures associated with a hypothetical situation involving no implementation of the water management agreement to examine the impact of choice set considerations on welfare measures.====Section 2 provides background on the study area. The relevant literature is reviewed in section 3. We also discuss two different approaches to define the unit of choice in residential choice modelling settings as well as their advantages, disadvantages, and the appropriate context for their application. The forth section outlines our empirical model specifications including our strategy used to define choice sets. The data employed in this study are described in section 5, while section 6 presents the empirical results and welfare measures. Section 7 concludes the analysis.",Analysis of the economic impact of water management policy on residential prices: Modifying choice set formation in a discrete house choice analysis,https://www.sciencedirect.com/science/article/pii/S1755534518300289,12 July 2018,2018,Research Article,115.0
Großmann Heiko,"Otto-von-Guericke-Universität Magdeburg, Fakultät für Mathematik, Institut für Mathematische Stochastik, Postfach 4120, 39016 Magdeburg, Germany","Received 22 September 2017, Revised 14 February 2018, Accepted 3 April 2018, Available online 6 June 2018, Version of Record 16 September 2019.",https://doi.org/10.1016/j.jocm.2018.04.001,Cited by (2),"A method is presented which facilitates the practical construction of designs for stated choice experiments in which the choice sets are pairs of partial profiles and where, for a potentially large number of two-level attributes, the main effects and two-factor interactions are to be estimated. Although partly heuristic, the approach has a sound theoretical basis and can be used to generate utility-neutral designs for the multinomial logit model which possess a high statistical efficiency. Applying the method neither requires expert knowledge of design theory nor specialized software and is illustrated with several examples.","We consider generic stated choice experiments in which the choice sets are pairs of alternatives and where one is interested in estimating the main effects and two-factor interactions of a potentially large number (say ten or more) of qualitative attributes. Every alternative is represented by a combination of attribute levels and in the marketing literature these combinations are usually referred to as “profiles”. When all attributes in the experiment are used to specify the alternatives, they are called full profiles. By contrast, alternatives which are specified by using only a subset of the alternatives are called partial profiles.====Even with pairs, processing the attribute information in each choice set is a formidable task when full profiles are used (see, e.g. Bridges et al., 2011). For this reason, the use of partial profiles may be considered. The question then arises how a partial-profile design can be generated from which all effects of interest can be estimated and which also performs well in terms of statistical efficiency criteria.====This paper presents a practical approach to this problem for the situation where all attributes have two levels. The assumed model is the multinomial logit model (MNL) for pairs of alternatives and designs will be constructed under the indifference assumption of equal choice probabilities. In the literature such designs are also referred to as being “utility-neutral” (Huber and Zwerina, 1996).====The work presented here is motivated by a study in health research which is currently being planned by a group of researchers in the UK who contacted the author to help with the construction of a partial-profile choice design for pairs with eleven attributes, each at two levels, and where only four attributes are to be shown to the respondents at any one time. The purpose of the proposed study is to develop better explanations of osteoarthritis for patients. Each attribute represents a domain, derived from a theoretical framework, that may be important for explaining this long-term condition, e.g. likely prognosis, current management options. The two levels of each attribute are statements illustrating current standard practice and a proposed new standard. The goal of the choice experiment is to quantify patients’ preferences for the different statements. The presence of interactions between statements was considered to be plausible and cognitive burden was a major concern.====Since the match between the model for generating an efficient design and the model for estimating the parameters, respectively the mechanism that generates the data, has been shown to be important (Bliemer et al., 2009; Bliemer and Rose, 2010), it seems worthwhile to comment briefly on the modeling assumptions before turning to the method of design construction which is the main topic of the paper.====Eye-tracking results of Meiβner et al. (2017) provide evidence that, contrary to choice sets of size five, pairs lead to fixation patterns that are consistent with an additive difference evaluation strategy which in turn is consistent with the MNL model. Moreover, when using pairs, respondents make more use of the available attribute level information and achieve higher hit rates in holdout tasks. Based on these findings, the authors recommend pairs for the type of application considered here. This is not to say that, for instance, triples should not be considered as another possibility, but rather that there is some empirical support for using pairs in certain situations.====A more advanced model that might be considered is the panel mixed logit model. Bliemer and Rose (2010) showed in two case studies with fixed priors that, in terms of local statistical efficiency, designs created for the MNL model perform relatively well when the panel mixed logit model is used to estimate the model parameters. Hence, designs for the MNL model seem to exhibit a certain degree of robustness. In view of the computational resources that are needed for generating efficient designs for the panel mixed logit model, Bliemer and Rose (2010, p. 732) suggest that efficient designs for the MNL model may be a “good starting point”.====The validity of partial-profile choice experiments may be compromised by the problem of inference making or imputation. This refers to the question how respondents treat attributes that are not shown in choice sets which consist of partial profiles. Do they ignore levels of attributes that are not presented or do they infer these levels in some way?====Contrary to main-effects-only models, assuming that levels of absent attributes are identical is not sufficient for obtaining valid estimates of interaction effects from partial-profile choice experiments since attributes that are not shown may interact with those that are presented (Bradlow, 2005; Chrzan, 2010). Instead, such estimates are only valid if respondents ignore absent attributes when making their choices. Bradlow et al. (2004) presented a model for how respondents might be using information from previous choice tasks to infer levels of absent attributes. By contrast, the research reviewed by Alba and Cooke (2004) seems to indicate that there is no compelling evidence that respondents infer levels of absent attributes in choice experiments. In what follows, it is therefore assumed that respondents have been instructed to ignore absent attributes and that this instruction has been efficient.====Comprehensive reviews of design theory for choice experiments have been presented from different perspectives (Burgess et al., 2012; Street and Burgess, 2012; Rose and Bliemer, 2014; Grossmann and Schwabe, 2015). Generally speaking, two broad approaches to the construction of statistically efficient designs can be distinguished. The first approach rests on the indifference assumption of equal choice probabilities, which is equivalent to assuming that the model parameters in the systematic part of the model are equal to zero, and uses mathematical techniques in order to create utility-neutral designs and to prove results about their performance. By contrast, the second approach utilizes prior information about the model parameters and/or their joint distribution and is more computational in nature. Efficient designs are typically constructed by using algorithms and simulation studies are used to evaluate their properties.====Constructing designs by using prior information about the model parameters can be beneficial when this information is reliable. However, when a choice experiment is to be conducted in an entirely new area, as is the case for our motivating example, such information will not be available and adopting the indifference assumption at the design stage appears to be reasonable. Likewise, utility-neutral designs are often useful for pilot studies. These examples illustrate that for certain problems the indifference assumption made here is appropriate.====In the statistical literature, optimal and efficient designs for partial-profile choice experiments with pairs have been derived by Graβhoff et al. (2004), Groβmann et al. (2006, 2009, 2014) and Manna and Das (2016) for the case where only the main effects of the attributes are to be estimated. Moreover, Manna and Das (2016) presented further results for estimating only the main effects in a model that contains main effects and two-factor interactions. Optimal and efficient designs for estimating both the main effects and two-factor interactions have been constructed by Graβhoff et al. (2003) and Groβmann (2017). Algorithms of Kessels et al. (2011, 2015) and Palhazi Cuervo et al. (2016) can be used to create partial-profile choice designs for estimating main effects. Similar algorithms that can handle main effects and interactions when only some of the attributes are shown to the respondents do not seem to be available.====Luyten et al. (2015) used pairs of partial profiles in a choice experiment, where the alternatives were described by seven attributes with mixed numbers of levels. In addition to the main effects, selected two-factor interactions were estimated. A Bayesian ====-efficient design for this experiment was generated by using a modified version of the algorithms of Kessels et al. (2011, 2015). The two partial profiles in each pair had different levels for four of the attributes and a constant level for each of the remaining attributes. With this type of partial profiles, all attributes are shown to the respondents in every choice question, whereas the partial profiles in the current paper use only some of the attributes.====It is an open empirical question which type of partial profiles is more suitable. The type of partial profiles used by Luyten et al. (2015) does not reduce the number of attributes the respondents need to consider. For experiments with even more than seven attributes this means that long lists of attribute levels need to be processed. Using all attributes avoids the potential problem of attribute level imputation but may lead to the use of non-compensatory, e.g. lexicographic, decision rules. Luyten et al. (2015) used colour to highlight the attributes with non-constant attributes with the goal of making the choice task easier. Although this may reduce the complexity of the choice task, there is also a risk that respondents may neglect the levels of the constant attributes which are however important for estimating the interactions. By limiting the number of attributes shown, the partial profiles used in the current paper reduce the amount of information respondents need to process in a more straightforward manner.====The remainder of the paper is organized as follows. Section 2 describes the design problem in more detail and presents a formula for calculating the efficiency of partial-profile designs. The practical method for generating designs, which is the main contribution of the paper, is presented in Sections 3 A practical approach, 4 Creating partial-profile designs from position arrays. The paper concludes with a discussion of the significance of the results.",A practical approach to designing partial-profile choice experiments with two alternatives for estimating main effects and interactions of many two-level attributes,https://www.sciencedirect.com/science/article/pii/S1755534517301574,6 June 2018,2018,Research Article,116.0
van Cranenburgh Sander,"Delft University of Technology, Faculty of Technology, Policy & Management, Transport and Logistics Group, Jaffalaan 5, 2628 BX, Delft, The Netherlands,The University of Sydney, Institute of Transport and Logistics Studies, Australia","Received 9 June 2017, Revised 2 February 2018, Accepted 20 February 2018, Available online 6 April 2018, Version of Record 29 May 2019.",https://doi.org/10.1016/j.jocm.2018.02.003,Cited by (3),"Due to the surge in the amount of data that are being collected, analysts are increasingly faced with very large data sets. Estimation of sophisticated discrete choice models (such as Mixed Logit models) based on these typically large data sets can be computationally burdensome, or even infeasible. Hitherto, analysts tried to overcome these computational burdens by reverting to less computationally demanding choice models or by taking advantage of the increase in computational resources. In this paper we take a different approach: we develop a new method called Sampling of Observations (SoO) which scales down the size of the choice data set, prior to the estimation. More specifically, based on information-theoretic principles this method extracts a subset of observations from the data which is much smaller in volume than the original data set, yet produces statistically nearly identical results. We show that this method can be used to estimate sophisticated discrete choice models based on data sets that were originally too large to conduct sophisticated choice analysis.","In numerous fields, recent technological advances have led to a surge in the amount of data that are being collected. These emerging data sources are changing the data landscape as well as the methods by which data are analysed. For instance, in the field of transport mobile phone, GPS, WIFI, and public transport smartcard data (Iqbal et al., 2014; Jánošíková et al., 2014; Prato et al., 2014; Farooq et al., 2015) are nowadays complementing or fully replacing traditional travel survey methods (Rieser-Schüssler, 2012), and data-driven methods (such as e.g. machine learning), as opposed to theory-driven methods, are increasingly becoming part of the standard toolbox of transport analysts (Wong et al., 2017). Moreover, it is widely believed that the amount of data that are being collected will continue to increase rapidly in the decades to come (Witlox, 2015).====Although these emerging data sources are widely believed to assist in understanding and solving numerous societal problems, they pose all sorts of new challenges to analysts. For choice modellers one major challenge relates to the computational burden. In particular, the size of these new data renders estimation of sophisticated state-of-the-art discrete choice models, such as Mixed Logit models (Revelt and Train, 1998) computationally burdensome, or even technically infeasible (Vlahogianni et al., 2015). This, in turn, is limiting the use of these emerging data sources in numerous fields where choice models are used. Moreover, even if model estimation is technically feasible on the large data set, many different model specifications are often tested. Therefore, long estimation times (which for current data sets may already easily take several days) quickly become prohibitive.====To deal with increasingly large choice data sets two types of approaches are commonly taken by analysts. The first approach is to revert to less computationally demanding models, such as Multinomial Logit (McFadden, 1974) and Nested Logit (Daly and Zachery, 1978) models. However, despite being very effective in reducing the computational efforts, this approach severely limits the analyst's ability to adequately model complex types of choice behaviour. As such, this approach is far from desirable. The second commonly taken approach is to modify estimation code in order to increase computational power, e.g. by taking advantage of parallel computing or cluster computing facilities. Recent technological advances have made it easier to employ cloud computing facilities and high performance computation clusters. However, many analysts do not have access to such facilities and existing widely available estimation software is typically not ready for taking full advantage of these technologies.====To the best of the authors’ knowledge, no efforts have been undertaken to scale down large choice data sets,==== such that initially large data sets can be used with standard discrete choice estimation software packages, such as Biogeme, Nlogit, and Alogit. While removing valid observations is considered a sin by many analysts working with discrete choice models, in fields like machine-learning down-scaling of data sets is more common practice (e.g. Arnaiz-González et al., 2016; Loyola et al., 2016). As we will argue in this paper, using a carefully sampled subset of choice observations can give nearly identical estimation results as compared to using the complete dataset. Hence, we believe that this approach is worth exploring from a practical point of view.====This study proposes a new information theoretic-based method that lowers the computational burden to estimate sophisticated discrete choice models based on large data sets. The method – which we call Sampling of Observations (SoO) – is inspired by, and closely related to, efficient experimental design. It reduces the size of the data by combining practices from the field of experimental design in Stated Choice (SC) studies (see e.g. Rose and Bliemer, 2009), with established notions from information theory (Shannon and Weaver, 1949). SoO constructs a subset of the data that consists of a manageable number of observations that are ==== highly informative on the behaviour that is being studied. It does so by sampling observations from the full data set in such a way that the D-error statistic of the subset is minimised, which means that Fisher information is maximised. The D-error is evaluated using what we call the sampling model. This model is a ‘simplified’ version of the sophisticated choice model that the analyst ultimately wishes to estimate. By using a simple model in the sampling stage, SoO is computationally cheap and fast to conduct.====The remaining part of this paper is organised as follows. Section 2 presents the methodology on information theoretic-based sampling of observations. Section 3 explores the efficacy of the method using Monte Carlo analyses. Finally, Section 4 closes with conclusions and a discussion.",Information theoretic-based sampling of observations,https://www.sciencedirect.com/science/article/pii/S1755534517301124,6 April 2018,2018,Research Article,117.0
"Schmid Basil,Axhausen Kay W.","Institute for Transport Planning and Systems (IVT), ETH Zürich, Stefano-Franscini-Platz 5, CH - 8093 Zürich, Switzerland","Received 24 May 2017, Revised 31 January 2018, Accepted 2 March 2018, Available online 26 March 2018, Version of Record 29 May 2019.",https://doi.org/10.1016/j.jocm.2018.03.001,Cited by (53),"This paper aims at explaining the choice between online and in-store shopping for typical search and ====Respondents with pro-online shopping attitudes have a higher shopping cost sensitivity, which can be explained by the expanded choice set when effectively considering both purchasing channels. They also exhibit a higher choice probability of online shopping for groceries compared to electronic appliances, given the nature of ==== being preferably purchased in-store, while the pleasure of shopping shows no substantial effect on choice behavior.====Results reveal a user profile of pro-online shoppers that is mainly characterized by a technology-oriented generation of younger and well-educated men. Also, given the relatively high value of ==== compared to the value of delivery time, we show that especially for electronic appliances, avoiding a shopping trip produces more benefits than waiting for the delivery of ordered products.","Information and communication technologies (ICT) have experienced a persistent increase in usage over the last decades, which, in the context of online shopping, allow for a more flexible spatial and temporal accomplishment of shopping activities (Mokhtarian, 2004). A shift from traditional store towards online shopping has been ongoing for some time, and has become more and more important in terms of market shares and individual behavior, as discussd in Rudolph et al. (2015) for the case of Switzerland. Regarding the interdependencies with travel behavior, Mokhtarian et al. (2006) argue that apart from expanding individuals’ choice sets, the potential effects of ICT are ambiguous and require further empirical investigations (see also e.g. Salomon (1986), Farag et al. (2007) and Cao (2009), for extended literature reviews on the topic). But what are the key attributes in individual decision making for either visiting a store or shopping online? How do people value travel, delivery and shopping/ordering time when directly facing the trade-offs between these two alternative shopping channels? Is there a difference between product categories, and how do socio-economic characteristics and soft factors, such as attitudes towards shopping and ICT related aspects, affect these trade-offs?====The data analyzed in this paper was collected as part of a project==== investigating how a world with restricted car ownership would affect choice, travel and scheduling behavior. Importantly, the absence of private cars was justified to the respondents by car-reducing policy developments, suggested by an increased public support of carpooling and free-floating car sharing systems, leaving public transportation as the only traditional reference mode for longer distances. The main objective of the project is to investigate how today's people behave in a possible future situation where private cars were no longer part of their daily travel (Schmid and Axhausen, 2015; Schmid et al., 2016). In the context of shopping, the main motivation is to explore how under such conditions, the choice behavior between in-store and online shopping and the heterogeneity in taste parameters can be explained by socio-demographics, attitudes and perceptions. However, although important for the overall project guidelines, the reader has to be alerted that presented results only hold under the current hypothetical situation, and cannot be generalized to real world applications.====30 years after the first study investigating the demand for teleshopping using discrete choice analysis (Manski and Salomon, 1987), we present an innovative survey design and a sophisticated modeling approach by investigating the relative importance of attributes related to the choice between in-store and online shopping for two product categories: Groceries, a typical experience good, and standard electronic appliances, a typical search good. The key characteristics of search goods can more easily be evaluated from externally provided information, while experience goods need to be physically inspected or tried (e.g. Peterson et al., 1997). Results provide new insights on purchasing channel preferences by allowing attribute sensitivities to differ by product type: In Switzerland, electronic appliances are often purchased online, while the main product characteristics of groceries are mainly obtained in-store (Rudolph et al., 2015). Importantly, multi-channel shopping, i.e. explicitly distinguishing between pre-purchase and purchase channels (Mokhtarian and Tang, 2013; Zhai et al., 2017), and multi-purpose shopping trips (Leszczyc et al., 2004) were ruled out to break down the experimental design to a manageable level of complexity.====Two latent variables (LVs) that are hypothesized to affect the choice of the shopping channel were tested, capturing the acceptance level of online shopping and the pleasure of shopping: We applied an integrated choice and latent variable (ICLV) modeling approach that enables the simultaneous estimation of attitudes defined by various socio-economic characteristics (Ben-Akiva et al., 2002), allows for a dedicated representation of the decision process and helps to structure respondent heterogeneity efficiently and more intuitively compared to a reduced form Mixed Logit model (Vij and Walker, 2016). Further considerations with ICLV models arise when dealing with panel data, which, even in advanced literature, was often not taken into account (see e.g. Kim et al. (2014), for an overview of hybrid choice models applied in travel behavior research). One main contribution of this paper is the application of advanced econometric methods to better understand individual decision making in the context of shopping channel choice, which, to our best knowledge, is the first alternative-specific hybrid choice model using stated preference data in the field of shopping behavior research.====The structure of the paper is organized as follows: Section 2 presents a short literature review on the factors affecting the shopping channel preferences. Section 3 describes the survey methods and explains how the attitudes towards online shopping and the pleasure of shopping were assessed. Section 4 provides a overview on the modeling framework. Section 5 presents the results and discusses the implications on behavior and valuation indicators. Section 6 provides a discussion, some concluding remarks and the main limitations of the study.",In-store or online shopping of search and experience goods: A hybrid choice approach,https://www.sciencedirect.com/science/article/pii/S1755534517300787,26 March 2018,2018,Research Article,118.0
"Sever Ivan,Verbič Miroslav,Klarić Sever Eva","Institute for Tourism, Vrhovec 5, 10000 Zagreb, Croatia,Faculty of Economics, University of Ljubljana, Kardeljeva ploščad 17, 1000 Ljubljana, Slovenia,Institute for Economic Research, Kardeljeva ploščad 17, 1000 Ljubljana, Slovenia,Department of Endodontics and Restorative Dentistry, School of Dental Medicine, University of Zagreb, Gundulićeva 5, 10000 Zagreb, Croatia","Received 4 September 2017, Revised 16 March 2018, Accepted 16 March 2018, Available online 24 March 2018, Version of Record 16 September 2019.",https://doi.org/10.1016/j.jocm.2018.03.005,Cited by (6),"This paper investigated the effects of including an opt-out option and the cost attribute on the elicited preference structure and response error variance in a discrete choice experiment (DCE) valuing the preferences for the delivery of dental care. The mixed logit framework was used for testing the effects of survey design features on respondents' preferences and scale. The standard practice of testing these effects was further expanded by using the structural choice modelling (SCM) framework. Recent studies have suggested that not offering respondents an opt-out option may distort the utility estimates. However, the influence of the opt-out option, framed as a ‘substitute care provider’ alternative, on the preferences for dental care and response error variance was not significant. When the cost attribute was added to the choice sets, the rank order of the attributes remained the same, and overall preferences did not differ significantly. This indicates that respondents did not change their decision rule. However, they were not very consistent in their preferences for all attributes. Including an extra cost attribute significantly increased the response error variance. The findings indicate that the cost attribute could be safely used, at least in similar contexts, without concerns for disturbing the preferences for other attributes. We did not find strong evidence that the effect of including an extra cost attribute is any different from the expected effect of including any other choice attribute; therefore, its influence may not be as relevant as some of the previous studies may have suggested.","Discrete choice experiments (DCEs) are a popular stated preference technique to elicit human preferences in a number of fields, such as health, transportation, marketing and environmental economics (de Bekker-Grob et al., 2012; Hoyos, 2010; Louviere et al., 2000; Mandeville et al., 2014). They describe a good in terms of a number of characteristics or attributes (e.g. waiting time, price of the treatment, etc.). The attributes can take different values, which are combined in an experimental design to describe different choice alternatives (Louviere et al., 2008). Two or more alternatives are offered in each choice set, and respondents are asked to choose their preferred alternative. Respondents’ choices imply implicit trade-offs between the levels of the attributes they would be willing to make, which could be used to estimate the weight or relative importance people assign to various service attributes (Hol et al., 2010; Hoyos, 2010; Mandeville et al., 2014). When the cost is included as an attribute, marginal utility estimates from DCE model can be converted into willingness-to-pay (WTP) estimates for improvements in the levels of other attributes.====The design of a DCE is still a developing field, and a number of studies investigated the effects of different survey design features, such as the selection of attributes and their levels, on the preferences of respondents (e.g. DeShazo and Fermo, 2002; Hensher, 2006). The cost attribute is particularly important in this context, as many DCE surveys have been focused on estimating welfare measures needed for policy-making purposes. This also applies to the economic valuation of health care, which is the focus of this paper. In some countries, such as UK and Denmark, where health care is largely free at the point of use, the monetary valuation of health care may be problematic because respondents are not used to paying out-of-pocket fees for health services. This may result in ignorance of the cost attribute or in a large number of protest responses (Pedersen et al., 2011; Ratcliffe, 2000). However, British and Danish citizens are used to paying considerable out-of-pocket fees for some health care services, such as dental care. US citizens are also familiar with paying for health care services. Therefore, the cost attribute may realistically describe a health care context and play a significant role in shaping patients' preferences. Previous studies examined the effects of the different levels chosen for the cost attribute, of the type of payment vehicle and ordering of the cost and other attributes, and the effects of the cost attribute itself on the elicited preferences and WTP estimates, but the findings were not always consistent (e.g., Carlsson and Martinsson, 2008; Kjaer et al., 2006; Pedersen et al., 2011; Ratcliffe, 2000; Skjoldborg and Gyrd-Hansen, 2003). Kjaer et al. (2006) conducted a DCE to study patients’ preferences for lubricant treatments of psoriasis in Denmark. The authors investigated ordering effects with respect to the cost attribute, operationalized as a monthly out-of-pocket expenditure, and detected a higher price sensitivity when the cost attribute was placed at the end of the program description. Furthermore, Skjoldborg and Gyrd-Hansen (2003) studied the preferences of the Danish population for hospitals and organization of the health care system in general, where the cost attribute was operationalized as either an out-of-pocket payment per hospitalization or an extra tax payment per year. The authors tested the effect of the range of the cost attribute, and concluded that a wider cost range is associated with a higher WTP. Furthermore, Veldwijk et al. (2014) studied the preferences of Dutch patients diagnosed with type 2 diabetes mellitus and demonstrated that not including an opt-out option (i.e. a 'neither' alternative) when valuing lifestyle programs through a DCE distorts welfare estimates and predicted uptake of health care programs. In their review of DCEs studying the job preferences of health workers, Mandeville et al. (2014) concluded that almost a third of studies included an opt-out alternative.====This paper builds on the study of Pedersen et al. (2011) who investigated patients’ preferences for organizational characteristics in general health care practice in Denmark. The authors analyzed whether the inclusion of cost attribute influences preferences for other DCE attributes and error variance in the context of both forced and unforced choices. The cost attribute was operationalized as a user fee for consultation. The utility and scale parameters were not affected when the cost attribute was included in an unforced DCE with a status quo option. On the other hand, in a forced choice, when respondents were not offered a status quo option, the inclusion of the cost attribute tended to change the choice behavior. The test of equal utility parameters was rejected, and rank order, marginal rates of substitution and error variance differed between DCEs with and without the cost attribute. Our paper investigates similar research questions in the context of dental care. A survey was conducted at the dental clinic of the School of Dental Medicine, University of Zagreb, Croatia. The school clinic, which serves as a platform for student training, is a part of the public health care system, so dental care is currently provided without any out-of-pocket payments. As publicly funded health care is under increasing financial pressure, governments often encourage market-oriented reforms and reduced state intervention (Giovanella and Stegmuller, 2014). Dental school clinics are increasingly faced with reduced government funding and forced to search for alternative funds on the real market, such as introduction of fees or out-of-pocket payments for their services (Kim et al., 2012). When introducing fees for public health care services, particular attention has to be given to the behavioral responses of patients (Kiiskinen et al., 2010).====Not including an opt-out option in a DCE may affect WTP estimates and predicted probability analysis. Existing studies suggest that the unforced choice format with the option to opt out or choose a status quo should be applied when such an alternative is available in real life (Lancsar and Louviere, 2008). While the appropriate format of the opt-out option is often not easy to identify, it should resemble the actual choice situation faced by individuals as closely as possible (Pedersen and Gyrd-Hansen, 2013). Instead of following a standard practice and using a ‘neither’ or status quo alternative to frame the opt-out option, a distinctive ‘substitute’ alternative was offered to respondents, i.e. the possibility to substitute faculty dental care with private dental care. Since the patients were surveyed at the Department of Endodontics and Restorative Dentistry, the hypothetical delivery of dental care described to patients was associated with an endodontic one-root canal treatment. In this context of dental care, which describes acute health care, ‘no treatment’ is not an eligible alternative in real life and was therefore not included in the choice sets. A status quo option was also not considered eligible, since one of the study objectives was to value faculty dental care, currently provided without out-of-pocket costs, in monetary terms, and to assess behavioral responses of patients to the introduction of fees. However, patients have other options, most notably dental care in the private dental office, which was used as an opt-out alternative, to simulate the real market conditions and account for potential substitution effects between the public and private dental care providers.====The effects of including/excluding the cost attribute were analyzed by using the same set of respondents – each respondent answered two groups of choice tasks that differed with respect to the inclusion/exclusion of the cost attribute, thereby avoiding a split sample design (in which particular respondent answers either a DCE with the cost attribute or a DCE without the cost attribute) used in the study of Pedersen et al. (2011). In that way, we were able to control for (un)observed differences in respondents' characteristics and their preferences that might confound the effects of including/excluding the cost attribute from the choice task. To account for preference heterogeneity across respondents when analyzing the effects of unforced/forced choices and of including/excluding the cost attribute, a more flexible mixed logit (MXL) model was used instead of multinomial logit (MNL) model previously considered by Pedersen et al. (2011). Not accounting for unobserved preference heterogeneity may bias the parameter estimates (Hole, 2008; Hoyos, 2010). Furthermore, this paper contributes to the literature by expanding the practice of testing the effects of survey design features on respondents' preferences and choice behavior. The structural choice modelling (SCM) approach, introduced by Rungie et al. (2011), was used to examine to what extent the preferences for a particular attribute of dental care are correlated across DCEs with and without the cost attribute.====This paper investigated the effects of including an opt-out option, framed as a ‘substitute care provider’ alternative, and the cost attribute in a DCE on the elicited preference structure and magnitude of response error variance, and whether the consistency in individuals' preferences differs across the choice attributes when an extra cost attribute is added to the choice sets. If the preferences, rather than the variance, are influenced by the design properties (i.e. including an extra cost attribute) this is more troubling for the validity of the DCE (Bech et al., 2011).",Cost attribute in health care DCEs: Just adding another attribute or a trigger of change in the stated preferences?,https://www.sciencedirect.com/science/article/pii/S1755534517301501,24 March 2018,2018,Research Article,119.0
"Lhéritier Alix,Bocamazo Michael,Delahaye Thierry,Acuna-Agost Rodrigo","Innovation and Research Division, Amadeus S.A.S., 485 Route du Pin Montard, 06902 Sophia Antipolis Cedex, France","Received 29 May 2017, Revised 16 February 2018, Accepted 16 February 2018, Available online 16 March 2018, Version of Record 29 May 2019.",https://doi.org/10.1016/j.jocm.2018.02.002,Cited by (57)," Multinomial Logit model in terms of accuracy and computation time, with less modeling effort.","There is a growing interest within the travel industry in better understanding how customers choose between different itineraries when searching for flights. Such an understanding can help travel providers, either airlines or travel agents, to better adapt their offer to market conditions and customer needs, thus increasing their revenue. This can be used for filtering alternatives, sorting them or even for changing some attributes in real-time (e.g., changing the price).====The field of customer choice modeling is dominated by traditional statistical approaches, such as the Multinomial Logit (MNL) model, that are linear with respect to attributes and are tightly bound to their assumptions about the error distribution. While these models offer the dual advantage of simplicity and readability, they lack flexibility to handle collinear attributes and correlations between alternatives. A large part of the existing modeling work focuses on adapting these modeled distributions, so that they can match observed behavior. Nested (NL) and Cross-Nested (CNL) Logit models are good examples of this: they add terms for highly specific attribute interactions, so that substitution patterns between sub-groups of alternatives can be captured. Another strong limitation is their inability to model different behavior according to individual specific variables. Latent Class MNL models (Greene and Hensher, 2003) extend MNL to take into account different individual segments whose number must be specified in advance. In these models, class membership is also represented with a Multinomial Logit, therefore suffering the same drawbacks mentioned before.====In our particular problem, there are two main segments to be taken into account. Business and leisure air passengers behave very different when it comes to booking flights. Business passengers tend to favor alternatives with convenient schedules such as shorter connection times and time preferences. On the other hand, leisure passengers are very price sensitive, which means that they can accept a longer connection time if this is reflected in a lower price of the tickets. The problem is that the customer segment is not explicitly known when the traveler is searching, however it could be derived by combining different factors. For example, industry experts know that business passengers have a tendency to book with lower lead time and are not predisposed to stay on Saturday nights. However, these are not “black or white” rules, which reinforces the need for a model able to detect these patterns depending on the data and actual customer behavior.====In this work, we propose and evaluate a novel approach to choice modeling through the use of machine learning techniques. The selected machine learning methods can model non-linear relationships between feature values and the target class, allow collinear features, and have more modeling flexibility to automatically learn implicit customer segments.====Decision trees are well adapted to our problem as model bifurcations (branches) are well-disposed to automatically partition the customers into hierarchical segments and, at the same time, capture non-linear relationships within attributes of alternatives and characteristics of the decision maker, if this has a positive impact on the prediction accuracy. In particular, we have chosen to work with machine learning methods based on ensembles of decision trees, namely Random Forests (RF) (Breiman, 2001), which further improve the performance over unseen data.====In contrast to previous applications of MNL to air itinerary choice (e.g., (Newman et al., 2016)), our dataset is much more complex. We consider round-trip alternatives, multiple markets (origin/destination pairs), different travelers profiles, and different points of sale; all together in a single data set of sessions. Thus, alternatives differ between sessions and they cannot easily be labeled in few discrete categories in contrast to classical MNL examples such as {car, bus, air}. Moreover, the number of alternatives is not constant and it varies up to 50 in addition to the fact that some of them could be highly correlated (e.g., same outbound but different inbound flights). This increases the complexity and makes the weaknesses of MNL more evident.====The rest of this paper is organized as follows: Section 2 presents a literature review of related works. Section 3 introduces the background and problem formulation for the experiments. Section 4 presents a novel approach based on machine learning. Section 5 presents numerical experiments and their results. Finally, Section 6 draws the main conclusions and present some perspectives of this work.",Airline itinerary choice modeling using machine learning,https://www.sciencedirect.com/science/article/pii/S1755534517300969,16 March 2018,2018,Research Article,120.0
"La Paix Puello Lissy,Chowdhury Saidul,Geurs Karst","Centre for Transport Studies, University of Twente, P.O. Box 217, 7500 AE Enschede, The Netherlands","Received 25 June 2017, Revised 5 March 2018, Accepted 7 March 2018, Available online 15 March 2018, Version of Record 29 May 2019.",https://doi.org/10.1016/j.jocm.2018.03.003,Cited by (5),"This paper examines the effects of socioeconomic characteristics, trip characteristics and life events on outdoor leisure activities and leisure duration in the Netherlands, based on 14 554 observations from three waves of data from The Netherlands Mobility Panel (in Dutch: ====). A standard mixed logit as well as a ‘zero-leisure’ scaled model was estimated to cover interpersonal and intrapersonal heterogeneity, The model was estimated for weekends, weekdays, transport mode choice of the activity, and specific leisure activity.====The results show that ====The paper highlights the importance of analysing duration of activities for different activity types and days of the week and underlines the strong link of temporal (week, year) and spatial (activity type location) dimensions with transport mode choice.","In transport-related research, the flexibility of outdoor leisure and recreational activities confers complexity to the demand models. The type of activity for which a journey is undertaken (such as sports or shopping) also determines the type of location (land use). In recent decades, the demand for recreational activities has increased because of increasing wealth, aging populations and changing lifestyles (Grigolon et al., 2013). It has therefore become important to analyse the expected demand for such locations, which is also associated with the duration of the undertaken activities and the transport needs of those locations. For example, the availability of parking facilities, operating times of public transport, proximity to leisure locations and connectivity by public transport can limit the duration and frequency of the leisure activities a person or family engages in.====Most non-work travel activity models focus on activity travel patterns for household interactions, recreational activities, and mode choice on holidays or weekends. Models for non-work journeys include a broad spectrum of trip purposes, such as family visits, shopping, as well as school- and church-related activities (Huang and Levinson, 2015). Bhat and Gossen (2004) looked at walking, jogging, riding a bicycle, and driving (touring) without having a specific destination as recreational activities. Bhat et al. (2004a) have examined how intra-household interactions influence weekday in-home and out-of-home activity generation; Habib and Hui (2015) have analysed the travel scheduling behaviour of older people for daily activities by using the framework of Bhat et al. (2004a). Arman et al. (2015) developed a model for household activity and vehicle allocation. Pozsgay and Bhat (2001) explored an attraction-end choice model of home-based urban recreational trips and Wang et al. (2015) investigated holiday travel behaviour characteristics in a trip chain. Overall, though, there is still little attention for non-work (leisure) activities.====The duration of outdoor leisure activities leads to dynamism in the transport market. Several factors influence the duration of leisure activities, such as a person's variety of activities and locations (intrapersonal variation). Studies based on longitudinal data have been conducted to analyse activity participation (Axhausen et al., 2002) and temporal and spatial variability of leisure activities (Schlich et al., 2004). These studies highlight the importance of analysing leisure traffic, and the relevance of modelling longitudinal data that covers both changing situations and sporadic behaviour.====Similarly, it is known that the mixture distributions can be discrete or continuous, and previous work show that any sample population may be decomposed into discrete segments that differ in their awareness of and proclivity towards certain mobility pattern (Vij et al., 2013). Multiple discrete continuous structures (MDC) have emerged as popular framework (Bhat, 2008), applied to goods consumption when continuous nature is involved, such as distance and time. Recently, MDC models have been extended to an integrated latent variable and choice model of time allocation (Enam et al., 2018), highlighting the importance of socioeconomic characteristics and individual's heterogeneity in activity durations. Also, some model studies treat activity type and duration as a continuous variable, e.g. multiple-discrete continuous extreme value models –MDCEV- (Calastri et al., 2017) rather than treating duration of activities as categorical manifestations. In most of the cases, this model structure assumes that individuals have a constant daily (24 h) budget of activity duration. An utility function for each (discrete) alternative is estimated and linked with the good consumption. And, a generic set of ‘consumption’ (e.g. duration) parameters is estimated.====However, individuals can have apriori preferences for a specific category of good consumption, in this case leisure activity duration (i.e. short or long duration leisure activity), depending on their preferences, activity patterns and available time budget during the day. So, longer durations do not always mean higher utilities. For example, a long duration leisure trip to an amusement park in the weekend versus a short duration lunch walk at a working day. Also, Calastri et al. (2017) indicated that consumer's preference structure might be such that she/he might not derive additional utility from additional consumption beyond a certain maximum level of consumption of a product. Exploring this required flexibility, some authors consider multiple constraints in time allocation models (Castro et al., 2012). Being flexibility a strong motivation in this paper to analyse duration of activities as categories instead of continuous variable, which allows the estimation of range-specific parameters, when an individual faces a single choice situation at the time.==== Previous research highlighted the advantages of discretization of alternatives in choice models. For example, Antonini et al. (2006) generated discrete categories of physical space, via the discretization of the set of walking alternatives. And, Vovsha and Bradley (2004) developed a hybrid discrete-duration model with temporal resolution (discretization) of 1 h. As described in Vovsha and Bradley (2004), this model has the advantages of a discrete choice structure (the flexibility and easy to estimate and apply) and the advantages of a duration model as the parsimonious structure with a few parameters that support any level of temporal resolution including continuous time.====Furthermore, the advantages of a repeated panel data are considered here to account for individual's heterogeneity as consumer's preference via (panel effects of) a mixed logit structure, see La Paix Puello et al. (2017). None of the previous modelling studies on discrete-continuous structures of activity durations have considered long term repeated panel data.====This paper presents an analysis of the duration of out-of-home leisure activities as discrete choice categories. The objective of our study was to investigate the factors that affect duration of outdoor leisure activities in a medium-term perspective (3 years). We explore shopping, sports/hobbies, tourism, walking, and other leisure activities and we took three years of data from The Netherlands Mobility Panel (MPN) to conduct the analysis on. The novelty in this paper comes from both the applied method as the characteristics of the data used. This paper develops an extended mixed logit modelling framework which uses discrete measurements of activity durations to disentangle range-specific effects of the estimated parameters, and a scale parameter of ‘zero-leisure days’ to incorporate temporal interdependencies between activities. Furthermore, we use data from the MPN, a unique panel data set in size (currently the largest ongoing mobility panel in the world) and scope (a very rich set of variables). This allows estimations of the effects of socioeconomic characteristics, trip characteristics and life events on outdoor leisure activities and leisure duration, and incorporation of interpersonal and intrapersonal variations in leisure activities and duration over time.====The remainder of this paper is organised as follows. Section 2 introduces the databases that were used for trip characteristics, socioeconomic characteristics, life events and other variables. Section 3 discusses the model setup (setup of alternatives, specification of variables, and conceptual model). Section 4 explores the statistics of the respondents and the variables collected from the database. Section 5 discusses the model results and Section 6 highlights the main findings of our study and provides recommendations for future work.",Using panel data for modelling duration dynamics of outdoor leisure activities,https://www.sciencedirect.com/science/article/pii/S1755534517301161,15 March 2018,2018,Research Article,121.0
"Wong Timothy,Brownstone David,Bunch David S.","National University of Singapore, U.C. Irvine, U.C. Davis, USA","Received 26 May 2017, Revised 22 December 2017, Accepted 4 February 2018, Available online 24 February 2018, Version of Record 29 May 2019.",https://doi.org/10.1016/j.jocm.2018.02.001,Cited by (4),"This paper examines the common practice of aggregating choice alternatives within discrete choice models. We carry out a Monte Carlo study based on realistic vehicle choice data for sample sizes ranging from 500–10,000 individuals. We consider methods for aggregation proposed by McFadden (1978) and Brownstone and Li (2017) as well as the more commonly used methods of choosing a representative disaggregate alternative or averaging the attributes across disaggregate alternatives. The results show that only the “broad choice” aggregation method proposed by Brownstone and Li provides unbiased parameter estimates and confidence bands. Finally, we apply these aggregation methods to study households’ choices of new 2008 model vehicles from the National Household Travel Survey (NHTS) where 1120 unique vehicles are aggregated into 235 make/model classes. Consistent with our Monte Carlo results we find large differences between the resulting estimates across different aggregation methods.","This paper studies the practice of aggregating choice alternatives within discrete choice models. Assume that a researcher can establish the “ideal” level of detail for defining choice alternatives in a discrete choice problem, and denote these “exact choices.” Researchers have often faced situations where exact choices yield choice set sizes that are too large for practical model estimation. In the literature for the main example considered here (vehicle choice), choice sets have frequently been defined at a lower level of detail (e.g., vehicle type), by aggregating over the relevant exact choices. In some cases, choices might be observed at a lower level of detail, so researchers estimate models by aggregating exact choices to the observed level. However, this practice of aggregation miss-specifies the true choice set of interest. Previous work (Brownstone and Li, 2017, Wong, 2015) investigated this concern within the context of the Berry, Levinsohn, and Pakes (BLP) choice model for micro- and macro-level data. This paper compares commonly used methods for estimating choice models where choices need to be aggregated. In addition to the aggregation methods given by McFadden (1978) and Brownstone and Li (2017), we also investigate the common practices of averaging attributes across the aggregated alternatives (Lave and Train, 1979, Bento et al., 2009) and choosing one alternative to represent the alternatives being aggregated. Common candidates for representative alternatives include the modal alternative (Berry et al., 2004) and the “base” alternative, which often is the alternative with the most basic features (Berry et al., 1995, Petrin, 2002.).====The key problem we are trying to solve is that the exact alternative chosen by a household is not observed. If it were and the true data generating process is multinomial logit, then fitting a standard logit model with a choice set containing the chosen alternatives and a sample of other non-chosen alternatives would lead to consistent but inefficient estimation (due to the Independence of Irrelevant Alternatives property of the logit model). The method recently proposed by Fernández-Antolín et al. (2017) averages across many such estimators and is therefore an improvement over just choosing one set of representative alternatives, but is inconsistent if the chosen alternative is not fully observed (as is the case with common U.S. data sources).====We investigate the issue of potential aggregation bias by generating results for a specific example (new vehicle choice) based on previous empirical work using the 2009 National Household Transportation Survey (NHTS). For specific time intervals, new vehicle purchases correspond to household choices of 2008 model year vehicles. In any recent model year there are well over 1000 vehicle configurations available, which can vary in important ways (e.g., fuel economy and performance) due to alternative engine, drivetrain, and transmission offerings. Modelling choice at the make/model level reduces the number to about 230 options, implying considerable aggregation. For example, there are 7 different trim lines under the Honda Civic label, and over 100 trim lines under the Ford F-150 make/model label, with notable variation in vehicle equipment that affects fundamental attributes such as purchase price and fuel consumption rate.====Using these data, we generate a much simpler Monte Carlo setting to isolate the impact of aggregating alternatives from the possible effects of model misspecification. The data generation process is specified as conditional logit with an outside good, 3 make/models grouped into “cars” and 3 make/models grouped into “trucks.” There are 98 distinct alternative vehicles that are grouped into make/models. In one case there was only one vehicle assigned to a particular make/model which corresponds to some vehicles in the real marketplace (e.g. Toyota Prius in the early 2000s), and the remaining make/models correspond to between 2 and 55 vehicles. We consider alternative aggregation methods from the literature for estimating the 7-alternative choice model corresponding to only observing the make/model. As is true in the real U.S. vehicle market we assume that there are data on the attributes of each vehicle, but unlike BLP we do not assume that we have any macro-level market share information for the vehicles. McFadden’s (1978) aggregation procedure only requires information on the mean and covariances of the attributes being aggregated, but it is only valid for aggregating alternatives at the lowest level of a Nested Logit choice model. We examine the performance of the estimators with sample sizes ranging from 500 to 10,000. This range encompasses most of the applied literature.====The Monte Carlo results show that the only method that performs well for all sample sizes is the “broad choice” estimator described in Brownstone and Li (2017). This is not surprising since this estimator is the maximum likelihood estimator for this problem. Both the coefficient estimators and their covariance estimators are biased for the other methods. Simply averaging attributes clearly leads to measurement error, and this is not helped by including the logarithm of the number of vehicles being aggregated as is done in some studies. McFadden’s (1978) method would be consistent if the joint distribution of the attributes that are aggregated are multivariate normal, but this approximation is not satisfied in our Monte Carlo design even if we relax the implied constraints on the parameters.====Finally, we apply some of the estimation procedures used in the Monte Carlo design to real vehicle choice data. We do not apply the “representative alternative” method since it has no theoretical justification and performed poorly in the Monte Carlo experiments reported in Appendix B. We use data from the 2009 NHTS survey supplemented by detailed data on attributes for each of the 1120 2008 model year vehicles. Like Train and Winston (2007) we do not include an outside good since it would include both purchasing used vehicles and not buying any vehicles. We recognize that not including the outside good is only justified if the data generating process is Nested Logit with “Buy New”, “Buy Used”, and “No Buy” at the top level, but we do not have the data necessary to deal with used car attributes. The purpose of this empirical exercise is to show that the problems with aggregation methods found in the Monte Carlo results also apply with real data. Since the NHTS only collects data on make, model, and year for each vehicle, these 1120 vehicles need to be aggregated into 235 make/model classes. As expected from the Monte Carlo results, we find large differences between the estimates produced from the various methods. The confidence bands for willingness to pay estimates do not overlap, and the Broad Choice and McFadden's method yield larger willingness to pay estimates than averaging attributes or choosing representative vehicles.====This paper shows that it is critical to properly account for the biases introduced when aggregating alternatives. We have demonstrated the importance of these biases in both a Monte Carlo study and an empirical example using the conditional logit model in the simplest case where there is no external market share data available. Our earlier work shows that incorporating external market share data does improve the quality of the estimates, but does not alleviate the problems caused by aggregating alternatives. The Broad Choice maximum likelihood method is the only one that performs well in our Monte Carlo study, and we expect that it will continue to perform well in other applications with more flexible discrete choice models. The simple expedients of averaging attributes or picking a “representative” alternative perform very poorly in our studies, and we expect them to perform at least as poorly in other situations.",Aggregation biases in discrete choice models,https://www.sciencedirect.com/science/article/pii/S1755534517300933,24 February 2018,2018,Research Article,122.0
"Vondolia Godwin K.,Navrud Ståle","Norwegian College of Fishery Science, UiT The Arctic University of Norway, Box 6050, 9037 Tromsø, Norway,School of Economics and Business, Norwegian University of Life Sciences, Aas, Norway","Received 16 May 2017, Revised 18 December 2017, Accepted 31 January 2018, Available online 9 February 2018, Version of Record 29 March 2019.",https://doi.org/10.1016/j.jocm.2018.01.003,Cited by (12),"An increasing number of ==== studies adopt both monetary and non-monetary payment modes to elicit preferences for goods and services in developing and transition countries. The extent to which these alternative payment modes approximate the underlying human preferences for these goods and services is poorly understood. The circumstances under which monetary and non-monetary welfare measures can be combined for efficient estimation of welfare measures and to guide public resource allocation also remain unclear. In a split-sample design, we present a choice experiment on the purchase of ==== in which insurance premiums are paid in money, labour time and harvests. We use an integrated choice-modeling framework to test for differences in relative scale parameters among these three alternative payment modes. We find that the relative scale parameters for non-monetary payment modes are lower than the relative scale for monetary payment mode. We argue that the two non-monetary payment modes exhibit higher degrees of uncertainties in the choice experiment. We discuss possible causes and the implications of these results for the design of ==== studies and the use of resulting welfare measures in cost-benefit analyses.","The use of non-monetary numeraires to elicit stated preferences for non-market goods and services has been in vogue in developing and emerging countries for over two decades. Some of the early uses of non-monetary payment modes in stated preferences include Swallow and Woudyalew, 1994, Shyamsundar and Kramer, 1996 and Echessah et al. (1997). The main motivation for using the non-monetary numeraires in these earlier applications is to present scenarios that are more familiar and realistic in economies with high degrees of subsistence (Whittington, 2010). For instance, Shyamsundar and Kramer (1996) use rice to value tropical rainforest protection among local peoples of Madagascar as cash transactions are uncommon among these peoples. However, recent studies have provided additional arguments to suggest that non-monetary numeraires are, in fact, more preferable to monetary exchanges for non-market goods and services in these countries. For instance, Asquith et al. (2008) find that respondents prefer non-monetary payments to monetary payments in exchange for environmental services. Moreover, Brouwer et al. (2008) show in a follow-up survey that zero monetary bids can be eliminated in stated preference studies by the adoption of non-monetary numéraires. Furthermore, O'Garra (2009) suggests that the respondents with lexicographic preferences may find non-monetary exchanges such as time more acceptable in exchange for environmental goods and services. These factors may account for the further uses of non-monetary numeraires for stated preference elicitation in developing countries (see e.g. Brouwer et al., 2008, O'Garra, 2009, Rai and Scarborough, 2013, Vondolia et al., 2014, Gibson et al., 2016).====The increasing use of non-monetary payment modes for stated preference elicitation often in parallel with monetary payment modes raises questions regarding which of these payment modes produce good approximations of the underlying welfare measures. From a standard economic theory, largely based on time allocation model (Becker, 1965), one can deduce that the payment mode should be irrelevant to the estimation of welfare measures. This is because different payment modes are convertible as have been demonstrated by studies that derive welfare measures for different payment modes (see Larson and Shaikh, 2002, Eom and Larson, 2006). However, Lee et al. (2015) conclude from experimental studies that decisions in money and time domains influence the stability of consumer preferences. Specifically, Lee et al. (2015) observe that decisions in money are processed analytically and decisions in time are processed affectively. The insight is that there are qualitative differences in the processing of decisions in money and time; and thus consumer preferences are unstable.====The adoption of different payment modes in stated preference elicitation has bearings on resource allocations decisions. One of the main uses of stated preferences is to provide welfare measures for use in cost-benefit analyses (see Navrud and Pruckner, 1997). Theoretically, the use of non-monetary numeraires in evaluations have long been noted to depend on whether good is question is a public good or private good (see e.g. Brekke, 1997, Dreze, 1998). Specifically, Brekke (1997) notes that the choice of numéraire matters in the evaluation of public goods provision but not in the purchase of private goods. This is because the choice of numéraires requires normalization of prices and marginal utility; and these normalizations make the choice of numéraires to matter for public goods but not for private goods (Brekke, 1997). In addition, Dreze (1998) assesses the consequences of numéraire for private goods, and notes that under market imperfections e.g. rationing and market segmentation, the choice of numéraire matters for private goods as well. On this account, one would be interested to know the implications of conducting stated preferences in monetary and non-monetary payment modes in order to address some of the effects of payment modes in project evaluations using different numeraires.====A number of studies have investigated the effects of using non-monetary numeraires rather than monetary payments largely based on contingent valuation (CV) studies. The CV studies that compare non-monetary payment modes with monetary payment modes often find that respondents are more likely to state a positive willingness-to-pay (WTP) under non-monetary payment modes (see e.g. Brouwer et al., 2008, Vondolia et al., 2014). Vondolia et al. (2014) attribute some of these disparities in CV responses to experience with these modes of payments. In many of these studies, the respondents usually prefer to contribute time rather than contribute money (Whittington et al., 1990). Consequently, the welfare estimates from CV studies have been found to be higher under non-monetary numeraires. For example, Echessah et al. (1997) use the average wage rate of a casual worker to convert WTP in time into monetary measure and observe that the mean WTP is higher when labour payment mode is used. O'Garra (2009) finds in a CV study on the valuation of fishing grounds that willingness to contribute time is three time higher than WTP estimated in monetary value when the adjusted wage rate is used to convert the willingness to contribute time into monetary value. In a choice experiment, Rai and Scarborough (2013) find that the shadow value of household labour is lower than the market wage rate among farm households.====The above anomalies have been used to question the extent to which resultant non-monetary welfare measures can be used to evaluate the allocation of scarce resources. The main thrust of this criticism is an assessment of the use of non-monetary payment vehicles by Ahlheim et al., 2010, Ahlheim et al., 2017 who present theoretical and empirical results to argue that the adoption of alternative payment modes such as labour time in CV studies cannot be relied upon for decisions on the allocation of funds for public projects. This is because the “dollar is a dollar” rule does not hold for welfare estimates elicited using alternative payment modes. Secondly, the evidence on the quality of survey responses under monetary and non-monetary payment modes from CV studies are inconclusive. Larson et al. (2004) found that in CV the mean standard error of WTP in money terms was $1.29, while the mean standard error of WTP in terms of time was 5.93 h. This implies that the willingness to pay time is noisier than the willingness to pay money because the latter is a more familiar mechanism for value expression than former. In a similar test of uncertainty in CV, Pondorfer and Rehdanz (2015) compare the uncertainties in stated willingness to contribute time and money for a local public good using CV and find that the uncertainty is reduced when WTP is elicited in labour rather than in money.====Few choice experiments have assessed the effects of payment options. Gyrd-Hansen and Skjoldborg (2008) compare the reactions to out-of-pocket payments with tax payments. The results indicate that the opposition towards out-of-pocket payments is more pronounced than in tax payments. In addition, it was found that focus shifted from the quality attributes and this could induce higher levels of random error. However, it is not possible to link these shifts in quality attributes to changes in preferences as scale parameter is confounded with parameter estimates (see Hess and Rose, 2012). The present study contributes to the existing applications on the use of non-monetary payment vehicles in stated preference elicitation by examining the effects of different numeraires on survey responses in choice experiments in a developing country context. The specific aim is to assess uncertainties among money, labour time and harvests payment modes in using split sample choice experiments on the demand for a private good (i.e. flood insurance) among smallholder farmers in a developing country. The choice of private good is to provide the minimum levels of market distortions against which to evaluate differences in uncertainties among the three payment modes. The choice of these non-monetary payment modes were informed by the applications in the literature in which labour time and harvest are adopted to elicit preferences in stated preference surveys. Furthermore, the purchase of insurance using labour time is one of interventions proposed to the increase the purchase of insurance for climate change adaptation (Bals et al., 2006, Oxfam America, 2012).====For the estimation, we adopt an integrated modeling framework to test for differences in relative scale parameters corresponding to the three different payment modes. In the integrated modeling framework, we jointly model choice experiments conducted on the purchase of flood insurance in which insurance premiums are required in money, labour time and harvest to allow relative scale parameters to differ among these three payment vehicles. We find that the relative scale parameters for non-monetary payment modes of labour time and harvests are lower than the relative scale for monetary payment mode. Given the similarities in context for the choice experiments e.g. complexities and estimations, we interpret these results to mean that these non-monetary payment modes exhibit higher degrees of uncertainties. These findings suggest that the requirements for sensitivity analyses in cost-benefit analyses using non-monetary welfare estimates from stated preferences are higher. This is mainly because the responses under these non-monetary payment modes are noisier.====The rest of the paper is structured as follows: Section 2 presents the derivation of welfare under multiple constraints and section 3 focusses on the integrated econometric modelling of choice experiments under multiple payment modes. This is followed by a description of the study area and the design of the choice experiments in section four. Section 5 presents the results from the choice experiments, and section six concludes.",Are non-monetary payment modes more uncertain for stated preference elicitation in developing countries?,https://www.sciencedirect.com/science/article/pii/S1755534517300507,March 2019,2019,Research Article,123.0
"Hess Stephane,Palma David","Institute for Transport Studies and Choice Modelling Centre, University of Leeds, UK","Available online 4 June 2019, Version of Record 31 July 2019.",https://doi.org/10.1016/j.jocm.2019.100170,Cited by (281),"The community of choice modellers has expanded substantially over recent years, covering many disciplines and encompassing users with very different levels of econometric and computational skills. This paper presents an introduction to ====, a powerful new freeware package for ==== that aims to provide a comprehensive set of modelling tools for both new and experienced users. ==== also incorporates numerous post-estimation tools, allows for both classical and Bayesian estimation, and permits advanced users to develop their own routines for new model structures.","Choice modelling techniques have been used across different disciplines for over four decades (see McFadden, 2000 for a retrospective and Hess and Daly, 2014 for recent contributions and applications across fields). For the majority of that time, the number of users of especially the most advanced models was rather small, and similarly, a small number of software packages was used by this community. In the last two decades, the pool of users of choice models has expanded dramatically, in terms of their number as well as the breadth of disciplines covered. At the same time, we have seen the development of new modelling approaches, and gains in computer performance as well as software availability have given an ever broader group of users access to ever more advanced models.====These developments have also seen a certain fragmentation of the community in terms of software, which in part runs along discipline lines. Notwithstanding the most advanced users who develop their own code for often their own models, there is first a split between the users of commercial software and those using freeware tools. Commercial packages have historically been computationally more powerful but may have more limitations in terms of available model structures or the possibility for customisation. On the other hand, freeware packages may have limitations in terms of performance and user friendliness but may benefit from more regular developments to accommodate new model structures.====A further key differentiation between packages is the link between user inputs and interface and the actual underlying methodology. Many existing packages, both freeware and commercial, are black box tools where the user has little or no knowledge of what goes on “under the hood”. While this has made advanced models accessible to a broader group of users, a disconnect between theory and software not only increases the risk of misinterpretations and misspecifications, but can also hide relevant nuances of the modelling process and mistakenly give the impression that choice models are “easy tools” to use. On the other hand, software that relies on users to code all components from scratch arguably imposes too high a bar in terms of access.====Existing software also almost exclusively allows the use of only either classical estimation techniques or Bayesian techniques. This fragmentation again runs largely in parallel with discipline boundaries and has only served to further contribute to the lack of interaction/dialogue between the classical and Bayesian communities. A final difference arises in terms of software environment. While commercial software usually provides a custom user interface, freeware options in general (though not exclusively) rely on existing statistical or econometric software and are made available as packages within these. The latter at times means that ==== packages are not really free to use (if the host software is not), while there are also cases of software being accessible only in either Windows or Linux, not both.====The above points served in large part as the motivation for the development of ==== (cf. Fig. 1). Our aims were:====While ==== is easy to use, we also remain of the opinion that users of choice modelling software should understand the actual process that happens during estimation. For this reason, the user needs to explicitly include or exclude calls to specific functions that are model and dataset specific. For example, in the case of repeated choice data, the user needs to include a call to a function that takes the product across choices for the same person (====). Or in the case of a mixed logit model, the user needs to include a call to a function that averages across draws (==== and/or ====). If calls to these functions are missing when needed, or if a user makes a call to a function that should not be used in the specific model, the code will fail, and provide the user with feedback about why this happened. This is in our view much better than the software permitting users to make mistakes and fixing them behind the scenes.==== is the culmination of many years of development of individual choice modelling routines, starting with code developed by Hess while at Imperial College (cf. Hess, 2005) using Ox (Doornik, 2001). This code was gradually transitioned to ==== at the University of Leeds, with substantial further developments once Palma joined the team in Leeds, bringing with him ideas developed at Pontificia Universidad Católica de Chile (cf. Palma, 2016). No code is an island, and we have been inspired especially by ALogit (ALogit, 2016) and Biogeme (Bierlaire, 2003), and ==== mirrors at least some of their features.====This paper presents a brief introduction to the capabilities of ====. We focus on the case of a hybrid choice model so as to give an illustration of the functionalities of the package. We illustrate this using both classical and Bayesian estimation and also explain a number of pre-estimation and post-estimation functions. Of course, in the context of an academic paper, we can only scrape the surface of the full level of detail, and furthermore, software packages change over time. For this reason, a more detailed manual (which also shows full details on function inputs) along with numerous examples (with data) and a user forum is available on the ==== website (====). Users can also obtain help on specific functions directly in R, using e.g. ?==== for help on the ==== function. The syntax in the present paper is for ==== version 0.0.8, but should remain forward compatible where not otherwise noted in the online manual. We strongly recommend prospective users to study the actual manual in detail rather than just relying on the short overview in the present paper.====This paper does not include any comparisons with other packages in terms of capabilities or speed, so as not to risk misrepresentations but also given the growing number of freeware tools, some of which we might not be aware of. The code has been widely tested to ensure accuracy. In our view, any speed comparison offers little practical benefit. For simple models, there is a clear advantage for highly specialised code, while, for complex models, any benchmarking is impacted substantially by the specific implementation and degree of optimisation used.====The remainder of this paper is organised as follows. The following section briefly talks about installation. Section 3 discusses the econometric setup for our empirical example. Section 4 then presents the hybrid choice model application using classical estimation, with the Bayesian version covered in Section 5. A number of other functions are discussed in Section 6 before we present a summary in Section 7.",": A flexible, powerful and customisable freeware package for choice model estimation and application",https://www.sciencedirect.com/science/article/pii/S1755534519300703,4 June 2019,2019,Research Article,127.0
"van Cranenburgh Sander,Collins Andrew T.","Delft University of Technology, Faculty of Technology, Policy & Management, Transport and Logistics Group, the Netherlands,Institute of Transport and Logistics Studies (ITLS), The University of Sydney Business School, The University of Sydney, NSW, Australia","Received 8 August 2018, Revised 2 April 2019, Accepted 2 April 2019, Available online 8 April 2019, Version of Record 14 May 2019.",https://doi.org/10.1016/j.jocm.2019.04.002,Cited by (5),"At the time of creating an experimental design for a stated choice experiment, the analyst often does not precisely know which model, or decision rule, he or she will estimate once the data are collected. This paper presents two new software tools for creating stated choice experimental designs that are simultaneously efficient for regret minimisation and utility maximisation decision rules. The first software tool is a lean, easy-to-use and free-of-charge experimental design tool, which is dedicated to creating designs that incorporate regret minimisation and utility maximisation decision rules. The second tool constitutes a newly developed extension of Ngene – a widely used and richly featured software tool for the generation of experimental designs. To facilitate the use of the new software tools, this paper presents clear worked examples. It focusses on practical issues encountered when generating such decision rule robust designs, such as how to obtain priors and how to deal with alternative specific parameters. Furthermore, we analyse the robustness of the designs that we created using the new software tools. Our results provide evidence that designs optimised for one decision rule can be inefficient for another – highlighting the added value of decision rule robust designs.","Stated Choice (SC) experiments are widely used to acquire understanding of choice behaviour in a variety of research fields, including but not limited to transportation, marketing, and health and environmental economics (Louviere et al., 2000; Street and Burgess, 2007; Rose and Bliemer, 2009; de Bekker-Grob et al., 2012). In SC experiments respondents are presented with choice tasks involving two or more hypothetical alternatives, which are described by a set of attributes and attribute levels. Respondents are asked to assess the alternatives and make a choice, typically of their most preferred alternative. Prior to the SC experiment the analyst creates the experimental design, which involves allocating attribute levels to the alternatives of each choice task. There are different approaches to generate experimental designs, of which so-called efficient designs are most common at present. Efficient designs aim to maximise the information obtained from the SC data, resulting in more reliable parameter estimates for a given number of observations (Rose and Bliemer, 2009; Kessels et al., 2011).====To date, research and software for experimental design have almost exclusively been based on the (often implicit) assumption that decision-makers make choices using a (linear-additive) Random Utility Maximisation (RUM) decision rule. However, a growing number of studies have found overwhelming evidence that decision-makers may opt for other types of decision rules when making choices (Kivetz et al., 2004; Hess et al., 2012; Leong and Hensher, 2012; Guevara and Fukushi, 2016; Hancock et al., 2018; Van Cranenburgh and Alwosheel, 2019). In light of this, very recently a method to create efficient experimental designs for one alternative decision rule, namely Random Regret Minimisation (RRM), has been proposed (Van Cranenburgh et al., 2018). RRM models postulate that decision-makers choose the alternative that provides them with minimum regret, which is caused by the need to trade-off attributes of alternatives during the decision making process (Chorus, 2010). Unlike its RUM counterpart, RRM models feature a particular reference-dependent type of semi-compensatory behaviour.====One particularly important result of the study by Van Cranenburgh et al. (2018) is that they find that designs that are efficient for estimating RUM models can be highly inefficient for estimating RRM models, and vice versa. Therefore, they advocate taking multiple decision rules into account when creating efficient experimental designs. To create such ‘decision rule robust designs’ they propose to use a model averaging approach, akin to the approach taken by Rose et al. (2009) to account for uncertainty regarding the model specification (e.g., multinomial logit, mixed logit). But, although the theory to devise designs which are robust toward the uncertainty on the side of the analyst regarding the underlying decision rule has recently been established, the burden to actually generate these designs is currently high: it requires extensive software coding on the side of the analyst.====This paper aims to lower the burden for analysts who wish to create SC experimental designs that are simultaneously efficient for estimating RUM and RRM models. In particular, it presents two software tools in which such decision rule robust designs can be generated. The first software tool is called ==== (RDG). RDG is a lean, easy-to-use and free-of-charge experimental design tool, running in a MATLAB environment. RDG is confined to the design of unlabelled experiments with three alternatives. The second tool constitutes a newly developed extension of Ngene. Ngene is an established, highly versatile commercial software dedicated to the design of SC experiments (ChoiceMetrics, 2018). To facilitate creating decision rule robust efficient designs using the two new software tools, this paper presents clear and worked out examples. It focusses on practical issues encountered when generating such efficient designs, such as how to obtain priors and how to deal with alternative specific parameters.====The remaining part of this paper is organised as follows. Section 2 briefly revisits efficient design theory for RRM models. Section 3 shows how to create efficient designs using the two software tools. In this section the robustness of the created designs is also analysed. Finally, section 4 provides a conclusion and discusses new avenues for further research.",New software tools for creating stated choice experimental designs efficient for regret minimisation and utility maximisation decision rules,https://www.sciencedirect.com/science/article/pii/S1755534518300940,8 April 2019,2019,Research Article,129.0
